quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Deployability,"For future upgraders, there's something about pandas 1.1.5 that causes `pylint` to fail. It seems like underlying ast processing library `astroid` enters infinite recursive loop when pandas 1.1.5 is installed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9804#issuecomment-741921520:11,upgrade,upgraders,11,https://hail.is,https://github.com/hail-is/hail/pull/9804#issuecomment-741921520,2,"['install', 'upgrade']","['installed', 'upgraders']"
Deployability,"For linreg, logrem, lmmreg, and skat:; - changed Python implementation to annotate or select on `x` if not a field and always pass `x_field`, which must be float64 but may have missing values.; - changed Scala linreg, logrem, lmmreg, and skat to take `xField` rather than `xExpr`. Updated Scala tests with selectEntry accordingly.; - replaced RegressionUtils `inputVector` with `setMeanImputedDoubles`; - removed `dataset` parameter from Python. Now all methods that take a dataset and one or more required expressions on that dataset now only take the expressions. Updated docs, tests, tutorial accordingly.; - added `req_tstring` to linear_mixed_regression and `We plan to change the interface to this method in Hail 0.2 while maintaining its functionality.` The constraint is due to string assumption made when comparing and filtering column keys against keys on KinshipMatrix. Since the latter is going away (and marked as such), I don't think it's worth more changes to remove the constraint.; - made docs more consistent and variable names more generic (sample=>col, variant=>row, etc)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3289:281,Update,Updated,281,https://hail.is,https://github.com/hail-is/hail/pull/3289,2,['Update'],['Updated']
Deployability,"For my pipeline code, I need a way to iterate through the list of jobs submitted and collect their error codes to determine if a pipeline failed and if so print out the log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5200:7,pipeline,pipeline,7,https://hail.is,https://github.com/hail-is/hail/pull/5200,2,['pipeline'],['pipeline']
Deployability,"For now, downgrade:; ```; pip3 install 'ipython<8.17'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14166#issuecomment-1896447300:31,install,install,31,https://hail.is,https://github.com/hail-is/hail/issues/14166#issuecomment-1896447300,1,['install'],['install']
Deployability,"For posterity, update the date in the change log to reflect when PR #12987 was merged and the release made. Very minor, and probably already less important than it was last week, but for future readers it's useful for these to be aligned…",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13047:15,update,update,15,https://hail.is,https://github.com/hail-is/hail/pull/13047,2,"['release', 'update']","['release', 'update']"
Deployability,"For reasons completely unclear to me, on Mac OS X, you have to install a JDK to; get the `java` command line tool. https://stackoverflow.com/questions/34074039/java-command-line-requires-jdk-on-mac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9088:63,install,install,63,https://hail.is,https://github.com/hail-is/hail/pull/9088,1,['install'],['install']
Deployability,For some reason this is timing out after a minute in my dev deploy even though I've removed the heartbeat and it's working on a local server I have running. Need to investigate further.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9636#issuecomment-715604047:60,deploy,deploy,60,https://hail.is,https://github.com/hail-is/hail/pull/9636#issuecomment-715604047,1,['deploy'],['deploy']
Deployability,"For some reason we delete secrets when re-creating them. This creates a race condition which Batch; appears to encounter whenever it is under heavy load during a deploy. @cseed, do you recall why you chose to delete and then use apply in ci/create_database.py?. I am using the command sequence suggested here: https://stackoverflow.com/questions/45879498/how-can-i-update-a-secret-on-kubernetes-when-it-is-generated-from-a-file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10217:162,deploy,deploy,162,https://hail.is,https://github.com/hail-is/hail/pull/10217,2,"['deploy', 'update']","['deploy', 'update-a-secret-on-kubernetes-when-it-is-generated-from-a-file']"
Deployability,"For some reason, all output was suppressed so I wasn't even seeing my debugging output. 502 is labelled ""transient"" but it's not always. I still don't know what's wrong, but something like this would have prevented me from running in circles trying to figure out what the hell changed (this works in a PR, it's only broken in dev deploy, don't know why yet).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7759:330,deploy,deploy,330,https://hail.is,https://github.com/hail-is/hail/pull/7759,1,['deploy'],['deploy']
Deployability,"For some reason, the old application, identified by BH4D9OD16A has been disabled. This updates us to the new one which is still valid.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14283:87,update,updates,87,https://hail.is,https://github.com/hail-is/hail/pull/14283,1,['update'],['updates']
Deployability,"For terra I currently have the front-end and driver running in the same pod, so I can't have the front-end listening on port 443 since it's in use by something else in the pod. Seemed like a reasonable enough change on its own. There will be an entirely separate `deployment.yaml` for terra so better to expose small options like these there than in the python code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12613:264,deploy,deployment,264,https://hail.is,https://github.com/hail-is/hail/pull/12613,1,['deploy'],['deployment']
Deployability,"For testing, we start a server with `python ci/ci.py`. That doesn't appear to work. Do we need to pip install it instead?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4709#issuecomment-435169908:102,install,install,102,https://hail.is,https://github.com/hail-is/hail/pull/4709#issuecomment-435169908,1,['install'],['install']
Deployability,For that reason I'm somewhat skeptical and given the future of our scala code feel like we shouldn't spend much effort on non-trivial updates,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12226#issuecomment-1259454277:134,update,updates,134,https://hail.is,https://github.com/hail-is/hail/pull/12226#issuecomment-1259454277,1,['update'],['updates']
Deployability,"For the `hail` python package, even if some things don't work great/at all, I think it would be nice if it at least installed on windows. `uvloop` is unsupported on windows, so I add a little logic to ensure that it's not requried on windows and the copy tool doesn't fail if it's not found.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11740:116,install,installed,116,https://hail.is,https://github.com/hail-is/hail/pull/11740,1,['install'],['installed']
Deployability,"For the partitioning algorithm, I updated the test to confirm that all the individuals in the unrelated set are mutually unrelated. For PC-AiR, I updated the test to compare the loadings to PCA on just the unrelated individuals. The loadings are NumPy close. The scores are slightly different though because they are calculated differently. When there are related individuals, the scores are calculated by multiplying the loadings and the standardized genotypes. When there are no related individuals, the scores are calculated by multiplying the columns of the appropriate singular matrix with the eigenvalues. So for the scores, I just add a regression test. (In my testing, I observed that most of the scores were less than 1% different. However, there were a few differences that were larger around 20% or 30%. Not sure if this is a cause for concern because the calculation approaches are different and the SVD is approximate.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14326#issuecomment-1977943502:34,update,updated,34,https://hail.is,https://github.com/hail-is/hail/pull/14326#issuecomment-1977943502,2,['update'],['updated']
Deployability,Force merging due to broken batch. Hand deploying CI now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8424#issuecomment-607411084:40,deploy,deploying,40,https://hail.is,https://github.com/hail-is/hail/pull/8424#issuecomment-607411084,1,['deploy'],['deploying']
Deployability,Forgot to authorize when I did dev deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6645:35,deploy,deploy,35,https://hail.is,https://github.com/hail-is/hail/pull/6645,1,['deploy'],['deploy']
Deployability,Forgot to change patch number,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7597:17,patch,patch,17,https://hail.is,https://github.com/hail-is/hail/pull/7597,1,['patch'],['patch']
Deployability,Forgot to update this usage of the `pr_table` macro when fixing search bars.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10194:10,update,update,10,https://hail.is,https://github.com/hail-is/hail/pull/10194,1,['update'],['update']
Deployability,"From Cotton:. I think the request/s and request latency metrics in Grafana are not actually the metrics for the Kubernetes service as we'd hoped. In particular, the reqs/s makes no sense. This post:. https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-4-the-kubernetes-api-server-72f1e1210770. indicates we should have an apiserver_request_count and apiserver_request_latencies_bucket which sound like what we want. They give ""the Prometheus configuration for getting metrics from the Kubernetes API server, even in environments where the masters are hosted for you"" (and I think our master is hosted for us in GKE). In particular, we have no analogous apiserver scrape config with ""role: endpoints"" in our setup. Here is the apiserver code with all the metrics they collect: https://github.com/kubernetes/apiserver/blob/master/pkg/endpoints/metrics/metrics.go",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6495:459,configurat,configuration,459,https://hail.is,https://github.com/hail-is/hail/issues/6495,1,['configurat'],['configuration']
Deployability,From Cotton:. Items to be address:. - [ ] Recursively make push the jupyter image and embed its entire hash in your Docker image or deployment; - [ ] Remove unused stuff for building images. cc @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5822:132,deploy,deployment,132,https://hail.is,https://github.com/hail-is/hail/issues/5822,1,['deploy'],['deployment']
Deployability,"From a fresh clone, the above (modified with `rm -f`) fails with: ; ```bash; $ rm -f hail/upload-remote-test-resources && make -C hail upload-remote-test-resources; make: Entering directory '/home/edmund/.local/src/hail/hail'; # # If hailtop.aiotools.copy gives you trouble:; # gcloud storage cp -r src/test/resources/\* gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/; # gcloud storage cp -r python/hail/docs/data/\* gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/; python3 -m hailtop.aiotools.copy -vvv 'null' '[\; {""from"":""src/test/resources"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/""},\; {""from"":""python/hail/docs/data"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/""}\; ]' --timeout 600; /home/edmund/.local/src/hail/.venv/bin/python3: Error while finding module specification for 'hailtop.aiotools.copy' (ModuleNotFoundError: No module named 'hailtop'); make: *** [Makefile:355: upload-remote-test-resources] Error 1; make: Leaving directory '/home/edmund/.local/src/hail/hail'; ```. I'll try again with `hailtop` installed - just wanted to point out the dependency failure in `Makefile`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1887719777:1100,install,installed,1100,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1887719777,1,['install'],['installed']
Deployability,"From that VM, I can get into the container, install the jdk, then run jstack on one of the hung JVMs.; ```; # jstack 1433; ...; ""pool-1-thread-1"" #18 prio=5 os_prio=0 tid=0x00007f50c4f23000 nid=0x82c waiting on condition [0x00007f5084eeb000]; java.lang.Thread.State: WAITING (parking); 	at sun.misc.Unsafe.park(Native Method); 	- parking to wait for <0x00000000e8ddaea0> (a scala.concurrent.impl.Promise$CompletionLatch); 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); 	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242); 	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258); 	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263); 	at scala.concurrent.Await$.$anonfun$result$1(package.scala:220); 	at scala.concurrent.Await$$$Lambda$2201/1092639564.apply(Unknown Source); 	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:57); 	at scala.concurrent.Await$.result(package.scala:146); 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:145); ...; ```. This is the line that waits to upload the compiled code for the workers to Google Cloud Storage. The other threads appear to be waiting on the memory service:; ```; ""pool-2-thread-2"" #27 prio=5 os_prio=0 tid=0x00007f5028ad9000 nid=0x88d waiting on condition [0x00007f50274fc000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:44,install,install,44,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['install'],['install']
Deployability,"From the man page:. ```; -t Don't run, just test the configuration file. The nginx; checks configuration for correct syntax and then tries; to open files referred in configuration.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4429:53,configurat,configuration,53,https://hail.is,https://github.com/hail-is/hail/pull/4429,3,['configurat'],['configuration']
Deployability,"Full error message:. ```[Stage 2:> (0 + 0) / 16]2018-02-28 23:41:58 Hail: INFO: interval filter loaded 2453 of 103675 partitions; 2018-02-28 23:42:08 Hail: WARN: deprecation: 'drop_cols' will be removed before 0.2 release; Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 415, in check_all; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 53, in check; hail.typecheck.check.TypecheckFailure. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 220, in <module>; try_slack(args.slack_channel, main, args); File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 94, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 187, in main; vds, sample_table = generate_qc_annotations(vds, medians=not args.skip_medians); File ""<decorator-gen-616>"", line 2, in __getitem__; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 478, in _typecheck; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 425, in check_all; TypeError: __getitem__: parameter 'item': expected (str or tuple[(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression]),(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression])]), found int: '0'```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3038:214,release,release,214,https://hail.is,https://github.com/hail-is/hail/issues/3038,1,['release'],['release']
Deployability,Fully deploy is now working again.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4592#issuecomment-432499924:6,deploy,deploy,6,https://hail.is,https://github.com/hail-is/hail/issues/4592#issuecomment-432499924,1,['deploy'],['deploy']
Deployability,"Functions like `sample_qc` and `variant_qc` do not produce the expected results on sex chromosomes for samples represented as diploid homozygotes or missing. To me, it seems like the right solution is to represent XY individuals as haploid on X and Y, and XX individuals as 0-ploid (but called) on Y. To my estimation, variant_qc will mostly compute *the right thing*™ if the data is represented in such a way. Updates to the call stats aggregator to count by ploidy may produce QoL improvements in this world as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7930:411,Update,Updates,411,https://hail.is,https://github.com/hail-is/hail/issues/7930,1,['Update'],['Updates']
Deployability,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:750,integrat,integration,750,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665,2,['integrat'],['integration']
Deployability,"Furthermore, update build.gradle to (finally) not print all the; Unsafe warnings.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9221:13,update,update,13,https://hail.is,https://github.com/hail-is/hail/pull/9221,1,['update'],['update']
Deployability,"G.yml</li>; <li>Additional commits viewable in <a href=""https://github.com/Textualize/rich/compare/v12.6.0...v13.5.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=rich&package-manager=pip&previous-version=12.6.0&new-version=13.5.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). You can trigger a rebase of this PR by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>. > **Note**; > Automatic rebases have been disabled on this pull request as it has been open for over 30 days.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13380:8115,upgrade,upgrade,8115,https://hail.is,https://github.com/hail-is/hail/pull/13380,3,['upgrade'],['upgrade']
Deployability,"GCR is disabled. I can delete the artifacts whenever we're confident they are unneeded. I'll probably delete this week. Dataproc buckets exist, but we're waiting on https://github.com/hail-is/hail/pull/14270 to merge before we delete the multi-regional buckets. US VEP and Datasets API are copied and ready for use. Europe is in process. UK is being moved in order to rename it from -uk- to -europe-west2- to comply with the new regional naming scheme. Once everything is transitioned, we need to merge https://github.com/hail-is/hail/pull/14286 and release. Then loudly inform everyone of the loss of these buckets. Then we delete them before March 1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13507#issuecomment-1939789668:550,release,release,550,https://hail.is,https://github.com/hail-is/hail/issues/13507#issuecomment-1939789668,1,['release'],['release']
Deployability,GE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERS,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:10950,release,release,10950,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,"GES</li>; <li><a href=""https://github.com/pygments/pygments/commit/c97762448b1e4eac8d74b8d88415f23c32aa0cdd""><code>c977624</code></a> Refactor PythonConsoleLexer as a DelegatingLexer (<a href=""https://redirect.github.com/pygments/pygments/issues/2412"">#2412</a>)</li>; <li><a href=""https://github.com/pygments/pygments/commit/50dd4d80e25c4c4afab503d41b471a536ed2af13""><code>50dd4d8</code></a> Python console: do not require output that looks like a traceback to be valid...</li>; <li><a href=""https://github.com/pygments/pygments/commit/96a0cdf200ab8a36dc5f6f748f3b9d01c05cb91b""><code>96a0cdf</code></a> PythonTracebackLexer: minor tweak in docstring</li>; <li><a href=""https://github.com/pygments/pygments/commit/569eea6ee85ec4d679bb38a890c167b58ee727dd""><code>569eea6</code></a> Enable Sphinx nitpicky mode and fix warnings (<a href=""https://redirect.github.com/pygments/pygments/issues/2403"">#2403</a>)</li>; <li><a href=""https://github.com/pygments/pygments/commit/b018a65cb6ef51596c2cb8d6c97f0d79d9fa2ae7""><code>b018a65</code></a> Prepare for next release.</li>; <li>See full diff in <a href=""https://github.com/pygments/pygments/compare/2.15.0...2.15.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pygments&package-manager=pip&previous-version=2.15.0&new-version=2.15.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12909:3434,release,release,3434,https://hail.is,https://github.com/hail-is/hail/pull/12909,1,['release'],['release']
Deployability,"GHA token permissions to be read-only</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/ac61b73da703df53707c31030b4ea51aab22d43c""><code>ac61b73</code></a> Backport publish workflow and process to 1.26.x</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1fd77edc1a1373c9a7e762de148f19f1e2edd418""><code>1fd77ed</code></a> Release 1.26.10</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/37ba00248424ea3cdf556cc3e7aa81ce0bf40382""><code>37ba002</code></a> [1.26] Update paid contributor program with early feedback</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/dddbab3612ead7d39d1dc33a5a504703a8d0eecf""><code>dddbab3</code></a> [1.26] Bump RECENT_DATE</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/6dd01c74102db0d608687953e351e31df3f31d9f""><code>6dd01c7</code></a> [1.26] Update docs for re-using HTTP connections after streaming</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2049c91f732ae4fec0216c0697dee7822c25db10""><code>2049c91</code></a> Adds changing branches for installing from git docs for 1.26.x</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/cb4950545be4d427557ce863539c08655c9bdd6e""><code>cb49505</code></a> [1.26] Improve testing for IPv6 scoped addresses</li>; <li>Additional commits viewable in <a href=""https://github.com/urllib3/urllib3/compare/1.26.9...1.26.11"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.9&new-version=1.26.11)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12104:3833,install,installing,3833,https://hail.is,https://github.com/hail-is/hail/pull/12104,1,['install'],['installing']
Deployability,"GHSA-4m77-cmpx-vjc4</a>)</li>; </ul>; <h3>Bugs fixed</h3>; <ul>; <li>Fixes focus indicator on input checkbox for Firefox <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15612"">#15612</a> (<a href=""https://github.com/alden-ilao""><code>@​alden-ilao</code></a>)</li>; </ul>; <h3>Documentation improvements</h3>; <ul>; <li>Fix link to yarn docs in extension migration guide <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15640"">#15640</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab/graphs/contributors?from=2023-12-29&amp;to=2024-01-19&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Abrichet+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​brichet</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Afcollonval+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​fcollonval</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Agithub-actions+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajtpio+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajupyterlab-probot+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​jupyterlab-probot</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Akrassowski+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ameeseeksmachine+updated%3A2023-12-29..2024-01-19&amp;type=Issues""><code>@​meeseeksmachine</code></a> | <a href=""https",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14184:1796,update,updated,1796,https://hail.is,https://github.com/hail-is/hail/pull/14184,1,['update'],['updated']
Deployability,GVS team confirms their pipeline containing interval literals went from >50 GB (crashing at that point) to less than 11GB! 👏,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13748#issuecomment-1791063434:24,pipeline,pipeline,24,https://hail.is,https://github.com/hail-is/hail/issues/13748#issuecomment-1791063434,3,['pipeline'],['pipeline']
Deployability,"Gah, OK, I think I have it now, but there was one more detail:. The gradle configuration `testCompileOnly` [1] *does not* inherit from the `shadow` configuration (as evidence see [this search](https://github.com/search?q=repo%3Ajohnrengelman%2Fshadow%20extendsFrom&type=code) of the shadow repo). We must explicitly request that `shadow` dependencies are included in the compile-time class path of the tests. This is as it should be: the things in `shadow` are things which are provided to us by our runtime environment. That's true of both the *test* runtime environment and the normal runtime environment. The Gradle Shadow plugin takes a different perspective by default, it suggests that `shadow` dependencies shouldn't be used in the tests at all. [1] NB: `testCompile` does not exist but you don't get an error if you try to use it, thanks for nothing gradle.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1710414563:75,configurat,configuration,75,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1710414563,2,['configurat'],['configuration']
Deployability,"Gah, caching. The changes haven't actually been deployed by ci yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5282#issuecomment-463364346:48,deploy,deployed,48,https://hail.is,https://github.com/hail-is/hail/issues/5282#issuecomment-463364346,1,['deploy'],['deployed']
Deployability,"Gateway receives the user IP (thanks to #8045). However, gateway is an HTTP; proxy, so packets from gateway necessarily come from gateway's IP. Gateway; places the user IP into the HTTP header `X-Real-IP`. All downstream servers; must: log `X-Real-IP` and forward `X-Real-IP` unadulterated. This PR makes that; change for `router`. - fix router Makefile (`domain` is now in `global`); - add `proxy.conf` which configures the standard proxy headers (importantly:; forwards `X-REAL-IP`); - for non-notebook servers, `include` the `proxy.conf`; - for notebook, update to include proxy headers; - override default `access_log` (which required checking in the default; `nginx.conf`); - lift other `http` directives into `nginx.conf` now that it is checked in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8058:558,update,update,558,https://hail.is,https://github.com/hail-is/hail/pull/8058,1,['update'],['update']
Deployability,"Getting this with current master on the cloud:. ```; Use of uninitialized value in hash element at /vep/ensembl-tools-release-85/scripts/variant_effect_predictor/Bio/EnsEMBL/Variation/Utils/VEP.pm line 4255, <VARS> line 1.; [Stage 18:=> (273 + 410) / 13592]Traceback (most recent call last):; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 75, in <module>; main(args, pops); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 51, in main; 'va.rf').write(args.output + "".autosomes.vds"", overwrite=True); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/utils.py"", line 452, in post_process_vds; vds = vds.vep(config=vep_config, csq=True, root='va.info.CSQ', force=True); File ""<decorator-gen-110>"", line 2, in vep; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/pyhail-attr.zip/hail/java.py"", line 93, in handle_py4j; hail.java.FatalError: NoSuchElementException: None.get; [Stage 18:=> (277 + 409) / 13592]java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@2a632cbb rejected from java.util.concurrent.ThreadPoolExecutor@974d518[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2913]; ```. Lmk if you need more log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1518:118,release,release-,118,https://hail.is,https://github.com/hail-is/hail/issues/1518,1,['release'],['release-']
Deployability,"Given a list of things that look like; ```; {'batch_id': 767,; 'job_id': 1,; 'state': 'Success',; 'spec': {'command': ['/bin/bash',; '-c',; 'set -e; mkdir -p /io/pipeline/pipeline-f0c3c92aa1c4/__TASK__0/; true'],; 'image': 'gcr.io/hail-vdc/benchmark_tpoterba:latest',; 'job_id': 1,; 'mount_docker_socket': False,; 'resources': {'cpu': '1', 'memory': '7G'},; 'pvc_size': '100G',; 'secrets': [{'namespace': 'batch-pods',; 'name': 'dking-gsa-key',; 'mount_path': '/gsa-key',; 'mount_in_copy': True}],; 'env': []},; 'attributes': {'task_uid': '__TASK__0', 'name': 'replicate_0'},; 'status': {'worker': 'batch-worker-default-5t5e9',; 'batch_id': 767,; 'job_id': 1,; 'attempt_id': 'be692b',; 'user': 'dking',; 'state': 'succeeded',; 'container_statuses': {'main': {'name': 'main',; 'state': 'succeeded',; 'timing': {'pulling': {'start_time': 1576710190946,; 'finish_time': 1576710248882,; 'duration': 57936},; 'creating': {'start_time': 1576710248882,; 'finish_time': 1576710248963,; 'duration': 81},; 'runtime': {'start_time': 1576710248963,; 'finish_time': 1576710250461,; 'duration': 1498},; 'starting': {'start_time': 1576710248963,; 'finish_time': 1576710249898,; 'duration': 935},; 'running': {'start_time': 1576710249898,; 'finish_time': 1576710250461,; 'duration': 563},; 'uploading_log': {'start_time': 1576710250464,; 'finish_time': 1576710250742,; 'duration': 278},; 'deleting': {'start_time': 1576710250743,; 'finish_time': 1576710250776,; 'duration': 33}},; 'container_status': {'state': 'exited',; 'started_at': '2019-12-18T23:04:09.890460985Z',; 'finished_at': '2019-12-18T23:04:10.111873413Z',; 'out_of_memory': False,; 'exit_code': 0}}},; 'start_time': 1576710248963,; 'end_time': 1576710250461},; 'msec_mcpu': 2796766,; 'cost': '$0.0000'}; ```. I'd like to be able to load this list as a struct with one command. One would think this arcane magic would do it:; ```; In [16]: t = hl.Table.parallelize([hl.struct(**x) for x in jobs]) ; ```; but of course, nested fields. Probably some partia",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7778:162,pipeline,pipeline,162,https://hail.is,https://github.com/hail-is/hail/issues/7778,2,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,Going to update the elasticsearch version and make a different PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9686#issuecomment-724956104:9,update,update,9,https://hail.is,https://github.com/hail-is/hail/pull/9686#issuecomment-724956104,1,['update'],['update']
Deployability,"Good catch, updated!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14619#issuecomment-2260855350:12,update,updated,12,https://hail.is,https://github.com/hail-is/hail/pull/14619#issuecomment-2260855350,1,['update'],['updated']
Deployability,"Good idea, I'll check. I feel like I initially found this in deep in a redhat tutorial, but ultimately found it again at the bottom of the [man page](https://man7.org/linux/man-pages/man8/xfs_quota.8.html). I was following this example:; ```; Enabling project quota on an XFS filesystem (restrict files in; log file directories to only using 1 gigabyte of space). # mount -o prjquota /dev/xvm/var /var; # echo 42:/var/log >> /etc/projects; # echo logfiles:42 >> /etc/projid; # xfs_quota -x -c 'project -s logfiles' /var; # xfs_quota -x -c 'limit -p bhard=1g logfiles' /var. Same as above without a need for configuration files. # rm -f /etc/projects /etc/projid; # mount -o prjquota /dev/xvm/var /var; # xfs_quota -x -c 'project -s -p /var/log 42' /var; # xfs_quota -x -c 'limit -p bhard=1g 42' /var; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10467#issuecomment-834771396:607,configurat,configuration,607,https://hail.is,https://github.com/hail-is/hail/pull/10467#issuecomment-834771396,1,['configurat'],['configuration']
Deployability,Good point. I'm going to first make sure that it would have failed on a sparse matrix with the `.get` and then show that the same test (hopefully!) passes for the updated PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5500#issuecomment-468696410:163,update,updated,163,https://hail.is,https://github.com/hail-is/hail/issues/5500#issuecomment-468696410,1,['update'],['updated']
Deployability,"Got annoyed with the constant re-tagging of images that don't need to be rebuilt, and decided to play a little make golf along the way. cc @jigold This should dramatically reduce the number of tags for hail-ubuntu from make-deployed images, though the number of layers in the container registry should not change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12841:224,deploy,deployed,224,https://hail.is,https://github.com/hail-is/hail/pull/12841,1,['deploy'],['deployed']
Deployability,Got blocked on dev deploy not working (500). Will try again in the morning.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-539818744:19,deploy,deploy,19,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539818744,1,['deploy'],['deploy']
Deployability,"Got it. In spite of my claim ""I was trying to avoid bulk operations"" I see that close_batch scans over all ready jobs (a bulk operation) to update ready_cores. I'm not quite sure what to do here. I'm not actually sure if a long-running query on close_batch is going to cause problems and it is not trivial to make it incremental.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7714#issuecomment-565101526:140,update,update,140,https://hail.is,https://github.com/hail-is/hail/pull/7714#issuecomment-565101526,1,['update'],['update']
Deployability,"Got it; for my own future reference I'm going to link to the [Cloud Dataproc Image version list](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions) and note that the 1.5 series, currently in [preview](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-1.5), does use Scala 2.12 as of December 2019 according to the [release notes](https://cloud.google.com/dataproc/docs/release-notes). Also, there's an [issue](https://issuetracker.google.com/issues/132603281) on Google Cloud's insanely janky issue tracker for the Scala version upgrade.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8009#issuecomment-580692854:296,release,release-,296,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-580692854,4,"['release', 'upgrade']","['release', 'release-', 'release-notes', 'upgrade']"
Deployability,Got removed moving to the new CI. Don't want to bother with installing R.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6080:60,install,installing,60,https://hail.is,https://github.com/hail-is/hail/issues/6080,1,['install'],['installing']
Deployability,Got rid of the cleanup on deploy. Still fixed the Makefile.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5955#issuecomment-486799272:26,deploy,deploy,26,https://hail.is,https://github.com/hail-is/hail/pull/5955#issuecomment-486799272,1,['deploy'],['deploy']
Deployability,"Got this error in a migration step when dev deploying. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 598, in _read_bytes; data = await self._reader.readexactly(num_bytes); File ""/usr/lib/python3.7/asyncio/streams.py"", line 679, in readexactly; await self._wait_for_data('readexactly'); File ""/usr/lib/python3.7/asyncio/streams.py"", line 473, in _wait_for_data; await self._waiter; File ""/usr/lib/python3.7/asyncio/selector_events.py"", line 804, in _read_ready__data_received; data = self._sock.recv(self.max_size); ConnectionResetError: [Errno 104] Connection reset by peer. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""create_database.py"", line 263, in <module>; loop.run_until_complete(async_main()); File ""/usr/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""create_database.py"", line 259, in async_main; await migrate(database_name, db, i, m); File ""create_database.py"", line 201, in migrate; (to_version, to_version, name, script_sha1)); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 26, in wrapper; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 229, in just_execute; async with self.start() as tx:; File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 114, in __aenter__; await tx.async_init(self.db_pool, self.read_only); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 135, in async_init; await cursor.execute('START TRANSACTION;'); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 428, in query; await self._read_query_result(unbuffere",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8761:44,deploy,deploying,44,https://hail.is,https://github.com/hail-is/hail/pull/8761,1,['deploy'],['deploying']
Deployability,Great change. Can you:; - delete chi1; - replace uses of chi1 with your more general version in the few places it appears in the code; - update the docs in HailExpressionLanguage.md to reflect only your version,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1393#issuecomment-280140767:137,update,update,137,https://hail.is,https://github.com/hail-is/hail/pull/1393#issuecomment-280140767,1,['update'],['update']
Deployability,"Great question. The need for two clients is we have an asynchronous one that ci uses and a synchronous one that pipeline uses. The client code in `aioclient.py` is the asynchronous one and the code in `client.py` is for the synchronous one. Rather than duplicating the code as was done before, I either had to make the synchronous client use the asynchronous code or vice versa. It was easier to make the asynchronous code synchronous by using the `run_until_complete` function and wrapping calls to the asynchronous classes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6208#issuecomment-497540673:112,pipeline,pipeline,112,https://hail.is,https://github.com/hail-is/hail/pull/6208#issuecomment-497540673,1,['pipeline'],['pipeline']
Deployability,"Great! I think we are good on the client. I'll modify it later today to use the new scheme. Before we get into the nitty gritty of the actual implementation, can we move on to the UI components and the semantics of using the client in `test_batch.py`? There's also a change to how the batch fields `time_closed` and `time_created` are used. I also added `time_updated`. I think the new semantics are:. - time_created -- time the batch was created; - time_updated -- time the last update was committed; - time_completed -- time the last time n_completed == n_jobs regardless of whether there are outstanding updates that haven't been committed. For old batches:; - time_created => same; - time_closed => time_updated; - time_completed => same. I also changed what the batch state means:; > There are only two batch states in the database: running and complete. A batch starts out as complete until an update is committed at which point if the n_jobs > 0, it will change to running. There are no longer ""open"" batches. . It's possible I didn't actually implement exactly what I described above as I was having a hard time figuring out whether time_updated should be equal to time_completed if the batch has no outstanding jobs to run. That's why I'd like to take a step back and make sure we agree on the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1218125677:480,update,update,480,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1218125677,3,['update'],"['update', 'updates']"
Deployability,Great! I think we should run the integration tests as part of `testAll` and kill the integration tests in the CI.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1827#issuecomment-302116530:33,integrat,integration,33,https://hail.is,https://github.com/hail-is/hail/pull/1827#issuecomment-302116530,2,['integrat'],['integration']
Deployability,"Great! No worries!. On Tue, 21 Apr 2020, 19:12 Patrick Schultz, <notifications@github.com>; wrote:. > @astheeggeggs <https://github.com/astheeggeggs> Thanks for the bug; > report. It lead to finding a rather serious bug. See; > https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; > for more details on what other regressions could have been affected. A new; > release should go out today with the fix.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/8349#issuecomment-617327917>, or; > unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABVQA76SRJ64CRAO36BG2GLRNXO2FANCNFSM4LS2ZGUA>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8349#issuecomment-617334301:402,release,release,402,https://hail.is,https://github.com/hail-is/hail/issues/8349#issuecomment-617334301,1,['release'],['release']
Deployability,"Great! So here's what the docs look like now:; https://hail.is/docs/devel/methods/genetics.html#hail.methods.nirvana. Here's the Python source:; https://github.com/hail-is/hail/blob/master/python/hail/methods/qc.py. You can see the built docs of this PR by clicking on Details next to the passing 2.2.0 test, and then clicking on Docs, e.g.:; https://ci.hail.is/viewLog.html?buildId=63354&buildTypeId=HailSourceCode_PRsOnly_HailTestJarSpark220&tab=report_project8_Docs. I'd appreciate if you could:; - ensure the docs are still accurate and add information on what version(s) of Nirvana is compatible.; - update the schema in the documentation to match your changes in Scala; - try running the same pipeline with a few block sizes to see whether its reasonable to reduce the default block size so that users will get more parallelism by default. I suspect a user with a 1 million variant VCF would prefer running 100 cores with 10k variants each to 2 cores with 500k variants each. I'd be surprised if the per-block overhead is so high to outweigh the benefit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3266#issuecomment-379138339:605,update,update,605,https://hail.is,https://github.com/hail-is/hail/pull/3266#issuecomment-379138339,2,"['pipeline', 'update']","['pipeline', 'update']"
Deployability,Great. Just address the localSize == 0 thing (fix or tell me why I'm wrong) and update to master and I'll merge.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/326#issuecomment-213803970:80,update,update,80,https://hail.is,https://github.com/hail-is/hail/pull/326#issuecomment-213803970,1,['update'],['update']
Deployability,"Great. So what I'm also interested in comparing is, if I just need, say, hail/pipeline/test, what's the download full tar and extract (of just hail/pipeline/test) vs download just hail/pipeline/test tar with full extract?. > There's something to be said for tar'ing everything except for .git, but I didn't carefully check which steps need it and which steps do not. I would have hoped no downstream steps need .git, but some build steps do trivially (e.g. look at the hash). Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090:78,pipeline,pipeline,78,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090,3,['pipeline'],['pipeline']
Deployability,"HANGES.rst [ci skip]</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/e37b25087d39bd54495380a9898c8c7a2a4698d1""><code>e37b250</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7244"">#7244</a> from radarhere/imagefont_max_string_length</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/d398fedb9d5af22316c715d2066176d15031d439""><code>d398fed</code></a> Added underscores for readability</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/1fe1bb49c452b0318cad12ea9d97c3bef188e9a7""><code>1fe1bb4</code></a> Added ImageFont.MAX_STRING_LENGTH</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/7c945f5131cf8596084b32af582f90a43b090540""><code>7c945f5</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7243"">#7243</a> from radarhere/releasenotes</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/0fb69fa821155c1b213f3f3488d3057b6ba7c154""><code>0fb69fa</code></a> Added release notes for <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7123"">#7123</a></li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/b7f1af77fd6ec9468438e25f72b003c16a9e6661""><code>b7f1af7</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7230"">#7230</a> from nulano/add-pyproject.toml</li>; <li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/9.5.0...10.0.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=9.5.0&new-version=10.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase ma",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:14211,release,release,14211,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['release'],['release']
Deployability,"HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/ffc91a940f531841cb7b25f898afbc247b29bce4""><code>ffc91a9</code></a> Prepare release 0.3.0 (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/9"">#9</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/9c2ad2b8226d285272ebee1180354e4e02408b62""><code>9c2ad2b</code></a> Add note about new maintainers (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/6"">#6</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/10a10e7c4508ac4d858cbe7c8ac9e46575c2bb5c""><code>10a10e7</code></a> Pass tag to workflow call (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/5"">#5</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/e0c7fd6723bd63db4183352d21dfbebd6c2553b1""><code>e0c7fd6</code></a> Prepare v0.3.0.rc1 with new release workflow (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/3"">#3</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/7d8e520aa7d0e71ef7144ce381c8a41464e687dc""><code>7d8e520</code></a> Modernize build and test infra (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/2"">#2</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/51f5bfe83178871fe2ee80df6b8e13ed54a2d897""><code>51f5bfe</code></a> Add GitHub Actions CI, test with 3.9 to 3.11 (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/1"">#1</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/bdbcc5d9650a8e8382979f089df3307dd4121b49""><code>bdbcc5d</code></a> Bump up coverage percentage cut at tox</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/7c7e4add6cb9f1a47a2303f819c8472491f6ebbb""><code>7c7e4ad</code></a> Add support for RFC 8769 (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/136"">#136</a>)</li>; <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12928:7467,release,release,7467,https://hail.is,https://github.com/hail-is/hail/pull/12928,1,['release'],['release']
Deployability,"HTTP/1.1"", ""response_status"": 200, ""response_size"": 279, ""request_header"": {""Referer"": ""-"", ""User-Agent"": ""Python/3.6 aiohttp/3.5.4""}}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:34,991"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; ```. Recall that pod creation happens in the background, so the `batches/9/create` and `batches/9/close` endpoints return to the client before pods are necessarily created. See batch.py:866, `Batch.close`. Likewise, the `batches/9/cancel` endpoint returns before the individual jobs are cancelled. There are now at most three concurrent threads of control interacting with the database and k8s. Eventually this sequence of log messages appears three times in quick succession. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6617:5249,update,update,5249,https://hail.is,https://github.com/hail-is/hail/issues/6617,1,['update'],['update']
Deployability,"Ha, couldn't have come at a better time -- this is the profile trace from a split/densify/sampleqc pipeline:. ![image](https://user-images.githubusercontent.com/10562794/126006170-ed653c9c-ec36-4d29-95b9-732be6313cca.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10672#issuecomment-881705887:99,pipeline,pipeline,99,https://hail.is,https://github.com/hail-is/hail/pull/10672#issuecomment-881705887,1,['pipeline'],['pipeline']
Deployability,"Had a quick chat with Konrad and since the other VEP bug is unrelated to these changes, it would be great if we could get this into master (if you're happy with these changes obviously) as at the moment we have to use different jars for different parts of the pipeline.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1531#issuecomment-286852400:260,pipeline,pipeline,260,https://hail.is,https://github.com/hail-is/hail/pull/1531#issuecomment-286852400,1,['pipeline'],['pipeline']
Deployability,"Haha, I had intentionally not assigned you until I sorted what I anticipate is a long tail of issues. Nonetheless, updated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12908#issuecomment-1515268289:115,update,updated,115,https://hail.is,https://github.com/hail-is/hail/pull/12908#issuecomment-1515268289,1,['update'],['updated']
Deployability,"Hail 0.1 isn't tested against or believed to work with Spark 2.2. Can you update to Hail 0.2 (devel)? 0.1 will be fully deprecated when 0.2 is released, and is already in its end of life process.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946#issuecomment-405779642:74,update,update,74,https://hail.is,https://github.com/hail-is/hail/issues/3946#issuecomment-405779642,2,"['release', 'update']","['released', 'update']"
Deployability,Hail CI Deploy for 0.1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4225:8,Deploy,Deploy,8,https://hail.is,https://github.com/hail-is/hail/pull/4225,1,['Deploy'],['Deploy']
Deployability,Hail Release 0.2.101: Upstream 1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12267:5,Release,Release,5,https://hail.is,https://github.com/hail-is/hail/pull/12267,1,['Release'],['Release']
Deployability,"Hail appears to have executed the exact same write command twice. The first write driver ends at 2023-10-13T01:17:55Z and the next write driver starts at 2023-10-13T01:18:11Z, just 16 seconds later. Batch: https://batch.hail.is/batches/8058522; Just the drivers: https://batch.hail.is/batches/8058522?q=name%3Dexecute%28...%29_driver. Driver & frontend logs indicate the first driver job completed and was almost immediately followed by a resubmission of the entire pipeline. https://cloudlogging.app.goo.gl/1344nayXTgaqKhCz8. # OLD. ### What happened?. NB: This is a development build 87398e1b514e. I think my comments below might be misleading. We purposely `WriteMetadata` multiple times, but with different `MetadataWriter`s. Unfortunately, this information does not appear in the SSA IR for some reason?. ---. The ""Relevant log output"" contains the last IR printed before the code was executed. The observed error was:. <details>; <summary>Expand me for the full trace. ```; Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```. </summary>. ```; Traceback (most recent call last):; File ""/Users/wlu/PycharmProjects/aou_gwas/scripts/pre_process_random_pheno.py"", line 345, in <module>; ); File ""/Users/wlu/PycharmProjects/aou_gwas/scripts/pre_process_random_pheno.py"", line 297, in main; mt = mt.filter_rows(mt.locus.in_autosome()); File ""<decorator-gen-1358>"", line 2, in write; File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hail/typecheck/check.py"", line 587, in wrapper; return __original_func(*args_, **kwargs_); File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hail/matrixtable.py"", line 2738, in write; Env.backend().execute(ir.MatrixWrite(self._mir, writer)); File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hail/backend/service_backend.py"", line 541, in execute; return self._cancel_on_ctrl_c(self._async_execute(ir, timed=time",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:466,pipeline,pipeline,466,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['pipeline'],['pipeline']
Deployability,"Hail depends on the `decorator` module, and in this case it looks like you've got it but it's out of date. The following should fix it:. ```bash; pip install -U decorator; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1818#issuecomment-302064968:150,install,install,150,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-302064968,1,['install'],['install']
Deployability,"Hail doesn't have a conda package -- the bioconda package there was not uploaded by the Hail Team (could even be malware -- we don't know). It's certainly a very old version. If you install Hail with pip, you should pick up the latest version 0.2.100 and have access to hl.vds, which is somewhat recent functionality.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-1262357111:182,install,install,182,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262357111,1,['install'],['install']
Deployability,"Hail has a set of commands that can be strung together by a user on the command line to create an analysis pipeline. We have a few users with development backgrounds who have started to build their own commands. It would be great if they could just throw those in their CLASSPATH and then run them directly from the command line by name. We'd have to pick a shell-friendly syntax, so something like, but not:. `$ hail importvcf ... $com.company.CustomLifeSavingAnalysis ...`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/353#issuecomment-240550514:107,pipeline,pipeline,107,https://hail.is,https://github.com/hail-is/hail/issues/353#issuecomment-240550514,1,['pipeline'],['pipeline']
Deployability,Hail is deployed to the Python package index: https://pypi.org/project/hail/. Adding a conda recipe isn't a high-priority task right now. What are the reasons that a PyPI package is insufficient?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8352#issuecomment-603528845:8,deploy,deployed,8,https://hail.is,https://github.com/hail-is/hail/issues/8352#issuecomment-603528845,1,['deploy'],['deployed']
Deployability,Hail is missing deploys 0.2.114 and 0.2.115 in Azure,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050:16,deploy,deploys,16,https://hail.is,https://github.com/hail-is/hail/issues/13050,1,['deploy'],['deploys']
Deployability,"Hail is no longer distributed as a zip file. You should use `make install-on-cluster` (described further [here](https://hail.is/docs/0.2/install/other-cluster.html)) to install Hail on a cluster. I do not recommend creating a wheel or zip file because the native binaries included in that wheel or zip file might be incompatible with the computer on which you install that file. If you are absolutely certain that the build machine *and the executing machine* can share native binaries, then you can do this:; ```; make wheel HAIL_COMPILE_NATIVES=1; ```; This produces a wheel file at `build/deploy/dist/hail-0.2.XX-py3-none-any.whl` where XX is the current patch version of Hail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10844#issuecomment-914353777:66,install,install-on-cluster,66,https://hail.is,https://github.com/hail-is/hail/issues/10844#issuecomment-914353777,6,"['deploy', 'install', 'patch']","['deploy', 'install', 'install-on-cluster', 'patch']"
Deployability,Hail requires Java 8. Please install Java 8: https://hail.is/docs/0.2/getting_started.html#requirements,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6747#issuecomment-515511257:29,install,install,29,https://hail.is,https://github.com/hail-is/hail/issues/6747#issuecomment-515511257,1,['install'],['install']
Deployability,Hail should have `hl.verify_installation` which checks that hail is properly installed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5051:77,install,installed,77,https://hail.is,https://github.com/hail-is/hail/issues/5051,1,['install'],['installed']
Deployability,"Hail tries to do a lot of data integrity checks and warn the user about problems. We've found a number of bugs in upstream tools and workflows that were arguably incorrect. But generating warnings when importing a 2TB file is a challenge in Spark. Right now we use Spark's Accumulators to accumulate classes of error messages and write them out at the end of the pipeline run (see the VCFReport object). However, we use them in non-actions and get incorrect reports (due to job restarts or reused stages in the pipeline). I have an idea about how to fix this by accumulating only at the end of a successful mapPartitions operation and recording the stageId and taskAttemptId from the TaskContext. The accumulator should only accumulate one of the reports from a successful mapPartitions. Using this, I wanted to build an abstraction for reporting warnings and other messages reliably on large import steps. If this works, we plan to float it up to the Spark mailing list to see if it can be of use, or at least write a nice blog post explaining how to get reliable accumulators in Spark. See the discussion here for the current situation:. http://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable. Closed as won't fix:. https://issues.apache.org/jira/browse/SPARK-732. Of course, I might be missing something obvious and this won't work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/371#issuecomment-240550289:363,pipeline,pipeline,363,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-240550289,2,['pipeline'],['pipeline']
Deployability,"Hail's benchmarks were kind of their own thing and a little neglected.; This change moves the benchmarks into the `hail/python` folder and updates them to use pytest with a custom plugin/set of pytest hooks.; Now, benchmarks can be run from the command line like any pytest.; This change removes the `benchmark-hail` (or `hailbench`) utility. Benchmarks are marked by `pytest.mark.benchmark` (via the `@benchmark` decorator).; By convention, benchmarks are python tests whose names are prefixed by `benchmark_` and are located in files with the same prefix.; Nothing enforces this, however, so you could name your benchmarks `test_*` and put them in files named `test_*.py`.; Benchmarks may import and use any test code or utilities defined in `test/`.; The results of each benchmark are outputted as json lines (`.jsonl`) to the file specified by the `--output` pytest arg or stdout. The folder structure should be familiar, resembling our `test/` directory.; I believe this is flexible enough to add `hailtop` benchmarks should we so wish:; ```; pytest.ini - hoisted from `test/` to include benchmark marks; benchmark/; - conftest.py for custom pytest command line args ; - hail/; - confest.py for custom plugin that runs hail benchmarks; - benchmark_*.py hail query benchmark code; - tools/; - shared utilites, including the `@benchmark`; ```; Supporting pytest fixtures required writing a custom plugin to run benchmarks, as using off-the-shelf; solutions like `pytest-benchmark` would forbid method level fixtures like `tmp_path` etc.; The plugin is designed to run ""macro-benchmarks"" (ie long-running tests) and fully supports pytest parameterisation.; For each benchmark, the plugin initialises hail and then repeats (for a number of iterations defined by the pytest mark); acquiring fixtures, timing invocation and tearing-down fixtures, finally stopping hail. It is therefore unsuitable for; microbenchmarks, for which we currenly have none in python. If we add them we'd need to tweak this s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14565:139,update,updates,139,https://hail.is,https://github.com/hail-is/hail/pull/14565,1,['update'],['updates']
Deployability,"Hail's optimizer should be smart enough to push `TableFilter` into a `TableExplode`. Consider these two equivalent pipelines on *tiny* data, a ten-by-ten matrix. ```; import hail as hl; mt = hl.balding_nichols_model(3, 10, 10); t = mt.entries(); t.filter(t.GT.is_hom_ref()).export('foo.tsv'); ```; ```; foo.tsv; merge time: 45.459ms; ```. ```; import hail as hl; mt = hl.balding_nichols_model(3,10,10); mt.filter_entries(mt.GT.is_hom_ref()).entries().export('foo2.tsv'); ```; ```; foo2.tsv; merge time: 23.856ms; ```. This will likely also require improving Hail's filter movement. I observed a `TableFilter` getting stuck behind a `TableMapGlobals`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6905:115,pipeline,pipelines,115,https://hail.is,https://github.com/hail-is/hail/issues/6905,1,['pipeline'],['pipelines']
Deployability,"HailContext initialization overrides any existing log4j configuration, which can lead to the logs ending up in an unexpected location. This PR adds an option to HailContext initialization to skip this configuration step. I also included two unrelated changes to this PR:; - Not bundling the transitive dependencies for `com.indeed:lsmtree-core:1.0.7`, which don't seem to be needed and can lead to classpath conflicts.; - Allowing the `quiet` option during initialization to silence the warning issued when initializing with pip-installed Hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8571:56,configurat,configuration,56,https://hail.is,https://github.com/hail-is/hail/pull/8571,3,"['configurat', 'install']","['configuration', 'installed']"
Deployability,"HailException and LowererUnsupportedOperation get returned as 400 with the error message,; other exceptions as 500 with stack trace. Also, some docker fixes. @jigold, I think this might explain why deploying from your computer is so slow. Docker includes the file permissions in the metadata when checking the image cache. The CI uses umask 022 (group not writable). I changed up my computer, and noticed everything was being rebuilt from scratch (requiring me to push massive images). I added some checks in docker/Makefile that the expectations for the image. I couldn't find a way to fix this globally. If the checks trigger, I think the solution is to set your umask to 022 going forward and chmod -R g-w your Hail source tree.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8636:198,deploy,deploying,198,https://hail.is,https://github.com/hail-is/hail/pull/8636,1,['deploy'],['deploying']
Deployability,"Half finished. SQL is probably wrong. Need to integrate the job filtering with the parameters into the api calls, write tests, integrate with batch_client. I'm sure there's more to do.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6607:46,integrat,integrate,46,https://hail.is,https://github.com/hail-is/hail/pull/6607,2,['integrat'],['integrate']
Deployability,Hand deploy succeeded. CI appears OK now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8402#issuecomment-606704529:5,deploy,deploy,5,https://hail.is,https://github.com/hail-is/hail/pull/8402#issuecomment-606704529,1,['deploy'],['deploy']
Deployability,Hand deploy successful. Monitoring logs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8424#issuecomment-607417090:5,deploy,deploy,5,https://hail.is,https://github.com/hail-is/hail/pull/8424#issuecomment-607417090,1,['deploy'],['deploy']
Deployability,"Hand deployed (currently running). Only visible change is to router, add proxy rules from ukbb-hail.is to the ukbb-rg servers. The web site has two parts: static HTML served by nginx and a interactive, data-driven Shiny site run by R/shiny/shiny server. Servers run as stateful sets. Static HTML and ; data for Shiny were hand-populated. Currently giving them each one core. Given how much state is involved here, it's not clear how to autoscale this like we do with the other services.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6573:5,deploy,deployed,5,https://hail.is,https://github.com/hail-is/hail/pull/6573,1,['deploy'],['deployed']
Deployability,"Happy to sit down and go through what this is about. This is current running on the cluster:. ```; $ kubectl get pods; NAME READY STATUS RESTARTS AGE; ...; spark-master-ffcfbf95c-gth5s 1/1 Running 0 4m; spark-worker-699db74c7-lsd9v 1/1 Running 0 11h; spark-worker-699db74c7-plgdd 1/1 Running 0 11h; ```. but I haven't automated deployment yet. I'm currently building the hail image from a distribution I hand built, but I'll switch over to the standard distribution once this goes in: https://github.com/hail-is/hail/pull/4554 (it fixed some bugs that showed up in this deployment). That's the `gs://hail-cseed/hail-test.zip` stuff. This was surprisingly difficult to get working. The main culprit, I think, is that Spark makes it impossible to bind and advertise different addresses for the Spark master. In the end I faked it out with:. > echo ""0.0.0.0 spark-master"" >> /etc/hosts. which works but seems a bit dubious.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4560:328,deploy,deployment,328,https://hail.is,https://github.com/hail-is/hail/pull/4560,2,['deploy'],['deployment']
Deployability,Has `_prev_nonnull` in python been updated to use the new `Aggregator2`?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5414#issuecomment-466549827:35,update,updated,35,https://hail.is,https://github.com/hail-is/hail/pull/5414#issuecomment-466549827,1,['update'],['updated']
Deployability,"Haven't been able to have pipeline benchmarks finish, but from the looks of things this change does not make things significantly slower or faster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7060#issuecomment-535609382:26,pipeline,pipeline,26,https://hail.is,https://github.com/hail-is/hail/pull/7060#issuecomment-535609382,1,['pipeline'],['pipeline']
Deployability,"Haven't figured it out yet, but reproduced the error with a simpler pipeline that just uses one annotate instead of `sample_qc`:. ```; P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = mt.annotate_cols(n_called = hl.agg.filter(hl.is_defined(mt.GT), hl.agg.count())); mt = mt.filter_cols(mt.n_called > 0); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception as e:; print(""\n[FAIL] with "", N, ""partitions""); raise e; break; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944#issuecomment-652065734:68,pipeline,pipeline,68,https://hail.is,https://github.com/hail-is/hail/issues/8944#issuecomment-652065734,2,['pipeline'],['pipeline']
Deployability,"Haven't updated python docs yet, but the Scala code should be fine to start reviewing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1884:8,update,updated,8,https://hail.is,https://github.com/hail-is/hail/pull/1884,1,['update'],['updated']
Deployability,"Heh, so turns out that `test_weird_urls` is missing the `@pytest.mark.asyncio` decorator, and so it was getting skipped with a warning this whole time. The pytest upgrade added auto-detection of async tests and so it ran this broken test for the first time. I'm PR'ing to treat most warnings as errors in #12322.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11974#issuecomment-1278201498:163,upgrade,upgrade,163,https://hail.is,https://github.com/hail-is/hail/pull/11974#issuecomment-1278201498,1,['upgrade'],['upgrade']
Deployability,"Hello developers,; Sorry for resurrecting the issue. I have the same problem with a different Error. ```; conda create -n hail2; conda activate hail2; conda install -c bioconda hail; pip install gnomad; ```. Installation has no issues. But below command throws error. This is the same on two different machines that I have tried so far. Removing conda env, or changing env location, has not helped me so far. ```; import hail; from gnomad.sample_qc.ancestry import assign_population_pcs. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/sample_qc/ancestry.py"", line 9, in <module>; from gnomad.utils.filtering import filter_to_autosomes; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/utils/filtering.py"", line 9, in <module>; from gnomad.resources.resource_utils import DataException; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/__init__.py"", line 3, in <module>; from .resource_utils import *; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 162, in <module>; class VariantDatasetResource(BaseResource):; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 173, in VariantDatasetResource; def vds(self, force_import: bool = False) -> hl.vds.VariantDataset:; AttributeError: module 'hail' has no attribute 'vds'; ```. Could anyone please recommend me how to circumvent this?; Thanks for the amazing package.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275:157,install,install,157,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275,3,"['Install', 'install']","['Installation', 'install']"
Deployability,"Hello, I am having similar problems. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements ; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; ` hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz')`; `py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1`; Any help? Best, Zillur",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374:39,install,installed,39,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374,2,['install'],['installed']
Deployability,"Hello, I have installed hail using pycharm not from github. Now getting the attached error. My command was:; ```; import hail as hl; hl.init(); import os; from hail.plot import show; [hail.err.txt](https://github.com/hail-is/hail/files/3570397/hail.err.txt). from pprint import pprint; hl.plot.output_notebook(). hl.utils.get_1kg('data/'); hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). mt = hl.read_matrix_table('data/1kg.mt'); mt.rows().select().show(5); ```; Any help? Best, Zillur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6982:14,install,installed,14,https://hail.is,https://github.com/hail-is/hail/issues/6982,1,['install'],['installed']
Deployability,"Hello,when I build Hail to run locally,I encounter this problem,how can I fix it ? . [root@**\* hail]# gradle installDist; Using a seed of [1] for testing.; Build file '/**_/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala; /**_/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:3: object time is not a member of package java; import java.time._; ^; /***/hail/src/main/scala/org/broadinstitute/hail/driver/ExportVCF.scala:76: not found: value LocalDate; sb.append(s""##fileDate=${LocalDate.now}\n""); ^; two errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':compileScala'.; ; > Compilation failed; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 45.869 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453:110,install,installDist,110,https://hail.is,https://github.com/hail-is/hail/issues/453,1,['install'],['installDist']
Deployability,"Hello? Anyone who can help for Hail 0.2 on Azure DataBrick?. After pip install lots of problems came out.... can't find Java Package , import hail.plot , hl.init(). According to document. https://docs.azuredatabricks.net/applications/genomics/tertiary/hail.html#create-a-hail-cluster. I've pip install hail. set ENABLE_HAIL=true in Cluster Environment Setting. However. import hail as hl; hl.init(sc, idempotent=True); ; AttributeError: module 'hail' has no attribute 'init'. Also another document. https://docs.azuredatabricks.net/applications/genomics/tertiary/hail.html. import hail as hl; import hail.expr.aggregators as agg; hl.init(sc, idempotent=True). ModuleNotFoundError: No module named 'hail.expr'. Anyone can give a solution?; Thanks a lot !!!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7298:71,install,install,71,https://hail.is,https://github.com/hail-is/hail/issues/7298,2,['install'],['install']
Deployability,"Here is a Hail log.... I will work on getting the YARN container logs next. . more /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:513,configurat,configuration,513,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['configurat'],['configuration']
Deployability,Here is a straight-line pipeline that replicates the high memory use. In my experience this can get up to 100GiB of RAM use. https://gist.github.com/danking/3432deabd997ce08515b2088e202a039. The VDS file is privileged. Next steps:. - [ ] replicate on a public VDS like the HGDP/1KG VDS.; - [ ] delete as much code as possible from this file to reduce the possible causes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13606#issuecomment-1717799683:24,pipeline,pipeline,24,https://hail.is,https://github.com/hail-is/hail/issues/13606#issuecomment-1717799683,1,['pipeline'],['pipeline']
Deployability,"Here's a link with an absolute time window: https://cloudlogging.app.goo.gl/gXAWZpZtUiV8jphXA. This is the assertion's stack trace:; ```; at scala.Predef$.assert(Predef.scala:208); at is.hail.QoBOutputStreamManager.createOutputStream(QoBAppender.scala:38); at org.apache.logging.log4j.core.appender.OutputStreamManager.getOutputStream(OutputStreamManager.java:165); at org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStreamManager.java:250); at org.apache.logging.log4j.core.appender.OutputStreamManager.flushBuffer(OutputStreamManager.java:283); at org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:294); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.directEncodeEvent(AbstractOutputStreamAppender.java:217); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.tryAppend(AbstractOutputStreamAppender.java:208); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:199); at org.apache.logging.log4j.core.config.AppenderControl.tryCallAppender(AppenderControl.java:161); ```. And the line of our code that triggers the logger appender:; ```; is.hail.JVMEntryway$2.run(JVMEntryway.java:139); ```. On that line, we should have already evaluated line 97:; ```; QoBOutputStreamManager.changeFileInAllAppenders(logFile);; ```; Which updates the filename for all `QoBOutputStreamManager`s. We should be the only ones allocating `QoBOutputStreamManager` (it has no magic annotations, we don't pass its constructor anywhere). We should only allocate `QoBOutputStreamManager` in its associated object. We always put it into the map in `getInstance`. We don't synchronize the other methods though, so that could be the issue? If we have a stale version of that map?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13242#issuecomment-1703383030:1406,update,updates,1406,https://hail.is,https://github.com/hail-is/hail/issues/13242#issuecomment-1703383030,1,['update'],['updates']
Deployability,"Here's a sketch of what I think the lowering pipeline for BlockMatrix would look like. The relevant bits:. - BlockMatrixType gets an additional `definedBlocks` field to track sparseness.; - LowerBlockMatrixIR.lower on BlockMatrixIRs defines the transformations from BlockMatrixIRs to BlockMatrixStage, and LowerBlockMatrixIR.lower on value IRs with BlockMatrixIR children define rules for transforming the lowered BlockMatrixStage children into IRs, similarly to lowering in TableIRs.; - BlockMatrixStage consists of basically 3 things:; - blockContext: a matrix of contexts necessary for each partition computation (e.g. filenames of blocks that each partition needs to read, literal NDArray values, etc.); - body: the transformation of blockContext that represents the actual NDArray in each partition of the BlockMatrix.; - broadcastVals (currently unused, perhaps unnecessary): values that we'd potentially want to broadcast to all nodes to use in computation. I could see this being useful in specific broadcast operations, but I'd also be happy to take it out until we have a use case.; - ctxName and ctxType are used to reference the block context within the body of computation. Lowering each node would consist of two parts:; - Defining the transformation in LowerBlockMatrixIR; - Propagating the sparsity transformation correctly through each BlockMatrixIR node, so that lowering functions can use it. This looks pretty different, depending on the node being lowered; Filter, for example, will need to lift the sparsity propagation logic from the FilterRDD where it's currently defined, while for BlockMatrixMap it's mostly a matter of making explicit the implicit densification that (sometimes) happens within the node, and then propagating sparsity accordingly. I've lifted the matrix multiply as an illustration of how this would work, although we can't test it until we have at least one entrypoint and one exit; if this looks reasonable I can clean it up and PR it when I get back on Mo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8051:45,pipeline,pipeline,45,https://hail.is,https://github.com/hail-is/hail/pull/8051,1,['pipeline'],['pipeline']
Deployability,"Here's an update to the action table. | Part | Description | Action |; | --- | --- | --- |; | GT | the hard call | _minning_ or _subsetting_ |; | AD | allele depth | the filtered allele's column is eliminated, e.g. filtering allele 1 transforms `[25,5,20]` to `[25,20]` |; | DP | number of informative reads | no change |; | PL | Phred-likelihoods for each allele pair | _minning_ or _subsetting_ |; | GQ | genotype quality | increasing-sort PL and take `PL[1] - PL[0]` |. We choose either _minning_ or _subsetting_ consistently for all parts. I now feel:; - when _minning_ (i.e. _believe real_) we should move AD value to reference.; - when _subsetting_ (i.e. _believe not-real_) we should subtract removed depth from DP",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/551#issuecomment-240825234:10,update,update,10,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240825234,1,['update'],['update']
Deployability,"Here's the deadlock that I'm observing now on this branch. I don't have a great understanding of what's happening. I believe Transaction 1 to be [here](https://github.com/hail-is/hail/blob/40d8882470af71f2d08dd1aa6b723357ca8a1245/batch/sql/estimated-current.sql#L1186-L1188) in MJC, and Transaction 2 to be [here](https://github.com/hail-is/hail/blob/40d8882470af71f2d08dd1aa6b723357ca8a1245/batch/sql/estimated-current.sql#L449-L453) in the jobs_after_update trigger. Looking at the second transaction in context now, it looks like that is probably another MJC transaction toward the end of its run after it updated the jobs table. I think it would make sense then that T2 would still hold the lock for `instances_free_cores_mcpu` but I'm not sure where the contention for `batch_inst_coll_cancellable_resources` is coming from, as I don't see how T1 could be holding any form of lock on it. Either way it seems like how we use these tables is similarly a mess. ```; *** (1) TRANSACTION:; TRANSACTION 644409381, ACTIVE 0 sec starting index read; mysql tables in use 1, locked 1; LOCK WAIT 39 lock struct(s), heap size 3520, 50 row lock(s), undo log entries 28; MySQL thread id 1941960, OS thread handle 140297909716736, query id 1869168359 10.32.3.8 dgoldste updating; UPDATE instances_free_cores_mcpu; SET free_cores_mcpu = free_cores_mcpu + cur_cores_mcpu; WHERE instances_free_cores_mcpu.name = in_instance_name; *** (1) WAITING FOR THIS LOCK TO BE GRANTED:; RECORD LOCKS space id 1263041 page no 3 n bits 264 index PRIMARY of table `dgoldste`.`instances_free_cores_mcpu` trx i; d 644409381 lock_mode X locks rec but not gap waiting; Record lock, heap no 192 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (tot; al 36 bytes);; 1: len 6; hex 00002668e81a; asc &h ;;; 2: len 7; hex 710000071136b3; asc q 6 ;;; 3: len 4; hex 800029fe; asc ) ;;. *** (2) TRANSACTION:; TRANSACTI",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116:609,update,updated,609,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116,2,['update'],['updated']
Deployability,"Here's the diff when I replace the current `CreateDatabaseStep` with the new `CreateDatabase2Step`. ```diff; diff --git a/ci/ci/build.py b/ci/ci/build.py; index cbc186ef2d..7ea399713b 100644; --- a/ci/ci/build.py; +++ b/ci/ci/build.py; @@ -240,8 +240,10 @@ class Step(abc.ABC):; return CreateNamespaceStep.from_json(params); if kind == 'deploy':; return DeployStep.from_json(params); - if kind in ('createDatabase', 'createDatabase2'):; + if kind == 'createDatabase':; return CreateDatabaseStep.from_json(params); + if kind == 'createDatabase2':; + return CreateDatabase2Step.from_json(params); raise BuildConfigurationError(f'unknown build step kind: {kind}'); ; def __eq__(self, other):; @@ -967,7 +969,7 @@ date; ); ; ; -class CreateDatabaseStep(Step):; +class CreateDatabase2Step(Step):; def __init__(self, params, database_name, namespace, migrations, shutdowns, inputs, image):; super().__init__(params); ; @@ -989,12 +991,7 @@ class CreateDatabaseStep(Step):; self.create_database_job = None; self.cleanup_job = None; ; - if params.scope == 'dev':; - self.database_server_config_namespace = params.code.namespace; - else:; - self.database_server_config_namespace = DEFAULT_NAMESPACE; -; - self.cant_create_database = is_test_deployment or params.scope == 'dev'; + self.cant_create_database = is_test_deployment; ; # MySQL user name can be up to 16 characters long before MySQL 5.7.8 (32 after); if self.cant_create_database:; @@ -1005,6 +1002,11 @@ class CreateDatabaseStep(Step):; self._name = database_name; self.admin_username = f'{database_name}-admin'; self.user_username = f'{database_name}-user'; + elif params.scope == 'dev':; + dev_username = params.code.config()['user']; + self._name = f'{dev_username}-{database_name}'; + self.admin_username = f'{dev_username}-{database_name}-admin'; + self.user_username = f'{dev_username}-{database_name}-user'; else:; assert params.scope == 'test'; self._name = f'{params.code.short_str()}-{database_name}-{self.token}'; @@ -1030,7 +1032,7 @@ cl",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600:337,deploy,deploy,337,https://hail.is,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600,2,"['Deploy', 'deploy']","['DeployStep', 'deploy']"
Deployability,"Here's the error:. ```; 2427:2016-12-07 16:34:33 ERROR TaskSetManager:75 - Task 257 in stage 3.0 failed 4 times; aborting job; 2435:2016-12-07 16:34:33 ERROR Hail:93 - hail: annotatesamples expr: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 257 in stage 3.0 failed 4 times, most recent failure: Lost task 257.3 in stage 3.0 (TID 590, nid00026.urika.com): scala.MatchError: ArrayBuffer(3.549E-4) (of class scala.collection.mutable.ArrayBuffer); ```. Log: /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log. Here's the full pipeline:. ```; /mnt/lustre/tpoterba/bin/hail -l /mnt/lustre/gtiao/hail_logs/PCAWG.iteration_test_compare_methods.log \; 	read -i file:///mnt/lustre/gtiao/PCAWG/data/PCAWG.full_callset.chr_ALL.GQ20_AB.split.updated.WGS_1KG_tissue_annot.promoters.QCed.vds \; 	annotatesamples table -i file:///mnt/lustre/gtiao/PCAWG/germline_callset/housekeeping/Broad_callset.115k_SNP.8PC.ethnicity_inference.txt \; 	-e Sample -c 'sa.annots.Ethnicity = table.Ethnicity' \; 	annotatesamples expr -c 'sa.AF_hist = gs.filter(g => g.isCalledNonRef).map(g => va.info.AF).hist(0, 1, 100)' \; 	annotateglobal expr -c 'global.AF_hist = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	exportsamples -c 'SAMPLE = s.id, AF_hist = sa.AF_hist, Ethnicity = sa.annots.Ethnicity, Tissue = sa.annots.tissue_type' \; 	-o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.txt \; 	filtersamples expr -c '(sa.annots.tissue_type != ""BRCA"") && (sa.annots.Ethnicity == ""EUR"")' --keep \; 	filtersamples expr --keep -c 'samples.collect().sortBy(x => runif(0.0, 1.0))[:250]' \; 	annotateglobal expr -c 'global.AF_hist.iter1 = samples.map(s => sa.AF_hist.binFrequencies).sum()' \; 	variantqc filtervariants expr -c 'va.qc.AC >= 1' --keep \; 	exportvariants -o file:///mnt/lustre/gtiao/PCAWG/hist_AFs_by_sample.iter1.promoter_variants.txt \; 	-c 'CHROM = v.contig, POS = v.start, REF = v.ref, ALT = v.alt, TARGET = va.promoter_target, AC = va.qc.AC, AC_To",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1151:585,pipeline,pipeline,585,https://hail.is,https://github.com/hail-is/hail/issues/1151,2,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,Here's the reference for the behavior of `gsutil cp` with directory naming. https://cloud.google.com/storage/docs/gsutil/commands/cp#how-names-are-constructed. Pipeline and CI will need to make sure the inputs to Batch actually produce what is intended.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5879:160,Pipeline,Pipeline,160,https://hail.is,https://github.com/hail-is/hail/pull/5879,1,['Pipeline'],['Pipeline']
Deployability,"Here's the terraform configurations in GCP and Azure:. - GCP: Batch has admin storage permissions, as granted here https://github.com/hail-is/hail/blob/1f5e1540c04abfde58ead1084841fec5aa6e0ed3/infra/gcp/main.tf#L415-L424. We also grant it a Viewer role on the query bucket after that which seems redundant. We should really not grant it global storage admin and instead give it admin for just the query bucket and other associated batch buckets. I checked in hail-vdc and batch does not have the global storage admin role, and it has the Viewer role on the query bucket. I've changed that role now to admin on the query bucket. - Azure: Story is simpler. The `query` storage container is part of the `batch` storage account. The batch SP has ownership over the `batch` storage account and by extension all of the containers inside it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011:21,configurat,configurations,21,https://hail.is,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011,2,['configurat'],['configurations']
Deployability,"Here's what _local_ looks like now. Note that I've already converted to a `vds` this time. ```; dking@wmb16-359 # rm -rf foo && time ../hail/build/install/hail/bin/hail read -i profile.vds ibd -o 'foo' ; hail: info: running: read -i profile.vds; [Stage 1:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:=====================================================> (210 + 4) / 214]hail: info: timing:; read: 3.047s; ibd: 4m35.1s; ../hail/build/install/hail/bin/hail read -i profile.vds ibd -o 'foo' 924.50s user 16.11s system 333% cpu 4:42.04 total; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/738#issuecomment-249995538:147,install,install,147,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-249995538,2,['install'],['install']
Deployability,"Hey @JKosmicki,. Your branch has diverged from master a fair bit at this point. I can get this PR moving again if you do two simple things for me:; - rebase your commit on hail-is's master; - apply a patch I created, which fixes some compile errors. If you don't already have a remote (you can list remotes with `git remote -v`) for `hail-is/hail`, let's create one:. ``` bash; git remote add hi https://github.com/hail-is/hail.git; ```. I'll refer to this remote as `hi` from now on. If you already had a remote for `hail-is/hail` then substitute its name below for `hi`. First, we rebase to get the latest code from `hail-is/hail`'s `master` branch. ``` bash; git fetch hi; git rebase hi/master tdt; ```. And now we download [this `.patch` file](https://github.com/danking/hail/commit/6ea3d77684596abf171920e014c2aedd2a209f9c.patch) and apply it to the `tdt` branch:. ``` bash; git am the/path/to/that/file/you/downloaded.patch; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/753#issuecomment-248645143:200,patch,patch,200,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-248645143,8,['patch'],['patch']
Deployability,"Hey @anh151 !. I'm sorry you're having trouble with Hail. The message ""Container killed on request. Exit code is 137"" comes from Apache Spark, our underlying distributed compute framework. It indicates that the worker machines have insufficient RAM. In general, using worker machines with higher RAM-to-core ratios will help. If you're on GCP, try the n1-highmem family. Some other suggestions:. 1. Hail is a ""lazy"" system. Your entire pipeline is executed, from the beginning, when you run ""export"" or ""write"". That means that Hail has to do all of that work at once. You can ease the memory pressure by performing less operations at once, by writing an intermediate file (and reading back in and proceeding with it).; 2. We recommend against directly exporting from a complex operation (like group-by-aggregate). Instead, grab the cols table and write it to Hail's fast, binary, parallel format: `.cols().select('field_of_interest').write('my-cols.ht')`. Then read that table and export that: `hl.read_table('my-cols.ht').field_of_interest.export(...)`. Exporting to a text file requires more memory because we have to construct ASCII strings.; 3. Always use a compressed export: `.export('foo.tsv.bgz')` or `hl.export_vcf(..., 'foo.vcf.bgz')`. This won't help your memory problem, but you can avoid parsing strings to create loci by constructing an `hl.Locus` which is the Python-side representation of loci (`hl.locus` is the inside-Hail representation):; ```python3; def create_intervals(data):; return [; hl.Locus(chromosome, start, reference_genome=""GRCh38""); for i, (chromosome, start) in data[[""CHROM"", ""POS""]].iterrows(); ]; ```. Please reply here if you're still having problems after incorporating the above suggestions as it may indicate a more fundamental issue with Hail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1674035485:436,pipeline,pipeline,436,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1674035485,1,['pipeline'],['pipeline']
Deployability,"Hey @daniel-goldstein, just checking what caused this issue. Was it the open-batches PR, or prior to the 100 release?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12235#issuecomment-1261571243:109,release,release,109,https://hail.is,https://github.com/hail-is/hail/pull/12235#issuecomment-1261571243,1,['release'],['release']
Deployability,"Hey @daniel-goldstein, super keen for this fix! Anything we can do on our side to test this, or ease this PR getting merged and triggering a release?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14576#issuecomment-2177245584:141,release,release,141,https://hail.is,https://github.com/hail-is/hail/pull/14576#issuecomment-2177245584,1,['release'],['release']
Deployability,"Hey @danking. I've also got a PR open for this issue (#9250) which includes a check for the minimum required version of `gcloud`. Also, including the `--secondary-worker-type` flag increases the minimum `gcloud` version to [291.0.0](https://cloud.google.com/sdk/docs/release-notes#28500_2020-03-17) (released in May).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9262#issuecomment-673163784:267,release,release-notes,267,https://hail.is,https://github.com/hail-is/hail/pull/9262#issuecomment-673163784,2,['release'],"['release-notes', 'released']"
Deployability,"Hey @dlcotter ! Thanks for the report. I anticipate a fix in the next version of Hail. For now, I think you can fix with `pip3 install 'parsimonious>=0.9'` or downgrading to Python 3.10",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12759#issuecomment-1458660443:127,install,install,127,https://hail.is,https://github.com/hail-is/hail/issues/12759#issuecomment-1458660443,1,['install'],['install']
Deployability,Hey @mhebrard !. I'm really sorry Hail has been such a pain to install. This looks to me like a Scala version incompatibility. In your Makefile you specified this:; ```; ... SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.0; ```; [EMR's docs](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-691-release.html) confirms that Scala 2.12.15 should be installed. I think my next questions are:; 1. Which `spark-shell` is that?; 2. What latent JVMs are around?; 3. What's the class path and what's on it?; 4. What scala executables are around?. ```; which spark-shell; spark-shell --version; which java; java -version; which scala; scala -version; echo $CLASSPATH; ```. That `SettingsOps` is an implicit nested class of the `MutableSettings` object. It is definitely present in [2.13](https://github.com/scala/scala/blob/2.13.x/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L70-L88) and [2.12](https://github.com/scala/scala/blob/2.12.x/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L83-L94). It appears to be missing in [2.11](https://github.com/scala/scala/blob/2.11.x/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L64-L68). It appears to have arrived in [2.12.14](https://github.com/scala/scala/commit/3bd24299fc34e5c3a480206c9798c055ca3a3439).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1766477525:63,install,install,63,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1766477525,4,"['Release', 'install', 'release']","['ReleaseGuide', 'install', 'installed', 'release']"
Deployability,"Hey @tomwhite, sorry for the massive delay. There was some concern about not having instructions generic to any cluster in the docs, so I've restructured your PR a bit more to capture the generic Spark cluster instructions and then have a separate section on getting started with a Cloudera cluster. I also opted for ""Cloudera"" instead of ""CDH"" because I don't think our users will recognize the acronym. Does that seem OK to you?. I made my changes as [a PR into your branch](https://github.com/tomwhite/hail/pull/1/files). Also, don't worry about the failing integration test, that's a CI issue on our end. It should resolve it self after the next new commit to your branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1452#issuecomment-290546429:561,integrat,integration,561,https://hail.is,https://github.com/hail-is/hail/pull/1452#issuecomment-290546429,1,['integrat'],['integration']
Deployability,"Hey @williambrandler ! Thanks for your contribution to Hail. We endeavor to keep our docs always accurate and up-to-date. Our continuous deployment system verifies the correctness of our Google Dataproc and Azure HDInsight instructions before releasing a new version of Hail to PyPI. Does Databricks have an open source program that would provide us with free credits to incorporate the Databricks platform into our continuous deployment process? Alternatively, I'm comfortable accepting these new instructions with a disclaimer that clearly identifies these instructions as contributed by Databricks and not maintained by the Hail team. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11800#issuecomment-1112572464:126,continuous,continuous,126,https://hail.is,https://github.com/hail-is/hail/pull/11800#issuecomment-1112572464,8,"['continuous', 'deploy']","['continuous', 'deployment']"
Deployability,"Hey Hail,; I've been trying to get Hail working in a HPC environment. I was hoping to get multiple users to work on hail at the same time using the same shared filesystem. My design was to use a central code and library repository where there is a $CODE_HOME/hail/ and a $CODE_HOME/miniconda/ python installation, which all users PATHs are pointing to. This worked fine for both interactive and spark-submit uses with a single user, but today when I was testing with multiple users the HailContext would fail to form intermittently on a call to hc = HailContext() with either one of two errors. Note, each user today was ssh'ed into a different node and we were all using different jupyter notebooks simultaneously. There were five of us, and everytime we would all try to start HailContext at least one of us would fail out with these errors. Most of the time all five of us would fail out. Also note that concurrent calls to python only would be fine, with from hail import * working fine. Any help at all would be wonderful, as we would really like to work collaboratively on the cluster at the same time and all be referencing the same hail and python installations so we can keep our code synchronized. The first error that we would get would be. ---------; OSError Traceback (most recent call last); <ipython-input-11-2841f1963bb0> in <module>(); ----> 1 hc_rav = HailContext(). /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir); 45; 46 from pyspark import SparkContext; ---> 47 SparkContext._ensure_initialized(); 48; 49 self._gateway = SparkContext._gateway. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 254 with SparkContext._lock:; 255 if not SparkContext._gateway:; --> 256 SparkContext._gateway = gateway or launch_gateway(conf); 257 SparkContext._jvm =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:300,install,installation,300,https://hail.is,https://github.com/hail-is/hail/issues/1525,1,['install'],['installation']
Deployability,"Hey Nick, wanted to loop you in on an offline discussion I had with Cotton about this. First, thank you for picking up review responsibilities! I'll just do a brief review focusing on interaction of this change with intended directions for hailctl. Here are the conclusions from our discussion:. 1. This is a breaking change to the hailctl interface. We're OK with that.; 2. Although we are OK making breaking changes, we should get Grace's team on board and update their scripts/repos before merging/releasing. For that reason this will sit for a few weeks until their current urgent analysis push is done.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767171298:459,update,update,459,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767171298,1,['update'],['update']
Deployability,Hey Tim! Just curious what the status on this bug is. I also just got this error (I did update to the newest version incase it was fixed).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7824#issuecomment-597216721:88,update,update,88,https://hail.is,https://github.com/hail-is/hail/issues/7824#issuecomment-597216721,1,['update'],['update']
Deployability,"Hi - I receive the following error when running the `variantqc` example from the [Getting Started documentation](https://hail.is/getting_started.html):. `$ ./build/install/hail/bin/hail read ~/sample.vds splitmulti variantqc -o ~/variantqc.tsv sampleqc -o ~/sampleqc.tsv; `. `hail: fatal: variantqc: parse error: ""-o"" is not a valid option`. Leaving `variantqc` out runs without error and generates the sampleqc output.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1017:164,install,install,164,https://hail.is,https://github.com/hail-is/hail/issues/1017,1,['install'],['install']
Deployability,"Hi @Sun-shan,. First, I should note that we do not currently test hail against Spark version 2.2.0, I recommend using Spark 2.1.1 or 2.0.2. Spark versions aside, the error you encountered is unrelated to Spark, as far as I know. What version of the `decorator` package is installed on your machine? `decorator` version 4.0.10 should work correctly. Unfortunately, we are still looking for a python dependency management solution. My apologies that you've run into this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-336903534:272,install,installed,272,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-336903534,1,['install'],['installed']
Deployability,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-240446097:249,install,installed,249,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097,7,"['install', 'release']","['installDist', 'installed', 'installed-packages', 'release', 'release-']"
Deployability,"Hi @alanmejiamaza ,. Just to be clear, you did `pip install hail` and then you opened a notebook and ran something like:; ```; import hail as hl; hl.init(); from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(); ht = hl.utils.range_table(1000); ht = mt.annotate(DP = hl.rand_unif(0, 100)); p = hl.plot.histogram(ht.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); show(p); ```; And the plot didn't appear? Did you get a message saying ""BokehJS 1.4.0 successfully loaded.""? What version of Jupyter are you using? What web browser are you using?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12717#issuecomment-1452599951:52,install,install,52,https://hail.is,https://github.com/hail-is/hail/issues/12717#issuecomment-1452599951,2,['install'],['install']
Deployability,Hi @daniel-goldstein! I'm actually running this through Amazon CodeBuild so these are logs from an actual Amazon Linux 2 Image running on an EC2 instance build... So I don't know if that makes a difference here. I see what you're saying about the `xargs -0` however wouldn't this still be a change to the installation files for Hail or is that something that's likely happening in one of the files that I'm using and just haven't found it yet?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255333669:305,install,installation,305,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255333669,1,['install'],['installation']
Deployability,"Hi @danking, sorry this took me a little to test. I think there's a problem with the latest changes, in my dev-deploy, it failed on the '`create_certs` and `create_accounts`, with the error:. ```; FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.7/dist-packages/hailtop/hail_version'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-791067814:111,deploy,deploy,111,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-791067814,1,['deploy'],['deploy']
Deployability,"Hi @danking, this should be working now! I realises that hailtop isn't installed through `setup.py`, but `setup-hailtop.py`. The setup also requires `include_package_data=True`, so I've added that into both. I've added one more line into the `Dockerfile.service-base` to check whether the version was actually installed correctly, as other services use this, and better to fail early. This is working on my local dev-deploy (that uses CPG infrastructure though). Later edit: this file is re-installing hailtop and breaking this PR: https://github.com/hail-is/hail/blob/main/docker/Dockerfile.service-java-run-base",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-797251759:71,install,installed,71,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-797251759,4,"['deploy', 'install']","['deploy', 'installed', 'installing']"
Deployability,"Hi @nawatts, your comments are very welcome, and I appreciate your perspective as a hailctl user. OK, mulling over your comments, I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. I think this also addresses the issue `hailctl dataproc submit` not supporting `--`, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: `hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters`. What do you think?. > it would be nice if the --configuration/--gcloud-configuration argument was consistent across hailctl dataproc commands; > It would also be nice to standardize on kebab case for all arguments. Agree on both accounts, will fix. Thanks again!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-758080935:703,configurat,configuration,703,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758080935,2,['configurat'],['configuration']
Deployability,"Hi @pettyalex, thank you for the detailed and thoughtful issue. Hopefully I can shed some light and address all your concerns. I think the assertion on Java 8 and 11 was an overly defensive precaution put in place some time ago, as hail uses some unsafe JVM APIs that have been deprecated for a while. But as you noted, the world goes on in Java 17 and I don't see a reason Hail shouldn't be compatible. Since most of our closest users use Hail on GCP Dataproc, we generally keep in lock-step with their platform which is unfortunately still on Java 11 so that is what we test against and officially support. Nevertheless, we should remove the restriction and add some light validation in CI against Java 17 and advertise it as unofficially supported until such a time that Dataproc moves to Java 17. Hopefully Spark 3.6 will force their hand. The release process for 0.2.129 is already underway but expect this to be resolved in 0.2.130. Thanks for your suggestions regarding bundling the JRE and the GC options, we'll definitely consider them. Regarding the `module-info.class` nonsense, my apologies. That just seems like a bug we should fix. I will create a separate tracking issue for that but I'm not yet sure where that will get prioritized. If it is more than an annoyance for you, please let us know. Regarding conda-forge, I don't think we currently have the bandwidth or demand (that we know of) to add more distribution systems. Again, this is something where hearing from the community is the best way to figure out how to direct our efforts. Hopefully this addresses your concerns. Please do follow up if I've missed anything or open more issues if you encounter new problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14433#issuecomment-2030358704:848,release,release,848,https://hail.is,https://github.com/hail-is/hail/issues/14433#issuecomment-2030358704,1,['release'],['release']
Deployability,"Hi @vladsaveliev, sorry for the delay on this. I'm going to leave this for when @danking comes back next week because I think there's some nuance around our dev cert expiration that I'm not 100% on. In the meantime, I looked and saw you try to delete then try to create repeatedly. Could you instead use a dry-run then apply trick like [this](https://stackoverflow.com/questions/45879498/how-can-i-update-a-secret-on-kubernetes-when-it-is-generated-from-a-file)? We do this for some other keys in `build.yaml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10188#issuecomment-802861885:398,update,update-a-secret-on-kubernetes-when-it-is-generated-from-a-file,398,https://hail.is,https://github.com/hail-is/hail/pull/10188#issuecomment-802861885,1,['update'],['update-a-secret-on-kubernetes-when-it-is-generated-from-a-file']
Deployability,"Hi @williambrandler, it does seem like everyone is getting hit with this issue. We pinned Jinja2 to 3.0.3 once this broke our tests and it should be fixed now. I'm going to close this issue but if you still experience these problems on the latest release please re-open and we'll address it promptly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705#issuecomment-1099734097:247,release,release,247,https://hail.is,https://github.com/hail-is/hail/issues/11705#issuecomment-1099734097,1,['release'],['release']
Deployability,"Hi @yc000000, unfortunately, I don't know how Amazon Glue works. I don't know the purpose of copying the Hail jar into S3. You might try uploading the wheel file instead of the zip file. . You might also try instead using the HMS DBMI scripts for starting a [Hail Cluster on AWS Spot instances](https://github.com/hms-dbmi/hail-on-AWS-spot-instances). ---. Installing Hail on a cluster should only require a couple steps. Once you have a Spark cluster, you just need to:. 1. ssh to the master node of the Spark Cluster.; 2. clone the hail repository.; 3. run `make -C hail/hail install HAIL_COMPILE_NATIVES=1`. After that you should be able to submit Python files using Hail to the cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10844#issuecomment-914493872:357,Install,Installing,357,https://hail.is,https://github.com/hail-is/hail/issues/10844#issuecomment-914493872,2,"['Install', 'install']","['Installing', 'install']"
Deployability,"Hi Cotton,. Interesting that this is during tablet creation not while inserting data.; Looks like this is a known issue, but with no fix or workaround yet that I; can see:. https://issues.cloudera.org/plugins/servlet/mobile#issue/KUDU-383. Does it work if you retry, or delete the table and retry? I successfully; imported chr1 from 1k genomes on a 6 node cluster. This would create fewer; tablets though as it only covers one chromosome, so I should try with the; full dataset - I'll do that in the next few days when I'm back from; travelling. Thanks for trying it out. Do you have any more review comments for the PR?. Cheers,; Tom; On 11 Apr 2016 21:29, ""cseed"" notifications@github.com wrote:. Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to; 40m to fix a The requested number of tablets is over the permitted maximum; (100) error. I was able to write a small table. When I tried to write a; larger file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208722298:719,install,installed,719,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298,1,['install'],['installed']
Deployability,"Hi Jerome, yup, the first three require plink 1.9 and the fourth requires qctool. I'm surprised FisherExactSuite didn't fail as well, perhaps you have R installed or pulled Hail before that went into master. Thanks for the feedback, super helpful!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240399174:153,install,installed,153,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240399174,2,['install'],['installed']
Deployability,"Hi Tim,. What's the problem with this implementation? I've tested it and it works... On Wed, Sep 21, 2016 at 11:07 AM, Tim Poterba notifications@github.com; wrote:. > Laurent, I was totally wrong about being able to do this per-command --; > I'm really sorry. I thought that it would be possible to create a new; > configuration just for this command and use that, but this is only possible; > for HadoopConfigurations and not SparkContexts. Can you reopen the old; > PR? That model is our only option.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/826#issuecomment-248641543, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgcPW4xK16W3DlZfdE5U6RTcVmJthks5qsUhMgaJpZM4KC1O-; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/826#issuecomment-248643185:315,configurat,configuration,315,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248643185,1,['configurat'],['configuration']
Deployability,"Hi Tim,; Sorry for the bother again. I am following this short tutorial as described [here](https://gnomad.broadinstitute.org/news/2021-09-using-the-gnomad-ancestry-principal-components-analysis-loadings-and-random-forest-classifier-on-your-dataset/). The code snippet was working properly with earlier. Now that I have installed hail from pip, I have this error while running RF model. ```; ht, rf_model = assign_population_pcs(; ... ht,; ... pc_cols=ht.scores,; ... fit=fit,; ... ). 2022-09-29 14:55:46 Hail: INFO: Coerced sorted dataset (0 + 1) / 1]; INFO (gnomad.sample_qc.ancestry 224): Found the following sample count after population assignment: sas: 1; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/bioinfoRD/ARCdata/Projects_AMT/conda_envs/hail/lib/python3.10/site-packages/gnomad/sample_qc/ancestry.py"", line 235, in assign_population_pcs; min_assignment_prob=min_prob, error_rate=error_rate; UnboundLocalError: local variable 'error_rate' referenced before assignment; ```. Could you please suggest what might be happening here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759:320,install,installed,320,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759,1,['install'],['installed']
Deployability,"Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to 40m to fix a `The requested number of tablets is over the permitted maximum (100)` error. I was able to write a small table. When I tried to write a larger file (~900 exomes) and I got:. ```; hail: writekudu: caught exception: org.kududb.client.NonRecoverableException: Too many attempts: KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6, DeadlineTracker(timeout=10000, elapsed=7721), Deferred@1490962783(state=PENDING, result=null, callback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505), errback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505))); ```. In the Kudu logs, I'm seeing tons of:. ```; W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable: CreateTablet request on kudu.tserver.TabletServerAdminService from 69.173.65.227:42904 dropped due to backpressure. The service queue is full; it has 50 items.; ```. Suggestions on how to proceed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208516279:20,install,installed,20,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208516279,1,['install'],['installed']
Deployability,"Hi Tpoterba,. That would be a big help. I've attached the patch file I used. [HadoopFS.scala.patch.zip](https://github.com/hail-is/hail/files/6023321/HadoopFS.scala.patch.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10087#issuecomment-783486496:58,patch,patch,58,https://hail.is,https://github.com/hail-is/hail/issues/10087#issuecomment-783486496,3,['patch'],['patch']
Deployability,"Hi Vlad, thanks for the PR! I'm afraid there are some internal migrations we're making that are probably not clear from just looking at the codebase. Are you up to date on our `main`? We've found working with `config.mk` cumbersome because it can be stale if you switch between different instances of Batch (e.g. one deployed in azure and the other in GCP). > DOCKER_ROOT_IMAGE used to build batch workers and benchmark. I've recently updated the scripts for building the batch worker VM image to query kubernetes directly and we should probably do the same for benchmark. > HAIL_TEST_GCS_BUCKET used to build query; KUBERNETES_SERVER_URL used to build amundsen. These services are both currently deleted in our `main`. > PROJECT, ZONE, REGION are probably not need, but might make sense to add for consistency. These will fail in an Azure deployment, and while we want to move away from `config.mk` entirely, we would at least want it to contain configurations that are valid across clouds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11371#issuecomment-1041941055:317,deploy,deployed,317,https://hail.is,https://github.com/hail-is/hail/pull/11371#issuecomment-1041941055,8,"['configurat', 'deploy', 'update']","['configurations', 'deployed', 'deployment', 'updated']"
Deployability,"Hi all,. Here's the error message that I get when I go to install all of my python packages (scipy/uvloop/etc). ```; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; --; 872 | amazon-ebs: rm -rf build/deploy; 873 | amazon-ebs: mkdir -p build/deploy; 874 | amazon-ebs: mkdir -p build/deploy/src; 875 | amazon-ebs: cp ../README.md build/deploy/; 876 | amazon-ebs: rsync -r \; 877 | amazon-ebs: --exclude '.eggs/' \; 878 | amazon-ebs: --exclude '.pytest_cache/' \; 879 | amazon-ebs: --exclude '__pycache__/' \; 880 | amazon-ebs: --exclude 'benchmark_hail/' \; 881 | amazon-ebs: --exclude '.mypy_cache/' \; 882 | amazon-ebs: --exclude 'docs/' \; 883 | amazon-ebs: --exclude 'dist/' \; 884 | amazon-ebs: --exclude 'test/' \; 885 | amazon-ebs: --exclude '*.log' \; 886 | amazon-ebs: python/ build/deploy/; 887 | amazon-ebs: # Clear the bdist build cache before building the wheel; 888 | amazon-ebs: cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; 889 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-eb",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:58,install,install,58,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,7,"['deploy', 'install']","['deploy', 'install']"
Deployability,"Hi everyone, ; I've been trying to get Hail up and running on my laptop and our HPC cluster and I keep running into the same problem. The install goes fine, but when I run the tests it fails out on both my laptop and our cluster at the same point, here : . > 14:17:27.809; [ERROR] [system.err] hail: info: while writing:; 14:17:27.809 [ERROR] [system.err] /tmp/testExportKT.tsv; 14:17:27.810 [ERROR] [system.err] merge time: 7.677ms; 14:17:28.591 [ERROR] [system.err] hail: info: Coerced sorted dataset; 14:17:30.368 [ERROR] [system.err] .hail: info: Coerced sorted dataset; 14:17:31.306 [ERROR] [system.err] ...; 14:17:31.904 [ERROR] [system.err] ==================================================================; 14:17:31.905 [ERROR] [system.err] ERROR: test_dataset (hail.tests.ContextTests); 14:17:31.905 [ERROR] [system.err] ----------------------------------------------------------------------; 14:17:31.905 [ERROR] [system.err] Traceback (most recent call last):; 14:17:31.905 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/tests.py"", line 181, in test_dataset; 14:17:31.906 [ERROR] [system.err] sample2.grm('gcta-grm-bin', '/tmp/sample2.grm'); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/dataset.py"", line 1988, in grm; 14:17:31.906 [ERROR] [system.err] self.hc._run_command(self, pargs); 14:17:31.906 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 90, in _run_command; 14:17:31.907 [ERROR] [system.err] raise_py4j_exception(e); 14:17:31.907 [ERROR] [system.err] File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/java.py"", line 87, in raise_py4j_exception; 14:17:31.907 [ERROR] [system.err] raise FatalError(msg, e.java_exception); 14:17:31.908 [ERROR] [system.err] FatalError: NoSuchMethodError: breeze.linalg.DenseVector$.canSetD()Lbreeze/generic/UFunc$InPlaceImpl2;; 14:17:31.908 [ERROR] [system.err]; 14:17:31.908 [ERROR] [system.err",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419:138,install,install,138,https://hail.is,https://github.com/hail-is/hail/issues/1419,1,['install'],['install']
Deployability,"Hi folks,. In evaluating Hail to see whether it fits my use case (a variant frequency database) I ran into an issue with importing VCF files from GIAB. It turns out that these use type `String` for the `PS` `##FORMAT` entry. Subsequently, Hail fails to import these with the error:; ```; is.hail.utils.HailException: HG001.vcf.gz:column 492: invalid character 'P' in integer literal; ```; This is because of the default behaviour of `htsjdk` to ""repair"" these according to the VCF ""standard"". `htsjdk` exposes `codec.disableOnTheFlyModifications` to toggle this behaviour which can be called from somewhere around https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1143. Ideally I would like to expose this toggle also at the `import_vcf` method of Hail.; I'll create a PR to do so accordingly ASAP. Comments/questions?. Thanks!. Regards,. Mark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6012:550,toggle,toggle,550,https://hail.is,https://github.com/hail-is/hail/issues/6012,2,['toggle'],['toggle']
Deployability,"Hi! . I know this is out of the blue, but we would like the ability to fetch resource_usage data from an endpoint programmatically to evaluate our job performance. I thought it might be worth suggesting this upstream to see if it's something you'd like too :). This PR:; 1. Use the internal method to fetch the dataframes for a job; 2. Transform the data frame to dictionary with `orient='split'`. And FWIW, here's how to convert it back into a dataframe:. ```python; import pandas as pd. response = {} # response from Hail Batch; dataframes = {; key: pd.DataFrame(data=values['data'], columns=values['columns']); for key, values in response.items(); }; ```. I tested this in on a dev deploy and it worked pretty well, but happy to add testing if you can direct me to a place to add it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14328:685,deploy,deploy,685,https://hail.is,https://github.com/hail-is/hail/pull/14328,1,['deploy'],['deploy']
Deployability,"Hi! Are you trying to compile hail from source? You can get `lz4` on OS X with Homebrew by doing something like `brew install lz4`, or `apt-get install liblz4-dev` on a Debian-flavored Linux, but we don't (currently) ship with that dependency because the C++ code isn't enabled yet. If you don't need to build from source, but just want a local version to play around with, you can either use `pip` or download the prebuilt distribution following the instructions here: https://hail.is/docs/stable/getting_started.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4651#issuecomment-433220724:118,install,install,118,https://hail.is,https://github.com/hail-is/hail/issues/4651#issuecomment-433220724,2,['install'],['install']
Deployability,Hi! Found this long closed thread but had a related question - any updates on implementing PC-Air in Hail in the intervening years by any chance?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3490#issuecomment-1182425769:67,update,updates,67,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-1182425769,1,['update'],['updates']
Deployability,"Hi!. Trying to calculate polygenic risk score with code from the [Polygenic Score Calculation](https://hail.is/docs/0.2/guides/genetics.html#polygenic-score-calculation), getting error with stacktrace:. `2022-05-14 12:09:07 Hail: INFO: Running Hail version 0.2.94-f0b38d6c436f; 2022-05-14 12:09:08 SparkContext: WARN: Using an existing SparkContext; some configuration may not take effect.; 2022-05-14 12:09:08 root: INFO: RegionPool: initialized for thread 30: Thread-4; 2022-05-14 12:09:09 MemoryStore: INFO: Block broadcast_0 stored as values in memory (estimated size 34.3 KiB, free 434.4 MiB); 2022-05-14 12:09:09 MemoryStore: INFO: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 434.4 MiB); 2022-05-14 12:09:09 BlockManagerInfo: INFO: Added broadcast_0_piece0 in memory on 10.40.3.21:33951 (size: 3.2 KiB, free: 434.4 MiB); 2022-05-14 12:09:09 SparkContext: INFO: Created broadcast 0 from broadcast at SparkBackend.scala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:355,configurat,configuration,355,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['configurat'],['configuration']
Deployability,"Hi!; This is an odd error message to get -- is your repository updated to the current master? There was an update to the `importannotations table` module a few weeks ago, before which the `-e` option didn't exist. . We are in the midst of a documentation reorganization, so I apologize if it's difficult to find things at the moment. From the cloned repository, all test files are at `src/test/resources/*`. . This command worked for me just now:. ```; hail importannotations table src/test/resources/variantAnnotations.alternateformat.tsv --impute -e '`Chromosome:Position:Ref:Alt`' write -o tmp.vds; ```. The `-e` argument uses an expression to specify how to construct a `Variant`, which in this case is just the column name since the type of that column is `Variant`. If we don't use the `--impute` argument, we can construct it with . ```; -e 'Variant(`Chromosome:Position:Ref:Alt`)'; ```. More info on that [here](https://github.com/broadinstitute/hail/blob/master/docs/commands/ImportAnnotations.md)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/561#issuecomment-238502640:63,update,updated,63,https://hail.is,https://github.com/hail-is/hail/issues/561#issuecomment-238502640,2,['update'],"['update', 'updated']"
Deployability,"Hi, I am using hail in spark, but encounter some problem.; I followed the ""Getting Started"" to deploy hail , and build Hail from source; (https://hail.is/docs/stable/getting_started.html). I set the environmental variables as follows:; ```; export SPARK_HOME=/opt/Software/spark/spark-2.0.2-bin-hadoop2.6; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```; I put the vcf file in hadoop， as follows:; ```; [hdfs@tele-1 root]$ hdfs dfs -ls /hail/test; Found 1 items; -rw-r--r-- 3 hdfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076:95,deploy,deploy,95,https://hail.is,https://github.com/hail-is/hail/issues/2076,1,['deploy'],['deploy']
Deployability,"Hi, I just tried installing hail with pip but it has very stringent version dependencies.; For example, it requires [pandas>0.22,<0.24](https://github.com/hail-is/hail/blob/04344d214361daede1417e74b206b739eff9ae87/hail/python/requirements.txt#L10) which causes a `pip install hail` to downgrade my pandas version. Would it be feasible to remove all those `<0.x` in the dependencies?; Usually, new versions of e.g. pandas introduce less problems to the user than downgrades caused by pip packages (`pip` does not resolve dependencies in contrast to e.g. `conda`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299:17,install,installing,17,https://hail.is,https://github.com/hail-is/hail/issues/7299,2,['install'],"['install', 'installing']"
Deployability,"Hi, I think this is outdated given the new annotation database.; cheers,. > On Feb 13, 2017, at 3:47 PM, jbloom22 <notifications@github.com> wrote:; > ; > @andgan <https://github.com/andgan> any update on this issue? should it remain open?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub <https://github.com/hail-is/hail/issues/174#issuecomment-279517149>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ADIkAnlW6KE-f6entdPPA6wzrnTBTrz6ks5rcMF1gaJpZM4HLmN9>.; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/174#issuecomment-279574525:195,update,update,195,https://hail.is,https://github.com/hail-is/hail/issues/174#issuecomment-279574525,1,['update'],['update']
Deployability,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1003:781,deploy,deploy,781,https://hail.is,https://github.com/hail-is/hail/issues/1003,2,['deploy'],['deploy']
Deployability,"Hi, I'm getting the same error trying to build Hail on Amazon Linux on an EMR cluster.; The suggested fix from issue #454 did not work. To reproduce:; - Create EMR cluster (using default Amazon Linux AMI ami-044cb769); - Install git (`sudo yum install git`); - Install gradle . > #!/bin/bash; > cd /root; > gradle_package=`curl -s http://services.gradle.org/distributions --list-only | sed -n 's/.*\(gradle-.*.all.zip\).*/\1/p' | egrep -v ""milestone|rc"" | head -1`; > gradle_version=`ls ${gradle_package} | cut -d ""-"" -f 1,2`; > mkdir /opt/gradle; > wget -N http://services.gradle.org/distributions/${gradle_package}; > unzip -oq ./${gradle_package} -d /opt/gradle; > ln -sfnv ${gradle_version} /opt/gradle/latest; > printf ""export GRADLE_HOME=/opt/gradle/latest\nexport PATH=\$PATH:\$GRADLE_HOME/bin"" > /etc/profile.d/gradle.sh; > . /etc/profile.d/gradle.sh; > hash -r ; sync; > gradle -v; - gradle -v. > [...]; > Gradle 2.6; > [...]; > Build time: 2015-08-10 13:15:06 UTC; > Build number: none; > Revision: 233bbf8e47c82f72cb898b3e0a96b85d0aad166e; > Groovy: 2.3.10; > Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013; > JVM: 1.7.0_101 (Oracle Corporation 24.95-b01); > OS: Linux 4.4.11-23.53.amzn1.x86_64 amd64; - Clone hail from commit 6382678846a9c187d448713f26a2c38f21a683db; - `$ gradle installDist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453#issuecomment-229750270:221,Install,Install,221,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229750270,4,"['Install', 'install']","['Install', 'install', 'installDist']"
Deployability,"Hi, I'm studying Hail and installing Hail on spark. I have plan that run GWAS about 1000 genomes. So, I install and set up hail on spark. **<my environment>**; Linux: Centos 7.8; Python: 3.7.3 (anaconda); Apache spark: spark-2.2.0-bin-hadoop2.6; Hadoop: hadoop-2.6.0; Java -version (info. I'm using linux server by korea Institution, So i can't use root permission); openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); Hail version: 0.2.68. **<My workflow>**; 1. Run start-master.sh and start-slaves.sh in spark sbin directory.; 2. (bash) pyspark. I got message below. ![image](https://user-images.githubusercontent.com/78582088/121848873-9f001980-cd25-11eb-9e9d-8854f204c8c3.png); ![image](https://user-images.githubusercontent.com/78582088/121848900-a58e9100-cd25-11eb-9655-58100401e0d3.png); ![image](https://user-images.githubusercontent.com/78582088/121848915-a9baae80-cd25-11eb-90ff-a4f12df5b322.png). How can i set up hail on spark?; Do i need to change java version?. Thank you for your services. My <bashrc>, <conf/spark-defaults.conf> and <./spark-env.sh> are below. **<.bashrc>**; ```; #SPARK; export SPARK_HOME=/home/edu1/tools/spark-2.2.0-bin-hadoop2.6; export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/python:$PATH; export PYTHONPATH=$HAIL_HOME/python:$SPARK_HOME/python:$(echo ${SPARK_HOME}/python/lib/py4j-*-src.zip):$PYTHONPATH. # Hail; export HAIL_HOME=/home/edu1/miniconda2/envs/Hail-on-spark/lib/python3.7/site-packages/hail; export PATH=$PATH:$HAIL_HOME/bin; export PYTHONPATH=$PYTHONPATH:$HAIL_HOME/python; export SPARK_CLASSPATH=$HAIL_HOME/backend/hail-all-spark.jar. # JAVA (I just can modify .bashrc, so This would not apply to java path.); export JAVA_HOME=/home/edu1/tools/jdk-1.8.0_231; export PATH=$PATH:$JAVA_HOME/bin; export CLASSPATH=$JAVA_HOME/lib/tools.jar. # Hadoop; export HADOOP_INSTALL=/home/edu1/tools/hadoop-2.6.0; export PAHT=$PATH:$HADOOP_INSTALL/bin; export LD_LIBRARY_PATH=$H",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590:26,install,installing,26,https://hail.is,https://github.com/hail-is/hail/issues/10590,2,['install'],"['install', 'installing']"
Deployability,"Hi, Is there a plan to merge this branch in soon? We are testing out HAIL for some of our in-house pipelines and an ability to import bgens would be really handy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/243#issuecomment-233953902:99,pipeline,pipelines,99,https://hail.is,https://github.com/hail-is/hail/pull/243#issuecomment-233953902,1,['pipeline'],['pipelines']
Deployability,"Hi, just proposing a possible fix (at least it fixes things for me). After `raise_for_status=True` [was set](https://github.com/hail-is/hail/pull/9864/files) to default to True for `ClientSession` objects, [these lines](https://github.com/hail-is/hail/blob/9303a3aeccc3c374130b845bdaa5b22a84a64ea5/hail/python/hailtop/hailctl/dev/deploy/cli.py#L48-L53) that handle the response and redirect the remote traceback locally, are no longer reachable. This PR sets `raise_for_status=False` for dev deploy, similar to how it [was handled for dev query](https://github.com/hail-is/hail/pull/9864/files#diff-3d7267e81281c8027cba60607e398a7d5fbfcc16481cddf45017948889b15715). Thanks @lgruen for helping to locate the issue!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10058:330,deploy,deploy,330,https://hail.is,https://github.com/hail-is/hail/pull/10058,2,['deploy'],['deploy']
Deployability,"Hi, not sure if this is the right avenue, but I'd also like to report a similar `orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0)` bug first reported by https://discuss.hail.is/t/hail-fails-after-installing-it-on-a-single-computer/3653. Hail installed from https://anaconda.org/sfe1ed40/hail; EDIT: the same error occurs after `pip install hail` into a fresh conda env, which produced hail `version 0.2.130-bea04d9c79b5`. Terminal output: ; ```; Python 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail as hl; hl.init(); >>> hl.init(); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.4.1; SparkUI available at http://xxxx:xxxx; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.127-d18228b9bc5b; LOGGING: writing to xxxx.log; >>> hl.utils.range_table(10).collect(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1234>"", line 2, in collect; File ""/xxxx/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/xxxx/lib/python3.10/site-packages/hail/table.py"", line 2213, in collect; return Env.backend().execute(e._ir, timed=_timed); File ""/xxxx/lib/python3.10/site-packages/hail/backend/backend.py"", line 188, in execute; result, timings = self._rpc(ActionTag.EXECUTE, payload); File ""/xxxx/lib/python3.10/site-packages/hail/backend/py4j_backend.py"", line 219, in _rpc; error_json = orjson.loads(resp.content); orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0); ```. Log file:; ```; 2024-04-25 16:07:16.773 Hail: INFO: SparkUI: http://xxxx:xxxx; 2024-04-25 16:07:21.589 Hail: I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076:218,install,installing-it-on-a-single-computer,218,https://hail.is,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076,3,['install'],"['install', 'installed', 'installing-it-on-a-single-computer']"
Deployability,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584:512,configurat,configuration,512,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584,2,['configurat'],['configuration']
Deployability,"Hi, thanks for the quick response!. I updated the original post with more info to reproduce.; Please let me know if I can get you more information about the Scala environment!. All the best,; Michael",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453#issuecomment-229753452:38,update,updated,38,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229753452,1,['update'],['updated']
Deployability,"Hi,. Here are our latest updates to the johnc branch originally created by John Compitello. We have improved the overall annotation performance by increasing the default block_size to 500K. Please let us know if you have any questions. Best,; Shuli & the Nirvana team",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2300:25,update,updates,25,https://hail.is,https://github.com/hail-is/hail/pull/2300,1,['update'],['updates']
Deployability,"Hi,. I tried to compile Hail version 852f92aac4532abc2fc743e0629840a7f6d86496; but it failed with:. >$ ./gradlew -Dspark.version=2.2.0 shadowJar archiveZip; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.1.tar.gz; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++11 -Ilibsimdpp-2.1 -Wall -Werror -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -c NativeBoot.cpp; NativeBoot.cpp:1:0: error: bad value (sandybridge) for -march= switch; #include <jni.h>; ^; make: *** [build/NativeBoot.o] Error 1; :nativeLib FAILED. I'm compiling on Amazon's EMR, emr-5.10.0, where I install miniconda to get python 3.6 installed (as a bootstrap action), and run manually:. > sudo yum update -y; sudo yum install g++ cmake git -y; sudo mkdir -p /etc/alternatives/jre; sudo ln -s /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.171-7.b10.37.amzn1.x86_64/include /etc/alternatives/jre/include; git clone https://github.com/broadinstitute/hail.git; cd hail/; git checkout 852f92aac4532abc2fc743e0629840a7f6d86496; ./gradlew -Dspark.version=2.2.0 shadowJar archiveZip",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4305:682,install,install,682,https://hail.is,https://github.com/hail-is/hail/issues/4305,4,"['install', 'update']","['install', 'installed', 'update']"
Deployability,"High level design:; * `ApplySeeded` takes a child of type `trngstate`. It is always constructed as a free reference `__rng_state`, which must be defined by a parent, who has the responsibility to ensure the seeded function is given a distinct rng state on each invocation.; * Any node with a child which may be executed more than once (e.g. `TailLoop`, stream nodes, table nodes) must use `Let('__rng_state', RNGSplit(Ref('__rng_state'), uid), child)`, where `uid` is a long or tuple of longs which is guaranteed to be distinct on every execution of `child`.; * Uids are typically created at the leaves of pipelines (`TableRead`, `StreamRange`, etc.), and propagated upwards. There was a phase-ordering conflict that had to be worked around:; * IRs must be given explict rng state and uid semantics as early as possible, to ensure determinism.; * The transformation to explicitly pass rng states and uids must happen during IR construction. If it happened later, it would create new IR objects, which would defeat the python CSE pass (which only recognizes equivalent subexpressions when they are represented by the same python object).; * The rng explication requires some type information.; * Types on the python IR are assigned after the IR is fully constructed. To fix this:; * `Ref`'s must be given a type at construction; * `TopLevelReference`s are the only case that needs to be constructed before a type is known. But they are always constructed wrapped in a `SelectFields` or `GetField`, whose type is known at construction. I added new IR classes `SelectedTopLevelReference` and `ProjectedTopLevelReference` for these two cases, which are thin wrappers which don't appear in the rendered IR.; * `construct_expr` always assigns a type to the ir. Bottom-up type construction will later assert equality with the assigned type. This caught some existing bugs, where expression type and ir type didn't agree.; * At construction of the root node of a stream/table/matrixtable pipeline (i.e. a non-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11847:606,pipeline,pipelines,606,https://hail.is,https://github.com/hail-is/hail/pull/11847,1,['pipeline'],['pipelines']
Deployability,"Hmm, actually GKE seems to vary in how quickly it gets new versions out. It [supported k8s 1.11](https://cloud.google.com/kubernetes-engine/release-notes) for Early Access Partners (EAPs) within about a month. 1.12 was released a little less than a month ago. The first generally available GKE k8s 1.10 release was May 15, and k8s 1.10 was released on March 26th, so that's less than two months from k8s to GKE.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145:140,release,release-notes,140,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145,4,['release'],"['release', 'release-notes', 'released']"
Deployability,"Hmm, looks like it's something like `spark.executorEnv.FOO=...` based on this: https://spark.apache.org/docs/latest/configuration.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583434753:116,configurat,configuration,116,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583434753,1,['configurat'],['configuration']
Deployability,Hmm. I see that `Annotation.copy` has been updated to not rely on `Region`s. I'll reimplement these methods in terms of `Annotation.copy`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3353#issuecomment-380450020:43,update,updated,43,https://hail.is,https://github.com/hail-is/hail/pull/3353#issuecomment-380450020,1,['update'],['updated']
Deployability,Hmm. It sounds like you’re saying that you’re using some kind of prepackaged version of Hail on AWS. The Hail team only maintains tooling for Microsoft and Google. Can you direct me at whatever Amazon tools you’re using? It sounds like someone at Amazon needs to fix their installation script.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255339494:273,install,installation,273,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255339494,1,['install'],['installation']
Deployability,Hmm. I’ll have to sort this out tomorrow. Not sure what’s going on with that. It seems like the shadowTestJar target is probably not correctly pulling in the testImolemebtation configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1709453126:177,configurat,configuration,177,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1709453126,1,['configurat'],['configuration']
Deployability,Hmm. OrderedRDD and OrderedPartitioner are being phased out in master. OrderedRDD2 and OrderedPartitioner2 are in. We should probably have an offline discussion about how the linear algebra routines are going to interact with the new RegionValue-based stuff. There seem to be two competing goals here: getting something working for UKB and building something that will integrate with the new 0.2 stuff. We should probably have a chat about how to navigate this.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2171#issuecomment-326647606:369,integrat,integrate,369,https://hail.is,https://github.com/hail-is/hail/pull/2171#issuecomment-326647606,1,['integrat'],['integrate']
Deployability,"Hmm. This means every dev deploy will generate a new root key. I'm worried about the derived keys and trust lists. After this runs, any service which was not dev deployed needs to know to reload the trust list and start using the new key. For example, if you dev deploy batch, then separately dev deploy query, the new query will get cert errors when talking to batch, I think. I will give some thought this week to the right long-term certificate strategy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10188#issuecomment-804338199:26,deploy,deploy,26,https://hail.is,https://github.com/hail-is/hail/pull/10188#issuecomment-804338199,4,['deploy'],"['deploy', 'deployed']"
Deployability,"Hmm. this works: https://internal.hail.is/dking/site/vendors/vanta/viz.min.js, where is it not getting loaded correctly? I probably need another rule in dev deploy's site's nginx to fix the path.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8923#issuecomment-639035804:157,deploy,deploy,157,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639035804,1,['deploy'],['deploy']
Deployability,"Hmmmm, I still don't totally understand why we're hitting this specific import error. The system pip should still be able to install and run hail, I think -- I'd expect either an import error saying that `hail` cannot be found (if it's installed somewhere not on the Python path), or success. I still want to replicate in a docker, will report back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-533279733:125,install,install,125,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533279733,2,['install'],"['install', 'installed']"
Deployability,"Hmmmm. OK, I think we're getting tripped up by MANIFEST.in and/or setup.py. This bit in setup.py:. ```; package_data={; 'hail': ['hail_pip_version',; 'hail_version',; 'experimental/datasets.json'],; 'hail.backend': ['hail-all-spark.jar'],; 'hailtop.hailctl': ['hail_version', 'deploy.yaml']},; ```. Should be; ```; package_data={; 'hail': ['hail_pip_version',; 'hail_version',; 'experimental/datasets.json'],; 'hail.backend': ['hail-all-spark.jar'],; 'hailtop': ['hail_version'],; 'hailtop.hailctl': ['deploy.yaml']},; ```. A similar change needs to be made to `setup-hailtop.py`. And let's add a MANIFEST.in file in the same directory with this contents:; ```; include hail/hail_pip_version; include hail/hail_version; include hail/experimental/datasets.json; include hail/backend/hail-all-spark.jar; include hailtop/hail_version; include hailtop/hailctl/deploy.yaml; ```. And you'll need to copy that into Dockerfile.service-base:; ```; COPY hail/python/MANIFEST.in /hailtop/MANIFEST.in; ```. My bad on all this, I didn't fully check the Dockerfiles!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-793064791:277,deploy,deploy,277,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-793064791,3,['deploy'],['deploy']
Deployability,Holding on this until #10056 goes in because it removes the need for pypi credentials in deploying site.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10066#issuecomment-783517593:89,deploy,deploying,89,https://hail.is,https://github.com/hail-is/hail/pull/10066#issuecomment-783517593,1,['deploy'],['deploying']
Deployability,Hopefully the dev doc can do most of the explaining here. Not sure exactly how often we want to rotate. Putting this up here now to get feedback and will apply the key updates it to `hail-vdc` later this week after a trial on my own cluster.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11015:168,update,updates,168,https://hail.is,https://github.com/hail-is/hail/pull/11015,1,['update'],['updates']
Deployability,How do we build the OSX objects for continuous deployment?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1245:36,continuous,continuous,36,https://hail.is,https://github.com/hail-is/hail/issues/1245,2,"['continuous', 'deploy']","['continuous', 'deployment']"
Deployability,"However, a `NameError` is surprising here: I would've thought that this would be an attribute error instead. Let us know any updates!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1818#issuecomment-301725642:125,update,updates,125,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-301725642,1,['update'],['updates']
Deployability,"Huh, you can't request changes on your own PR. So, right now the hail/apiserver dependency is cyclic. I'll need to fix that to get testing and deploying working right.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5624#issuecomment-473961308:143,deploy,deploying,143,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-473961308,1,['deploy'],['deploying']
Deployability,"Huh; somehow you received a development version of Spark after installing Hail?. ```; Using Scala version 2.12.13, OpenJDK 64-Bit Server VM, 11.0.21; Branch HEAD; Compiled by user liangchi on 2023-02-11T02:24:04Z; Revision 5103e00c4ce5fcc4264ca9c4df12295d42557af6; Url https://github.com/apache/spark; Type --help for more information.; ```. https://github.com/apache/spark/commit/5103e00c4ce5fcc4264ca9c4df12295d42557af6",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1772997892:63,install,installing,63,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1772997892,1,['install'],['installing']
Deployability,I added a makePyHailDocs gradle task that uses spark.home System property to set up the PYTHONPATH and copyPyHailDocs that moves them to build/www/pyhail. You should be able to build the docs with:. ```; hail $ gradle -Dspark.home=/path/to/spark-1.6.2-bin-hadoop2.6 createDocs; ```. I also installed Spark 1.6.2 on ci.hail.is and added spark.home to the relevant build steps.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1061#issuecomment-258921388:290,install,installed,290,https://hail.is,https://github.com/hail-is/hail/pull/1061#issuecomment-258921388,1,['install'],['installed']
Deployability,I added a new `trait BroadcastSerializable` that tries to verify classes implementing this trait are only serialized when broadcasting. It works by getting the current stack trace and verifying that serialization only happens within a call to a `broadcast` method on the class. `ReferenceGenome` and `RVDPartitioner` implement `BroadcastSerializable`. @chrisvittal This also reduces the size of the RDD broadcast in the VCF combiner pipeline.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5512#issuecomment-469069004:433,pipeline,pipeline,433,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-469069004,1,['pipeline'],['pipeline']
Deployability,"I added the capability for the deploy config to find the domain from setting it in the config.ini file. This way users only use `hailctl config set domain` rather than `hailctl dev config set domain`. In addition, we use this new capability to make a test in Batch work on Azure. CC: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11113:31,deploy,deploy,31,https://hail.is,https://github.com/hail-is/hail/pull/11113,1,['deploy'],['deploy']
Deployability,"I added the cli. I kind of winged it looking at how `hailctl dev deploy` was done. It seems to work though:. ```; (base) wmecc-475:hail jigold$ hailctl batch billing; usage: hailctl batch billing [-h] {list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession o",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:65,deploy,deploy,65,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006,1,['deploy'],['deploy']
Deployability,"I added the configuration option for the minimum number of workers that should be present at any time. I tested this in my namespace. I'd like you to double check the logic is correct for the number of workers needed as I derived it by working through examples:. ```python3; n_live_instances = self.n_instances_by_state['pending'] + self.n_instances_by_state['active']; n_standing_instances_needed = max(0, self.min_instances - self.n_instances); n_standing_instances_needed = min(; n_standing_instances_needed,; self.max_live_instances - n_live_instances,; self.max_instances - self.n_instances,; remaining_instances_per_autoscaler_loop,; # 20 queries/s; our GCE long-run quota; 300,; ); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12742:12,configurat,configuration,12,https://hail.is,https://github.com/hail-is/hail/pull/12742,1,['configurat'],['configuration']
Deployability,"I added this feature because I am tired of every time I want to dev deploy and try out new changes, it triggers a new build in CI. I'd prefer to put a new label on the PR rather than close it each time or make a copy of the branch and test the copy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13540:68,deploy,deploy,68,https://hail.is,https://github.com/hail-is/hail/pull/13540,1,['deploy'],['deploy']
Deployability,I added to ci2 but not pipeline. https://github.com/hail-is/hail/pull/6150,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6149#issuecomment-494557340:23,pipeline,pipeline,23,https://hail.is,https://github.com/hail-is/hail/pull/6149#issuecomment-494557340,1,['pipeline'],['pipeline']
Deployability,"I addressed all your comments except making the decision how this should be structured between MySQL and Python. Once we make that decision, then I'll make the changes and double check with dev deploy that the costs are still the same and the resource aggregation is correct. Sound good?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8759#issuecomment-628002023:194,deploy,deploy,194,https://hail.is,https://github.com/hail-is/hail/pull/8759#issuecomment-628002023,1,['deploy'],['deploy']
Deployability,"I addressed the comments, except for the following, which I plan to do as separate PRs:; - ~~put list of domains in a file~~,; - least privileged: gateway-service-account (with certs read) and letsencrypt-sa (with certs update),; - second process in gateway pod to bump nginx on cert change",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4624#issuecomment-433179293:220,update,update,220,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-433179293,1,['update'],['update']
Deployability,I addressed the grafana situation. I dev deployed into my namespace and I'm able to log in. I cannot test that I've addressed the issue because the default namespace is still using the old version and thus I hit 401s there before even reaching the dev namespace.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12380#issuecomment-1297584226:41,deploy,deployed,41,https://hail.is,https://github.com/hail-is/hail/pull/12380#issuecomment-1297584226,1,['deploy'],['deployed']
Deployability,"I agree cancel_after_n_failures should be on the group. That lets us match Spark semantics for QoB. 1. I agree, callback per group seems valuable.; 2. I agree attributes seem useful on groups.; 3. I agree, not much value in updates being at the job-group level. . Depends what you mean by prefix search, if that means `LIKE ""X%""`, I think that'll be quite fast on a normal index because you can jump directly to the first record whose prefix is X. I don't see how a fulltext index could do any better in that case. On the other hand, if you mean `LIKE ""%X""` then I agree, a normal index is useless and MySQL will do a table scan. In that case, I expect a fulltext index to be a substantial improvement. > I believe my plan is basically already doing this. It might not be clear because I didn't put the migrations in. But basically all of the current batches tables are now indexed by batch_id, job_group_id where the current ""batch"" has job_group_id = 1. Ah, that sounds good. So the plan would be to drop, for example, `aggregated_batch_resources_v2` and the other tables which are now replaced with the job group ones? That's exactly what I had in mind.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12697#issuecomment-1450945048:224,update,updates,224,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1450945048,2,['update'],['updates']
Deployability,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:396,deploy,deploy,396,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595,1,['deploy'],['deploy']
Deployability,I already deployed this because CI is can't deploy anything until this lands. We're currently running rather old CI code (because we neglected to rebuild & deploy CI image when we changed it),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4441:10,deploy,deployed,10,https://hail.is,https://github.com/hail-is/hail/pull/4441,3,['deploy'],"['deploy', 'deployed']"
Deployability,"I already have these packages installed, and there was no `netcdf` issue with my version of R. @maccum is going to install the latest version of R fresh and try to add all the packages and see if tests pass. Thanks Meredith!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3281#issuecomment-379056688:30,install,installed,30,https://hail.is,https://github.com/hail-is/hail/pull/3281#issuecomment-379056688,2,['install'],"['install', 'installed']"
Deployability,I already updated the test namespace secret.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5972:10,update,updated,10,https://hail.is,https://github.com/hail-is/hail/pull/5972,1,['update'],['updated']
Deployability,I also added some debugging logs to try and figure out why you were getting a batch with no jobs when you dev deployed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6670#issuecomment-512577477:110,deploy,deployed,110,https://hail.is,https://github.com/hail-is/hail/pull/6670#issuecomment-512577477,1,['deploy'],['deployed']
Deployability,"I also added some flair to our ""integration tests"" (test.sh).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9821:32,integrat,integration,32,https://hail.is,https://github.com/hail-is/hail/pull/9821,1,['integrat'],['integration']
Deployability,I also fixed local/dev docs building. You need Hail installed for the docs build to work because it tries to `import` the classes for which you're building docs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13855:52,install,installed,52,https://hail.is,https://github.com/hail-is/hail/pull/13855,1,['install'],['installed']
Deployability,"I also prefer the second option in https://github.com/hail-is/hail/pull/9842#issuecomment-758128554. The more I think about this, the more problematic the notion of having an opaque list of ""arguments to pass through to gcloud"" seems. For example, `hailctl dataproc start` may run multiple gcloud commands: one to start the cluster and another to apply tags to the master node. In that case, we'd want to pass through extra args to the cluster start command, but not the apply tags command. Or `hailctl dataproc modify`, where we might want to accept extra args for `gcloud dataproc clusters update`. Those args shouldn't be passed through to the `gcloud compute ssh` commands, but options like `--project` or `--configuration` should. It seems like pass through arguments would be best handled on a case by case basis for each hailctl command. That would probably make any approach that required parsing those pass through options more cumbersome to use.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-758143032:592,update,update,592,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758143032,2,"['configurat', 'update']","['configuration', 'update']"
Deployability,I also pushed a commit to make it harder to accidentally `make deploy` into the main namespace. `site`'s `make deploy` now requires a `NAMESPACE` argument to be set.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8923#issuecomment-639136314:63,deploy,deploy,63,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639136314,2,['deploy'],['deploy']
Deployability,"I am (temporarily) defeated. Getting a working python 3.7 with pip on a standard distribution turns out to be non-trivial. Since ci doesn't specifically rely on 3.7 yet, I say we start with 3.6 and upgrade when it isn't too painful. It seems everyone is onboard with this. I am happy to move forward if you are, @jigold.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5623#issuecomment-474131577:198,upgrade,upgrade,198,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474131577,1,['upgrade'],['upgrade']
Deployability,"I am getting following error while using spark submit with --class ""is.hail.driver.Main"" /test/spark/hail15may.jar. java.lang.ClassNotFoundException: is.hail.driver.Main; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.spark.util.Utils$.classForName(Utils.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:693); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1807:528,deploy,deploy,528,https://hail.is,https://github.com/hail-is/hail/issues/1807,6,['deploy'],['deploy']
Deployability,I am going to close this pull request and open a new one with more docstring updates.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14241#issuecomment-1922467179:77,update,updates,77,https://hail.is,https://github.com/hail-is/hail/pull/14241#issuecomment-1922467179,1,['update'],['updates']
Deployability,"I am running AWS EMR 6.2.0, including; * Hadoop 3.2.1; * Python 3.7.10; * Java 1.8.0; * Spark 3.0.1; * Scala 2.12.10. On that cluster, I am building HAIL version 0.2.74-0c3a74d12093 using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.0.1; ```. Compilation succeeds. I am able to load Hail into my Zeppelin notebook pyspark kernel; I am able to load and describe hail table stored in AWS S3. ```; %pyspark; # Import and launch Hail; import hail as hl; hl.init(sc); # Load files; ht_s = hl.read_table('s3://npm-hail/SG10K_Health_r5.3.1/n9770/SG10K_Health_r5.3.1.n9770.samples.ht'); # Desctibe; ht_s.describe(); ```. I am not able to show data. ```; %pyspark; ht_s.show(); ```. the `show()` call run infinitely into zeppelin and SparkUI note a started job, but no task launched. FYI hail version 0.2.60 on same EMR stack works without issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10832:216,install,install-on-cluster,216,https://hail.is,https://github.com/hail-is/hail/issues/10832,1,['install'],['install-on-cluster']
Deployability,"I am running AWS EMR 6.3.0, including; * Hadoop 3.2.1; * Python 3.7.10; * Java 1.8.0; * Spark 3.1.1; * Scala 2.12.10. On that cluster, I try to build HAIL version 0.2.60-de1845e1c2f6 using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.1.1; ```. I got the error: . ```; Task :compileScala; Pruning sources from previous analysis, due to incompatible CompileSetup. /opt/broad-hail/hail/src/main/scala/is/hail/backend/service/ServiceBackend.scala:37: method toString in class IOUtils is deprecated: see corresponding Javadoc for more information.; new GoogleStorageFS(IOUtils.toString(is)); one error found; ```. FYI the same version of hail (0.2.60) on EMR 6.2.0 (same stack but with Spark 3.0.1) works without issue using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.0.1; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10831:217,install,install-on-cluster,217,https://hail.is,https://github.com/hail-is/hail/issues/10831,2,['install'],['install-on-cluster']
Deployability,I am still getting the same error when I take the type explicitly from the table I am trying to transform. Updated code is here:; https://github.com/chrisvittal/hail/blob/404cbd2b3255fc58656801febccce6ed98e594b9/hail/python/hail/experimental/vcf_combiner.py#L13-L59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5435#issuecomment-467992354:107,Update,Updated,107,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467992354,1,['Update'],['Updated']
Deployability,"I am testing hail build on spark3 (v0.2.89, spark 3.1.2) and getting the following error with jinja2 (see below).; From the error it seems like this is due to Hail's dependency of bokeh using the latest version of jinja2. Downgrading jinja2 to 3.0.0 solves the problem, and it seems like other people have seen this too with the latest release of jinja2:. https://github.com/holoviz/panel/issues/3260. This may be transient and may be solved by bokeh / jinja2 folks but thought I'd let you know in case you hit this issue. ```; ../conda/envs/glow/lib/python3.7/site-packages/bokeh/core/templates.py:43: in <module>; from jinja2 import Environment, Markup, FileSystemLoader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.ja",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:336,release,release,336,https://hail.is,https://github.com/hail-is/hail/issues/11705,1,['release'],['release']
Deployability,"I am using Hail 0.2.54. However, I also tested with the latest build.gradle file. I run the following make install command:; `make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.2`. However, I got this error message which did not appear before. ` > Could not resolve org.scalanlp:breeze-natives_2.11:+.; Required by:; project :; > Failed to list versions for org.scalanlp:breeze-natives_2.11.; > Unable to load Maven meta-data from https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml.; > Could not get resource 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'.; > Could not GET 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'. Received status code 500 from server: Server Error; * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.; * Get more help at https://help.gradle.org. BUILD FAILED in 29s; make: *** [build/libs/hail-all-spark.jar] Error 1`. It seems that is caused by https://repo.hortonworks.com/content/repositories/releases/ server is done.; I am wondering whether there is any maven substitute can be used temporarily to compile hail.jar?. Thanks in advance for your help.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9419:107,install,install,107,https://hail.is,https://github.com/hail-is/hail/issues/9419,6,"['install', 'release']","['install', 'releases']"
Deployability,"I attempted to use the TLS stuff and I've decided against it for this PR. It appears that the batch tests do not work locally at all. The whole essence of this PR is getting the shuffler IR tested and into the mainline. I started making the fixes necessary to support local testing of a local server and when that proved complicated investigated how BatchClientSuite works locally. It seems that using the existing TLS stuff would require fixing all the TLS stuff to allow for at least local->remote testing, if not local->local testing. This PR is already very complex, I'd like to get it merged so we can move forward separately with deploying and eventually harmonizing with the existing TLS infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8726#issuecomment-650380852:636,deploy,deploying,636,https://hail.is,https://github.com/hail-is/hail/pull/8726#issuecomment-650380852,1,['deploy'],['deploying']
Deployability,I avoid printing the full exception into the body in most cases. Seems prudent to not expose too much about our internals. CI already uses a broad except and prints the full message when building PRs so I adopted that for building the branch (`unwatched_branch.deploy`) in dev deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8828:261,deploy,deploy,261,https://hail.is,https://github.com/hail-is/hail/pull/8828,2,['deploy'],['deploy']
Deployability,"I believe that this patch and the original version both prevent this. They would both lock the parent directory of `path` and thus would prevent concurrent copying. Also, it appears that as long as we don't use the `-d` option, and the filenames are unique, `gsutil rsync` kinda already does what we want. I also feel like the orignal approach was way too aggressive, it seems like it was serializing _all_ locked filesystem operations in the entire filesystem subtree since it would wait for the lock on every parent other than `/` of the requested file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9523#issuecomment-701598366:20,patch,patch,20,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701598366,1,['patch'],['patch']
Deployability,I believe the fix here is to update the 'entry points' to the Spark and Local backends as we did similarly to the Service backend in #14567.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14650#issuecomment-2266139361:29,update,update,29,https://hail.is,https://github.com/hail-is/hail/issues/14650#issuecomment-2266139361,1,['update'],['update']
Deployability,"I believe the next step should be the minimal changes to move the global variables in `server/globals.py` to a database table(s). We also need to think about sql configuration. For now, I suppose we can just create a table manually, but we need a longer term strategy for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-456848381:162,configurat,configuration,162,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456848381,1,['configurat'],['configuration']
Deployability,I believe this is the only thread that needs to be updated: https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/Duplicate.20Sharded.20VCFs.20when.20exporting.20v3.20Hail.20Table/near/348554844,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12969#issuecomment-1532226084:51,update,updated,51,https://hail.is,https://github.com/hail-is/hail/pull/12969#issuecomment-1532226084,1,['update'],['updated']
Deployability,I believe this missing line basically erased all of the above `build_wheel_for_azure` step and the cloud optional changes I put our build system think a missing step is ok. If you look at the most recent deploy it doesn't include `build_wheel_for_azure` even though the `deploy` step depends on it. I'll separately make a fix for that,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11356:204,deploy,deploy,204,https://hail.is,https://github.com/hail-is/hail/pull/11356,2,['deploy'],['deploy']
Deployability,"I believe this will solve a bug Alicia is hitting, but I am having; trouble replicating it without the ability to run her pipeline.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7455:122,pipeline,pipeline,122,https://hail.is,https://github.com/hail-is/hail/pull/7455,1,['pipeline'],['pipeline']
Deployability,"I believe you're referencing [Chrome Bug 675308](https://bugs.chromium.org/p/chromium/issues/detail?id=675308) which suggests that lines thicker than 1 are broken in some versions of Chrome. My change uses line width 1. The screenshots you shared seem to suggest that Brave is making the lines somewhat fainter than Safari. I can't explain that. The visuals look the same to me across Chrome and Safari. I don't have Firefox installed to check a non-WebKit renderer. At this pointI feel that I have said my piece. It feels brittle to fiddle with device pixel ratio and line width to try and simulate line widths smaller than a pixel. If you think the only acceptable solution is the proposed changes in this PR, then let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8964#issuecomment-646218723:425,install,installed,425,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-646218723,1,['install'],['installed']
Deployability,"I broke this when I separated out the dependencies for hail into two layers:. 1. hailtop dependencies; 2. hail dependencies, which builds on top of the hailtop dependencies. This fix does two things:; - Use the full dependencies in 1 & 2; - Use fully pinned dependencies when installing on clusters which seems better than using our wide-range dependencies. I left the `install-deps` and `install-dev-deps` as the normal requirements files as those are meant for development (I think?) but am happy to take opinions on whether we should use fully pinned deps there as well. I have so far been going by the rule of thumb of fully-pinned for CI and production environments, more lax rules for dev environments. See [here](https://github.com/hail-is/hail/pull/12446#discussion_r1030986069) for additional context. cc: @tpoterba, any idea why the test dataproc test succeeded?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12510:276,install,installing,276,https://hail.is,https://github.com/hail-is/hail/pull/12510,3,['install'],"['install-deps', 'install-dev-deps', 'installing']"
Deployability,I can dev deploy it as well,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8920#issuecomment-638971214:10,deploy,deploy,10,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-638971214,1,['deploy'],['deploy']
Deployability,"I can move some of it over to a python diagnose, but I'm also targeting the ""I installed hail, but import hail fails"".",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5910#issuecomment-484684741:79,install,installed,79,https://hail.is,https://github.com/hail-is/hail/pull/5910#issuecomment-484684741,1,['install'],['installed']
Deployability,I can reproduce the error in the current master with:. ```; ./build/install/hail/bin/hail importvcf ~/sample2.vcf splitmulti annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' count; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-241806708:68,install,install,68,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241806708,1,['install'],['install']
Deployability,"I can stack this change with the change that defines the function, I do test that the `vcf_combiner` pipeline runs in `test_impex.py::VCFTests::test_combiner_works`. That may be sufficient, since it would fail without this change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5471#issuecomment-469340908:101,pipeline,pipeline,101,https://hail.is,https://github.com/hail-is/hail/pull/5471#issuecomment-469340908,1,['pipeline'],['pipeline']
Deployability,"I can write an RFC for how to do this with regards to billing updates and the database. I don't think it's too difficult, but it will take a bit of work to add some new metadata that says whether a resources is `by_time` or `by_unit` and compute usage accordingly per billing update. If we are just using the bytes uploaded and downloaded that are tracked by the resource usage monitor, then I think we can do a first pass at adding this functionality. If we have to track by IP address, I don't know how to do that and would have to look into it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13428#issuecomment-1692021482:62,update,updates,62,https://hail.is,https://github.com/hail-is/hail/issues/13428#issuecomment-1692021482,2,['update'],"['update', 'updates']"
Deployability,"I can't update that until this goes in, though. Otherwise everything else will fail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1572#issuecomment-287901053:8,update,update,8,https://hail.is,https://github.com/hail-is/hail/pull/1572#issuecomment-287901053,1,['update'],['update']
Deployability,I can’t help you without an error message or description of what didn’t work. I recommend waiting for the next release which should come out today.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12844#issuecomment-1501909992:111,release,release,111,https://hail.is,https://github.com/hail-is/hail/issues/12844#issuecomment-1501909992,1,['release'],['release']
Deployability,"I changed that the `_update_token` is no longer cached. I thought it was the source of a bug, but it was something else that was an issue. However, I felt it was confusing and I didn't see what value it provided as we have retries on all of our client operations that would need the token and the token is not used in future operations to submit the update.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12199#issuecomment-1255509466:350,update,update,350,https://hail.is,https://github.com/hail-is/hail/pull/12199#issuecomment-1255509466,1,['update'],['update']
Deployability,"I changed the `deploy` step to now take the built docs and publish them to a folder in hail-common. In the future, `make_pip_versioned_docs` will be deleted, so that we stop rebuilding an old version of the docs on every commit. We will simply serve the most recent version of the docs from here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11294:15,deploy,deploy,15,https://hail.is,https://github.com/hail-is/hail/pull/11294,1,['deploy'],['deploy']
Deployability,"I changed the fam_expr string argument to **fam_args that are checked in Python. I also changed the args to Python stye (is_case instead of isCase) and updated the import_fam and importFam docs/tests. Porting of import_plink will need similar translation. I've temporarily commented out the last two `assertRaises` tests as they've uncovered a bug in other code, talking to Tim about it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2750:152,update,updated,152,https://hail.is,https://github.com/hail-is/hail/pull/2750,1,['update'],['updated']
Deployability,I changed this late in the PR and forgot to update the Makefile.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10394:44,update,update,44,https://hail.is,https://github.com/hail-is/hail/pull/10394,1,['update'],['update']
Deployability,"I checked out your branch, ran `make install-hailctl`, started a cluster, connected to a notebook, and ran `hl.utils.range_table(1_000_000, 10000)._force_count()`. Did not see any monitor UI show up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7087#issuecomment-532861246:37,install,install-hailctl,37,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532861246,1,['install'],['install-hailctl']
Deployability,I checked the database and was surprised to see the SKUs weren't necessarily unique to a specific region. But it makes sense when I looked at their API here: https://cloud.google.com/billing/docs/reference/rest/v1/services.skus/list#sku. I think we should put this in and address what happens if they change the SKU of a particular region if that occurs in the future. We'll just get a bunch of error messages with no price updates and it shouldn't impact users. ~~I will also manually check this in Azure.~~ I checked in both GCP and Azure and the updates looked fine with no errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13607#issuecomment-1726229597:424,update,updates,424,https://hail.is,https://github.com/hail-is/hail/pull/13607#issuecomment-1726229597,2,['update'],['updates']
Deployability,I checked this works with dev deploy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9255:30,deploy,deploy,30,https://hail.is,https://github.com/hail-is/hail/pull/9255,1,['deploy'],['deploy']
Deployability,"I couldn't get rid of gcloud, sadly. `kubectl` is still to pervasively integrated into what build.yaml does. Although removing it would save us 100s of MB of unnecessary transfer and extraction, using the ci_utils_image for this purpose delays deployment of most services by an unacceptable amount of time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10502#issuecomment-845459536:71,integrat,integrated,71,https://hail.is,https://github.com/hail-is/hail/pull/10502#issuecomment-845459536,2,"['deploy', 'integrat']","['deployment', 'integrated']"
Deployability,"I created a new multi-branch configuration that should be better for what we are trying to accomplish. This should fix issues 2 and 3. . For the reproducibility of errors, that will probably take both setting the random seed parameter in Hail for all random tests and getting Jenkins to give better error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/335#issuecomment-214377125:29,configurat,configuration,29,https://hail.is,https://github.com/hail-is/hail/issues/335#issuecomment-214377125,1,['configurat'],['configuration']
Deployability,"I created the cluster using hailctl as hailctl dataproc --beta start hailjupy --vep GRCh37 --optional-components=ANACONDA,JUPYTER --enable-component-gateway --bucket bucketname --project projectname --region us-central1. The following error occurs when trying to read table from bucket,; table1 = hl.read_table(‘gs://…ht’), . FatalError: HailException: incompatible file format when reading: gs://gnomad-public/release/3.0/ht/genomes/gnomad.genomes.r3.0.sites.ht; supported version: 1.1.0, found 1.2.0. Java stack trace:; is.hail.utils.HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); at is.hail.utils.package$.fatal(package.scala:74); at is.hail.variant.RelationalSpec$.readMetadata(MatrixTable.scala:54); at is.hail.variant.RelationalSpec$.readReferences(MatrixTable.scala:71); at is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:586); at is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.23-aaf52cafe5ef; Error summary: HailException: incompatible file format when reading: gs://…ht; supported version: 1.1.0, found 1.2.0. Kindly tell me how can i resolve it?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7647:411,release,release,411,https://hail.is,https://github.com/hail-is/hail/issues/7647,1,['release'],['release']
Deployability,"I decided to break off this chunk from another PR that has stalled. That PR will ultimately build on this to add all developers automatically to dev AND test namespaces, but this should be an improvement for now. A few things in here:. - Deleted all the `DatabaseResource` stuff in the auth driver. Since databases now are created and destroyed with the namespace and not the developer, this is basically dead code.; - Added the ability to add a user for an existing hail identity. This is only permitted in dev namespaces and serves as a way for developers to use the same hail identity across namespaces. There is one caveat here: `create_initial_account.py` tries to copy the `<dev-name>-gsa-key` secret from default into the developer namespace and this code will *not* do that anymore. For the developer to submit jobs to the namespace, they must first manually copy in the secret from `default` if it does not already exist inside the namespace. This is awkward, but IMO acceptable because:; - the copying code in `create_initial_account.py` is already broken anyway because when that script is run in a dev deploy it does not have access to production secrets; - I hope that when we eventually go keyless we can delete the gsa key secrets and this whole problem goes away.; - I feel like it's not too bad to do this manual one time copy as opposed to maintaining code that is privileged enough to reach across namespaces. Seems error prone and like a security headache.; - Deletes `create_initial_account.py` in favor of using our actual API to create the dev user.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13180:1114,deploy,deploy,1114,https://hail.is,https://github.com/hail-is/hail/pull/13180,1,['deploy'],['deploy']
Deployability,"I decided to try to do this in two passes since making changes to deploy logic always finnicky on its own. I think this does the right thing though. . Does build.yaml support a way to say ""depend on this step x if we are doing x at all""? Redeploying the website will have to happen after the `deploy` step runs in the future and publishes the latest version of the docs to hail-common.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11294#issuecomment-1024606310:66,deploy,deploy,66,https://hail.is,https://github.com/hail-is/hail/pull/11294#issuecomment-1024606310,2,['deploy'],['deploy']
Deployability,"I deleted the pod; ```; # k delete pod batch-2554-job-4-main-cc8d4 -n batch-pods; ```; Batch logs when batch discovered 2554 task 4 ""failed"":; ```; INFO | 2019-06-25 12:37:07,611 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-cc8d4; INFO | 2019-06-25 12:37:07,671 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-cc8d4; INFO | 2019-06-25 12:37:07,671 | batch.py | update_job_with_pod:989 | job (2554, 4) mark complete; WARNING | 2019-06-25 12:37:07,676 | batch.py | mark_complete:495 | job (2554, 4) has pod batch-2554-job-4-main-cc8d4 which is terminated but has no timing information. {'api_version': 'v1',; 'kind': 'Pod',; 'metadata': {'annotations': None,; 'cluster_name': None,; 'creation_timestamp': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'deletion_grace_period_seconds': 30,; 'deletion_timestamp': datetime.datetime(2019, 6, 25, 12, 37, 37, tzinfo=tzlocal()),; 'finalizers': None,; 'generate_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipelin",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:218,update,update,218,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,['update'],['update']
Deployability,"I deployed this to the cluster. There was a permissions bootstrap issue that batch could never deploy something that could deploy stuff, because it itself didn't have deploy privileges.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4596#issuecomment-431613221:2,deploy,deployed,2,https://hail.is,https://github.com/hail-is/hail/pull/4596#issuecomment-431613221,4,['deploy'],"['deploy', 'deployed']"
Deployability,I dev deployed `auth` to make sure the navbar didn't break.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13589:6,deploy,deployed,6,https://hail.is,https://github.com/hail-is/hail/pull/13589,1,['deploy'],['deployed']
Deployability,"I dev deployed all *_image steps on a single worker running `main` and saw many fail with corrupted filesystems. I imagine this is because multiple jobs were extracting the same filesystem into the same place. The previous change to using a r/w lock for pulling and deleting images is correct, but we must lock on the image id when *extracting* the actual filesystem. With this change everything passed in my dev. The `BATCH_WORKER_IMAGE_ID` fix from before didn't actually work because of not properly escaping the `{` in the f-string. I also moved the `docker rmi` step to be first in the image cleanup process because I imagine if docker refuses to remove an image we shouldn't remove it from our own cache either.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10675:6,deploy,deployed,6,https://hail.is,https://github.com/hail-is/hail/pull/10675,1,['deploy'],['deployed']
Deployability,I dev deployed it [here](https://internal.hail.is/dgoldste/website/docs/batch/tutorial.html). I clicked around a fair amount but didn't check all the links by hand. The only reason I can think of rendering at runtime is rewriting links but I grepped for '{{' and '{%' in the batch docs on a prod pod and found nothing. Also running jinja over post-sphinx rendered files concerns me in general.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10190#issuecomment-799456662:6,deploy,deployed,6,https://hail.is,https://github.com/hail-is/hail/pull/10190#issuecomment-799456662,1,['deploy'],['deployed']
Deployability,I dev deployed it and this looks good.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8018#issuecomment-580526141:6,deploy,deployed,6,https://hail.is,https://github.com/hail-is/hail/pull/8018#issuecomment-580526141,1,['deploy'],['deployed']
Deployability,I dev deployed this and it still is working fine. It's still pretty slow and I was getting rate limit exceeded errors still trying to attch/detach 64 disks. Average operation time was still 15 seconds. I think part of the problem might be the delay starts at 0.1 for retry_transient_errors. We can consider making this a parameter and setting it to a higher number for this use case.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10630#issuecomment-872539838:6,deploy,deployed,6,https://hail.is,https://github.com/hail-is/hail/pull/10630#issuecomment-872539838,1,['deploy'],['deployed']
Deployability,"I did update all the random tests in the randomness PR, if I disabled any it was definitely unintentional.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12588#issuecomment-1397385080:6,update,update,6,https://hail.is,https://github.com/hail-is/hail/pull/12588#issuecomment-1397385080,1,['update'],['update']
Deployability,I didn't test this yet -- do you want me to try the updated docs or should we wait until we redeploy the infrastructure next? Documentation is [here](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/network_security_group).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11027:52,update,updated,52,https://hail.is,https://github.com/hail-is/hail/pull/11027,1,['update'],['updated']
Deployability,I didnt quite fix all of the select for updates when getting the instance state. Don’t merge until I get a chance to fix it (computer ran out of battery),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7833#issuecomment-573028759:40,update,updates,40,https://hail.is,https://github.com/hail-is/hail/pull/7833#issuecomment-573028759,1,['update'],['updates']
Deployability,"I discovered [issue forms](https://github.blog/changelog/2021-06-23-issues-forms-beta-for-public-repositories/) the other day and thought it might be helpful for directing users to the discussion forum / Zulip chatroom. With this configuration, when someone opens an issue, they'll be presented with some options:; ![Screen Shot 2023-01-13 at 8 01 11 AM](https://user-images.githubusercontent.com/1156625/212326189-214fb8b2-e210-4c96-8b52-7000d5025148.png). If they choose to report a bug, they'll be presented with a form prompting for Hail version and log output.; ![Screen Shot 2023-01-13 at 8 01 46 AM](https://user-images.githubusercontent.com/1156625/212326274-affeaa80-adec-45c9-b436-73059c6fc841.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12595:230,configurat,configuration,230,https://hail.is,https://github.com/hail-is/hail/pull/12595,1,['configurat'],['configuration']
Deployability,"I discovered this when I tried to run a vcf combiner pipeline. To me, this signals that we need better knowledge of where integration tests live and how to add to them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5033#issuecomment-449468618:53,pipeline,pipeline,53,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449468618,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,"I do not fully understand the Azure git tagging scheme, but [this commit](https://github.com/Azure/azure-sdk-for-java/commit/054df3fb74098f4ee30eeb1df70df1e40438d169) appears to have made `close` idempotent. It was merged in June of 2022. That commit resolved [an issue](https://github.com/Azure/azure-sdk-for-java/issues/24782) reporting an error very similar to our own. All the azure version changes update the Azure packages to their latest versions as of 2023-05-09 1713 ET. Fixes #12976",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13018:403,update,update,403,https://hail.is,https://github.com/hail-is/hail/pull/13018,1,['update'],['update']
Deployability,I do not know where `static` came from. The shiny docs indicate that static files; should be placed inside a `www` directory which is a sibling to the `app.R` file. Already deployed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8930:173,deploy,deployed,173,https://hail.is,https://github.com/hail-is/hail/pull/8930,1,['deploy'],['deployed']
Deployability,I do not recall but I think it was after the upgrade to major version 8,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14366#issuecomment-2253343407:45,upgrade,upgrade,45,https://hail.is,https://github.com/hail-is/hail/pull/14366#issuecomment-2253343407,1,['upgrade'],['upgrade']
Deployability,"I don't know how to test this works unless I can dev deploy to my own copy of CI that's running with the new changes. The issue I was seeing is the database step runs fine, but the new tables weren't actually created.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9249:53,deploy,deploy,53,https://hail.is,https://github.com/hail-is/hail/pull/9249,1,['deploy'],['deploy']
Deployability,I don't know how to write this operation other than to do an insert on duplicate key update or to do a for loop and update each attempt individually. @danking Do you know of a better way to update multiple values in a single query?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11998#issuecomment-1222867456:85,update,update,85,https://hail.is,https://github.com/hail-is/hail/pull/11998#issuecomment-1222867456,3,['update'],['update']
Deployability,"I don't know if your deployment does this, but this is the sequence that I see with the edit page. <img width=""1242"" alt=""Screen Shot 2020-11-12 at 2 21 18 PM"" src=""https://user-images.githubusercontent.com/1693348/98986116-78110900-24f2-11eb-91f1-d9e35b826b16.png"">; <img width=""1263"" alt=""Screen Shot 2020-11-12 at 2 21 29 PM"" src=""https://user-images.githubusercontent.com/1693348/98986126-7b0bf980-24f2-11eb-8ebd-97e1319068df.png"">; <img width=""1262"" alt=""Screen Shot 2020-11-12 at 2 21 37 PM"" src=""https://user-images.githubusercontent.com/1693348/98986135-7d6e5380-24f2-11eb-9f77-9161800e0a59.png"">; <img width=""1256"" alt=""Screen Shot 2020-11-12 at 2 21 44 PM"" src=""https://user-images.githubusercontent.com/1693348/98986141-7fd0ad80-24f2-11eb-8b38-23b9ea5dd32c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9684#issuecomment-726287286:21,deploy,deployment,21,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-726287286,1,['deploy'],['deployment']
Deployability,I don't know why I'm getting pylint errors for Pipeline. The only thing I can think of is I changed the PR build environment with the new docker image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5563#issuecomment-471732362:47,Pipeline,Pipeline,47,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-471732362,1,['Pipeline'],['Pipeline']
Deployability,"I don't like `inNonParX`. It will confuse people (it confused me). What about `inHemiX`? You're already using that in CopyState. So `(v.contig == ""X"" || v.contig == ""23"") == (inParX || inHemiX)`. Check the plink chromosome numbers in Variant until the reference fix goes in. The Pos versions are fine. When you change the expression language, you have to update the docs, too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/499#issuecomment-235660516:355,update,update,355,https://hail.is,https://github.com/hail-is/hail/pull/499#issuecomment-235660516,1,['update'],['update']
Deployability,"I don't love what I've had to do with the deploy config stuff. That's in my opinion the most finicky part of this (has already broken multiple times) and it's mostly our fault, because we overload the `namespace` parameter with both identifying the namespace in Kubernetes and signifying whether the environment is prod or not. All I want really is to change the `domain` to a domain and path prefix, and not have the namespace have such an impact on routing. Like what if `namespace` didn't affect routing, but if the deploy config only gave a domain with no path e.g. `hail.is`, we use subdomains so `batch.hail.is`, but if we provided a domain with a path prefix like `internal.hail.is/dgoldste`, we make the batch root `internal.hail.is/dgoldste/batch`?. Alternative: Actually have and use a `base_path` in the deploy config. This would be used in dev and terra environments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13944#issuecomment-1785575616:42,deploy,deploy,42,https://hail.is,https://github.com/hail-is/hail/pull/13944#issuecomment-1785575616,3,['deploy'],['deploy']
Deployability,"I don't think I actually understand how artifact and snapshot dependencies work in TeamCity. I thought a build by the main build configuration (the regular CI) would trigger a build of the docs build configuration. This was not the case and I'm not sure why. I've set up the docs build to trigger on any change to master. Unfortunately, we have to `compileScala` twice because these are separate builds. I'll add an issue to clean this up and make it more sensible. There's got to be a right way to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/733#issuecomment-244475645:129,configurat,configuration,129,https://hail.is,https://github.com/hail-is/hail/issues/733#issuecomment-244475645,2,['configurat'],['configuration']
Deployability,"I don't think I'm quite satisfied with this implementation, although I think that it will do the things we need it to do, for the most part. It mostly adheres to the structure that @chrisvittal was implementing in #5228, although I had some questions/notes about implementation/interface and would love some input:. * I'd like to implement (in python) some sort of while loop construct, but I'm not sure what it looks like. I think I'd like to think of that as our primary loop construct since the general recursive loop function can be confusing to start with in terms of what's allowed (what is tail-recursive? what is non-tail-recursive?) if you're just trying to implement some convergence criteria. Some initial thoughts on interface:; ```; 1:; loop = hl.WhileBuilder(i=0, x=0); loop.cond(loop.i < 10); .update(x = loop.x + i, ; i = loop.i + 1); .result(loop.x). 2: ; loop = hl.while_loop(; lambda i, x: i < 10, ; lambda i, x: (x + i, i + 1), ; lambda i, x: x, ; 0, 0). 3:; loop = hl.while_loop(; lambda i, x: ; hl.loop.cond(i < 10); .update(i + 1, x + i); .result(x).; 0, 0); ```; but I'm not sure I really like any of them.; * the scoping for `Recur` is difficult to check and enforce.; * It's difficult to check if a TailLoop is invalidly attempting to recur a function from a different loop (in a nested environment), except by the type signature. I think I could give each loop a name so that `Recur` unambiguously refers to a loop defined in the surrounding scope, mostly treating `Recur` as something of a function reference.; * The code generation is rather inconsistent, since the `Recur` node technically has the same type as the return type of the function, but the code generated needs to be a jump node with no actual value (which makes the generated EmitTriplet look a lot like it has type TVoid!); * @patrick-schultz proposed a similar design, but with two additional types to make the difference between the type of the `Recur` concept and the actual return type more explicit. I ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614:809,update,update,809,https://hail.is,https://github.com/hail-is/hail/pull/7614,1,['update'],['update']
Deployability,"I don't think it is bad to have both. They have two different use cases. I envisioned `head` as being a mechanism to test pipelines on small amounts of data. `take` seems to be useful if someone actually wants to look at each object in the first n rows of data. However, it does add extra methods to VariantDataset when `take` is equivalent to `head().collect()`. Thinking back to the group/ungroup discussion, we decided to add those methods even though they could be implemented by the user in expr. However, I think those operations were more complicated than `take`. I don't have strong feelings either way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2204#issuecomment-328148402:122,pipeline,pipelines,122,https://hail.is,https://github.com/hail-is/hail/pull/2204#issuecomment-328148402,1,['pipeline'],['pipelines']
Deployability,I don't think it is used anymore. Builds are failing because it is returning 500. > Unable to load Maven meta-data from https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml.; > Could not get resource 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'.; > Could not GET 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'. Received status code 500 from server: Server Error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9418:170,release,releases,170,https://hail.is,https://github.com/hail-is/hail/pull/9418,3,['release'],['releases']
Deployability,"I don't think so - as long as the same version of Spark and Hail are installed, it should work with 2.0.2 or 2.1.0 or CDH deployments of those versions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321255115:69,install,installed,69,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321255115,2,"['deploy', 'install']","['deployments', 'installed']"
Deployability,"I don't think there's any good reason why the other pip packages are in a separate RUN step. In fact, using one pip invocation should ensure we get compatible versions whereas what we have now doesn't ensure that. I have no objections to using the hail/python/pinned-requirements.txt. If we install that and all these extra dependencies in one layer then install the hail wheel after (without -U), I *think* it should see all the dependencies are met and just install the hail package.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12301#issuecomment-1275353805:291,install,install,291,https://hail.is,https://github.com/hail-is/hail/pull/12301#issuecomment-1275353805,3,['install'],['install']
Deployability,"I don't think this is actually a bug -- I think this line doesn't support valid requirements.txt files with comments:; ```; sed '/^pyspark/d' python/requirements.txt \| xargs python3 -m pip install -U; ```. Instead, let's try:. ```; cat python/requirements.txt | sed '/^pyspark/d' | grep -v ""^#""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10352#issuecomment-826851749:190,install,install,190,https://hail.is,https://github.com/hail-is/hail/issues/10352#issuecomment-826851749,1,['install'],['install']
Deployability,"I don't think this keeps too much garbage in memory. Your next method extracts exactly the data it needs from its producer. No garbage there, you asked for only data you absolutely need. You stated (via `addReferenceTo`) that your region references these child regions, so that memory must be accessible at least as long as your region is accessible. Whoever is consuming your data can release all this memory by clearing the region you're using. The only nodes which should be clearing are folks who call `next` multiple times *and don't need that data to have the same lifetime*. This is true for filter, only surviving values must live, other values' lifetimes may end when we discover they fail the filter condition. It's also true for `write` because after one value is dumped into a file, it is no longer needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7952#issuecomment-578798366:386,release,release,386,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-578798366,2,['release'],['release']
Deployability,"I don't think we should merge this. This PR got a bit out of hand and then died when I changed focus to the query service. There are two distinct changes that I had hoped to unify: nicely rendered dev-docs *and* generated docs for gear, web_common, & hailtop. I got stuck after getting each one rendering OK but not integrated with one another. I also never got to dynamic rendering of the header (i.e. logged in users see batch > batches, etc.). The first thing you should check out are the rendered library docs:; ```; (cd docs && make html && (cd build/html && python3 -m http.server)); # now navigate to http://localhost:8000/; ```; You'll notice the Hail CSS is missing a bunch of styles to make functions render nicely. Take a look at the generated HTML. Sphinx includes a few style tags that we should probably define. I also fixed a few docs issues. There are many more broken references to fix. ![Screen Shot 2021-02-09 at 11 07 34 PM](https://user-images.githubusercontent.com/106194/107463257-acb15280-6b2b-11eb-8a26-129697009ef8.png); ![Screen Shot 2021-02-09 at 11 07 50 PM](https://user-images.githubusercontent.com/106194/107463256-acb15280-6b2b-11eb-82ff-48b6d83f2f0f.png). Now you should check out the rendered dev docs:; ```; (cd site && make render && cd docs && python3 -m http.server); ```; ![Screen Shot 2021-02-09 at 11 11 07 PM](https://user-images.githubusercontent.com/106194/107463544-555fb200-6b2c-11eb-9b23-39f66f0f4b12.png); ![Screen Shot 2021-02-09 at 11 11 16 PM](https://user-images.githubusercontent.com/106194/107463545-555fb200-6b2c-11eb-9901-5af07effc814.png). ---. What's left to do?. 1. Make the header dynamic (i.e. logged-in users see their name, etc.)?; 2. Move the dev-docs and the python library docs into one location.; 3. Finish modifying `site` so that it hosts two servers: `hail.is` and `docs.hail.is`. `docs.hail.is` displays some landing page from which we can navigate to dev-docs or python library docs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10027:316,integrat,integrated,316,https://hail.is,https://github.com/hail-is/hail/pull/10027,1,['integrat'],['integrated']
Deployability,I don't understand why Kubernetes wants to put `batch-deployment` and the service `batch` in the `batch-pods` namespace.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432381399:54,deploy,deployment,54,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432381399,1,['deploy'],['deployment']
Deployability,"I dropped a bunch of args to spark and accidentally prepended every Hail QoB pipeline name with ""Hail""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11691:77,pipeline,pipeline,77,https://hail.is,https://github.com/hail-is/hail/pull/11691,1,['pipeline'],['pipeline']
Deployability,I duplicated globals.py so I can get the tests going. I'll think about how to organize sharing between the client in the server. Might just be to install the client on the server image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6423#issuecomment-504153212:146,install,install,146,https://hail.is,https://github.com/hail-is/hail/pull/6423#issuecomment-504153212,1,['install'],['install']
Deployability,I ended up posting on the forum. I did update to the newest version. It did not generate an error this time. The job ran much further but hung with 4 tasks left. . John,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106#issuecomment-599765896:39,update,update,39,https://hail.is,https://github.com/hail-is/hail/issues/8106#issuecomment-599765896,1,['update'],['update']
Deployability,"I eventually found the command line below that worked. It would be helpful to update the Getting Started page to include any necessary command line --conf parameters. ` spark-submit --jars build/libs/hail-all-spark.jar --conf ""spark.driver.extraClassPath=file:///restricted/projectnb/genpro/github/hail/build/libs/hail-all-spark.jar"" --conf ""spark.executor.extraClassPath=file:////restricted/projectnb/genpro/github/hail/build/libs/hail-all-spark.jar"" --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator --py-files build/distributions/hail-python.zip --num-executors 6 test.py; `",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342#issuecomment-380210064:78,update,update,78,https://hail.is,https://github.com/hail-is/hail/issues/3342#issuecomment-380210064,1,['update'],['update']
Deployability,"I feel a bit like a cheat here since there's been a fair bit of work since you last reviewed. Most of it was fixes of tiny bugs that the CI revealed. There was [one, kind of notable, change](https://github.com/hail-is/hail/pull/5194/commits/f95e4e0ff1cdd2865cf703aa27f780c7f162316c). I removed Spark from the Dockerfile. It is no longer necessary because the pip install will pull the correct version of Spark. To avoid pulling Spark on each PR build, I cache 2.2.0 (and all our other pip dependencies) in the hail conda env in the docker image. Doing this required that I move our requirements into a requirements file which is parameterized by spark version. A good follow up PR would be to either a) entirely remove dependency on conda or b) generate the conda `environment.yml` from `requirements.txt.in`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333:363,install,install,363,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333,1,['install'],['install']
Deployability,I feel like this will only be useful once it updates all requirements files at once. It's missing one here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11696#issuecomment-1081106041:45,update,updates,45,https://hail.is,https://github.com/hail-is/hail/pull/11696#issuecomment-1081106041,1,['update'],['updates']
Deployability,"I finally figured out how to get the authorization bearer token for the Grafana robot into Grafana automatically. The problem I'm running into right now is when we load a datasource from a configuration file, we can not edit any of the settings in the UI. We'd want to make sure all the prometheus settings we want are inside the new config file. I also don't want to accidentally overwrite any of the existing configuration.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10772:189,configurat,configuration,189,https://hail.is,https://github.com/hail-is/hail/pull/10772,2,['configurat'],['configuration']
Deployability,"I find the SHOUTing is part of what makes the file feel so visually overwhelming, but there's much less SHOUTing now because there's just fewer keywords. Updated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13372#issuecomment-1673746693:154,Update,Updated,154,https://hail.is,https://github.com/hail-is/hail/pull/13372#issuecomment-1673746693,1,['Update'],['Updated']
Deployability,"I find the current installation docs totally overwhelming. If you are using Hail; on a Mac you should not see any unnecessary crap about Linux, clusters, and; BLAS. This change introduces four flows: mac, linux, dataproc, cluster. Each page's; complexity matches the true complexity of installing Hail on that platform. In; particular, note how simple the Linux and Mac OS X pages are. It also clears up the ""other cluster"" case. Our current docs are too complex and; don't push people towards simple solutions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9017:19,install,installation,19,https://hail.is,https://github.com/hail-is/hail/pull/9017,2,['install'],"['installation', 'installing']"
Deployability,"I fixed scorecard deploy stuff and now it is working with `dev deploy`. I also pushed some CSS changes:; - body { margin: 0; } that removes the extra header spacing; - but added an 0 8px 8px 8px margin to #content; - simplified the header layout CSS; - fixed the header item clickable area, should be bigger and uniform across header items; - fixed the misalignment on Safari. I'm pretty happy with this for this iteration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-531991835:18,deploy,deploy,18,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-531991835,4,['deploy'],['deploy']
Deployability,"I fixed up refreshing from batch on deploy jobs recently, but now we might find a job from a previous CI instance (which was killed by an update say, or a node loss) and that job might have a different job id than the one we expected (if we eagerly launched a deploy before we refreshed from batch). That's totally OK, as long as the target SHAs match, because if the target SHAs match, then the job *must* be deploying the same thing we intended to deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4723#issuecomment-435512500:36,deploy,deploy,36,https://hail.is,https://github.com/hail-is/hail/pull/4723#issuecomment-435512500,5,"['deploy', 'update']","['deploy', 'deploying', 'update']"
Deployability,"I forgot that we still had cron jobs running gcr-cleaner daily. This could have been conflicting with the new cleanup policy deletion settings. Let's reopen if this occurs again. Posting the job configurations here before I delete the jobs. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:195,configurat,configurations,195,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,1,['configurat'],['configurations']
Deployability,I forgot to include the changes in #14056 to the scala code as well. This favors using `basePath` in the Scala deploy config over the `defaultNamespace`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14195:111,deploy,deploy,111,https://hail.is,https://github.com/hail-is/hail/pull/14195,1,['deploy'],['deploy']
Deployability,"I found a linux that doesn't have `<execinfo.h>` as part of `libc`, thus requiring an external library. This is mostly so I can work on my desktop again, which had gotten to a pretty screwed up state regarding updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8387#issuecomment-605674200:210,update,updates,210,https://hail.is,https://github.com/hail-is/hail/pull/8387#issuecomment-605674200,1,['update'],['updates']
Deployability,"I get the following version: ; ``; $ java -version; java version ""1.8.0_111""; Java(TM) SE Runtime Environment (build 1.8.0_111-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.111-b14, mixed mode); ``; I'm using virtualenv to run python 2.7 and I think I installed all the dependencies and python libraries that were required. Any further idea on what I can do to get this to work?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319675148:256,install,installed,256,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319675148,1,['install'],['installed']
Deployability,I get this error trying to install GWASTools,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3273#issuecomment-377701080:27,install,install,27,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377701080,1,['install'],['install']
Deployability,"I got fed up with ksync not working the way I wanted and wrote my own; version. Assuming `repo/devbin` is on your path, then:. ```; sync.sh batch dking; ```. will keep the web_common, gear, hailtop, and batch repositories all up to date; in k8s. It just copies all the files over from your local machine, so its not; the fastest thing in the world. It takes maybe 2 seconds to get back to a; working container. It also assumes there's one container per-pod. It is completely disabled in the default namespace (I remove it in; deployment.yaml if we are in deploy mode).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9759:526,deploy,deployment,526,https://hail.is,https://github.com/hail-is/hail/pull/9759,2,['deploy'],"['deploy', 'deployment']"
Deployability,"I gotta go to bed but it looks like maybe we need more role bindings all for `""get""` for all the objects we want to deploy in `batch-pods`? Or maybe we're applying that `deployment.yaml` to the wrong namespace? That one should be deployed in the default namespace. So maybe the answer is to ignore this until that PR gets merged? At this point I think we should just force merge the PR @cseed. NB: this is still without the `deploy-svc` PR merged due to some other yet undiagnosed weirdness, see #4608 for a taste.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432084571:116,deploy,deploy,116,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432084571,4,['deploy'],"['deploy', 'deploy-svc', 'deployed', 'deployment']"
Deployability,"I grepped through and fixed all instances of 'ci2' and 'hail-ci' (the old service name). When this goes in, we'll have to had-delete the old ci2 deployment and service.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6193:145,deploy,deployment,145,https://hail.is,https://github.com/hail-is/hail/pull/6193,1,['deploy'],['deployment']
Deployability,"I guess I was afraid of somebody copying and pasting from the README. But if they're going to do that, they're not going to get right version anyway. You don't know the release hash until it goes in, which is annoying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5915#issuecomment-484732538:169,release,release,169,https://hail.is,https://github.com/hail-is/hail/pull/5915#issuecomment-484732538,1,['release'],['release']
Deployability,"I guess it depends whether you want up to date or just compatible, the maintainers seem to be of the opinion that you should either always update the lock file immediately or set upper bounds if you're not ok with a certain upgrade, seen [here](https://github.com/jazzband/pip-tools/issues/882). Continuous work, but maybe the right way to go honestly. In that case trivially make a build.yaml step that asserts the lock file is valid and up to date.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11842#issuecomment-1131859471:139,update,update,139,https://hail.is,https://github.com/hail-is/hail/pull/11842#issuecomment-1131859471,3,"['Continuous', 'update', 'upgrade']","['Continuous', 'update', 'upgrade']"
Deployability,"I guess the Spark 2.4 upgrade broke apiserver, but honestly I don't know how this could have worked before. Deployed by hand and verified it's working. I'm talking in methods tomorrow, might try to demo this. It would be nice if it is working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5863:22,upgrade,upgrade,22,https://hail.is,https://github.com/hail-is/hail/pull/5863,2,"['Deploy', 'upgrade']","['Deployed', 'upgrade']"
Deployability,I had the same issue - I did `pip install pypandoc` and it worked fine after that :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9742#issuecomment-816041422:34,install,install,34,https://hail.is,https://github.com/hail-is/hail/issues/9742#issuecomment-816041422,1,['install'],['install']
Deployability,"I had to make a few changes to CI to make it test against a local version of the latest batch. Long term, we'll actually deploy batch and CI into a fresh namespace to do testing. For now, we explicitly start batch in the test container for CI to test against.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4930#issuecomment-446008184:121,deploy,deploy,121,https://hail.is,https://github.com/hail-is/hail/pull/4930#issuecomment-446008184,1,['deploy'],['deploy']
Deployability,"I ham fingered the name of the credentials. Unfortunately, PR tests do not test the deployment script.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4266:84,deploy,deployment,84,https://hail.is,https://github.com/hail-is/hail/pull/4266,1,['deploy'],['deployment']
Deployability,"I hardcoded us-central1, which is the only thing we're using right now. Otherwise, we'd have to change CI before deploy. Also, clearly a fixed global zone is naive, so I think we have to reconsider the GCP configuration going forward. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8016:113,deploy,deploy,113,https://hail.is,https://github.com/hail-is/hail/pull/8016,2,"['configurat', 'deploy']","['configuration', 'deploy']"
Deployability,"I have a branch where I've upgraded the dependency to libsimdpp-2.1 and resolved issues around depreciation warnings, this does solve the issue. We would need to discuss if we want to upgrade the dependency, and benchmark against the new version to see if it causes any performance regression. Branch is [here](https://github.com/chrisvittal/hail/tree/libsimdpp-2.1)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955#issuecomment-406297780:27,upgrade,upgraded,27,https://hail.is,https://github.com/hail-is/hail/issues/3955#issuecomment-406297780,2,['upgrade'],"['upgrade', 'upgraded']"
Deployability,"I have a mild preference to get https://github.com/hail-is/hail/pull/12769/files into this release so I don't have to release again tomorrow, but I will release again tomorrow if necessary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12770#issuecomment-1460486196:91,release,release,91,https://hail.is,https://github.com/hail-is/hail/pull/12770#issuecomment-1460486196,3,['release'],['release']
Deployability,"I have a partial setup of the website in k8s, will poll for a new hash and update the docs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4375:75,update,update,75,https://hail.is,https://github.com/hail-is/hail/pull/4375,1,['update'],['update']
Deployability,"I have build hail ,using ""gradle installDist"", the ""./hail -h"" can display:. [root@**\* bin]# ./hail -h; usage: hail [global options] <cmd1> [cmd1 args]; [<cmd2> [cmd2 args] ... <cmdN> [cmdN args]]. But ,When excuting “gradle check” and ""gradle coverage"", encounter ""100 tests completed, 3 failed :test FAILED"" ""Build FAILED"" , how to fix ? Thanks . [root@**\* hail]# gradle check; Using a seed of [1] for testing.; Build file '*****/hail/build.gradle': line 188; useAnt has been deprecated and is scheduled to be removed in Gradle 3.0. The Ant-Based Scala compiler is deprecated, please see https://docs.gradle.org/current/userguide/scala_plugin.html.; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :compileTestJava UP-TO-DATE; :compileTestScala UP-TO-DATE; :processTestResources UP-TO-DATE; :testClasses UP-TO-DATE; :test. ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); ........... FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite). ..........; 100 tests completed, 3 failed; :test FAILED. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > There were failing tests. See the report at: file:///****/hail/build/reports/tests/index.html; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --deb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457:33,install,installDist,33,https://hail.is,https://github.com/hail-is/hail/issues/457,1,['install'],['installDist']
Deployability,"I have diagnosed the root cause of the issue observed by; both Patrick and Chris: incorrect spilling of method parameter variables. This patch makes the bug impossible to replicate using proper interfaces,; though does not fix the underlying issue in LIR. Here's a way to replicate:. ```; val mb = kb.genEmitMethod(""btree_foo"", FastIndexedSeq[ParamType](typeInfo[Long]), typeInfo[Unit]); mb.voidWithBuilder { cb =>; val arg = mb.getCodeParam[Long](1).asInstanceOf[Settable[Long]]. cb.assign(arg, arg + 1L). (0 until 100).foreach { i =>; cb.println(s""i=$i, arg="", arg.toS); }. }; cb.invokeVoid(mb,const( 0L)); ```. called with `0`, this prints `1` until i=84, then starts printing 0 again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9946:137,patch,patch,137,https://hail.is,https://github.com/hail-is/hail/pull/9946,1,['patch'],['patch']
Deployability,"I have no explanation for the behavior of `pip`, it simply refuses to upgrade to the latest cloud tools. ```; + pip search cloudtools; cloudtools (1.1.16) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools. real	0m0.867s; user	0m0.649s; sys	0m0.084s; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/46/78/966c9af5b88a01af73bb56486e853c00ff4865de0bf380282aa54fdec43a/cloudtools-1.1.15-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.15. real	0m1.718s; user	0m1.378s; sys	0m0.158s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700:70,upgrade,upgrade,70,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700,7,"['Install', 'install', 'upgrade']","['Installing', 'install', 'installed', 'upgrade']"
Deployability,I have no idea if I've updated everything I need to update here. I haven't pushed the updated CI; build image or changed cloudtools (yet). Update: pushed ci build image. cc @cseed @danking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5756:23,update,updated,23,https://hail.is,https://github.com/hail-is/hail/pull/5756,4,"['Update', 'update']","['Update', 'update', 'updated']"
Deployability,"I have not tested this, though I faithfully copied the commands from existing; deploy scripts (except for creating a github release). A change that I think is valuable regardless of automation is the conversion of; deploy from a series of Makefile targets to a bash script. I also add a deploy build.yaml step which simply calls the deploy script,; setting up appropriate credentials. I only had to add one set of credentials: the PyPI credentials. I've already; created that secret in the cluster. Hand deploys are still very easy. You need curl >=7.55.0 (that version; implemented reading headers from a file). You need to set up two things:; 1. create $HOME/.pypirc and put this there:; ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```; 2. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:; ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```; Now, to do a hand deploy run:; ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FOR_hail-is/hail; ```. The github credentials are used to create a GitHub release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8533:79,deploy,deploy,79,https://hail.is,https://github.com/hail-is/hail/pull/8533,9,"['deploy', 'release']","['deploy', 'deploys', 'release']"
Deployability,I have now installed the pre-commit hooks on my new machine.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12959#issuecomment-1531617322:11,install,installed,11,https://hail.is,https://github.com/hail-is/hail/pull/12959#issuecomment-1531617322,1,['install'],['installed']
Deployability,"I have to go, but what happens when you update the rows to 0. Does the trigger run? I presume the n_cancelled_creating_jobs stay at 0 because now the additions / subtractions are in sync?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11278#issuecomment-1023511163:40,update,update,40,https://hail.is,https://github.com/hail-is/hail/pull/11278#issuecomment-1023511163,1,['update'],['update']
Deployability,"I haven't run very many Hail pipelines since ASHG, so there hasn't been much opportunity to see this bug. Sorry I can't help more!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4418#issuecomment-454131636:29,pipeline,pipelines,29,https://hail.is,https://github.com/hail-is/hail/issues/4418#issuecomment-454131636,1,['pipeline'],['pipelines']
Deployability,"I haven't yet Chris, but `astroid` also hasn't released in ~4 months, so I still have to build and check if it's fixed yet. Planning on doing so when I have some time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9804#issuecomment-742033412:47,release,released,47,https://hail.is,https://github.com/hail-is/hail/pull/9804#issuecomment-742033412,1,['release'],['released']
Deployability,I implemented all pods and pvcs for a job have the same name. That way we let k8s be responsible for keeping track of the atomicity of pod and pvc updates rather than our database.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6519#issuecomment-507063867:147,update,updates,147,https://hail.is,https://github.com/hail-is/hail/pull/6519#issuecomment-507063867,1,['update'],['updates']
Deployability,I installed hail and finally everything went well without any missing package.; When I ran it to test it. It gave me the following error. Check the screen capture for more details.; `./build/install/hail/bin/hail \; importvcf src/test/resources/sample.vcf \; write -o ~/sample.vds`; ![error](https://cloud.githubusercontent.com/assets/2621305/22890051/0a0a737c-f203-11e6-84f1-aa51c8278ca5.png),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1377:2,install,installed,2,https://hail.is,https://github.com/hail-is/hail/issues/1377,2,['install'],"['install', 'installed']"
Deployability,"I installed latest hail (0.2.120-f00f916faf78), gnomad (e6f042a74c91e462b77fca24d070c815e02f6f5b), and gnomad_qc (0c52cf47e48fa5b503d874e96482ea4286474c71). I cloned the repo in question; ```bash; pip3 uninstall hail gnomad gnomad_qc. pip3 install -U \; hail \; git+https://github.com/broadinstitute/gnomad_methods.git \; git+https://github.com/broadinstitute/gnomad_qc.git. git clone git@github.com:broadinstitute/gnomad-readviz.git; ```. I applied this patch:; ```diff; diff --git a/step1__select_samples.py b/step1__select_samples.py; index c159207..9ba1812 100644; --- a/step1__select_samples.py; +++ b/step1__select_samples.py; @@ -38,14 +38,7 @@ def hemi_expr(mt):; ; def main(args):; ; - hl.init(log=""/select_samples"", default_reference=""GRCh38"", idempotent=True, tmp_dir=args.temp_bucket); - meta_ht = hl.import_table(args.sample_metadata_tsv, force_bgz=True); - meta_ht = meta_ht.key_by(""s""); - meta_ht = meta_ht.filter(hl.is_defined(meta_ht.cram_path) & hl.is_defined(meta_ht.crai_path), keep=True); - meta_ht = meta_ht.repartition(1000); - meta_ht = meta_ht.checkpoint(; - re.sub("".tsv(.b?gz)?"", """", args.sample_metadata_tsv) + "".ht"", overwrite=True, _read_if_exists=True); -; + hl.init(log=""/tmp/select_samples"", default_reference=""GRCh38"", idempotent=True, tmp_dir=args.temp_bucket); vds = gnomad_v4_genotypes.vds(); ; # see https://github.com/broadinstitute/ukbb_qc/pull/227/files; @@ -55,19 +48,8 @@ def main(args):; ; v4_qc_meta_ht = meta.ht(); ; - mt = vds.variant_data; - #mt = vds.variant_data._filter_partitions([41229]); -; - mt = mt.filter_cols(v4_qc_meta_ht[mt.s].release); -; - meta_join = meta_ht[mt.s]; - mt = mt.annotate_cols(; - meta=hl.struct(; - sex_karyotype=meta_join.sex_karyotype,; - cram=meta_join.cram_path,; - crai=meta_join.crai_path,; - ); - ); + #mt = vds.variant_data; + mt = vds.variant_data._filter_partitions([41229]); ; logger.info(""Adjusting samples' sex ploidy""); lgt_expr = hl.if_else(; @@ -88,9 +70,9 @@ def main(args):; logger.info(""Filter variants wi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13248#issuecomment-1703383664:2,install,installed,2,https://hail.is,https://github.com/hail-is/hail/issues/13248#issuecomment-1703383664,3,"['install', 'patch']","['install', 'installed', 'patch']"
Deployability,"I installed pandas 1.5.2 and ran every test with the word pandas in the name:; ```; =============================================== short test summary info ===============================================; PASSED test/hail/table/test_table.py::Tests::test_from_pandas_mismatched_object_rows; PASSED test/hail/table/test_table.py::Tests::test_from_pandas_missing_and_nans; PASSED test/hail/table/test_table.py::Tests::test_from_pandas_objects; PASSED test/hail/table/test_table.py::Tests::test_from_pandas_works; PASSED test/hail/table/test_table.py::test_to_pandas; PASSED test/hail/table/test_table.py::test_to_pandas_flatten; PASSED test/hail/table/test_table.py::test_to_pandas_null_ints; PASSED test/hail/table/test_table.py::test_to_pandas_nd_array; PASSED test/hail/table/test_table.py::test_literal_of_pandas_NA_and_numpy_int64; PASSED test/hail/table/test_table.py::test_literal_of_pandas_NA_and_numpy_int32; ======================================== 10 passed, 1357 deselected in 40.40s =========================================; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12580#issuecomment-1396345811:2,install,installed,2,https://hail.is,https://github.com/hail-is/hail/pull/12580#issuecomment-1396345811,1,['install'],['installed']
Deployability,"I integrated these changes into https://github.com/broadinstitute/hail/pull/652 except I used block instead of block-inline. I also fixed the line height. (Problem was not using display: block, padding was per-line.) With the line-height fixed, font-size: 0.8em feels too small. How do you think it looks now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/644#issuecomment-241179566:2,integrat,integrated,2,https://hail.is,https://github.com/hail-is/hail/pull/644#issuecomment-241179566,1,['integrat'],['integrated']
Deployability,"I investigated this and it will ~half cluster start up time from 4m30s to 2m30s. Should we do this @cseed? We can use the hail deployment to generate a new image each time master changes (that ensures its always younger than 30 days, given our pace of development).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4038#issuecomment-422929159:127,deploy,deployment,127,https://hail.is,https://github.com/hail-is/hail/issues/4038#issuecomment-422929159,1,['deploy'],['deployment']
Deployability,I just built the image using dev deploy of this branch to check its size and it's only grown by 8MB. What else is causing disk usage growth?. ```; # docker image ls gcr.io/hail-vdc/ci-intermediate:anrd6xyjsrnd; REPOSITORY TAG IMAGE ID CREATED SIZE; gcr.io/hail-vdc/ci-intermediate anrd6xyjsrnd 2a479e5a34c4 53 seconds ago 363MB; # docker image ls gcr.io/hail-vdc/batch-worker:g76daybmi5g1 ; REPOSITORY TAG IMAGE ID CREATED SIZE; gcr.io/hail-vdc/batch-worker g76daybmi5g1 6782b39e4d31 45 hours ago 355MB; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8440#issuecomment-609899181:33,deploy,deploy,33,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-609899181,1,['deploy'],['deploy']
Deployability,"I just found a bug in my GWAS pipeline where I was excluding most of the X-chromosome SNPs for cohorts with no gender reported because I used the flag ""--allow-no-sex"". Unfortunately, this meant HWE was calculated using both the unknown males and females (rather than females only). We should make sure HWE calculations in Hail on the X-chromosome are done smarter than what is done in PLINK!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/200:30,pipeline,pipeline,30,https://hail.is,https://github.com/hail-is/hail/issues/200,1,['pipeline'],['pipeline']
Deployability,I just noticed that we didn't add a foreign key constraint on the jobs table for the batch update. I think unfortunately we should add it... thoughts?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12154#issuecomment-1248619288:91,update,update,91,https://hail.is,https://github.com/hail-is/hail/pull/12154#issuecomment-1248619288,1,['update'],['update']
Deployability,"I just tried cloned locally and `gradle installDist` worked on the first try. We've gotten this compiler error sporadically for a few months, and every time it's been resolved by rebuilding after `gradle clean`. I'll continue to investigate, and see if I can find the source of the problem (could be a compiler bug).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453#issuecomment-229752430:40,install,installDist,40,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229752430,1,['install'],['installDist']
Deployability,I kept things simple. Next round we should add a richer example. Note we'll update 1.5.2 to 1.6.2 once fix is in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/690:76,update,update,76,https://hail.is,https://github.com/hail-is/hail/pull/690,1,['update'],['update']
Deployability,I kinda feel like we can't ship this in 0.2 release in its current state of brokenness,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4193#issuecomment-422375767:44,release,release,44,https://hail.is,https://github.com/hail-is/hail/issues/4193#issuecomment-422375767,1,['release'],['release']
Deployability,"I like this plan overall. Some comments to consider:. - I don't think batch should persist job intermediates which I think more as a pipe (|) or a temporary file in a traditional script. This means the downstream batch clients (CI, Pipeline) should be copying files they want to persist to something like gs. One thing we should be sure here is that the solution doesn't involve excessive copies, e.g. we don't want to generate batch steps that just copy from a temporary persisted location in gs to a permanent location in gs.; - We need a way to refer to individual inputs/outputs. A bioinformatics command might output a massive data file and a report file, and we want to run a command to process or format the report file, but we don't want to copy the (unused) massive data file unnecessarily. Copying everything is a fine start.; - This might be covered by ""parse and exec series of commands"", but we want to be able to specify a series of stacked containers to execute, e.g. in the case of Pipeline, a user command to execute followed by a pipeline-controlled container with Pipeline gs credentials to copy the output to Pipeline (or user) controlled bucket. Same for CI.; - In the long run, we're going to want to be able to control the size of the local disk for user jobs (e.g. a bioinformatics command that needs 1TB of scratch space) separate from the host node's local file system. This will go into the job configuration. We probably don't need this for CI.; - We should separate the database management from the job execution. Probably easiest to make this a monolithic service, but we could separate them. One will interact with batch users to update the database and watch the database, and the other to reconcile the database and k8s by running jobs and updating results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739:232,Pipeline,Pipeline,232,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739,7,"['Pipeline', 'configurat', 'pipeline', 'update']","['Pipeline', 'configuration', 'pipeline-controlled', 'update']"
Deployability,"I like this, `uvloop.install()` has always made me a bit unsettled.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7208#issuecomment-539151226:21,install,install,21,https://hail.is,https://github.com/hail-is/hail/pull/7208#issuecomment-539151226,1,['install'],['install']
Deployability,"I like this. That + depending on the `release` step ensures that we only submit the benchmarks on the exact sha that we release. When we eventually split these steps out into their own release pipeline, we can just delete the file and use the normal `depends_on: release` behavior to achieve the same result.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14398#issuecomment-2027251314:38,release,release,38,https://hail.is,https://github.com/hail-is/hail/pull/14398#issuecomment-2027251314,5,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"I liked your diff idea, so I added a new file: batch/sql/estimated-current.txt. This is meant to be the SQL we'd use for initial.sql if we recreated the batch database. It should have collective migrations applied to it. So when we add a new migration, we should update the estimated current which will give informative documentation for the current change. I use ""estimated"" and ""txt"" because it isn't tested or validated in any way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7916:263,update,update,263,https://hail.is,https://github.com/hail-is/hail/pull/7916,1,['update'],['update']
Deployability,"I looked at the yarn logs. It looks like it is not finding the GLIBCXX_3.4.18 lib. This is how the hail script is being submitted... ```; module load anaconda3/5.2.0; source activate hail2; module load gcc/7.2.0; module load lz4/1.8.3; module load spark/2.2.1; echo ""Export env vars""; export HAIL_HOME=/restricted/projectnb/genpro/github/hail/hail; export PYTHONPATH=""${PYTHONPATH:+$PYTHONPATH:}$HAIL_HOME/build/distributions/hail-python.zip""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:953,deploy,deploy-mode,953,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258,1,['deploy'],['deploy-mode']
Deployability,"I looked at this again and I think we can do this with `online: true`. It's a quick enough migration where it shouldn't impact the driver for too long that it's trying to query the long tables. If there's a problem and the driver can't make forward progress once the database migration is done, we can just shut down the deployment to 0 replicas. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13810#issuecomment-1810311153:321,deploy,deployment,321,https://hail.is,https://github.com/hail-is/hail/pull/13810#issuecomment-1810311153,1,['deploy'],['deployment']
Deployability,"I looked over the code and it looks fine, but I'm having trouble understanding the bigger picture of what you're trying to accomplish. I see that you have a new step that creates a test database in the default namespace in the test scope. Then you create the database config secret from this new database. And then deploy_ci depends on it, which makes sense because it needs the secret to be able to create new databases. And this is all only in the test scope. It looks like you cleaned up the build database in the case of dev deploy, which is fine too. > we also create a ""test_instance"" database that will be used as the database instance inside the tests. I don't understand what you wrote here because test_instance database doesn't seem to be used at all. Aren't we still creating the same batch and ci databases? I don't see what the test_instance database is buying you except to be able to make the database config secret that doesn't have the root username and password. I also don't quite understand what's going on in the build_cant_create_database build step. Shouldn't those secrets already exist? Won't this fail?. I'm sorry if I'm missing something obvious.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906:529,deploy,deploy,529,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906,1,['deploy'],['deploy']
Deployability,"I made `_prev_nonnull` private since it's pretty specific to this use case, but I can imagine we'll want to generalize it to something like `take` which aggregates the last `n` values instead of the first `n` values. Here is an example pipeline against `test-chr22.mt`:. ```; import hail as hl. mt = hl.read_matrix_table('test-chr22.mt'); mt = mt._filter_partitions(range(8)) # make small. # restrict to two samples; mt = mt.filter_cols((mt.s == 'V33335') | (mt.s == 'NWD157935')); mt = mt.annotate_rows(__n = hl.agg.count_where(hl.is_defined(mt.GT))); mt = mt.filter_rows(mt.__n > 0). print(; mt.count()). def show_mt(mt):; entry_fields = ['GT']; if 'END' in mt.entry:; entry_fields.append('END'); (mt.select_rows(); .select_entries(*entry_fields); ._localize_entries('__entries', '__cols'); .show()). show_mt(mt). mt = hl.experimental.densify(mt); show_mt(mt). mt.describe(); ```. which produces sparse and dense samples:. ```; +----------------+-------------------------------------+; | locus | __entries |; +----------------+-------------------------------------+; | locus<GRCh38> | array<struct{GT: call, END: int32}> |; +----------------+-------------------------------------+; | chr22:10510746 | [NA,(0/0,10510769)] |; | chr22:10510770 | [NA,(1/1,NA)] |; | chr22:10510771 | [NA,(0/0,10510891)] |; | chr22:10511207 | [NA,(0/0,10511390)] |; | chr22:10511272 | [(0/0,10511390),NA] |; | chr22:10511391 | [NA,(1/1,NA)] |; | chr22:10511392 | [(0/0,10511393),(0/0,10511477)] |; | chr22:10511397 | [(0/0,10511403),NA] |; | chr22:10511406 | [(0/0,10511418),NA] |; | chr22:10511420 | [(0/0,10511420),NA] |; +----------------+-------------------------------------+; showing top 10 rows. +----------------+-------------------------+; | locus | __entries |; +----------------+-------------------------+; | locus<GRCh38> | array<struct{GT: call}> |; +----------------+-------------------------+; | chr22:10510746 | [NA,(0/0)] |; | chr22:10510770 | [NA,(1/1)] |; | chr22:10510771 | [NA,(0/0)] |; | chr22:10511",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5281:236,pipeline,pipeline,236,https://hail.is,https://github.com/hail-is/hail/pull/5281,1,['pipeline'],['pipeline']
Deployability,I made recent changes such that all targets for anything hail are now in sync and installable in one go (earlier we had separate requirements files between query and batch that could accidentally diverge and were possibly not installable together). Forgot to update the docs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12890:82,install,installable,82,https://hail.is,https://github.com/hail-is/hail/pull/12890,3,"['install', 'update']","['installable', 'update']"
Deployability,I made the docs build in docs/<hailVersion> so the header links work in either local build or the web site. I will update the master branch docs deploy script when this is ready to go in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2923#issuecomment-367442418:115,update,update,115,https://hail.is,https://github.com/hail-is/hail/pull/2923#issuecomment-367442418,2,"['deploy', 'update']","['deploy', 'update']"
Deployability,"I made the name change `gce-deploy-config` to `worker-deploy-config`. Feel free to change the name. Also, I'm pretty sure this name change won't break the deploy because it's created as part of the deploy process. But it has been 198 days since the secret was updated... FYI: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11002:28,deploy,deploy-config,28,https://hail.is,https://github.com/hail-is/hail/pull/11002,5,"['deploy', 'update']","['deploy', 'deploy-config', 'updated']"
Deployability,I made the timeout for the standing workers to be 5 minutes for the test scope and 2 hours for the deploy scope.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8850#issuecomment-632344001:99,deploy,deploy,99,https://hail.is,https://github.com/hail-is/hail/pull/8850#issuecomment-632344001,1,['deploy'],['deploy']
Deployability,"I made these changes and a few others in LinearRegressionCommand (flatMap instead of Array.concat), and updates the docs to reflect removal of --output. Remerged with master and pushed. Back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/292#issuecomment-212196570:104,update,updates,104,https://hail.is,https://github.com/hail-is/hail/pull/292#issuecomment-212196570,1,['update'],['updates']
Deployability,"I made these updates to Scala LocalMatrix as I was building the Python interface, to more closely mirror NumPy functions and name the symbolic operators. I no longer intend to expose LocalMatrix in Python in its current form, but rather to localize BlockMatrix ""directly"" to NumPy and vice versa. Still, LocalMatrix in Scala is a useful local model for how I'll update BlockMatrix to be more NumPy like (e.g. broadcasting), and a step toward building a region-value based ndarray. I think these are good changes, isolated to LocalMatrix, so my preference is to merge them in now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3071:13,update,updates,13,https://hail.is,https://github.com/hail-is/hail/pull/3071,2,['update'],"['update', 'updates']"
Deployability,"I manually changed the Cloud SQL automated backups storage from the `us` multi-region to `us-central1`. There's no reason to store it in a multi-region and it's more expensive. What I didn't realize is that this configuration is actually owned by the terraform that we have managing the lifecycle of the database and other infra, so we need to update the terraform to reflect the desired (and current) state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14568:212,configurat,configuration,212,https://hail.is,https://github.com/hail-is/hail/pull/14568,2,"['configurat', 'update']","['configuration', 'update']"
Deployability,I mean I can close it since we re-updated the same function. It had just been a previous task of the day.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11529#issuecomment-1065511689:34,update,updated,34,https://hail.is,https://github.com/hail-is/hail/pull/11529#issuecomment-1065511689,1,['update'],['updated']
Deployability,"I misunderstood the issue originally. The exit status was set *in the sub-shell*, so; it did not affect the parent shell's environment. Instead, I run the command in; a sub-shell and update the variable in the parent shell. I also had to fix the issues that arose while the check wasn't honored.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9377:183,update,update,183,https://hail.is,https://github.com/hail-is/hail/pull/9377,1,['update'],['update']
Deployability,"I need this extra debugging information to understand what is going on in Azure with deleted VMs still showing up in the portal with ResourceNotFound errors. Miah and Greg are running into this same problem in their deployment. My guess is what is happening is the worker is active and working fine, but then the deployment gets ""Canceled"" because the OMSAgent takes too long to deploy. So our loop then cancels the deployment which messes up the state in Azure of the already deployed and running VM. I popped the parameters from the deployment result in case it contains sensitive data (I'm mainly worried about any private SSH keys).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13231:216,deploy,deployment,216,https://hail.is,https://github.com/hail-is/hail/pull/13231,6,['deploy'],"['deploy', 'deployed', 'deployment']"
Deployability,I need to fix the pipeline pylint errors in this branch as well...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5616#issuecomment-473565743:18,pipeline,pipeline,18,https://hail.is,https://github.com/hail-is/hail/pull/5616#issuecomment-473565743,1,['pipeline'],['pipeline']
Deployability,I need to make sure this all works when ci is deploying in a test namespace.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7101:46,deploy,deploying,46,https://hail.is,https://github.com/hail-is/hail/pull/7101,1,['deploy'],['deploying']
Deployability,I need to rethink the test-tiny-limit and test-zero-limit. This is going to fail every time after the first merge on the deployment.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9354#issuecomment-705066148:121,deploy,deployment,121,https://hail.is,https://github.com/hail-is/hail/pull/9354#issuecomment-705066148,1,['deploy'],['deployment']
Deployability,I need to split this up and think about how we're going to release the change.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9598#issuecomment-718269756:59,release,release,59,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-718269756,1,['release'],['release']
Deployability,I need to test this with dev deploy and make sure it actually works. But would appreciate feedback on the design before I start doing that.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8445:29,deploy,deploy,29,https://hail.is,https://github.com/hail-is/hail/pull/8445,1,['deploy'],['deploy']
Deployability,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7523:23,integrat,integration,23,https://hail.is,https://github.com/hail-is/hail/pull/7523,4,"['Update', 'integrat']","['Update', 'integration']"
Deployability,"I needed this to deploy 0.2.21. `make deploy` relies on the docs having been deployed (by CI) but there's no guarantee that happens -- we went a week without any successful deploys of master commits, for instance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6991#issuecomment-528461237:17,deploy,deploy,17,https://hail.is,https://github.com/hail-is/hail/pull/6991#issuecomment-528461237,4,['deploy'],"['deploy', 'deployed', 'deploys']"
Deployability,I needed this when running PR tests. It is also useful for dev deploys. I also fixed a minor bug in install-editable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6805:63,deploy,deploys,63,https://hail.is,https://github.com/hail-is/hail/pull/6805,2,"['deploy', 'install']","['deploys', 'install-editable']"
Deployability,"I never tested that PR that got merged (whoops!) and CI tests are insufficient; to catch this case (we should beef those up, asana task added). The issue was that I thought the method to issue an HTTP get request was `get`,; but it was `getitem`. This PR fixes that. This error occured during `update` and; thus prevented all forward progress of CI.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8402:294,update,update,294,https://hail.is,https://github.com/hail-is/hail/pull/8402,1,['update'],['update']
Deployability,I noticed in #12526 that you added `scope` to the website Makefile but I don't think it's needed as using `deploy` is a little cleaner.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12558:107,deploy,deploy,107,https://hail.is,https://github.com/hail-is/hail/pull/12558,1,['deploy'],['deploy']
Deployability,"I noticed that jobs in test deployments were deadlocking because we weren't spinning up extra instances (compared to the production version of Batch). Although each job could fit on an open instance, its allocated share is still less than the core request for that job. This PR aims to increase the probability in which we ignore an exceed shares error the more we have these errors such that at a certain point the rate will be 100% and we'll be able to continue scheduling.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9464:28,deploy,deployments,28,https://hail.is,https://github.com/hail-is/hail/pull/9464,1,['deploy'],['deployments']
Deployability,"I noticed that the gnomAD mitochondria datasets were pointing at the `gs://gnomad-public-requester-pays` bucket. The Google Cloud Public Datasets version should be up to date now. Also updated the documented schema for chrM sites. Some information about those changes is available in the gnomAD change log: https://gnomad.broadinstitute.org/news/2021-08-rename-filter-in-mitochondria-dataset-and-minor-format-changes/. And finally, since these were the only two datasets that reference `gnomad-public-requester-pays`, removed `gnomad-public-requester-pays` from the list of annotation DB buckets used by `hailctl dataproc`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11282:185,update,updated,185,https://hail.is,https://github.com/hail-is/hail/pull/11282,1,['update'],['updated']
Deployability,I now unconditionally update. The update system will either find another PR that was started in the meantime or it will start a new batch,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8398#issuecomment-609908738:22,update,update,22,https://hail.is,https://github.com/hail-is/hail/pull/8398#issuecomment-609908738,2,['update'],['update']
Deployability,"I observed a cluster with it set to the default idle time of 30 seconds in Azure and the workers were continuously thrashing leading up to 49 instances being created over the course of a PR. With an idle time of 120 seconds, there was no thrashing and 28 instances were created over the course of the PR (16 standard + job private etc.). The cluster nicely scaled down at the end of the PR. It looked like a couple of times the `standard-np` pool scaled up and then scaled down so I assume the `standard` pool wasn't at full capacity while that was happening. It might be worth configuring the `standard-np` pool to be 4 or 5 standing instances with 16 cores and see what happens -- that might help as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13314:102,continuous,continuously,102,https://hail.is,https://github.com/hail-is/hail/pull/13314,1,['continuous'],['continuously']
Deployability,I only update to 3.2.11 since that's what listed on the Spark maven page:. https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.11/2.2.0. When we update to Spark 2.4 (soon) we should update further. May address #5744,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5751:7,update,update,7,https://hail.is,https://github.com/hail-is/hail/pull/5751,3,['update'],['update']
Deployability,I only use install-benchmark,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9567#issuecomment-704535423:11,install,install-benchmark,11,https://hail.is,https://github.com/hail-is/hail/pull/9567#issuecomment-704535423,1,['install'],['install-benchmark']
Deployability,"I originally added a `build.yaml` step to this that ran the script on every deploy, but I think there are some subtleties in there around releases that are best discussed in follow-up PR and may distract from some immediate goals around getting AR up and working. This will help me easily get the images we need to move over into AR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12204#issuecomment-1251646501:76,deploy,deploy,76,https://hail.is,https://github.com/hail-is/hail/pull/12204#issuecomment-1251646501,2,"['deploy', 'release']","['deploy', 'releases']"
Deployability,"I picked the name since Cronus is the father of Zeus. Perhaps Saturn is more appropriate. Open to suggestions here. The UX flow:. 1. User loads up `https://hail.is/cronus` and sees a form with a button.; 2. Pressing the button starts a pod running Jupyter for the user that no one else has access to; 3. refreshing the page or going to `https://hail.is/cronus` again redirects to the Jupiter instance; 4. to get a fresh Jupyter instance, the user can clear their cookies. The components:. - a flask app (`cronus/cronus.py`) which launches pods and handles authentication (via cookies); - an nginx reverse proxy which uses `auth_request` to check the permissions with the flask app; - a pod running `Jupyter notebook` with hail `pip` installed. TODO:. - [x] add make targets to generate the `cronus-job` image (the jupyter notebook image); - [ ] maybe simplify the directives used in nginx? I kept throwing shit at it until it worked; - [ ] figure out how to teach flask url_for to use a root other than `/`. I don't know what HTTP proxy headers to set to inform it that it lives at a subdirectory of `hail.is`; - [ ] get rid of the button? creating a new pod needs to be a `POST` so that the web browser doesn't access twice or eagerly access it, etc. maybe I can use javascript on the root page to make the post request and redirect the page.; - [ ] testing? I could add some basic things, but the most time consuming and annoying thing was getting the reverse proxy settings right and testing that requires an nginx instance. @cseed I randomly assigned, should I be picking from you and Tim? What's the plan for review on these new things?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576:733,install,installed,733,https://hail.is,https://github.com/hail-is/hail/pull/4576,1,['install'],['installed']
Deployability,"I ported pipeline to new batch, and in doing so I ripped out `copy_service_account_name` and just replaced its uses with `user`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5605#issuecomment-473422952:9,pipeline,pipeline,9,https://hail.is,https://github.com/hail-is/hail/pull/5605#issuecomment-473422952,1,['pipeline'],['pipeline']
Deployability,I propose saving the configuration question to a different PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9769#issuecomment-738084194:21,configurat,configuration,21,https://hail.is,https://github.com/hail-is/hail/pull/9769#issuecomment-738084194,1,['configurat'],['configuration']
Deployability,I pushed another commit with Sphinx docs for HailContext. They can be generated by running:. ```; /path/to/hail/python/pyhail/docs $ PYTHONPATH=/path/to/spark-1.6.2-bin-hadoop2.6/python:/path/to/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip make html; ```. Then the docs will be found in `_build/html`. I don't yet know how to install pyspark through gradle.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1061#issuecomment-258503049:335,install,install,335,https://hail.is,https://github.com/hail-is/hail/pull/1061#issuecomment-258503049,1,['install'],['install']
Deployability,"I pushed another commit. Toplevel hail-ci-build.sh handles all known projects (and skips them if they aren't there yet or don't have a build script). Also, deploy batch only when changed. I will also make this generic by adding a get-deployed-sha.sh to retrieve the current deployed sha to compare against when considering redeploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4414#issuecomment-424099229:156,deploy,deploy,156,https://hail.is,https://github.com/hail-is/hail/pull/4414#issuecomment-424099229,3,['deploy'],"['deploy', 'deployed', 'deployed-sha']"
Deployability,I put the WIP tag back on until I've fully debugged and fixed the azure deployment.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12724#issuecomment-1454085562:72,deploy,deployment,72,https://hail.is,https://github.com/hail-is/hail/pull/12724#issuecomment-1454085562,1,['deploy'],['deployment']
Deployability,I put the time_updated field in. I'll test it with dev deploy once everything else is ready to go.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12199#issuecomment-1252827587:55,deploy,deploy,55,https://hail.is,https://github.com/hail-is/hail/pull/12199#issuecomment-1252827587,1,['deploy'],['deploy']
Deployability,I ran into this today...spent a bit of time debugging and was able to ssh to one of the workers and poke around the `docker` logs. The issue appears to be some kind of race between the `docker` install that the `VEP` initialization script does and the limited number of retries by `systemd` to get the `docker` daemon up and running. . Adding `sudo service docker restart` at the end of the the `VEP` initialization bash script worked as a short term fix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1589956412:194,install,install,194,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1589956412,1,['install'],['install']
Deployability,"I ran the select ... from (select ...) part of the UPDATE which fixes; this issue. It completed in ~72 seconds, so I think this change is actually; plenty fast to run.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11278:51,UPDATE,UPDATE,51,https://hail.is,https://github.com/hail-is/hail/pull/11278,1,['UPDATE'],['UPDATE']
Deployability,"I realize I forgot to update the latest-hash lines, I'll fix that now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4220#issuecomment-416617372:22,update,update,22,https://hail.is,https://github.com/hail-is/hail/pull/4220#issuecomment-416617372,1,['update'],['update']
Deployability,"I really borked our SQL linting. This PR is short but it catches a few critical problems. 1. The point of `check-sql.sh` is to detect modifications or deletions of SQL files in PRs and fail if such a change occurs. Currently on `main` it does not detect modifications. In #13456, I removed the `delete-<service>-tables.sql` files (intentionally), so added the `^D` to the `grep` regex to indicate that it is OK to have a deletion. What I inadvertently did though is change the rule from ""It's ok to have Additions of any file OR Modifications of estimated-current.sql / delete-<service>-tables.sql"" to ""It's ok to have Additions OR Modifications OR Deletions of estimated-current.sql / delete-<service>-tables.sql"". Really this should have been ""It's ok to have Additions OR Modifications of estimated-current.sql OR Deletions of delete-<service>-tables.sql"". I've changed it to reflect that rule. 2. Rules currently silently *pass* in CI with an error message that git is not installed. In #13437 I changed the image used to run the linters and inadvertently didn't include `git` which `check-sql.sh` needs to run. Here's how it failed in a sneaky way:; - Since `git` is not installed, all calls to `git` fail, but the script is not run with `set -e` so every line of the script is executed; - Since `git` lines fail, `modified_sql_file_list` remains empty; - Since `modified_sql_file_list` remains empty, it appears to the check at the end that everything checked out; - The if statement runs successfully and the script returns with error code 0. To fix this I do a few things:; - installed `git` in the linting image; - `set -e` by default and only enable `set +e` later on when necessary (because we don't want a failed `git diff` to immediately exit); - Do away with the file checking and instead check the error code of the grep. If nothing survives the grep filter, which means there were no illegal changes made, grep will return with exit code 1. So we treat that exit code as a success.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13745:977,install,installed,977,https://hail.is,https://github.com/hail-is/hail/pull/13745,3,['install'],['installed']
Deployability,"I reassigned you to #6121 -- it shouldn't need much review because it's a copy of cloudtools. Then we can PR this onto that, and I'll get to hacking on the deployment strategy discussed over email yesterday.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6118#issuecomment-493507016:156,deploy,deployment,156,https://hail.is,https://github.com/hail-is/hail/pull/6118#issuecomment-493507016,1,['deploy'],['deployment']
Deployability,"I removed the extra ``bin`` - when I run hc=HailContext(), I get the following: . ``>>> hc = HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-422>"", line 2, in __init__; File ""/Users/ih/languages/hail.is/hail/python/hail/typecheck/check.py"", line 226, in _typecheck; return f(*args, **kwargs); File ""/Users/ih/languages/hail.is/hail/python/hail/context.py"", line 83, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); TypeError: 'JavaPackage' object is not callable``. I am realizing that pip still installs pyspark 2.2.0 - is this the issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319706791:586,install,installs,586,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319706791,1,['install'],['installs']
Deployability,I removed the spaces after sys_platform which allowed my installation from source to work.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12002:57,install,installation,57,https://hail.is,https://github.com/hail-is/hail/pull/12002,1,['install'],['installation']
Deployability,I renamed a Makefile target but didn't update the deploy script.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4745#issuecomment-436859081:39,update,update,39,https://hail.is,https://github.com/hail-is/hail/pull/4745#issuecomment-436859081,2,"['deploy', 'update']","['deploy', 'update']"
Deployability,"I replaced the maven source ; maven { url ""https://repo.hortonworks.com/content/repositories/releases/"" }. by; maven { url ""https://nexus-private.hortonworks.com/nexus/repository/maven-central/"" }; maven { url ""https://nexus-private.hortonworks.com/nexus/service/rest/repository/browse/maven-central/"" }. then I can compile again:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9419#issuecomment-688740276:93,release,releases,93,https://hail.is,https://github.com/hail-is/hail/issues/9419#issuecomment-688740276,1,['release'],['releases']
Deployability,"I reran the benchmarks with nothing else running on my laptop. I think this should work right?. ```; git checkout pc-relate && \; git status && \; python3 -m benchmark_hail run -t pc_relate,pc_relate_big && \; git checkout master && \; (cd ../../hail && make install) && \; git status && \; git checkout pc-relate && \; python3 -m benchmark_hail run -t pc_relate,pc_relate_big; ```; Benchmark should import the installed hail. pc-relate branch:; ```; 2020-01-24 18:38:08,147: INFO: burn in: 30.09s; 2020-01-24 18:38:35,904: INFO: run 1: 27.75s; 2020-01-24 18:39:03,001: INFO: run 2: 27.09s; 2020-01-24 18:39:29,144: INFO: run 3: 26.14s; ```; master:; ```; 2020-01-24 18:41:08,254: INFO: burn in: 32.71s; 2020-01-24 18:41:37,239: INFO: run 1: 28.98s; 2020-01-24 18:42:05,393: INFO: run 2: 28.15s; 2020-01-24 18:42:33,411: INFO: run 3: 28.01s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7962#issuecomment-578344916:259,install,install,259,https://hail.is,https://github.com/hail-is/hail/pull/7962#issuecomment-578344916,2,['install'],"['install', 'installed']"
Deployability,I saw this happen to the job that was supposed to be running the deploy-svc PR.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4608:65,deploy,deploy-svc,65,https://hail.is,https://github.com/hail-is/hail/issues/4608,1,['deploy'],['deploy-svc']
Deployability,I second this one. Had to install the `parsimonious` module which was missing from my Anaconda Python.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2978#issuecomment-370497953:26,install,install,26,https://hail.is,https://github.com/hail-is/hail/issues/2978#issuecomment-370497953,1,['install'],['install']
Deployability,I see the docs for the PyHail API but is there a getting started guide available yet? Also are there any plans to make a PyHail package available for installation through PyPI?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218:150,install,installation,150,https://hail.is,https://github.com/hail-is/hail/issues/1218,1,['install'],['installation']
Deployability,"I see what is happening. . The Hail cluster install instructions specify the following for a spark cluster:. export PYSPARK_SUBMIT_ARGS=""\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; pyspark-shell"". On our cluster, this will run as a local job. It needs a ""--master yarn"" for an argument. Running it locally probably is related to the out of memory error and the limited cores. I will rerun this with the --master yarn argument. . Regarding the bgen file versus matrix table, are you suggesting, it would be faster to run an analysis such as a logistic regression starting with the bgen file instead of the imported bgen mt file. The phenotypes would need to annotated the imported bgen mt every time. Just trying to understand the trade offs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780#issuecomment-439414395:44,install,install,44,https://hail.is,https://github.com/hail-is/hail/issues/4780#issuecomment-439414395,1,['install'],['install']
Deployability,"I see, so our proposed fix is this? https://github.com/src-d/jgscm/compare/master...danking:patch-1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480383944:92,patch,patch-,92,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480383944,1,['patch'],['patch-']
Deployability,I semi-tested this as follows: deployed in auth and created a user. The logic then failed with 403 Forbidden because the dev namespace gsa can't create service accounts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8798#issuecomment-628891104:31,deploy,deployed,31,https://hail.is,https://github.com/hail-is/hail/pull/8798#issuecomment-628891104,1,['deploy'],['deployed']
Deployability,"I separated setting the email from enabling pipeline uploads, so people can set the email once and use hl.upload_log() without arguments independent of pipeline upload.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4543#issuecomment-429567973:44,pipeline,pipeline,44,https://hail.is,https://github.com/hail-is/hail/pull/4543#issuecomment-429567973,2,['pipeline'],['pipeline']
Deployability,"I should have checked quay.io for 1.12.0, which doesn't exist: https://quay.io/repository/skopeo/stable?tab=tags&tag=v1.12.0. Even though 1.12.0 was released two weeks ago https://github.com/containers/skopeo/releases",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12938:149,release,released,149,https://hail.is,https://github.com/hail-is/hail/pull/12938,2,['release'],"['released', 'releases']"
Deployability,"I still dislike job_group_tree because trees are usually represented in terms of their ancestor or parent-child relationships, neither of which are what this is. That said, I don't think we should block this PR on a naming quibble. We can do renames separately. We should update the RFC to be clear about the unique identifiers of the three things groups, jobs, and batches. (In particular, jobs are uniquely identifier by batch id and job id, group id is not part of it).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13475#issuecomment-1760376350:272,update,update,272,https://hail.is,https://github.com/hail-is/hail/pull/13475#issuecomment-1760376350,2,['update'],['update']
Deployability,I still haven't managed to set it up to work from that. I hit errors while installing the Genesis stuff,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3281#issuecomment-379045312:75,install,installing,75,https://hail.is,https://github.com/hail-is/hail/pull/3281#issuecomment-379045312,1,['install'],['installing']
Deployability,"I still need to figure out where to move the pipeline tests to and whether to rename the BatchBackend to BatchServiceBackend. Making a PR now so I can see if there are any other bugs. Also, I didn't rename where the docs path is in the header navbar yet. This needs to be done when we release a new Hail release after this PR goes in. @johnc1231 @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8453:45,pipeline,pipeline,45,https://hail.is,https://github.com/hail-is/hail/pull/8453,3,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"I still need to test this with dev deploy, but at least it's on the radar again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12848#issuecomment-1680688341:35,deploy,deploy,35,https://hail.is,https://github.com/hail-is/hail/pull/12848#issuecomment-1680688341,1,['deploy'],['deploy']
Deployability,I still need to test this works with dev deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11118:41,deploy,deploy,41,https://hail.is,https://github.com/hail-is/hail/pull/11118,1,['deploy'],['deploy']
Deployability,"I suppose we have lost to stand up a working CI (b/c it must talk to batcH) without a working router. Or, really, we've lost the ability to do that without copying the service definition from the router's list. I'm OK with this. I agree `make deploy` not also setting up the service definition is out of sync with our previous behavior. I could engineer some system to have one definition but two uses of the `Service` definition but it just doesn't feel important enough to justify it. We rarely if ever deploy the services by hand without deploying the router first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8504#issuecomment-611711990:243,deploy,deploy,243,https://hail.is,https://github.com/hail-is/hail/pull/8504#issuecomment-611711990,3,['deploy'],"['deploy', 'deploying']"
Deployability,I suspect these will also cause deploy problems if left unspecified.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4619:32,deploy,deploy,32,https://hail.is,https://github.com/hail-is/hail/pull/4619,1,['deploy'],['deploy']
Deployability,"I swapped out the read/write for a serialize/deserialize thing; got some times for a pipeline that was just read -> densify -> write on the same matrix table and got the following times:. old: [19.249044491007226, 20.367718594003236, 18.515784285002155, 18.116745114995865, 18.104985954996664]; new: [15.649361574003706, 16.205813756998396, 17.644299295003293, 16.90835837100167, 17.12396009400254]. I'd be interested in trying this on a wider matrix table, since I think that's when we start running into issues.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6580#issuecomment-509490837:85,pipeline,pipeline,85,https://hail.is,https://github.com/hail-is/hail/pull/6580#issuecomment-509490837,1,['pipeline'],['pipeline']
Deployability,"I tested `wget http://batch/jobs` from within the cluster...that works fine. However, I do see something potentially relevant in the output, pertaining to it seems a CI deployment attempt of batch (I don't see a timestamp, but this is the last job run). ```; deployment.yaml\nkubectl delete persistentvolumeclaim --all --namespace test\nError from server (Forbidden): persistentvolumeclaims is forbidden: User \""system:serviceaccount:batch-pods:deploy-svc\"" cannot list persistentvolumeclaims in the namespace \""test\""\nMakefile:70: recipe for target 'deploy' failed\nmake: *** [deploy] Error 1\n""},""state"":""Complete""}]; ```. Namely `system:serviceaccount:batch-pods:deploy-svc cannot list persistentvolumeclaims in the namespace \""test\`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751:169,deploy,deployment,169,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751,6,['deploy'],"['deploy', 'deploy-svc', 'deployment']"
Deployability,"I tested deploy locally, and things worked great.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6136#issuecomment-494081311:9,deploy,deploy,9,https://hail.is,https://github.com/hail-is/hail/pull/6136#issuecomment-494081311,1,['deploy'],['deploy']
Deployability,I tested this a bit manually. I'm pretty confident its right but I'm sure we'll hit some snag on the next deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11916:106,deploy,deploy,106,https://hail.is,https://github.com/hail-is/hail/pull/11916,1,['deploy'],['deploy']
Deployability,I tested this by applying it to the cluster directly (I ran make deploy in the `gateway/` project). It will continue to run until a master deploy blows it away.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4645#issuecomment-433180843:65,deploy,deploy,65,https://hail.is,https://github.com/hail-is/hail/pull/4645#issuecomment-433180843,2,['deploy'],['deploy']
Deployability,I tested this by applying it to the cluster directly (I ran make deploy in the gateway/ project). It will continue to run until a master deploy blows it away.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4648#issuecomment-433191315:65,deploy,deploy,65,https://hail.is,https://github.com/hail-is/hail/pull/4648#issuecomment-433191315,2,['deploy'],['deploy']
Deployability,"I tested this by hand with the following schenario:. 1. Deployed this branch and set the standard pool to use 4-core workers. Then submitted two jobs, with a total of 3 cores requested between them. The state looked as follows after a worker spun up:; <img width=""643"" alt=""Screenshot 2023-05-15 at 5 16 30 PM"" src=""https://github.com/hail-is/hail/assets/24440116/28d94a41-ff37-4331-8f4f-c2e649fd1c62"">. 2. I then bumped the instance version and redeployed. Then submitted a 1 core job. This should be able to fit on the existing worker, but batch instead spins up a new worker because the instance version has changed to 25 and the existing worker is version 24.; <img width=""639"" alt=""Screenshot 2023-05-15 at 5 19 41 PM"" src=""https://github.com/hail-is/hail/assets/24440116/92460dae-6aee-40b6-b6f3-46363dbdb12a"">. <img width=""643"" alt=""Screenshot 2023-05-15 at 5 20 59 PM"" src=""https://github.com/hail-is/hail/assets/24440116/5256762f-b361-40a7-a6d2-1aaa61f7b4ae"">. 3. The following steps I consider additional verification. I submitted another job, and observed it falling on the new worker instead of the most utilized worker. <img width=""641"" alt=""Screenshot 2023-05-15 at 5 21 36 PM"" src=""https://github.com/hail-is/hail/assets/24440116/176a9055-6ba7-49f1-88dc-c8496147db07"">. 4. I then cancelled the first batch and submitted another job and observed it fell on the new worker instead of the least utilized worker. So this step and the previous step confirm that regardless of utilization the old worker is not considered. We also see that the old worker receives no more work and successfully deactivates.; <img width=""644"" alt=""Screenshot 2023-05-15 at 5 23 03 PM"" src=""https://github.com/hail-is/hail/assets/24440116/d671a947-da44-4f05-bcdb-2fe6df28a655"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13055#issuecomment-1548633947:56,Deploy,Deployed,56,https://hail.is,https://github.com/hail-is/hail/pull/13055#issuecomment-1548633947,1,['Deploy'],['Deployed']
Deployability,"I tested this by running `make deploy` in the `image-fetcher/` directory. You can take a look at the pods with `kubectl`, they are in the default namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4647#issuecomment-433188993:31,deploy,deploy,31,https://hail.is,https://github.com/hail-is/hail/pull/4647#issuecomment-433188993,1,['deploy'],['deploy']
Deployability,I tested this with dev deploy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7907:23,deploy,deploy,23,https://hail.is,https://github.com/hail-is/hail/pull/7907,1,['deploy'],['deploy']
Deployability,I tested this with dev deploy using both my broad developer account and my personal account to make sure regular users only saw their own billing information.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10656:23,deploy,deploy,23,https://hail.is,https://github.com/hail-is/hail/pull/10656,1,['deploy'],['deploy']
Deployability,I tested this works with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8760#issuecomment-626929606:29,deploy,deploy,29,https://hail.is,https://github.com/hail-is/hail/pull/8760#issuecomment-626929606,1,['deploy'],['deploy']
Deployability,"I tested with dev deploy, this looks good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8002#issuecomment-580248649:18,deploy,deploy,18,https://hail.is,https://github.com/hail-is/hail/pull/8002#issuecomment-580248649,1,['deploy'],['deploy']
Deployability,I think I addressed all the comments. I also noticed some get-deployed-sha.sh scripts were broken (name of deployment was wrong) and I fixed them.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4509#issuecomment-428393236:62,deploy,deployed-sha,62,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-428393236,2,['deploy'],"['deployed-sha', 'deployment']"
Deployability,I think I addressed most of the comments. I haven't tested the new code -- I don't want to do that until we're happy with it. I don't know that I like how this is turning out. I think we're conflating what `hailctl config init` should be which is intitializing an environment configuration file versus a quick start to using Hail Batch and QoB. My intention for this feature was to idiot proof the latter especially for the ATGU workshop and make it as few commands as possible. I worry that needing to run `hailctl auth login` before this is not a just run this single command and then you can get going with little effort. I don't think `hailctl config init` is the right place for what I have written. Maybe it should be `hailctl batch quick-start`???,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1662716382:276,configurat,configuration,276,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1662716382,1,['configurat'],['configuration']
Deployability,"I think I addressed the changes in the client, but I'm sure it's not 100% correct yet. Will deal with any bugs later. Can we move on to the implementation? I'm thinking the best place to start is `_create_jobs` and then the SQL implementation of what it means to create and utilize the new updates infrastructure, but happy to do what's easiest for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1219720049:290,update,updates,290,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219720049,1,['update'],['updates']
Deployability,I think I got all of the fixes. I tested the UI with dev deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8916:57,deploy,deploy,57,https://hail.is,https://github.com/hail-is/hail/pull/8916,1,['deploy'],['deploy']
Deployability,"I think I got everything, but I still want to test it again by hand in the morning once we're sure there's no other changes to make. I was testing the before migration by running dev deploy from master and cancelling a batch while it was submitting along with some completed ones and then tried to deploy the new version and made sure the ready cores etc. were now correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7851#issuecomment-574442792:183,deploy,deploy,183,https://hail.is,https://github.com/hail-is/hail/pull/7851#issuecomment-574442792,2,['deploy'],['deploy']
Deployability,I think I have some quibbles on dropping the granularity of tracking cpus by zone and the terminology but I'm going to try deploying this in my project and see that we get the behavior we want.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12078#issuecomment-1206702152:123,deploy,deploying,123,https://hail.is,https://github.com/hail-is/hail/pull/12078#issuecomment-1206702152,1,['deploy'],['deploying']
Deployability,"I think I know the problem! We are testing against Plink 1.9, but you have the old version 1.07 (which is my fault for linking the plink base page). Install it from the link below and please try again:. [https://www.cog-genomics.org/plink2](https://www.cog-genomics.org/plink2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-230288836:149,Install,Install,149,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230288836,1,['Install'],['Install']
Deployability,"I think I should return the google service account email rather than the name. I propose that I return instead of:. ```; 'gsa_name': 'projects/hail-vdc/serviceAccounts/user-f2khk67pq8a9pc38wnbjigarg@hail-vdc.iam.gserviceaccount.com',; ```. this:. ```; 'gsa_email': 'user-f2khk67pq8a9pc38wnbjigarg@hail-vdc.iam.gserviceaccount.com',; ```. This is because, although google service account management occurs by the `name`, acl operations appear to use the `email` or `uniqueId<int>`. Also, it appears that we don't need to specify the projectId to take operations on the [google service account](https://cloud.google.com/iam/docs/creating-managing-service-accounts#deleting_a_service_account). For reference, the google service account creation response. ```; {'name': 'projects/hail-vdc/serviceAccounts/user-<some_ascii>@hail-vdc.iam.gserviceaccount.com', 'projectId': 'hail-vdc', 'uniqueId': '<some_int_id>', 'email': 'user-<some_ascii>@hail-vdc.iam.gserviceaccount.com', 'displayName': 'user', 'etag': 'MDEwMjE5MjA=', 'oauth2ClientId': '<some_int_id>'}; ```. I will update this PR to return gsa_email (and optionally gsa_projectId, although omitting for now because we have only 1, and may always have only 1).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5633#issuecomment-474443321:1066,update,update,1066,https://hail.is,https://github.com/hail-is/hail/pull/5633#issuecomment-474443321,1,['update'],['update']
Deployability,"I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488:143,update,updates,143,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488,4,['update'],"['update', 'updates']"
Deployability,"I think it could be enough to `make deploy` prometheus in your own namespace, kick off a couple PR builds if there aren't any, and check that your prometheus doesn't contain any of those entries. You can run queries through the prometheus UI.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10732#issuecomment-892827072:36,deploy,deploy,36,https://hail.is,https://github.com/hail-is/hail/pull/10732#issuecomment-892827072,1,['deploy'],['deploy']
Deployability,"I think it will actually just work out of the box. In particular, the updated file paths that specify the full destination also work with gsutil. The build.yaml changes are being run by the production batch, so we already know those are working with gsutil. I'm running some benchmarks vs gsutil now and will have numbers in a while. If that all looks good, I vote to merge this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10131#issuecomment-790877506:70,update,updated,70,https://hail.is,https://github.com/hail-is/hail/pull/10131#issuecomment-790877506,1,['update'],['updated']
Deployability,I think it's safe. I was more worried about what the namespace should be and whether for developers it should have the service_namespace. I think the deploy config only tells you how to get the correct URL. There's nothing special about it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9437#issuecomment-691241075:150,deploy,deploy,150,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691241075,1,['deploy'],['deploy']
Deployability,"I think one of the following needs to happen:; 1. we document the pc relate setup sufficiently; 2. we precompute results somewhere that PC-Relate runs and test against that. I feel strongly that any PRs that introduce new testing dependencies must also include the relevant information to install those dependencies, probably in the ""getting started developing"" doc or somewhere like that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3274#issuecomment-377932960:289,install,install,289,https://hail.is,https://github.com/hail-is/hail/pull/3274#issuecomment-377932960,1,['install'],['install']
Deployability,"I think tests of services are failing during deployment because wait-for Service (which probes /healthcheck) hits the old service, then the service goes down during (re)deployment. Here is an example test failure: first few tests pass then the rest fail due to connection timeout: https://ci2.hail.is/jobs/1413/log. This doesn't quite make sense, because batch and apiserver both have readiness checks, so the rollout should be have now downtime (although some of the tests could hit the old service which could fail if there were differences). I think this is a good chance but I'm not totally confident. Interested in your thoughts. Also, I can't seem the find the different between `wait deployment` and `rollout status`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6011:45,deploy,deployment,45,https://hail.is,https://github.com/hail-is/hail/pull/6011,5,"['deploy', 'rollout']","['deployment', 'rollout']"
Deployability,"I think that's the fix, actually. Just hasn't been deployed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9628#issuecomment-714496429:51,deploy,deployed,51,https://hail.is,https://github.com/hail-is/hail/issues/9628#issuecomment-714496429,1,['deploy'],['deployed']
Deployability,I think the gnomAD group would probably prefer Hail point to the `gcp-public-data--gnomad` version of this dataset once that is updated. @gtiao,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10169#issuecomment-795555847:128,update,updated,128,https://hail.is,https://github.com/hail-is/hail/pull/10169#issuecomment-795555847,1,['update'],['updated']
Deployability,"I think the issue is that I used `pip3` to install `hail`, following the docs. Unfortunately, `conda` only installs `pip` into the `hail` environment and doesn't create the `pip3` alias, so I was using my system-wide `pip3` to install `hail`. When I use `pip` to install `hail` within the `hail` conda environment, everything works fine. Maybe just update the docs to use `pip` instead of `pip3`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-533266332:43,install,install,43,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533266332,5,"['install', 'update']","['install', 'installs', 'update']"
Deployability,"I think the proposed new default and the option to change it is much more intuitive than the current behavior and worth a change. Though, I think it would be most polite to announce it on zulip/email list a week or two in advance of the release (which you may already planned to do).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11884#issuecomment-1145290949:237,release,release,237,https://hail.is,https://github.com/hail-is/hail/pull/11884#issuecomment-1145290949,2,['release'],['release']
Deployability,I think this PR still has good changes. We should avoid nesting when possible; I fear it leads to confusing situations. I'm gonna take it off the release 0.2.125 checklist though,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13677#issuecomment-1779897341:146,release,release,146,https://hail.is,https://github.com/hail-is/hail/pull/13677#issuecomment-1779897341,1,['release'],['release']
Deployability,"I think this can go in instead of #8730. I ran dev deploy with master and then didn't delete the database and ran the tests with the new version. The billing UI page reported the correct values. I also ran the new version with the check functions in the background and got no errors. I can probably double check the UI batches cost are correct, but let's wait until we're happy with the code before I do anymore testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8759:51,deploy,deploy,51,https://hail.is,https://github.com/hail-is/hail/pull/8759,1,['deploy'],['deploy']
Deployability,"I think this covers most spots. I updated the python_requires, left the docker images, tried to update batch docs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11219#issuecomment-1011493435:34,update,updated,34,https://hail.is,https://github.com/hail-is/hail/pull/11219#issuecomment-1011493435,2,['update'],"['update', 'updated']"
Deployability,I think this fixes deploy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4587:19,deploy,deploy,19,https://hail.is,https://github.com/hail-is/hail/pull/4587,1,['deploy'],['deploy']
Deployability,"I think this is a known scheduler bug in Spark 1.5, where cancelled executors are incorrectly counted as failed. This will be fixed by an upgrade that will be installed this week. As a temporary fix, I increased the failed job retry count to 30. You hit this, although I don't see any genuine errors in your job. This is exasperated by jobs where each partition takes a long time to run. You can make the partition size smaller by increasing the number of partitions. I suggest you try it again with `-n 1000`. I increased the retry count in `hail-new-vep` to 50.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/302#issuecomment-210903100:138,upgrade,upgrade,138,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-210903100,2,"['install', 'upgrade']","['installed', 'upgrade']"
Deployability,"I think this is finally at a place where it's not a whole lot slower than the current version. I timed this with two changes that are not yet in master: #4069, and a change that adds a shuffle flag to MatrixMapRows (to shuffle instead of coerce if we're changing the key). I haven't broken that out as a separate change yet because I'm not super sure about it. I've put some timings below (`profile` and `profile75` are left aligned; `chr22` and `profile225` contain moving variants). ```; profile | new, no shuffle | [ 38.77559779, 19.34779241, 30.00672880]; profile | old | [ 20.66201923, 22.53258884, 19.24919411]. profile75 | old | [ 57.11803599, 55.57265945, 55.78256949]; profile75 | new, no shuffle | [ 84.90397896, 47.73250838, 48.00151558]; profile75 | new, shuffle | [102.61078402, 52.47020745, 47.33059412]. profile225 | old | [166.27919411, 169.35251976, 162.12195974]; profile225 | new, no shuffle | [573.15814854, 249.25716389, 249.55162573]; profile225 | new, shuffle | [288.81559407, 130.46181546, 129.05702945]. chr22 | old | [343.85546785, 344.91024148, 337.22106485]; chr22 | new, no shuffle | [916.14408915, 497.77703920, 497.592671299]; ```; cc @cseed . edit: updated timings with the fix for RepartitionedRDD:. ```; profile225-cut new 108.9945367 053.5403599 050.8885137; profile225 new 249.1994185 134.2645221 132.4985398; chr22 new 269.2498286 266.3412299 265.5198199; ```. (profile225-cut is the same size as profile75, but with a non-left-aligned variant). (also: fixes #4023, fixes #4024)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4076:1181,update,updated,1181,https://hail.is,https://github.com/hail-is/hail/pull/4076,1,['update'],['updated']
Deployability,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:118,rollout,rollout,118,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626,10,"['Deploy', 'deploy', 'rollout']","['Deployment', 'deploy', 'deployment', 'rollout']"
Deployability,I think this is ready for another look. I'll cleanup the database once we're happy with everything and I no longer need to test with dev deploy (I don't want to have to nuke the database).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1276247643:137,deploy,deploy,137,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1276247643,1,['deploy'],['deploy']
Deployability,"I think this is ready. Adds a createDatabase2 step that adds database migrations. What's a database migration? The idea is that a database starts as an empty database and is built up or modified over time by a series of patches or migrations. The database has a version which is the number of migrations applied (starting from 1). This is stored in the table `{database_name}_migration_version`. Each migration involves running a `.sql` or `.py` script. The logic that applies migrations computes a checksum of these scripts and stores them in the database in table `{database_name}_migrations`. When applying migrations again in the future, these checksums are verified. A create database step now looks like (from the CI tests):. ```; - kind: createDatabase2; name: hello2_database; databaseName: hello2; migrations:; - name: create-tables; script: /io/sql/create-hello2-tables.sql; - name: insert; script: /io/sql/insert.py; inputs:; - from: /repo/ci/test/resources/sql; to: /io/; namespace:; valueFrom: default_ns.name; dependsOn:; - default_ns; - copy_files; ```. migrations is a the list of migrations that need to be applied to get the current version. So the idea is, if you want to change the schema of the database, you just add another migration at the end to make the changes you want. After this goes in, I'll make a separate PR to switch everything to this new createDatabase2 step.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7674#issuecomment-562891346:220,patch,patches,220,https://hail.is,https://github.com/hail-is/hail/pull/7674#issuecomment-562891346,1,['patch'],['patches']
Deployability,"I think this is the TSV from the public release, so I'll put it on a bucket.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2089#issuecomment-321580612:40,release,release,40,https://hail.is,https://github.com/hail-is/hail/pull/2089#issuecomment-321580612,1,['release'],['release']
Deployability,"I think this is the direction we need to head with Batch. Mapping DB things to objects and then using recursive functions over the graph is not going to scale. This uses one database call to:; - set the state of every non-always-run, incomplete job in the given batch to `Cancelled`; - move the state from `Running` to `Pending` for every; - job whose parents all succeeded, and every; - always-run job whose parents all completed; - get a list of every `Ready` or `Cancelled` job; - update the batch to cancelled and closed. Then uses a loop to delete k8s resources and create pods, as appropriate for the given job. I think we can go further! We should make our k8s requests in parallel (I don't see anyway to delete a *list* of jobs in k8s [only to delete a whole namespace]) and we should avoid retrieving every column from the database. We only need a few things to `create_pod` or `delete_k8s_resources`. cc: @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6578:484,update,update,484,https://hail.is,https://github.com/hail-is/hail/pull/6578,1,['update'],['update']
Deployability,"I think this is what was wrong with the `git_make_bash_image` taking a minute each time. Since every image without a `publishAs` uses `ci-intermediate`, the `ci-intermediate:cache-PR-X` tag is left pointing to whichever anonymous image built last in the PR run. This is certainly never `git_make_bash_image`, so every time it gets rebuilt, the cache-from that it is using points to an an image whose layers do not include a layer that is `RUN apt-get update && apt-get install -y git make bash`. If this PR runs twice, hopefully we'll see the first step go super quick.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12285:451,update,update,451,https://hail.is,https://github.com/hail-is/hail/pull/12285,2,"['install', 'update']","['install', 'update']"
Deployability,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:336,deploy,deploy-svc-list-test-pvc,336,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060,2,['deploy'],"['deploy-svc', 'deploy-svc-list-test-pvc']"
Deployability,I think this resolves a number of our issues actually wherein CI keeps hearing about jobs that it's already updated its internal state about.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5638:108,update,updated,108,https://hail.is,https://github.com/hail-is/hail/pull/5638,1,['update'],['updated']
Deployability,"I think this will fix the problem @xiaolicbs is having on dataflow. Some (somewhat) unrelated remarks on why `-b 16` didn't work for him. It's more subtle than I made out on gitter. `-b` sets the min block size, yes. `sc.defaultMinPartitions` is `sc.defaultParallelism`. On Cray that is 260 or however many cores we're asking for. On dataflow under YARN, that is dynamically updated based on the number of cores we have. If we're still building the DAG and haven't requested any executors yet, it is 2. We can set the default parallelism (although it won't update dynamically anymore) with `--conf spark.default.parallelism=250` for example. We should probably do this on dataflow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/824#issuecomment-248511352:375,update,updated,375,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248511352,2,['update'],"['update', 'updated']"
Deployability,"I think this will work, but I haven't tested it with dev deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7917:57,deploy,deploy,57,https://hail.is,https://github.com/hail-is/hail/pull/7917,1,['deploy'],['deploy']
Deployability,"I think tying the reset to the iterator is a mistake. First, iterator is the wrong abstraction here. Whole-stage code generation should use the aggregator/array strategy we're using in Emit to generate nothing, conditionals and loops for map, filter and flatMap, respectively. Ideally read ... do stuff ... write will generate an RDD with no per-element iterators at all. I want to make sure this picture is clear. Second, we want to vectorize in the database sense: we want to process multiple rows together in batches. Then overall structure of a stage is a loop over the batches, and and a loop within batches. Thus, the common case should not be we reset after every element, so I think it's the wrong direction to bake it in. The place where we do this should be interface points with the Spark stack which should be looked at with scorn and derision and as the organizing model. Finally, this points to an ongoing difference in our views about the meaning of context. I see context as serving two purposes (neither of which involve reset):. - First, context is a set of resources needed to process a partition that should be released when the partition is complete. For example, I'm working on GenomicsDB which needs to localize a GenomicsDB shard to a local file that needs to be cleaned up when the partition is complete. - Second, it is a way to tell an iterator where to return its value. (This is the ""current"" region business.). I'd be happy to separate these, but I don't see clean way. In no case do I see generic logic to manage the lifetime of regions (e.g. knowing when to call reset) inside the Context.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3365#issuecomment-381180739:1131,release,released,1131,https://hail.is,https://github.com/hail-is/hail/pull/3365#issuecomment-381180739,2,['release'],['released']
Deployability,"I think we also need to be clear when installing something that will break everyone's local tests (email, dev post, something).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3273#issuecomment-377700023:38,install,installing,38,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377700023,2,['install'],['installing']
Deployability,I think we need it to be offline unless we're willing to tolerate up to 5-10 mins of not being able to cancel a batch and some alerts. The only parts that would be referencing the wrong tables are in the `Canceller` and `notify_batch_complete`. I think scheduling and MJC would just work because we update those stored procedures and don't change the child code. We can shut batch down though for the migration. Seems safest although more of a pain.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13810#issuecomment-1812841477:299,update,update,299,https://hail.is,https://github.com/hail-is/hail/pull/13810#issuecomment-1812841477,1,['update'],['update']
Deployability,"I think we need this fix:; ```; commit 4cb998d1c7cbc9954d66c6e39d7fd48b0e936f51 (HEAD -> add-version-endpoint); Author: Daniel King <dking@broadinstitute.org>; Date: Mon Mar 22 17:47:22 2021 -0400. fix. diff --git a/build.yaml b/build.yaml; index 7a100adec8..256ca99c91 100644; --- a/build.yaml; +++ b/build.yaml; @@ -86,7 +86,7 @@ steps:; mkdir repo; cd repo; {{ code.checkout_script }}; - make -C hail python/hail/hail_version python/hail/hail_pip_version; + make -C hail python-version-info; git rev-parse HEAD > git_version; outputs:; - from: /io/repo/auth/sql; diff --git a/ci/test/resources/build.yaml b/ci/test/resources/build.yaml; index 3b1df5214c..b994d2787c 100644; --- a/ci/test/resources/build.yaml; +++ b/ci/test/resources/build.yaml; @@ -27,10 +27,13 @@ steps:; mkdir repo; cd repo; {{ code.checkout_script }}; + make -C hail python-version-info; timeout: 300; outputs:; - from: /io/repo; to: /; + - from: /io/repo/hail/python/hail/hail_version; + to: /hail_version; dependsOn:; - inline_image; - kind: buildImage; @@ -52,6 +55,10 @@ steps:; publishAs: service-base; dependsOn:; - base_image; + - copy_files; + inputs:; + - from: /hail_version; + to: /hail_version; - kind: buildImage; name: hello_image; dockerFile: ci/test/resources/Dockerfile; ```; EDIT: updated with more changes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-804418107:214,a/b,a/build,214,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-804418107,3,"['a/b', 'update']","['a/build', 'updated']"
Deployability,"I think we should find a time to discuss this in person if the following explanation doesn't make sense. . Right now, for small batches, we send one REST request to the server to both create the batch and create the jobs. However, if we want one REST request for an update (ideal for the query service and low latency jobs?), we have to use relative job ids because (1) we don't know the absolute start index of the jobs until we've gotten the start id of the update back from the server and (2) the job dependencies can be a mix of known job ids that have already been previously submitted in a previous creation/update. The negative job IDs are a way to deal with a mix of relative ids within an update and known, submitted job ids. We can simplify things if we require all updates make two requests to the server to (1) get the start id and establish the update and then (2) submit new jobs with all absolute job IDs. I'd have to make sure this will actually simplify things because I also ran into a bifurcation in how the job IDs are handled in `BatchBuilder.create_job()`. We currently populate the spec with a job id before we've made any requests to the server. We need to know how many total jobs there are before we can figure out the job ids because the API for creating a new update requires reserving a block of job IDs which then returns the start id. This complexity is because we allow multiple updates to occur simultaneously to a batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1215919856:266,update,update,266,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215919856,16,['update'],"['update', 'updates']"
Deployability,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:746,install,installs,746,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887,2,['install'],['installs']
Deployability,"I think we should just always mount the deploy config. That might trigger some latent bugs where we don't do quite the right thing, but we can fix those.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-767891514:40,deploy,deploy,40,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-767891514,1,['deploy'],['deploy']
Deployability,"I think we should remove tutorials from devel for now, and add them back in before 0.2 release",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2067#issuecomment-320243639:87,release,release,87,https://hail.is,https://github.com/hail-is/hail/issues/2067#issuecomment-320243639,1,['release'],['release']
Deployability,I think we'll need to hand deploy this fix and restart ci.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6377:27,deploy,deploy,27,https://hail.is,https://github.com/hail-is/hail/pull/6377,1,['deploy'],['deploy']
Deployability,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:130,rollout,rollout,130,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522,8,"['deploy', 'rollout', 'update']","['deployment', 'deployments', 'rollout', 'updated']"
Deployability,"I thought about this some more. I think the correct solution is to have two values for `n_jobs` in the `job_groups` table. There should be `n_jobs` (direct child jobs) and `n_jobs_recursive` (all descendent jobs). This way the UI and the status will make sense for whichever use case is most applicable. To do this, we'll need to write a migration that adds the new column and backfills the column. Right now, those two values are the same so it's just a copy of one column to another column. We might actually be able to do this migration in one update command. The other tables had hundreds of millions of rows and would have taken days and crashed the db if there was not enough memory. I can test this locally on my laptop by creating a test database and inserting 10 million records and then seeing how long it takes to do the update. ```; mysql> select count(*) from job_groups limit 10;; +----------+; | count(*) |; +----------+; | 8122788 |; +----------+; 1 row in set (16.75 sec); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14170#issuecomment-1934138382:547,update,update,547,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1934138382,2,['update'],['update']
Deployability,"I thought self.steps was still used. For example in `build`:. ```; def build(self, batch, code, scope):; assert scope in ('deploy', 'test', 'dev'). for step in self.steps:; if step.scopes is None or scope in step.scopes:; step.build(batch, code, scope). if scope == 'dev':; return. step_to_parent_steps = defaultdict(set); for step in self.steps:; for dep in step.all_deps():; step_to_parent_steps[dep].add(step). for step in self.steps:; parent_jobs = flatten([parent_step.wrapped_job() for parent_step in step_to_parent_steps[step]]). log.info(f""Cleanup {step.name} after running {[parent_step.name for parent_step in step_to_parent_steps[step]]}""). if step.scopes is None or scope in step.scopes:; step.cleanup(batch, scope, parent_jobs); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7722#issuecomment-568536375:123,deploy,deploy,123,https://hail.is,https://github.com/hail-is/hail/pull/7722#issuecomment-568536375,1,['deploy'],['deploy']
Deployability,"I thought the config file was better for QoB where versioning and backwards compatibility is important and there could be substantially more configuration in the future. I don't think we mount that config file in the worker regardless for DockerJobs, but I could be wrong.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12662#issuecomment-1420895778:141,configurat,configuration,141,https://hail.is,https://github.com/hail-is/hail/pull/12662#issuecomment-1420895778,1,['configurat'],['configuration']
Deployability,"I thought the purpose of the cache was to cache the latest version in production. Let's take service-base as an example. There's the deployment in production that we care about. But every PR is now going to change the cache each time to what it thinks service-base is. This means that the last 4 layers for service-base will change for every time we run a test PR and it changes hailtop, gear, or web-common. If you don't like this change, then feel free to close it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152568213:133,deploy,deployment,133,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152568213,1,['deploy'],['deployment']
Deployability,"I tracked down why this is happening. The old code stored the (compressed) genotype data per variant in a buffer and decoded it in BgenRecord.getValue. The new code decodes eagerly, but only if the entries are needed. I assume the intention was to mark the entries as unneeded during the scan, but not when decoding the actual values, but this wasn't done. It isn't done easily, either, since we can't set a per-Hadoop import configuration, see: https://github.com/hail-is/hail/issues/3861. Options:. - go back to the old code that stashes the compressed value and evaluates lazily,; - have separate InputFormat/RecordReader for scan and decode,; - stop using Hadoop InputFormat to load BGEN and just code it in directly in Spark, where it is trivial to pass different parameters to scan and decode. I personally vote for the latter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3862:426,configurat,configuration,426,https://hail.is,https://github.com/hail-is/hail/issues/3862,1,['configurat'],['configuration']
Deployability,"I tried it in both raw python and pyspark and I got a new error. Seem to be a problem with the profile having too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:925,configurat,configuration,925,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978,1,['configurat'],['configuration']
Deployability,"I tried to install hail for another researcher here at the NBER. I'm getting errors when trying to run hail because the version of glibc on the server is too old (released in 2010... crazy). Do you know of any way to get around this? I.e. would building hail from source work?. ### Hail version:. `0.2.3`, installed from pip. (Installed Spark 2.2.2 separately and set `SPARK_HOME` accordingly). . ### What you did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733:11,install,install,11,https://hail.is,https://github.com/hail-is/hail/issues/4733,4,"['Install', 'install', 'release']","['Installed', 'install', 'installed', 'released']"
Deployability,"I tried to look at this, but libsimdpp is completely spamming the diff visualizer. I'm back to thinking we shouldn't include this in the repo. We should either assume it is installed or download it during the build process. I'm inclined to do the former for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092#issuecomment-261979579:173,install,installed,173,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-261979579,1,['install'],['installed']
Deployability,"I updated all packages in conda, ran all python tests, and then updated/added the versions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4352:2,update,updated,2,https://hail.is,https://github.com/hail-is/hail/pull/4352,2,['update'],['updated']
Deployability,"I updated every call site to match the new parameters. This refactoring removes some code duplication, changes a stack trace in `hailctl auth user` to a nice print message, and adds a parameter (`client_session`) which I will use in my forthcoming TCP Proxy PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9715:2,update,updated,2,https://hail.is,https://github.com/hail-is/hail/pull/9715,1,['update'],['updated']
Deployability,"I updated terraform but. 1. GCP Terraform state is still local on my laptop. 2. GCP Terraform appears to not configure global-config. As such, I cannot thread the name of the bucket through to the tests the way we do with TEST_STORAGE_URI. For now, I've hardcoded the name (which is what we were doing previously). When we eventually get to testing recreation of GCP in a new project we'll have to address the global config then.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12964:2,update,updated,2,https://hail.is,https://github.com/hail-is/hail/pull/12964,1,['update'],['updated']
Deployability,"I updated the ""before attempts"" trigger because there was a bug where the start and end time on error (i.e. create fails) are both None and then when the instance gets deactivated, the reason is overwritten to deactivated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7949#issuecomment-579919520:2,update,updated,2,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-579919520,1,['update'],['updated']
Deployability,"I updated the HTTP to HTTPS redirect in `/etc/apache2/sites-enabled/000-default.conf` to preserve the sub-domains:. ``` apache; RewriteEngine on; RewriteCond %{HTTPS} off; RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI} [END,QSA,R=permanent]; ```. and I modified the `VirtualHost` set up to use [name-based VirtualHost discrimination](https://httpd.apache.org/docs/2.4/vhosts/name-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsenc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:2,update,updated,2,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['update'],['updated']
Deployability,I updated the dev deploy too.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8923#issuecomment-639040701:2,update,updated,2,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639040701,2,"['deploy', 'update']","['deploy', 'updated']"
Deployability,I updated this one too.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5622#issuecomment-477363453:2,update,updated,2,https://hail.is,https://github.com/hail-is/hail/pull/5622#issuecomment-477363453,1,['update'],['updated']
Deployability,"I use a Mac and try to install hail.; I use Mojave; I installed pyenv to modify my python versions.; I installed Python 3.7.9 since you recommend to use Python 3.7 as the latest version.; I then did a pip install hail, and it fails with pyspark:. Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1.tar.gz (215.7 MB); ERROR: Command errored out with exit status 1:; command: /Users/spascal/.pyenv/versions/3.7.9/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-pip-egg-info-vlaj8k6d; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/; Complete output (47 lines):; Could not import pypandoc - required to package PySpark; WARNING: The wheel package is not available.; ERROR: Command errored out with exit status 1:; command: /Users/spascal/.pyenv/versions/3.7.9/bin/python3.7 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-ggmq8ipk; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/; Complete output (8 lines):; no pandoc found, building platform unspecific wheel.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9742:23,install,install,23,https://hail.is,https://github.com/hail-is/hail/issues/9742,6,['install'],"['install', 'install-', 'installed']"
Deployability,I used FOR UPDATE when I should have used LOCK IN SHARE MODE. I added the diff between initial.sql and the new migration at the top. Not sure if this is helpful or what the standard for figuring out how to track changes should be.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7885:11,UPDATE,UPDATE,11,https://hail.is,https://github.com/hail-is/hail/pull/7885,1,['UPDATE'],['UPDATE']
Deployability,"I used filters for the following images when I've run the Azure cleanup script, but we should double check these make sense still in light of changing how we use ""cache"" and there aren't any additional images or ones that we don't want to delete that are in this list:. ```; --filter 'auth:.*' \; --filter 'base:.*' \; --filter 'base_spark_3_2:.*' \; --filter 'batch:.*' \; --filter 'batch-driver-nginx:.*' \; --filter 'batch-worker:.*' \; --filter 'benchmark:.*' \; --filter 'blog_nginx:.*' \; --filter 'ci:.*' \; --filter 'ci-intermediate:.*' \; --filter 'ci-utils:.*' \; --filter 'create_certs_image:.*' \; --filter 'echo:.*' \; --filter 'grafana:.*' \; --filter 'hail-base:.*' \; --filter 'hail-build:.*' \; --filter 'hail-buildkit:.*' \; --filter 'hail-run:.*' \; --filter 'hail-run-tests:.*' \; --filter 'hail-pip-installed-python37:.*' \; --filter 'hail-pip-installed-python38:.*' \; --filter 'hail-ubuntu:.*' \; --filter 'memory:.*' \; --filter 'monitoring:.*' \; --filter 'notebook:.*' \; --filter 'notebook_nginx:.*' \; --filter 'prometheus:.*' \; --filter 'service-base:.*' \; --filter 'service-java-run-base:.*' \; --filter 'test-ci:.*' \; --filter 'test-monitoring:.*' \; --filter 'test-benchmark:.*' \; --filter 'website:.*' \; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211#issuecomment-1255120349:820,install,installed-,820,https://hail.is,https://github.com/hail-is/hail/pull/12211#issuecomment-1255120349,2,['install'],['installed-']
Deployability,I used pip install - currently struggling to install the pyspark 2.0.2 version after downgrading to spark 2.0.2 . ``$SPARK_HOME is /Users/ih/languages/spark-2.0.2-bin-hadoop2.7/bin``,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319702678:11,install,install,11,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319702678,2,['install'],['install']
Deployability,I used the relevant portion of deploy.sh to create this PR: https://github.com/DataBiosphere/terra-docker/pull/303,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11369:31,deploy,deploy,31,https://hail.is,https://github.com/hail-is/hail/pull/11369,1,['deploy'],['deploy']
Deployability,I vaguely remember things working on Java 9 when someone joined the group and installed that. Am I imagining that?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4896#issuecomment-444558390:78,install,installed,78,https://hail.is,https://github.com/hail-is/hail/issues/4896#issuecomment-444558390,1,['install'],['installed']
Deployability,I verified that the right foreign key constraints were deleted in my namespace with dev deploy and I didn't touch the foreign key constraints in the other databases.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11938#issuecomment-1162407420:88,deploy,deploy,88,https://hail.is,https://github.com/hail-is/hail/pull/11938#issuecomment-1162407420,1,['deploy'],['deploy']
Deployability,"I verified this manually. The deploy account didn't exist when I started this PR, but it still had a role grant in the project's IAM policy. Now the account still does not exist *and* the role grant is gone. The `login` forces you to switch to your account since we're deleting the service account (as which you're probably authenticated).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8178:30,deploy,deploy,30,https://hail.is,https://github.com/hail-is/hail/pull/8178,1,['deploy'],['deploy']
Deployability,"I verified this works on 1.5.2, 1.6.3 and 2.0.2, but fails if we remove the relevant configuration settings (maxPartitionBytes, parquet.blocks.size).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1170#issuecomment-266622157:85,configurat,configuration,85,https://hail.is,https://github.com/hail-is/hail/pull/1170#issuecomment-266622157,1,['configurat'],['configuration']
Deployability,"I want cancel_after_n_failures to be on a job group. The things a job group doesn't have which maybe it should is:; - callback; - attributes; - updates. I think updates should be on a batch and not part of a job group. An update can add jobs to multiple job groups. Otherwise, the batches table should only have static fields that apply to the entire batch. I think we can do callbacks and attributes on a job group. I added a PATCH endpoint to be able to update a job group's cancel_after_n_attributes as the hailtop.batch interface was going to automatically generate job groups without any configuration settings. As for the full text search, I think prefix searches are faster with full text search than with a regular index, but I could be wrong. We'd have to benchmark it. > If we made batches simpler, does that ease complexity and decrease code duplication? In particular, what if batches didn't contain jobs at all? Instead, a batch contains exactly one job group. That job group contains zero or more job groups. Job groups manage: resource aggregation, cancellation, etc. I believe my plan is basically already doing this. It might not be clear because I didn't put the migrations in. But basically all of the current batches tables are now indexed by batch_id, job_group_id where the current ""batch"" has job_group_id = 1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12697#issuecomment-1450603163:144,update,updates,144,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1450603163,11,"['PATCH', 'configurat', 'update']","['PATCH', 'configuration', 'update', 'updates']"
Deployability,I want to do testing with dev deploy. Putting this up so I can get feedback.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7910:30,deploy,deploy,30,https://hail.is,https://github.com/hail-is/hail/pull/7910,1,['deploy'],['deploy']
Deployability,I want to get this out for Grace's team to run VEP. No reason we can't cut another release tomorrow.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9984#issuecomment-773391418:83,release,release,83,https://hail.is,https://github.com/hail-is/hail/pull/9984#issuecomment-773391418,1,['release'],['release']
Deployability,"I want to respond to some of your objections to unique_ptr. > If you buy into using std::unique_ptr, then everyone who writes or reads the code has to get; their head around the massively confusing and counter-intuitive concept of move semantics (a; form of assignment which modifies the source) and the somewhat bizarre terminology and syntax; used to express that in C++. I agree that move semantics takes getting used to, but I think it is much too integrated into modern C++ to ignore, going far beyond unique_ptr. Writing interfaces that take advantage of move semantics requires understanding rvalue-references in more detail, but for users of those move-enabled interfaces I think the guidelines are easy to teach: a variable will only be modified by moving if it is explicitly tagged with a `std::move`, so all you have to remember is ""after a `std::move(foo)`, the variable `foo` may only be assigned to or deleted."". > And then you get into a whole host of associated design decisions (I'm holding this as a unique_ptr,; but I want to pass it to a function - should I pass it as a raw pointer ? a raw reference ? a reference; to the unique_ptr ?). Keeping in mind the model that letting a function/class `foo` hold a `unique_ptr<Widget>` means explicitly ""`foo` owns this Widget, and is responsible for deleting it or passing ownership somewhere else"", these questions have pretty clear answers. * If a function `bar` takes a `Widget` but isn't concerned with its lifetime management, it should take its argument as a `Widget*` or `Widget&`, with the usual reasoning to choose between them. The caller owns the widget, and the lifetime of `bar` is nested inside that of its caller, so lifetime management isn't an issue.; * If `bar` takes a `Widget` and needs to take ownership, it should take its argument as a `unique_ptr<Widget>`. This serves as documentation that the function is taking over responsibility for deleting the Widget, in a way enforced by the compiler.; * A `unique_ptr<Wid",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3718#issuecomment-396669638:452,integrat,integrated,452,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396669638,2,['integrat'],['integrated']
Deployability,"I was able to install hail on my Macbook using a virtenv for python 2.7 and was able to import hail. . However, when I want to create a hail context as outlined in the installation guide, I get the following message: ; ``; Python 2.7.13 (default, Jul 18 2017, 09:16:53) ; [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-422>"", line 2, in __init__; File ""/Users/ih/languages/hail.is/hail/python/hail/typecheck/check.py"", line 226, in _typecheck; return f(*args, **kwargs); File ""/Users/ih/languages/hail.is/hail/python/hail/context.py"", line 68, in __init__; SparkContext._ensure_initialized(); File ""/Users/ih/hailenv/lib/python2.7/site-packages/pyspark/context.py"", line 283, in _ensure_initialized; SparkContext._gateway = gateway or launch_gateway(conf); File ""/Users/ih/hailenv/lib/python2.7/site-packages/pyspark/java_gateway.py"", line 77, in launch_gateway; proc = Popen(command, stdin=PIPE, preexec_fn=preexec_func, env=env); File ""/usr/local/Cellar/python/2.7.13_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 390, in __init__; errread, errwrite); File ""/usr/local/Cellar/python/2.7.13_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 1024, in _execute_child; raise child_exception; OSError: [Errno 2] No such file or directory; ``; Any idea what is going on with my installation, did somebody encounter this before already?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062:14,install,install,14,https://hail.is,https://github.com/hail-is/hail/issues/2062,3,['install'],"['install', 'installation']"
Deployability,"I was able to narrow this down a bit further. The issue appears due to this statement: https://github.com/broadinstitute/gnomad-browser/blob/80430090645ce087aa54d67688a4f0920ad1c8fd/data-pipeline/src/data_pipeline/datasets/gnomad_v3/gnomad_v3_variants.py#L127-L143. `subsets` contains 8 elements: `{'non_cancer', 'tgp', 'controls_and_biobanks', 'non_neuro', None, 'non_topmed', 'hgdp', 'non_v2'}`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533#issuecomment-1341807609:187,pipeline,pipeline,187,https://hail.is,https://github.com/hail-is/hail/issues/12533#issuecomment-1341807609,1,['pipeline'],['pipeline']
Deployability,"I was confused at how these could work because `self.conn.commit` and `self.conn.rollback` are async functions, but then I couldn't find any invocations of these methods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11367:81,rollback,rollback,81,https://hail.is,https://github.com/hail-is/hail/pull/11367,1,['rollback'],['rollback']
Deployability,"I was having trouble figuring out how to handle the token and the attributes in hailtop.batch_client.aioclient.Batch. When we create an update from a Batch that already existed perhaps in a different process, we don't have the attributes and token. I made a contract where `commit_update` always returns the token and attributes regardless of whether the BatchBuilder already has that infromation. However, we could also get that information available lazily and cache the result. In addition, the `n_jobs` returned to the client are the number of jobs that are committed and not the same as the `n_jobs` in the batches table. Things to do before merging:; 1. Get rid of the batch updates additions to the UI2. ; 2. Double check the GCP LogsExplorer to make sure there are no silent error messages especially with regards to cancellation.; 3. Have @danking look over the SQL stored procedure for `commit_batch_update` to make sure that query is going to perform as good as what is possible given the complexity of the check.; 4. Run a test batch with the old client (I just checked out the current version of main). You need to make sure both create and create-fast are accounted for and succeed. I've been using the following script to make sure we're using the slow path in addition to the fast path with a regular small test job:. ```python3; from hailtop.batch import ServiceBackend, Batch; import secrets. backend = ServiceBackend(billing_project='hail'); b = Batch(backend=backend); # 8 * 256 * 1024 = 2 MiB > 1 MiB max bunch size; for i in range(8):; j1 = b.new_job(); long_str = secrets.token_urlsafe(256 * 1024); j1.command(f'echo ""{long_str}"" > /dev/null'); batch = b.run(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1226043347:136,update,update,136,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1226043347,2,['update'],"['update', 'updates']"
Deployability,"I was hoping that a greater restructure would lead to a more elegant solution, but after so many footguns it just felt worth making this change. The main point is shelling out in the Makefiles to ask Kubernetes directly for what the global config values are instead of hard-coding them. Specifically, the changes are:. - `KUBERNETES_SERVER_URL` was simply not used anymore in the Makefiles so I deleted it.; - `DOCKER_ROOT_IMAGE`, `INTERNAL_IP`, `IP` and `CLOUD` were only used in a couple Makefiles so I moved the `kubectl` invocation into where they're used so that Makefiles that don't depend on those variables won't incur the cost of querying them; - `DOCKER_PREFIX` and `DOMAIN` were used pretty widely, so I kept them in config.mk because most Makefiles will need to query those values anyway. The added startup time for `make deploy` feels pretty insignificant.; - With this change in place, there's no need to render config.mk anymore so I deleted the shell function for doing so",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11414:834,deploy,deploy,834,https://hail.is,https://github.com/hail-is/hail/pull/11414,1,['deploy'],['deploy']
Deployability,I was just concerned that I hadn't tested dataproc after the changes and didn't want the release to fail. There wasn't anything about the actual release I changed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14071#issuecomment-1885047981:89,release,release,89,https://hail.is,https://github.com/hail-is/hail/pull/14071#issuecomment-1885047981,2,['release'],['release']
Deployability,I was looking for a PR to be the first deployed ;),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4235#issuecomment-417332020:39,deploy,deployed,39,https://hail.is,https://github.com/hail-is/hail/pull/4235#issuecomment-417332020,1,['deploy'],['deployed']
Deployability,"I was seeing crashes on workers in a local install because the Hail jar was loaded by a Spark class loader rather than the system class loader, so the toString in:. > String name = ClassLoader.getSystemResource(""include"").toString();. was failing with a null pointer exception. Fixed this two ways: don't unpack includes on the worker (compilation should only happen on the master) and use the same class loader that loaded the NativeCode object. Also some reformatting and style changes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4554:43,install,install,43,https://hail.is,https://github.com/hail-is/hail/pull/4554,1,['install'],['install']
Deployability,"I was thinking we would get rid of that update completely in MJC. If you want to know the state or the time completed, then you query the tokenized table. We won't store these values explicitly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039648911:40,update,update,40,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039648911,1,['update'],['update']
Deployability,"I went with version 0.2 based on the entry in the [changelog](https://pan.ukbb.broadinstitute.org/docs/changelog):. ```; Version 0.2. Added LD scores and matrices (released Oct 29, 2020).; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10186#issuecomment-804358709:164,release,released,164,https://hail.is,https://github.com/hail-is/hail/pull/10186#issuecomment-804358709,1,['release'],['released']
Deployability,"I went with your suggestions, changing `until` to `to` and using `foldLeft` rather than `flatMap` since it's cleaner on memory (even if slightly slower still). We can always speed these back up in any use case where they become a bottleneck, but right now they won't be. In @maccum 's pruning case there is a rectangle (window) per variant, but also a more appropriate single-pass algorithm. Definitely a performance hit on a rather heinous example using my original code versus your versions, but it's nothing compared to the distributed block matrix computations that follow in pipelines:; ```; val gp = GridPartitioner(512, 100000, 100000); val rnd = new scala.util.Random; def rects = Array.fill(100000){; val i = rnd.nextInt(90000); val j = rnd.nextInt(20000); Array[Long](i, i + 10000, i, i + 10000); }. outer: keep; inner: array. time: 66.642ms; time: 66.549ms; time: 64.039ms; time: 74.439ms. outer: for; inner: array. time: 1.251s; time: 1.389s; time: 1.439s; time: 1.353s. outer: keep; inner: for. time: 723.906ms; time: 715.612ms; time: 721.161ms; time: 707.852ms. outer: for; inner: for. time: 1.820s; time: 1.842s; time: 2.011s; time: 1.718s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3094#issuecomment-372717222:580,pipeline,pipelines,580,https://hail.is,https://github.com/hail-is/hail/pull/3094#issuecomment-372717222,1,['pipeline'],['pipelines']
Deployability,I will deploy after this merges,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4693:7,deploy,deploy,7,https://hail.is,https://github.com/hail-is/hail/pull/4693,1,['deploy'],['deploy']
Deployability,I will make the sparsifying rename/doc update a separate PR. @konradjk I also added another test along the same vein but for a matrix that has been filtered for a subset of rows and columns.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5511#issuecomment-469262861:39,update,update,39,https://hail.is,https://github.com/hail-is/hail/pull/5511#issuecomment-469262861,1,['update'],['update']
Deployability,I will need to manually deploy this when it lands.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8583#issuecomment-616612347:24,deploy,deploy,24,https://hail.is,https://github.com/hail-is/hail/pull/8583#issuecomment-616612347,1,['deploy'],['deploy']
Deployability,I will not wait for #14113. It's a really important improvement but there's also many bug fixes in here that need to be released.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14086#issuecomment-1887439034:120,release,released,120,https://hail.is,https://github.com/hail-is/hail/pull/14086#issuecomment-1887439034,1,['release'],['released']
Deployability,I will rebase after #4812 is in. This is a one-line change that adds `./gradlew deploy` to `hail-ci-deploy.sh`. The `python/deploy.sh` script only takes action if the latest deployed hail version does not match the current hail version. The current hail version is defined by `hailShortVersion` and `hailPipVersion` in `build.gradle`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4814:80,deploy,deploy,80,https://hail.is,https://github.com/hail-is/hail/pull/4814,4,['deploy'],"['deploy', 'deployed']"
Deployability,"I would also suggest the following as necessary for an upcoming release:. * #13728. Google's gcsfuse APT repository currently produces 502 Bad Gateway errors when accessed via http, which shows no sign of being resolved any time soon. I've commented on #13728 noting how this PR can work around the problem. At present (since early October), the `batch_worker_image` job always fails with 502 during a hail deployment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13806#issuecomment-1763650602:64,release,release,64,https://hail.is,https://github.com/hail-is/hail/issues/13806#issuecomment-1763650602,2,"['deploy', 'release']","['deployment', 'release']"
Deployability,"I'd argue this is a nicer UX - Having an ""invalid"" or ""unknown"" type lets people with weird alleles (and people do have weird alleles) actually run their pipelines instead of erroring out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3491#issuecomment-386425195:154,pipeline,pipelines,154,https://hail.is,https://github.com/hail-is/hail/pull/3491#issuecomment-386425195,2,['pipeline'],['pipelines']
Deployability,"I'd like to move master to Spark 2 and Scala 2.11. These changes get us as close as possible. They include:. - remove SparkExport, use reflection to get path of partition when loading from parquet; - remove SparkManager; - upgrade to Kudu 1.1.0 (Spark 2 support). The distance between this and Spark 2 is very small, see https://github.com/hail-is/hail/commit/95a588cfa72391d4303bf6891fd017ec211989db. When the master moves to Spark 2, we can maintain a spark1 branch until the on-prem machines get upgraded. Ideally, the spark1 branch could get rebased automatically as part of the CI, although I'm not quite sure how we'd handle conflicts. Alternatively, we could maintain a spark2 -> spark1 diff in the repo that gets applied as part of testing. Fixes https://github.com/hail-is/hail/issues/1117",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1124:223,upgrade,upgrade,223,https://hail.is,https://github.com/hail-is/hail/pull/1124,2,['upgrade'],"['upgrade', 'upgraded']"
Deployability,"I'd like to release to fix the PLINK reader bug, the `from_pandas` bug, and get the faster distinct key joining in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11419:12,release,release,12,https://hail.is,https://github.com/hail-is/hail/pull/11419,1,['release'],['release']
Deployability,I'd like to release to get weighted linear regression support for linear regression rows released.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10649:12,release,release,12,https://hail.is,https://github.com/hail-is/hail/pull/10649,2,['release'],"['release', 'released']"
Deployability,"I'd like your initial feedback before I start testing this on Azure. A substantially earlier version seemed to work fine on GCP with dev deploy. The major conceptual change I made is a `resource` now contains a `prefix` and a `version`. The `resource_name` is just `{prefix}/{version}`. The prefixes for GCP are the same as they were before and don't vary by region. However, the new prefixes for Azure are region specific. The version is `1` for all current resources. . I added a `latest_resource_versions` table that has the prefix mapped to the latest version. This is used to generate the current resource names. There is a new CloudResourceManager that is in charge of managing the spot billing pricing cache and updating the prices in the cache and the database from the cloud provider's API. Since I couldn't easily rename resources to products everywhere in the database due to anonymous foreign key constraints, I had to rename the existing `CloudResourceManager` to `CloudDriverAPI`. Feel free to suggest a better name. The GCPResourceManager is a skeleton right now, but we'll have to flesh it out in the new year when GCP moves to spot billing with varying prices. For the `AzureResourceManager`, I use a new pricing client to grab the latest vm and disk prices. I support all possible disk prices, but for now, I limited the VM query to just get the machine types we support right now. In the future, we could get all VM prices, but the query is around 40 seconds for that compared to 2 seconds now. I was worried if we had such a slow query that blocked driver startup, that would be bad and this is fine for now. There are two classes I added: a `Resource` and a `Price`. The Price is only implemented for Azure and is used to store cost results from the pricing API. The resource has a couple of different mixin classes with an abstract method to generate the quantified resource depending on the type (ex: ComputeResourceMixin). Then there's `AzureDiskResource`, `AzureVMResource`, e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11092:137,deploy,deploy,137,https://hail.is,https://github.com/hail-is/hail/pull/11092,1,['deploy'],['deploy']
Deployability,"I'd prefer a solution that addresses https://github.com/hail-is/hail/issues/4875. I'm happy to make the required change, but I feel like this answer will look weird to people who normally use `pip install x.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4877#issuecomment-443822085:197,install,install,197,https://hail.is,https://github.com/hail-is/hail/pull/4877#issuecomment-443822085,1,['install'],['install']
Deployability,"I'll add more tests, and I'm still considering whether to rip out or integrate WriteBlocksRDD for IRM. But given how similar this is to the latter, I'm ready for feedback on the new code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2559:69,integrat,integrate,69,https://hail.is,https://github.com/hail-is/hail/pull/2559,1,['integrat'],['integrate']
Deployability,"I'll add to the log of breaking changes. I've also updated the name from `position_morgan` to `cm_position`, as Plink 2.0 has settled on centimorgans: https://www.cog-genomics.org/plink/2.0/formats#bim. With this change, I can again import 1kg plink files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3623:51,update,updated,51,https://hail.is,https://github.com/hail-is/hail/pull/3623,1,['update'],['updated']
Deployability,I'll approve once the deploy-svc one goes in,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4601#issuecomment-431987841:22,deploy,deploy-svc,22,https://hail.is,https://github.com/hail-is/hail/pull/4601#issuecomment-431987841,1,['deploy'],['deploy-svc']
Deployability,I'll deploy to cluster by hand when this is merged,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4446#issuecomment-424727330:5,deploy,deploy,5,https://hail.is,https://github.com/hail-is/hail/pull/4446#issuecomment-424727330,1,['deploy'],['deploy']
Deployability,I'll do the deploy now!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8760#issuecomment-649874088:12,deploy,deploy,12,https://hail.is,https://github.com/hail-is/hail/pull/8760#issuecomment-649874088,1,['deploy'],['deploy']
Deployability,I'll pick this one up in medium term since NDArray stuff has involved a lot of fleshing out of numpy integration,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3699#issuecomment-550356741:101,integrat,integration,101,https://hail.is,https://github.com/hail-is/hail/issues/3699#issuecomment-550356741,1,['integrat'],['integration']
Deployability,"I'll stew on this a little further and I have yet to look closely at the queries themselves, but my first two questions are:. 1. I'm not opposed to adding tokens to the `batches_n_jobs_in_complete_states` table, but I'm not sure why this is related to the other pieces of this PR / job groups. Aren't tokens purely a performance optimization?. 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. > (C) The new server code deploys with the new mark_batch_complete code that runs periodically. Eventually the newly completed batches since the migration will get set to ""complete"". It seems to me like it would be preferable to instead first update application code to mark the batch complete if it is not complete, *then* remove the now redundant marking complete of the batch from the trigger. Then there is no delay after the migration where batches are not complete for some time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701536412:554,deploy,deploys,554,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701536412,2,"['deploy', 'update']","['deploys', 'update']"
Deployability,"I'll think about this more, but making the CI version explicit rather than implicit would at least provide a clear progression of necessary deploys. We'd want to tag the CI versions in git so that folks know which commits are necessary to achieve the step-wise transition. Let's find some time to chat next next week.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11122#issuecomment-985759470:140,deploy,deploys,140,https://hail.is,https://github.com/hail-is/hail/pull/11122#issuecomment-985759470,2,['deploy'],['deploys']
Deployability,I'll update them to run `python3 -m pip`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-533269724:5,update,update,5,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533269724,1,['update'],['update']
Deployability,I'm adding an additional migration PR that will put in the backwards compatibility for the updates table etc.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1220817788:91,update,updates,91,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1220817788,1,['update'],['updates']
Deployability,"I'm attempting to build hail from a clone of this repository's master branch, as a local install on my laptop, under Debian GNU/Linux version 8. The gradle script successfully downloaded and installed the various Java dependencies, but gcc chokes on the C source code. I get:; $ ./gradlew shadowJar; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1520:89,install,install,89,https://hail.is,https://github.com/hail-is/hail/issues/1520,2,['install'],"['install', 'installed']"
Deployability,I'm beginning to think the configuration stuff is confusing because it ignores the distinction between clients and servers.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513#issuecomment-611765252:27,configurat,configuration,27,https://hail.is,https://github.com/hail-is/hail/pull/8513#issuecomment-611765252,1,['configurat'],['configuration']
Deployability,I'm closing this because the pipeline causing these issues had a number of other complicating issues. We can re-open if/when we find a pipeline whose issues we're confident are due to a large alleles array rather than something else.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13584#issuecomment-1738201066:29,pipeline,pipeline,29,https://hail.is,https://github.com/hail-is/hail/issues/13584#issuecomment-1738201066,2,['pipeline'],['pipeline']
Deployability,I'm closing this while debugging with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7949#issuecomment-579013590:42,deploy,deploy,42,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-579013590,1,['deploy'],['deploy']
Deployability,"I'm convinced. @johnc1231, also change ""TruncatedBeta"" to ""TruncatedBetaDist"" global annotation. Be sure to update the documentation as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1332#issuecomment-276719598:108,update,update,108,https://hail.is,https://github.com/hail-is/hail/issues/1332#issuecomment-276719598,1,['update'],['update']
Deployability,"I'm currently expanding BlockMatrix binary ops to work naturally between BlockMatrix and 2d ndarray, with the useful ones for LMM being broadcasting ndarray over rows or columns of BlockMatrix. That'll round out the linear algebra, so that next week I'll begin refactoring the pipeline, with the interesting bits being the form of Python form of the small ""global model"" (initially using Scala side as black box) and then applying the local model to the pair or RowMatrices `X` and `PX` (read from BlockMatrices) to get per variant results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3114#issuecomment-373451854:277,pipeline,pipeline,277,https://hail.is,https://github.com/hail-is/hail/pull/3114#issuecomment-373451854,1,['pipeline'],['pipeline']
Deployability,I'm dev deploying now to take a look!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9982#issuecomment-774142818:8,deploy,deploying,8,https://hail.is,https://github.com/hail-is/hail/pull/9982#issuecomment-774142818,1,['deploy'],['deploying']
Deployability,"I'm fairly certain I know understand this and the AoU VDS creation issue. In Dataproc versions 1.5.74, 2.0.48, and 2.1.0, Dataproc introduced ""memory protection"" which is a euphemism for a newly aggressive OOMKiller. When the OOMKiller kills the JVM driver process, there is no hs_err_pid...log file, no exceptional log statements, and no clean shutdown of any sockets. The process is simply SIGTERM'ed and then SIGKILL'ed. From Hail 0.2.83 through Hail 0.2.109 (released February 2023), Hail was pinned to Dataproc 2.0.44. From Hail 0.2.15 onwards, `hailctl dataproc`, by default, reserves 80% of the advertised memory of the driver node for the use of the Hail Query Driver JVM process. For example, Google advertises that an n1-highmem-8 has 52 GiB of RAM, so Hail sets the `spark:spark.driver.memory` property to `41g` (we always round down). Before aggressive memory protection, this setting was sufficient to protect the driver from starving itself of memory. Unfortunately, Hail 0.2.110 upgraded to Dataproc 2.1.2 which enabled ""memory protection"". Moreover, in the years since Hail 0.2.15, the memory in use by system processes on Dataproc driver nodes appears to have increased. Due to these two circumstances, the driver VM's memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790:463,release,released,463,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790,2,['release'],['released']
Deployability,"I'm fine removing copy-paste-tokens. They were for a prototype with Terra. We obviously are pursuing a different approach now. Hmm. I suppose old versions of hailctl have no way to know that the fix is to upgrade to a newer version of hailctl? Like, the server can't send a message in the auth failure? We can just ask our local users to upgrade. As long as there's a stable & robust version of query that they can rely on, I think they're happy to upgrade. Which version of hailctl is compatible with new auth?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13531#issuecomment-1767018024:205,upgrade,upgrade,205,https://hail.is,https://github.com/hail-is/hail/issues/13531#issuecomment-1767018024,3,['upgrade'],['upgrade']
Deployability,"I'm forking ci => ci2 to address the existing ci issues. I plan to steal lots of code from ci, this is just a skeleton to get started. Along the way, I'm going to convert everything to async. I already deployed the ci2 service, so the gateway change shouldn't break anything.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5782:202,deploy,deployed,202,https://hail.is,https://github.com/hail-is/hail/pull/5782,1,['deploy'],['deployed']
Deployability,"I'm generalizing the CI's deploy system. https://github.com/hail-is/ci/pull/77. In particular, I no longer assume you need to authorize from to a gcloud account. Instead, I just mount you a volume. Each repo will have some secrets that it can authorize with. This is safe to merge now because before the CI changes go in, this just re-authorizes. When the CI changes merge, we'll go back to authorizing once, except that the command is in `hail-ci-deploy.sh` instead of baked into the CI system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4261:26,deploy,deploy,26,https://hail.is,https://github.com/hail-is/hail/pull/4261,2,['deploy'],['deploy']
Deployability,"I'm going to benchmark this today and if nothing has changed since Cotton's benchmarks, I think we're good for a merge. I had hoped to begin working with Grace's team to use this tool on Thursday. I can build a wheel, since I don't anticipate a release happening so fast.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10752#issuecomment-896959032:245,release,release,245,https://hail.is,https://github.com/hail-is/hail/pull/10752#issuecomment-896959032,1,['release'],['release']
Deployability,I'm going to change this branch to instead add `pre-commit` with `black` as one of its commit hooks. Developers can choose to `pre-commit install` which will set up useful hooks and on commit will run `black` over any modified files for directories that opt into it (which I'll start as just CI).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9931#issuecomment-769157671:138,install,install,138,https://hail.is,https://github.com/hail-is/hail/pull/9931#issuecomment-769157671,1,['install'],['install']
Deployability,I'm going to temporarily close this while I debug with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8961#issuecomment-644384619:59,deploy,deploy,59,https://hail.is,https://github.com/hail-is/hail/pull/8961#issuecomment-644384619,1,['deploy'],['deploy']
Deployability,I'm going to test this with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7909#issuecomment-575790241:32,deploy,deploy,32,https://hail.is,https://github.com/hail-is/hail/pull/7909#issuecomment-575790241,1,['deploy'],['deploy']
Deployability,I'm gonna close on account of this is the dev requirements. We don't mandate developer python installation systems and they might be using a package manager for this. Developers should update using whatever system they're using to manage python.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12896#issuecomment-1518317989:94,install,installation,94,https://hail.is,https://github.com/hail-is/hail/pull/12896#issuecomment-1518317989,2,"['install', 'update']","['installation', 'update']"
Deployability,I'm gonna force merge this since it only changes the deploy script and I'm not sure the PR build job will finish before the deploy job re-deploys.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4449#issuecomment-424797229:53,deploy,deploy,53,https://hail.is,https://github.com/hail-is/hail/pull/4449#issuecomment-424797229,3,['deploy'],"['deploy', 'deploys']"
Deployability,I'm good with this if @tpoterba and @johnc1231 are especially with the installing Hail script.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11187#issuecomment-1006773562:71,install,installing,71,https://hail.is,https://github.com/hail-is/hail/pull/11187#issuecomment-1006773562,1,['install'],['installing']
Deployability,"I'm good with this, I'll approve whenever you're ready to handle CI maybe breaking. Don't want to mess up dev deploys somehow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6973#issuecomment-526692251:110,deploy,deploys,110,https://hail.is,https://github.com/hail-is/hail/pull/6973#issuecomment-526692251,1,['deploy'],['deploys']
Deployability,I'm having trouble replicating this. Do you happen to remember the pipeline you were executing?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13064#issuecomment-1559836307:67,pipeline,pipeline,67,https://hail.is,https://github.com/hail-is/hail/issues/13064#issuecomment-1559836307,1,['pipeline'],['pipeline']
Deployability,I'm intending to debug/fix this today. I'll update this evening about progress.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10722#issuecomment-903834761:44,update,update,44,https://hail.is,https://github.com/hail-is/hail/issues/10722#issuecomment-903834761,1,['update'],['update']
Deployability,"I'm losing track of all the threads. Trying to summarize:; - Leave the CORS stuff, I don't understand it well enough to have an opinion anyway.; - Global gzip settings with gzip_min_length.; - Back out Docker changes, separately PR upgrade to nginx on Debian (would be my preference).; - Leave auth notebook commented out (although it makes me uncomfortable) and let's keep discussing how to solve it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5244#issuecomment-460333195:232,upgrade,upgrade,232,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460333195,1,['upgrade'],['upgrade']
Deployability,"I'm merging into https://github.com/hail-is/hail/pull/13325, but @daniel-goldstein , I'd still like your review of the updated code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13326#issuecomment-1656118980:119,update,updated,119,https://hail.is,https://github.com/hail-is/hail/pull/13326#issuecomment-1656118980,1,['update'],['updated']
Deployability,I'm missing something. Why shouldn't test deployments benefit from and contribute to the cache? Why isolate them somewhere else?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152542361:42,deploy,deployments,42,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152542361,1,['deploy'],['deployments']
Deployability,"I'm not 100% sure if this is true, but I've seen someone say on Stack Overflow that Macs only get BLAS and LAPACK natives after XCode is installed. We say on our Getting Started that these should automatically work on OSX, but we've also always required a C compiler as a getting started step. Now that we are going to distribute prebuilt packages, it may be the case that users won't download XCode because they don't need to and as such won't get the natives. We should verify whether or not natives are present prior to XCode installation and update docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1913:137,install,installed,137,https://hail.is,https://github.com/hail-is/hail/issues/1913,3,"['install', 'update']","['installation', 'installed', 'update']"
Deployability,"I'm not sure I follow. Can you elaborate more on ""We aren't including the test repo on deploy""? I thought we were always testing everything on deploy (ie running Hail, batch, pipeline tests etc).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7814#issuecomment-571721872:87,deploy,deploy,87,https://hail.is,https://github.com/hail-is/hail/pull/7814#issuecomment-571721872,3,"['deploy', 'pipeline']","['deploy', 'pipeline']"
Deployability,"I'm not sure I understand. `hail/python/hail/docs/_templates/layout.html` references `/navbar.css` which should be present on the deployed site. I might misunderstand `conf.py`, but, AFAICT, this makes unused copies of navbar.css and the PNG: https://hail.is/docs/0.2/navbar.css and https://hail.is/docs/0.2/hail-logo-cropped.png. The page at https://hail.is/docs/0.2/ loads `hail.is/navbar.css`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8956#issuecomment-644213411:130,deploy,deployed,130,https://hail.is,https://github.com/hail-is/hail/pull/8956#issuecomment-644213411,1,['deploy'],['deployed']
Deployability,"I'm not sure if this is the right change, but I'm pretty sure the Azure deployment was stuck because `_heal` kept aborting early on a GitHub post error. See #13050 for context.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13115:72,deploy,deployment,72,https://hail.is,https://github.com/hail-is/hail/pull/13115,1,['deploy'],['deployment']
Deployability,"I'm not sure the right way to test these. I certainly get errors when I don't have the memoization rules within my new linear regression rows pipeline, but I don't know what triggers the rebuild rules and a complicated linear algebra pipeline doesn't seem like a good way to unit test these anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8038:142,pipeline,pipeline,142,https://hail.is,https://github.com/hail-is/hail/pull/8038,2,['pipeline'],['pipeline']
Deployability,"I'm not sure this is the ""query-way"" of doing these checks, but having this feature would have saved Cal from running an expensive pipeline and the output file ""disappearing"" at the end.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12775:131,pipeline,pipeline,131,https://hail.is,https://github.com/hail-is/hail/pull/12775,1,['pipeline'],['pipeline']
Deployability,"I'm not sure what the best/easiest thing to do is. I think we can accomplish the same thing by copying and pasting the delete tables step (runImage) into a dev branch while testing. . The other easiest thing I can think of is to add a `run_if_requested=True` option to each build step config and modify ci to skip over steps that aren't specifically requested in dev deploy. I don't think a new step is a good idea because what if I want an optional runImage step or an optional Deploy step, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7705#issuecomment-565009061:367,deploy,deploy,367,https://hail.is,https://github.com/hail-is/hail/pull/7705#issuecomment-565009061,2,"['Deploy', 'deploy']","['Deploy', 'deploy']"
Deployability,"I'm not sure what to do about this. I don't think I can update this to use Python semantics instead of java semantics (n = number of times to split, instead of number of split results) without breaking pipelines.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7893#issuecomment-613679220:56,update,update,56,https://hail.is,https://github.com/hail-is/hail/issues/7893#issuecomment-613679220,2,"['pipeline', 'update']","['pipelines', 'update']"
Deployability,"I'm not sure where these should live, but I wanted to move them off my laptop and into a place where people can access them, and the repo seems as good of a place as any. There are svgs for a bunch of icons in both blue and white, as well as high-ish resolution images of both versions of the logo and 32x32 icon pngs. I haven't started integrating them into website stuff yet, but I figured that raw images should have a central-ish place to live anyways. All of the images in the PR are as below:; ![all](https://user-images.githubusercontent.com/19789871/91755326-ece1d180-eb98-11ea-83ce-eec6b13ab18f.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9382:337,integrat,integrating,337,https://hail.is,https://github.com/hail-is/hail/pull/9382,1,['integrat'],['integrating']
Deployability,"I'm not sure whether we should add this proactively or not, and to be clear I don't intend users to generally use this, but this is the best solution I can think of so far for @illusional's question about what to do when we have removed support for the old hail access tokens but users still are forced to run a pipeline on an old hail version. Old hail access tokens are stored in JSON in `~/.hail/tokens.json`, so I believe (though have not yet tested, that the following should allow a user to use an old version of hail against a version of batch that only supports cloud access tokens:. On the *new* version of hail, run. ```; hailctl auth login; hailctl auth print-access-token | jq -R -c '{ default: . }' > ~/.hail/tokens.json; ```. Then switch to an old version and proceed as usual (but don't run `hailctl auth login`!).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13934#issuecomment-1783136528:312,pipeline,pipeline,312,https://hail.is,https://github.com/hail-is/hail/pull/13934#issuecomment-1783136528,2,['pipeline'],['pipeline']
Deployability,I'm not sure why my exception didn't get picked up by the aiohttp.ClientOSError block. I added a plain OSError just in case. Feel free to push back on that. It's possible I didn't have updated is_transient_error code when I got the original error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7340:185,update,updated,185,https://hail.is,https://github.com/hail-is/hail/pull/7340,1,['update'],['updated']
Deployability,"I'm now happy with the updates, ready for another look",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233967833:23,update,updates,23,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233967833,1,['update'],['updates']
Deployability,"I'm personally OK not supporting a long lineage of Hail versions on QoB. If we don't have to make people update for a few versions, great, but we should be OK making people run a pip upgrade.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1527997196:105,update,update,105,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1527997196,2,"['update', 'upgrade']","['update', 'upgrade']"
Deployability,"I'm pretty sure that Hail isn't being installed correctly. But since you're not the one installing it, we can't really help. Let's continue discussion on the forum: https://discuss.hail.is/t/using-hail0-2-on-azure-databrick/1128",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7298#issuecomment-542178553:38,install,installed,38,https://hail.is,https://github.com/hail-is/hail/issues/7298#issuecomment-542178553,2,['install'],"['installed', 'installing']"
Deployability,I'm putting a wip tag on this so we make sure we dev deploy and test the behavior one last time before merging.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10630#issuecomment-872355294:53,deploy,deploy,53,https://hail.is,https://github.com/hail-is/hail/pull/10630#issuecomment-872355294,1,['deploy'],['deploy']
Deployability,I'm putting the WIP tag on until we get the batch database migration deployed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13980#issuecomment-1798843967:69,deploy,deployed,69,https://hail.is,https://github.com/hail-is/hail/pull/13980#issuecomment-1798843967,1,['deploy'],['deployed']
Deployability,"I'm rather inclined to wait for @cseed's commentary on this before merging it. I'm not sure if this is the best solution, but it seems like a viable way to hold necessary configuration parameters.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613#issuecomment-290197728:171,configurat,configuration,171,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290197728,1,['configurat'],['configuration']
Deployability,"I'm really not very sure what's going on here, but right now on main when I run `make -C hail install && python3 -c 'import hail'` I get the following:. ```; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/usr/local/Caskroom/miniconda/base/envs/hail7/lib/python3.7/site-packages/hail/__init__.py"", line 45, in <module>; from .table import Table, GroupedTable, asc, desc # noqa: E402; File ""/usr/local/Caskroom/miniconda/base/envs/hail7/lib/python3.7/site-packages/hail/table.py"", line 14, in <module>; from hail.expr.types import hail_type, tstruct, types_match, tarray, tset, dtypes_from_pandas; ImportError: cannot import name 'dtypes_from_pandas' from 'hail.expr.types' (/usr/local/Caskroom/miniconda/base/envs/hail7/lib/python3.7/site-packages/hail/expr/types.py); ```. The following changes fixed my import woes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11975:94,install,install,94,https://hail.is,https://github.com/hail-is/hail/pull/11975,1,['install'],['install']
Deployability,"I'm running hail version 0.2.23 and python 3.7.4 in a conda environment as described in the [installation instructions for mac os](https://hail.is/docs/0.2/getting_started.html#installing-hail-on-mac-os-x-or-gnu-linux-with-pip). The tutorials worked fine when I ran them from the command line using ipython, but when I launched a jupyter notebook, I kept getting a `ModuleNotFoundError` when running `import hail as hl`. I solved the problem by first running `conda install jupyter` in my virtual environment. Am i the only one who had this issue? If not, please consider adding this extra step to the installation instructions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7136:93,install,installation,93,https://hail.is,https://github.com/hail-is/hail/issues/7136,4,['install'],"['install', 'installation', 'installing-hail-on-mac-os-x-or-gnu-linux-with-pip']"
Deployability,"I'm seeing deploy failures where the tests start failing part way through because batch becomes unavailable, for example: https://ci2.hail.is/jobs/2886/log. However, this can't be the whole story, because batch has a readiness check and it isn't clear why it should go unavailable. Either way, this seems safer because it makes sure you pick up the intended version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6093:11,deploy,deploy,11,https://hail.is,https://github.com/hail-is/hail/pull/6093,1,['deploy'],['deploy']
Deployability,"I'm sorry I had to duplicate everything :(; Here's the diff from the old migration script. I fixed the SQL and also only throw an exception after all bad records have been found:. ```diff; diff --git a/batch/sql/dedup_billing_project_users.py b/batch/sql/dedup_billing_project_users_v2.py; index 5533e63e5e..b840a30e17 100644; --- a/batch/sql/dedup_billing_project_users.py; +++ b/batch/sql/dedup_billing_project_users_v2.py; @@ -96,6 +96,8 @@ async def audit_changes(db):; chunk_offsets.append(offset); chunk_offsets = list(zip(chunk_offsets[:-1], chunk_offsets[1:])); ; + failing_bp_users = []; +; if chunk_offsets != [(None, None)]:; for offsets in chunk_offsets:; start, end = offsets; @@ -130,25 +132,25 @@ FROM (; LOCK IN SHARE MODE; ) AS old; LEFT JOIN (; - SELECT billing_project, `user`, resource_id, CAST(COALESCE(SUM(`usage`), 0) AS SIGNED) AS `usage`; + SELECT billing_project, `user`, deduped_resource_id, CAST(COALESCE(SUM(`usage`), 0) AS SIGNED) AS `usage`; FROM aggregated_billing_project_user_resources_v3; + LEFT JOIN resources ON resources.resource_id = aggregated_billing_project_user_resources_v3.resource_id; {where_statement}; - GROUP BY billing_project, `user`, resource_id; + GROUP BY billing_project, `user`, deduped_resource_id; LOCK IN SHARE MODE; -) AS new ON old.billing_project = new.billing_project AND old.`user` = new.`user` AND old.deduped_resource_id = new.resource_id; +) AS new ON old.billing_project = new.billing_project AND old.`user` = new.`user` AND old.deduped_resource_id = new.deduped_resource_id; WHERE new.`usage` != old.`usage`; LIMIT 100;; ''',; where_args + where_args); ; bad_bp_user_records = [record async for record in bad_bp_user_records]; - failing_bp_users = []; for record in bad_bp_user_records:; print(f'found bad bp user record {record}'); failing_bp_users.append((record['billing_project'], record['user'])); ; - if bad_bp_user_records:; - raise Exception(f'errors found in audit'); + if failing_bp_users:; + raise Exception(f'errors foun",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13117:200,a/b,a/batch,200,https://hail.is,https://github.com/hail-is/hail/pull/13117,2,['a/b'],['a/batch']
Deployability,"I'm testing out the dev deploy now, but realized this is super slow because Dan's PR changing how the base image is done with a new hail_ubuntu image is in now. Just FYI.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9684#issuecomment-723127410:24,deploy,deploy,24,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-723127410,1,['deploy'],['deploy']
Deployability,"I'm thinking that I want to have this:. Basic setup for the domain, remote_tmpdir, billing_project, and only select one region; ```; hailctl init; ```. Which will prompt for:; 1. GCP project; 2. Hail domain; 3. region; 4. remote tmpdir location. And set:; - domain; - batch/billing_project; - batch/regions; - batch/remote_tmpdir; - batch/backend; - query/backend. And then print out a message with the default settings with instructions on how to change any of the settings and warnings about the configuration by using `hailctl config set ...`. I'll have the autocomplete PR merged (hopefully) by then so we can make that nice. The trial billing project is automatically used and we don't prompt for that. Then we'll have optional flags for components to configure; ```; hailctl init --container-registry --requester-pays-buckets --extra-query-settings --extra-batch-settings; ```; This would prompt for the same as above as well as advanced regions settings, query settings, and create the container registry. And lastly; ```; hailctl config check; ```; which prints warnings about misspecified regions and tmpdirs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1650549451:498,configurat,configuration,498,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1650549451,1,['configurat'],['configuration']
Deployability,"I'm trying to move all NDArray emission to emitI and update the style a bit, with the plan to eventually overhaul `NDArrayEmitter` to be more in line with the `PCode` interface. This PR updates `NDArrayMatmul` and `NDArrayInv` emit rules to be `emitI` rules, and cleans it up a bit. The diff isn't great, main changes were doing those `emit / flatmap` calls at the top to create `PCode`s, then using the corresponding `PValue`s everywhere I could instead of relying on `PType` methods. All the ""setup"" and missingness check code also went away since the `flatmap`s handle that stuff.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9438:53,update,update,53,https://hail.is,https://github.com/hail-is/hail/pull/9438,2,['update'],"['update', 'updates']"
Deployability,"I'm trying to move our codebase away from relying on the notion of a K8s namespace for routing because this model does not hold in alternative deployment environments such as Terra. Further, having a `-n` option in `hailctl auth` commands is awkward because it can only be used for dev environments yet is visible to users in the help menu. As such, this is a breaking change only for developers. The functionality is not really gone though because you can replace any `hailctl auth … -n dgoldste` with `HAIL_DEFAULT_NAMESPACE=dgoldste hailctl auth …`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14069:143,deploy,deployment,143,https://hail.is,https://github.com/hail-is/hail/pull/14069,1,['deploy'],['deployment']
Deployability,I'm update the readme separately. And we can discuss updating the links later.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4559:4,update,update,4,https://hail.is,https://github.com/hail-is/hail/pull/4559,1,['update'],['update']
Deployability,"I'm using it locally (installed using `./gradlew installDist`) but working on our cluster, rather than with `spark-submit`. I have not loaded the spark module on the cluster. Is Hail installing its own spark libraries? Is there a way to configure the tmp dir for these?. Thanks",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/902#issuecomment-251732459:22,install,installed,22,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251732459,3,['install'],"['installDist', 'installed', 'installing']"
Deployability,"I'm using java 1.8,; `java version ""1.8.0_71""; Java(TM) SE Runtime Environment (build 1.8.0_71-b15); Java HotSpot(TM) 64-Bit Server VM (build 25.71-b15, mixed mode); `; Although I realized Spark was the requirement, however, I'm unsure how to install spark2.1.1. I have downloaded and unzipped the file spark-2.1.1-bin-hadoop2.7. UPDATE: I reinstalled JDK8 and now the :compileScala error has gone away. Build was successful.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-302834246:243,install,install,243,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302834246,2,"['UPDATE', 'install']","['UPDATE', 'install']"
Deployability,I'm using one hail environment or everything now and I need these to test hail. I also added twine which we'll need for deploying anything to PyPI including cloudtools.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5916:120,deploy,deploying,120,https://hail.is,https://github.com/hail-is/hail/pull/5916,1,['deploy'],['deploying']
Deployability,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:207,deploy,deployment,207,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141,1,['deploy'],['deployment']
Deployability,"I've addressed the two comments: now using an `entry_fields` parameter and throwing an error if 'dosage' is requested and any variant is multi-allelic. Docs updated accordingly. I considered setting dosage on multi-allelics to missing rather than throwing an error, but I think error is safest since I could imagine the missingness leading to QC confusion, and if users want dosage in the presence of multi-allelics than they should either use a custom expression or split and then use `hl.gp_dosage`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2930#issuecomment-366829913:157,update,updated,157,https://hail.is,https://github.com/hail-is/hail/pull/2930#issuecomment-366829913,1,['update'],['updated']
Deployability,I've already applied this change to the cluster since k8s-config.yaml isn't actually auto-deployed yet.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5518:90,deploy,deployed,90,https://hail.is,https://github.com/hail-is/hail/pull/5518,1,['deploy'],['deployed']
Deployability,"I've already deployed these changes. Without these changes, I've locked myself out of my dev namespace because my dev namespace refuses to speak HTTP now (😄).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8486:13,deploy,deployed,13,https://hail.is,https://github.com/hail-is/hail/pull/8486,1,['deploy'],['deployed']
Deployability,"I've already deployed to the cluster to test. Unless master deploys, you can see it here: https://grafana.azure.hail.is/datasources/edit/kgzY5Io7k. I also fixed the grafana Makefile.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14106:13,deploy,deployed,13,https://hail.is,https://github.com/hail-is/hail/pull/14106,2,['deploy'],"['deployed', 'deploys']"
Deployability,I've asked Wenhan to run this pipeline with a JAR that has extra debugging information enabled https://github.com/hail-is/hail/compare/main...danking:hail:debug-13979.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834228219:30,pipeline,pipeline,30,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834228219,1,['pipeline'],['pipeline']
Deployability,"I've been supporting Hana as much as I can, but she needs someone who can be more dedicated and responsive than me. She uses a k8s cluster. She has a SEQR frontend deployment. She also has a Hail deployment (statefulset maybe?). The Hail pod has an SSD mounted read-only. That SSD has all the SEQR data in Hail Table form. There are many tables with annotations (variant metadata, like ""probability this variant is damaging"" or ""likely causes this to happen to the protein""). There are also ""per-family"" tables which contain all the sequences within a single family. Many queries are directly against a particular family. Those tables are small and quick to read. There's also one giant table containing all the sequences from all the families. That table is large and expensive to read. A lot of our engineering work has been around making sure queries against that table are fast. Tim, at one point, had enough of her system locally that he could experiment with running queries on his laptop against his SSD. He hacked on the queries themselves and on Hail itself until the bandwidth was fast enough that the queries should complete fast enough on the full dataset. Fast enough varies but generally a couple tens of seconds is OK. The work here is to pair with Hana to diagnose performance issues and make changes until the queries are acceptably fast. The first thing I would do is update her to the latest Hail (with the array decoder improvement as well as the memory overhead stuff on which Daniel is working). Then, with Hana's help, test the timing of some queries. If the queries are still too slow, your options are:; 1. Check the log files and the IR. Are there unnecessary shuffles? Is the code really large? Can we do less work maybe?; 2. Have Hana help you replicate her setup locally. You just need a slice of the data and enough of SEQR to run a query. Now hook up a profiler. What's slow? Can we do something about that?. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882:1751,update,update,1751,https://hail.is,https://github.com/hail-is/hail/issues/13882,1,['update'],['update']
Deployability,"I've changed BlockMatrix.from(lm: BDM[Double]) so that each executor is transmitted only the blocks it needs (~num_blocks/num_executors) rather than all of them: ""[TorrentBroadcast](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-TorrentBroadcast.html) uses a BitTorrent-like protocol for block distribution (that only happens when tasks access broadcast variables on executors)."". In another branch, I've verified on GCP that distributing and then localizing a 10k x 10k matrix is twice as fast (about 15s vs 30s). Distributing and then writing a 25k by 25k matrix (5GB) with 10+2 standard 8-core workers takes about 30s with the new method but fails for every partition with the old method (months ago I believe I sometimes got the old method to work at this scale using high mem. It's needed for LMM). Note the matrix only has 49 partitions at the new default blockSize so I had more cores than needed for the experiment. I've also added a method to write a local matrix as a block matrix. I use ParRange to parallelize writing from master. Writing and then reading should be the safest way to distribute a big local matrix at the beginning of complex pipelines, and I think it avoids some of the memory overhead associated with broadcast.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2848:1171,pipeline,pipelines,1171,https://hail.is,https://github.com/hail-is/hail/pull/2848,1,['pipeline'],['pipelines']
Deployability,"I've left out batch and gear so as not to interfere with #10920, but I intend to do those once there are no huge batch PRs on deck. I think I'm the only one with the pre-commit hooks installed because currently I can't make any small changes to batch files without introducing a ton of format changes (without disabling the pre-commit hooks).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11017:183,install,installed,183,https://hail.is,https://github.com/hail-is/hail/pull/11017,1,['install'],['installed']
Deployability,"I've manually deployed site to fix the broken links, but site will continue to point at 05a792599, while hail itself will successfully deploy (because it is deployed before batch is).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4592#issuecomment-431594790:14,deploy,deployed,14,https://hail.is,https://github.com/hail-is/hail/issues/4592#issuecomment-431594790,3,['deploy'],"['deploy', 'deployed']"
Deployability,"I've now addressed the sequence dictionary point made by @laserson, as well as rebasing on master. . Persisting annotations is a challenge as it looks like the schema is dynamic - is that right? I'm not sure how that will work with Kudu, which expects a fixed schema at table-creation time. . BTW here's an updated link to the Kudu issue I mentioned earlier: https://issues.apache.org/jira/browse/KUDU-383. Note that it's not a blocker for this PR as there's a workaround.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-220610690:307,update,updated,307,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-220610690,1,['update'],['updated']
Deployability,"I've rebased John's branch, added the default block_size change by @shulik7 at Nirvana, and made a few additional small changes including adding the @typecheck_method and @record_method decorators (the latter is now required, else history goes histrionic). The configuration file, init script, and resource files are in`gs://hail-common/nirvana`. The init script `gs://hail-common/nirvana/nirvana-init-GRCh37.sh` makes local copies of the resource files and .net: ; ```; #!/bin/bash. mkdir -p /nirvana/Data/Cache; mkdir -p /nirvana/Data/References; mkdir -p /nirvana/Data/SupplementaryDatabase. #Data is copied for use with Nirvana 1.6.2 as of June 19 2017; gsutil -m cp -r gs://hail-common/nirvana/Data/Cache/24/GRCh37 /nirvana/Data/Cache; gsutil -m cp gs://hail-common/nirvana/Data/References/5/Homo_sapiens.GRCh37.Nirvana.dat /nirvana/Data/References; gsutil -m cp -r gs://hail-common/nirvana/Data/SupplementaryDatabase/39/GRCh37 /nirvana/Data/SupplementaryDatabase; gsutil -m cp -r gs://hail-common/nirvana/netcoreapp1.1 /nirvana; gsutil -m cp gs://hail-common/nirvana/nirvana-cloud-GRCh37.properties /nirvana. chmod -R 777 /nirvana. apt-get -y install curl libunwind8 gettext; curl -sSL -o dotnet.tar.gz https://go.microsoft.com/fwlink/?linkid=843453; mkdir -p /opt/dotnet && sudo tar zxf dotnet.tar.gz -C /opt/dotnet; ln -s /opt/dotnet/dotnet /usr/local/bin; ```. The properties file `nirvana-cloud-GRCh37.properties` points Nirvana to these local resources:; ```; hail.nirvana.location = /nirvana/netcoreapp1.1/Nirvana.dll; hail.nirvana.cache = /nirvana/Data/Cache/GRCh37/Ensembl84; hail.nirvana.reference = /nirvana/Data/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.supplementaryAnnotationDirectory = /nirvana/Data/SupplementaryDatabase/GRCh37; ```. I started a cluster with the init script and ran Nirvana on all of `profile225.vcf`, and later exported results for just a region bounding the gene CABIN1:; ```; from hail import *; hc = (HailContext()). (hc; .import_vcf(path='gs:/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701:261,configurat,configuration,261,https://hail.is,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701,1,['configurat'],['configuration']
Deployability,"I've reformatted the urls in the `gs://hail-datasets-us`, `gs://hail-datasets-eu`, and `s3://hail-datasets-us-east-1` buckets with a new naming scheme to make things more consistent and clean up the buckets. . This updates the `datasets.json` file with these new urls. Currently there are two copies of each dataset in each bucket, one at the old url and one at the new url. I will remove the datasets at the old urls in a few months to avoid disrupting users not running the latest release.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10526:215,update,updates,215,https://hail.is,https://github.com/hail-is/hail/pull/10526,2,"['release', 'update']","['release', 'updates']"
Deployability,I've removed the `make test-deploy` stuff to simplify this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-481837110:28,deploy,deploy,28,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-481837110,2,['deploy'],['deploy']
Deployability,"I've seen this before, the PyPI databases are out of sync. You can see the latest with list but not install",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4241#issuecomment-418776942:100,install,install,100,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418776942,1,['install'],['install']
Deployability,I've updated docs and code to ensure that NaN and infinite values are caught.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2893#issuecomment-365318621:5,update,updated,5,https://hail.is,https://github.com/hail-is/hail/pull/2893#issuecomment-365318621,1,['update'],['updated']
Deployability,I've updated the PR accordingly — feel free to wordsmith the warning message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14700#issuecomment-2384183254:5,update,updated,5,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2384183254,1,['update'],['updated']
Deployability,I've updated the code to address the easy comments. The annotated.globalSignature and hadoop_writer ones I don't know how and whether to fix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1991#issuecomment-316414396:5,update,updated,5,https://hail.is,https://github.com/hail-is/hail/pull/1991#issuecomment-316414396,1,['update'],['updated']
Deployability,I've updated the docs. ; One remaining issue is how to allow multiple `--plugin` options (http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html) ?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1712#issuecomment-298847712:5,update,updated,5,https://hail.is,https://github.com/hail-is/hail/pull/1712#issuecomment-298847712,1,['update'],['updated']
Deployability,"I've updated this to allow stream consumers to choose an allocation strategy independently of the producer: the producer is written to allocate temporary regions that it owns, as needed, but the consumer can override that behavior to actually put all temporary allocations in a single region. This is implemented via the two abstract interfaces `StagedRegion` and `StagedOwnedRegion`, which have two implementations, one of which can create temporary regions, and one which cannot. The producer is written using the interface, and the consumer passes in one of the concrete implementations, determining the allocation strategy. Currently, only the non-allocating instance is ever used, which generates the same code we did before, so this should have zero run-time effect, even if we continue to update more stream nodes to use smarter region management. We can figure out separately how to introduce the other strategy (use less memory, but with more allocator overhead) in a controlled way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106#issuecomment-664591165:5,update,updated,5,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-664591165,2,['update'],"['update', 'updated']"
Deployability,"I've updated this with the code from Hadoop-BAM (in 7.6.0), so it's ready for review. Can you take a look please @cseed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/426#issuecomment-229917065:5,update,updated,5,https://hail.is,https://github.com/hail-is/hail/pull/426#issuecomment-229917065,1,['update'],['updated']
Deployability,"IIUC the reason we `tar` the hail wheel to move it between jobs in the CI pipeline is because the wheel name must contain the pip version and that is not known statically in `build.yaml`. However, it would be just as effective to copy the wheel around inside a directory, and then we don't have to do all this tar'ing and untar'ing. cc @ehigham I'm happy to hold off on this if it would bork your branch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13912:74,pipeline,pipeline,74,https://hail.is,https://github.com/hail-is/hail/pull/13912,1,['pipeline'],['pipeline']
Deployability,IL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/gi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3778,deploy,deploy-,3778,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,IL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c H,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:9254,deploy,deploy-,9254,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,ILoop.scala:914); at scala.tools.nsc.interpreter.ILoop.mkReader$1(ILoop.scala:920); at scala.tools.nsc.interpreter.ILoop.$anonfun$chooseReader$4(ILoop.scala:926); at scala.tools.nsc.interpreter.ILoop.$anonfun$chooseReader$3(ILoop.scala:926); at scala.tools.nsc.interpreter.ILoop.chooseReader(ILoop.scala:926); at org.apache.spark.repl.SparkILoop.$anonfun$process$1(SparkILoop.scala:138); at scala.Option.fold(Option.scala:251); at org.apache.spark.repl.SparkILoop.newReader$1(SparkILoop.scala:138); at org.apache.spark.repl.SparkILoop.preLoop$1(SparkILoop.scala:142); at org.apache.spark.repl.SparkILoop.$anonfun$process$10(SparkILoop.scala:203); at org.apache.spark.repl.SparkILoop.withSuppressedSettings$1(SparkILoop.scala:189); at org.apache.spark.repl.SparkILoop.startup$1(SparkILoop.scala:201); at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:236); at org.apache.spark.repl.Main$.doMain(Main.scala:78); at org.apache.spark.repl.Main$.main(Main.scala:58); at org.apache.spark.repl.Main.main(Main.scala); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.base/java.lang.reflect.Method.invoke(Method.java:566); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958); at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203); at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90); at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837:4056,deploy,deploy,4056,https://hail.is,https://github.com/hail-is/hail/issues/13837,9,['deploy'],['deploy']
Deployability,IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z build/deploy/dist/hail-0.2.128-py3-none-any.whl ']'; + echo WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo GITHUB_OAUTH_HEADER_FILE=abc123; GITHUB_OAUTH_HEADER_FILE=abc123; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo HAIL_GENETICS_HAIL_IMAGE=abc123; HAIL_GENETICS_HAIL_IMAGE=abc123; + for varname in '$arguments'; + '[' -z a ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; + for varname in '$arguments'; + '[' -z b ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; + for varname in '$arguments'; + '[' -z c ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=c; HAIL_GENETICS_HAILTOP_IMAGE=c; + for varname in '$arguments'; + '[' -z d ']'; + ec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:11978,deploy,deploy,11978,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy']
Deployability,IP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename scripts/release.sh; ++ basename scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'REMOTE is unset or empty'; REMOTE is unset or empty; + exit 1; make: *** [release] Error 1. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:15427,deploy,deploy-,15427,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,8,"['deploy', 'release']","['deploy-', 'release']"
Deployability,ITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:6508,deploy,deploy-,6508,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,"If CI jobs are to apply cleanup policies to the Artifact Registry, CI needs to have the permissions on the AR to do so. Prior to this change, CI only had viewer on the AR and used a different service account named `gcr-push` to push images to the registry. I don't see the point in having this second service account and think that CI should have these permissions (because it basically does already if it can submit jobs with the `gcr-push` service account). So the main point of this PR is to give CI admin on the AR, and then we can follow up by deleting the `gcr-push` service account. Separately:. Not sure how this ever worked, but I needed to update the google terraform provider to get `cleanup_policy_dry_run = false`. That upgrade now made the DNS zone description mandatory so I added one. I thin the DNS zone is no longer used, but removing it is for another time. There was also for some reason a duplicate resource for CI's viewer role on the AR repo, so I changed one to admin and deleted the other. cc @ehigham",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14266:650,update,update,650,https://hail.is,https://github.com/hail-is/hail/pull/14266,2,"['update', 'upgrade']","['update', 'upgrade']"
Deployability,"If a job is ""Running"", but the pod will always enter ""CrashLoopBackOff"" (due to a bad image), it will never finish. If a batch is deleted, the job is not marked cancelled so it is returned at the top of the refresh loop, but when the updated job is looked up using `undeleted` records, it is missing. This causes an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6737:234,update,updated,234,https://hail.is,https://github.com/hail-is/hail/issues/6737,1,['update'],['updated']
Deployability,"If the yaml sets the step to `runIfRequired: true`, this change should only run the step if specifically asked for when dev deploying.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7722:124,deploy,deploying,124,https://hail.is,https://github.com/hail-is/hail/pull/7722,1,['deploy'],['deploying']
Deployability,"If there are live instances, batch2 won't startup. Instances get the instance pool via the app, so it needs to be set before calling InstancePool.async_init. Also, convert cores and memory to strings in pipeline. The batch2 API requires a string for cpu.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7442:203,pipeline,pipeline,203,https://hail.is,https://github.com/hail-is/hail/pull/7442,1,['pipeline'],['pipeline']
Deployability,"If this passes, I'll update the setup.py",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6214#issuecomment-497073906:21,update,update,21,https://hail.is,https://github.com/hail-is/hail/pull/6214#issuecomment-497073906,1,['update'],['update']
Deployability,"If we always mount a deploy config (which sounds like a great idea), I think it would be nice to also always mount the user tokens too (similar to how the GSA key is always mounted). That way, nested batches ""just work"". But happy to make it a configurable option, if you prefer that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-768605979:21,deploy,deploy,21,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-768605979,1,['deploy'],['deploy']
Deployability,"If you dev deploy right now you'll likely see warnings like this:. ```; (hail) dgoldste@wmce3-cb7 hail % hailctl dev deploy -b hail-is/hail:main -s merge_code; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x7fb64d58f1f0>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x7fb64d5a1520>, 1.908669784)]']; connector: <aiohttp.connector.TCPConnector object at 0x7fb64d579040>; Created deploy batch, see https://ci.hail.is/batches/7992015; (hail) dgoldste@wmce3-cb7 hail %; ```. `HailCredentials` recently changed such that now they contain resources (GCP or Azure credentials) that require closing, so `hail_credentials()` needs to be used as a context manager or you get those unclosed errors on exit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13602:11,deploy,deploy,11,https://hail.is,https://github.com/hail-is/hail/pull/13602,3,['deploy'],['deploy']
Deployability,"If you sort the GitHub PRs by `is:pr is:open sort:updated-desc`, you'll see that some PRs which have merge conflicts are continuously tested by CI. Really old PRs that are stuck in this state reach a GitHub quota for status updates to a PR for a given SHA. It seems to me that we shouldn't retest any PR that is `source_sha_failed`, and I think this should resolve that particular issue, but am not sure if this is necessarily the best place to insert this logic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10654:50,update,updated-desc,50,https://hail.is,https://github.com/hail-is/hail/pull/10654,3,"['continuous', 'update']","['continuously', 'updated-desc', 'updates']"
Deployability,Implement java version checker in hail.init() and fail if wrong version of java installed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4896:80,install,installed,80,https://hail.is,https://github.com/hail-is/hail/issues/4896,1,['install'],['installed']
Deployability,"Implemented image untagging for image cleanup steps (like is done in GCR) for Azure. Since old layers still should be used for caching, this just removes the tag used for an image in a test build. We can then do something like [here](https://docs.microsoft.com/en-us/azure/container-registry/container-registry-auto-purge#run-in-an-on-demand-task) where you can purge untagged layers that are older than some number of weeks where we believe they're no longer relevant to the layer cache. I also switched out the `registry-push-credentials` that CI uses to build images from the ACR admin login to CI's service principal and eliminated the admin login from the ACR terraform resource. I dev deployed CI and manually verified after a deploy that a tag that was cleaned up no longer showed up in acr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11100:691,deploy,deployed,691,https://hail.is,https://github.com/hail-is/hail/pull/11100,2,['deploy'],"['deploy', 'deployed']"
Deployability,Improve deploy logic,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6042:8,deploy,deploy,8,https://hail.is,https://github.com/hail-is/hail/pull/6042,1,['deploy'],['deploy']
Deployability,"In `TableMapRows.execute`, we have the following code at the top of a function that gets passed to `tableMapPartitions`:. ```; val globalRegion = ctx.freshRegion; val globals = if (rowIterationNeedsGlobals); globalsBc.value.readRegionValue(globalRegion); else 0; ```. The problem with the above is that even though the globals were only broadcasted once, they could be read into memory multiple times, and don't get cleaned up until the end of the partition where spark closes the context. Imagine a pipeline that alternated between calling map rows and filtering (the filterings prevent the adjacent maps from being simplified into one). Every time we called map, we'd run the above code, reading a new copy of the globals into a new `globalRegion` in a way that won't be cleaned up until the end of the partition. . As per Cotton: You can fix this by having `SerializedRegionValue` (which is the thing being broadcast in `globalsBC`) hold on to a pointer that is returned for each user, but has to get closed by the same callback that's clearing the contexts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8244:500,pipeline,pipeline,500,https://hail.is,https://github.com/hail-is/hail/issues/8244,1,['pipeline'],['pipeline']
Deployability,"In a new environment,; ```; cd hail; make install; make pytest; ```; fails with; ```; ...; ERROR: usage: setup.py [options] [file_or_dir] [file_or_dir] [...]; setup.py: error: unrecognized arguments: --instafail --self-contained-html --html=../build/reports/pytest.html; inifile: None; rootdir: /path/to/hail/hail/python; ```. because the pytest plugins in hail/python/dev-requirements.txt are not installed. This documents the need to install them before running tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7942:42,install,install,42,https://hail.is,https://github.com/hail-is/hail/pull/7942,3,['install'],"['install', 'installed']"
Deployability,"In a world without gsa keys and hail auth tokens in secrets, the only other k8s secrets that we sometimes add to the job spec are the user ssl config which I believe to be unused (will address in a separate PR) and the `worker-deploy-config` secret. I don't really get why this is in a secret though, this is information already known to the worker and we can effortlessly add this to every job. Not only that, we can write the deploy-config once and mount it as read-only in all containers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13056:227,deploy,deploy-config,227,https://hail.is,https://github.com/hail-is/hail/pull/13056,2,['deploy'],['deploy-config']
Deployability,"In a89d64a, I modified `build.yaml` to release the wheel we had already built and tested. Unbeknownst to me was that we rebuild the wheel with a different version of `hail/python/hailtop/hailctl/deploy.yaml` and releasing the version used for testing borked `hailctl dataproc` commands. To fix this, we'll rebuild the wheel but use the `jar` we've already built and tested. This is safe to do as far as I know because we don't bundle any information into the jar that depends on the make flag `DEPLOY_REMOTE`. Fixes: #14452",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453:39,release,release,39,https://hail.is,https://github.com/hail-is/hail/pull/14453,2,"['deploy', 'release']","['deploy', 'release']"
Deployability,"In addition to the main change:; - I made all the global configuration options editable now that we have flexible billing,; - We now check the worker cores and standing worker cores against the worker type to make sure they are valid (there is no highmem/highcpu-1).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9285#issuecomment-674977850:57,configurat,configuration,57,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674977850,1,['configurat'],['configuration']
Deployability,"In anticipation of changes to the CI system, I've moved the local tutorial files to `/usr/local/hail-tutorial-files` and updated the CI configuration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1374#issuecomment-279443114:121,update,updated,121,https://hail.is,https://github.com/hail-is/hail/pull/1374#issuecomment-279443114,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"In fact I had Firth mixed into this branch but ripped it out when it was making the update too complicated. Whereas Wald, LRT, and score only require fitting the null model once, the Firth LRT requires fitting the null and full models per variant. So plan is to add Firth, support for subsetting samples per variant (rather than imputing missing genotypes), and better tests by comparing Hail and R results for randomly generated datasets. I'd also like to add more [optional] user control on convergence criteria and on what's returned in annotations (for example, statistics for the other covariates...these are computed anyway...also on the null fit in globals). And there are ways to speed up the numerical linear algebra, this is a first pass. Do you have thoughts on Firth LRT versus Wald? My understanding is that LRT is better calibrated for p-value, but would the Wald standard error for Firth be a useful annotation as well? Also, check out v1 of doc: ; https://github.com/jbloom22/hail/blob/jb_logreg3/docs/LogisticRegression.md",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/585#issuecomment-239686959:84,update,update,84,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239686959,1,['update'],['update']
Deployability,"In fact, changing `px.t * dpa` to `(dpa.t * px).t` also resolves the bug, without the need to copy `px`. Updated accordingly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4229#issuecomment-416813716:105,Update,Updated,105,https://hail.is,https://github.com/hail-is/hail/pull/4229#issuecomment-416813716,1,['Update'],['Updated']
Deployability,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9259:634,configurat,configuration,634,https://hail.is,https://github.com/hail-is/hail/pull/9259,1,['configurat'],['configuration']
Deployability,"In noticed this was missing when I tried to run the benchmarks, which use pipeline.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7018:74,pipeline,pipeline,74,https://hail.is,https://github.com/hail-is/hail/pull/7018,1,['pipeline'],['pipeline']
Deployability,"In order to start using Google or AAD access tokens instead of hail-minted tokens, we need to be able to identify a service account in the system by its UID at their identity provider. In Google, this UID is the `uniqueId` field of the Service Account. In AAD, it is the Service Principal Object ID. In an upcoming change, we'll update the user creation process to add this ID upon user creation, at which point we will be able to safely remove this code that updates existing records. I've marked this PR as `full-deploy` so the AUS folks can roll this commit out specifically before this column is relied on. This way we can safely remove the loop in a follow-up PR and know they will have run this code to populate the column.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13207:329,update,update,329,https://hail.is,https://github.com/hail-is/hail/pull/13207,3,"['deploy', 'update']","['deploy', 'update', 'updates']"
Deployability,"In particular, a sub-pipeline executed multiple times as part of a larger pipeline must be deterministic.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4017:21,pipeline,pipeline,21,https://hail.is,https://github.com/hail-is/hail/issues/4017,2,['pipeline'],['pipeline']
Deployability,"In particular, batch leaves its pods around when it gets killed by a deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5665:69,deploy,deploy,69,https://hail.is,https://github.com/hail-is/hail/issues/5665,1,['deploy'],['deploy']
Deployability,"In particular, isn't it possible that you have the n-1th and nth job racing to complete. Everyone else is already done. Call the n-1th job's transaction T1 and the nth job's transaction T2. Both race down to this statement in MJC:; ```; UPDATE batches; SET time_completed = new_timestamp,; `state` = 'complete'; WHERE id = in_batch_id AND n_completed = batches.n_jobs;; ```. That will now need to have a sum(n_completed) over all tokens. The isolation level is repeatable read. Assume T1 and T2 generate non equal tokens. T1 and T2 may both snapshot the state of the database before either T1 or T2 executes. T1 and T2 will necessarily see the changes they've made (which affect distinct rows because they have distinct tokens), but neither is required to see the changes the other has made. I think the only way to guarantee that at least one of T1 or T2 sees the database with sum(n_completed) == n_jobs is for both of them to LOCK IN SHARE MODE when doing the sum(n_completed). That will cause lock contention. Maybe that's OK? In the worst case you could have this happen:. 1. Job 1 executes all the way to just before the sum(n_completed).; 2. Job 2 executes all the way to modifying the volatile state.; 3. Job 1 blocks waiting for Job 2 to modify the volatile state.; 4. Job 3 executes all the way to modifying the volatile state.; 5. Job 1 and 2 now wait for Job 3 to modifying the volatile state.; 6. ...; 7. Job 1, 2, 3, n-1 now all wait for Job n to modify the volatile state.; 8. Job 1...n finally execute the sum, all in parallel. I guess that's not terrible, it just means that the latency of Job 1 is extended as long as other jobs can race in before it grabs a shared lock on all the rows.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039642916:237,UPDATE,UPDATE,237,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039642916,1,['UPDATE'],['UPDATE']
Deployability,"In particular, it should link to the index.html for the deploy,e.g. `gs://hail-ci-0-1/deploy/f69da9402030173e68b33608f38d94c45a1d7928/index.html`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4437#issuecomment-424558979:56,deploy,deploy,56,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-424558979,2,['deploy'],['deploy']
Deployability,"In response to #7299. It would be nice to get to a more automated way of doing this, but for now we should just update this assuming it passes all tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7301:112,update,update,112,https://hail.is,https://github.com/hail-is/hail/pull/7301,1,['update'],['update']
Deployability,"In the spirit of consolidation, this is just translating the current nginx config that is handling TLS for the batch driver to envoy. Doesn't have any special dependencies or deployment complications. Note that this time running Envoy as root is required to listening on the same port (443) that nginx currently listens on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12383:175,deploy,deployment,175,https://hail.is,https://github.com/hail-is/hail/pull/12383,1,['deploy'],['deployment']
Deployability,"In this PR:. - I add `BlockMatrix.from_ndarray`. The implementation isn't great, it just basically just evals the ndarray and adds NDArray support to `ValueToBlockMatrix`. A better version wouldn't cross the Python / Java boundary at all, but I want something that works on all backends, so for now this will have to suffice. Any solution will at least need to communicate the shape of the ndarray back to python, since it's tracked in the block matrix type. ; - With this new method, I can now get many tests in `test_linalg.py` to run on the local / service backends. Most BM lowering was apparently untested before, so some bug fixes were necessary, including:; - Support requiredness analysis on BlockMatrix, even though the answer is always required. ; - Use `CompileAndEvaluate` rather than `Interpret` to evaluate the child node in `ValueToBlockMatrix`. ; - Casting between Int32 and Int64 in various places in lowering. Almost always the culprit was a bad interaction between ndarray shapes (which are Int64) and `StreamRange` argument (which is an Int32). This is sort of a pervasive ndarray problem that will need to be systematically addressed at some point. I don't anticipate anyone making a BlockMatrix with blocks big enough to blow 32 bits though. ; - Lots of fixes to the `BlockMatrixBroadcast` rule for getting diagonal of a BlockMatrix, as it was clearly never run. It had `MakeStream(StreamIR)` which was not allowed, it didn't update the context appropriately, and it used the wrong axis to determine if something was a row vector.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10447:1448,update,update,1448,https://hail.is,https://github.com/hail-is/hail/pull/10447,1,['update'],['update']
Deployability,"In trying to test this (from your branch, ran pip install on /hail/python just in case). ```sh; (hail) alex:~/projects/hail/hail/python:$ hailctl dev deploy -b cseed:batch-web -s deploy_auth,deploy_router,deploy_notebook2; Traceback (most recent call last):; File ""/miniconda3/envs/hail/bin/hailctl"", line 11, in <module>; sys.exit(main()); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/__main__.py"", line 100, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/cli.py"", line 52, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 66, in main; loop.run_until_complete(submit(args)); File ""/miniconda3/envs/hail/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete; return future.result(); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 57, in submit; batch_id = await ci_client.dev_deploy_branch(args.branch, steps); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 46, in dev_deploy_branch; self._deploy_config.url('ci', '/api/v1alpha/dev_deploy_branch'), json=data) as resp:; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 1005, in __aenter__; self._resp = await self._coro; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 581, in _request; resp.raise_for_status(); File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; headers=self.headers); aiohttp.client_exceptions.ClientResponseError: 500, message='Internal Server Error'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733:50,install,install,50,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733,5,"['deploy', 'install']","['deploy', 'install']"
Deployability,"Include the call-stack of the compiler when we emit assertions into generated code. This is useful for showing us the trace of the code that emitted a `.get` on `IEmitCode`, for example. To do this, add a `hailBuildConfiguration` enum {`release`|`debug`} into `build-info.properties`, parsed as `HAIL_BUILD_CONFIGURATION`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14372:237,release,release,237,https://hail.is,https://github.com/hail-is/hail/pull/14372,1,['release'],['release']
Deployability,"Includes the logistic case of the SKAT algorithm. Changes include updates to the SKAT, SKATSuite, and SKATmodel Scala files including updating python tests and other documentation. . Another significant change in the code is that the tests in SKATSuite have been re-written to ensure less code is duplicated while also added in permutation and noise tests for the skat algorithm which are not apart of the CI testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2153:66,update,updates,66,https://hail.is,https://github.com/hail-is/hail/pull/2153,1,['update'],['updates']
Deployability,"Increasing the executor memory per core to 20G/core seemed to help get by this memory error. . It would be useful to have some rule of thumbs for estimating memory requirements based on number of samples and variants. spark-submit --verbose --master yarn --deploy-mode client \; --num-executors 12\; --executor-cores 4\; --jars $JAR \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; --conf ""spark.driver.extraClassPath=$JAR"" \; --conf ""spark.executor.extraClassPath=$JAR"" \; --executor-memory 80G\; --driver-memory 60g\; --driver-cores 1\; --name ""$1"" \; --conf spark.yarn.executor.memoryOverhead=8000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3463#issuecomment-385433303:257,deploy,deploy-mode,257,https://hail.is,https://github.com/hail-is/hail/issues/3463#issuecomment-385433303,1,['deploy'],['deploy-mode']
Deployability,"Information below. It isn't totally clear what to do here. I think the k8s refresh loop should probably restart pods (mark_unscheduled) that have been scheduled but aren't running after a timeout (few mins). ```; $ kubectl -n batch-pods describe pods batch-3-job-41-39d17b; Name: batch-3-job-41-39d17b; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-1f89/10.128.0.101; Start Time: Fri, 12 Jul 2019 13:17:15 -0400; Labels: app=batch-job; batch_id=3; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; job_id=41; task=main; user=ci; uuid=f53f127847864f1cbf7d4bdc911a6646; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: gcr.io/hail-vdc/ci-intermediate:oyyg6y2um4kx; Image ID: ; Port: <none>; Host Port: <none>; Command:; bash; -c; set -e; gcloud -q auth activate-service-account --key-file=/test-gsa-key/privateKeyData; gsutil -m cp -r /test/resources/* gs://hail-test-1c9nm/sj0nb47zqys1/pipeline/input/; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 100m; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-3-job-41-39d17b (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /test-gsa-key from test-gsa-key (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age F",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6625:990,pipeline,pipeline,990,https://hail.is,https://github.com/hail-is/hail/issues/6625,1,['pipeline'],['pipeline']
Deployability,"Inheritance + dependsOnMethods (Krishnan Mahadevan)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/cbeust/testng/commit/b94395dea479308ea3fe825269730b960f44d805""><code>b94395d</code></a> Bump version to 7.7.1 for release</li>; <li><a href=""https://github.com/cbeust/testng/commit/89dc5845fcb46c26af187e50ea907a7382d06e72""><code>89dc584</code></a> Streamline overloaded assertion methods for Groovy</li>; <li><a href=""https://github.com/cbeust/testng/commit/5ac0021d14f7eb00804fe235aaefc5c2fbce57d1""><code>5ac0021</code></a> Adding release notes</li>; <li><a href=""https://github.com/cbeust/testng/commit/c0e1e772f1fc0ab2142f3a4114a2b8cfe60fa7e1""><code>c0e1e77</code></a> Adjust version reference in deprecation msgs.</li>; <li><a href=""https://github.com/cbeust/testng/commit/011527d9bf0f91a40539f5e5467cc106888810d9""><code>011527d</code></a> Bump version to 7.7.0 for release</li>; <li><a href=""https://github.com/cbeust/testng/commit/7846c444a411647f7e401a097224702188c93835""><code>7846c44</code></a> Deprecate support for running JUnit tests</li>; <li><a href=""https://github.com/cbeust/testng/commit/8630a7e8fe12985d71c00212f9362fd38fb0cb9e""><code>8630a7e</code></a> Ensure ITestContext available for JUnit4 tests</li>; <li><a href=""https://github.com/cbeust/testng/commit/7070b020def0089d0d9dc695a5762ad16e974ce6""><code>7070b02</code></a> Streamline dependsOnMethods for configurations</li>; <li><a href=""https://github.com/cbeust/testng/commit/d7e0bb1cbcd7933d34d704678e75cbaf42704505""><code>d7e0bb1</code></a> Deprecate support for running Spock Tests</li>; <li><a href=""https://github.com/cbeust/testng/commit/ca7a3a293008389096be75fea4936af8e5f79650""><code>ca7a3a2</code></a> Ensure All tests run all the time</li>; <li>Additional commits viewable in <a href=""https://github.com/cbeust/testng/compare/testng-6.8.21...7.7.1"">compare view</a></li>; </ul>; </details>; <br />. [![D",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:15575,release,release,15575,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['release'],['release']
Deployability,Installation of 0.1 works; `git clone -b 0.1 https://github.com/broadinstitute/hail.git`; The only thing to add is that you have to do `chmod u+x gradlew` in order to `$ ./gradlew -Dspark.version=2.0.2 shadowJar`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2067#issuecomment-320256546:0,Install,Installation,0,https://hail.is,https://github.com/hail-is/hail/issues/2067#issuecomment-320256546,1,['Install'],['Installation']
Deployability,"Installing certbot is hard so I stopped doing that. Instead, I use the cerbot docker image. I also eliminated `sed` use in the letsencrypt directory. We now maintain the options-ssl-nginx.conf file ourselves. I copied the settings from certbot; GitHub. They're straightforward, as a part of regular package versioning maintenance we should also; reconsider our cipher suites and TLS versions. We now have a `make run DRY_RUN=true` option which can be run repeatedly without affecting the default certs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9971:0,Install,Installing,0,https://hail.is,https://github.com/hail-is/hail/pull/9971,1,['Install'],['Installing']
Deployability,Installing from PyPI leads to a broken install,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14630:0,Install,Installing,0,https://hail.is,https://github.com/hail-is/hail/issues/14630,2,"['Install', 'install']","['Installing', 'install']"
Deployability,"Installing hail from the whl is going to break this cache every time so we end up installing the gcloud sdk on every build, which takes 40 seconds. It'd be nice to also move the extra pip packages further up the image but I think they're ordered this way so that pip gives us hail-compatible versions. I think the best fix would be to track these packages in a requirements.txt that is pinned and constrained on the hail package's requirements.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12301:0,Install,Installing,0,https://hail.is,https://github.com/hail-is/hail/pull/12301,2,"['Install', 'install']","['Installing', 'installing']"
Deployability,"Instead of using `gsutil` we use hailtop.aiotools.copy from the new `HAILGENETICS_HAIL_IMAGE`. Previously, deploying the pip-versioned image was a manual asynchronous step that mostly happened in response to user requests. 1. Actually build and test hailgenetics/hail and hailgenetics/genetics on every build.; 2. On deploy, push the newly built hailgenetics/hail and hailgenetics/genetics images to both docker hub and gcr.io/hail-vdc/; 3. Provide the built-for-this-PR hailgenetics/hail as an env var to the tests.; 4. By default use the hailgenetics/hail image for the currently published pip version for FS operations. Allow overriding by environment variable.; 5. Remove now unnecessary publish-public-images.sh.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11091:107,deploy,deploying,107,https://hail.is,https://github.com/hail-is/hail/pull/11091,2,['deploy'],"['deploy', 'deploying']"
Deployability,Integrate OrderedRDD2 in VSM.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2308:0,Integrat,Integrate,0,https://hail.is,https://github.com/hail-is/hail/pull/2308,1,['Integrat'],['Integrate']
Deployability,Integrate show with jupyter,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5663:0,Integrat,Integrate,0,https://hail.is,https://github.com/hail-is/hail/issues/5663,1,['Integrat'],['Integrate']
Deployability,"Intention is two-fold:. - get ready to move the k8s cluster from broad-ctsa to hail-vdc; - automate as much of our infrastructure build out as possible. Ultimately, changing GCP or k8s infrastructure should involve pushing to something like vdc/. We should regularly test rebuild from scratch. Outline of changes:. - added a new project directory, vdc/; - parameterize projects by GCP project for GCR, set from gcloud project config; - parameterize site by domain name and IP address; - GCP resources are deployed using the Google Deployment Manager; - added a MySQL 5.6 instance (to be used by upload, others); - ugprades gke to latest version. Doesn't handle CI yet. I think we need a setting for CI where it runs the tests and tracks its internal state but doesn't do anything on Github.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4545:505,deploy,deployed,505,https://hail.is,https://github.com/hail-is/hail/pull/4545,2,"['Deploy', 'deploy']","['Deployment', 'deployed']"
Deployability,"Interesting: I tried in a new session (after checking out master, installing-editable), and this time I could break things with out-of-bounds slices, but `hl.eval(a[0:a.shape[0],0:1]) ` and `hl.eval(a[0:a.shape[0],0:2])` worked ok. So what's going on?. ```python; In [22]: hl.eval(a[0:a.shape[0],0:1]) ; Out[22]: ; array([[1],; [2],; [3],; [4],; [5]], dtype=int32). In [23]: hl.eval(a[0:a.shape[0],0:1]) ; Out[23]: ; array([[1],; [2],; [3],; [4],; [5]], dtype=int32). In [24]: a = a.T . In [25]: hl.eval(a) ; Out[25]: ; array([[ 1, 2, 3, 4, 5],; [ 7, 6, 8, 9, 10]], dtype=int32). In [26]: a = a.T . In [27]: hl.eval(a) ; Out[27]: ; array([[ 1, 7],; [ 2, 6],; [ 3, 8],; [ 4, 9],; [ 5, 10]], dtype=int32). In [28]: hl.eval(a[0:a.shape[0],0:1]) ; Out[28]: ; array([[1],; [7],; [0],; [2],; [0]], dtype=int32). In [32]: hl.eval(a[0:a.shape[0],0:1]) ; Out[32]: ; array([[ 1],; [ 7],; [ 0],; [ 4],; [32749]], dtype=int32); ``` . totally broken. Seems like 2 problems: 1) out of bounds checks not being done on inner dimension. 2) strides get scrambled between transposes, or something isn't being cleared after transposition.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9144#issuecomment-663267587:66,install,installing-editable,66,https://hail.is,https://github.com/hail-is/hail/issues/9144#issuecomment-663267587,2,['install'],['installing-editable']
Deployability,Interval upgrade,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1443:9,upgrade,upgrade,9,https://hail.is,https://github.com/hail-is/hail/pull/1443,1,['upgrade'],['upgrade']
Deployability,Is it possible to check this with dev deploy?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10190#issuecomment-799425185:38,deploy,deploy,38,https://hail.is,https://github.com/hail-is/hail/pull/10190#issuecomment-799425185,1,['deploy'],['deploy']
Deployability,"Is it possible to update the function ; `hl.plot.histogram`; to not require legend, and/or to use explicit legend field as mention in BokehJS warning. * Hail version 0.2.32-a5876a0a2853; * BokehJS 2.0.0; * Sniplet; ```py; p = hl.plot.histogram(; ht.meta.age,; range=(0, 100),; bins=10,; legend='age',; title='Age distribution'; ); show(p); ```; * Output. ```sh; # If no `legend` argument; ValueError: Bad 'legend' parameter value: None; ```; ```sh; # If `legend` argument prvided; BokehDeprecationWarning: 'legend' keyword is deprecated, use explicit 'legend_label', 'legend_field', or 'legend_group' keywords instead; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8318:18,update,update,18,https://hail.is,https://github.com/hail-is/hail/issues/8318,1,['update'],['update']
Deployability,"Is there a possibility of adding an exception for users who need to pass a SparkContext to `hl.init`? . In our use case, our notebooks use the sparkmagic kernel to communicate with livy running on multiple clusters. Sparkmagic automatically creates a SparkContext when connecting to livy on the cluster master node (AWS EMR). And in releases prior to 0.2.20 we were passing that SparkContext to `hl.init`. . In our case, the JAR is in our class path.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7080#issuecomment-536709685:333,release,releases,333,https://hail.is,https://github.com/hail-is/hail/issues/7080#issuecomment-536709685,1,['release'],['releases']
Deployability,Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded? Maybe have k8s get the deployment yaml configuration before deleting each deployment and then reapply the yamls for each shutdown step in order. I'm worried this PR will break all dev deploys with migrations in them as you can't just do `make -c batch deploy`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7855#issuecomment-573683956:102,deploy,deployments,102,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573683956,6,"['configurat', 'deploy']","['configuration', 'deploy', 'deployment', 'deployments', 'deploys']"
Deployability,Is this high prio enough to get in before we release 0.2.26?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7368#issuecomment-545942557:45,release,release,45,https://hail.is,https://github.com/hail-is/hail/pull/7368#issuecomment-545942557,1,['release'],['release']
Deployability,"Is this still an issue without command line? If so, can we update the issue to python terms?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/322#issuecomment-291943984:59,update,update,59,https://hail.is,https://github.com/hail-is/hail/issues/322#issuecomment-291943984,1,['update'],['update']
Deployability,"Issue I ran into: I need to pass each child IR's ptype to the join point loop body. There isn't a very easy way to get one PType out of an IndexedSeq[PType] of ptypes. For instance, even though srvb.advance() runs inside the body of the JoinPoint loop, its staticIdx does not update (since the loop only iterates at runtime). I want to pass Code[IndexedSeq[PType]] but that isn't possible. Will work on tomorrow. . edit:. I think I need to do something like this to access individual ptypes (but within toEmitTriplet loop body): . ```scala; case x@MakeStream(elements, t) =>; val e = coerce[PStreamable](x.pType).elementType; implicit val eP = TypedTriplet.pack(e); sequence(elements.map { ir => TypedTriplet(e, emitIR(ir, env)) }); .map(_.untyped); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583963692:276,update,update,276,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583963692,1,['update'],['update']
Deployability,"It adds the last applied configuration annotation which is used to know if a subsequent configuration is different. ```; (base) # kubectl -n default create secret generic foo \ ; --save-config --dry-run=client -o yaml; apiVersion: v1; kind: Secret; metadata:; annotations:; kubectl.kubernetes.io/last-applied-configuration: |; {""kind"":""Secret"",""apiVersion"":""v1"",""metadata"":{""name"":""foo"",""creationTimestamp"":null}}; creationTimestamp: null; name: foo; namespace: default; (base) # kubectl -n default create secret generic foo \; --dry-run=client -o yaml ; apiVersion: v1; kind: Secret; metadata:; creationTimestamp: null; name: foo; namespace: default. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10217#issuecomment-809406141:25,configurat,configuration,25,https://hail.is,https://github.com/hail-is/hail/pull/10217#issuecomment-809406141,3,['configurat'],['configuration']
Deployability,"It doesn't build out of the box, but it should. That just needs py4j and breeze version numbers. Separately, we should probably upgrade cloudtools to Dataproc 1.3-deb9 which has Spark 2.3.1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4578#issuecomment-431608757:128,upgrade,upgrade,128,https://hail.is,https://github.com/hail-is/hail/issues/4578#issuecomment-431608757,1,['upgrade'],['upgrade']
Deployability,"It doesn't seem like headless mode is in effect, at least in the most recent published image. Will grab this and play around with it. Tested Dan's image in app.hail.is, seems to work, except for all of the .js/.css resources; first guess is SSL, but it's clearly a diff issue. I can't connect to your workers, can to his. Will update in a bit. Yours:; (notebook) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?acce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942:327,update,update,327,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942,2,['update'],['update']
Deployability,"It is able to execute a trivial pipeline without the JVM on the client. The countdown down to a fully functional Hail Query service begins now. I will start running the Python tests against the service to benchmark our progress. The main blockers are:; - Table lowering @tpoterba @patrick-schultz @catoverdrive ; - The shuffle service @tpoterba @danking ; - Reading, writing and threading the (per-user, for the query service) filesystem through execution. I'll be working on this.; - A Batch backend for distributed execution. I will do this once there is enough functionality to execute something big/interesting. It's happening!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8468:32,pipeline,pipeline,32,https://hail.is,https://github.com/hail-is/hail/pull/8468,1,['pipeline'],['pipeline']
Deployability,"It is mighty fishy that both azure and google failed the callback test. What are we missing? If MJC returns, then the database was clearly updated. Subsequent DB queries should see those changes. total_jobs_in_batch won't change during the lifetime of the batch, so that should be correct (though we should probably LOCK IN SHARE MODE anyway). Assuming I'm reading the [reference manual](https://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html) correctly, that select should see the result of the UPDATE *or a later state*. The updates to a single row are serial. So there must exist a transaction that takes it from n_jobs-1 to n_jobs. That transaction thus must see n_jobs for new_n_completed. That transaction thus ought to update batches. Once that transaction is committed the subsequent query for notification should see the changes...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1040809121:139,update,updated,139,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1040809121,7,"['UPDATE', 'update']","['UPDATE', 'update', 'updated', 'updates']"
Deployability,"It is not free and we get emails about having too many page views pretty frequently. I suspect this is due to the jobs page having a download icon. The font provided by Google with its material design icons seems to be free to access at any scale. I got the GitHub octocat from GitHub's website. It's a bit bigger. <img width=""1130"" alt=""this PR"" src=""https://github.com/hail-is/hail/assets/106194/31e1cc67-ce9f-4e1f-a6b2-64258a8596c0"">; <img width=""1130"" alt=""main"" src=""https://github.com/hail-is/hail/assets/106194/ce9cc44d-3332-4b88-b733-4ac46a9f8e16"">. I didn't actually dev deploy batch to check the other assets but I suspect they're fine enough. This is what the question mark in a circle looks like: https://fonts.google.com/icons?selected=Material%20Symbols%20Outlined%3Ahelp%3AFILL%400%3Bwght%40400%3BGRAD%400%3Bopsz%4024 And this is what the download icon looks like: https://fonts.google.com/icons?selected=Material%20Symbols%20Outlined%3Adownload%3AFILL%400%3Bwght%40400%3BGRAD%400%3Bopsz%4024",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14302:580,deploy,deploy,580,https://hail.is,https://github.com/hail-is/hail/pull/14302,1,['deploy'],['deploy']
Deployability,"It is probably sufficient to verify it runs successfully and produces an expected output. https://github.com/hail-is/hail/pull/3872 makes the VEP invocation much more flexible, including running dockerized VEP (although there's still the question of installing the data files). We should probably make similar changes to Nirvana. The Nirvana [wiki](https://github.com/Illumina/Nirvana/wiki/Getting-Started) says it runs ""in Docker"" but I a quick Google didn't turn up any images.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4021:250,install,installing,250,https://hail.is,https://github.com/hail-is/hail/issues/4021,1,['install'],['installing']
Deployability,"It looks good except for the behavior I was seeing. I can try redeploying my version if you think that's what the issue is (the edit page ""update"" doesn't return a page with the description I just added even though it's there on the resources page).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9684#issuecomment-726289411:139,update,update,139,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-726289411,1,['update'],['update']
Deployability,It looks like `gs://gcp-public-data--gnomad/papers/2019-tx-annotation/pre_computed/all.possible.snvs.tx_annotated.021520.ht` has been updated now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10169#issuecomment-820648738:134,update,updated,134,https://hail.is,https://github.com/hail-is/hail/pull/10169#issuecomment-820648738,1,['update'],['updated']
Deployability,"It looks like it is failing when trying to start Java. Do you have Java installed? What version? This is what I get on my mac:. ```; $ java -version; java version ""1.8.0_31""; Java(TM) SE Runtime Environment (build 1.8.0_31-b13); Java HotSpot(TM) 64-Bit Server VM (build 25.31-b07, mixed mode); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319673723:72,install,installed,72,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319673723,1,['install'],['installed']
Deployability,"It looks like it's called `netcdf` on `brew`. ```; dking@wmb16-359 # brew info netcdf; netcdf: stable 4.6.0 (bottled); Libraries and data formats for array-oriented scientific data; https://www.unidata.ucar.edu/software/netcdf; Not installed; From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/netcdf.rb; ==> Dependencies; Build: cmake ✘; Required: hdf5 ✘, gcc ✘; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3273#issuecomment-377930479:232,install,installed,232,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377930479,1,['install'],['installed']
Deployability,"It looks like permissions for deleting disks and VMs are broken for the `delete_batch_instances` CI step. This job also hung for a long time and then got restarted. There's some other wonky things about this PR, but it just seems like the main issue was the Batch deployment was cancelled mid-run and the driver didn't have time to cleanup those 2 VMs that weren't responding before being shut off. Then the cleanup step isn't actually working so they didn't get cleaned up. The only remaining question I have is why these VMs weren't starting up correctly. There were at least 5 in this one PR that didn't start up in time before the driver was shut down. https://batch.hail.is/batches/7908998/jobs/207",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13554#issuecomment-1737433569:264,deploy,deployment,264,https://hail.is,https://github.com/hail-is/hail/issues/13554#issuecomment-1737433569,1,['deploy'],['deployment']
Deployability,"It looks like the Parquet schema in the file stores the `nullable = false` (as a `required` field), but it's ignored when being read back. I ran the following on a Spark 2 shell (updated from the Stack Overflow question, note that the Parquet handling has been rewritten since then, see https://issues.apache.org/jira/browse/SPARK-9095):. ```scala; scala> import org.apache.spark.sql._; import org.apache.spark.sql._. scala> import org.apache.spark.sql.types._; import org.apache.spark.sql.types._. scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc); warning: there was one deprecation warning; re-run with -deprecation for details; sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@625f5712. scala> val schema = StructType(Seq(StructField(""foo"", IntegerType, false))); schema: org.apache.spark.sql.types.StructType = StructType(StructField(foo,IntegerType,false)). scala> val df1 = sqlContext.createDataFrame(sc.parallelize(Array(Row(1))), schema); df1: org.apache.spark.sql.DataFrame = [foo: int]. scala> df1.printSchema; root; |-- foo: integer (nullable = false). scala> df1.write.parquet(""temp.df1""); ; scala> val df2 = sqlContext.read.parquet(""temp.df1""); df2: org.apache.spark.sql.DataFrame = [foo: int]. scala> df2.printSchema; root; |-- foo: integer (nullable = true); ```. Then. ```bash; parquet-tools schema part-r-00000-94aa6aa3-4799-4b78-9717-5397c8e983f9.snappy.parquet; message spark_schema {; required int32 foo;; }; ```. Which shows that the field is `required`, not `optional`. Also. ```bash; parquet-tools meta part-r-00000-94aa6aa3-4799-4b78-9717-5397c8e983f9.snappy.parquet; creator: parquet-mr version 1.5.0-cdh5.7.0 (build ${buildNumber}) ; extra: org.apache.spark.sql.parquet.row.metadata = {""type"":""struct"",""fields"":[{""name"":""foo"",""type"":""integer"",""nullable"":false,""metadata"":{}}]} . file schema: spark_schema ; ----------------------------------------------------------------------------------------------------------------------------",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1421#issuecomment-281967861:179,update,updated,179,https://hail.is,https://github.com/hail-is/hail/pull/1421#issuecomment-281967861,1,['update'],['updated']
Deployability,"It looks like the configuration files for `xfs_quota` serve mostly to persist mappings from project name -> project id, and project id -> filesystem path. We keep that information in the worker anyway, and `xfs_quota` (way deep in its documentation) allows you to specify a path and project id in the project-creation command and then use a project id in the command that sets limits. This avoids locking on configuration files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10467:18,configurat,configuration,18,https://hail.is,https://github.com/hail-is/hail/pull/10467,2,['configurat'],['configuration']
Deployability,"It looks like you have two options:; 1. Install the Gradle ppa: https://launchpad.net/~cwchien/+archive/ubuntu/gradle; ; In a nutshell, uninstall the previous version of Gradle and then run:; ; ```; sudo add-apt-repository ppa:cwchien/gradle; sudo apt-get update; sudo apt-get install gradle-2.14.1; ```; 2. Download the the latest complete distribution of Gradle 2:; ; https://gradle.org/gradle-download/; ; Go to Previous Release and select 2.14.1 and download the complete distribution. Gradle is written in Java and it is pre-compiled. No need to build it. Run `gradle-2.14.1/bin/gradle` and you should be good to go.; ; Gradle 3 was just released a few days ago. We haven't tested against it, so I would recommend Gradle 2 for now.; ; I'll update the documentation to warn about Gradle 2.10. Let me know if either of these work for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240306249:40,Install,Install,40,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240306249,6,"['Install', 'Release', 'install', 'release', 'update']","['Install', 'Release', 'install', 'released', 'update']"
Deployability,"It seems not right to pin to a specific patch version of python 3.7. This is unnecessarily more generous, but seems fine.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7615:40,patch,patch,40,https://hail.is,https://github.com/hail-is/hail/pull/7615,1,['patch'],['patch']
Deployability,It seems that the latest version of Ubuntu doesn't have rsync installed. I need this for developing on Batch to get the sync.py script to work.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13087:62,install,installed,62,https://hail.is,https://github.com/hail-is/hail/pull/13087,1,['install'],['installed']
Deployability,"It seems the biggest remaining issue is that CI doesn't have access to hail imports when testing hailtop. Could we address this? Do you want a PR?. edit: nvm, patched here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-670044799:159,patch,patched,159,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-670044799,1,['patch'],['patched']
Deployability,"It sounds like this is the kind of configuration change that needs a separate tracker issue, so I've created https://github.com/hail-is/hail/issues/14718 to encompass all the various actions we'll need to take. That issue can also cover the impact assessment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14717#issuecomment-2400754176:35,configurat,configuration,35,https://hail.is,https://github.com/hail-is/hail/pull/14717#issuecomment-2400754176,1,['configurat'],['configuration']
Deployability,"It takes one minute to build the docs *even if nothing has changed since the; last build*. There are a few things that lengthen the feedback cycle:. - We defeat Sphinx's input cache by deleting and re-copying over all the source; files.; - We defeat Sphinx's output cache by `mv`ing the output to a new location.; - We check that Hail is installed (at a cost of two seconds) *every* time we; build the docs. This isn't necessary, Sphinx prints a reasonable message; (""cannot import ..."") if Hail is not installed.; - We create a wheel file every time we build the docs at a cost of several; seconds.; - We recreate the tutorials tar even if it has not changed. Instead, I propose this PR:. - Do not copy the source files.; - Copy the output to the new location.; - Do not check hail is installed.; - Do not even install Hail.; - Use Make to check if the tutorial tar need be recreated. Regarding not installing Hail: even install-editable takes two seconds. It is; the developer's responsibility to ensure the right version of Hail is; installed. When you check out a branch just run `make install-editable`; once. Then edit the docs to your heart's desire, never re-install Hail. With this PR it takes ~3.5 seconds to rebuild the docs if nothing has; changed. We do work proportional to the number of changed files, not; proportional to all files. Sphinx itself takes 2-3 seconds, so we can't do much; better than this. Dice came up Patrick, but I imagine @tpoterba has thoughts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9348:338,install,installed,338,https://hail.is,https://github.com/hail-is/hail/pull/9348,9,['install'],"['install', 'install-editable', 'installed', 'installing']"
Deployability,It was an old worker that didn't have the idempotent updates. The error was in create.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8193#issuecomment-592582922:53,update,updates,53,https://hail.is,https://github.com/hail-is/hail/pull/8193#issuecomment-592582922,1,['update'],['updates']
Deployability,It was unused / never updated.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10408:22,update,updated,22,https://hail.is,https://github.com/hail-is/hail/pull/10408,1,['update'],['updated']
Deployability,"It's a successful build for a previous version of master. I'm working on a PR to note when ci2 builds are out of date. We only update on approved PR at a time, so it is in a queue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6028#issuecomment-491016361:127,update,update,127,https://hail.is,https://github.com/hail-is/hail/pull/6028#issuecomment-491016361,1,['update'],['update']
Deployability,"It's already deployed, so, I guess that's how I tested it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9391#issuecomment-685116503:13,deploy,deployed,13,https://hail.is,https://github.com/hail-is/hail/pull/9391#issuecomment-685116503,1,['deploy'],['deployed']
Deployability,"It's interesting, this reading problem is basically a little configuration language, but somehow it's really hard to write the performance we want without lots of code duplication.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1778#issuecomment-300169255:61,configurat,configuration,61,https://hail.is,https://github.com/hail-is/hail/pull/1778#issuecomment-300169255,1,['configurat'],['configuration']
Deployability,It's just a string so `json.loads` fails on it. Not sure why I did that anyway. This has been broken on CI for a bit now. CI still manages fine because it checks everything on an interval but the callback helps it respond immediately to when batches finish for a PR test or deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12631:274,deploy,deploy,274,https://hail.is,https://github.com/hail-is/hail/pull/12631,1,['deploy'],['deploy']
Deployability,"It's not clear to me what the change that broke this was. Must have been when we added the pipeline docs, but I don't see what used to be calling `upload-docs`. Anyway, calling it now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8196:91,pipeline,pipeline,91,https://hail.is,https://github.com/hail-is/hail/pull/8196,1,['pipeline'],['pipeline']
Deployability,"It's not so much so that **we** can check out a tagged release, as we have already worked around the problem. But I would expect that you and any other installations will also run into the same `batch_worker_image` failure. We have been running our production instance using gcsfuse 1.2.0 for about a week now, and I think @illusional will agree with me that we haven't seen any problems from it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13728#issuecomment-1769212622:55,release,release,55,https://hail.is,https://github.com/hail-is/hail/pull/13728#issuecomment-1769212622,2,"['install', 'release']","['installations', 'release']"
Deployability,"It's not the data, it's a complicated pipeline to replicate. I can try harder.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7455#issuecomment-549889843:38,pipeline,pipeline,38,https://hail.is,https://github.com/hail-is/hail/pull/7455#issuecomment-549889843,1,['pipeline'],['pipeline']
Deployability,"It's obvious that I'm the only one using SBT 😉 . I removed this spark helper thing that data bricks has abandoned. It's not hard to specify the right spark dependencies manually. In fact, we do that in `build.gradle` already. I don't know what the deal with hadoopClient, but it didn't seem necessary for my tests to pass. We don't use SBT for deployment, so I'm not worried. I'm not sure how all the http4s and json4s stuff got pulled in. They're not present in grade, so I removed them. I also bumped the SBT version for no particular reason. 🤷‍♀ . It works.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8357:344,deploy,deployment,344,https://hail.is,https://github.com/hail-is/hail/pull/8357,1,['deploy'],['deployment']
Deployability,"It's quite cumbersome to have to check whether everything is defined along the way, e.g. in a pipeline like this:; ```; tcl = tcl.map(lambda tc: tc.annotate(; csq_score=hl.case(); .when((tc.lof == 'HC') & (tc.lof_flags == ''), csq_score(tc) - no_flag_score); .when((tc.lof == 'HC') & (tc.lof_flags != ''), csq_score(tc) - flag_score); .when(tc.lof == 'LC', csq_score(tc) - 10); .when(tc.polyphen_prediction == 'probably_damaging', csq_score(tc) - 0.5); .when(tc.polyphen_prediction == 'possibly_damaging', csq_score(tc) - 0.25); .when(tc.polyphen_prediction == 'benign', csq_score(tc) - 0.1); .default(csq_score(tc)); )); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3128:94,pipeline,pipeline,94,https://hail.is,https://github.com/hail-is/hail/issues/3128,1,['pipeline'],['pipeline']
Deployability,"It's quite likely CI was restarted, the job ids are fairly low. There's a current bug wherein CI will spin up a bunch of jobs, kill them all then start them again when it first starts (there's a subtle issue WRT to whether we update state from GH or batch first).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5550#issuecomment-471026187:226,update,update,226,https://hail.is,https://github.com/hail-is/hail/issues/5550#issuecomment-471026187,1,['update'],['update']
Deployability,"JIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZJJJZJZJIJZJJIJZZJJIJZZJZJZJZJZJZJZJZJZJIJJIJJZJZJZJZLis/hail/io/OutputBuffer;; ```. Presumably this used to work fine in earlier Hail versions. However, it seems impossible to revert to such a version at the moment, as 0.2.81 is the oldest version that one can still start a Dataproc cluster with -- earlier versions use a Debian image without a fix to the `log4j` vulnerability. 0.2.81 yields a different error (`Class too large`):. ```; Traceback (most recent call last):; File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/gnomad_v3_variants.py"", line 53, in <module>; run_pipeline(pipeline); File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/pyfiles_6qxp6bz4.zip/data_pipeline/pipeline.py"", line 200, in run_pipeline; File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/pyfiles_6qxp6bz4.zip/data_pipeline/pipeline.py"", line 167, in run; File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/pyfiles_6qxp6bz4.zip/data_pipeline/pipeline.py"", line 133, in run; File ""<decorator-gen-1123>"", line 2, in write; File ""/opt/conda/default/lib/python3.8/site-packages/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.8/site-packages/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 110, in execute; raise e; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 86, in execute; result_tuple = self._jhc.backend().executeEncode(jir, stream_codec); File ""/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1304, in __call__; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:14483,pipeline,pipeline,14483,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['pipeline'],['pipeline']
Deployability,"JIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZJJJZJZJIJZJJIJZZJJIJZZJZJZJZJZJZJZJZJZJIJJIJJZJZJZJZLis/hail/io/OutputBuffer;; ```. Presumably this used to work fine in earlier Hail versions. However, it seems impossible to revert to such a version at the moment, as 0.2.81 is the oldest version that one can still start a Dataproc cluster with -- earlier versions use a Debian image without a fix to the `log4j` vulnerability. 0.2.81 yields a different error (`Class too large`):. ```; Traceback (most recent call last):; File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/gnomad_v3_variants.py"", line 53, in <module>; run_pipeline(pipeline); File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/pyfiles_6qxp6bz4.zip/data_pipeline/pipeline.py"", line 200, in run_pipeline; File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/pyfiles_6qxp6bz4.zip/data_pipeline/pipeline.py"", line 167, in run; File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/pyfiles_6qxp6bz4.zip/data_pipeline/pipeline.py"", line 133, in run; File ""<decorator-gen-1123>"", line 2, in write; File ""/opt/conda/default/lib/python3.8/site-packages/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.8/site-packages/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 110, in execute; raise e; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 86, in execute; result_tuple = self._jhc.backend().executeEncode(jir, stream_codec); File ""/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1304, in __call__; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 29, in deco; raise FatalError('%s\n\nJava stack trace:\n%s\n'; hail.utils.java.FatalErro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:14594,pipeline,pipeline,14594,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['pipeline'],['pipeline']
Deployability,"JJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZJJJZJZIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZIZIZIZIZIZJIJJIJZJJJZJZJIJZJJIJZZJJIJZZJZJZJZJZJZJZJZJZJIJJIJJZJZJZJZLis/hail/io/OutputBuffer;; ```. Presumably this used to work fine in earlier Hail versions. However, it seems impossible to revert to such a version at the moment, as 0.2.81 is the oldest version that one can still start a Dataproc cluster with -- earlier versions use a Debian image without a fix to the `log4j` vulnerability. 0.2.81 yields a different error (`Class too large`):. ```; Traceback (most recent call last):; File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/gnomad_v3_variants.py"", line 53, in <module>; run_pipeline(pipeline); File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/pyfiles_6qxp6bz4.zip/data_pipeline/pipeline.py"", line 200, in run_pipeline; File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/pyfiles_6qxp6bz4.zip/data_pipeline/pipeline.py"", line 167, in run; File ""/tmp/ae72f79b93284c2293f6c466fe80f1c8/pyfiles_6qxp6bz4.zip/data_pipeline/pipeline.py"", line 133, in run; File ""<decorator-gen-1123>"", line 2, in write; File ""/opt/conda/default/lib/python3.8/site-packages/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.8/site-packages/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 110, in execute; raise e; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 86, in execute; result_tuple = self._jhc.backend().executeEn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:14273,pipeline,pipeline,14273,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['pipeline'],['pipeline']
Deployability,"Java 17 features are now allowed in our source code and we will no longer support older versions of java. We've also updated dependencies to fix security issues. There are several small bug fixes as well.</p>; <h3>JSON dependency:</h3>; <p>We've dropped the MJSON library which was no longer being updated and replaced it with a similarly small json library from org.json</p>; <h2>What's Changed</h2>; <ul>; <li>Migrate to Java 17 by <a href=""https://github.com/lbergelson""><code>@​lbergelson</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1649"">samtools/htsjdk#1649</a></li>; <li>Remove low-value progress logging message by <a href=""https://github.com/nh13""><code>@​nh13</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1659"">samtools/htsjdk#1659</a></li>; <li>removed redundant code by <a href=""https://github.com/KleinSamuel""><code>@​KleinSamuel</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1664"">samtools/htsjdk#1664</a></li>; <li>Update snappy-java and migrate mjson to org.json to address CVEs by <a href=""https://github.com/bbimber""><code>@​bbimber</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1670"">samtools/htsjdk#1670</a></li>; <li>Remove incorrect zero-length-B-array checks <a href=""https://github.com/gileshall""><code>@​gileshall</code></a> and <a href=""https://github.com/jmarshall""><code>@​jmarshall</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1674"">samtools/htsjdk#1674</a></li>; <li>add SINGULAR platform to read group by <a href=""https://github.com/omicsorama""><code>@​omicsorama</code></a> in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1635"">samtools/htsjdk#1635</a></li>; </ul>; <h2>New Contributors</h2>; <ul>; <li><a href=""https://github.com/KleinSamuel""><code>@​KleinSamuel</code></a> made their first contribution in <a href=""https://redirect.github.com/samtools/htsjdk/pull/1664"">samtools/htsjdk#1664</a></li>; <li><a href=""https",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13576:2414,Update,Update,2414,https://hail.is,https://github.com/hail-is/hail/pull/13576,1,['Update'],['Update']
Deployability,Java Exception Immediately After MacOS Installation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10524:39,Install,Installation,39,https://hail.is,https://github.com/hail-is/hail/issues/10524,1,['Install'],['Installation']
Deployability,Jb updated ImportAnnotations doc to reflect annotatesamples fam,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/305:3,update,updated,3,https://hail.is,https://github.com/hail-is/hail/pull/305,1,['update'],['updated']
Deployability,"Jobs with large logs (>2GiB-ish) can break workers because the current worker code attempts to load the whole log as `bytes` before uploading it to blob storage. This loading into `bytes` also plagues the batch front end when loading logs from blob storage to present to the user.; ; This updates the worker and front end to always stream through logs, never load them into memory. Additionally, in order to make page loads in the UI reasonable, we limit the length of the log that is shown in the UI, with some advice to download the file if it's too large to render on the page. Fixes #13329",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14076:289,update,updates,289,https://hail.is,https://github.com/hail-is/hail/pull/14076,1,['update'],['updates']
Deployability,"John, one improvement to this that I think we should consider: add a build step that changes the url to a versioned link. This will be a bit involved since you won't be able to link inside of a GitHub release (which is a zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7570#issuecomment-557265229:201,release,release,201,https://hail.is,https://github.com/hail-is/hail/pull/7570#issuecomment-557265229,1,['release'],['release']
Deployability,Jupyter-server is already updated and we're not adding setup tools to requirements.txt,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13516#issuecomment-1708540616:26,update,updated,26,https://hail.is,https://github.com/hail-is/hail/pull/13516#issuecomment-1708540616,1,['update'],['updated']
Deployability,Just double checking -- I'm thinking I can dev deploy only the step `upload_query_jar` for those specific missing commits and then copy the resulting JAR to the appropriate place. Or can I build the JAR locally?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1559883168:47,deploy,deploy,47,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1559883168,1,['deploy'],['deploy']
Deployability,"Just for reference, I had to make the following changes:; 1. There's a new table that tracks attempts that have been already aggregated. The reason I can't put this variable into the attempts table is due to a circular update that's not allowed.; 2. I added a dummy variable to the attempts table that is just a way to get the update trigger to run. We find all complete attempts from the batches state and update the dummy variable for the attempts table in chunks. Within the new trigger, if the attempt has not been aggregated yet, then we use `NEW.end - NEW.start` as the time to aggregate with. If the attempt has been aggregated, then we use the original way which is just to take the difference between New and Old. At the end, we find all attempts that have not been aggregated (not in the new table that keeps track of whether an attempt has been aggregated) and update the dummy variable to run the trigger. I don't think we need to take any locks on the attempts table because any future writes that occur while we do the last processing will already be aggregated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11996#issuecomment-1178118437:219,update,update,219,https://hail.is,https://github.com/hail-is/hail/pull/11996#issuecomment-1178118437,4,['update'],['update']
Deployability,"Just for the record, I have another patch coming that will ignore the partitioner on `InvalidClassException`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/728#issuecomment-244372653:36,patch,patch,36,https://hail.is,https://github.com/hail-is/hail/pull/728#issuecomment-244372653,1,['patch'],['patch']
Deployability,Just got something like this on a completely separate pipeline. This appears to occur after grouping on something and not including a previous key. FWIW works with a4f79a3b3269. cc @jbloom22,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4799#issuecomment-440070333:54,pipeline,pipeline,54,https://hail.is,https://github.com/hail-is/hail/issues/4799#issuecomment-440070333,1,['pipeline'],['pipeline']
Deployability,Just hit it again on another pipeline that has some `.key_by`s - this is now a showstopper ☹️,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4314#issuecomment-420343076:29,pipeline,pipeline,29,https://hail.is,https://github.com/hail-is/hail/issues/4314#issuecomment-420343076,1,['pipeline'],['pipeline']
Deployability,"Just to be clear, this pipeline was what I wrote when trying to replicate the bug Duncan was seeing, but it hit a different assertion error than the one he was hitting. He was hitting ""local in the wrong method builder"" problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150:23,pipeline,pipeline,23,https://hail.is,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150,2,['pipeline'],['pipeline']
Deployability,Just to make sure I understand -- the variable rename is to make sure it is clear that `HAIL_PRODUCTION_DOMAIN` means something different than `HAIL_DOMAIN` and is only applicable for CI? This is because the other services will have the correct deploy config?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14164#issuecomment-1898843580:245,deploy,deploy,245,https://hail.is,https://github.com/hail-is/hail/pull/14164#issuecomment-1898843580,2,['deploy'],['deploy']
Deployability,"Just to update, when updating to 0.2.125 I discovered a bug :( https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/array.20index.20out.20of.20bounds.20error.20on.20dict.2Eget",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1783285545:8,update,update,8,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1783285545,1,['update'],['update']
Deployability,"Just updated the path, should be good now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10169#issuecomment-820680874:5,update,updated,5,https://hail.is,https://github.com/hail-is/hail/pull/10169#issuecomment-820680874,1,['update'],['updated']
Deployability,Keeping the most recent 10 `deploy-`m `pr-` and `dev-` images seems reasonable. I think we also use the `cache-` prefix. We maybe should keep 10 each of those?. Anything that's untagged is absolutely good to delete.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13441#issuecomment-1679605182:28,deploy,deploy,28,https://hail.is,https://github.com/hail-is/hail/issues/13441#issuecomment-1679605182,1,['deploy'],['deploy']
Deployability,"Known facts:; - the last CI k8s deployment that started the deploy=1 batch on April 27th was active since at least April 25th.; - For a small window of time that I looked at on April 25th, it kept getting errors when trying to get the Github status for possible merge candidates: 12848, 12849, 12547. There might be other PRs at later dates. I saw at least the same errors for 12848 on April 27th. I'm going to throw out a hypothesis. I merged the dedup attempt resources PR on April 19th. The PRs that were stacked on previous commits of that PR now have merge conflicts with the set of commits that actually got merged. This caused problems because the next merge candidates CI was selecting was causing bad GitHub rate limit requests for exceeding the number of statuses. So it kept retrying that same merge candidate. CI didn't get restarted at least from the 25th to the 27th so the merge candidate never would have been refreshed. We know that there's less GKE node turnover in Azure, so not unexpected that the ci pod wouldn't get redeployed on its own. I'm thinking it's possible that I merged the database trigger fix on April 27th in response to the excessive deadlocks we noticed and then rebased the subsequent stacked PRs that had merge conflicts, thus unblocking CI, but I'm not sure (it's really hard to get what I want from the Azure log analytics system). I think the ""bug fix"" here is to reassess the code in CI and possibly harden it where we select the merge candidate and try to get the status so it doesn't block deployments. I have a screenshot from April 25th below in case it's helpful. The log analytics query that is helpful is:. ```; ContainerLog; | where ContainerID == ""273584134970cdae08cf0d412461862e2a0e558888a52c91870ca46a146cbb8a""; | order by TimeGenerated; ```. <img width=""1085"" alt=""Screen Shot 2023-05-24 at 12 58 18 PM"" src=""https://github.com/hail-is/hail/assets/1693348/e2da08b6-5982-46cb-9e2c-2178a19f2f86"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1561641011:32,deploy,deployment,32,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1561641011,3,['deploy'],"['deploy', 'deployment', 'deployments']"
Deployability,"Konrad got his relatedness estimates, so I'm tabling this PR until I have more time to clean it up for a proper release into 0.2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2821#issuecomment-362299166:112,release,release,112,https://hail.is,https://github.com/hail-is/hail/pull/2821#issuecomment-362299166,1,['release'],['release']
Deployability,"Kudu 0.9.0 was released a few days ago, and it has a re-written Spark library so we don't need the `org.kududb.spark` package any more. It also fixes bugs, like the one @cseed saw with the context not being shut down. Annotations still don't work though - is there a way to get their schema early on so we can create a database table for them?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-226535225:15,release,released,15,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-226535225,1,['release'],['released']
Deployability,"Kyle brought this up. `conda activate` doesn't actually work, unless you use the package installer, or manually append conda.sh to your path.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6738:89,install,installer,89,https://hail.is,https://github.com/hail-is/hail/issues/6738,1,['install'],['installer']
Deployability,"LTRjZjJhNTdhZDkzOCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""d384d00e-b18b-41bc-871f-4cf2a57ad938"",""prPublicId"":""d384d00e-b18b-41bc-871f-4cf2a57ad938"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13718:4998,patch,patch,4998,https://hail.is,https://github.com/hail-is/hail/pull/13718,2,"['patch', 'upgrade']","['patch', 'upgrade']"
Deployability,"LWJmMWY5Mzc1NTVhYyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""759202b1-ae50-4125-b3a5-bf1f937555ac"",""prPublicId"":""759202b1-ae50-4125-b3a5-bf1f937555ac"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13836:5066,patch,patch,5066,https://hail.is,https://github.com/hail-is/hail/pull/13836,2,"['patch', 'upgrade']","['patch', 'upgrade']"
Deployability,"LWY3ZGM4YjIwOTVhNiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""6e02a47f-633e-4605-b359-f7dc8b2095a6"",""prPublicId"":""6e02a47f-633e-4605-b359-f7dc8b2095a6"",""dependencies"":[{""name"":""ipython"",""from"":""7.34.0"",""to"":""8.10.0""},{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""},{""name"":""tornado"",""from"":""6.2"",""to"":""6.3.3""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-IPYTHON-3318382"",""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412"",""SNYK-PYTHON-TORNADO-5537286"",""SNYK-PYTHON-TORNADO-5840803""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[531,null,null,509,384,494],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Remote Code Execution (RCE)](https://learn.snyk.io/lesson/improper-input-validation/?loc&#x3D;fix-pr); 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [More lessons are available in Snyk Learn](https://learn.snyk.io/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13933:5066,patch,patch,5066,https://hail.is,https://github.com/hail-is/hail/pull/13933,2,"['patch', 'upgrade']","['patch', 'upgrade']"
Deployability,Labelling as a bug because this can adversely affect CI pipelines.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13722#issuecomment-1737675608:56,pipeline,pipelines,56,https://hail.is,https://github.com/hail-is/hail/issues/13722#issuecomment-1737675608,1,['pipeline'],['pipelines']
Deployability,"Lastly, a bunch of LOC come from the lock files. Those can be ignored for review purposes; they just maintain versioning information, ensure installs are consistent.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454272823:141,install,installs,141,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454272823,1,['install'],['installs']
Deployability,Later versions of IDEA can't run tests with our current testng and scalatest versions. Update to latest and fix fallout.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14277:87,Update,Update,87,https://hail.is,https://github.com/hail-is/hail/pull/14277,1,['Update'],['Update']
Deployability,"Laurent, I was totally wrong about being able to do this per-command -- I'm really sorry. I thought that it would be possible to create a new configuration just for this command and use that, but this is only possible for `HadoopConfiguration`s and not `SparkContext`s. Can you reopen the old PR? That model is our only option.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/826#issuecomment-248641543:142,configurat,configuration,142,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248641543,1,['configurat'],['configuration']
Deployability,Let me know if you think this is good and whether I need to test the UI with dev deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13262:81,deploy,deploy,81,https://hail.is,https://github.com/hail-is/hail/pull/13262,1,['deploy'],['deploy']
Deployability,Let me know if you want me to dev deploy this one last time and test the changes. I haven't done so recently.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11750#issuecomment-1167512297:34,deploy,deploy,34,https://hail.is,https://github.com/hail-is/hail/pull/11750#issuecomment-1167512297,1,['deploy'],['deploy']
Deployability,Let me know when this is good and I'll test the deploy script manually by commenting out everything not related to the GAR cleanup.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13703:48,deploy,deploy,48,https://hail.is,https://github.com/hail-is/hail/pull/13703,1,['deploy'],['deploy']
Deployability,"Let's enumerate the use-cases. In all cases, the point is to run the client script in the cloud.; 1. Submit a QoB pipeline.; 2. Submit a Batch pipeline.; 3. Submit a single-Python-job batch to execute some python code remotely.; 4. Same as (3) but using local-mode Hail (as opposed to (1)). I think use-case (4) is rare, though useful. If we're pitching `hailctl batch submit` as the QoB replacement for `hailctl dataproc submit`, then I think people will be very confused if (4) happens. I am somewhat frustrated that we have these different deployment strategies. It seems like unnecessary intellectual burden for a user to think about when they're just trying to run some Hail code. One option is to have different commands. Submitting a QoB job is done like this:; ```; hailctl qob submit ...; ```; And submitting a batch job is done like this:; ```; hailctl batch submit ...; ```. It pains me to think about trying to explain the difference between (1) and (4) to a scientist. At least with two distinct commands we can sort of ignore (4) and say: if you want to run Hail Query on Hail Batch use `hailctl qob` and if you want to run a normal batch pipeline use `hailctl batch`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12471#issuecomment-1324113118:114,pipeline,pipeline,114,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1324113118,4,"['deploy', 'pipeline']","['deployment', 'pipeline']"
Deployability,Let's get https://github.com/hail-is/hail/pull/12854 into this release. I slapped high:prio on it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12853#issuecomment-1500424252:63,release,release,63,https://hail.is,https://github.com/hail-is/hail/pull/12853#issuecomment-1500424252,1,['release'],['release']
Deployability,"Let's not approve this right now. I don't think deploying will make a difference, but I'd rather not risk it while the BroadE is happening.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4863#issuecomment-443234023:48,deploy,deploying,48,https://hail.is,https://github.com/hail-is/hail/pull/4863#issuecomment-443234023,1,['deploy'],['deploying']
Deployability,"Let's say we have the following structure. gs://bucket/a/b/foo.txt; gs://bucket/a/b/bar.txt; gs://bucket/a/baz.txt. In the old design with rsync, we could be doing in parallel the following:. ```; gsutil rsync gs://bucket/a; ```. and. ```; gsutil rsync gs://bucket/a/b/; ```. I was worried we would delete files in the rsync in the middle of one job doing the copying. Basically some kind of race condition. However, with the new design of the cache, this might not be a problem where we're not using gsutil rsync but instead writing our own version of rsync. Cotton's fleshing out the design of that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9523#issuecomment-701580816:55,a/b,a/b,55,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701580816,4,['a/b'],"['a/b', 'a/baz']"
Deployability,Let's see if we can update to newest pandas.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9804:20,update,update,20,https://hail.is,https://github.com/hail-is/hail/pull/9804,1,['update'],['update']
Deployability,"Lh. # __TASK__1 read_input; cp gs://hail-jigold/input.bed Aw2arWP9.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim Aw2arWP9.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevqS.haps; __RESOURCE__17=GLx",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1207,pipeline,pipeline,1207,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938,1,['pipeline'],['pipeline']
Deployability,"Like a release? I think the terminology is a little overloaded. In services code we consider ""deploy"" as the CI pipeline that runs whenever main is updated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12510#issuecomment-1329692943:7,release,release,7,https://hail.is,https://github.com/hail-is/hail/pull/12510#issuecomment-1329692943,4,"['deploy', 'pipeline', 'release', 'update']","['deploy', 'pipeline', 'release', 'updated']"
Deployability,"LocalMatrix follows NumPy's broadcast rules (restricted to two-dimensional ndarrays), and I've tried to mirror the Numpy interface for all functions where it's reasonable to do so. I still need time to add a bunch of Python tests of the interface, but I'd be glad for feedback/review in the meantime. In a subsequent PR, I'll expose the rest of BlockMatrix's binary ops in Python with the similar syntax and rules. These changes will provide the matrix functionality needed for a clean mixed models pipeline (modulo a few Scala black boxes that I can return to once I have something working) and will hopefully be generally useful for adding/porting more methods in Python. Current longer-term plan is to expose RowMatrix as well, and consider how to best unify the interfaces. And one day LocalMatrix will actually be a NumPy array...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3064:499,pipeline,pipeline,499,https://hail.is,https://github.com/hail-is/hail/pull/3064,1,['pipeline'],['pipeline']
Deployability,"Long awaited, this change prompts the batch driver to only schedule jobs on workers with the most recent instance version, i.e. matches the `INSTANCE_VERSION` global variable. This way we can make backwards incompatible changes between the worker and driver without having to manually kill the whole fleet. This will allow pre-existing workers to finish gracefully, as they will just stop receiving work when the new batch driver is deployed and eventually die off. ### Scheduler changes; Just skips instances where the instance version doesn't match `INSTANCE_VERSION`. ### Autoscaler changes; Cluster stats like free mcpu and live instances are tracked per instance version. The autoscaler now only looks at instances of the latest version when deciding whether it needs more workers. This way we don't get stuck unable to schedule new jobs until the old workers die off because there technically are enough cores available to meet demand but they are from old workers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13055:433,deploy,deployed,433,https://hail.is,https://github.com/hail-is/hail/pull/13055,1,['deploy'],['deployed']
Deployability,"Looking at fonts in the network panel of Chrome dev tools:. On https://hail.is/, I see https://ka-f.fontawesome.com/releases/v5.15.3/webfonts/free-fa-brands-400.woff2, which looks like it matches the font family specified by the `fab` class (""Font Awesome 5 Brands""). On https://hail.is/docs/0.2/index.html however, the only Font Awesome font I see is https://hail.is/docs/0.2/_static/fonts/fontawesome-webfont.woff2?v=4.7.0.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10277#issuecomment-813532470:116,release,releases,116,https://hail.is,https://github.com/hail-is/hail/pull/10277#issuecomment-813532470,1,['release'],['releases']
Deployability,"Looking at the IR generated by table.flatten, this snippet:; ```; >>> import hail as hl; >>> t = hl.utils.range_table(10); >>> t2 = t.annotate(**{f'f{i}': i for i in range(5)}); >>> t2.flatten().collect(); ```; generates the following IR:; ```; (GetField rows; (TableCollect; (TableMapRows; (TableOrderBy (Aidx); (TableMapRows; (TableRange 10 8); (InsertFields; (SelectFields (idx); (Ref row)); None; (f0; (I32 0)); (f1; (I32 1)); (f2; (I32 2)); (f3; (I32 3)); (f4; (I32 4))))); (Let __uid_3; (Ref row); (InsertFields; (SelectFields (); (SelectFields (idx f0 f1 f2 f3 f4); (Ref row))); None; (idx; (GetField idx; (Ref __uid_3))); (f0; (GetField f0; (Ref __uid_3))); (f1; (GetField f1; (Ref __uid_3))); (f2; (GetField f2; (Ref __uid_3))); (f3; (GetField f3; (Ref __uid_3))); (f4; (GetField f4; (Ref __uid_3)))))))); ```; If we look at the last `TableMapRows` IR, the entire thing `(Let __uid_3 …)` is entirely a no-op, but we're still compiling and generating code for the (post-optimization) IR:; ```; (InsertFields; (SelectFields (); (Ref row)); None; (idx; (GetField idx; (Ref row))); (f0; (GetField f0; (Ref row))); (f1; (GetField f1; (Ref row))); (f2; (GetField f2; (Ref row))); (f3; (GetField f3; (Ref row))); (f4; (GetField f4; (Ref row)))); ```. (cc @tpoterba I added a second `ForwardLets` in `Optimize` before the `Simplify`, although I'm not sure that's actually the correct place to put it; in this case, I think it may eventually come out in the wash given how many passes we make through any given pipeline, but I've noticed that currently our python tends to generate IR of the form:; ```; (TableMapRows; (Let __uid_n; (Ref row); <mapped value, sometimes using (Ref __uid_n) and sometimes (Ref row)>; ```; and that redundant binding at the top level means that the first Simplify pass misses quite a few optimizations! I'm not super attached to leaving it there, but I do think we might want to consider forwarding Lets on any IRs from python before optimization.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7719:1511,pipeline,pipeline,1511,https://hail.is,https://github.com/hail-is/hail/pull/7719,1,['pipeline'],['pipeline']
Deployability,"Looking at the `…/Packages` URL in the previous comment, 1.2.0 is now available (and 1.1.0 does not appear to be there). In our recent local hail update deployment, the `batch_worker_image` job failed repeatedly due to GoogleCloudPlatform/gcsfuse#1424. We worked around this as initially suggested on that issue with populationgenomics/hail@607408bee752dabca48d9a2732b14d32813ace9f, but later comments on the issue suggest that the better approach would be this PR with an additional change to access the apt repo via https:. ```diff; - echo ""deb http://packages.cloud.google.com/apt $GCSFUSE_REPO main"" | tee /etc/apt/sources.list.d/gcsfuse.list && \; + echo ""deb https://packages.cloud.google.com/apt $GCSFUSE_REPO main"" | tee /etc/apt/sources.list.d/gcsfuse.list && \; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13728#issuecomment-1763644502:146,update,update,146,https://hail.is,https://github.com/hail-is/hail/pull/13728#issuecomment-1763644502,2,"['deploy', 'update']","['deployment', 'update']"
Deployability,"Looking at the auth image, it is down from 2.76GB in main to 674MB. The hail-ubuntu image underneath it has stayed basically the same at about half the new auth image. Nearly all of the 674MB is split evenly between the layer that installs python in hail-ubuntu and the layer that installs the pip dependencies in the auth image. I've not yet inspected the hail-ubuntu layer, but for the pip dependencies the main offenders are:. ```; 77M	/usr/local/lib/python3.7/dist-packages/googleapiclient; 76M	/usr/local/lib/python3.7/dist-packages/botocore; 33M	/usr/local/lib/python3.7/dist-packages/_sass.abi3.so; 29M	/usr/local/lib/python3.7/dist-packages/kubernetes_asyncio; 20M	/usr/local/lib/python3.7/dist-packages/uvloop; 14M	/usr/local/lib/python3.7/dist-packages/pip; 14M	/usr/local/lib/python3.7/dist-packages/cryptography; 8.9M	/usr/local/lib/python3.7/dist-packages/google; 7.9M	/usr/local/lib/python3.7/dist-packages/pygments; 7.0M	/usr/local/lib/python3.7/dist-packages/azure; 5.0M	/usr/local/lib/python3.7/dist-packages/setuptools; 4.2M	/usr/local/lib/python3.7/dist-packages/aiohttp; 2.5M	/usr/local/lib/python3.7/dist-packages/googlecloudprofiler; 2.2M	/usr/local/lib/python3.7/dist-packages/yaml; 2.2M	/usr/local/lib/python3.7/dist-packages/hailtop; 2.0M	/usr/local/lib/python3.7/dist-packages/rich; 1.6M	/usr/local/lib/python3.7/dist-packages/pyasn1_modules; 1.5M	/usr/local/lib/python3.7/dist-packages/boto3; 1.4M	/usr/local/lib/python3.7/dist-packages/pkg_resources; 1.4M	/usr/local/lib/python3.7/dist-packages/oauthlib; 1.1M	/usr/local/lib/python3.7/dist-packages/pycparser; ```. Most of a gigabyte still feels annoyingly bloated but might just have to do for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1459376308:231,install,installs,231,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1459376308,2,['install'],['installs']
Deployability,Looks like this failed due to the wrong init script. Probably just need to update cloudtools on the CI server?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4347#issuecomment-422132532:75,update,update,75,https://hail.is,https://github.com/hail-is/hail/pull/4347#issuecomment-422132532,1,['update'],['update']
Deployability,Looks like this has been removed: https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/guides/v2-upgrade-guide#changes-in-v200. #assign services,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11698:117,upgrade,upgrade-guide,117,https://hail.is,https://github.com/hail-is/hail/pull/11698,1,['upgrade'],['upgrade-guide']
Deployability,"Looks like this mainly drops 3.6 support, but I'll rope it into a dev deployed branch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11520#issuecomment-1061046995:70,deploy,deployed,70,https://hail.is,https://github.com/hail-is/hail/pull/11520#issuecomment-1061046995,1,['deploy'],['deployed']
Deployability,"Looks like this:; ```; (py37) dking@wmb16-359 # ./install-gcs-connector.sh . To set the active account, run:; $ gcloud config set account `ACCOUNT`. created key [bd10c2da666d327144166cc71ba13075dbd7ea26] of type [json] as [/Users/dking/.hail/gcs-keys/gcs-connector-key.json] for [842871226259-compute@developer.gserviceaccount.com]; mkdir: /Users/dking/anaconda2/envs/py37/lib/python3.7/site-packages/pyspark/conf: File exists; success; ```; I tested it by running `python -c 'import hail as hl; hl.read_table(""gs://danking/gnomad-test.mt"").describe()'`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4500:50,install,install-gcs-connector,50,https://hail.is,https://github.com/hail-is/hail/pull/4500,1,['install'],['install-gcs-connector']
Deployability,Looks like we need to update the tests too.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7361#issuecomment-545585187:22,update,update,22,https://hail.is,https://github.com/hail-is/hail/pull/7361#issuecomment-545585187,1,['update'],['update']
Deployability,Looks like you need to [update the Google Artifact Registry cleanup policies](https://batch.hail.is/batches/8076011/jobs/210) to account for your new image. Instructions to do so are in the error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13936#issuecomment-1783512175:24,update,update,24,https://hail.is,https://github.com/hail-is/hail/pull/13936#issuecomment-1783512175,1,['update'],['update']
Deployability,"Looks ready. It'll break many a pipeline, so can you whip up a discuss post on the change that includes last commit before it goes in?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4535#issuecomment-430342019:32,pipeline,pipeline,32,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-430342019,1,['pipeline'],['pipeline']
Deployability,"Love it! Though email, not slack integration? 😛",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4509#issuecomment-428052468:33,integrat,integration,33,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-428052468,1,['integrat'],['integration']
Deployability,Lowercase characters and no github password request in release script,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7536:55,release,release,55,https://hail.is,https://github.com/hail-is/hail/pull/7536,1,['release'],['release']
Deployability,"Made the following changes:. - Disabled dataproc tests; - Moved dataproc tests to Makefile, to be run before manual deploys; - Add back VEP cluster test script; - Removed cloudtools config files; - Removed the latest-build functionality; - Added VEP scripts to hailctl/dataproc/resources; - Changed init_notebook to pip install hail wheels, picking up; dependencies automatically; - add out-of-date check (once per day) to hailctl startup",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6250:116,deploy,deploys,116,https://hail.is,https://github.com/hail-is/hail/pull/6250,2,"['deploy', 'install']","['deploys', 'install']"
Deployability,"Made this change backwards compatible. Note that I have not made any changes to worker.py in this PR anymore, so there's no danger of incompatibility. I tested the JAR from this PR against default and ran a simple hail query to see that it behaved as usual. Separately, I made #12246, dev deployed it, then ran this same JAR against my dev namespace to see that it added all worker jobs to the same batch as the driver job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12222#issuecomment-1262670715:289,deploy,deployed,289,https://hail.is,https://github.com/hail-is/hail/pull/12222#issuecomment-1262670715,2,['deploy'],['deployed']
Deployability,"Major Changes:; - never delete CI jobs, only cancel them; - Mergeable (success) and Failure build states include the job that triggered the build state; - if a PR's build state has a job, link to that job. Minor Changes:; - fix location of dk-test instance; - test that proxy processes are still alive (if proxy creation fails, the process usually exits); - provide `HAIL_CI_GCS_PATH` for developers to set an alternative deploy bucket and path-within-bucket (now that `gs://hail-ci-0-1` is protected)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5054:422,deploy,deploy,422,https://hail.is,https://github.com/hail-is/hail/pull/5054,1,['deploy'],['deploy']
Deployability,Make Caitlin's BGEN PRS Pipeline Fast,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3727:24,Pipeline,Pipeline,24,https://hail.is,https://github.com/hail-is/hail/pull/3727,1,['Pipeline'],['Pipeline']
Deployability,Make TabixReader take a hadoop configuration in the constructor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5033:31,configurat,configuration,31,https://hail.is,https://github.com/hail-is/hail/pull/5033,1,['configurat'],['configuration']
Deployability,"Make sure that we output well formed VCF 4.5, this includes things like VCF 4.4's phased haploid calls (this will also require updates to our parser).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14744:127,update,updates,127,https://hail.is,https://github.com/hail-is/hail/issues/14744,1,['update'],['updates']
Deployability,"Make targets for building docs were renamed in #8086 and #9348. `make install-dev-deps` does not cover all build dependencies for docs (it does not install pandoc), so added a note about what is required.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9458:70,install,install-dev-deps,70,https://hail.is,https://github.com/hail-is/hail/pull/9458,2,['install'],"['install', 'install-dev-deps']"
Deployability,Making way for the release fix,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13832#issuecomment-1781334414:19,release,release,19,https://hail.is,https://github.com/hail-is/hail/pull/13832#issuecomment-1781334414,1,['release'],['release']
Deployability,Matches up the `gcsfs` version between services and hail query. Right now services developers have to pip install the `docker/requirements.txt` to run stuff like `sync.py` or resolve imports in editor. Might not be ideal but this at least allows you to pip install all the different requirements together for now.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9988:106,install,install,106,https://hail.is,https://github.com/hail-is/hail/pull/9988,2,['install'],['install']
Deployability,"Matt S fortuitously asked a question that lead me to https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/v3.0.0/gcs/INSTALL.md , so I'm trying that now. That might be the last necessary fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1961689398:126,INSTALL,INSTALL,126,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1961689398,1,['INSTALL'],['INSTALL']
Deployability,Maybe do automatic deployment here? With precompiled binaries?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1307:19,deploy,deployment,19,https://hail.is,https://github.com/hail-is/hail/issues/1307,1,['deploy'],['deployment']
Deployability,"Maybe we should make an issue tag for things to fix before 0.2? We can't fix this without making a breaking interface change, but it's a really easy change that we shouldn't forget to include once we do 0.2. More generally, it seems like it would be good if we knew what we were planning on including before next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1887#issuecomment-305268107:313,release,release,313,https://hail.is,https://github.com/hail-is/hail/issues/1887#issuecomment-305268107,1,['release'],['release']
Deployability,Maybe we should update the WARNING message to be clear that this is a transient error and we've automatically retried it and there's nothing to be worried about?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649877:16,update,update,16,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649877,2,['update'],['update']
Deployability,"Merge branch 'release-1.26.6' into develop</li>; <li>See full diff in <a href=""https://github.com/boto/boto3/compare/1.26.6...1.26.8"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=boto3&package-manager=pip&previous-version=1.26.6&new-version=1.26.8)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12458:6613,upgrade,upgrade,6613,https://hail.is,https://github.com/hail-is/hail/pull/12458,3,['upgrade'],['upgrade']
Deployability,"Merge the Nealelab/hail-datasets repository into mainline hail. This is the set of scripts that are used to generate the datasets for the Datasets API. Next steps are to clean up these scripts (there's something called `old/`?) and to figure out a strategy for deploying new datasets when they're merged. This PR just literally merges the master branch from Nealelab/hail-datasets into hail-is/hail. cc: @liameabbott, thanks!, @GreatBrando",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6579:261,deploy,deploying,261,https://hail.is,https://github.com/hail-is/hail/pull/6579,1,['deploy'],['deploying']
Deployability,Metadata server updates: scopes endpoint and allow omission of trailing slashes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14566:16,update,updates,16,https://hail.is,https://github.com/hail-is/hail/pull/14566,1,['update'],['updates']
Deployability,Might be related to phasing. The FORMAT fields is GT and a typical genotype is 0|1. There is a copy here: /broad/1kg/ftp/release/20130502.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/348:121,release,release,121,https://hail.is,https://github.com/hail-is/hail/issues/348,1,['release'],['release']
Deployability,"Miniconda definitely includes the necessary bash_profile modifications, and provides a smaller installation (no unneeded deps): 46.2MB for Mac cli installer, vs 435MB. Tried to make documentation a bit more readable (whitespace) and explicit. <img width=""713"" alt=""Screenshot 2019-07-25 15 05 59"" src=""https://user-images.githubusercontent.com/5543229/61901550-045a7f80-aeee-11e9-89a7-2dd8cc8a27ca.png"">. cc @johnc1231",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6739:95,install,installation,95,https://hail.is,https://github.com/hail-is/hail/pull/6739,2,['install'],"['installation', 'installer']"
Deployability,Minor doc updates.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/418:10,update,updates,10,https://hail.is,https://github.com/hail-is/hail/pull/418,1,['update'],['updates']
Deployability,"Missed one. Hand deployed, verified it works end-to-end with this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8770:17,deploy,deployed,17,https://hail.is,https://github.com/hail-is/hail/pull/8770,1,['deploy'],['deployed']
Deployability,Missed renaming this instance of 'deploy' varable to 'scope',MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6641:34,deploy,deploy,34,https://hail.is,https://github.com/hail-is/hail/pull/6641,1,['deploy'],['deploy']
Deployability,Mitigation: `pip3 install 'ipython<8.17'`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14099#issuecomment-1858099553:18,install,install,18,https://hail.is,https://github.com/hail-is/hail/issues/14099#issuecomment-1858099553,1,['install'],['install']
Deployability,Mmm. We should upgrade AKS to 1.21.x. I'll address this after March 28th though.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11602#issuecomment-1069373611:15,upgrade,upgrade,15,https://hail.is,https://github.com/hail-is/hail/pull/11602#issuecomment-1069373611,1,['upgrade'],['upgrade']
Deployability,Modify compiler arguments to emit warnings required for scalafix.; Fix failures that arise from the new build configuration.; Run scalafix.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14103:110,configurat,configuration,110,https://hail.is,https://github.com/hail-is/hail/pull/14103,1,['configurat'],['configuration']
Deployability,More concerning is why we just didn't deploy for a week,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1559928342:38,deploy,deploy,38,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1559928342,1,['deploy'],['deploy']
Deployability,More details at #8058. - Include the user's IP in the site logs.; - Fix out of date Makefile. I recognize there's duplication of log format. Abstracting over that doesn't seem *that* valuable and requires putting the shared configuration into a file in the root of hail and then arranging for the shared config file to be in the docker context. It's all kind of annoying and seems low value.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8059:224,configurat,configuration,224,https://hail.is,https://github.com/hail-is/hail/pull/8059,1,['configurat'],['configuration']
Deployability,"More extensive cloud tests are showing speedups in the combiner pipeline compared to master, about 15-20 seconds per partition, but that adds up quickly at scale.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5828#issuecomment-481367173:64,pipeline,pipeline,64,https://hail.is,https://github.com/hail-is/hail/pull/5828#issuecomment-481367173,1,['pipeline'],['pipeline']
Deployability,"Moreover, the deploy jobs keep trying to access things in the batch-pods namespace, but the things they need to deploy are in the regular batch namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4592#issuecomment-431594049:14,deploy,deploy,14,https://hail.is,https://github.com/hail-is/hail/issues/4592#issuecomment-431594049,2,['deploy'],['deploy']
Deployability,"Most of the changes here are totally fine, but yes, I think we should hold off on changing paths. I have a to-do item to look into configuring / monkey patching the Sphinx function that's trying to generate links for the signatures. Dan is also OOO this week so not high prio.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9403#issuecomment-688855000:152,patch,patching,152,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-688855000,1,['patch'],['patching']
Deployability,Most pipelines will take the old path.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10293:5,pipeline,pipelines,5,https://hail.is,https://github.com/hail-is/hail/pull/10293,2,['pipeline'],['pipelines']
Deployability,"Mostly random bugs that didn't get flexed until trying to run CI jobs and batch tests within jobs.; - The IP addresses used for jobs immediately got out of sync with GCP and I needed to add an `internal.hail` entry to the worker and job's `/etc/hosts` so that default batch could submit to dev batch.; - GCP's metadata server and DNS nameserver are both 169.254.169.254. Azure has a separate IP address for the latter, so I added this configuration to the CloudWorkerAPI. Something that's not addressed here is that I needed to comment out the resource requirements for build image jobs to make them run on standards. The common 2 vCPU / 10 Gi storage / 7.5 Gi Mem lands on standards in GCP but highcpu on azure, which doesn't have disks implemented yet. I'm not sure what the correct step forward on that front is. Otherwise, dev deploying batch should be possible! I ran into multiple issues where my user's sql config was messed up because it was created from a buggy branch. I tried to fix these for the other dev namespaces (dan's which was made later was fine) but there may be some bits I missed. I got as far as running `test_batch_0` and the tests start (!) but fail quickly because of a blob permission issue on the dev driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11071:435,configurat,configuration,435,https://hail.is,https://github.com/hail-is/hail/pull/11071,2,"['configurat', 'deploy']","['configuration', 'deploying']"
Deployability,"Motivation for this change: I want to keep the global configuration information in one place, and that's going to be the K8s default/global-config secret. In particular, I want to get rid of config.mk and just pull the relevant information from K8s. The secret currently like this:. ```; $ k get -o json secret global-config | jq '.data | map_values(@base64d)'; {; ""default_namespace"": ""..."",; ""docker_root_image"": ""..."",; ""domain"": ""..."",; ""gcp_project"": ""..."",; ""gcp_region"": ""..."",; ""gcp_zone"": ""..."",; ""gsuite_organization"": ""..."",; ""internal_ip"": ""..."",; ""ip"": ""..."",; ""kubernetes_server_url"": ""...""; }; ```. default_namespace will always be the name of the namespace the secret is in. This adds gsuite_organization which will be used by auth to restrict logins to a fixed GSuite organization, e.g. broadinstitute.org. I have created this secret on our production K8s cluster. The Terraform script will also create it. With this change, CI creates global-config in test and dev ""default"" namespaces based on the one from where CI is operating. The only field that currently needs to be updated is default_namespace. The plan is to pull from global-config in deployments instead of threading these global configuration(s) through CI. I made this change in the CI tests. FYI @lgruen. I'm going to break up the infra-1 work into a few separate PRs to keep it all clear and manageable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9777:54,configurat,configuration,54,https://hail.is,https://github.com/hail-is/hail/pull/9777,4,"['configurat', 'deploy', 'update']","['configuration', 'deployments', 'updated']"
Deployability,"Move installed location of hail-all-spark.jar to from hail/ to; hail/backend/. We were not finding the jar properly with pkg_resources, and so were not; setting the paths appropriately for pip installs, causing 'JavaPackage; not callable' errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8405:5,install,installed,5,https://hail.is,https://github.com/hail-is/hail/pull/8405,2,['install'],"['installed', 'installs']"
Deployability,"Moved the cheatsheets a few weeks ago, had to wait for a website update to safely take the old ones out of the repo. . For reference, new location is here: ; https://github.com/hail-is/hail/blob/master/hail/python/hail/docs/_static/cheatsheets/hail_tables_cheat_sheet.pdf; https://github.com/hail-is/hail/blob/master/hail/python/hail/docs/_static/cheatsheets/hail_matrix_tables_cheat_sheet.pdf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8061:65,update,update,65,https://hail.is,https://github.com/hail-is/hail/pull/8061,1,['update'],['update']
Deployability,"Moves multi-pod deployments over to using Headless Services, which enables client-side load-balancing to the underlying pods. See #12095 for more context. The reason I put this in its own PR is that Kubernetes won't let me apply the `clusterIP: None` changes to existing `Services`, and I must delete the `Service` resources first. I can manually delete and apply new headless services in a way that is compatible with what is currently on main and with just a few seconds of downtime, but I should do this manually just before this PR merges.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12094:16,deploy,deployments,16,https://hail.is,https://github.com/hail-is/hail/pull/12094,1,['deploy'],['deployments']
Deployability,Moving everything to hailctl. Deployment PR coming soon.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6131:30,Deploy,Deployment,30,https://hail.is,https://github.com/hail-is/hail/pull/6131,1,['Deploy'],['Deployment']
Deployability,"My assumption has been that each fresh pipeline should get a new `EmitModuleBuilder` and `ExecuteContext`, if this is the case, I believe what I have done here should be alright.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9044#issuecomment-652737719:39,pipeline,pipeline,39,https://hail.is,https://github.com/hail-is/hail/pull/9044#issuecomment-652737719,1,['pipeline'],['pipeline']
Deployability,"My concern is if we release the code as is now, then we cannot change the buckets where the VEP data is stored without breaking backwards compatibility. Therefore, one idea I had was to keep a single up-to-date hail manifest file in GCS with the current information on what configurations are supported, where the data lives, what VEP version etc as a way of versioning the supported configurations. I did not try and implement this yet. I wanted to run the idea by you first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1514997203:20,release,release,20,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1514997203,3,"['configurat', 'release']","['configurations', 'release']"
Deployability,My dev deploy now has the light up graph 🥳,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8928#issuecomment-639218266:7,deploy,deploy,7,https://hail.is,https://github.com/hail-is/hail/pull/8928#issuecomment-639218266,1,['deploy'],['deploy']
Deployability,"My issue with both of those names is the same as with `_tree`. It's not clear if you're storing the non-transitive or the transitive relation & its not clear if self-edges are included. I want a name that unambiguously says ""the self-edge and all the ancestor edges are in here"" or a name that is more domain-specific like ""job groups that need to be updated when a job in this job group is changed"". . `job_group_self_and_ancestor` feels like the shortest name so far that satisfies my concerns, but I'm open to other ones that are clear about self-edge-ness and transitive-ness.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13475#issuecomment-1761867398:351,update,updated,351,https://hail.is,https://github.com/hail-is/hail/pull/13475#issuecomment-1761867398,2,['update'],['updated']
Deployability,"My main motivation is that I can use up-to-date versions of NumPy, SciPy and Pandas. > I think we're also feeling quite sour on conda at the moment as well. In particular, I had to fix the [environment.yml for LDSC](https://github.com/bulik/ldsc/pull/168) because **recent versions of conda removed scipy==0.18 from their registry**. Wow, that's bad. I did not know conda removes old packages. Are you using some continuous integration?; We are solving these issues by some policy that pull requests have to pass CI building. ; If there is a new version of e.g. Pandas, CI will install it when somebody pushes a commit.; In case the new package version breaks something, CI fails and we get a notification to fix it. This way, we can in general keep up to date with the latest package versions and only rise the minimum version number of dependencies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542194340:413,continuous,continuous,413,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542194340,3,"['continuous', 'install', 'integrat']","['continuous', 'install', 'integration']"
Deployability,"My team is pretty excited about hail being released with support for Spark 3.5. One thing I noticed is that it looks like the plan is to [restrict to Spark 3.5.0](https://github.com/hail-is/hail/pull/14158/files#diff-7e9fff5f09cc109665f7fe9baa107affaac24f5dc5a0aa8bc3769221a4c6c328R53) - would it be possible to allow some wiggle room for minor releases? Spark has been beginning to release upgrades much more often than in the past, so restricting to 3.5.0 will prevent access to bug fixes, feature enhancements, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1900997582:43,release,released,43,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1900997582,4,"['release', 'upgrade']","['release', 'released', 'releases', 'upgrades']"
Deployability,"NAPSHOT</li>; <li>Additional commits viewable in <a href=""https://github.com/qos-ch/slf4j/compare/v_1.7.25...v_2.0.6"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.slf4j:slf4j-api&package-manager=gradle&previous-version=1.7.25&new-version=2.0.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12714:3287,upgrade,upgrade,3287,https://hail.is,https://github.com/hail-is/hail/pull/12714,3,['upgrade'],['upgrade']
Deployability,NB: we cannot use python 3.6 because ipython-notebook hasn't been updated yet.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2951:66,update,updated,66,https://hail.is,https://github.com/hail-is/hail/pull/2951,1,['update'],['updated']
Deployability,NFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 1/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 2/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 3/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 4/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 5/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 6/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 7/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 8/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 9/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 10/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Iterations complete. Computing local QR; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 _reduced_svd: ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980:6643,update,updated,6643,https://hail.is,https://github.com/hail-is/hail/issues/12980,1,['update'],['updated']
Deployability,NFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 2/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 3/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 4/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 5/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 6/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 7/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 8/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 9/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 10/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Iterations complete. Computing local QR; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 _reduced_svd: Computing local SVD; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980:6800,update,updated,6800,https://hail.is,https://github.com/hail-is/hail/issues/12980,1,['update'],['updated']
Deployability,NFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 3/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 4/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 5/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 6/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 7/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 8/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 9/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 10/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Iterations complete. Computing local QR; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 _reduced_svd: Computing local SVD; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 blanczos_pca: SVD Complete. Computing conversion to PCs.; INFO batch_client.aioclient:aioclient.py:770 updated batch,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980:6957,update,updated,6957,https://hail.is,https://github.com/hail-is/hail/issues/12980,1,['update'],['updated']
Deployability,NFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 4/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 5/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 6/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 7/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 8/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 9/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 10/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Iterations complete. Computing local QR; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 _reduced_svd: Computing local SVD; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 blanczos_pca: SVD Complete. Computing conversion to PCs.; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980:7114,update,updated,7114,https://hail.is,https://github.com/hail-is/hail/issues/12980,1,['update'],['updated']
Deployability,"NFO: Getting 1 non-empty blocks out of 1 blocks; 2018-10-09 15:04:37 ShuffleBlockFetcherIterator: INFO: Started 0 remote fetches in 0 ms; 2018-10-09 15:04:37 Executor: INFO: Finished task 0.0 in stage 4.0 (TID 4). 1539 bytes result sent to driver; 2018-10-09 15:04:37 TaskSetManager: INFO: Finished task 0.0 in stage 4.0 (TID 4) in 7 ms on localhost (executor driver) (1/1); 2018-10-09 15:04:37 TaskSchedulerImpl: INFO: Removed TaskSet 4.0, whose tasks have all completed, from pool ; 2018-10-09 15:04:37 DAGScheduler: INFO: ResultStage 4 (collect at utils.scala:197) finished in 0.008 s; 2018-10-09 15:04:37 DAGScheduler: INFO: Job 2 finished: collect at utils.scala:197, took 0.051042 s; 2018-10-09 15:04:37 CodeGenerator: INFO: Code generated in 5.011153 ms; 2018-10-09 15:04:37 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:37 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table8508c46074` AS `zzz1`; WHERE (0 = 1); 2018-10-09 15:04:37 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:37 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table8508c46074`; 2018-10-09 15:04:38 root: INFO: optimize: before:; (TableCount; (TableKeyBy () False; (TableLiteral))); 2018-10-09 15:04:38 root: INFO: optimize: after:; (TableCount; (TableLiteral)); 2018-10-09 15:04:38 SparkContext: INFO: Starting job: fold at RVD.scala:361; 2018-10-09 15:04:38 DAGScheduler: INFO: Got job 3 (fold at RVD.scala:361) with 1 output partitions; 2018-10-09 15:04:38 DAGScheduler: INFO: Final stage: ResultStage 5 (fold at RVD.scala:361); 2018-10-09 15:04:38 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 15:04:38 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 15:04:38 DAGScheduler: INFO: Submitting ResultStage 5 (MapPartitionsRDD[28] at mapPartitions at ContextRDD.scala:137), which has no missing parents; 2018-10-09 15:04:38 MemoryStore: INF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:28667,configurat,configuration,28667,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['configurat'],['configuration']
Deployability,"NFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 Security",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8286,install,install,8286,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['install'],['install']
Deployability,"NFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 Secu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1488,install,install,1488,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['install'],['install']
Deployability,"NGELOG.md"">typing-extensions's changelog</a>.</em></p>; <blockquote>; <h1>Release 4.3.0 (July 1, 2022)</h1>; <ul>; <li>Add <code>typing_extensions.NamedTuple</code>, allowing for generic <code>NamedTuple</code>s on; Python &lt;3.11 (backport from <a href=""https://github-redirect.dependabot.com/python/cpython/issues/92027"">python/cpython#92027</a>, by Serhiy Storchaka). Patch; by Alex Waygood (<a href=""https://github.com/AlexWaygood""><code>@​AlexWaygood</code></a>).</li>; <li>Adjust <code>typing_extensions.TypedDict</code> to allow for generic <code>TypedDict</code>s on; Python &lt;3.11 (backport from <a href=""https://github-redirect.dependabot.com/python/cpython/issues/27663"">python/cpython#27663</a>, by Samodya Abey). Patch by; Alex Waygood (<a href=""https://github.com/AlexWaygood""><code>@​AlexWaygood</code></a>).</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python/typing_extensions/commit/9c0759a260fe126210a1e2026720000a3c40a919""><code>9c0759a</code></a> Prepare release 4.3.0 (<a href=""https://github-redirect.dependabot.com/python/typing_extensions/issues/52"">#52</a>)</li>; <li><a href=""https://github.com/python/typing_extensions/commit/1baf0a58b2b4c5327871d06801187cba47aa6975""><code>1baf0a5</code></a> Backport generic <code>TypedDict</code>s (<a href=""https://github-redirect.dependabot.com/python/typing_extensions/issues/46"">#46</a>)</li>; <li><a href=""https://github.com/python/typing_extensions/commit/7c28357e412cef215a7d66c0ef69b568b316678b""><code>7c28357</code></a> Add a backport of generic <code>NamedTuple</code>s (<a href=""https://github-redirect.dependabot.com/python/typing_extensions/issues/44"">#44</a>)</li>; <li><a href=""https://github.com/python/typing_extensions/commit/7198c63522f3f435f8610c88ed0f5d0cf9d26091""><code>7198c63</code></a> Add <code>.gitignore</code>, <code>.editorconfig</code>, <code>CONTRIBUTING.md</code> (<a href=""https://github-redirect.dependabot.com/python/typing_ex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12200:1271,release,release,1271,https://hail.is,https://github.com/hail-is/hail/pull/12200,1,['release'],['release']
Deployability,NPE on aggregator pipeline,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3731:18,pipeline,pipeline,18,https://hail.is,https://github.com/hail-is/hail/issues/3731,1,['pipeline'],['pipeline']
Deployability,"Namely, you can solve this. > I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control. without making your new configuration the default, right? The 2nd change you made restored the default behavior for the case of tokens.json, which, it seems to me, gives you inconsistent behavior for 'deploy-config.json', or if we ever changes tokens.json to some other name.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535660700:204,configurat,configuration,204,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535660700,2,"['configurat', 'deploy']","['configuration', 'deploy-config']"
Deployability,NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:11677,Configurat,Configuration,11677,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Configurat'],['Configuration']
Deployability,Need to update build.gradle and test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4578:8,update,update,8,https://hail.is,https://github.com/hail-is/hail/issues/4578,1,['update'],['update']
Deployability,Need to update list of annotations in sampleqc doc.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/851#issuecomment-250204932:8,update,update,8,https://hail.is,https://github.com/hail-is/hail/pull/851#issuecomment-250204932,1,['update'],['update']
Deployability,Need to update the GitHub web hook once this goes in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6399:8,update,update,8,https://hail.is,https://github.com/hail-is/hail/pull/6399,1,['update'],['update']
Deployability,"Need to update the version and regenerate backcompat files, should this pass.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12981#issuecomment-1533991561:8,update,update,8,https://hail.is,https://github.com/hail-is/hail/pull/12981#issuecomment-1533991561,1,['update'],['update']
Deployability,Needed to debug Laurent's pipeline. And we should be logging everything anyway -- this is the only way we get to see the post-extract-aggregators executed IR,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5749:26,pipeline,pipeline,26,https://hail.is,https://github.com/hail-is/hail/pull/5749,1,['pipeline'],['pipeline']
Deployability,"Needed to sub in commons-io, which we were getting transitively from lsmtree. I picked the version that hadoop/spark depend on at runtime. lsmtree depends on an older one but currently in `main` gradle upgrades it for the one requested by the runtime dependencies.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11708:202,upgrade,upgrades,202,https://hail.is,https://github.com/hail-is/hail/pull/11708,1,['upgrade'],['upgrades']
Deployability,Needs a Sphinx update. We should build the docs locally and spot check the results before making this change. In particular try `make -C website run` after updating Sphinx and make sure it looks fine.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11503#issuecomment-1059658439:15,update,update,15,https://hail.is,https://github.com/hail-is/hail/pull/11503#issuecomment-1059658439,1,['update'],['update']
Deployability,"Needs a `python3 -m black batch --line-length=120 --skip-string-normalization`; ```; PYTHONPATH=${PYTHONPATH:+${PYTHONPATH}:}../hail/python:../gear:../web_common python3 -m black . --line-length=120 --skip-string-normalization --check --diff; --- batch/driver/main.py	2023-04-05 14:40:12.638902 +0000; +++ batch/driver/main.py	2023-04-05 14:44:25.172615 +0000; @@ -1226,11 +1226,12 @@; ; INSERT INTO aggregated_billing_project_user_resources_v3 (billing_project, `user`, resource_id, token, `usage`); SELECT billing_project, `user`, resource_id, 0, `usage`; FROM scratch; ON DUPLICATE KEY UPDATE `usage` = `usage` + scratch.`usage`;; -'''); +'''; + ); ; await compact() # pylint: disable=no-value-for-parameter; ; ; async def compact_agg_billing_project_users_by_date_table(app):; @@ -1254,11 +1255,12 @@; ; INSERT INTO aggregated_billing_project_user_resources_by_date_v3 (billing_date, billing_project, `user`, resource_id, token, `usage`); SELECT billing_date, billing_project, `user`, resource_id, 0, `usage`; FROM scratch; ON DUPLICATE KEY UPDATE `usage` = `usage` + scratch.`usage`;; -'''); +'''; + ); ; await compact() # pylint: disable=no-value-for-parameter; ; ; USER_CORES = pc.Gauge('batch_user_cores', 'Batch user cores (i.e. total in-use cores)', ['state', 'user', 'inst_coll']); would reformat batch/driver/main.py. Oh no! 💥 💔 💥; 1 file would be reformatted, 93 files would be left unchanged.; make[1]: *** [Makefile:18: check] Error 1; make[1]: Leaving directory '/io/repo/batch'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12850#issuecomment-1518033925:589,UPDATE,UPDATE,589,https://hail.is,https://github.com/hail-is/hail/pull/12850#issuecomment-1518033925,2,['UPDATE'],['UPDATE']
Deployability,"New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:7218,configurat,configuration,7218,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['configurat'],['configuration']
Deployability,"New PR for NativeModule etc. - NativeModule now has a single big_mutex, so that it is single-threaded (releasing big_mutex; only while sleeping between polling file state). - The run_shell_get_first_line() has been removed, moving almost all configuration into the; module-build makefile; ; - Simplified Makefile. - Remove some historical code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4211:242,configurat,configuration,242,https://hail.is,https://github.com/hail-is/hail/pull/4211,1,['configurat'],['configuration']
Deployability,New dev deploy after switching from `gs://b` to `b` because that's apparently the format dataproc wants. https://ci.hail.is/batches/8125093,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14270#issuecomment-1938912707:8,deploy,deploy,8,https://hail.is,https://github.com/hail-is/hail/pull/14270#issuecomment-1938912707,1,['deploy'],['deploy']
Deployability,Nice update!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4113#issuecomment-413067829:5,update,update,5,https://hail.is,https://github.com/hail-is/hail/pull/4113#issuecomment-413067829,1,['update'],['update']
Deployability,"Nice work. Let's block release on this, clearly it was bugged before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8436#issuecomment-609932494:23,release,release,23,https://hail.is,https://github.com/hail-is/hail/pull/8436#issuecomment-609932494,2,['release'],['release']
Deployability,"Nice!!! This appears to have skipped all the extraneous stages, and my pipeline goes straight to the write stage!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4293#issuecomment-419658619:71,pipeline,pipeline,71,https://hail.is,https://github.com/hail-is/hail/pull/4293#issuecomment-419658619,1,['pipeline'],['pipeline']
Deployability,Nik ran into humanize not being imported to use Pipeline. I added it to the requirements.txt that I believe is the correct requirements.txt. I think the other ones in the docker folder are just for creating docker images.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8098:48,Pipeline,Pipeline,48,https://hail.is,https://github.com/hail-is/hail/pull/8098,1,['Pipeline'],['Pipeline']
Deployability,Nirvana update,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2300:8,update,update,8,https://hail.is,https://github.com/hail-is/hail/pull/2300,1,['update'],['update']
Deployability,"No changelog because the copy tool is not properly public, though; maybe it can be public now. The main problem was that we were saturating our network bandwidth; with data we did not need. Why? As far as I can tell, `release` on; an `aiohttp.ClientResponse` is insufficient to stop the receipt of; more bytes. I realized one proper fix was to simply tell the cloud storage vendors; how much data we wanted to receive. That is the change to `open_from`; which I have made pervasively to all clouds. I also changed `release` to `close` because that seems more correct.; I do not understand why we only `release`d it before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11778:218,release,release,218,https://hail.is,https://github.com/hail-is/hail/pull/11778,3,['release'],['release']
Deployability,"No no, I reset the codecs afterwards. I tested and it works as intended; (loading a .gz annotation file with the Gzip codec). I'm trying to fix the; small letter / capital issue (thanks Daniel), but it Git seems to be; case-insensitive when it comes to files... On Wed, Sep 21, 2016 at 11:19 AM, Tim Poterba notifications@github.com; wrote:. > This sets the configuration permanently -- any following commands will use; > the overridden codecs. Setting a global option is almost certainly better; > than getting this kind of leakage, I think; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/826#issuecomment-248645129, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgYRNZnsCXFQnDx9z5wRR1WD4rr0cks5qsUr_gaJpZM4KC1O-; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/826#issuecomment-248646084:358,configurat,configuration,358,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248646084,1,['configurat'],['configuration']
Deployability,No sorry I'm asking if [this](https://github.com/hail-is/hail/blob/3a8be2b37ec387cbf0664354148a66e5930f2f88/infra/azure/modules/batch/main.tf#L27-L40) is unused and can be deleted or whether we are using it somewhere (and need to update it).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13440#issuecomment-1688974347:230,update,update,230,https://hail.is,https://github.com/hail-is/hail/pull/13440#issuecomment-1688974347,1,['update'],['update']
Deployability,"No state. I printed the session in the logs (I was deploying into prod to test while I had a broken cookie). My session had nothing set except that it was marked as created on January 7th. This despite that I saw a log statement from when I hit /login,m that clearly showed me session with all the right values. My guess is that there was some old signing key or somehow the session got corrupted so you can decode it to get an empty session and add new fields but they fail to be written back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8052#issuecomment-583226388:51,deploy,deploying,51,https://hail.is,https://github.com/hail-is/hail/pull/8052#issuecomment-583226388,2,['deploy'],['deploying']
Deployability,"No update yet, sorry. We've just found a pipeline that may replicate it more easily, so hoping for a fix soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-683888038:3,update,update,3,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-683888038,2,"['pipeline', 'update']","['pipeline', 'update']"
Deployability,No worker deploy config secret,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13211:10,deploy,deploy,10,https://hail.is,https://github.com/hail-is/hail/pull/13211,1,['deploy'],['deploy']
Deployability,"No, but it will be included in the next release, which should be in the next week or two.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14297#issuecomment-1967694813:40,release,release,40,https://hail.is,https://github.com/hail-is/hail/pull/14297#issuecomment-1967694813,1,['release'],['release']
Deployability,"No, but its the same string I used to install hail dependencies for Terra's notebooks. I can run a test now",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9502#issuecomment-698497283:38,install,install,38,https://hail.is,https://github.com/hail-is/hail/pull/9502#issuecomment-698497283,1,['install'],['install']
Deployability,"No, it will be in 0.2.129 release date currently unknown, but we think it will be early March.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14297#issuecomment-1967979634:26,release,release,26,https://hail.is,https://github.com/hail-is/hail/pull/14297#issuecomment-1967979634,1,['release'],['release']
Deployability,"No, the framework has changed a lot since then so it just works now. My guess for why this happens would be memory issues, but not sure. What's your pipeline?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053#issuecomment-419657089:149,pipeline,pipeline,149,https://hail.is,https://github.com/hail-is/hail/issues/3053#issuecomment-419657089,1,['pipeline'],['pipeline']
Deployability,"No, the input strings are all on `gs://` but in the error I get:. ```; subprocess.CalledProcessError: Command '#!/bin/bash; # change cd to tmp directory; cd /tmp//pipeline-dc5b53d50f45/. cp /Users/konradk/Dropbox (Partners HealthCare)/src/python/gnomad_hail/gs:/phenotype...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5759#issuecomment-479599017:163,pipeline,pipeline-,163,https://hail.is,https://github.com/hail-is/hail/issues/5759#issuecomment-479599017,1,['pipeline'],['pipeline-']
Deployability,"No, you're totally right, good catch! Fixed. How did this ever work? Because `get-deployed-sha.sh` was universally broken? I also added the (recently added) spark project to the list of projects.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4573#issuecomment-431244990:82,deploy,deployed-sha,82,https://hail.is,https://github.com/hail-is/hail/pull/4573#issuecomment-431244990,1,['deploy'],['deployed-sha']
Deployability,"Nobody has ever asked for this. I'm tabling it for now. If it does come up, we should integrate with Hadoop-BAM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/35#issuecomment-208692497:86,integrat,integrate,86,https://hail.is,https://github.com/hail-is/hail/issues/35#issuecomment-208692497,1,['integrat'],['integrate']
Deployability,None:; raise reconstructed_error; > raise reconstructed_error.maybe_user_error(ir); E hail.utils.java.FatalError: NativeIoException: readAddress(..) failed: Connection reset by peer; E ; E Java stack trace:; E io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer; E 	at ; E ; E ; E ; E Hail version: 0.2.115-330031a5d973; E Error summary: NativeIoException: readAddress(..) failed: Connection reset by peer. /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:477: FatalError; ------------------------------ Captured log call -------------------------------; INFO batch_client.aioclient:aioclient.py:753 created batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 _make_tsm: found 1000 variants after filtering out monomorphic sites.; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 1/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 2/10; INFO batch_client.aioclient:aioclient.py:770 updated batch 3780293; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 3/10; INFO batch_client.aioclient:aioclient.py:770 up,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980:5157,update,updated,5157,https://hail.is,https://github.com/hail-is/hail/issues/12980,1,['update'],['updated']
Deployability,"Not a pressing issue. It may be helpful to have each makefile present a slightly more consistent user interface. For instance:; Letsencrypt: `run` (there is a deploy but this is one of the earlier dependencies); Gateway: `deploy`; Notebook: `deploy`; Scorecard: `deploy` (there is a `run` but it's used to execute scorecard.py). @danking may have thoughts on this, or not :)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5413:159,deploy,deploy,159,https://hail.is,https://github.com/hail-is/hail/issues/5413,4,['deploy'],['deploy']
Deployability,"Not all of the Hail Tables and MatrixTables that are publicly available on the gnomAD [downloads](https://gnomad.broadinstitute.org/downloads) page are currently available in the Datasets API/Annotation DB. . This PR makes the following changes to the datasets available via the Hail Datasets API/Annotation DB:. - Add `gnomad_genome_sites` Table, versions: 3.1.1, 3.1.2; - Add `gnomad_hgdp_1kg_subset_dense` MatrixTable, version: 3.1.2; - Rename `gnomad_hgdp_1kg_callset` MatrixTable to `gnomad_hgdp_1kg_subset_dense`, version: 3.1; - Add `gnomad_hgdp_1kg_subset_sparse` MatrixTable, version: 3.1.2; - Add `gnomad_hgdp_1kg_subset_sample_metadata` Table, version: 3.1.2; - Add `gnomad_hgdp_1kg_subset_variant_annotations` Table, version: 3.1.2; - Add `gnomad_variant_co-occurrence` Table, version: 2.1.1; - Add `gnomad_pca_variant_loadings` Table, versions: 2.1, 3.1. Other general changes:. - Add/update the schema `.rst` files, for the datasets listed above, for the [docs](https://hail.is/docs/0.2/datasets/schemas.html); - Set the example dataset loaded in `hl.experimental.load_dataset` to be the most recent `gnomad_hgdp_1kg_subset_dense` MatrixTable (version 3.1.2)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11608:898,update,update,898,https://hail.is,https://github.com/hail-is/hail/pull/11608,1,['update'],['update']
Deployability,Not for integrating in the current state.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/979:8,integrat,integrating,8,https://hail.is,https://github.com/hail-is/hail/pull/979,1,['integrat'],['integrating']
Deployability,"Not in our batch jobs, but we do use tokens to submit on behalf of different users, eg: https://github.com/populationgenomics/analysis-runner/blob/d7f6d8fa5b61ac20f9952c50ee9ce27bd0ba5974/server/main.py#L105. So outside yes, but inside - we updated hail batch and I'm pretty confident it's using Oauth.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14059#issuecomment-1841779614:241,update,updated,241,https://hail.is,https://github.com/hail-is/hail/pull/14059#issuecomment-1841779614,1,['update'],['updated']
Deployability,"Not possible until more batch infrastructure is done, but want to get the change out to Konrad. ```; def test_gcs_file_localization(self):; p = Pipeline(); input = p.read_input(f'{gcs_input_dir}/hello.txt'); t = p.new_task(); t.command(f'cat {input} > {t.ofile}'); p.write_output(t.ofile, f'{gcs_output_dir}/hello.txt'); p.run(); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5767:144,Pipeline,Pipeline,144,https://hail.is,https://github.com/hail-is/hail/issues/5767,1,['Pipeline'],['Pipeline']
Deployability,"Not sure this is the right fix, but I believe we can't update a batch once it's been cancelled still.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12519:55,update,update,55,https://hail.is,https://github.com/hail-is/hail/pull/12519,1,['update'],['update']
Deployability,Not sure why these are not already ignored. They seem to get created when I; run `./gradlew install-editable`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8797:92,install,install-editable,92,https://hail.is,https://github.com/hail-is/hail/pull/8797,1,['install'],['install-editable']
Deployability,Not sure. It doesn't throw an error on my version of anaconda. Which I've been meaning to update for awhile because all of the make files fail with my version of conda...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5563#issuecomment-471737999:90,update,update,90,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-471737999,1,['update'],['update']
Deployability,"Not that it helps in the commit history, but at least it's somewhere... this PR fixed that the secrets for batch deployment needed the deploy flag on which secret to use depending on whether we're testing or in production.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6168#issuecomment-495361303:113,deploy,deployment,113,https://hail.is,https://github.com/hail-is/hail/pull/6168#issuecomment-495361303,2,['deploy'],"['deploy', 'deployment']"
Deployability,Not using anything prepackaged specifically for amazon. Just running the `make install-on-cluster HAIL_COMPILE_NATIVES=1 SPARK_VERSION=3.1.2` command on my instance which kicks off the install and runs through the requirements wheel through the Makefile. So wouldn't line 238 of the [Makefile](https://github.com/hail-is/hail/blob/57537fea08d4dcb1548a4ab1f55f093eae9bd016/hail/Makefile#L238) need to be adjusted with the changes?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255358708:79,install,install-on-cluster,79,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255358708,2,['install'],"['install', 'install-on-cluster']"
Deployability,"Not yet deployed by default. Builds Jupyter image that runs against apiserver by default. This will be used by the notebook service to spin up a demo notebook. The make target `run-hail-jupyter-pod` launches a single pod running the image by hand. Image includes Hadoop Google Storage connector. Deployment mounts a secret with a service account key to access files which can access the bucket gs://haas-scratch. This completes the backend work for the Feb 5 data meeting demo. Round trip for a trivial job is about 80ms. From the Jupyter notebook running agains the apiserver:. ```; %%time; hl.utils.range_matrix_table(346, 100).count(); CPU times: user 20.6 ms, sys: 5.17 ms, total: 25.8 ms; Wall time: 81.2 ms; ```. For a job that hits Spark, around 300ms:. ```; mt = hl.utils.range_matrix_table(346, 100); mt = mt.annotate_rows(x = mt.row_idx*mt.row_idx); t = mt.rows(). %%time; t.aggregate(hl.agg.sum(t.x)); CPU times: user 12.6 ms, sys: 1.21 ms, total: 13.8 ms; Wall time: 304 ms; 13747445; ```. Things that hit Google storage are noticeably slower:. ```; %%time; mt = hl.read_matrix_table('gs://haas-scratch/sample.mt'); mt.aggregate_rows(hl.agg.sum(hl.len(mt.alleles))); CPU times: user 28.3 ms, sys: 2.63 ms, total: 30.9 ms; Wall time: 1.58 s; ```. I think we can speed this up by caching read IR. Also, the RVD metadata is stored in a separate file, so executing the read requires an additional read besides reading the actual data file. We might want to rethink our directory layout.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5213:8,deploy,deployed,8,https://hail.is,https://github.com/hail-is/hail/pull/5213,2,"['Deploy', 'deploy']","['Deployment', 'deployed']"
Deployability,"Note some highlights from the log:; ```; #12 42.27 ./Bio/tmp/Bio-DB-HTS-2.9 - moving files to ./biodbhts; #12 42.27 - making Bio::DB:HTS; #12 42.40 Checking prerequisites...; #12 42.40 requires:; #12 42.40 ! Bio::Root::Version is not installed; #12 42.40 ; #12 42.40 ERRORS/WARNINGS FOUND IN PREREQUISITES. You may wish to install the versions; #12 42.40 of the modules indicated above before proceeding with this installation; #12 42.40 ; #12 42.40 Run 'Build installdeps' to install missing prerequisites.; ```; ```; #13 138.3 Building and testing Test2-Suite-0.000152 ... ! Installing Test2::V0 failed. See /root/.cpanm/work/1682614674.13506/build.log for details. Retry with --force to force install it.; #13 150.9 FAIL; #13 150.9 --> Working on FFI::CheckLib; #13 150.9 Fetching http://www.cpan.org/authors/id/P/PL/PLICEASE/FFI-CheckLib-0.31.tar.gz ... OK; #13 150.9 Configuring FFI-CheckLib-0.31 ... OK; #13 151.1 ==> Found dependencies: Test2::V0, Test2::Require::EnvVar, Test2::Require::Module; #13 151.1 ! Installing the dependencies failed: Module 'Test2::Require::EnvVar' is not installed, Module 'Test2::V0' is not installed, Module 'Test2::Require::Module' is not installed; #13 151.1 ! Bailing out the installation for FFI-CheckLib-0.31. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12946#issuecomment-1526412230:234,install,installed,234,https://hail.is,https://github.com/hail-is/hail/issues/12946#issuecomment-1526412230,12,"['Install', 'install']","['Installing', 'install', 'installation', 'installdeps', 'installed']"
Deployability,"Note that pandas 2.0.0 [removes the deprecated `DataFrame.iteritems()`](https://pandas.pydata.org/docs/whatsnew/v2.0.0.html#removal-of-prior-version-deprecations-changes), which is used by bokeh-1.4.0. That particular old version of bokeh is listed in _hail/python/requirements.txt_ but it is thus incompatible with pandas 2; so one or the other of these pinnings probably needs to be revisited. (This incompatibility has caused the [large_cohort unit test failure](https://github.com/populationgenomics/production-pipelines/actions/runs/4782280056/jobs/8501466504?pr=354#step:5:134) in populationgenomics/production-pipelines#354.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1519857581:515,pipeline,pipelines,515,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1519857581,2,['pipeline'],['pipelines']
Deployability,"Note this PR replaces the previous [Feature/sas token merge](https://github.com/hail-is/hail/pull/12877) because the original PR branch got jacked up beyond repair. All the comments on the earlier PR are responded to there and addressed in the code for this one. This PR is to enable `hail-az/https` Azure file references to contain SAS tokens to enable bearer-auth style file access to Azure storage. Basic summary of the changes:; - Update `AzureAsyncFS` url parsing function to look for and separate out a SAS-token-like query string. Note: made fairly specific to SAS tokens - generic detection of query string syntax interferes with glob support and '?' characters in file names; - Added `generate_sas_token` convenience function to `AzureAsyncFS`. Adds new azure-mgmt-storage package requirement.; - Updated `AzureAsyncFS` to use `(account, container, credential)` tuple as internal `BlobServiceClient` cache key; - Updated `AzureAsyncFSURL` and `AzureFileListEntry` to track the token separately from the name, and extend the base classes to allow returning url with or without a token; - Update `RouterFS.ls` function and associated listfiles function to allow for trailing query strings during path traversal; - Update `AsyncFS.open_from` function to handle query-string urls in zero-length case; - Change to existing behavior: `LocalAsyncFSURL.__str__` no longer returns 'file:' prefix. Done to make `str()` output be appropriate for input to `fs` functions across all subclasses; - Updated `InputResource` to not include the SAS token as part of the destination file name; - Updated `inter_cloud/test_fs.py` to generically use query-string-friendly file path building functions to respect the new model, where it is no longer safe to extend URLs by just appending new segments with `+ ""/""` because there may be a query string, and added `'sas/azure-https'` test case to the fixture. Running tests for the SAS case requires some new test variables to allow the test code to generate SAS toke",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13140:435,Update,Update,435,https://hail.is,https://github.com/hail-is/hail/pull/13140,4,['Update'],"['Update', 'Updated']"
Deployability,"Note: I have not updated the batch page for CI pipelines or the batch page for non-CI batch because I wanted to first run this change by the rest of the team, hence the WIP status on the PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13148#issuecomment-1579334984:17,update,updated,17,https://hail.is,https://github.com/hail-is/hail/pull/13148#issuecomment-1579334984,2,"['pipeline', 'update']","['pipelines', 'updated']"
Deployability,Notebook deploy is broken,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4656:9,deploy,deploy,9,https://hail.is,https://github.com/hail-is/hail/issues/4656,1,['deploy'],['deploy']
Deployability,"Notebook2 was _literally_ unusable (no favicon). Instead of copying and pasting the favicon link 5 times, I also extracted out the shared elements into a template, and extended it in all other views. How this works:; `layout.html`: contains all shared elements, and marks places where children can insert content (`{% block title %}{% endblock %}`, `{% block head %}{% endblock %}`, `{% block content %}{% endblock %}`). Every other file extends this. The 2 templates that weren't updated (admin-login.html, and workers.html) are placeholders from notebook1 that haven't been updated for notebook 2 yet; they should work, but don't use notebook2 styles, and therefore don't have shared elements to wrap in layout.html. This all works. cc @cseed, @jigold, @danking, @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5827:481,update,updated,481,https://hail.is,https://github.com/hail-is/hail/pull/5827,2,['update'],['updated']
Deployability,Nothing suspicious there. Something is going wrong in the executors. I think the only way we're gonna solve this is by running a pipeline and looking at the executor logs. I'm at a complete loss for how Jupyter could affect what happens on the executors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13690#issuecomment-1731963811:129,pipeline,pipeline,129,https://hail.is,https://github.com/hail-is/hail/issues/13690#issuecomment-1731963811,1,['pipeline'],['pipeline']
Deployability,"Now it seems you should use string.isMissing, while this returns an error.; I suggest isMissing(a), which makes clear the proper use of the syntax.; In Brief: update help here: https://github.com/broadinstitute/hail/blob/master/docs/HailExpressionLanguage.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/374:159,update,update,159,https://hail.is,https://github.com/hail-is/hail/issues/374,1,['update'],['update']
Deployability,"Now passing by killing safety on readPartitions, I've updated the comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2367#issuecomment-340074758:54,update,updated,54,https://hail.is,https://github.com/hail-is/hail/pull/2367#issuecomment-340074758,1,['update'],['updated']
Deployability,"Now that createDatabase is gone, rename createDatabase2Step => createDatabase2, and accept createDatabase in build.yaml for creating database. In follow up PRs, I will:; - rename createDatabase2 => createDatabase in build.yaml,; - don't support createDatabase2, completing the change. I can't do this in one change because this PR is tested/deployed by the _previous_ CI, not the one in this PR, so it has to be done in stages. Such is the microservices life.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7887:341,deploy,deployed,341,https://hail.is,https://github.com/hail-is/hail/pull/7887,1,['deploy'],['deployed']
Deployability,"Now there are `hail-uk-vep` and `hail-eu-vep` buckets. Both are requester pays. This PR adds mappings from regions in those areas to the corresponding VEP data replicates. Once this goes in, we can do a release, after which we can change the permissions on the current hail vep folder to be not public (eventually we will delete, but maybe there's something in there I haven't noticed yet that people need / want).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8289:203,release,release,203,https://hail.is,https://github.com/hail-is/hail/pull/8289,1,['release'],['release']
Deployability,O batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 1/4; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 2/4; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 3/4; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 krylov_factorization: Beginning iteration 4/4; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 krylov_factorization: Iterations complete. Computing local QR; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 _reduced_svd: Computing local SVD; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO backend.service_backend:java.py:190 blanczos_pca: SVD Complete. Computing conversion to PCs.; INFO batch_client.aioclient:aioclient.py:770 updated batch 3776913; INFO batch_cl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:24525,update,updated,24525,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['update'],['updated']
Deployability,"OK, I cleaned this up a bit. Now stacked on: https://github.com/hail-is/hail/pull/5826. Summary of changes:; - added heal; - pipeline is now batch rather than job centeric; - batch logs page shows logs for all batch jobs; - GET /batches/{id} endpoint now returns entire array of jobs, instead of the state counter and exit_codes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5814#issuecomment-481079454:125,pipeline,pipeline,125,https://hail.is,https://github.com/hail-is/hail/pull/5814#issuecomment-481079454,1,['pipeline'],['pipeline']
Deployability,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5891:534,deploy,deploy,534,https://hail.is,https://github.com/hail-is/hail/pull/5891,6,"['configurat', 'deploy']","['configuration', 'deploy', 'deployment']"
Deployability,"OK, I made a suite of additional changes:; - create docker/requirements.txt,; - batch doesn't use conda,; - pr-builder (rebuilt) installs docker/requirements.txt (same requirements as base image),; - put Spark in base image, removed spark-base image,; - set ENV IN_HAIL_CI=1 in hail-ci scripts,; - pull remote images and use --cache-from, allow push and deploy in CI, build locally only otherwise",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5655#issuecomment-475123687:129,install,installs,129,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475123687,2,"['deploy', 'install']","['deploy', 'installs']"
Deployability,"OK, I reverted the retry on apt-get install, but kept the refactoring that makes apt-get update and pip use the same retry script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9906#issuecomment-767084684:36,install,install,36,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-767084684,2,"['install', 'update']","['install', 'update']"
Deployability,"OK, I think I addressed all the comments. Here is a summary of the code changes I made:; - use *-tokens instead of *-jwt for the session tokens; - db event to clean up sessions,; - there was a bunch of legacy garbage in batch/ and batch/Makefile that I nuked,; - added hailtop test for deploy_config,; - fixed up hail ServiceBackend (which isn't actually tested now); - create_user => insert_user. I think there is still some legacy garbage in apiserver/, but that's not being deployed right now so I just left it. Let me know if I missed anything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-528993572:477,deploy,deployed,477,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-528993572,1,['deploy'],['deployed']
Deployability,"OK, I think I addressed the comments and it is ready for another look. I tested the batch2 UI witih deploy dev and it seems good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7530#issuecomment-555241754:100,deploy,deploy,100,https://hail.is,https://github.com/hail-is/hail/pull/7530#issuecomment-555241754,1,['deploy'],['deploy']
Deployability,"OK, I think I've addressed all the comments. I made some additional changes:. - pipe input file directly into tar instead of writing to disk (writing to SSDs, I get ~100MB/s/core download saturating gs://),; - report records read,; - directly create OrderedRVD instead of coercing,; - updated GenomicsDB to latest: 0.9.2-proto-3.0.0-beta-1+ed318f7e815 which involved revising GenomicsDBFeatureReader ctor call",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3537#issuecomment-388564861:285,update,updated,285,https://hail.is,https://github.com/hail-is/hail/pull/3537#issuecomment-388564861,1,['update'],['updated']
Deployability,"OK, I think this PR is OK wrt that issue. I introduce one SQL command and it uses FOR UPDATE because we later insert that record.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-575754308:86,UPDATE,UPDATE,86,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575754308,1,['UPDATE'],['UPDATE']
Deployability,"OK, I think this is ready to go.; - Migration tested with passing colors (!); - I disabled the check incremental loop (but left the code in for future debugging); - I updated estimated-current.txt with the latest from improve-cancel.sql",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7933#issuecomment-577704626:167,update,updated,167,https://hail.is,https://github.com/hail-is/hail/pull/7933#issuecomment-577704626,1,['update'],['updated']
Deployability,"OK, I won't be able to fix this. @ehigham @patrick-schultz @daniel-goldstein some combo of you three can probably figure it out. The local backend tests that hit requester pays buckets are failing with new Spark. New Spark needs new GCS hadoop connector (see the Dockerfiles). New GCS hadoop connector has [brand new configuration parameters](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/v3.0.0/gcs/INSTALL.md). Somehow I managed to make the normal Spark backend work correctly but the Local backend (which still, afaik, uses Spark & Hadoop for filesystems) is still trying to pick up CI's credentials instead of the test account's credentials. ```; E hail.utils.java.FatalError: GoogleJsonResponseException: 403 Forbidden; E GET https://storage.googleapis.com/storage/v1/b/hail-test-requester-pays-fds32/o/zero-to-nine?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata&userProject=hail-vdc; E {; E ""code"": 403,; E ""errors"": [; E {; E ""domain"": ""global"",; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist)."",; E ""reason"": ""forbidden""; E }; E ],; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist).""; E }; E ; E Java stack trace:; E java.io.IOException: Error accessing gs://hail-test-requester-pays-fds32/zero-to-nine; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1986); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1882); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloud",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236:317,configurat,configuration,317,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236,3,"['INSTALL', 'configurat', 'update']","['INSTALL', 'configuration', 'updated']"
Deployability,"OK, I won't notify you about importlib-metadata again, unless you re-open this PR or update it yourself. 😢",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11596#issuecomment-1069575150:85,update,update,85,https://hail.is,https://github.com/hail-is/hail/pull/11596#issuecomment-1069575150,1,['update'],['update']
Deployability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14507#issuecomment-2206596777:40,release,release,40,https://hail.is,https://github.com/hail-is/hail/pull/14507#issuecomment-2206596777,16,"['release', 'update']","['release', 'updates']"
Deployability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an [`ignore` condition](https://docs.github.com/en/code-security/supply-chain-security/configuration-options-for-dependency-updates#ignore) with the desired `update_types` to your config file. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13576#issuecomment-1709450787:40,release,release,40,https://hail.is,https://github.com/hail-is/hail/pull/13576#issuecomment-1709450787,1275,"['configurat', 'patch', 'release', 'update']","['configuration-options-for-dependency-updates', 'patch', 'release', 'releases', 'updates']"
Deployability,"OK, I'll upgrade to 3.7. There is a Python ppa with the latest version.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5623#issuecomment-474007645:9,upgrade,upgrade,9,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474007645,1,['upgrade'],['upgrade']
Deployability,"OK, I'm not sure how to fix this but the work is to explain to the GCS Hadoop Connector which credentials we want it to use. See the failure here: https://batch.hail.is/batches/8136069/jobs/49 . It uses CI's credentials instead of the test credentials. We use core-site.xml to do this in Spark <3.5, but the GCS connector is different in Spark 3.5 and it uses different configuration parameters. My most recent change did not successfully configure it. Daniel G can help you a bit with credentials in Batch if that's necessary but the real work is to figure out how to tell the GCS Hadoop Connector to use the /gsa-key/key.json file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1961672844:370,configurat,configuration,370,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1961672844,1,['configurat'],['configuration']
Deployability,"OK, I've made most of the changes and I'd appreciate some feedback before I finalize the PR. Notable changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:168,configurat,configuration,168,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,2,['configurat'],['configuration']
Deployability,"OK, a third option:. Gradle has support for something called a Gradle wrapper, a set of distribution scripts that download and run a specific version of Gradle. I just added a Gradle wrapper for 2.14.1 to the master branch. You should now be able to build the local version of Hail with `gradlew installDist` or `./gradlew shadowJar` to build the shadow (fat, uber) jar to run against a Spark cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240309173:296,install,installDist,296,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240309173,1,['install'],['installDist']
Deployability,"OK, cool. Thanks for adding the dataproc tests! I'll approve this once the current release goes in, then we'll try this out for the next release, 0.2.38.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8550#issuecomment-613603462:83,release,release,83,https://hail.is,https://github.com/hail-is/hail/pull/8550#issuecomment-613603462,2,['release'],['release']
Deployability,"OK, everything checks out in dev deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7996#issuecomment-580044196:33,deploy,deploy,33,https://hail.is,https://github.com/hail-is/hail/pull/7996#issuecomment-580044196,1,['deploy'],['deploy']
Deployability,"OK, fixed and deployed. It works now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536246943:14,deploy,deployed,14,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536246943,1,['deploy'],['deployed']
Deployability,"OK, for the record, I changed ci-agent to:; - only exist in batch-pods,; - have deploy permissions in both default and batch-pods. In the future, I think we should:; - have a service account just for creating and destroying namespace (creator-and-destroyer-of-namespaces) used by create namespace step,; - the deploy step should use the admin account for the target namespace,; - and similarly for create database. This means we'll eventually need to include the namespace with the service account in batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7470#issuecomment-551005475:80,deploy,deploy,80,https://hail.is,https://github.com/hail-is/hail/pull/7470#issuecomment-551005475,2,['deploy'],['deploy']
Deployability,"OK, so that was way more pain than I expected. Apparently when you're importing a JS module which needs authentication, you must specify the `crossorigin` attribute to the `script` tag. If you lack that attribute, no headers are sent. Since dev deploys are limited to developer access only (even though there is no in-dev-namespace authentication), this obviously doesn't work. Why does the word `crossorigin` mean send headers to the same origin? Who knows! Anyway. I don't think that bug should block this PR. That bug is an underlying issue with dev deploy. The fix for the module import bug is here: https://github.com/hail-is/hail/pull/8928",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8923#issuecomment-639135078:245,deploy,deploys,245,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639135078,2,['deploy'],"['deploy', 'deploys']"
Deployability,"OK, so, this is apparently an issue where browsers have not yet really implemented the standard correctly. From my reading of [HTML 2.6.4](https://html.spec.whatwg.org/#cors-settings-attributes), `crossorigin=""anonymous""` ought to be sufficient for requests to the same origin as the page containing the script tag. Jake Archibald has an informative [blog post](https://jakearchibald.com/2017/es-modules-in-browsers/) about modules. He links to a [demo](https://module-script-tests-sreyfhwvpq.now.sh/cookie-page) of three cross origin configurations. The three options are:; ```; <script type=""module"" src=""cookie-script""></script>; <script type=""module"" crossorigin="""" src=""cookie-script?1""></script>; <script type=""module"" crossorigin=""use-credentials"" src=""cookie-script?2""></script>; ```; I'm using Safari Version 13.1 (14609.1.20.111.8). I usually only see the very last script working. However, inexplicably, I have seen the first one very rarely work. All I've been doing is refreshing here and there as I try to understand this. The Safari inspector confirms that the cookie is only sent with thee last option. So, anyway, I'm adding `crossorigin=""use-credentials""`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8928#issuecomment-639218007:535,configurat,configurations,535,https://hail.is,https://github.com/hail-is/hail/pull/8928#issuecomment-639218007,1,['configurat'],['configurations']
Deployability,"OK, so, this really feels like bad data. We just merged https://github.com/hail-is/hail/commit/98adcce1d07001995b0819fd6afe161bf34ba840 which fixed https://github.com/hail-is/hail/issues/13979 . Google Cloud Storage's Java library was very rarely returning just flat-out bad data. The frequency of occurrence on one particularly large pipeline appears to be 1/30000 tasks (0.003% or 3 in 100,000). The tasks were reading two files, the larger of which was 131MiB. The Java library reads in 8MiB chunks so that's at least 17 network requests per partition. That puts the frequency of this closer to 1 in 1,000,000 requests or 1 in 10TiB of data read. Before we had Zstandard, it seems that this data corruption either (a) was unnoticed (b) caused a rare decoding error or (c) caused segfaults. After we added Zstandard (0.2.119), decompression often failed due to corrupt data. It seems to me that Zstandard more aggressively verifies integrity than LZ4 does. OK, so, when was this bug introduced in Hail? As far as I can tell, this new code path was added in google-cloud-storage 2.17.0 almost one year ago: https://github.com/googleapis/java-storage/commit/94cd2887f22f6d1bb82f9929b388c27c63353d77 . We upgraded to 2.17.1 (😭 ) in Hail 0.2.109 https://github.com/hail-is/hail/commit/fec0cc2263c04c00e02cef5dda8ec46916717152 . All of the attempts above could have been plagued by this rare transient data corruption error. OK, action items:. - [ ] Ask Cal and Lindo to try their pipelines again with the next release of Hail 0.2.127.; - [x] Hail must introduce large-scale testing before releases. We, sadly, cannot assume our underlying storage libraries are reliable. https://github.com/hail-is/hail/issues/14082. Once the first action item is successfully completed, I will close this issue. For the second action item, I have created a separate ticket.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1845732114:335,pipeline,pipeline,335,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1845732114,5,"['pipeline', 'release', 'upgrade']","['pipeline', 'pipelines', 'release', 'releases', 'upgraded']"
Deployability,"OK, so. Dev deployed `site` uses the same docs and www as non-dev-deployed site. Unfortunately, this means we have a root-URL issue. I use [an nginx `sub_filter`](https://github.com/hail-is/hail/blob/master/site/site.sh) rule to fix links and anchors. I didn't add any rules for javascript modules. The right fix is to move to a system that can build for different environments and set the root URL properly. Until we switch to that hypothetical new system, I'll add a rule that properly rewrites JS module imports. We can't fix this with symlinks or any kind of redirection. The root issue is that we, the users, are outside of the system and the way we specify to whom we're talking is `internal.hail.is/NAMESPACE/SERVICE/`. Cotton wanted to do `service.namespace.internal.hail.is` but there were some TLS wildcard issues. It is perhaps worth revisiting this at some point because it is a perennial issue for us.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8923#issuecomment-639077704:12,deploy,deployed,12,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639077704,2,['deploy'],['deployed']
Deployability,"OK, so. I added test_dataproc as a separate build step and pulled that code out of the makefile into a script. I'm running a dev-deploy test of it here: ~~https://ci.hail.is/batches/32232~~. I added a commit to this branch (not present in the aforementioned dev deploy) that exits 0 if the git tag already exists. This prevents running the Dataproc tests on every master commit. I broadened the scope to include `dev`. This means that developers can run a deploy with `hailctl dev deploy hail-is/hail:master deploy`. I haven't tested the deploy step. I volunteer to do the next deploy and deal with whatever is broken. EDIT: Copying data between jobs loses the chmod settings so I had to switch away from `./`, that was why the last batch failed. https://ci.hail.is/batches/32237",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8533#issuecomment-613147506:129,deploy,deploy,129,https://hail.is,https://github.com/hail-is/hail/pull/8533#issuecomment-613147506,7,['deploy'],['deploy']
Deployability,"OK, switched to no pip installs. the hailjwt error was due to using python instead of python3. Makefile now defines PYTHON variable that sets the path correctly before invoking python3. Addressed other comments as well. ---. Don't approve yet, I discovered a race condition wrt pod creation and updates from k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312:23,install,installs,23,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312,2,"['install', 'update']","['installs', 'updates']"
Deployability,"OK, thanks, will update the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5509#issuecomment-468830620:17,update,update,17,https://hail.is,https://github.com/hail-is/hail/pull/5509#issuecomment-468830620,1,['update'],['update']
Deployability,"OK, the pip installed version uses a local copy of the annotation db JSON file. I cleaned up the lens thing a bit put it in a separate file and used ABCs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7178#issuecomment-542408914:12,install,installed,12,https://hail.is,https://github.com/hail-is/hail/pull/7178#issuecomment-542408914,1,['install'],['installed']
Deployability,"OK, there was a `justSpark` ""configuration"". I dunno what that means, it wasn't referenced by the other configurations. I initially made it `implementation` which includes it in the shadow JAR. I switched it to `shadow` b/c mllib (the only thing in the justSpark) should be provided by Spark at runtime anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1708500255:29,configurat,configuration,29,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1708500255,2,['configurat'],"['configuration', 'configurations']"
Deployability,"OK, this is now higher priority for me. The Query-on-Batch tests are absolutely hammering the database with huge spikes in deadlock errors during working hours (when deploys trigger tests).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039618265:166,deploy,deploys,166,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039618265,1,['deploy'],['deploys']
Deployability,"OK, this is passing. Sorry for another big PR, I think I'm getting close to converging and I'll start spreading things around. Summary of changes:; - break batch into two services: batch2 and batch2-driver; - batch2 handles connections to the client, but has no driver; - driver has the driver and all the control loops; - workers now connect to batch2-driver, not batch2; - batch2 hits batch2-driver after close, cancel and delete to actually handle the jobs; - also removed abort and jsonify from utils and prefer the native aiohttp interfaces. I'll change the batch2 deployment in a later PR to run in triplicate, tolerate preemptibles and have a horizontal autoscaler.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072:570,deploy,deployment,570,https://hail.is,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072,1,['deploy'],['deployment']
Deployability,"OK, this passes all the tests except for `test_vcf_parser_golden_master__gvcf_GRCh37` which inexplicably hangs. I've marked that as skip. I've attached the WIP tag because the longest tests now take 47 minutes. I'll leave this PR up as a canary for when a `main` change fails service tests. However, I won't merge it until we improve test latency. cc: @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11444#issuecomment-1069516954:238,canary,canary,238,https://hail.is,https://github.com/hail-is/hail/pull/11444#issuecomment-1069516954,1,['canary'],['canary']
Deployability,"OK, this should fix routing from internal.hail.is. The gateway routes internal.hail.is/ns/svc to router.ns with Host: svc.internal so the ns router can dispatch to the right server block off the Host. We could dispatch off the URL, but that would mean the default and private namespaces dispatch different, doubling the complexity of the router NGINX configuration. Change the host back for grafana and prometheus which generate a redirect otherwise. The monitoring and gateway changes are deployed and everything seems to be working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6928:351,configurat,configuration,351,https://hail.is,https://github.com/hail-is/hail/pull/6928,2,"['configurat', 'deploy']","['configuration', 'deployed']"
Deployability,"OK, update from Google: they suggest we check if the preemptible quota is non-zero and assume that if it is non-zero preemptible is in use and if it is zero normal quota is in use. In very rare cases, people can increase and then reduce their preemptible quota and Hail will not work properly. Google wasn't interested in providing an API to detect this case. I'll change this PR accordingly sometime next week.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10588#issuecomment-868865354:4,update,update,4,https://hail.is,https://github.com/hail-is/hail/pull/10588#issuecomment-868865354,1,['update'],['update']
Deployability,"OK, will change to exec/wait and fix the merge conflict (there was a conflict due to the upgrade to; libsimdpp-2.1, I tried to fix that but may need to do more, or there may be a new issue). My preference is to leave the build-command execution on the C++ side because it's (arguably); easier to read/understand the combined Scala + C++ functionality if the Scala NativeModule is a ; trivial wrapper and all the substance is on one side, in this case in C++. Since that's also acceptable to; you, I'll keep it that way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4211#issuecomment-416093339:89,upgrade,upgrade,89,https://hail.is,https://github.com/hail-is/hail/pull/4211#issuecomment-416093339,1,['upgrade'],['upgrade']
Deployability,"OK, yeah, I need a coherent strategy for someone deploying a fresh Hail install. If folks are not continuously deploying every commit, they might miss a tagged commit. @daniel-goldstein , thoughts on how we might support things that should be run on a tagged commit for users who are not deploying every commit?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11122#issuecomment-985723595:49,deploy,deploying,49,https://hail.is,https://github.com/hail-is/hail/pull/11122#issuecomment-985723595,5,"['continuous', 'deploy', 'install']","['continuously', 'deploying', 'install']"
Deployability,"ONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2938,Deploy,DeployConfig,2938,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,2,['Deploy'],['DeployConfig']
Deployability,"ORY.md"">requests's changelog</a>.</em></p>; <blockquote>; <h2>2.31.0 (2023-05-22)</h2>; <p><strong>Security</strong></p>; <ul>; <li>; <p>Versions of Requests between v2.3.0 and v2.30.0 are vulnerable to potential; forwarding of <code>Proxy-Authorization</code> headers to destination servers when; following HTTPS redirects.</p>; <p>When proxies are defined with user info (<a href=""https://user:pass@proxy:8080"">https://user:pass@proxy:8080</a>), Requests; will construct a <code>Proxy-Authorization</code> header that is attached to the request to; authenticate with the proxy.</p>; <p>In cases where Requests receives a redirect response, it previously reattached; the <code>Proxy-Authorization</code> header incorrectly, resulting in the value being; sent through the tunneled connection to the destination server. Users who rely on; defining their proxy credentials in the URL are <em>strongly</em> encouraged to upgrade; to Requests 2.31.0+ to prevent unintentional leakage and rotate their proxy; credentials once the change has been fully deployed.</p>; <p>Users who do not use a proxy or do not supply their proxy credentials through; the user information portion of their proxy URL are not subject to this; vulnerability.</p>; <p>Full details can be read in our <a href=""https://github.com/psf/requests/security/advisories/GHSA-j8r2-6x86-q33q"">Github Security Advisory</a>; and <a href=""https://nvd.nist.gov/vuln/detail/CVE-2023-32681"">CVE-2023-32681</a>.</p>; </li>; </ul>; <h2>2.30.0 (2023-05-03)</h2>; <p><strong>Dependencies</strong></p>; <ul>; <li>; <p>⚠️ Added support for urllib3 2.0. ⚠️</p>; <p>This may contain minor breaking changes so we advise careful testing and; reviewing <a href=""https://urllib3.readthedocs.io/en/latest/v2-migration-guide.html"">https://urllib3.readthedocs.io/en/latest/v2-migration-guide.html</a>; prior to upgrading.</p>; <p>Users who wish to stay on urllib3 1.x can pin to <code>urllib3&lt;2</code>.</p>; </li>; </ul>; <h2>2.29.0 (2023-04-26)</h2>; <p><s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13091:3817,deploy,deployed,3817,https://hail.is,https://github.com/hail-is/hail/pull/13091,6,['deploy'],['deployed']
Deployability,"Of course. Cotton also had a few comments, I will integrate too. On Tuesday, September 20, 2016, Tim Poterba notifications@github.com; wrote:. > Let's get this in. Can you rebase?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/541#issuecomment-248438901, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgSLomjBSxoQFMUqrmHl1RdXl_mD3ks5qsE7igaJpZM4Jb_3Y; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/541#issuecomment-248439671:50,integrat,integrate,50,https://hail.is,https://github.com/hail-is/hail/pull/541#issuecomment-248439671,1,['integrat'],['integrate']
Deployability,"Oh and the pipeline is a series of:; ```; next_vds = hl.read_matrix_table('{}/parts/part_{}.vds'.format(root, base)); next_vds = next_vds.select_rows(next_vds.row_id); vds = vds.union_cols(next_vds); ```; (about 30 of them and then a write)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053#issuecomment-369956421:11,pipeline,pipeline,11,https://hail.is,https://github.com/hail-is/hail/issues/3053#issuecomment-369956421,1,['pipeline'],['pipeline']
Deployability,"Oh grrr, the current build is extra borked because a minor version bump in the azure blob storage library broke. I'll back out that change. Oh I think I totally misunderstood the ""pip-installed images"". [here](https://ci.hail.is/batches/774665/jobs/63) is the job I was initially confused about. Not clear why in that batch only the pip-installed hail failed the lint and not the `check_hail` step",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11502#issuecomment-1062078939:184,install,installed,184,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1062078939,4,['install'],['installed']
Deployability,Oh nice I had seen that but didn’t know how well it worked. Would be nice for notebook (to watch fine-grained state updates),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483868483:116,update,updates,116,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483868483,1,['update'],['updates']
Deployability,"Oh right sorry that doesn't make the Linux prebuilt, which you need to build locally on Linux and check in. CI should build those before testing but doesn't yet. The workaround @catoverdrive showed me when I had to remake the prebuilt was to build it in a docker container with the CI image on my machine. Would be happy to help you get them built tomorrow and/or make the appropriate updates to CI and deploy",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-492911668:385,update,updates,385,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492911668,2,"['deploy', 'update']","['deploy', 'updates']"
Deployability,"Oh yikes, looks like something in the LLVM dependencies requires/installs python3.8. I wonder whether that is necessary",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12526#issuecomment-1352514979:65,install,installs,65,https://hail.is,https://github.com/hail-is/hail/pull/12526#issuecomment-1352514979,1,['install'],['installs']
Deployability,"Oh, and the app is meant to operate behind HTTPS; when deployed, running the web app with ; `npm run start` instead of `npm run prod-test` will enable secureOnly cookies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454272121:55,deploy,deployed,55,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454272121,1,['deploy'],['deployed']
Deployability,Oh. I remember why now. When we were using rsync I was worried someone would simultaneous try to do this:. cp gs://bucket/a/b/; cp gs://bucket/a/. Then they would clobber each other.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9523#issuecomment-701545652:122,a/b,a/b,122,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701545652,1,['a/b'],['a/b']
Deployability,Ok @tpoterba I added the environment variables where I think you originally wanted them in the pipeline task.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583557445:95,pipeline,pipeline,95,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583557445,1,['pipeline'],['pipeline']
Deployability,"Ok I just looked at the scala code, and this must have happened around the sex chromosomes when my dataset shifted to haploid (or more specifically, a mix of haploid and diploid calls for male and female). I'll write in the workaround for my own pipeline, but you might want to have a more explicit error message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465#issuecomment-385281263:246,pipeline,pipeline,246,https://hail.is,https://github.com/hail-is/hail/issues/3465#issuecomment-385281263,1,['pipeline'],['pipeline']
Deployability,Ok I will for the new release.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12844#issuecomment-1502350952:22,release,release,22,https://hail.is,https://github.com/hail-is/hail/issues/12844#issuecomment-1502350952,1,['release'],['release']
Deployability,Ok clearly I need to install g++.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5354#issuecomment-464188941:21,install,install,21,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464188941,2,['install'],['install']
Deployability,Ok thank you for the update let me try 2.2.0 . Highly appreciate a quick reply.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3001#issuecomment-368912193:21,update,update,21,https://hail.is,https://github.com/hail-is/hail/issues/3001#issuecomment-368912193,1,['update'],['update']
Deployability,"Ok, I think I sorted out the make->mill dependency propagation. Any real target which invokes mill to build it now depends on a target `FORCE` which is always out-of-date, so mill is always invoked. But mill will not change the modification time if it doesn't need to, so downstream targets aren't forced to be run. For example, we have targets; ```; FORCE:. SHADOW_JAR := out/assembly.dest/out.jar; $(SHADOW_JAR): FORCE; 	$(mill) assembly. PYTHON_JAR := python/hail/backend/hail-all-spark.jar; $(PYTHON_JAR): $(SHADOW_JAR); 	cp -f $(SHADOW_JAR) $@. .PHONY: python-jar; python-jar: $(PYTHON_JAR); ```. If I remove the python jar and invoke make, it runs mill then copies:; ```; ❯ rm python/hail/backend/hail-all-spark.jar. ❯ make python-jar; bash millw assembly; [95/110] compile; [info] compiling 10 Scala sources to /Users/pschulz/hail/mill-worktree/hail/out/compile.dest/classes ...; [info] done compiling; [110/110] assembly; cp -f out/assembly.dest/out.jar python/hail/backend/hail-all-spark.jar; ```. If run again, mill is invoked to check for changes, but as the jar doesn't change it isn't copied again:; ```; ❯ make python-jar; bash millw assembly; [105/110] memory.resources; ```. If I change some scala sources, the jar is updated and copied:; ```; ❯ make python-jar; bash millw assembly; [95/110] compile; [info] compiling 10 Scala sources to /Users/pschulz/hail/mill-worktree/hail/out/compile.dest/classes ...; [info] done compiling; [110/110] assembly; cp -f out/assembly.dest/out.jar python/hail/backend/hail-all-spark.jar; ```. If I change some scala sources in a way that doesn't actually affect the jar, such as modifying comments, mill is smart enough to not change the jar, so it won't be copied again:; ```; ❯ make python-jar; bash millw assembly; [95/110] compile; [info] compiling 1 Scala source to /Users/pschulz/hail/mill-worktree/hail/out/compile.dest/classes ...; [info] done compiling; [105/110] memory.resources; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147#issuecomment-1930325793:1234,update,updated,1234,https://hail.is,https://github.com/hail-is/hail/pull/14147#issuecomment-1930325793,1,['update'],['updated']
Deployability,"Ok, I think I'm getting a much better understanding of the situation, thank you! I think this is a lot easier for at least me to understand and if I'm not mistaken, it's mostly just moving around of the same code into `JobSpec`, right? I think `submit` is a lot easier to follow now, though I do sympathize with the pain of the three round-trips for small updates. What do you think about the following proposal?. 1. Jobs are always submitted to the server with job_ids relative to the update that is being submitted (sorry for the complete reversal, this is just an idea!). This shouldn't have any affect on the current create/create-fast since only 1 update means relative and absolute job ids are the same. This also means that the client doesn't need to know what the `start_job_id` is ahead of submitting a bunch. Submitting the update could return the `start_job_id` such that the client can rectify its local `Job` objects with absolute IDs like you do in `_get_job_specs_as_json` after the fact. The server will take care of doing `absolute_job_id = update_start_job_id + in_update_job_id`.; 2. parent_ids are represented as positive integers that are tagged with a type, either `in-update` or `absolute`. This is very similar to what you were previously doing with negative numbers, but I think baking it into the schema is going to be less foot-gunny than negative numbers, and the client doesn't have to do any special logic of counting backwards. Sorry if it's similar to what you were doing before but I think it has taken me a while to fully understand the limitations here. I also think it wouldn't take much changes to this current client implementation to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1217244185:356,update,updates,356,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1217244185,5,['update'],"['update', 'updates']"
Deployability,"Ok, I've downloaded JSON for the batch, default, and CI dashboards (I don't think any of the other ones are really used?), and recorded the configurations for the GCP and prometheus datasources, so I think I should be able to quickly reconfigure grafana if everything is lost.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10772#issuecomment-906539923:140,configurat,configurations,140,https://hail.is,https://github.com/hail-is/hail/pull/10772#issuecomment-906539923,1,['configurat'],['configurations']
Deployability,"Ok, getting back up to speed on this. There've been a number of changes on either side of this project, so going to give new timings and profiling results for the two queries. Here are mean timings for the two queries, run 5 times, and taking the mean of all but the first. It seems there's been a considerable regression since November on the second query, highlighting our need to get automated benchmark runs in per release (https://github.com/hail-is/hail/issues/14221). | query	| spark |; |-------|-------|; | 0	| 7s |; | 1	| 87s |. Attached are YourKit profiler results of the two queries. 'fast' refers to query 0 and 'slow' to the longer-running query 1.; [seqr-profile-data.zip](https://github.com/hail-is/hail/files/14103185/seqr-profile-data.zip); [seqr-logs.zip](https://github.com/hail-is/hail/files/14104795/seqr-logs.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1917732829:419,release,release,419,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1917732829,1,['release'],['release']
Deployability,"Ok, just used this script to release 0.2.28 without a problem so I think we should merge this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7536#issuecomment-557703477:29,release,release,29,https://hail.is,https://github.com/hail-is/hail/pull/7536#issuecomment-557703477,1,['release'],['release']
Deployability,"Ok, so I thought I left a comment on here but I guess I didn't: when I tested this with dev deploy, I didn't see any plots show up, got JS console errors. So I'm not sure this was quite ready to be merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11712#issuecomment-1097208394:92,deploy,deploy,92,https://hail.is,https://github.com/hail-is/hail/pull/11712#issuecomment-1097208394,1,['deploy'],['deploy']
Deployability,"Ok, so, llvm-dev requires python3-yaml. Attempting to pip install version 6 (our dep) onto the system fails. So, that's fun.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12410#issuecomment-1299393355:58,install,install,58,https://hail.is,https://github.com/hail-is/hail/pull/12410#issuecomment-1299393355,1,['install'],['install']
Deployability,"Ok, this is seeming not related to this closed github issue. Can you make a post on our support forum: https://discuss.hail.is/ explaining what step went wrong (I'm not sure where you're trying to deploy to AWS or what your installation procedure is here)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3001#issuecomment-908575361:197,deploy,deploy,197,https://hail.is,https://github.com/hail-is/hail/issues/3001#issuecomment-908575361,2,"['deploy', 'install']","['deploy', 'installation']"
Deployability,"Ok, this should pass. I'll deploy 0.2.1 after master is merged. I'll get self-deployment working soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4498#issuecomment-430451063:27,deploy,deploy,27,https://hail.is,https://github.com/hail-is/hail/pull/4498#issuecomment-430451063,2,['deploy'],"['deploy', 'deployment']"
Deployability,"Ok. I thought about this some more. What you've implemented is essentially a ""taint"" in Kubernetes. I ultimately want both taints and something more complicated that will have to be integrated into the scheduler. I think for now your change is self contained and it will be easy to transform later on without too much complexity or breaking changes for users. I think if you want to rename ""label"" to ""taint"" then that might make it clearer what's going on. cc: @daniel-goldstein",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11879#issuecomment-1145343746:182,integrat,integrated,182,https://hail.is,https://github.com/hail-is/hail/pull/11879#issuecomment-1145343746,2,['integrat'],['integrated']
Deployability,"Ok. Still working on getting the tests to pass and cleaning things up. However, I ran into a small snag. The code below needs to be ironed out. Should the number of jobs and state of the job group be recursive or specific to that job group? It's a bit weird for the billing and cancellation to be nested, but the number of jobs etc. are not. More concretely, if a child batch is running, should the parent also be running even if it has no direct child jobs that are running? Thoughts?. cc: @daniel-goldstein . ```mysql; UPDATE batches SET; `state` = 'running',; time_completed = NULL,; n_jobs = n_jobs + expected_n_jobs; WHERE id = in_batch_id;. ### FIXME FIXME what should the state be of nested job groups?; UPDATE job_groups; INNER JOIN (; SELECT batch_id, job_group_id, CAST(COALESCE(SUM(n_jobs), 0) AS SIGNED) AS staged_n_jobs; FROM job_groups_inst_coll_staging; WHERE batch_id = in_batch_id AND update_id = in_update_id; GROUP BY batch_id, job_group_id; ) AS t ON job_groups.batch_id = t.batch_id AND job_groups.job_group_id = t.job_group_id; SET `state` = 'running', time_completed = NULL, n_jobs = n_jobs + t.staged_n_jobs;; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14170#issuecomment-1932547516:521,UPDATE,UPDATE,521,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1932547516,2,['UPDATE'],['UPDATE']
Deployability,"On April 2016, this exact issue was reported: https://issues.jenkins-ci.org/browse/JENKINS-34177. This issue results in Jenkins using arbitrary amounts of disk space (until builds fail due to a full disk). JENKINS-34177 was closed as a duplicate of: https://issues.jenkins-ci.org/browse/JENKINS-2111 which has been open since 2008. The thread of conversation seems to be about how difficult it would be to ensure that all files are deleted across all slaves. There have been no updates to that thread since October of 2015.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/655#issuecomment-241418469:478,update,updates,478,https://hail.is,https://github.com/hail-is/hail/issues/655#issuecomment-241418469,1,['update'],['updates']
Deployability,"On Oct 21, 2017, at 1:27 PM, Sun-shan <notifications@github.com> wrote:. > hail: info: SparkUI: http://10.131.101.159:4040; > Welcome to; > __ __ <>__; > / // /__ __/ /; > / __ / _ `/ / /; > // //_,/// version 0.1-f69b497; > ; > print sc; > ; > >>> print hc >>> hc.import_vcf() Traceback (most recent call last): File """", line 1, in TypeError: import_vcf() takes at least 2 arguments (1 given) >>> hc.import_vcf('/hail/sample.vcf') [Stage 0:> (0 + 1) / 2]Traceback (most recent call last): File """", line 1, in File """", line 2, in import_vcf File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)) hail.java.FatalError: SparkException: Failed to get broadcast_4_piece0 of broadcast_4. This type of Spark exception seems to be related to the configuration option spark.cleaner.ttl. Have you set that to a value other than the default?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338442661:822,configurat,configuration,822,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338442661,1,['configurat'],['configuration']
Deployability,"On Ubuntu 20.10, with Python 3.8.6 and hail 0.2.64 installed from pip, I get: ​I get `TypeError: an integer is required (got type bytes)` immediately upon importing hail. A full transcript is below. (pyve is an alias to create a python virtual env and activate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:51,install,installed,51,https://hail.is,https://github.com/hail-is/hail/issues/10197,7,"['Install', 'install']","['Installing', 'install', 'installation', 'installed']"
Deployability,"On the current release (0.2.119), `is.hail.rvd.RVD.localSort` attempts to serialize an entire `is.hail.rvd.RVD` object when called, which causes some operations, like reading a set of VCF files, to fail. That particular operation no longer fails due to some combination of the changes made to reading VCF files in https://github.com/hail-is/hail/pull/13206 and https://github.com/hail-is/hail/pull/13229, but the serialization could still cause issues if `is.hail.rvd.RVD.localSort` is called in another context. . See https://hail.zulipchat.com/#narrow/stream/123011-Hail-Query-Dev/topic/Possible.20bug.20in.20Hail.200.2E2.2E119/near/378610151 for more information.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13313:15,release,release,15,https://hail.is,https://github.com/hail-is/hail/pull/13313,1,['release'],['release']
Deployability,"On the docs for 'Other Spark Clusters', it states that Hail is built on Spark 2.4; `Hail should work with any Spark 2.4.x cluster built with Scala 2.11.`; https://hail.is/docs/0.2/install/other-cluster.html. My understanding is that this is now Spark 3.1? Just want to check this. Cheers!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10351:180,install,install,180,https://hail.is,https://github.com/hail-is/hail/issues/10351,1,['install'],['install']
Deployability,"On the issue of how to give people a standardized environment with Hail +; compiler + everything else, one; approach is to build a Bitnami package, which then installs itself into a; directory tree with zero/minimal; dependencies or interactions with anything outside its tree. I've used; that for web services like Jenkins.; It's just one step short of using containers - but since it doesn't require; a containerized OS, I think it works; for laptops etc. I believe the package could have all the stuff we currently manage my; manual install, viz JDK, Spark, Python-3.6,; R, R packages, as well as Hail and a friendly-C++17-capable compiler. All; without perturbing anything else; on the system. See https://bitnami.com. I took a similar approach at PhysicsSpeed, though without using any bitnami; tools because we had less than; zero dollars :-(. I don't know if this adds any value in the containerized/cloud environment,; where custom machine images; are presumably the way to go. But it makes setup easy for standalone use. Regards; Richard. On Thu, Aug 2, 2018 at 10:44 PM Richard Cownie <rcownie@broadinstitute.org>; wrote:. > We have a difference of opinion about the risks involved in using whatever; > compiler happens to show up as $(CXX); > to try to compile arbitrarily large auto-generated C++ files, and maybe; > about what happens when that fails; > and gives an error message about something in the middle of 12000 lines of; > code that bears no obvious relationship; > to what the user is doing. Or when that compiler takes 15 minutes to; > compile it. It's the C++ equivalent of; > the JVM ""no, that's just too much bytecode"". Or worst of all, it compiles; > it but the code gives the wrong answers; > because that particular compiler has a bug, and we never tested the; > combination of our codegen with *that*; > compiler/version.; >; > A couple of years ago I was seeing g++ take 40-60 seconds to compile; > something that clang did in 2 seconds; > (fairly heavily templated cod",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287:159,install,installs,159,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287,2,['install'],"['install', 'installs']"
Deployability,"On views narrower than the height of the graph image + 100px, the slide box will overflow the adjacent install section. Fixed using inline style to avoid css caching issues until the build process is fixed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8992:103,install,install,103,https://hail.is,https://github.com/hail-is/hail/pull/8992,1,['install'],['install']
Deployability,"Once the current auth overhaul is through and we go ""keyless"", the only secret left that we mount into user jobs is the deploy config, which at that point feels kind of silly. It's also nearly always the same deploy config value that we serialize and write to a file for every job. It seems cleaner and simpler to me that we create one deploy config for the worker, and the worker readonly mounts that config into every job. Note that this is overridable so that any pre-existing jobs that specify a deploy-config secret and all the special deploy-config stuff that we do in build.yaml should not be affected",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13203:120,deploy,deploy,120,https://hail.is,https://github.com/hail-is/hail/pull/13203,5,['deploy'],"['deploy', 'deploy-config']"
Deployability,"Once this goes in I will make a second PR (to breaking) that that removes api1, integrates api2, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2824:80,integrat,integrates,80,https://hail.is,https://github.com/hail-is/hail/pull/2824,1,['integrat'],['integrates']
Deployability,"Once upon a time, we did option 1. We took that out because there was no way to know in general that the within-key sort would be small enough to do locally. In fact, `RVD.makeCoercer` still supports this. It takes a `partitionKey` argument, and if the first scan over the data determines that it's already sorted by the partitionKey, but not the full key, it does a local sort. But it looks like all calls will have set `partitionKey` to be the full key. I like option 2. `key_by` could take an optional `local_sort_by` parameter, which is a prefix of the key, and set that as the partition key. We'd need to update the TableKeyBy lowering to support this, but that shouldn't be hard.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13455#issuecomment-1683930595:610,update,update,610,https://hail.is,https://github.com/hail-is/hail/issues/13455#issuecomment-1683930595,1,['update'],['update']
Deployability,"Once we can draw from Cassandra (with the likes of `annotatevariants cass`, we can write/read/compare in the usual way. We need a Cassandra cluster for testing. There are three options: an embedded server as part of Hail, assume the user has installed Cassandra locally, or run against a fixed server. I set up a single-node Cassandra install on hail-ci. In the spirit of small commits, I want to be able to test in experimental/untested/in progress work so we don't get so much divergence. We need a way to mark it. I will investigate the options.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/282#issuecomment-208368921:242,install,installed,242,https://hail.is,https://github.com/hail-is/hail/pull/282#issuecomment-208368921,2,['install'],"['install', 'installed']"
Deployability,"One last thing, we now have a file called `CanLowerEfficiently.scala` (rebase if you don't have it). In that file, you have to update the `TableTail` case to match `TableHead`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10798#issuecomment-903821822:127,update,update,127,https://hail.is,https://github.com/hail-is/hail/pull/10798#issuecomment-903821822,1,['update'],['update']
Deployability,"One of the problems with the new model is that tests on the deployed version are going to leave billing project poop everywhere unless we actually go in and delete them, hmmm....",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9553#issuecomment-705227991:60,deploy,deployed,60,https://hail.is,https://github.com/hail-is/hail/pull/9553#issuecomment-705227991,1,['deploy'],['deployed']
Deployability,"Only changes are as we discussed: rename/add notebook2 labels where appropriate. Tested in cluster, appears to work although the deployment is stuck in Desired == 1, so I may have missed one of the notebook labels, or maybe `make deploy` not enough (I find the makefile a bit confusing still). Describe shows `ReplicaFailure True FailedCreate`, will figure out tomorrow. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5412:129,deploy,deployment,129,https://hail.is,https://github.com/hail-is/hail/pull/5412,2,['deploy'],"['deploy', 'deployment']"
Deployability,Only create serializable and broadcasted HadoopConf once in HailContext and use everywhere else. I was seeing pipelines with MANY duplicate broadcasts the Hadoop configuration.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5422:110,pipeline,pipelines,110,https://hail.is,https://github.com/hail-is/hail/pull/5422,2,"['configurat', 'pipeline']","['configuration', 'pipelines']"
Deployability,"Oops, I had a bug where the readiness check was hitting the notebook service, not the actual notebook. Here is an updated scale test:. ```; successes: 10 / 10 = 1.0; mean time: 6.920137214660644; max time: 14.664504528045654; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112#issuecomment-534707944:114,update,updated,114,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534707944,1,['update'],['updated']
Deployability,"Oops, I messed up the last PR, I forget the ""3"" in ""miniconda3"" in the build image Docker. I presume deploys are broken now, but I haven't checked. Rebuilding a new docker image now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4404#issuecomment-423779185:101,deploy,deploys,101,https://hail.is,https://github.com/hail-is/hail/pull/4404#issuecomment-423779185,1,['deploy'],['deploys']
Deployability,"Oops, I still have to update the `vep` docs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3872#issuecomment-401411707:22,update,update,22,https://hail.is,https://github.com/hail-is/hail/pull/3872#issuecomment-401411707,1,['update'],['update']
Deployability,"Oops, quite right. I pulled this diff from the other install, I just fixed the pieces I deployed there. I didn't intend this to be a common include for all Makefiles (should it be?), but just pull together the relevant defines used in services Makefiles. I grep'ed to make sure the variables defined config.mk are now only defined there.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9339#issuecomment-679385861:53,install,install,53,https://hail.is,https://github.com/hail-is/hail/pull/9339#issuecomment-679385861,2,"['deploy', 'install']","['deployed', 'install']"
Deployability,Other changes:. - stop supporting python 2; - remove support for 0.1; - formatting. Coming soon to a PR near you:. - use google client library instead of `gsutil cat` (huge speedup); - upload configuration in deploy; include paths in hailctl package,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6136:192,configurat,configuration,192,https://hail.is,https://github.com/hail-is/hail/pull/6136,2,"['configurat', 'deploy']","['configuration', 'deploy']"
Deployability,"Our cluster is running Centos 6 which uses GLIBC 2.12 while the most recent version of Hail 0.2 requires GLIBC 2.14. It look like Broad must have moved to Centos 7 recently and new releases of their software now require GLIBC 2.14. . The following allowed us to run Hail 0.2 locally but not on the Hadoop Cluster. At least, we did not get an error and the tutorials would run. . LD_PRELOAD=/share/pkg/glibc/2.14/install/lib/libc.so.6 ipython . So I am also interested in a workaround for this that would allow Hail to run on a Centos 6 hadoop cluster. ; Is there a way to compile Hail with GLIBC 2.12? Or set a spark setting to use LD_PRELOAD for the hail jobs that run on the yarn master. Otherwise it looks like our IT staff will need to upgrade our 20 Hadoop nodes to Centos 7 which will require some planning and cluster downtime.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-439969754:181,release,releases,181,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-439969754,3,"['install', 'release', 'upgrade']","['install', 'releases', 'upgrade']"
Deployability,Our production images are mostly tagged with a `deploy-` prefix but there are the `third-party/images.txt` which we need to handle differently. ```; (base) dking@wm28c-761 gar-cleaner % k get pods -o json | jq -r '.items[].spec.containers[].image' | sort -u; ghost:3.0-alpine; prom/prometheus:v2.34.0; us-docker.pkg.dev/hail-vdc/hail/admin-pod:deploy-qd833uw7kcyn; us-docker.pkg.dev/hail-vdc/hail/auth:deploy-crsithjyoxfg; us-docker.pkg.dev/hail-vdc/hail/batch:deploy-kpd6nqk4t25o; us-docker.pkg.dev/hail-vdc/hail/batch:deploy-v1yv8cgd1003; us-docker.pkg.dev/hail-vdc/hail/blog_nginx:deploy-wnrqjf4h6qto; us-docker.pkg.dev/hail-vdc/hail/ci:deploy-du68h4bouvp9; us-docker.pkg.dev/hail-vdc/hail/envoyproxy/envoy:v1.22.3; us-docker.pkg.dev/hail-vdc/hail/grafana/grafana:9.1.4; us-docker.pkg.dev/hail-vdc/hail/monitoring:deploy-ljz4mgjf132m; us-docker.pkg.dev/hail-vdc/hail/notebook:deploy-gmftvyf0op87; us-docker.pkg.dev/hail-vdc/hail/notebook_nginx:deploy-n9uipfhjn3jg; us-docker.pkg.dev/hail-vdc/hail/website:deploy-gb1372nuge4g; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13441#issuecomment-1679603478:48,deploy,deploy,48,https://hail.is,https://github.com/hail-is/hail/issues/13441#issuecomment-1679603478,11,['deploy'],"['deploy', 'deploy-', 'deploy-crsithjyoxfg']"
Deployability,Our releaseJar example should not use Spark version 2.3.0 when we don't actually want users to use that version,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6920:4,release,releaseJar,4,https://hail.is,https://github.com/hail-is/hail/pull/6920,1,['release'],['releaseJar']
Deployability,"Our secret cache fails on ~1 in 10000 jobs. I observed this while running some; large scale tests which will soon become standard PR tests. In anticipation of this,; I fixed the k8s_cache. In particular, note how *everyone* who wins the lock tries to; remove it from the dictionary; however, only *one* task can do that successfully. The new code avoids locks entirely. It is a bit longer because I eagerly remove; out of date keys when I see them and use a future to notify all waiter simultaneouly. I also updated memory to use this cache for user credentials.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11040:508,update,updated,508,https://hail.is,https://github.com/hail-is/hail/pull/11040,1,['update'],['updated']
Deployability,"Our team is currently trying to run kinship analysis with [king()](https://hail.is/docs/0.2/methods/relatedness.html#hail.methods.king) on just under 110k samples. We have run this successfully in the past on 10k samples using a google cloud cluster with the following configuration. ```; hailctl dataproc start cluster --vep GRCh38 \; 	--requester-pays-allow-annotation-db \; 	--packages gnomad --requester-pays-allow-buckets gnomad-public-requester-pays \; 	--master-machine-type=n1-highmem-8 --worker-machine-type=n1-highmem-8 \; 	--num-workers=300	--num-secondary-workers=0 \; 	--worker-boot-disk-size=1000 \; 	--properties=dataproc:dataproc.logging.stackdriver.enable=true,dataproc:dataproc.monitoring.stackdriver.enable=true; ```; We are currently receiving a spark error when using this cluster for our larger dataset. ```; [Stage 10:=====> (69 + 656) / 729]; raise err; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 98, in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); File ""/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1304, in __call__; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 31, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.project-.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.gbsc-project.internal executor 3568): ExecutorLostFailure (executor 3568 e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:269,configurat,configuration,269,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['configurat'],['configuration']
Deployability,Override merging this because it only touches deploy things (which aren't tested) and it was waiting on other approved PRs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4616#issuecomment-432424552:46,deploy,deploy,46,https://hail.is,https://github.com/hail-is/hail/pull/4616#issuecomment-432424552,1,['deploy'],['deploy']
Deployability,Overriding and merging directly so it definitely gets included in next deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4583#issuecomment-431457123:71,deploy,deploy,71,https://hail.is,https://github.com/hail-is/hail/pull/4583#issuecomment-431457123,1,['deploy'],['deploy']
Deployability,Overriding review for hotfix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9406#issuecomment-686086418:22,hotfix,hotfix,22,https://hail.is,https://github.com/hail-is/hail/pull/9406#issuecomment-686086418,1,['hotfix'],['hotfix']
Deployability,"PR <a href=""https://redirect.github.com/jupyterlab/jupyterlab/issues/15524"">#15524</a>: Fix visual tests <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15578"">#15578</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Documentation improvements</h3>; <ul>; <li>Remove Python 3.0, Notebook 5 mentions from contributor docs <a href=""https://redirect.github.com/jupyterlab/jupyterlab/pull/15710"">#15710</a> (<a href=""https://github.com/JasonWeill""><code>@​JasonWeill</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyterlab/jupyterlab/graphs/contributors?from=2024-01-19&amp;to=2024-01-30&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3AFoSuCloud+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​FoSuCloud</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Agithub-actions+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Aj264415+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​j264415</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3AJasonWeill+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​JasonWeill</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajupyterlab-bot+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​jupyterlab-bot</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Ajupyterlab-probot+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​jupyterlab-probot</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyterlab%2Fjupyterlab+involves%3Akrassowski+updated%3A2024-01-19..2024-01-30&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14218:2877,update,updated,2877,https://hail.is,https://github.com/hail-is/hail/pull/14218,2,['update'],['updated']
Deployability,"PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI2NDVjMDg3ZS00NzIwLTRkZTgtYmI0NC00MWNkOTY0NTBmZjUiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjY0NWMwODdlLTQ3MjAtNGRlOC1iYjQ0LTQxY2Q5NjQ1MGZmNSJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/0ba777e1-bc27-41cc-aefa-0ed1a253829e?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/0ba777e1-bc27-41cc-aefa-0ed1a253829e?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""645c087e-4720-4de8-bb44-41cd96450ff5"",""prPublicId"":""645c087e-4720-4de8-bb44-41cd96450ff5"",""dependencies"":[{""name"":""aiohttp"",""from"":""3.8.6"",""to"":""3.9.2""}],""packageManager"":""pip"",""projectPublicId"":""0ba777e1-bc27-41cc-aefa-0ed1a253829e"",""projectUrl"":""https://app.snyk.io/org/danking/project/0ba777e1-bc27-41cc-aefa-0ed1a253829e?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-AIOHTTP-6209406"",""SNYK-PYTHON-AIOHTTP-6209407""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[581,718],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Improper Limitation of a Pathname to a Restricted Directory (&#x27;Path Traversal&#x27;)](https://learn.snyk.io/lesson/directory-traversal/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14220:3094,patch,patches-to-fix-vulnerabilities,3094,https://hail.is,https://github.com/hail-is/hail/pull/14220,4,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability,"PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI3ZGRlNzcwZi0yMzMyLTQ5ZjktOWI1My05ZDY1OGJlOTVjMmQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjdkZGU3NzBmLTIzMzItNDlmOS05YjUzLTlkNjU4YmU5NWMyZCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""7dde770f-2332-49f9-9b53-9d658be95c2d"",""prPublicId"":""7dde770f-2332-49f9-9b53-9d658be95c2d"",""dependencies"":[{""name"":""aiohttp"",""from"":""3.8.6"",""to"":""3.9.2""}],""packageManager"":""pip"",""projectPublicId"":""b72ce54d-5de3-48e5-a1d4-6f8967681a12"",""projectUrl"":""https://app.snyk.io/org/danking/project/b72ce54d-5de3-48e5-a1d4-6f8967681a12?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-AIOHTTP-6209406"",""SNYK-PYTHON-AIOHTTP-6209407""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[581,718],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Improper Limitation of a Pathname to a Restricted Directory (&#x27;Path Traversal&#x27;)](https://learn.snyk.io/lesson/directory-traversal/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14228:3042,patch,patches-to-fix-vulnerabilities,3042,https://hail.is,https://github.com/hail-is/hail/pull/14228,4,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability,"PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI5ZWNjYjQ0YS1jYWZiLTQ0OTgtYjU1NS02NDdmZjUwY2ExOTQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjllY2NiNDRhLWNhZmItNDQ5OC1iNTU1LTY0N2ZmNTBjYTE5NCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fdd23464-9a67-49b8-8d9c-08502282c5fb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fdd23464-9a67-49b8-8d9c-08502282c5fb?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""9eccb44a-cafb-4498-b555-647ff50ca194"",""prPublicId"":""9eccb44a-cafb-4498-b555-647ff50ca194"",""dependencies"":[{""name"":""aiohttp"",""from"":""3.8.6"",""to"":""3.9.2""}],""packageManager"":""pip"",""projectPublicId"":""fdd23464-9a67-49b8-8d9c-08502282c5fb"",""projectUrl"":""https://app.snyk.io/org/danking/project/fdd23464-9a67-49b8-8d9c-08502282c5fb?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-AIOHTTP-6209406"",""SNYK-PYTHON-AIOHTTP-6209407""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[581,718],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Improper Limitation of a Pathname to a Restricted Directory (&#x27;Path Traversal&#x27;)](https://learn.snyk.io/lesson/directory-traversal/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14225:3028,patch,patches-to-fix-vulnerabilities,3028,https://hail.is,https://github.com/hail-is/hail/pull/14225,4,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability,"PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJhZDFmMzFlYi1hYTcyLTQyMTYtOTgzNC01MDljMDdhOWFmNTMiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImFkMWYzMWViLWFhNzItNDIxNi05ODM0LTUwOWMwN2E5YWY1MyJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""ad1f31eb-aa72-4216-9834-509c07a9af53"",""prPublicId"":""ad1f31eb-aa72-4216-9834-509c07a9af53"",""dependencies"":[{""name"":""aiohttp"",""from"":""3.8.6"",""to"":""3.9.2""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-AIOHTTP-6209406"",""SNYK-PYTHON-AIOHTTP-6209407""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[581,718],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Improper Limitation of a Pathname to a Restricted Directory (&#x27;Path Traversal&#x27;)](https://learn.snyk.io/lesson/directory-traversal/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14227:3274,patch,patches-to-fix-vulnerabilities,3274,https://hail.is,https://github.com/hail-is/hail/pull/14227,4,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability,"PR to ensure they won't cause issues with your project. ------------. **Note:** *You are seeing this because you or someone else with access to this repository has authorized Snyk to open fix PRs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJlMzQ0ZjYzNy00MjQwLTQxNmEtYjE2Yi1kODhmYjc2YTUwZmYiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImUzNDRmNjM3LTQyNDAtNDE2YS1iMTZiLWQ4OGZiNzZhNTBmZiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/92d13c88-936f-40d3-b692-29e637c1a00c?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/92d13c88-936f-40d3-b692-29e637c1a00c?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""e344f637-4240-416a-b16b-d88fb76a50ff"",""prPublicId"":""e344f637-4240-416a-b16b-d88fb76a50ff"",""dependencies"":[{""name"":""aiohttp"",""from"":""3.8.6"",""to"":""3.9.2""}],""packageManager"":""pip"",""projectPublicId"":""92d13c88-936f-40d3-b692-29e637c1a00c"",""projectUrl"":""https://app.snyk.io/org/danking/project/92d13c88-936f-40d3-b692-29e637c1a00c?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-AIOHTTP-6209406"",""SNYK-PYTHON-AIOHTTP-6209407""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""updated-fix-title"",""pr-warning-shown"",""priorityScore""],""priorityScoreList"":[581,718],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Improper Limitation of a Pathname to a Restricted Directory (&#x27;Path Traversal&#x27;)](https://learn.snyk.io/lesson/directory-traversal/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14226:3019,patch,patches-to-fix-vulnerabilities,3019,https://hail.is,https://github.com/hail-is/hail/pull/14226,4,"['patch', 'update', 'upgrade']","['patch', 'patches-to-fix-vulnerabilities', 'updated-fix-title', 'upgrade']"
Deployability,PTypes 47: Update TableExplode ptype,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4834:11,Update,Update,11,https://hail.is,https://github.com/hail-is/hail/pull/4834,1,['Update'],['Update']
Deployability,P_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GE,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:10438,release,release,10438,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,P_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:dep,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:2639,deploy,deploy-,2639,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['deploy'],['deploy-']
Deployability,Pandas only releases [breaking changes in major versions](https://pandas.pydata.org/docs/development/policies.html). It seems safe; to be flexible on patch version. Just today I had an issue where I ran `pip install pandas` to; upgrade from an old pandas version and I landed on 1.1.5.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9819:12,release,releases,12,https://hail.is,https://github.com/hail-is/hail/pull/9819,4,"['install', 'patch', 'release', 'upgrade']","['install', 'patch', 'releases', 'upgrade']"
Deployability,Parameterize spark version in gradle install commands,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/399:37,install,install,37,https://hail.is,https://github.com/hail-is/hail/issues/399,1,['install'],['install']
Deployability,Parsy is transitive from curlylint. We can't update until they do.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12934#issuecomment-1708534298:45,update,update,45,https://hail.is,https://github.com/hail-is/hail/pull/12934#issuecomment-1708534298,1,['update'],['update']
Deployability,Part 1 of improving dev deploy usability. This PR:. - Moves definition of profiles out of CI so that new build steps can be added as part of development.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6660:24,deploy,deploy,24,https://hail.is,https://github.com/hail-is/hail/pull/6660,1,['deploy'],['deploy']
Deployability,Patch was merged. Closing issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9833#issuecomment-747571669:0,Patch,Patch,0,https://hail.is,https://github.com/hail-is/hail/issues/9833#issuecomment-747571669,1,['Patch'],['Patch']
Deployability,"Performance note:; to do an aggregation - export sites pipeline, master took 7m, this branch took 14s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/954#issuecomment-253405048:55,pipeline,pipeline,55,https://hail.is,https://github.com/hail-is/hail/pull/954#issuecomment-253405048,1,['pipeline'],['pipeline']
Deployability,Permit the Dockerfile to be specified inline:; ```yaml; - kind: buildImage; name: inline_image; dockerFile:; inline: |; FROM ubuntu:18.04; RUN apt-get update && apt-get install git; contextPath: .; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7592:151,update,update,151,https://hail.is,https://github.com/hail-is/hail/pull/7592,2,"['install', 'update']","['install', 'update']"
Deployability,"Pillow/commit/5beb0b66648db8b542bb5260eed79b25e33d643b""><code>5beb0b6</code></a> Update CHANGES.rst [ci skip]</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/cac6ffa7b399ea79b6239984d1307056a0b19af2""><code>cac6ffa</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7927"">#7927</a> from python-pillow/imagemath</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/f5eeeacf7539eaa0d93a677d7666bc7c142c8d1c""><code>f5eeeac</code></a> Name as 'options' in lambda_eval and unsafe_eval, but '_dict' in deprecated eval</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/facf3af93dabcbdd8cdbda8c3b50eefafa3bb04c""><code>facf3af</code></a> Added release notes</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/2a93aba5cfcf6e241ab4f9392c13e3b74032c061""><code>2a93aba</code></a> Use strncpy to avoid buffer overflow</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/a670597bc30e9d489656fc9d807170b8f3d7ca57""><code>a670597</code></a> Update CHANGES.rst [ci skip]</li>; <li>Additional commits viewable in <a href=""https://github.com/python-pillow/Pillow/compare/10.2.0...10.3.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=10.2.0&new-version=10.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recrea",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:15039,Update,Update,15039,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['Update'],['Update']
Deployability,"Pipeline failure, needs a bump :-/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6237#issuecomment-498882414:0,Pipeline,Pipeline,0,https://hail.is,https://github.com/hail-is/hail/pull/6237#issuecomment-498882414,1,['Pipeline'],['Pipeline']
Deployability,Pipeline fix verbose,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6332:0,Pipeline,Pipeline,0,https://hail.is,https://github.com/hail-is/hail/pull/6332,1,['Pipeline'],['Pipeline']
Deployability,Pipeline google backend,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6772:0,Pipeline,Pipeline,0,https://hail.is,https://github.com/hail-is/hail/pull/6772,1,['Pipeline'],['Pipeline']
Deployability,"Pipeline is a little involved but mostly annotate_rows(some_aggregators), followed by a group_cols_by().aggregate() -> annotate_rows(take())",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4263#issuecomment-418797402:0,Pipeline,Pipeline,0,https://hail.is,https://github.com/hail-is/hail/issues/4263#issuecomment-418797402,1,['Pipeline'],['Pipeline']
Deployability,"Please be picky! You can see what the new UI looks like by checking out this branch and running `make devserver SERVICE=batch`. If you can, I'd also appreciate a sanity check dev deploy to make sure the links to other apps work. I dev deployed it myself but it's hard to make sure I covered all the bases. I struggled a bit with making it mobile friendly but I hope this general approach is a good enough improvement over the current markup. I also don't have much of an opinion on colors if you have thoughts there. I was trying to go for cool and neutral and might have accidentally ended up with ""dentist office""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14562#issuecomment-2127828741:179,deploy,deploy,179,https://hail.is,https://github.com/hail-is/hail/pull/14562#issuecomment-2127828741,2,['deploy'],"['deploy', 'deployed']"
Deployability,Please update to be consistent with python/pyhail/docs/style-guide.txt and resubmit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1114#issuecomment-266934270:7,update,update,7,https://hail.is,https://github.com/hail-is/hail/pull/1114#issuecomment-266934270,6,['update'],['update']
Deployability,Plotly relies on behavior that Pandas is deprecated. See https://github.com/plotly/plotly.py/issues/4363 and https://github.com/plotly/plotly.py/pull/4379. The fix (https://github.com/plotly/plotly.py/pull/4379) merged into [main and was released in 5.18.0](https://github.com/plotly/plotly.py/commit/57e4d1d33c67c5cc715bec5c3c240dd6f4c3b10d).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13974:238,release,released,238,https://hail.is,https://github.com/hail-is/hail/pull/13974,1,['release'],['released']
Deployability,"Plus better error checking!. - Some bioinformatic tools expect a secondary implied file to be present. For example, sample.vcf and it's index file sample.vcf.tbi. This PR adds file localization such that if any file in a resource group is used, the entire resource group will be copied and not just the mentioned file. - Added a mentioned set that tracks whether a resource was defined in the command or declare resource group functions. Otherwise, you could do something like this which would throw an error upon execution:. ```python; p = Pipeline(); t = p.new_task(); p.write_output(t.undefined_variable, 'gs://foo/foo'); p.run(); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5455:541,Pipeline,Pipeline,541,https://hail.is,https://github.com/hail-is/hail/pull/5455,1,['Pipeline'],['Pipeline']
Deployability,Possible to install with glibc 2.12?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733:12,install,install,12,https://hail.is,https://github.com/hail-is/hail/issues/4733,1,['install'],['install']
Deployability,"Potential footgun but specifically limited to dev deploys. This allows you to explicitly exclude build steps when dev deploying. For example, `hailctl dev deploy -b … -s deploy_query --excluded_steps deploy_batch` would make sure that `deploy_batch` and any jobs only in the `deploy_batch` subtree are not run. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10276:50,deploy,deploys,50,https://hail.is,https://github.com/hail-is/hail/pull/10276,3,['deploy'],"['deploy', 'deploying', 'deploys']"
Deployability,Prep deploy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6174:5,deploy,deploy,5,https://hail.is,https://github.com/hail-is/hail/pull/6174,1,['deploy'],['deploy']
Deployability,"Pretty light on user facing updates this time, but we want to release to redeploy the docs to fix search.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9630:28,update,updates,28,https://hail.is,https://github.com/hail-is/hail/pull/9630,2,"['release', 'update']","['release', 'updates']"
Deployability,"Pretty simple pipeline, only failed about ~3/4 through the final (write) stage...; ```; mt = hl.read_matrix_table(); sample_group_filters = {; ""qc_samples_raw"": mt.meta.high_quality,; ""release_samples_raw"": mt.meta.release,; ""all_samples_raw"": True; }; mt = mt.select_cols(**sample_group_filters); mt = unphase_mt(mt.select_rows(*mt.row_key)); call_stats_expression = []; for group in sample_group_filters.keys():; call_stats_expression.append(; hl.struct(call_stats=hl.agg.call_stats(hl.agg.filter(mt[group], mt.GT), mt.alleles),; meta={'group': group}); ); mt.annotate_rows(qc_callstats=call_stats_expression).drop_cols().write(); ```. ```; Traceback (most recent call last):; File ""/tmp/a913d6ce5b814a63ad7af31060416237/pyscripts_Xr0D99.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/a913d6ce5b814a63ad7af31060416237/generate_qc_annotations.py"", line 247, in main; generate_call_stats(mt).write(annotations_mt_path(data_type, 'call_stats'), args.overwrite); File ""<decorator-gen-556>"", line 2, in write; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/matrixtable.py"", line 2027, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: IllegalArgumentException: requirement failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 1.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 1.0 (TID 10060, exomes3-sw-dfpw.c.broad-mpg-gnomad.internal, executor 134): java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:212); 	at is.hail.variant.Call$.alleleByIndex(Call.scala:128); 	at is.hail.expr.FunctionRegistry$$anonfun$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465:14,pipeline,pipeline,14,https://hail.is,https://github.com/hail-is/hail/issues/3465,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,Previous dev deploy passed. Rebased on main and submitted a new dev deploy: https://ci.hail.is/batches/8120683,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1927618210:13,deploy,deploy,13,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1927618210,2,['deploy'],['deploy']
Deployability,"Previous patches made the interval _type_ generic (e.g. Interval[T] instead of just Interval which implied interval of locus). This patch makes the intervals themselves generic. The interval doesn't carry the type (it's just a container for the start, end) so various interval operations need the ordering on the type that the interval is over to be passed in. Then everything else is follows from this. The next PR makes the intervals generic on the Python side. Along the way, I allows intervals to be improper, e.g., [7, 5), which are effectively empty. Note, intervals compare for equality if the endpoints are equal, so [7, 5) != [8, 5) even though they are both empty.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2699:9,patch,patches,9,https://hail.is,https://github.com/hail-is/hail/pull/2699,2,['patch'],"['patch', 'patches']"
Deployability,"Previously everything was fine because the deploy script called; both the image creation and the deploy targets in make. However,; if we need to manually deploy, it is easy to forget this. I; modified the Makefile to make the dependency explicit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4687:43,deploy,deploy,43,https://hail.is,https://github.com/hail-is/hail/pull/4687,3,['deploy'],['deploy']
Deployability,"Previously the mtime of hail/prebuilt/lib/**/*.{so,dylib}, would be; updated during `reset-prebuilt` which would cause the prebuilts to be; newer than libhail, as such they wouldn't be copied. Also use `$(dir file)` rather than `$(basename file)`, as it gives the; directory whereas basename gives the path minus the extension.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6940:69,update,updated,69,https://hail.is,https://github.com/hail-is/hail/pull/6940,1,['update'],['updated']
Deployability,"Previously we get a stack trace without the http response body. I tested this; locally on a branch that does not exist:. # hailctl dev deploy --branch danking/hail:shuffler-deploymefdsafdsa --steps test_shuffler; HTTP Response code was 400; error finding {""repo"": {""owner"": ""danking"", ""name"": ""hail""}, ""name"": ""shuffler-deploymefdsafdsa""} at GitHub",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8846:135,deploy,deploy,135,https://hail.is,https://github.com/hail-is/hail/pull/8846,3,['deploy'],"['deploy', 'deploymefdsafdsa']"
Deployability,"Previously, unauthenticated developers would get a 401 nginx page when hitting an internal url. This now sends them through the auth flow. In the spirit of deploying before merge, you can try this out!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9996:156,deploy,deploying,156,https://hail.is,https://github.com/hail-is/hail/pull/9996,1,['deploy'],['deploying']
Deployability,"Prior to Kubernetes 1.24, when a `ServiceAccount` called `sa-foo` is created, a corresponding `Secret` containing a token for the service account is created call `sa-foo-token-xxxx`. The `ServiceAccount` resource contains the name of the corresponding secret, and to use the service account in Batch the batch-driver discovers the secret name from the service account resource. In [Kubernetes >=1.24](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#urgent-upgrade-notes), creating a `ServiceAccount` no longer automatically creates a corresponding token secret. Our cluster auto-upgraded and PRs are failing because batch cannot find the secret field in the service account resource for SAs in test namespaces. From here on out, we need to make those token secrets ourselves. I explicitly added a token secret for the service accounts that need it and changed the batch-driver to handle both old and new service accounts. I tested this in my own project since I already had an instance of Hail Batch / CI up and running. It was running on 1.23 so did not encounter this issue, but I:; 1. Upgraded the cluster to 1.24; 2. Dev deployed and received the same error that we're now seeing in [PRs](https://ci.hail.is/batches/7103889/jobs/14); 3. Manually redeployed batch and CI (from this branch); 4. Dev deployed successfully",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12745:489,upgrade,upgrade-notes,489,https://hail.is,https://github.com/hail-is/hail/pull/12745,5,"['Upgrade', 'deploy', 'upgrade']","['Upgraded', 'deployed', 'upgrade-notes', 'upgraded']"
Deployability,"Prior to this we add an entry to /etc/hosts so that jobs can contact the batch front end to submit batches. This entry in default is `batch.hail`, and in a dev/pr namespace is `internal.hail`. The batch tests are jobs that run in default but submit test batches to the dev deployment and therefore need an entry for `internal.hail`. This essentially changes it to ""everything knows about internal but only default knows about default. We haven't seen this before because cloud dns in google provides these entries as a fallback, but I never set up in azure because it didn't seem necessary. If this succeeds, we can delete cloud dns in google",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11071#issuecomment-973003982:273,deploy,deployment,273,https://hail.is,https://github.com/hail-is/hail/pull/11071#issuecomment-973003982,1,['deploy'],['deployment']
Deployability,"Prometheus is now a StatefulSet, removing the need to delete a deployment and sleep in the monitoring Makefile. The storage has also been bumped up to 50Gi to prevent running out of storage in the future.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6447:63,deploy,deployment,63,https://hail.is,https://github.com/hail-is/hail/pull/6447,1,['deploy'],['deployment']
Deployability,"Prometheus storage was only 10Gb, so it filled up after 14 days. By default, Prometheus deletes logs after 15 days. I increased the storage size to 50Gb accordingly. I also decided to use this opportunity to switch Prometheus from a Deployment to a StatefulSet. This meant turning the PersistentVolume for PrometheusStorage in the monitoring.yaml file to a PersistentVolumeClaim within the Prometheus StatefulSet spec. However, the claim was configured to mount at the same location as the previous PersistentVolume, and I did not first delete the old PersistentVolume. As a result, the new 50Gb disk was not initially allocated. I resolved this by deleting the StatefulSet and the old PersistentVolume, then redeploying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6483#issuecomment-505884681:233,Deploy,Deployment,233,https://hail.is,https://github.com/hail-is/hail/issues/6483#issuecomment-505884681,1,['Deploy'],['Deployment']
Deployability,Protection re-enabled. Hand deploying CI now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8402#issuecomment-606678817:28,deploy,deploying,28,https://hail.is,https://github.com/hail-is/hail/pull/8402#issuecomment-606678817,1,['deploy'],['deploying']
Deployability,Provide option to skip log4j configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8571:29,configurat,configuration,29,https://hail.is,https://github.com/hail-is/hail/pull/8571,1,['configurat'],['configuration']
Deployability,"Pushed an additional commit:; - try to deploy all projects from the toplevel file; - at toplevel, only deploy if project has changed,; - added new build convention `<project>/get-deployed-sha.sh` which should output the current deployed sha for the project (if there is one)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4408#issuecomment-424187841:39,deploy,deploy,39,https://hail.is,https://github.com/hail-is/hail/pull/4408#issuecomment-424187841,4,['deploy'],"['deploy', 'deployed', 'deployed-sha']"
Deployability,"Pushed some more changes:; - first foray into RBAC; - created service account for batch; - batch run jobs in batch-pods namespace; - authorize with role binding; - hand-tested, batch is working. batch will now be found at `batch.default` instead of `batch` when running from batch-jobs namespace. I updated the batch Client to reflect this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4545#issuecomment-429681232:299,update,updated,299,https://hail.is,https://github.com/hail-is/hail/pull/4545#issuecomment-429681232,1,['update'],['updated']
Deployability,"Pushing pixels around. Summary of changes:; - make links blue, with underline :hover, :active; - added data-table class for striped, hover-highlighting tables, dark background headers, use all over; - put open_in_new on links that open new tabs; - made the search bars a bit wider (40%); - make Github links all be <title> #<number>, where number is a lighter gray; - format attributes of repos and batches in CI tighter; - added batch labels on PR page; - made artifacts link clickable (long overdue). @tpoterba I think this will fix most of your complaints from earlier today. I deployed just CI by hand to verify it works, looks good. I'm going to leave off styling for a bit after this and work on workshops in notebook2 and support many jobs in batch (paging, search, etc.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7097:581,deploy,deployed,581,https://hail.is,https://github.com/hail-is/hail/pull/7097,1,['deploy'],['deployed']
Deployability,Putting the tar in its own `COPY` layer means we drag around the tar in the resultant image. This uses a read-only mount for the subsequent `RUN` command to extract the tar without adding it to the image. This should save close to 100MB on the image size. Also realized that this happens in our pip-installed images.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11760:299,install,installed,299,https://hail.is,https://github.com/hail-is/hail/pull/11760,1,['install'],['installed']
Deployability,"PyCQA/pylint/issues/7425"">PyCQA/pylint#7425</a></p>; </li>; </ul>; <h1>What's New in astroid 2.12.7?</h1>; <p>Release date: 2022-09-06</p>; <ul>; <li>; <p>Fixed a crash in the <code>dataclass</code> brain for uninferable bases.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7418"">PyCQA/pylint#7418</a></p>; </li>; </ul>; <h1>What's New in astroid 2.12.6?</h1>; <p>Release date: 2022-09-05</p>; <ul>; <li>; <p>Fix a crash involving <code>Uninferable</code> arguments to <code>namedtuple()</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7375"">PyCQA/pylint#7375</a></p>; </li>; <li>; <p>The <code>dataclass</code> brain now understands the <code>kw_only</code> keyword in dataclass decorators.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7290"">PyCQA/pylint#7290</a></p>; </li>; </ul>; <h1>What's New in astroid 2.12.5?</h1>; <p>Release date: 2022-08-29</p>; <ul>; <li>; <p>Prevent first-party imports from being resolved to <code>site-packages</code>.</p>; <p>Refs <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7365"">PyCQA/pylint#7365</a></p>; </li>; <li>; <p>Fix <code>astroid.interpreter._import.util.is_namespace()</code> incorrectly; returning <code>True</code> for frozen stdlib modules on PyPy.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1755"">#1755</a></p>; </li>; </ul>; <h1>What's New in astroid 2.12.4?</h1>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/astroid/commit/65bca39bbf254bc760ac9d388e5a09333eaf5c87""><code>65bca39</code></a> Bump astroid to 2.12.8, update changelog</li>; <li><a href=""https://github.com/PyCQA/astroid/commit/fab511c1477d13262e9e33b015906d4bca683953""><code>fab511c</code></a> Fix crash in <code>dataclass</code> brain (<a href=""https://github-redirect.dependabot.com/P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12158:1647,Release,Release,1647,https://hail.is,https://github.com/hail-is/hail/pull/12158,1,['Release'],['Release']
Deployability,"PyPI</p>; <pre><code>python -m pip install --upgrade --pre pandas==1.5.0rc0; </code></pre>; <p>Or from conda-forge</p>; <pre><code>conda install -c conda-forge/label/pandas_rc pandas==1.5.0rc0; </code></pre>; <p>Please report any issues with the release candidate on the pandas issue tracker.</p>; <h2>Pandas 1.4.4</h2>; <p>This is a patch release in the 1.4.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.4/whatsnew/v1.4.4.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <p>Thanks to all the contributors who made this release possible.</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pandas-dev/pandas/commit/87cfe4e38bafe7300a6003a1d18bd80f3f77c763""><code>87cfe4e</code></a> RLS: 1.5.0</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/ecc700c8be8e4af2799dc18ce5f7e6328c80e976""><code>ecc700c</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/48627"">#48627</a> on branch 1.5.x (DOC: Last changes to release notes for 1....</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/e726483d70938f3bff67e95358841a1f6271b149""><code>e726483</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/48619"">#48619</a> on branch 1.5.x (REGR: Loc.setitem with enlargement raises...</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/f83e2fe3327ad85ae2e8c4ba469fe98383243dbf""><code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12292:2382,release,release,2382,https://hail.is,https://github.com/hail-is/hail/pull/12292,1,['release'],['release']
Deployability,PySpark is broken on Python 3.8:. - pyspark issue http://mail-archives.apache.org/mod_mbox/spark-issues/201910.mbox/%3CJIRA.13263529.1571661486000.10415.1571661540017@Atlassian.JIRA%3E; - pyspark fix: https://github.com/apache/spark/commit/811d563fbf60203377e8462e4fad271c1140b4fa. The fix has not been released yet. It's in 3.0.0-preview.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7513:303,release,released,303,https://hail.is,https://github.com/hail-is/hail/issues/7513,1,['release'],['released']
Deployability,"Pycharm thought `hl.nd` didn't exist, and I'm pretty sure we could; have had issues on certain python installations without this change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9061:102,install,installations,102,https://hail.is,https://github.com/hail-is/hail/pull/9061,1,['install'],['installations']
Deployability,"Python 3.6 is end-of-life in just over a month, and since it's the default version on an `ubuntu:18.04` image, it seemed a good time to upgrade across the board to 20.04. I'm not sure if there's a better time to do this or if we should make an announcement to users that the default image is changing. Feel free to let it sit here until we want to make the switch. After this, the only occurrence of `ubuntu:18.04` in the codebase is in the third-party images that we mirror in our registry (so it will still be there, just not used by default). cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11046:136,upgrade,upgrade,136,https://hail.is,https://github.com/hail-is/hail/pull/11046,1,['upgrade'],['upgrade']
Deployability,"Python 3.8 [added a validate parameter](https://docs.python.org/3/library/logging.html#logging.Formatter) to the stdlib Formatter which is on by default and doesn't like our format strings, which I guess python 3.7 is just too lenient about? Anyway I updated the format string to match the docs' recommendation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11839:251,update,updated,251,https://hail.is,https://github.com/hail-is/hail/pull/11839,1,['update'],['updated']
Deployability,"Python 3.9</li>; <li>Additional commits viewable in <a href=""https://github.com/thibaudcolas/curlylint/compare/v0.12.0...v0.13.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=curlylint&package-manager=pip&previous-version=0.12.0&new-version=0.13.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11494:8653,upgrade,upgrade,8653,https://hail.is,https://github.com/hail-is/hail/pull/11494,6,['upgrade'],['upgrade']
Deployability,"Python 3: <a href=""https://pypi.org/project/avro/1.11.0"">https://pypi.org/project/avro/1.11.0</a></li>; <li>Ruby: <a href=""https://rubygems.org/gems/avro/versions/1.11.0"">https://rubygems.org/gems/avro/versions/1.11.0</a></li>; </ul>; <p>Thanks to everyone for contributing!</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/apache/avro/compare/release-1.10.0...release-1.11.0"">compare view</a></li>; </ul>; </details>; <br />. Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11475:4113,upgrade,upgrade,4113,https://hail.is,https://github.com/hail-is/hail/pull/11475,3,['upgrade'],['upgrade']
Deployability,"Python integration tests often fail waiting to allocate highmem instances for worker jobs.; Since we control both APIs, it seems reasonable to move the testing burdon for vm allocation onto batch and use contract testing on the query driver side. These contract tests cover:; - uploading the the ServiceBackendRPConfig to remote storage in python; - reading that config and forwarding the relevant sections to the batch service in scala. Admittedly these are fairly busy tests and make bare a lot of lower-level implementation details. While I believe these tests are good to have, they perhaps don't warrant the time investment to properly refactor for cleaner mocking. Should details of the main implementation change, these will likely break. I've made tweaks to the python unittest annotations for backend test filtering. The old system skipped tests after all required fixtures had been acquired. Using `@pytest.mark.{feature}` allows us to exclude tests before fixtures are setup as well as add additional setup/teardown code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14512:7,integrat,integration,7,https://hail.is,https://github.com/hail-is/hail/pull/14512,1,['integrat'],['integration']
Deployability,"Python interface changes:; - filter_variants_all -> drop_variants; - filter_samples_all -> drop_samples; - renamed ""condition"" to ""expr"" in parameter names where appropriate. Removed gradle installDist",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1397#issuecomment-280419318:190,install,installDist,190,https://hail.is,https://github.com/hail-is/hail/pull/1397#issuecomment-280419318,1,['install'],['installDist']
Deployability,"Python versioning is a huge problem. Basically every time we have used unbounded dependency versions, we've gotten burned (some package updates and now Hail is broken for anyone who tries to install it). John could find out that 0.24 is supported, but then we'd have to pin at `<0.25`, so this doesn't solve the problem generally. I think we're also feeling quite sour on conda at the moment as well. In particular, I had to fix the [environment.yml for LDSC](https://github.com/bulik/ldsc/pull/168) because **recent versions of conda removed scipy==0.18 from their registry**.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542183134:136,update,updates,136,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542183134,2,"['install', 'update']","['install', 'updates']"
Deployability,"QL example from README</li>; <li>Additional commits viewable in <a href=""https://github.com/python-parsy/parsy/compare/v1.1.0...v1.4.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=parsy&package-manager=pip&previous-version=1.1.0&new-version=1.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself). </details>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12007:4664,upgrade,upgrade,4664,https://hail.is,https://github.com/hail-is/hail/pull/12007,3,['upgrade'],['upgrade']
Deployability,"QL/commit/7f032a699d55340f05101deb4d7d4f63db4adc11""><code>7f032a6</code></a> remove coveralls from requirements</li>; <li><a href=""https://github.com/PyMySQL/PyMySQL/commit/69f6c7439bee14784e0ea70ae107af6446cc0c67""><code>69f6c74</code></a> ruff format</li>; <li><a href=""https://github.com/PyMySQL/PyMySQL/commit/b4ed6884a1105df0a27f948f52b3e81d5585634f""><code>b4ed688</code></a> test json - mariadb without JSON type (<a href=""https://redirect.github.com/PyMySQL/PyMySQL/issues/1165"">#1165</a>)</li>; <li><a href=""https://github.com/PyMySQL/PyMySQL/commit/bbd049f40db9c696574ce6f31669880042c56d79""><code>bbd049f</code></a> Support error packet without sqlstate (<a href=""https://redirect.github.com/PyMySQL/PyMySQL/issues/1160"">#1160</a>)</li>; <li><a href=""https://github.com/PyMySQL/PyMySQL/commit/9694747ae619e88b792a8e0b4c08036572452584""><code>9694747</code></a> pyupgrade</li>; <li><a href=""https://github.com/PyMySQL/PyMySQL/commit/1f0b7856de4008e7e4c1e8c1b215d5d4dfaecd1a""><code>1f0b785</code></a> chore(deps): update codecov/codecov-action action to v4 (<a href=""https://redirect.github.com/PyMySQL/PyMySQL/issues/1158"">#1158</a>)</li>; <li><a href=""https://github.com/PyMySQL/PyMySQL/commit/1e28be81c24dde66f8acbf4c5e24f60d6b5e72e7""><code>1e28be8</code></a> chore(deps): update github/codeql-action action to v3 (<a href=""https://redirect.github.com/PyMySQL/PyMySQL/issues/1154"">#1154</a>)</li>; <li><a href=""https://github.com/PyMySQL/PyMySQL/commit/f13f054abcc18b39855a760a84be0a517f0da658""><code>f13f054</code></a> chore(deps): update actions/setup-python action to v5 (<a href=""https://redirect.github.com/PyMySQL/PyMySQL/issues/1152"">#1152</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/PyMySQL/PyMySQL/compare/v1.1.0...v1.1.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pymysql&package-manager=pip&previous-version=1.1.0&new-version=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14556:6617,update,update,6617,https://hail.is,https://github.com/hail-is/hail/pull/14556,1,['update'],['update']
Deployability,Quieter rsync on make install,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10493:22,install,install,22,https://hail.is,https://github.com/hail-is/hail/pull/10493,1,['install'],['install']
Deployability,R uses `ymin` and `ymax`. This was added after last release so isn't a user exposed bug yet.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11362:52,release,release,52,https://hail.is,https://github.com/hail-is/hail/pull/11362,1,['release'],['release']
Deployability,"RACE, RBRACE, SEMI = Suppress.using_each(&quot;(){};&quot;); </code></pre>; <p><code>using_each</code> will also accept optional keyword args, which it will pass through to the class initializer. Here is an expression for single-letter variable names that might be used in an algebraic expression:</p>; <pre><code>algebra_var = MatchFirst(; Char.using_each(string.ascii_lowercase, as_keyword=True); ); </code></pre>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pyparsing/pyparsing/blob/master/CHANGES"">pyparsing's changelog</a>.</em></p>; <blockquote>; <h2>Version 3.1.0 - June, 2023</h2>; <ul>; <li>Added <code>tag_emitter.py</code> to examples. This example demonstrates how to insert; tags into your parsed results that are not part of the original parsed text.</li>; </ul>; <h2>Version 3.1.0b2 - May, 2023</h2>; <ul>; <li>; <p>Updated <code>create_diagram()</code> code to be compatible with railroad-diagrams package; version 3.0. Fixes Issue <a href=""https://redirect.github.com/pyparsing/pyparsing/issues/477"">#477</a> (railroad diagrams generated with black bars),; reported by Sam Morley-Short.</p>; </li>; <li>; <p>Fixed bug in <code>NotAny</code>, where parse actions on the negated expr were not being run.; This could cause <code>NotAny</code> to incorrectly fail if the expr would normally match,; but would fail to match if a condition used as a parse action returned False.; Fixes Issue <a href=""https://redirect.github.com/pyparsing/pyparsing/issues/482"">#482</a>, raised by byaka, thank you!</p>; </li>; <li>; <p>Fixed <code>create_diagram()</code> to accept keyword args, to be passed through to the; <code>template.render()</code> method to generate the output HTML (PR submitted by Aussie Schnore,; good catch!)</p>; </li>; <li>; <p>Fixed bug in <code>python_quoted_string</code> regex.</p>; </li>; <li>; <p>Added <code>examples/bf.py</code> B",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13334:4290,Update,Updated,4290,https://hail.is,https://github.com/hail-is/hail/pull/13334,1,['Update'],['Updated']
Deployability,RE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$argu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:6634,release,release,6634,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,REMINDER TO SELF: Change the database configuration before merging in both GCP and Azure!!!!!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12813#issuecomment-1509030448:38,configurat,configuration,38,https://hail.is,https://github.com/hail-is/hail/pull/12813#issuecomment-1509030448,1,['configurat'],['configuration']
Deployability,"RFCs; implementations and did some refactors for JOSE:</p>; <ul>; <li>RFC8037: CFRG Elliptic Curve Diffie-Hellman (ECDH) and Signatures in JSON Object Signing and Encryption (JOSE)</li>; <li>RFC7638: JSON Web Key (JWK) Thumbprint</li>; </ul>; <p>We also fixed bugs for integrations:</p>; <ul>; <li>Fixed support for HTTPX&gt;=0.14.3</li>; <li>Added OAuth clients of HTTPX back via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/270"">#270</a></li>; <li>Fixed parallel token refreshes for HTTPX async OAuth 2 client</li>; <li>Raise OAuthError when callback contains errors via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/275"">#275</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/lepture/authlib/blob/master/docs/changelog.rst"">authlib's changelog</a>.</em></p>; <blockquote>; <h2>Version 0.15.5</h2>; <p><strong>Released on Oct 18, 2021.</strong></p>; <ul>; <li>Make Authlib compatible with latest httpx</li>; <li>Make Authlib compatible with latest werkzeug</li>; <li>Allow customize RFC7523 <code>alg</code> value</li>; </ul>; <h2>Version 0.15.4</h2>; <p><strong>Released on Jul 17, 2021.</strong></p>; <ul>; <li>Security fix when JWT claims is None.</li>; </ul>; <h2>Version 0.15.3</h2>; <p><strong>Released on Jan 15, 2021.</strong></p>; <ul>; <li>Fixed <code>.authorize_access_token</code> for OAuth 1.0 services, via :gh:<code>issue#308</code>.</li>; </ul>; <h2>Version 0.15.2</h2>; <p><strong>Released on Oct 18, 2020.</strong></p>; <ul>; <li>Fixed HTTPX authentication bug, via :gh:<code>issue#283</code>.</li>; </ul>; <h2>Version 0.15.1</h2>; <p><strong>Released on Oct 14, 2020.</strong></p>; <ul>; <li>Backward compitable fix for using JWKs in JWT, via :gh:<code>issue#280</code>.</li>; </ul>; <h2>Version 0.15</h2>; <p><strong>Released on Oct 10, 2020.</strong></p>; <p>This is the last release before v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11483:2506,Release,Released,2506,https://hail.is,https://github.com/hail-is/hail/pull/11483,1,['Release'],['Released']
Deployability,"Rahul reported this failing on the following gnomad pipeline:. ```; import hail as hl; from gnomad.utils.vep import process_consequences; from gnomad.resources.grch37 import gnomad. gnomad_v2_exomes = gnomad.public_release(""exomes""); ht_exomes = gnomad_v2_exomes.ht(); ht_exomes_proc = process_consequences(ht_exomes); ht_exomes_proc._force_count(); ```. No test case included, but this kind of error will be impossible soon; (when requiredness exists on EmitType, not SType/PType).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10286:52,pipeline,pipeline,52,https://hail.is,https://github.com/hail-is/hail/pull/10286,1,['pipeline'],['pipeline']
Deployability,Re-tested UI with dev deploy: looks good.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7606#issuecomment-557911795:22,deploy,deploy,22,https://hail.is,https://github.com/hail-is/hail/pull/7606#issuecomment-557911795,1,['deploy'],['deploy']
Deployability,"Re: your review @danking . We can make the HailContext available on the workers. As far as I can tell, we don't right now because we would need to serialize all the values of HailContext that aren't serializable, broadcast it, and change get to grab the broadcasted value. I could do that. It probably wouldn't take me that long, but this change reverts TabixReader to a behavior that it had during development due to Tim's concern that the hadoop configuration is not serializable. We thought the original version would be okay because TabixReader was only ever constructed on the driver. We were wrong, and considering that we intend to use this to read hundreds of thousands of files at a time, the parallelization is probably a good thing. This change fixes the bug I had in a way consistent with much of our codebase, without making larger changes to how we handle HailContext.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5033#issuecomment-449490579:448,configurat,configuration,448,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449490579,1,['configurat'],['configuration']
Deployability,Readme update,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1036:7,update,update,7,https://hail.is,https://github.com/hail-is/hail/pull/1036,1,['update'],['update']
Deployability,"Ready to look at. . Abstracts file system functionality. We no longer pass around a Hadoop Configuration w/ implicit methods defined in RichHadoopConfiguration. Instead we define an abstract FS class (could be a trait as well) to serve as our file system interface, and provide one Hadoop implementation to maintain existing functionality. The PR has many lines, but should hopefully be relatively easy to follow; mostly involves renaming. . cc @cseed , thanks @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083:91,Configurat,Configuration,91,https://hail.is,https://github.com/hail-is/hail/pull/6083,1,['Configurat'],['Configuration']
Deployability,Realized I forgot to update the GTEx `.rst` files along with the name changes in PR #10526.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10596:21,update,update,21,https://hail.is,https://github.com/hail-is/hail/pull/10596,1,['update'],['update']
Deployability,"Realized that the notebook python app should in fact speak https because it is exposed on the pod even though it does not have a service in front of it. For example, prometheus scrapes all visible ports on a pod and it anticipates https. This was triggering the deluge of errors from notebook and deploying this into default seems to have stopped them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10250:297,deploy,deploying,297,https://hail.is,https://github.com/hail-is/hail/pull/10250,1,['deploy'],['deploying']
Deployability,Rebased and dev deploy kicked off: https://ci.hail.is/batches/8122588,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1932835714:16,deploy,deploy,16,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1932835714,1,['deploy'],['deploy']
Deployability,"Records in the job_group_inst_coll_cancellable_resources table are dead once a; job group completes. We already compact records when a job group is cancelled. ; We are yet to do this for finished job groups. See the linked issue for a more ; detailed motivation. This change adds two background tasks:; 1. finds uncompacted groups of records for finished job groups and; compacts them by summing across the token field.; 2. finds compacted records for finished job groups and deletes them if; all associated resources are 0. The results of both tasks converge to a fixed point where the only remaining; records are for those jobs groups that are unfinished, cancelled or have; resources outstanding. I've taken care to optimise the underlying SQL queries as best as I can. Both; make heavy use of lateral joins to avoid explodes - the natural implementation; of both are prohibitively expensive. I've tested these tasks in a dev deploy where I created a number of batches and; observed that records from this table have indeed been compacted and destroyed; on completion. It's not immediately obvious to me how to automate testing for ; these. AFAICT, we lack any automated integration testing for these background ; tasks. Resolves: #14623",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14645:929,deploy,deploy,929,https://hail.is,https://github.com/hail-is/hail/pull/14645,2,"['deploy', 'integrat']","['deploy', 'integration']"
Deployability,Reflect updates to block matrix filtering in docs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7025:8,update,updates,8,https://hail.is,https://github.com/hail-is/hail/pull/7025,1,['update'],['updates']
Deployability,"Regarding history and fake pages: I’m confused as to why fake pages would be used, since upon refresh that fake page wouldn’t correspond to a real page, but this shouldn’t interfere. The behavior without this solution should be the same: the url is updated with a hash. If you’ve noticed a concrete issue, please share it, because I may not understand the specific use (e.g. RTD). Haven’t seen any issues in testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7334#issuecomment-544675789:249,update,updated,249,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544675789,1,['update'],['updated']
Deployability,"Regarding the `pyspark` issue, it looks like you have to use `--properties-file` and then put that comma-sepearted list as a newline-separated list in a file. That error message is pyspark's rather terrible way of telling you that it doesn't support a `--properties` option. Regarding the old version of VDS, the `master` branch of hail is now an unstable development branch. If you want a consistent user experience with backwards compatible interfaces, please check out and exclusively use the `0.1` branch. Tim discusses the wider change [here](http://discuss.hail.is/t/deployment-changes-branching-off-for-faster-development/261/1). The VDS format will likely change on the scale of days on the `master` branch. Regarding the `hail/scripts` folder, that is a repository of scripts that our build system uses as templates to create a pre-compiled, ready-to-go distribution that only requires a Spark installation. These distributions are available from the Google Storage API at gs://hail-common/distributions. If you're building from source, I recommend following exactly the steps listed [here](https://hail.is/docs/stable/getting_started.html#building-hail-from-source) so as to avoid any future hiccups. NB: the steps for [Running Hail Locally](https://hail.is/docs/stable/getting_started.html#running-hail-locally) are for using the pre-compiled distribution, not for the result of building hail directly from source.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551:573,deploy,deployment-changes-branching-off-for-faster-development,573,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551,4,"['deploy', 'install']","['deployment-changes-branching-off-for-faster-development', 'installation']"
Deployability,"Related (but for bash scripts): https://github.com/conda/conda/issues/7980. I haven't tried to install anaconda for some months, but when I first tried, ~6mo ago, definitely had this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6738#issuecomment-515163173:95,install,install,95,https://hail.is,https://github.com/hail-is/hail/issues/6738#issuecomment-515163173,1,['install'],['install']
Deployability,Related to https://github.com/hail-is/hail/pull/11985. Also update the MySQL version while we're at it. #assign services,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12025:60,update,update,60,https://hail.is,https://github.com/hail-is/hail/pull/12025,1,['update'],['update']
Deployability,Release # mismatch in the doc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12135:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/issues/12135,1,['Release'],['Release']
Deployability,Release 0.2.125,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13806:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/issues/13806,1,['Release'],['Release']
Deployability,Release 0.2.29,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7738:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/7738,1,['Release'],['Release']
Deployability,Release 0.2.30,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7772:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/7772,1,['Release'],['Release']
Deployability,Release 0.2.33,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8181:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/8181,1,['Release'],['Release']
Deployability,Release 0.2.34,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8292:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/8292,1,['Release'],['Release']
Deployability,Release 0.2.35,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8429:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/8429,1,['Release'],['Release']
Deployability,Release 0.2.38,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8592:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/8592,1,['Release'],['Release']
Deployability,Release 0.2.41,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8806:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/8806,1,['Release'],['Release']
Deployability,Release 0.2.43,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8881:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/8881,1,['Release'],['Release']
Deployability,Release 0.2.47,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9008:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/9008,1,['Release'],['Release']
Deployability,Release 0.2.56,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9386:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/9386,1,['Release'],['Release']
Deployability,Release 0.2.59,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9630:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/9630,1,['Release'],['Release']
Deployability,Release 0.2.60,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9703:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/9703,1,['Release'],['Release']
Deployability,"Release 0.2.60. Updated hail query change log. Didn't see any user changes for hail batch, let me know if I'm wrong @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9703:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/9703,2,"['Release', 'Update']","['Release', 'Updated']"
Deployability,Release 0.2.63,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10129:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/10129,1,['Release'],['Release']
Deployability,Release 0.2.64,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10183:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/10183,1,['Release'],['Release']
Deployability,Release 0.2.65,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10319:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/10319,1,['Release'],['Release']
Deployability,Release 0.2.66,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10419:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/10419,1,['Release'],['Release']
Deployability,Release 0.2.68,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10531:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/10531,1,['Release'],['Release']
Deployability,Release 0.2.71,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10649:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/10649,1,['Release'],['Release']
Deployability,Release 0.2.72,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10677:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/10677,1,['Release'],['Release']
Deployability,Release 0.2.73,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10692:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/10692,1,['Release'],['Release']
Deployability,Release 0.2.75,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10849:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/10849,1,['Release'],['Release']
Deployability,Release 0.2.78,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10996:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/10996,1,['Release'],['Release']
Deployability,Release 0.2.79,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11069:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/11069,1,['Release'],['Release']
Deployability,Release 0.2.80,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11158:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/11158,1,['Release'],['Release']
Deployability,Release 0.2.82,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11255:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/11255,1,['Release'],['Release']
Deployability,Release 0.2.83,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11298:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/11298,1,['Release'],['Release']
Deployability,Release 0.2.84,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11338:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/11338,1,['Release'],['Release']
Deployability,Release 0.2.85,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11363:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/11363,1,['Release'],['Release']
Deployability,Release 0.2.87,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11430:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/11430,1,['Release'],['Release']
Deployability,Release 0.2.90,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11560:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/11560,1,['Release'],['Release']
Deployability,Release 0.2.96,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11955:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/11955,1,['Release'],['Release']
Deployability,Release Change log 0.2.28,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7594:0,Release,Release,0,https://hail.is,https://github.com/hail-is/hail/pull/7594,1,['Release'],['Release']
Deployability,"Release Notes</h1>; <p>NumPy 1.23.5 is a maintenance release that fixes bugs discovered after; the 1.23.4 release and keeps the build infrastructure current. The; Python versions supported for this release are 3.8-3.11.</p>; <h2>Contributors</h2>; <p>A total of 7 people contributed to this release. People with a &quot;+&quot; by; their names contributed a patch for the first time.</p>; <ul>; <li><a href=""https://github.com/DWesl""><code>@​DWesl</code></a></li>; <li>Aayush Agrawal +</li>; <li>Adam Knapp +</li>; <li>Charles Harris</li>; <li>Navpreet Singh +</li>; <li>Sebastian Berg</li>; <li>Tania Allard</li>; </ul>; <h2>Pull requests merged</h2>; <p>A total of 10 pull requests were merged for this release.</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22489"">#22489</a>: TST, MAINT: Replace most setup with setup_method (also teardown)</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22490"">#22490</a>: MAINT, CI: Switch to cygwin/cygwin-install-action@v2</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22494"">#22494</a>: TST: Make test_partial_iteration_cleanup robust but require leak...</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22592"">#22592</a>: MAINT: Ensure graceful handling of large header sizes</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22593"">#22593</a>: TYP: Spelling alignment for array flag literal</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22594"">#22594</a>: BUG: Fix bounds checking for <code>random.logseries</code></li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22595"">#22595</a>: DEV: Update GH actions and Dockerfile for Gitpod</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22596"">#22596</a>: CI: Only fetch in actions/checkout</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22597"">#22597</a>: BUG: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12515:1273,install,install-action,1273,https://hail.is,https://github.com/hail-is/hail/pull/12515,1,['install'],['install-action']
Deployability,"Releases version 0.2.56. Stacked on #9373, since I'm mainly releasing for performance improvements in #9363, #9373, and #9374.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9386:0,Release,Releases,0,https://hail.is,https://github.com/hail-is/hail/pull/9386,1,['Release'],['Releases']
Deployability,"Relevant installed library versions:; hail==0.2.18; numpy==1.17.0; pandas==0.23.4; pyspark==2.4.1; scipy==1.3.0. Code ran:; ```; import hail as hl. Stack trace:; AttributeError: module 'hail' has no attribute 'expr'. AttributeError Traceback (most recent call last); in engine; ----> 1 import hail as hl. /home/cdsw/.local/lib/python3.6/site-packages/hail/__init__.py in <module>(); 29 from .expr import *; 30 from .genetics import *; ---> 31 from .methods import *; 32 from . import genetics; 33 from . import methods. /home/cdsw/.local/lib/python3.6/site-packages/hail/methods/__init__.py in <module>(); 4 import_plink, read_matrix_table, read_table, get_vcf_metadata, import_vcf, import_vcfs, \; 5 index_bgen, import_matrix_table; ----> 6 from .statgen import skat, identity_by_descent, impute_sex, \; 7 genetic_relatedness_matrix, realized_relationship_matrix, pca, \; 8 hwe_normalized_pca, pc_relate, split_multi, filter_alleles, filter_alleles_hts, \. /home/cdsw/.local/lib/python3.6/site-packages/hail/methods/statgen.py in <module>(); 5 ; 6 import hail as hl; ----> 7 import hail.expr.aggregators as agg; 8 from hail.expr.expressions import *; 9 from hail.expr.types import *. AttributeError: module 'hail' has no attribute 'expr'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762:9,install,installed,9,https://hail.is,https://github.com/hail-is/hail/issues/6762,1,['install'],['installed']
Deployability,Relevant line from build.gradle:. ```; bundled 'org.elasticsearch:elasticsearch-spark-20_2.11:' + elasticHadoopVersion(); ```. The `spark-20` and Scala `2.11` are hardcoded. These need to vary based on requested installation version.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9767:212,install,installation,212,https://hail.is,https://github.com/hail-is/hail/issues/9767,1,['install'],['installation']
Deployability,"Relevant to CNV work. Should hook up nicely with Jackie's pipeline work too, when that's done!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5208:58,pipeline,pipeline,58,https://hail.is,https://github.com/hail-is/hail/pull/5208,1,['pipeline'],['pipeline']
Deployability,"Remaining things to do:; - [ ] store references in Python, send references along with IR, and resolve them during parsing; - [ ] IBD (MatrixToTableApply); - [ ] index_bgen (Backend function); - [ ] maximal independent set; - [ ] import matrix table (MatrixReader); - [ ] concordance; - [x] MatrixTable.head (new IR?); - [ ] to/from_pandas that doesn't go through Spark (Arrow?). VEP and Nirvana should be converted to Pipeline when it is ready. I don't have a plan for ImportVCFs yet. Not counting linear algebra. Not counting making the Python front-end Java-free, those are other threads.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5197:418,Pipeline,Pipeline,418,https://hail.is,https://github.com/hail-is/hail/issues/5197,1,['Pipeline'],['Pipeline']
Deployability,"Remove conda from ci. Mirrors the changes in batch. I already tested deployment works by hand deploying a bogus version. It didn't have any watched targets though, so I didn't test everything. The pod starts successfully in python 3.6 (ci previously used python 3.7, see changes in shell_helper.py).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5800:69,deploy,deployment,69,https://hail.is,https://github.com/hail-is/hail/pull/5800,2,['deploy'],"['deploying', 'deployment']"
Deployability,Remove doc deploy to ci.hail.is,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4399:11,deploy,deploy,11,https://hail.is,https://github.com/hail-is/hail/pull/4399,1,['deploy'],['deploy']
Deployability,Removed sys_platform!='win32'. Broke installation on Amazon Linux 2 install. Is there different logic to put here?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136:37,install,installation,37,https://hail.is,https://github.com/hail-is/hail/pull/12136,2,['install'],"['install', 'installation']"
Deployability,Removed the line to template `service-account-batch-pods.yaml` as it no longer exists and added the missing `default_ns.name` field to the jinja environment for the deployment template.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10891:165,deploy,deployment,165,https://hail.is,https://github.com/hail-is/hail/pull/10891,1,['deploy'],['deployment']
Deployability,Removed “use this script” for fresh Ubuntu install,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2149:43,install,install,43,https://hail.is,https://github.com/hail-is/hail/pull/2149,1,['install'],['install']
Deployability,"Reopening this after some changes, mostly to see whether it works with g++-4.8.3 as installed; on the CI machines. The src/main/c/Makefile now builds a libboot.so with -fabi-version=2, which should work against; systems with g++-3.4.0 or later, and both libhail_abi_v2.so and libhail_abi_v9.so. The NativeCode; initialization then figures out which one to load. In theory this should work on MacOS systems back to MacOS 10.9 (Mavericks), which was the first; to use libc++ instead of libstdc++, and on Linux systems with g++3.4.0 or later. By default these libraries are built with ""-march=sandybridge"", which would work on all MacBook Pro's; released since 2011 (and is also the first cpu with AVX). In the medium term I favor the idea of packaging a known good tested compiler into the release, but ; believe that probably won't become critical until we're attempting whole-stage compilation, since the; generated PackDecoder's so far are relatively straightforward code and max out at about 2K lines.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-412736583:84,install,installed,84,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412736583,3,"['install', 'release']","['installed', 'release', 'released']"
Deployability,"Replace ProbabilityIterator with ProbabilityArray. This is slightly; faster and cleaner. Speeds up import_bgen, filter_variants(maf), linreg_multi_pheno; pipeline by about 5%. Remove samplePloidy array to reduce memory usage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1957:154,pipeline,pipeline,154,https://hail.is,https://github.com/hail-is/hail/pull/1957,1,['pipeline'],['pipeline']
Deployability,"Replaces #8533. I add two build steps: `test_dataproc` and `deploy`. Both of the new steps are; scoped for `dev` and `deploy`. However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8550:60,deploy,deploy,60,https://hail.is,https://github.com/hail-is/hail/pull/8550,5,"['deploy', 'release']","['deploy', 'release']"
Deployability,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2803:86,pipeline,pipeline,86,https://hail.is,https://github.com/hail-is/hail/issues/2803,1,['pipeline'],['pipeline']
Deployability,Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fcontexts\n2022-11-15 20:30:18.318 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fcontexts response 200\n2022-11-15 20:30:18.331 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Ff response 200\n2022-11-15 20:30:18.332 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: pty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY=: running job\n2022-11-15 20:30:18.333 Requester: INFO: request POST http://batch.hail/api/v1alpha/batches/6627669/update-fast\n2022-11-15 20:30:18.697 Requester: INFO: request POST http://batch.hail/api/v1alpha/batches/6627669/update-fast response 200\n2022-11-15 20:30:18.697 BatchClient: INFO: run: created update 2 for batch 6627669\n2022-11-15 20:30:18.697 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.802 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:18.852 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.866 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:18.917 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.934 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:18.985 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.999 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:19.049 Requester: INFO: requ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:24522,update,update-fast,24522,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['update'],['update-fast']
Deployability,Require partitioner.json.gz in VSM.read. Added hc.write_partitioning to update legacy VDSes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1285:72,update,update,72,https://hail.is,https://github.com/hail-is/hail/pull/1285,1,['update'],['update']
Deployability,Resolved by upgrade and mitigations. Created https://github.com/hail-is/hail/issues/6693 to track the more general issue of containers (non-buggily) running as root in our cluster.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6679#issuecomment-513230208:12,upgrade,upgrade,12,https://hail.is,https://github.com/hail-is/hail/issues/6679#issuecomment-513230208,1,['upgrade'],['upgrade']
Deployability,"Resolves #10747. I sshed into a VM and tried to do a clean install based on the issue raised above. As noted there, lz4 was missing from our cluster install docs. I also noticed `pip` returns a nonzero exit code if it tries to install something but doesn't find it, so I added some `|| true` to prevent a confusing error message. I also updated our examples to use Spark 3 by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10756:59,install,install,59,https://hail.is,https://github.com/hail-is/hail/pull/10756,4,"['install', 'update']","['install', 'updated']"
Deployability,Resolves:. > The provided deployment name 'batch-worker-pr-11104-default-fqwcdahrz1nj-highmem-k29sk-deployment' has a length of '67' which exceeds the maximum length of '64'. Please see https://aka.ms/arm-deploy for usage details.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11108#issuecomment-984122659:26,deploy,deployment,26,https://hail.is,https://github.com/hail-is/hail/pull/11108#issuecomment-984122659,3,['deploy'],"['deploy', 'deployment']"
Deployability,Returning HTTP status objects that subclass `Exception` [is deprecated](https://docs.aiohttp.org/en/stable/web_quickstart.html#exceptions) and will at some point block an upgrade to a higher version of aiohttp.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13379:171,upgrade,upgrade,171,https://hail.is,https://github.com/hail-is/hail/pull/13379,1,['upgrade'],['upgrade']
Deployability,"Revert ""[batch] Mount worker deploy config instead of using k8s secret""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13209:29,deploy,deploy,29,https://hail.is,https://github.com/hail-is/hail/pull/13209,1,['deploy'],['deploy']
Deployability,"Revert ""[batch] use one global deployment""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11163:31,deploy,deployment,31,https://hail.is,https://github.com/hail-is/hail/pull/11163,1,['deploy'],['deployment']
Deployability,"Reverts hail-is/hail#14461. We're hitting [github rate limits](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22ci%22%0A--%20severity%3DERROR%20OR%20WARNING;pinnedLogId=2024-04-18T15:42:12.920462831Z%2Fvdlhscspn377olu0;cursorTimestamp=2024-04-18T15:42:14.785330871Z;duration=P1D?project=hail-vdc) which is preventing actions like dev deploys. The limit is apparently [5,000 requests/hour](https://docs.github.com/en/rest/using-the-rest-api/rate-limits-for-the-rest-api?apiVersion=2022-11-28#primary-rate-limit-for-authenticated-users). This feels excessive to me, and I feel like we most be using the API poorly, but I want to just revert this before investigating further so CI doesn't get stuck.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14476:458,deploy,deploys,458,https://hail.is,https://github.com/hail-is/hail/pull/14476,1,['deploy'],['deploys']
Deployability,Reverts hail-is/hail#6376. This should just be in ci because the hailctl dev deploy should just send a request to the ci service rather than building things from a local computer.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6443:77,deploy,deploy,77,https://hail.is,https://github.com/hail-is/hail/pull/6443,1,['deploy'],['deploy']
Deployability,"Revised: You can switch the map and collect order to get more parallelism: groupBy, mapValues with computeUpperIndexBounds, collect, shift relative upper bound indices to absolute upper bound indices, zipWithIndex, feed into computeRectangles. Once we have durable partitionStarts on table, the whole pipeline can be done on the workers, with a final reduce to concatenate the blocksToKeep.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3185#issuecomment-379135882:301,pipeline,pipeline,301,https://hail.is,https://github.com/hail-is/hail/pull/3185#issuecomment-379135882,1,['pipeline'],['pipeline']
Deployability,"Right now if a `make ... deploy` step fails to build one of its images it continues (because the building now happens in `docker-build.sh`), which can result in running a service that expects an image not in GCR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10686:25,deploy,deploy,25,https://hail.is,https://github.com/hail-is/hail/pull/10686,1,['deploy'],['deploy']
Deployability,"Right now in master, the batch database gets cleared each time batch is deployed. Before we can remove this, we need to write all job task logs to GCS and insert the URI into the database. Otherwise, batch will try and read the logs for a previous job and not find them on the node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5845:72,deploy,deployed,72,https://hail.is,https://github.com/hail-is/hail/issues/5845,1,['deploy'],['deployed']
Deployability,"Right now we run dataproc tests only on release, not on every commit, because they're too expensive/slow. That way we never release a version that can't pass. I wonder if that's also the right strategy here -- adding QoB release tests for things that only go wrong at scale. That said, I don't want to block on that. Awesome change, thank you!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736#issuecomment-1499155038:40,release,release,40,https://hail.is,https://github.com/hail-is/hail/pull/12736#issuecomment-1499155038,3,['release'],['release']
Deployability,"Right now, if we accidentally make a wheel too big, it breaks our deploy. Let's avoid this by checking that we never let wheel get too big.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11592:66,deploy,deploy,66,https://hail.is,https://github.com/hail-is/hail/pull/11592,1,['deploy'],['deploy']
Deployability,"Right, we had to manually re-deploy CI for our setup as well. Pinging @daniel-goldstein just in case :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10107#issuecomment-805305318:29,deploy,deploy,29,https://hail.is,https://github.com/hail-is/hail/pull/10107#issuecomment-805305318,1,['deploy'],['deploy']
Deployability,Rolling into #13276,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13295#issuecomment-1652368804:0,Rolling,Rolling,0,https://hail.is,https://github.com/hail-is/hail/pull/13295#issuecomment-1652368804,1,['Rolling'],['Rolling']
Deployability,Rolling into #14471,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14477#issuecomment-2064381634:0,Rolling,Rolling,0,https://hail.is,https://github.com/hail-is/hail/pull/14477#issuecomment-2064381634,6,['Rolling'],['Rolling']
Deployability,Rolling into https://github.com/hail-is/hail/pull/13276,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13244#issuecomment-1652373979:0,Rolling,Rolling,0,https://hail.is,https://github.com/hail-is/hail/pull/13244#issuecomment-1652373979,14,['Rolling'],['Rolling']
Deployability,"Root cause: https://github.com/googleapis/google-auth-library-python/issues/443. `google-auth-oauthlib` does not pin any version of `google-auth` (I [created a PR](https://github.com/googleapis/google-auth-library-python-oauthlib/pull/71) to start pinning major & minor version). However, that wouldn't have saved us, the change (it increasingly appears to have been a bug, not an accidental breaking change) was introduced in the patch version 1.11.1. This pins *us* to 1.11.0. I am subscribed to the bug and will remove the patch-version pin once a fix is released.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8100:431,patch,patch,431,https://hail.is,https://github.com/hail-is/hail/pull/8100,3,"['patch', 'release']","['patch', 'patch-version', 'released']"
Deployability,"Rs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiI1NDBhNTVlYS05Y2JkLTRlZWEtYmJmZi00ZWU2NjlhZWJmYWQiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6IjU0MGE1NWVhLTljYmQtNGVlYS1iYmZmLTRlZTY2OWFlYmZhZCJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""540a55ea-9cbd-4eea-bbff-4ee669aebfad"",""prPublicId"":""540a55ea-9cbd-4eea-bbff-4ee669aebfad"",""dependencies"":[{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""}],""packageManager"":""pip"",""projectPublicId"":""fa47fca0-549b-41a3-8bf7-bcda4ca9a617"",""projectUrl"":""https://app.snyk.io/org/danking/project/fa47fca0-549b-41a3-8bf7-bcda4ca9a617?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,509],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13516:3711,patch,patch,3711,https://hail.is,https://github.com/hail-is/hail/pull/13516,2,"['patch', 'upgrade']","['patch', 'upgrade']"
Deployability,"Rs.*. For more information: <img src=""https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6InJyWmxZcEdHY2RyTHZsb0lYd0dUcVg4WkFRTnNCOUEwIiwiYW5vbnltb3VzSWQiOiJhZWIyYjAwNS1lYjhhLTRiMzgtYjkwMS04YzRmNTY2OGM3ZDYiLCJldmVudCI6IlBSIHZpZXdlZCIsInByb3BlcnRpZXMiOnsicHJJZCI6ImFlYjJiMDA1LWViOGEtNGIzOC1iOTAxLThjNGY1NjY4YzdkNiJ9fQ=="" width=""0"" height=""0""/>; 🧐 [View latest project report](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr). 🛠 [Adjust project settings](https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source&#x3D;github&amp;utm_medium&#x3D;referral&amp;page&#x3D;fix-pr/settings). 📚 [Read more about Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""aeb2b005-eb8a-4b38-b901-8c4f5668c7d6"",""prPublicId"":""aeb2b005-eb8a-4b38-b901-8c4f5668c7d6"",""dependencies"":[{""name"":""jupyter-server"",""from"":""1.24.0"",""to"":""2.7.2""},{""name"":""setuptools"",""from"":""39.0.1"",""to"":""65.5.1""}],""packageManager"":""pip"",""projectPublicId"":""20159ae6-a5aa-42fa-845a-c89f5bcbf999"",""projectUrl"":""https://app.snyk.io/org/danking/project/20159ae6-a5aa-42fa-845a-c89f5bcbf999?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-JUPYTERSERVER-5862881"",""SNYK-PYTHON-JUPYTERSERVER-5862882"",""SNYK-PYTHON-SETUPTOOLS-3180412""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,509],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Access Control Bypass](https://learn.snyk.io/lesson/broken-access-control/?loc&#x3D;fix-pr); 🦉 [Open Redirect](https://learn.snyk.io/lesson/open-redirect/?loc&#x3D;fix-pr); 🦉 [Regular Expression Denial of Service (ReDoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13517:3535,patch,patch,3535,https://hail.is,https://github.com/hail-is/hail/pull/13517,2,"['patch', 'upgrade']","['patch', 'upgrade']"
Deployability,Run tests on Cray & Dataflow during deploy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/743:36,deploy,deploy,36,https://hail.is,https://github.com/hail-is/hail/issues/743,1,['deploy'],['deploy']
Deployability,"Running on Apache Spark version 2.3.0; Hail version 0.2.12-9409c0635781. The follow error occurs when reading a matrix table. This code worked with Hail v2.8. ```; Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ad.v1/ad_parent_linreg_all_races_one_over_60.py"", line 70, in <module>; mt=hl.read_matrix_table(mt_fn); File ""<decorator-gen-1219>"", line 2, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 1704, in read_matrix_table; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 558, in __init__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 158, in typ; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/matrix_ir.py"", line 40, in _compute_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 104, in matrix_type; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/backend/backend.py"", line 87, in _to_java_ir; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/ir/base_ir.py"", line 163, in parse; File ""/share/pkg/spark/2.3.0/install/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py"", line 1160, in __call__; File ""/share/pkg.7/hail/0.2.12/install/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: MappingException: Parsed JSON values do not match with class constructor; args=; arg types=; constructor=public is.hail.variant.AbstractMatrixTableSpec(). Java stack trace:; org.json4s.package$MappingException: Parsed JSON values do not match with class con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5744:444,install,install,444,https://hail.is,https://github.com/hail-is/hail/issues/5744,5,['install'],['install']
Deployability,"Running the command `hailctl dataproc modify --update-hail-version` will update the version of hail running on the cluster to the version you're currently running locally. So if user has a cluster already and we tell them to update hail, they can just do:; ```; pip install hail -U; hailctl dataproc modify my-cluster --update-hail-version; ```. This PR resolves #6674",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6919:47,update,update-hail-version,47,https://hail.is,https://github.com/hail-is/hail/pull/6919,5,"['install', 'update']","['install', 'update', 'update-hail-version']"
Deployability,"Running this pipeline ; https://github.com/macarthur-lab/hail-elasticsearch-pipelines/blob/master/luigi_pipeline/seqr_loading.py#L39; on a non-Broad VCF with this header and several example variants:; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##GATKCommandLine=<ID=HaplotypeCaller,CommandLine=""HaplotypeCaller --genotyping-mode DISCOVERY --output /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/_SNPINDEL_CALLER/CALL_SNPINDELS/fork0/chnk00-u77951d7808/files/default.vcf --intervals /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_SNPINDEL_PHASER/_SNPINDEL_CALLER/CALL_SNPINDELS/fork0/chnk00-u77951d7808/files/default.vcf.bed --input /mnt/fast/3P5CH/3P5CH/PHASER_SVCALLER_CS/PHASER_SVCALLER/_LINKED_READS_ALIGNER/MERGE_POS_BAM/fork0/join-u77951d1e3c/files/pos_sorted_bam.bam --reference /home/fgc4/10x/refdata-GRCh38-2.1.0/fasta/genome.fa --emit-ref-confidence NONE --gvcf-gq-bands 1 --gvcf-gq-bands 2 --gvcf-gq-bands 3 --gvcf-gq-bands 4 --gvcf-gq-bands 5 --gvcf-gq-bands 6 --gvcf-gq-bands 7 --gvcf-gq-bands 8 --gvcf-gq-bands 9 --gvcf-gq-bands 10 --gvcf-gq-bands 11 --gvcf-gq-bands 12 --gvcf-gq-bands 13 --gvcf-gq-bands 14 --gvcf-gq-bands 15 --gvcf-gq-bands 16 --gvcf-gq-bands 17 --gvcf-gq-bands 18 --gvcf-gq-bands 19 --gvcf-gq-bands 20 --gvcf-gq-bands 21 --gvcf-gq-bands 22 --gvcf-gq-bands 23 --gvcf-gq-bands 24 --gvcf-gq-bands 25 --gvcf-gq-bands 26 --gvcf-gq-bands 27 --gvcf-gq-bands 28 --gvcf-gq-bands 29 --gvcf-gq-bands 30 --gvcf-gq-bands 31 --gvcf-gq-bands 32 --gvcf-gq-bands 33 --gvcf-gq-bands 34 --gvcf-gq-bands 35 --gvcf-gq-bands 36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:13,pipeline,pipeline,13,https://hail.is,https://github.com/hail-is/hail/issues/8469,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"SClient: INFO: Created HDFS_DELEGATION_TOKEN token 11364 for farrell on ha-hdfs:scc; 2019-01-22 13:11:26 Client: WARN: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 2019-01-22 13:11:29 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_libs__5184408978318087972.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_libs__5184408978318087972.zip; 2019-01-22 13:11:30 Client: INFO: Uploading resource file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/hail-all-spark.jar; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/share/pkg/spark/2.2.1/install/python/lib/pyspark.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/pyspark.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:31 Client: INFO: Submitting application application_1542127",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:15984,install,install,15984,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['install'],['install']
Deployability,"SQL migrations are not permitted to be modified. Unfortunately, our tests; previously did not verify this at all. Indeed, a PR merged which modified a SQL; file. This PR caused main to fail a deploy. This change verifies that no SQL migration is mutated in the source SHA relative; to the target SHA. One can also use it locally by running `make; check-services` from the root. Unfortunately, it does not work properly when run; on the main branch because there is no obvious point of comparison. I considered comparing against the previous commit, but that might cause; failures if we have to manually fix something in batch. As such, I prefer a; non-deploy only test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9544:192,deploy,deploy,192,https://hail.is,https://github.com/hail-is/hail/pull/9544,2,['deploy'],['deploy']
Deployability,"SQL/PyMySQL#1165</a></li>; </ul>; <h2>New Contributors</h2>; <ul>; <li><a href=""https://github.com/hugovk""><code>@​hugovk</code></a> made their first contribution in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1134"">PyMySQL/PyMySQL#1134</a></li>; <li><a href=""https://github.com/svaskov""><code>@​svaskov</code></a> made their first contribution in <a href=""https://redirect.github.com/PyMySQL/PyMySQL/pull/1145"">PyMySQL/PyMySQL#1145</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/PyMySQL/PyMySQL/compare/v1.1.0...v1.1.1"">https://github.com/PyMySQL/PyMySQL/compare/v1.1.0...v1.1.1</a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/PyMySQL/PyMySQL/blob/main/CHANGELOG.md"">pymysql's changelog</a>.</em></p>; <blockquote>; <h2>v1.1.1</h2>; <p>Release date: 2024-05-21</p>; <blockquote>; <p>[!WARNING]; This release fixes a vulnerability (CVE-2024-36039).; All users are recommended to update to this version.</p>; <p>If you can not update soon, check the input value from; untrusted source has an expected type. Only dict input; from untrusted source can be an attack vector.</p>; </blockquote>; <ul>; <li>Prohibit dict parameter for <code>Cursor.execute()</code>. It didn't produce valid SQL; and might cause SQL injection. (CVE-2024-36039)</li>; <li>Added ssl_key_password param. <a href=""https://redirect.github.com/PyMySQL/PyMySQL/issues/1145"">#1145</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyMySQL/PyMySQL/commit/2cab9ecc641e962565c6254a5091f90c47f59b35""><code>2cab9ec</code></a> v1.1.1</li>; <li><a href=""https://github.com/PyMySQL/PyMySQL/commit/521e40050cb386a499f68f483fefd144c493053c""><code>521e400</code></a> forbid dict parameter</li>; <li><a href=""https://github.com/PyMySQL/PyMySQL/commit/7f032a699d55340f05101deb4d7d4f63db4adc11""><code>7f032a6</code></a> remove coveralls from requireme",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14556:4710,update,update,4710,https://hail.is,https://github.com/hail-is/hail/pull/14556,1,['update'],['update']
Deployability,"SQLConfig requires an `ssl_mode`, this prevents auth from creating Developer accounts. I already deployed this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9406:97,deploy,deployed,97,https://hail.is,https://github.com/hail-is/hail/pull/9406,1,['deploy'],['deployed']
Deployability,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:3527,update,update,3527,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,15,"['Update', 'install', 'update']","['Update', 'installation', 'installed', 'update', 'update-args', 'update-hail-version']"
Deployability,Sadly we don't test the release step of `build.yaml` - adding `do-not-test` to not waste resources.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045925528:24,release,release,24,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045925528,1,['release'],['release']
Deployability,Same update as on homepage.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6761:5,update,update,5,https://hail.is,https://github.com/hail-is/hail/pull/6761,1,['update'],['update']
Deployability,Scala `Table` still has `Option`al key for now. This PR just makes the user visible changes needed for 0.2 release.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4285:107,release,release,107,https://hail.is,https://github.com/hail-is/hail/pull/4285,1,['release'],['release']
Deployability,"SciPy 1.11.1 Release Notes</h1>; <p>SciPy <code>1.11.1</code> is a bug-fix release with no new features; compared to <code>1.11.0</code>. In particular, a licensing issue; discovered after the release of <code>1.11.0</code> has been addressed.</p>; <h1>Authors</h1>; <ul>; <li>Name (commits)</li>; <li>h-vetinari (1)</li>; <li>Robert Kern (1)</li>; <li>Ilhan Polat (4)</li>; <li>Tyler Reddy (8)</li>; </ul>; <p>A total of 4 people contributed to this release.; People with a &quot;+&quot; by their names contributed a patch for the first time.; This list of names is automatically generated, and may not be fully complete.</p>; <h1>SciPy 1.11.0 Release Notes</h1>; <p>SciPy <code>1.11.0</code> is the culmination of 6 months of hard work. It contains; many new features, numerous bug-fixes, improved test coverage and better; documentation. There have been a number of deprecations and API changes; in this release, which are documented below. All users are encouraged to; upgrade to this release, as there are a large number of bug-fixes and; optimizations. Before upgrading, we recommend that users check that; their own code does not use deprecated SciPy functionality (to do so,; run your code with <code>python -Wd</code> and check for <code>DeprecationWarning</code> s).; Our development attention will now shift to bug-fix releases on the; 1.11.x branch, and on adding new features on the main branch.</p>; <p>This release requires Python <code>3.9+</code> and NumPy <code>1.21.6</code> or greater.</p>; <p>For running on PyPy, PyPy3 <code>6.0+</code> is required.</p>; <h1>Highlights of this release</h1>; <ul>; <li>Several <code>scipy.sparse</code> array API improvements, including <code>sparse.sparray</code>, a new; public base class distinct from the older <code>sparse.spmatrix</code> class,; proper 64-bit index support, and numerous deprecations paving the way to a; modern sparse array experience.</li>; <li><code>scipy.stats</code> added tools for survival analysis, multiple hypothe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13228:1207,upgrade,upgrade,1207,https://hail.is,https://github.com/hail-is/hail/pull/13228,2,"['release', 'upgrade']","['release', 'upgrade']"
Deployability,Scorecard should use a readiness probe to prevent traffic from being sent to scorecard before it has fully updated itself and is ready to serve traffic. https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/. An HTTP readiness probe that hits `GET /` should be sufficient.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6648:107,update,updated,107,https://hail.is,https://github.com/hail-is/hail/issues/6648,1,['update'],['updated']
Deployability,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:265,pipeline,pipeline,265,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527,4,['pipeline'],['pipeline']
Deployability,Secrets needed to dev deploy site.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10066:22,deploy,deploy,22,https://hail.is,https://github.com/hail-is/hail/pull/10066,1,['deploy'],['deploy']
Deployability,See Maryam's example pipeline posted to Slack #hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/798:21,pipeline,pipeline,21,https://hail.is,https://github.com/hail-is/hail/issues/798,1,['pipeline'],['pipeline']
Deployability,"See `pyproject.toml` for the isort configuration. This adds isort as a part of the `check` target for the services. To manually run isort, just run `isort .` anywhere in the hail repo. It will properly find the config file and ignore non-services code. isort has a `black` setting so it should be compatible with subsequent format checks. It doesn't immediately play well with pre-commit (ignores file excludes and runs where it shouldn't e.g. migrations) so I will separately have to look into running it there. When run, it does the following:. - Inserts line breaks to keep long imports in the desired line length; - Sorts items imported from a given module, it appears first by data type (classes before functions) and then alphabetically; - Groups and orders imports by: stdlib, third party, first party (set in pyproject.toml), and local (inferred from imports that start with `.`). I like this a lot; - Sorts imports within each group by `import`vs`from … import`, then alphabetically by module",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11231:35,configurat,configuration,35,https://hail.is,https://github.com/hail-is/hail/pull/11231,1,['configurat'],['configuration']
Deployability,See discuss post: https://discuss.hail.is/t/redirect-or-find-vep-or-other-error-output-from-a-hail-pipeline/1308/9?u=danking. It looks like Hail isn’t capturing all the VEP output. Can someone look into this? Probably the way we’re executing external commands needs to also capture stderr and print it. Assigning Tim for delegation.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8146:99,pipeline,pipeline,99,https://hail.is,https://github.com/hail-is/hail/issues/8146,1,['pipeline'],['pipeline']
Deployability,See https://cloud.google.com/sdk/docs/install#deb . ### Security Assessment. - [x] This change has a low security impact,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14708:38,install,install,38,https://hail.is,https://github.com/hail-is/hail/pull/14708,1,['install'],['install']
Deployability,"See https://github.com/broadinstitute/gnomad-browser/issues/914. In [the line in question](https://github.com/broadinstitute/gnomad-browser/blob/b497106d97773affd81b48eadfa5586259e011e5/data-pipeline/src/data_pipeline/data_types/gtex_tissue_expression.py#L14), we attempt to export a `Table` with ~13,000 columns, and get the following error: `is.hail.relocated.org.objectweb.asm.MethodTooLargeException: Method too large: __C19580collect_distributed_array.__m19633split_InsertFields ()V` (see above-referenced issue for full stacktrace). Hail version was 0.2.96-39909e0a396f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11972:191,pipeline,pipeline,191,https://hail.is,https://github.com/hail-is/hail/issues/11972,1,['pipeline'],['pipeline']
Deployability,See https://hail.zulipchat.com/#narrow/stream/123000-general/topic/Release.200.2E2.2E90/near/274617637,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11551:67,Release,Release,67,https://hail.is,https://github.com/hail-is/hail/pull/11551,1,['Release'],['Release']
Deployability,"See the FAQ Style Guide. **Annotations**; - [ ] Do I need to define the types when using `annotatesamples table`?; - [ ] How does Hail annotate variants overlapping different intervals in an interval list?; - [ ] How do I input phenotype information into Hail?; - [ ] Is there a way to see all annotations present in the dataset?. **Expression Language**; - [ ] Can I use regular expressions in the Hail expression language?; - [ ] how can i filter samples based on whether or not they have a particular variant?. **Data Representation**; - [ ] How are insertion and deletion variants coded in the VDS?; - [ ] How are the boundaries for Pseudo-autosomal variants determined?. **Exporting Data**; - [ ] How can I export all global annotations to a file?; - [ ] How do I export my data so there are separate VCFs per chromosome?; - [ ] How do I export my annotations as a JSON file?; - [ ] How do I export updated call statistics (AC, AF) to the info field of the VCF?. **Developer Tools**; - [ ] Is there a style guide I should use for IntelliJ?. **Importing Data**; - [ ] How do I import data from a VCF file?; - [ ] How do I import annotations in JSON format?; - [ ] Is the UCSC file 0 or 1 based?. **Methods**; - [ ] Does Hail handle sex chromosomes differently in variantqc and sampleqc?; - [ ] How do I parse the variant annotations from VEP to find the worst functional consequence?; - [ ] How do I find all variants where the functional change on the canonical transcript results in a missense mutation?; - [ ] Is rHetHom calculated over indels+SNPs or just SNPs?; - [ ] Are sampleqc and variantqc calculated only on PASS variants?. **Optimize Pipeline**; - [ ] When should I write my data to a VDS file?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/812:904,update,updated,904,https://hail.is,https://github.com/hail-is/hail/issues/812,2,"['Pipeline', 'update']","['Pipeline', 'updated']"
Deployability,Seeing 500 errors on create (maybe latest not deployed to your namespace),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112#issuecomment-535220649:46,deploy,deployed,46,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-535220649,1,['deploy'],['deployed']
Deployability,Seems like monkey patching with event can somehow override the timeout. We're not using event though. https://github.com/kennethreitz/requests/issues/3924#issuecomment-307502871,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5566#issuecomment-471054575:18,patch,patching,18,https://hail.is,https://github.com/hail-is/hail/issues/5566#issuecomment-471054575,1,['patch'],['patching']
Deployability,Seems to be issue at end:. pip2 install ./; hail-ci-build.sh: line 27: pip2: command not found,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5323#issuecomment-463207883:32,install,install,32,https://hail.is,https://github.com/hail-is/hail/pull/5323#issuecomment-463207883,1,['install'],['install']
Deployability,"Seen in a deploy:; ```; Warning: policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11602#issuecomment-1068671635:10,deploy,deploy,10,https://hail.is,https://github.com/hail-is/hail/pull/11602#issuecomment-1068671635,1,['deploy'],['deploy']
Deployability,"SelectGlobals(TensorIR, body: IR). In the body of TensorContract and TensorMap2, four refs are free: `l`, `r`, `i`, and `j`. In the body of TensorMap, three refs are free: `e`, `i`, `j`. In the body of TensorContract, all four refs are aggregables. In the TensorMap and TensorMap2, they are scalar values. No aggregations are allowed in the body of TensorSelectGlobals. It is just `SparkContext.broadcast`. ## From Python. C[[ u @ v ]] := TensorContract(; C[[ u ]],; C[[ v ]],; 1,; 0,; hl.agg.sum(l * r)). C[[ u + v ]] := TensorMap2(; C[[ u ]],; C[[ v ]],; l + r). C[[ u + 1 ]] := TensorMap(; C[[ u ]],; e + I32(1)). C[[ u + hl.ndarray(...) ]] := TensorMap(; TensorSelectGlobals(; C[[ u ]],; uuid1,; C[[ hl.ndarray(...) ]]); e + NDArrayIndex(GetField(""globals"", uuid1), i, j)). ## Transformations. This representation admits elegant transformations:. TensorMap2(TensorMap(u, x), v, body); <=>; TensorMap2(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). TensorMap(TensorMap(u, x), y); <=>; TensorMap(u, Let(uuid1, x, y[Ref(uuid1)/e])). TensorContraction(TensorMap(u, x), v, body); <=>; TensorContraction(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). the above rule needs care wrt aggregations, namely the let must be pushed under the aggregation, but no further. All these rules need to be careful because we don't want to lose the ability to send something through BLAS. Perhaps these rules should be left entirely to the ""pipeline""-level (see: Arcturus' recent work) optimizer (after translation to tables of small tensors is complete, at which point BLAS operations are explicit). ## Compilation. At first, we pattern match the items that can be represented via BlockMatrix, err'ing on unrepresentable expressions. C2[[ TensorMap(u, ApplyUnaryPrimOp(Plus(), Ref(""e""), F64(n))) ]]; =; u.scalarAdd(n). C2[[ TensorMap2(u, v, ApplyBinaryPrimOp(Plus(), Ref(""l""), Ref(""r""))) ]]; =; u.add(v). C2[[ TensorContraction(u, v, (ApplyAggOp Sum () None ((ApplyBinaryPrimOp `*` (Ref l) (Ref r))))) ]]; =; u.dot(v)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5195:2271,pipeline,pipeline,2271,https://hail.is,https://github.com/hail-is/hail/issues/5195,1,['pipeline'],['pipeline']
Deployability,"ServiceBackend/apiserver is known to not be working right now. It isn't being deployed or maintained. @johnc1231 and @catoverdrive were working on some tasks related to this. Few tasks:; - the global reference state in the JVM backend has to go, and needs to be stored in the Python client. This means reference information needs to be including along with queries.; - Table => CollectDArray lowering needs to be finished so apiserver can use the new `scheduler` to execute pipelines.; - Need to implement GoogleFS on the JVM side. I think someone just needs to take on ""get service backend working again"". As per our quarterly planning discussion, it might make sense to focus on upstream tasks for now (lowering, batch).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7068#issuecomment-531915477:78,deploy,deployed,78,https://hail.is,https://github.com/hail-is/hail/issues/7068#issuecomment-531915477,2,"['deploy', 'pipeline']","['deployed', 'pipelines']"
Deployability,"Set the home directory, otherwise docker-credential-gcr fails. Also update to a non-deprecated Ubuntu image version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9761:68,update,update,68,https://hail.is,https://github.com/hail-is/hail/pull/9761,1,['update'],['update']
Deployability,"Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 14:46:38 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@28f0ac7{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@49a30f89{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4495af6e{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6baf9f3b{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@562ad221{/static/sql,null,AVAILABLE,@Spark}; 2018-10-09 14:46:39 StateStoreCoordinatorRef: INFO: Registered StateStoreCoordinator endpoint; 2018-10-09 14:46:39 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:39 SparkSqlParser: INFO: Parsing command: SHOW TABLES; 2018-10-09 14:46:40 SparkContext: INFO: Starting job: collect at utils.scala:44; 2018-10-09 14:46:40 DAGScheduler: INFO: Got job 0 (collect at utils.scala:44) with 1 output partitions; 2018-10-09 14:46:40 DAGScheduler: INFO: Final stage: ResultStage 0 (collect at utils.scala:44); 2018-10-09 14:46:40 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 14:46:40 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 14:46:40 DAGScheduler: INFO: Submitting ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:41), which has no missing parents; 2018-10-09 14:46:40 MemoryStore: INFO: Block broadcast_0 stored as values in memory (estimated size 6.0 KB, free 366.3 MB); 2018-10-09 14:46:41 MemoryStore: INFO: Block broadcast_0_piece0 stored as bytes in memory (estimated ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:32357,configurat,configuration,32357,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['configurat'],['configuration']
Deployability,"Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 15:04:33 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@16ba3696{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2780d0b8{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7cea1161{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@696b1f0{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@14d32b0c{/static/sql,null,AVAILABLE,@Spark}; 2018-10-09 15:04:34 StateStoreCoordinatorRef: INFO: Registered StateStoreCoordinator endpoint; 2018-10-09 15:04:34 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:34 SparkSqlParser: INFO: Parsing command: SHOW TABLES; 2018-10-09 15:04:36 SparkContext: INFO: Starting job: collect at utils.scala:44; 2018-10-09 15:04:36 DAGScheduler: INFO: Got job 0 (collect at utils.scala:44) with 1 output partitions; 2018-10-09 15:04:36 DAGScheduler: INFO: Final stage: ResultStage 0 (collect at utils.scala:44); 2018-10-09 15:04:36 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 15:04:36 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 15:04:36 DAGScheduler: INFO: Submitting ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:41), which has no missing parents; 2018-10-09 15:04:36 MemoryStore: INFO: Block broadcast_0 stored as values in memory (estimated size 6.0 KB, free 366.3 MB); 2018-10-09 15:04:36 MemoryStore: INFO: Block broadcast_0_piece0 stored as bytes in memory (estimated ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:14842,configurat,configuration,14842,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['configurat'],['configuration']
Deployability,"Shit, one more fix needed to get deploy back on track. Missed in this change: https://github.com/hail-is/hail/pull/5644",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5655:33,deploy,deploy,33,https://hail.is,https://github.com/hail-is/hail/pull/5655,1,['deploy'],['deploy']
Deployability,Should actually figure out how to unify all these variables in one file since they're at least used in both hail-ci-deploy.sh and get-deployed-sha.sh right now,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5009:116,deploy,deploy,116,https://hail.is,https://github.com/hail-is/hail/pull/5009,2,['deploy'],"['deploy', 'deployed-sha']"
Deployability,"Should be fixed there. Also, the organization of site vs docs is super confusing imo. We have a site folder, which contains the Nginx configuration of site, and also the kube definition of the site deployment. Which makes a lot of sense. However, it also needs files in ../hail/build/www. Those files are built using a script in /hail/python/docs, which grabs www files from the working directory, which in our case should be /hail and not /site, copies those to its ./build/www, merges them with a bunch of files from /hail/python/docs/... , but not only that, it also compiles all of the templates for our cwd ./www. Oh and we also test hail import during doc build, which seems outside of what a documentation / static html build process should do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5282#issuecomment-463307913:134,configurat,configuration,134,https://hail.is,https://github.com/hail-is/hail/issues/5282#issuecomment-463307913,2,"['configurat', 'deploy']","['configuration', 'deployment']"
Deployability,Should fix failing deploy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4745:19,deploy,deploy,19,https://hail.is,https://github.com/hail-is/hail/pull/4745,1,['deploy'],['deploy']
Deployability,Should help a bit to speed up the the create-fast and update-fast endpoints if the user doesn't have to wait on this request to the driver.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12360:54,update,update-fast,54,https://hail.is,https://github.com/hail-is/hail/pull/12360,1,['update'],['update-fast']
Deployability,Should probably be stacked on #11636. I'll send an email notifying others that they need to upgrade their node pools by the time this goes in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11638#issuecomment-1076345988:92,upgrade,upgrade,92,https://hail.is,https://github.com/hail-is/hail/pull/11638#issuecomment-1076345988,1,['upgrade'],['upgrade']
Deployability,Shuffling reviewers since I'd like to get this merged soonish so I can turn on deployment for 0.1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4225#issuecomment-417308800:79,deploy,deployment,79,https://hail.is,https://github.com/hail-is/hail/pull/4225#issuecomment-417308800,1,['deploy'],['deployment']
Deployability,"SigningRequest API conditions were updated:; <ul>; <li>a <code>status</code> field was added; this field defaults to <code>True</code>, and may only be set to <code>True</code> for <code>Approved</code>, <code>Denied</code>, and <code>Failed</code> conditions</li>; <li>a <code>lastTransitionTime</code> field was added</li>; <li>a <code>Failed</code> condition type was added to allow signers to indicate permanent failure; this condition can be added via the <code>certificatesigningrequests/status</code> subresource.</li>; <li><code>Approved</code> and <code>Denied</code> conditions are mutually exclusive</li>; <li><code>Approved</code>, <code>Denied</code>, and <code>Failed</code> conditions can no longer be removed from a CSR (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/90191"">kubernetes/kubernetes#90191</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>) [SIG API Machinery, Apps, Auth, CLI and Node]</li>; </ul>; </li>; <li>Cluster admins can now turn off /logs endpoint in kubelet by setting enableSystemLogHandler to false in their kubelet configuration file. enableSystemLogHandler can be set to true only when enableDebuggingHandlers is also set to true. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/87273"">kubernetes/kubernetes#87273</a>, <a href=""https://github.com/SaranBalaji90""><code>@​SaranBalaji90</code></a>) [SIG Node]</li>; <li>Custom Endpoints are now mirrored to EndpointSlices by a new EndpointSliceMirroring controller. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91637"">kubernetes/kubernetes#91637</a>, <a href=""https://github.com/robscott""><code>@​robscott</code></a>) [SIG API Machinery, Apps, Auth, Cloud Provider, Instrumentation, Network and Testing]</li>; <li>CustomResourceDefinitions added support for marking versions as deprecated by setting <code>spec.versions[*].deprecated</code> to <code>true</code>, and for optionally overriding the defaul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:5167,configurat,configuration,5167,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['configurat'],['configuration']
Deployability,Simplified the dev deploy interface: just specify fully qualified branch (user/repo:branch) and a list of steps (instead of profile) which are transitively closed over dependencies. Pick up namespace from the user database.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6915:19,deploy,deploy,19,https://hail.is,https://github.com/hail-is/hail/pull/6915,1,['deploy'],['deploy']
Deployability,"Since #13211, all jobs by default have a deploy config mounted into the container. The `worker-deploy-config` secret is no longer necessary, so long as we properly configure the namespace that CI jobs need to talk to.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13343:41,deploy,deploy,41,https://hail.is,https://github.com/hail-is/hail/pull/13343,2,['deploy'],"['deploy', 'deploy-config']"
Deployability,"Since everyone is asking about hardcalls:. ```; # (cd ../hail && gradle installDist) && ../hail/build/install/hail/bin/hail read -i profile225-splitmulti-hardcalls.vds ibd -o hail.genome ; :nativeLib UP-TO-DATE; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :jar UP-TO-DATE; :startScripts UP-TO-DATE; :installDist UP-TO-DATE. BUILD SUCCESSFUL. Total time: 2.728 secs; hail: info: running: read -i profile225-splitmulti-hardcalls.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.619s; hail: info: timing:; read: 3.824s; ibd: 3m19.2s; total: 3m23.1s. # dc; 5 k; 3 60 * 23 + ; 23 / p; 8.82608; ```. about 9x slower now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092#issuecomment-260651639:72,install,installDist,72,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260651639,3,['install'],"['install', 'installDist']"
Deployability,"Since my interface package isn't ready for release yet, here's a reproducible example using just R and sparklyr, with a Hail jar somewhere. Again, this is happening on the Mac, sparklyr version 0.8.4.9004 (there is probably a newer one on CRAN, I doubt that it matters). ```; data(mtcars); hail_jar <- ""/path/to/your/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sparklyr::invoke(ht, ""count""); ```. Thanks a lot for your continued attention to this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-429475190:43,release,release,43,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-429475190,1,['release'],['release']
Deployability,"Since netcdf broke my R installation, I upgraded R. Now to revert to 3.3.1, I'm trying to install from the downloadable tarball and running into a bunch of errors. Is this worth it? Why don't we just test against a static results file?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3281#issuecomment-377952235:24,install,installation,24,https://hail.is,https://github.com/hail-is/hail/pull/3281#issuecomment-377952235,3,"['install', 'upgrade']","['install', 'installation', 'upgraded']"
Deployability,"Since recently adding metadata server support for batch jobs in GCP, `gcloud` should now ""Just Work"" using the CI service account in CI jobs without explicitly configuring it with a key file, so we no longer need this line. I tested that this succeeds with a dev deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14308:263,deploy,deploy,263,https://hail.is,https://github.com/hail-is/hail/pull/14308,1,['deploy'],['deploy']
Deployability,"Since the dataproc tests only run on main commits (not on every PR commit, due to cost), I submitted a dev deploy to test the latest commit to this branch against dataproc: https://ci.hail.is/batches/8119055. ```; hailctl dev deploy -b danking/hail:dataproc-2.2 -s test_dataproc-37 -s test_dataproc-38; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1924204910:107,deploy,deploy,107,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1924204910,2,['deploy'],['deploy']
Deployability,"Since the first build.py we have had two bugs. Dev deploys would regularly; re-deploy the router service in the default namespace. The more recent bug; is that the router port ought to be 443, not 80. Any time we dev deploy,; I suppose we have broken the mainline, but this must have been resolved by; deploys that came shortly thereafter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9726:51,deploy,deploys,51,https://hail.is,https://github.com/hail-is/hail/pull/9726,4,['deploy'],"['deploy', 'deploys']"
Deployability,"Since this doesn't get tested by CI normally, running a dev deploy here: https://ci.hail.is/batches/403111. If it succeeds, we should feel good I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11273#issuecomment-1022499548:60,deploy,deploy,60,https://hail.is,https://github.com/hail-is/hail/pull/11273#issuecomment-1022499548,1,['deploy'],['deploy']
Deployability,Site update latest hash path,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4771:5,update,update,5,https://hail.is,https://github.com/hail-is/hail/pull/4771,1,['update'],['update']
Deployability,"Sleeping used to work when we had a small number of PRs and other jobs. Now, test-CI deploy jobs need to wait a long time to start running. We should have always been polling for CI to be finished deploying, I was just lazy. Fixed now. Also should free up @jigold 's PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5230:85,deploy,deploy,85,https://hail.is,https://github.com/hail-is/hail/pull/5230,2,['deploy'],"['deploy', 'deploying']"
Deployability,"Slowly getting rid of the regions being threading through non-allocating functions, part i + 1. This changes the method signature of CodeOrdering method signatures from f(region1, v1, region2, v2) to f(v1, v2). Some other function signatures (e.g. PInterval.loadStart) were also updated as necessary. No functionality has been changed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6700:279,update,updated,279,https://hail.is,https://github.com/hail-is/hail/pull/6700,1,['update'],['updated']
Deployability,"Small doc update, mention spark ml in Table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3272:10,update,update,10,https://hail.is,https://github.com/hail-is/hail/pull/3272,1,['update'],['update']
Deployability,Small readme updates,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1802:13,update,updates,13,https://hail.is,https://github.com/hail-is/hail/pull/1802,1,['update'],['updates']
Deployability,"Small release, but version 0.2.86 didn't fully fix the `from_pandas` issue multiple users are experiencing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11430:6,release,release,6,https://hail.is,https://github.com/hail-is/hail/pull/11430,1,['release'],['release']
Deployability,Small update to logreg.md,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/848:6,update,update,6,https://hail.is,https://github.com/hail-is/hail/pull/848,1,['update'],['update']
Deployability,"Snyk's upgrade and patch logic](https://support.snyk.io/hc/en-us/articles/360003891078-Snyk-patches-to-fix-vulnerabilities). [//]: # (snyk:metadata:{""prId"":""92bcf51f-c710-4a85-9af1-5ae170a8797a"",""prPublicId"":""92bcf51f-c710-4a85-9af1-5ae170a8797a"",""dependencies"":[{""name"":""certifi"",""from"":""2021.10.8"",""to"":""2023.7.22""},{""name"":""cryptography"",""from"":""3.3.2"",""to"":""41.0.5""},{""name"":""requests"",""from"":""2.27.1"",""to"":""2.31.0""}],""packageManager"":""pip"",""projectPublicId"":""c1c98f6a-57c6-4ecc-a329-3b744cab74bd"",""projectUrl"":""https://app.snyk.io/org/danking/project/c1c98f6a-57c6-4ecc-a329-3b744cab74bd?utm_source=github&utm_medium=referral&page=fix-pr"",""type"":""auto"",""patch"":[],""vulns"":[""SNYK-PYTHON-CERTIFI-3164749"",""SNYK-PYTHON-CERTIFI-5805047"",""SNYK-PYTHON-CRYPTOGRAPHY-3172287"",""SNYK-PYTHON-CRYPTOGRAPHY-3314966"",""SNYK-PYTHON-CRYPTOGRAPHY-3315324"",""SNYK-PYTHON-CRYPTOGRAPHY-3315328"",""SNYK-PYTHON-CRYPTOGRAPHY-3315331"",""SNYK-PYTHON-CRYPTOGRAPHY-3315452"",""SNYK-PYTHON-CRYPTOGRAPHY-3315972"",""SNYK-PYTHON-CRYPTOGRAPHY-3315975"",""SNYK-PYTHON-CRYPTOGRAPHY-3316038"",""SNYK-PYTHON-CRYPTOGRAPHY-3316211"",""SNYK-PYTHON-CRYPTOGRAPHY-5663682"",""SNYK-PYTHON-CRYPTOGRAPHY-5777683"",""SNYK-PYTHON-CRYPTOGRAPHY-5813745"",""SNYK-PYTHON-CRYPTOGRAPHY-5813746"",""SNYK-PYTHON-CRYPTOGRAPHY-5813750"",""SNYK-PYTHON-CRYPTOGRAPHY-5914629"",""SNYK-PYTHON-CRYPTOGRAPHY-6036192"",""SNYK-PYTHON-REQUESTS-5595532""],""upgrade"":[],""isBreakingChange"":false,""env"":""prod"",""prType"":""fix"",""templateVariants"":[""pr-warning-shown""],""priorityScoreList"":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],""remediationStrategy"":""vuln""}). ---. **Learn how to fix vulnerabilities with free interactive lessons:**. 🦉 [Use After Free](https://learn.snyk.io/lesson/use-after-free/?loc&#x3D;fix-pr); 🦉 [Access of Resource Using Incompatible Type (&#x27;Type Confusion&#x27;)](https://learn.snyk.io/lesson/type-confusion/?loc&#x3D;fix-pr); 🦉 [Denial of Service (DoS)](https://learn.snyk.io/lesson/redos/?loc&#x3D;fix-pr)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13938:9094,patch,patch,9094,https://hail.is,https://github.com/hail-is/hail/pull/13938,2,"['patch', 'upgrade']","['patch', 'upgrade']"
Deployability,"So I did some simple formatting on the ""Filter loci by a list of locus intervals"" example. . The cloud sphinx theme you mentioned on zulip has toggleable sections that look a bit nicer. I could emulate that formatting by writing a sphinx extension if we wanted to get fancier, but what do you think of this layout?. IMAGE 1. <img width=""720"" alt=""screen shot 2018-08-22 at 11 23 34 am"" src=""https://user-images.githubusercontent.com/35241112/44473344-1eb11c80-a5fe-11e8-954d-41440a031d24.png"">. IMAGE 2; clicking on `show` would expose more content:. <img width=""699"" alt=""screen shot 2018-08-22 at 11 23 46 am"" src=""https://user-images.githubusercontent.com/35241112/44473350-2375d080-a5fe-11e8-98e9-31f1c3bb825c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4089#issuecomment-415074240:143,toggle,toggleable,143,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-415074240,2,['toggle'],['toggleable']
Deployability,"So I think we should fix this is in a slightly different way. First, I want to unify the log and status page. Second, I want the status to actually include the whole job configuration, of which status is just a sub-field, so you can look at detail to what you submitted. Then it would make sense for these links to always be present. That said, they obviously shouldn't be broken. Instead of 404, we should just say ""job is pending, no logs"" or wahtever. Let me make pass on the batch2 UI and then revisit this, OK?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7449#issuecomment-549809632:170,configurat,configuration,170,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549809632,1,['configurat'],['configuration']
Deployability,"So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. FYI, you can't dev deploy monitoring. It's part of the infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584:134,deploy,deploy,134,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584,2,['deploy'],['deploy']
Deployability,So it seems like the problem is with ; `gs://future-variant-calling/future-pipeline/future.vds`. trying to run `sample_qc` on it also fails in exactly the same way...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743#issuecomment-359552294:75,pipeline,pipeline,75,https://hail.is,https://github.com/hail-is/hail/issues/2743#issuecomment-359552294,1,['pipeline'],['pipeline']
Deployability,"So many methods or parts of methods (like most variant and sample qc stats, and mean or standard deviation in genotype stream imputation or normalization) rely solely on the histogram of gt counts (like nHomRef, nMissing). We can write less code and gain efficiency by including these as a variant fields (or reserved variant annotation?) that is automatically updated whenever datasets are joined or filtered (independent of things like the va.qc.nHet) and written and read with VDS. I also think this will help users not run full qc over and over when all they want is updated missingness of want AF for a method like IBD. I'm curious what others think.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1774:361,update,updated,361,https://hail.is,https://github.com/hail-is/hail/issues/1774,2,['update'],['updated']
Deployability,"So overall plan is:. - get 2.2.0 build support in (this patch); - stop testing 2.1.0 once it isn't being deployed,; - start testing 2.2.0 (I will need to update the CI image to install Spark 2.2.0),; - add 2.2.0 to the list of deployed versions,; - make Spark 2.2.0/Dataproc 1.2 the version in cloudtools,; - drop testing/deploy support for 2.0.2. Did I miss anything?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2656#issuecomment-355617537:56,patch,patch,56,https://hail.is,https://github.com/hail-is/hail/pull/2656#issuecomment-355617537,6,"['deploy', 'install', 'patch', 'update']","['deploy', 'deployed', 'install', 'patch', 'update']"
Deployability,So people can write/distribute their own analyses which can be called from Hail pipelines. javacmd org.braodinstitue.laurent.MyAwesomeAnalysis options...,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/353:80,pipeline,pipelines,80,https://hail.is,https://github.com/hail-is/hail/issues/353,1,['pipeline'],['pipelines']
Deployability,So the master merge commit status points to the master tests logs but not the deploy logs. I was hoping for the deploy logs (although the test logs are good if we have both).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4437#issuecomment-438075286:78,deploy,deploy,78,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438075286,2,['deploy'],['deploy']
Deployability,"So the problem is probably that CI is running on master, and it creates the global config in the PR namespace and it won't have docker_root_image. We'll also need to update our production global-config since we're not applying Terraform updates to our cluster. You should be able to do this now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10107#issuecomment-799714090:166,update,update,166,https://hail.is,https://github.com/hail-is/hail/pull/10107#issuecomment-799714090,2,['update'],"['update', 'updates']"
Deployability,"So the reason for that error is this rule is insufficient:. ```scala; case x if x.typ == TVoid =>; x.children.foreach(c => infer(c.asInstanceOf[IR])); PVoid; ```. We need to update the environment as well, in some cases, such as ArrayFor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-584451828:174,update,update,174,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584451828,1,['update'],['update']
Deployability,"So this is the same problem you discovered last year. It's supposed to get fixed in 4.0.0, but that hasn't been released yet. https://github.com/hail-is/hail/commit/337383674697f51dd6e04a3be3acedf4ed7a59a9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10432#issuecomment-841481516:112,release,released,112,https://hail.is,https://github.com/hail-is/hail/pull/10432#issuecomment-841481516,1,['release'],['released']
Deployability,"So we already support building with Spark 3.2 if you build your own jar. We just use 3.1 for our pypi release because it's what Google, AWS, and Azure have their respective Spark images set to last time I checked.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11707#issuecomment-1085063134:102,release,release,102,https://hail.is,https://github.com/hail-is/hail/issues/11707#issuecomment-1085063134,1,['release'],['release']
Deployability,"So, certain versions of bokeh require certain versions of pandas. I don't think we can simultaneously support bokeh 1.4 and pandas 2 in the Hail code base (because old bokeh is broken on new pandas). I think the fix is to just forcibly upgrade everyone to latest bokeh (3.x) and update Hail to support latest bokeh. I have a PR coming for this. I haven't checked if new bokeh supports old pandas. Nor do I know if we have old pandas usage lurking in the codebase. Can we make our `pinned-requirements.txt` use pandas 2.0, fix whatever issues arise, but leave `requirements.txt` flexible for folks?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1520463643:236,upgrade,upgrade,236,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1520463643,2,"['update', 'upgrade']","['update', 'upgrade']"
Deployability,Some fixes to my recent resiliency changes. These weren't caught because gateway and router-resolver are part of infrastructure that isn't automated by ci yet. I needed to make these changes to deploy them by hand (which I did successfully).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6211:194,deploy,deploy,194,https://hail.is,https://github.com/hail-is/hail/pull/6211,1,['deploy'],['deploy']
Deployability,Some notes from discussion:; 1. Maybe add a pricing page with up to date pricing for resources.; 2. It is difficult to determine _all_ the work that will run just from a hail pipeline.; 3. Teach users how to inspect the work that hail actually does?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14711#issuecomment-2397522782:175,pipeline,pipeline,175,https://hail.is,https://github.com/hail-is/hail/issues/14711#issuecomment-2397522782,1,['pipeline'],['pipeline']
Deployability,"Some progress and new blocker on this topic. I moved to emr-6.11.1 that come with spark 3.3.2 & scala 2.12.15.; I upgraded the environment to get python 3.9 and java 11. * `emr-6.11.1`; * Java: java -version `11.0.20` (/usr/bin/java); * Python: python --version `3.9.18` (/usr/bin/python3); * Hadoop: hadoop version `3.3.3` (/usr/bin/hadoop); * Spark: spark-shell --version `3.3.2` (usr/bin/spark-shell); * Scala: spark-shell --version `2.12.15` (usr/bin/spark-shell). ```sh; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.20.1; ```. But then once I build hail on this environment, the spark version is downgraded to 2.12.13 and the Java error above come back. ```sh; cd /tmp; git clone --branch 0.2.124 --depth 1 https://github.com/broadinstitute/hail.git; cd hail/hail/; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.2; ```. * Spark: spark-shell --version `3.3.2` (usr/bin/spark-shell); * Scala: spark-shell --version `2.12.13` (usr/bin/spark-shell). ```sh; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2; /_/; ; Using Scala version 2.12.13, OpenJDK 64-Bit Server VM, 11.0.20.1; ```. If I purposly build Hail for scala 2.12.13, the Java error above come back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1767910000:114,upgrade,upgraded,114,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1767910000,2,"['install', 'upgrade']","['install-on-cluster', 'upgraded']"
Deployability,"Some, if not most of the delay in reaching a running notebook server appears to be due to the use of services. Services provide little apparent benefit at the moment, esp. since we're already managing these pods using a deployment controller. Remove them in favor of accessing pods directly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5379:220,deploy,deployment,220,https://hail.is,https://github.com/hail-is/hail/issues/5379,1,['deploy'],['deployment']
Deployability,Someone changed this at one point and did not update the makefile. cc: @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8048:46,update,update,46,https://hail.is,https://github.com/hail-is/hail/pull/8048,1,['update'],['update']
Deployability,"Something still isn't right with my configuration. The service and deployment are all up. I can curl to the internal gateway and it shows up in the logs. However, I'm getting 404 with this query: `http://hail.internal/jigold/batch2/healthcheck`. I set up cloud dns to route hail.internal to the internal gateway. I verified no traffic is recorded in the jigold router. I tried adding and removing the `gateway` service account but it made no difference.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6918#issuecomment-524103114:36,configurat,configuration,36,https://hail.is,https://github.com/hail-is/hail/pull/6918#issuecomment-524103114,2,"['configurat', 'deploy']","['configuration', 'deployment']"
Deployability,"Sometimes new versions of packages introduce breaking changes, which is why we have version pins. I'll look at pandas shortly and see if there's any reason we can't update the upper bound on that range though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542180948:165,update,update,165,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542180948,1,['update'],['update']
Deployability,"Somewhat related to this is the deployment configuration. That's a little easier to handle on the user's side by including it in the job's image directly, but it would be pretty convenient if it could be mounted automatically as well (as `/deploy-config/deploy-config.json`, with `""location"": ""gce""` and the `domain` set accordingly). Should that be another method on `Job` / `Batch` or should all of this functionality be hidden behind something more abstract to enable nested batches (that's really the main use case, I suppose).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-767883386:32,deploy,deployment,32,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-767883386,4,"['configurat', 'deploy']","['configuration', 'deploy-config', 'deployment']"
Deployability,"Soon, I will add certificates and keys to the secrets and I want; to add configuration parameters that specify the paths to those; certificates and keys. Therefore, the mount locations of the; secrets must be the same everywhere so the paths are valid.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8416:73,configurat,configuration,73,https://hail.is,https://github.com/hail-is/hail/pull/8416,1,['configurat'],['configuration']
Deployability,"Sorry @tpoterba , there's still more work to do to integrate this throughout Hail without tests failing. That's the next step!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1559#issuecomment-287180359:51,integrat,integrate,51,https://hail.is,https://github.com/hail-is/hail/pull/1559#issuecomment-287180359,1,['integrat'],['integrate']
Deployability,Sorry about that! Didn't realize it needed to be updated in 2 places.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3179#issuecomment-374385002:49,update,updated,49,https://hail.is,https://github.com/hail-is/hail/pull/3179#issuecomment-374385002,1,['update'],['updated']
Deployability,"Sorry for a fairly late comment on this PR, but I was wondering about the default configuration:. > CHANGELOG: Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to specify which cloud regions a job can run in. The default value is a job can run in any available region. We're looking forward to the functionality in this PR particularly because we're hoping that it'll allow us to schedule workers in the US, while our Batch deployment is in Australia. However, by default we really need to make sure that workers won't be scheduled in the US, to avoid accidental egress charges, as all our datasets are located in Australia. For processing gnomAD data (which is located in the US), spinning up workers colocated with the data would be fantastic though. Hence we'd really need a configurable default value on the deployment level, I believe:. - Generally allow scheduling in AU + US regions (specifically `australia-southeast1` and `us-central1`).; - By default, pick any region in AU only (in practice `australia-southeast1`).; - Allow jobs to explicitly specify to run in the US (in practice `us-central1`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1275427218:82,configurat,configuration,82,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1275427218,3,"['configurat', 'deploy']","['configuration', 'deployment']"
Deployability,Sorry for not getting this done quicker. There's two new soon to be PRs in the stack that you can see as commits here:; - [Add infrastructure for updates](https://github.com/hail-is/hail/pull/12010/commits/72ff68e628b97bae439d04d4cb45e8508941e8bb); - [Cleanup adding update id infrastructure](https://github.com/hail-is/hail/pull/12010/commits/6364402e965a4f33248eba21639642e14a6f82be). I'll make PRs for them on Monday once you give me the green light that no other major database changes are needed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1221109305:146,update,updates,146,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1221109305,2,['update'],"['update', 'updates']"
Deployability,"Sorry no one answered this earlier (consider bugging us on hail.zulipchat.com or discuss.hail.is). The first thing I'd say is update to a newer version of hail and see if this still happens, as you're 15 versions behind",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106#issuecomment-597321135:126,update,update,126,https://hail.is,https://github.com/hail-is/hail/issues/8106#issuecomment-597321135,1,['update'],['update']
Deployability,"Sorry one more place I remembered this needs to be changed in benchmark-service/deployment.yaml. ```; {% if deploy %}; - name: HAIL_BENCHMARK_BUCKET_NAME; value: hail-test; - name: START_POINT; value: ""2020-11-01T00:00:00Z""; - name: INSTANCE_ID; value: ""WetqnMQMoqq2""; {% else %}; - name: HAIL_BENCHMARK_BUCKET_NAME; value: hail-test-dmk9z; {% endif %}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10807#issuecomment-905864074:80,deploy,deployment,80,https://hail.is,https://github.com/hail-is/hail/pull/10807#issuecomment-905864074,2,['deploy'],"['deploy', 'deployment']"
Deployability,Sorry this was a lot more broken than I thought. I didn't remember everything I stripped down for the previous PRs and didn't add back in. The commit update and update-fast endpoints need to return the `start_job_id`. Hopefully that's the last of these issues,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12199#issuecomment-1255533276:150,update,update,150,https://hail.is,https://github.com/hail-is/hail/pull/12199#issuecomment-1255533276,2,['update'],"['update', 'update-fast']"
Deployability,Sorry to step on your toes here a little bit @illusional! I updated aiohttp to 0.7.4 so you don't have to bump it here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10106#issuecomment-786920511:60,update,updated,60,https://hail.is,https://github.com/hail-is/hail/pull/10106#issuecomment-786920511,1,['update'],['updated']
Deployability,"Sorry you hit this -- I think I understand what happened. Since we don't include jupyter as a Hail package dependency (it's a large dependency and pulls in a host of transitive dependencies as well), when you ran `jupyter` you picked up a different `jupyter` (probably the conda base environment one, which uses an entirely different Python installation). . I don't want to add jupyter as a dependency, but we can certainly add a note in the tutorials landing page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7136#issuecomment-535590377:341,install,installation,341,https://hail.is,https://github.com/hail-is/hail/issues/7136#issuecomment-535590377,1,['install'],['installation']
Deployability,"Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`). I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400:125,install,installed,125,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400,4,['install'],['installed']
Deployability,"Sorry, didn't see this earlier -- . this is intentional. We do this so that we don't require users to recompile the C libraries when the install the Python library. Our native library distribution story is definitely a work in progress and this will change (hopefully improve) in the future. Feel free to ping us if this answer isn't sufficient!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10170#issuecomment-827847152:137,install,install,137,https://hail.is,https://github.com/hail-is/hail/issues/10170#issuecomment-827847152,1,['install'],['install']
Deployability,"Sorry, should have clarified - the above code ""monkey-patches"" the subprocess Popen function so it prints before it runs. Just pop the code above your `hc = HailContext(...)`, everything else can remain unchanged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319716779:54,patch,patches,54,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319716779,1,['patch'],['patches']
Deployability,"Sorry, the title is wrong, this fixes hail batch deployment",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4442#issuecomment-424543053:49,deploy,deployment,49,https://hail.is,https://github.com/hail-is/hail/pull/4442#issuecomment-424543053,1,['deploy'],['deployment']
Deployability,"Sorry, this is the same PR as #9241. I had to rebase on master to dev deploy efficiently. I added a test to the existing PR and fixed why my tables weren't being created with dev deploy. I also reduced the query size in the test scope by only querying 2 days. It only changed it from 10 MB scanned instead of 20+ MB scanned, but I figured that was better than nothing. The last commit from 35cf654 are the new changes. FYI: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9360:70,deploy,deploy,70,https://hail.is,https://github.com/hail-is/hail/pull/9360,2,['deploy'],['deploy']
Deployability,Sounds good. Updated this PR to add `first` and `last` methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9474#issuecomment-694910861:13,Update,Updated,13,https://hail.is,https://github.com/hail-is/hail/pull/9474#issuecomment-694910861,1,['Update'],['Updated']
Deployability,Source of instability in the short term; will add back after release,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8055:61,release,release,61,https://hail.is,https://github.com/hail-is/hail/pull/8055,1,['release'],['release']
Deployability,"Spark 3.1.1 is out, dataproc image should be updated from the release candidate dependency within a week or so I think. The only remaining issue I think is a weird one, a particular blockmatrix test is failing because json4s can't find a constructor for `BlockMatrixSparsity` objects. Looking into it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10054#issuecomment-790702427:45,update,updated,45,https://hail.is,https://github.com/hail-is/hail/pull/10054#issuecomment-790702427,2,"['release', 'update']","['release', 'updated']"
Deployability,"Spark depends on a very old verison of SLF4J. We cannot upgrade. This removes this message:; ```; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; ```. Which, IMO, really should be a stop-the-world error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14054:56,upgrade,upgrade,56,https://hail.is,https://github.com/hail-is/hail/pull/14054,1,['upgrade'],['upgrade']
Deployability,"Spark depends on a very old verison of SLF4J. We cannot upgrade. We added this dependency ages ago to fix some undocumented issue with logging and SLF4J. It seems reasonable to me that we should just accept whatever version of SLF4J that Spark provides. This removes this message:; ```; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; ```. Which, IMO, really should be a stop-the-world error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14055:56,upgrade,upgrade,56,https://hail.is,https://github.com/hail-is/hail/pull/14055,1,['upgrade'],['upgrade']
Deployability,"Spark doesn't have per-type requiredness but a nullable flag on array elements and struct fields. You just needed to set our required status based on their struct fields. Here is a patch that fixes it:. ```; diff --git a/src/main/scala/is/hail/expr/AnnotationImpex.scala b/src/main/scala/is/hail/expr/AnnotationImpex.scala; index 5039471..4cedfd8 100644; --- a/src/main/scala/is/hail/expr/AnnotationImpex.scala; +++ b/src/main/scala/is/hail/expr/AnnotationImpex.scala; @@ -45,11 +45,11 @@ object SparkAnnotationImpex extends AnnotationImpex[DataType, Any] {; case DoubleType => TFloat64(); case StringType => TString(); case BinaryType => TBinary(); - case ArrayType(elementType, _) => TArray(importType(elementType)); + case ArrayType(elementType, containsNull) => TArray(importType(elementType).setRequired(!containsNull)); case StructType(fields) =>; TStruct(fields.zipWithIndex; .map { case (f, i) =>; - (f.name, importType(f.dataType)); + (f.name, importType(f.dataType).setRequired(!f.nullable)); }: _*); }; ; diff --git a/src/test/scala/is/hail/methods/KeyTableSuite.scala b/src/test/scala/is/hail/methods/KeyTableSuite.scala; index 8a46826..dbb3485 100644; --- a/src/test/scala/is/hail/methods/KeyTableSuite.scala; +++ b/src/test/scala/is/hail/methods/KeyTableSuite.scala; @@ -380,9 +380,9 @@ class KeyTableSuite extends SparkSuite {; .flatten(); ; val df = kt.toDF(sqlContext); -// df.printSchema(); -// df.show(); - val kt2 = KeyTable.fromDF(hc, df); + df.printSchema(); + df.show(); + val kt2 = KeyTable.fromDF(hc, df, key = Array(""v"")); assert(kt2.same(kt)); }; ; ```. We should require the KeyTable row type to be required. We've always worked internally with the invariant that rows themselves can't be missing. (If they were, they wouldn't be in the table.) I commented in the print statements to see what was going on. You should probably delete them. I also had to set the key in `fromDF` the key isn't represented in DataFrames.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2417#issuecomment-343800938:181,patch,patch,181,https://hail.is,https://github.com/hail-is/hail/pull/2417#issuecomment-343800938,1,['patch'],['patch']
Deployability,Spark worker port (on spark-worker) 9000; Spark driver port (on apiserver) 9001; Block manager port (on apiserver and spark-worker) 9002. The apiserver was hanging trying to connect to the master (and therefore notebook2 notebooks trying to connect to it) without this. Tested hand deploy and it fixed it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5736:282,deploy,deploy,282,https://hail.is,https://github.com/hail-is/hail/pull/5736,1,['deploy'],['deploy']
Deployability,Specifically : https://github.com/akotlar/hail/blob/3b639cf77e2ad44c3422b619a36cf33523032953/notebook2/hail-ci-deploy.sh,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5451#issuecomment-467633773:111,deploy,deploy,111,https://hail.is,https://github.com/hail-is/hail/pull/5451#issuecomment-467633773,1,['deploy'],['deploy']
Deployability,"Sphinx 1.8 broke compatibility (https://github.com/sphinx-doc/sphinx/issues/5460) with a number of themes including `sphinx_rtd_theme`. Sphinx 1.8.3 restores compatibility (https://github.com/sphinx-doc/sphinx/pull/5590) with said themes. Moreover, sphinx_rtd_theme 0.4.2 (https://github.com/rtfd/sphinx_rtd_theme/pull/672) fixed itself to be compatible with Sphinx 1.8. I tested this locally and search works for me. I also updated our `pandas` dependency because our `setup.py` declares compatibility with only `0.23.x`. Because I edited the environment files, I rebuilt the pr builder image.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5059:425,update,updated,425,https://hail.is,https://github.com/hail-is/hail/pull/5059,1,['update'],['updated']
Deployability,Split the Hail query backends into separate file. One goal here is to have spark_backend.py the only file that imports pyspark. Also: Added init_service(). init() is for initialising with the Spark backend. init_service() is for initialising with the service backend. Don't import the backends unelss they are used. This will allow us to avoid importing pyspark (or even having it installed) when using the service backend.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8656:381,install,installed,381,https://hail.is,https://github.com/hail-is/hail/pull/8656,1,['install'],['installed']
Deployability,"Stacked on #10920 and #10965. # Summary of Changes. ## hailtop.aiocloud.aioazure; - Renamed AzureResourcesClient to AzureResourceManagementClient; - Added AzureResourcesClient that hits a different API than before; - Added `AzureBaseClient.get_next_link` and `AzureBaseClient.delete_and_wait`. ## Batch; - Added `batch.azure` which mirrors the functionality of `batch.gcp`; - Renamed `worker_local_ssd_data_disk` to `local_ssd_data_disk` in the PoolConfig; - Renamed `worker_pd_ssd_data_disk_size_gb` to `external_data_disk_size_gb` in the PoolConfig; - Added {Azure,GCP}UserCredentials to the worker to abstract away the names of environment variables and the mount paths of credentials in containers. ## Auth; - Added new fields in the auth database for `azure service principal name` and `azure_credentials_secret_name`; - Made `auth` only create `GSAResource` if CLOUD == 'gcp'. ## Gear; - Added `azure-vm` to the location options for `DeployConfig`. # Assumptions:; - Mapped `{'lowmen': 'F', 'standard': 'D', 'highmem': 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10970:940,Deploy,DeployConfig,940,https://hail.is,https://github.com/hail-is/hail/pull/10970,1,['Deploy'],['DeployConfig']
Deployability,"Stacked on #11995 . This PR creates all of the new `aggregated_*_resources_by_date` tables that will be used for real time billing and making our billing queries fast and also populates them! It will probably run for ~7-8 hours (online migration) and add an estimated 200 GB to the database. It will take around 5-6 hours to populate the tables and the remainder of the time is doing an audit. I think we should whiteboard what is going on in person, but the general idea is as follows:; 1. Revert any previous work and set the trigger back to the original state (idempotent); 2. Find the latest complete or open batch id. We know that a complete batch will not have updates to the attempts table. This is extremely important because the next steps can be done in parallel rather than serially.; 3. Find offsets for complete batches up to the batch id from Step 1 in groups of 100 attempts; 4. Randomize the offsets and have a burn in period of 5000 to avoid the birthday problem where we populate the `aggregated_*_resources_by_date` tables.; 5. In 10-way parallelism (maxes out a 4 core database), randomly populate the tables for each chunk.; 6. From the last offset (original first running batch id), we sequentially process attempts in groups of 100. We take note of where we are at with tracking any updates to the attempts table (`attempts_time_msecs_diff`), populate the `aggregated_*_resources_by_date` tables, and then do a final catchup step where we apply any updates from `attempts_time_msecs_diff` for any attempts that we have already processed.; 7. Once we have reached the ""end"" of the attempts table, we lock all tables of interest especially the `attempts` table, and do one last final processing step before we add the new triggers that will auto-populate the `aggregated_*_resources_by_date` tables.; 8. Then we perform an audit and make sure things look correct. (I might need to change or eliminate the billing_project audit query because there are 5 batches with ~20 jobs that ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11996:667,update,updates,667,https://hail.is,https://github.com/hail-is/hail/pull/11996,1,['update'],['updates']
Deployability,"Stacked on #11997. This PR enables real-time billing. It should be relatively straightforward. It uses the MySQL v8 instant add of a column. Therefore, we need to switch the Azure infrastructure to use V8.0 before this PR merges. I'm not sure how long this update will lock the attempts table, but hopefully it's not too long. I can test it out on my test database next week. This is an online migration. The key thing to double check is I got the trigger updates to use the new `rollup_time` correctly and am not missing any updates to the attempts where the rollup time needs to be updated as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11998:257,update,update,257,https://hail.is,https://github.com/hail-is/hail/pull/11998,4,['update'],"['update', 'updated', 'updates']"
Deployability,"Stacked on #12120 . This PR implements open batches. Future PRs will expose the functionality to users in the Query Service and hailtop.batch. There's a [design document](https://docs.google.com/document/d/168Mq5nNATmSrwzL4h1oYGBIFmcNlgFyHr_Vwjx59Zss/edit#heading=h.ghe60pdzl3mv) that specified all of the changes. To briefly summarize, there are now the concept of batch updates. Each job belongs inside an ""update"". The BatchClient has two types of builders now: UpdateBatchBuilder and CreateBatchBuilder. I play some tricks with the job ids being allowed to be negative numbers denoting relative to an offset to make things more efficient when updating a batch because you don't have to make multiple API calls to get the current job offset in the batch. There are only two batch states in the database: `running` and `complete`. A batch starts out as `complete` until an update is committed at which point if the n_jobs > 0, it will change to `running`. The main thing to look at implementation-wise is the new stored procedure `commit_batch_update` with a nasty update that will block progress on the batch while the update is in progress. I added the updates to the UI. We can get rid of it if it's too confusing. There's also a `Time Updated` column now in the UI instead of `Time Closed`.; <img width=""1573"" alt=""Screen Shot 2022-07-07 at 5 33 50 PM"" src=""https://user-images.githubusercontent.com/1693348/177875516-5f48e9a7-7fc2-4344-b3d2-c9560a846abe.png"">. <img width=""786"" alt=""Screen Shot 2022-07-07 at 5 34 07 PM"" src=""https://user-images.githubusercontent.com/1693348/177875535-e9b3a99f-bdc9-4a3b-8a53-5d20df05f161.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010:372,update,updates,372,https://hail.is,https://github.com/hail-is/hail/pull/12010,8,"['Update', 'update']","['UpdateBatchBuilder', 'Updated', 'update', 'updates']"
Deployability,"Stacked on #12757. - This PR gets the ranges of existing rows from the attempt_resources, aggregated_*_resources_v2 tables in bunches of 100 and then migrates each bunch by triggering an after update trigger for those rows that haven't been migrated. The triggers were added in #12757. ; - There's an audit at the end to make sure the new v3 tables give the same answer as the old v2 tables with duplicate resources.; - We use the same trick with a burn-in period to avoid the birthday problem with deadlocks.; - I added a function that generates the where statements programmatically based on looking at the where statement from previous migrations where we wrote out the where statement by hand. I think this way is less error-prone than writing out the where statement for each table, but it might be harder to reason about. Let me know if this way is too confusing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12761:193,update,update,193,https://hail.is,https://github.com/hail-is/hail/pull/12761,1,['update'],['update']
Deployability,"Stacked on #13475. This PR renames the following tables to have job groups in the name instead of batches. Note that this PR needs to shutdown the batch deployment (offline migration). I'm not 100% sure this is necessary, but I want to avoid a case where MJC of the database migration job cannot happen thus deadlocking the system. ```sql; RENAME TABLE batch_attributes TO job_group_attributes,; batches_cancelled TO job_groups_cancelled,; batches_inst_coll_staging TO job_groups_inst_coll_staging, ; batch_inst_coll_cancellable_resources TO job_group_inst_coll_cancellable_resources, ; aggregated_batch_resources_v2 TO aggregated_job_group_resources_v2,; aggregated_batch_resources_v3 TO aggregated_job_group_resources_v3, ; batches_n_jobs_in_complete_states TO job_groups_n_jobs_in_complete_states;; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13810:153,deploy,deployment,153,https://hail.is,https://github.com/hail-is/hail/pull/13810,1,['deploy'],['deployment']
Deployability,"Stacked on #13995. This PR adds job groups to the client, but a Batch does not have the capability to create new job groups. New operations on the server are to get a job group and list jobs in a job group. No changes are made to how batch updates are done.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14016:240,update,updates,240,https://hail.is,https://github.com/hail-is/hail/pull/14016,1,['update'],['updates']
Deployability,Stacked on #14016. This PR needs to have the client/server protocol for creating job groups for the four types of creation/update events hashed out and implemented. Basic tests are there. We still need tests for billing and cancellation to make sure the aggregation and cancellation operations work properly.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14018:123,update,update,123,https://hail.is,https://github.com/hail-is/hail/pull/14018,1,['update'],['update']
Deployability,"Stacked on #6980 . **Problem 1**: functions like `min`/`max` need to do a length check,; preventing us from implementing them in a single primitive fold. **Problem 2**: functions like `mean` use a struct as the accumulator,; leading to allocation (!) every element. **Solution**: make it possible to have multiple primitive; accumulators. This is `ArrayFold2`. The node is different from `ArrayFold` in that it:; * has as sequence of accumulators, not just one; * has a sequence of seq ops, one for each accumulator. Each of these; sequence ops can see all the accumulators, and will see the updated; value from sequence operations with a smaller index.; * has a result op, which is a function from accumulators to result. By changing `min`/`max` to use ArrayFold2 and inlining these functions,; we can get a reasonable speedup on `split_multi_hts`:. #6980 (this PR's parent):. ```; 2019-09-03 07:11:16,374: INFO: burn in: 42.33s; 2019-09-03 07:11:56,085: INFO: run 1: 39.71s; 2019-09-03 07:12:34,916: INFO: run 2: 38.83s; 2019-09-03 07:13:14,087: INFO: run 3: 39.17s; ```. PR:; ```; 2019-09-03 07:32:10,416: INFO: burn in: 38.03s; 2019-09-03 07:32:39,237: INFO: run 1: 28.82s; 2019-09-03 07:33:07,778: INFO: run 2: 28.50s; 2019-09-03 07:33:35,997: INFO: run 3: 28.21s; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6981:592,update,updated,592,https://hail.is,https://github.com/hail-is/hail/pull/6981,1,['update'],['updated']
Deployability,"Stacked on #7000. Adds a new IR renderer in Python which integrates a CSE pass. It would be easy to argue that a CSE pass should be separate from the renderer. But we can't easily make the Python IR mutable, because a given IR tree might be used in multiple larger IR (which is exactly what this pass is taking advantage of!) so mutation which depends on the larger context won't work. So rather than rebuild the entire IR every time we print, I decided for now this is best integrated into the renderer. I think longer term this should be ported to scala as a full CSE pass (which first does hash-consing/value-numbering to find all repeated subexpressions). This is not a simple algorithm, but I did my best to make it understandable. If anything feels harder to follow than it should be, I'd like to try to improve it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7009:57,integrat,integrates,57,https://hail.is,https://github.com/hail-is/hail/pull/7009,2,['integrat'],"['integrated', 'integrates']"
Deployability,"Stacked on #9346 . Changes:; - infrastructure needed for kill switch; - UI page; - Default value for the limit is None. Testing:; - In the database migration, there's two updates that populate the initial state of the aggregated_billing_resources_table. I tested this by hand using a database that hadn't been migrated previously, but this might be good to double check.; - I ran the `check_resource_aggregation` loop while running `test_batch` and made sure there were no errors.; - I tested the UI page editing the limits with negative values and gibberish by hand to make sure those failed. I also refreshed the page to make sure the values were in the database and the update worked. So here's a PR where I convinced myself it was correct a couple of days ago, but the longer this sits, the less confident I'm going to be that there's not a mistake somewhere, especially if there are a lot of changes that need to be made to the code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9354:57,kill switch,kill switch,57,https://hail.is,https://github.com/hail-is/hail/pull/9354,3,"['kill switch', 'update']","['kill switch', 'update', 'updates']"
Deployability,Stacked on: https://github.com/hail-is/hail/pull/5509. Broadcast once and reuse the same broadcast where. Never serialize (except via broadcast). Same pattern as RVDPartitioner and Hadoop configuration.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5512:188,configurat,configuration,188,https://hail.is,https://github.com/hail-is/hail/pull/5512,1,['configurat'],['configuration']
Deployability,Stacked on: https://github.com/hail-is/hail/pull/5891. I found getting .in (or not) consistent between the configuration and the files was just error prone. I think this is just simpler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5907:107,configurat,configuration,107,https://hail.is,https://github.com/hail-is/hail/pull/5907,1,['configurat'],['configuration']
Deployability,"Stacked on: https://github.com/hail-is/hail/pull/7031. Changes:; - primary change was to add `Tokens.namespace_token_or_error` which prints a friendly error of the user doesn't have the necessary authentication; - added `hailctl auth list`, and made `hailctl dev config` with no options print out the current configuration; - implemented @danking's suggestion: change some natural entrypoints (BatchClient, get_userinfo, etc.) to take optional `deploy_config` argument and load the default config if not given",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7035:309,configurat,configuration,309,https://hail.is,https://github.com/hail-is/hail/pull/7035,1,['configurat'],['configuration']
Deployability,"Stacked on: https://github.com/hail-is/hail/pull/7440. Changes:; - start the instance with a 1-time use activation token in the metadata; - on activation, clear the activation token, send the worker the normal token and batch-gsa-key; - upgrade the worker image to -6 which has the latest cloud-sdk (v269). As far as I can tell, the metadata server is still available from within the worker container after the upgrade, so I'm not 100% sure why this change was necessary. However, it will make things easier to lock down later. I think the picture we want is:; - store the worker and batch logs in different buckets,; - the worker instance service account only has instance.delete* and object.insert on the worker log bucket,; - the service account used by the worker only has object.insert on the batch logs bucket,; - we block access to the metdata srever from within the docker containers.Leaving this for reference:. https://stackoverflow.com/questions/32512597/block-docker-access-to-specific-ip. This isn't 100% trivial because the metadata server is also the DNS server. We could try blocking everything except udp/53. I think ideally, we'd put the docker containers on a different network that could only route to the outside and use a public DNS server like 8.8.8.8. *An instance doesn't need extra permissions to shut itself down, so we could just do `shutdown -h now` on the worker and have the batch driver actually delete the instance. I think once this goes in we can try scale up tests again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7447:237,upgrade,upgrade,237,https://hail.is,https://github.com/hail-is/hail/pull/7447,2,['upgrade'],['upgrade']
Deployability,"Stacks on #5430. Once #5430 is in, the changes here will be limited to: 1) notebook.py: login/logout routes, the provision of authorized users, auth0 lib, 2) index.html 3) header.html: update lines 12 and 13 to read user from session. Provides basic login page. Below are a few images of it in action. Looks like app.hail.is. Handles authorized and workshop-only login. Handles login only; future PR will extend to checking, refreshing the session. cc @cseed . screenshots (notebook create button not yet PR'd , auth0 page not yet styled). <img width=""1141"" alt=""screen shot 2019-02-25 at 11 17 37 pm"" src=""https://user-images.githubusercontent.com/5543229/53387218-d62f3e80-3953-11e9-8653-e4c6b0e8294a.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 00 pm"" src=""https://user-images.githubusercontent.com/5543229/53387219-d62f3e80-3953-11e9-8595-d7f1ea58a243.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 18 pm"" src=""https://user-images.githubusercontent.com/5543229/53387220-d62f3e80-3953-11e9-9fba-e4a93b0374ee.png"">; <img width=""1141"" alt=""screen shot 2019-02-25 at 11 18 33 pm"" src=""https://user-images.githubusercontent.com/5543229/53387221-d62f3e80-3953-11e9-9527-7c4589846a29.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5437:185,update,update,185,https://hail.is,https://github.com/hail-is/hail/pull/5437,1,['update'],['update']
Deployability,"Stacks on #5452. See next comment for some additional details. . This gets notebook2 into a state equivalent to that of notebook 1 for user-facing content, but with the styling of app.hail.is. 2 PRs remain: bring fine-grained state updates, remove services. PR-specific commits to preview diff after #5452 merged:; https://github.com/hail-is/hail/pull/5476/commits/2f180ed0bfb3b0dfb7224df1ef6afba0e1a9cbfc; https://github.com/hail-is/hail/pull/5476/commits/a60780e506e77cddf6afbcf388f9bfb027a32f8f; https://github.com/hail-is/hail/pull/5476/commits/1d58947696043f16ad864ee38c40e6dccf87cb1a. edit notes: Added relevant commits.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5476:232,update,updates,232,https://hail.is,https://github.com/hail-is/hail/pull/5476,1,['update'],['updates']
Deployability,"Stacks on #5526. Once that commit goes in, the only changes will be to the /wait route, which will now issue a Kubernetes watcher, `notebook-state.html` to support JS state updates (and some minor changes to the organization of the notebook reporting UI). This checks whether the route may be reached both for jobs that are Ready (at page refresh), and those that have just transitioned into ready state (as alerted by the websocket route). cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5538:173,update,updates,173,https://hail.is,https://github.com/hail-is/hail/pull/5538,1,['update'],['updates']
Deployability,"Stacks on #5526. Once that commit goes in, the only changes will be to the /wait route, which will now issue a Kubernetes watcher, notebook-state.html to support JS state updates (and some minor changes to the organization of the notebook reporting UI). Relevant commit: ; 4f4e2b6f875e33da5787f665de1400f0f00a3623. This checks whether the route may be reached both for jobs that are Ready (at page refresh), and those that have just transitioned into ready state (as alerted by the websocket route). Next update will generalize the UI to N notebooks, to handle the case that internet-synchrony issues cause 2 non-deleted notebooks to be generated. This still needs a bit of work; when reach call fails due to 502, it continues issuing 502's, requiring a refresh. First attempts to cache bust, client side and nginx-side failed. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5540:171,update,updates,171,https://hail.is,https://github.com/hail-is/hail/pull/5540,2,['update'],"['update', 'updates']"
Deployability,"Stacks on #5874. Commit specific to this pr are: https://github.com/hail-is/hail/pull/5878/commits/e959eaf270c5dd9966e3c9c96f21d4f914097012, https://github.com/hail-is/hail/pull/5878/commits/fcbb0dc6ec6c678a54655afda996ee6b1148f2a1. Minor oddity: the 'updated' property isn't always available for folders. I can get around this if needed. I return the bucket for the ""owner"" property because the google sa isn't returned in the response. Will change this to the sa email (read at GoogleStorageFS instantiation)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878:252,update,updated,252,https://hail.is,https://github.com/hail-is/hail/pull/5878,1,['update'],['updated']
Deployability,"Start of an IR node for joining two streams. ## Logic; Works off the ""producer"" pattern, where a stream is an object that you can; 1. Initialize, so the head of the stream is ready to be consumed.; 2. Step, so updates the current head to the next element in the stream. A join then takes two streams and its Step steps the left stream, the right stream, or both streams until a ""valid"" state is reached. E.g. In an outer join every step is a new valid state while an inner join must loop until the match condition is met. The node must also take some combining binary functions so as not to produce runtime tuples. Optional ""left"" and ""right"" functions determine at compilation time whether an unmatched pair of values should still produce a value (e.g. in a left join you always act, just either on just the left head or both). ## State of the code base; - Added `produce` abstract method to `ArrayEmitter`. Tests will fail until that is implemented on all the array nodes.; - Inserted a small hack to the `ir.ToArray` emit case that uses the producer route; - Added a test case for `ArrayJoin` (this one passes! :)); - Have not yet incorporated missingness into producer logic and have not implemented `consume` for ArrayJoin`.; - `Binds` needs to be updated to add the appropriate bindings for the children. Right now it just adds them all.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6516:210,update,updates,210,https://hail.is,https://github.com/hail-is/hail/pull/6516,2,['update'],"['updated', 'updates']"
Deployability,"Started playing with [typer](https://typer.tiangolo.com/) on the plane. It's a somewhat thin wrapper around `click`, and I'm not entirely sold on one vs the other, but either seems a lot neater than argparse (I remember Cotton tried this years ago but I haven't looked at that PR). Click gives us the easy composition of CLIs, function name as the command name, the use of docstring as the description, help output, and most of the core functionality you see. Typer is the one translating python type hints into click type-checking, default setting, etc. Typer also uses rich if it's installed and allows you to install shell completion which is pretty neat, not sure if you can get that through click or if you need to use a click plugin. I replaced a couple of the hailctl commands here with the typer/click version. If you like what you see I can keep going on the rest of the commands (you can look to `batch/cli.py` and `hailctl/__main__.py` as an example).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13109:584,install,installed,584,https://hail.is,https://github.com/hail-is/hail/pull/13109,2,['install'],"['install', 'installed']"
Deployability,Still need to update a use case or two.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8365#issuecomment-604286431:14,update,update,14,https://hail.is,https://github.com/hail-is/hail/pull/8365#issuecomment-604286431,1,['update'],['update']
Deployability,Still need to update ci.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8405#issuecomment-606756521:14,update,update,14,https://hail.is,https://github.com/hail-is/hail/pull/8405#issuecomment-606756521,1,['update'],['update']
Deployability,"Still needs a black reformat. You can run `make -C batch/ check` ahead of time to catch these errors or add pre-commit hooks. ```; PYTHONPATH=${PYTHONPATH:+${PYTHONPATH}:}../hail/python:../gear:../web_common python3 -m black . --line-length=120 --skip-string-normalization --check --diff; --- batch/cloud/gcp/driver/create_instance.py	2022-06-02 12:28:49.199357 +0000; +++ batch/cloud/gcp/driver/create_instance.py	2022-06-02 12:31:14.836500 +0000; @@ -78,15 +78,17 @@; 'automaticRestart': False,; 'onHostMaintenance': 'TERMINATE',; }; ; if preemptible:; - result.update({; - 'provisioningModel': 'SPOT',; - 'instanceTerminationAction': 'DELETE',; - 'preemptible': True,; - }); + result.update(; + {; + 'provisioningModel': 'SPOT',; + 'instanceTerminationAction': 'DELETE',; + 'preemptible': True,; + }; + ); ; return result; ; return {; 'name': machine_name,; would reformat batch/cloud/gcp/driver/create_instance.py; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11878#issuecomment-1144893521:564,update,update,564,https://hail.is,https://github.com/hail-is/hail/pull/11878#issuecomment-1144893521,2,['update'],['update']
Deployability,"Still waiting on info about Jacob's pipeline, but that appears to be where Alicia's pipeline stalled in #5320. Konrad is also seeing a ~40 minute delay between two stages inside the `count_cols()` before the initial (temp) BlockMatrix is written for ld_matrix, which seems to be for (local) computation of the lowered MatrixAnnotateColsTable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5410:36,pipeline,pipeline,36,https://hail.is,https://github.com/hail-is/hail/issues/5410,2,['pipeline'],['pipeline']
Deployability,"Strange, I can't reply to directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414:658,pipeline,pipelines,658,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414,4,"['configurat', 'pipeline']","['configuration', 'pipelines']"
Deployability,"Strange, I couldn't get a similar example to fail, either through `hl.eval` or in a pipeline:. ```; def test_define_function_locus(self):; contig2 = hl.experimental.define_function(; lambda l: l.contig, hl.tlocus(hl.get_reference('GRCh38'))); t = hl.utils.range_table(1); t = t.annotate(locus = hl.locus('chr22', 123, 'GRCh38')); t = t.annotate(contig = contig2(t.locus)); self.assertEqual(t.collect()[0]['contig'], 'chr22'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5471#issuecomment-469341849:84,pipeline,pipeline,84,https://hail.is,https://github.com/hail-is/hail/pull/5471#issuecomment-469341849,1,['pipeline'],['pipeline']
Deployability,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:168,deploy,deploy,168,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662,3,['deploy'],['deploy']
Deployability,"Style guide draft with 3 sample commands and HTML/JS code . Not intended for merging to master. If you're happy with this, then I'll update the rest of the commands with the style guide I specified. The style guide does not address how to format tables (work in progress). **Style Guide:**; - docs/DocsStyleGuide.md. **Example Markdown Command Files to look at:**; - docs/commands/annotateglobal_expr; - docs/commands/annotateglobal_list; - docs/commands/annotateglobal_table. **HTML/JS code:**; - docs/index.html; - docs/buildDocs.js",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/569:133,update,update,133,https://hail.is,https://github.com/hail-is/hail/pull/569,1,['update'],['update']
Deployability,"Submitting a pipeline now looks like:. ```; $ hail pipeline.py; Submitted batch 120, see https://batch2.hail.is/batches/120; Waiting for batch 120...; Batch 120 complete: failure; ```. FYI @konradjk Pipeline.run now passes through kwargs to the backend. BatchBackend supports two new args: wait (default True) to wait for the pipeline to finish, and open (default False) to open the batch URL in the browser. It no longer attempts to print the failed jobs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7672:13,pipeline,pipeline,13,https://hail.is,https://github.com/hail-is/hail/pull/7672,4,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline']"
Deployability,Successfully ran pipeline in #2377 and found identical output.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358385663:17,pipeline,pipeline,17,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358385663,1,['pipeline'],['pipeline']
Deployability,"Suggested fix:. ```scala; case class UpdatedRow(orig: Row, i: Int, update: Any) extends Row {; ...; }. ```; This gets you the update with one allocation and no copy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1629#issuecomment-290846410:37,Update,UpdatedRow,37,https://hail.is,https://github.com/hail-is/hail/issues/1629#issuecomment-290846410,3,"['Update', 'update']","['UpdatedRow', 'update']"
Deployability,"Summary of changes:; - Overhaul tmpdir handling. Remove most of the old code. Added local_tmpdir to `init`. tmpdir is the networked tmpdir. local_tmpdir is the tmpdir used for local files on both the driver and the executors. Added tmpdir and localTmpdir to ExecuteContext. ExecuteContext removes tmp files on close. Tmp file base is now required, try to give good base names. Tmp file names are now generated by being sufficiently random.; - Removed fs from HailContext. This involved threading ctx and fs through lots of code (most of the changes).; - Added ExecuteContext to EmitModuleBuilder and friends. This is necessary because EmitMethodBuilder gives generated code access to backend, fs, etc. which are carried by the ctx.; - Some IR (mostly readers, but also VEP, which needs to load the VEP configuration to determine its type) have overall parameters that control their behavior (e.g. the VCF reader path) but have to do IO to determine other state (like the matrix type, determined from the VCF header). This complicates pretty printing, serialization, and equality. I clarified this. In particular, I seperate the parameters (see, for example, MatrixVCFReaderParameters) which are specified on creation and used for serialization and equality from other derived state. IR no longer close over ctx or fs and they don't need to do IO after their intiial construction.; - MatrixSpec has subspecs for the marginal tables, and TableSpec has the global and rows RVD. These are now loaded on construction, so lowering no longer neesd to do IO.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8581:802,configurat,configuration,802,https://hail.is,https://github.com/hail-is/hail/pull/8581,1,['configurat'],['configuration']
Deployability,"Summary of changes:; - change notebook states to Scheduling, Initializing, Ready (Running was too suggestion of being ready); - remove ""a_notebook"" notebook name, replace with ""Creating Notebook..."" or ""Open Notebook[open_in_new]"" link; - change nb-state-container styling, blue/underline link on top, but the whole container still acts as a link; - made material icons bold. I'm not sure if I like this, it is bordering on cartoonish.; - make checkmark green to match other success/ok coloring using green (e.g. messages); - un-bold pod name, which is really only interested for us (maybe we should remove?). It is deployed in my namespace if you want to play with it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7155:616,deploy,deployed,616,https://hail.is,https://github.com/hail-is/hail/pull/7155,1,['deploy'],['deployed']
Deployability,"Summary;; I tried running hail with spark-submit and a .py script with a short pipeline to compare speed. Offending line:; ```; kt = vds_results.make_table('v = v', 'pval = va.pval').export(""output/test.txt""); ```; gives; ```; File ""<decorator-gen-93>"", line 2, in export; File ""/home/ludvig/Programs/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: SparkException: Job aborted due to stage failure: Task 0.0 in stage 5.0 (TID 1591) had a not serializable result: is.hail.io.bgen.BgenRecordV11$$anon$1; ```; ```; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.BgenRecordV11$$anon$1, value: BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527:79,pipeline,pipeline,79,https://hail.is,https://github.com/hail-is/hail/issues/2527,1,['pipeline'],['pipeline']
Deployability,"Support [Artifact Registry](https://cloud.google.com/artifact-registry) to store images. It's useful for our deployment in Australia, as GCR does not support our continent. With AR, we would save on network egress. The PR adds a `DOCKER_PREFIX` variable, which is passed along in the code together with `GCP_PROJECT` and others. To switch to AR, one would need to modify `DOCKER_PREFIX` in `config.mk`:. ```; REGION := us-central1; DOCKER_PREFIX := gcr.io/$(PROJECT); ```. ```; REGION := australia-southeast1; DOCKER_PREFIX := $(REGION)-docker.pkg.dev/$(PROJECT)/hail; ```. Also, when making an initial deployment with Terraform, there is an extra variable `use_artifact_registry = false` that controls whether to use AR or GCR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10107:109,deploy,deployment,109,https://hail.is,https://github.com/hail-is/hail/pull/10107,2,['deploy'],['deployment']
Deployability,"Suppose you have two branches with the same base commit. Neither branch has Scala changes. Consider `make shadowJar` when switching between these branches: it thinks there's nothing to do because the Scala code hasn't changed. This of course doesn't work because the python version *is* changing (note: make install-editable refreshes the version files) and Hail refuses to use an out of date jar. This adds a tiny make macro that lets make targets depend on variables that depend on the latent environment, like git SHAs. To create a target for such a variable add this line: `$(eval $(call ENV_VAR,VARIABLE_NAME))`. Any rule that depends on the value of `VARIABLE_NAME` should depend on the target `env/VARIABLE_NAME`. I also split `BUILD_INFO` into the scala parts and the python parts and moved the scala dependency down to the shadow jar rule, where it belongs. This bug was hidden because build.gradle still regenerates the build info every time shadowJar is called. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6867:308,install,install-editable,308,https://hail.is,https://github.com/hail-is/hail/pull/6867,1,['install'],['install-editable']
Deployability,"Sure! Now that we have docker working, CI should be deployable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4446#issuecomment-424729339:52,deploy,deployable,52,https://hail.is,https://github.com/hail-is/hail/pull/4446#issuecomment-424729339,1,['deploy'],['deployable']
Deployability,"Sure, I'd like to know how the pip deploy works.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6567#issuecomment-509644515:35,deploy,deploy,35,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509644515,1,['deploy'],['deploy']
Deployability,"Surfaced because sometimes k8s secrets 404 for CI pipelines and we got FK constraint failures because there is no batch 0. No danger of bad data being written, just noise and unnecessary database load. Thank you foreign key checks!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14373:50,pipeline,pipelines,50,https://hail.is,https://github.com/hail-is/hail/pull/14373,1,['pipeline'],['pipelines']
Deployability,"TE 2].; > ; > I don't think changing these will have the desired effect and may make it impossible for someone to reproduce the database. The only changes to _existing_ sql you'll need to make are in the sql strings in python code.; > ; > 2. This needs to be written as a migration and maybe could be simplified?; > ; > I think this needs to be done as a database migration. We'll have no need for a stored procedure once complete. You can assume current columns and constraints exist, dispense with the error checking and simplify. Can you convert this to a sql script and add it to the end of the list of migrations in `build.yaml`? You'll probably want `online: false` too. I fear you'll have to take inspiration from `rename-job-groups-tables.sql` by applying one `ALTER TABLE` command then drop and recreate EVERYTHING that references that name (constraints, triggers, procedures etc). This will likely involve copy+paste and rename. Alternatively, create, execute then drop the procedure within `rename-job-groups-cancelled`.; > ; > [NOTE 1] migration applied in `build.yaml`; > ; > The relevant build step in `build.yaml` can be found by searching for the entry starting with the yaml below. This controls which migrations are applied and in what order.; > ; > ```yaml; > kind: createDatabase2; > name: batch_database; > databaseName: batch; > ```; > ; > [NOTE 2] estimated-current.yaml; > ; > I don't agree with why we have this. It would be nice to generate this automatically. Anyway, please keep your changes to this file as it's meant for documentation purposes only. None of it is applied and who knows how much of it works. Got it! I wasn't sure how Hail usually does schema update. Based on your above description the process becomes clearer ro me. Here's my second try:. - Updated `build.yaml` in the `batch` database migrations section.; - Simplified the sql in `rename-job-groups-cancelled-column.sql`. Do you mean `estimated-current.sql` rather than `estimated-current.yaml` above?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2334778045:2105,update,update,2105,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2334778045,3,"['Update', 'update']","['Updated', 'update']"
Deployability,TE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:2978,deploy,deploy-,2978,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['deploy'],['deploy-']
Deployability,"TML Theme: Fix double brackets on citation references in Docutils 0.18+.; Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10534"">#10534</a>: Missing CSS for nav.contents in Docutils 0.18+. Patch by Adam Turner.</li>; </ul>; <h1>Release 5.0.1 (released Jun 03, 2022)</h1>; <h2>Bugs fixed</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10498"">#10498</a>: gettext: TypeError is raised when sorting warning messages if a node; has no line number. Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10493"">#10493</a>: HTML Theme: :rst:dir:<code>topic</code> directive is rendered incorrectly with; Docutils 0.18. Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10495"">#10495</a>: IndexError is raised for a :rst:role:<code>kbd</code> role having a separator.; Patch by Adam Turner.</li>; </ul>; <h1>Release 5.0.0 (released May 30, 2022)</h1>; <h2>Dependencies</h2>; <p>5.0.0 b1</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10164"">#10164</a>: Support <code>Docutils 0.18</code>_. Patch by Adam Turner.</li>; </ul>; <p>.. _Docutils 0.18: <a href=""https://docutils.sourceforge.io/RELEASE-NOTES.html#release-0-18-2021-10-26"">https://docutils.sourceforge.io/RELEASE-NOTES.html#release-0-18-2021-10-26</a></p>; <h2>Incompatible changes</h2>; <p>5.0.0 b1</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/907d27dc6506c542c11a7dd16b560eb4be7da5fc""><code>907d27d</code></a> Bump to 5.0.2 final</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/ed6970311349e54ceebe24ede255378fcd9d94e5""><code>ed69703</code></a> Update CHANGES for PR <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10535"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11925:3316,Release,Release,3316,https://hail.is,https://github.com/hail-is/hail/pull/11925,1,['Release'],['Release']
Deployability,"TODO. Goal is all non-stretch items done by late tomorrow night/early Friday morning. Friday - Sunday testing, Cotton takes a closer look on Monday. - [x] No SQL; store user / svc / token labels (all things that need to be validated before redirect); - [x] Websockets; - [x] Service, pod definitions, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215:310,update,updates,310,https://hail.is,https://github.com/hail-is/hail/pull/5215,3,"['Deploy', 'update']","['Deploy', 'updates']"
Deployability,"Table.scala:369); 	at is.hail.table.Table.aggregateJSON(Table.scala:364); 	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-414f3f183bd5; Error summary: RuntimeException: Class file too large!; ```; Code was:; ```; cutoff = 10. agg_expr = {; 'downsampling': hl.agg.collect(ht.downsamplings)[0]; }; locations = list(zip(('syn', 'mis', 'lof'), ('', '', '_classic_hc'))); agg_expr.update({; f'median_expected_{var}_{pop}': [hl.median(hl.agg.collect(ht[f'exp_{var}_{pop}{var_loc}'][i])) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'median_observed_{var}_{pop}': [hl.median(hl.agg.collect(ht[f'obs_{var}_{pop}{var_loc}'][i])) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'mean_expected_{var}_{pop}': [hl.agg.mean(ht[f'exp_{var}_{pop}{var_loc}'][i]) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'mean_observed_{var}_{pop}': [hl.agg.mean(ht[f'obs_{var}_{pop}{var_loc}'][i]) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'fraction_expected_{var}_{pop}': [hl.agg.fraction(ht[f'exp_{var}_{pop}{var_loc}'][i] > cutoff) for i in range(length)]; for length, pop in pop_lengths; for var, var_loc in locations; }); agg_expr.update({; f'fraction_obs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4516:2066,update,update,2066,https://hail.is,https://github.com/hail-is/hail/issues/4516,1,['update'],['update']
Deployability,"Tested as follows from a clean environment.; 1. Build the jars and wheel in release mode:; ```bash; HAIL_RELEASE_MODE=1 make -C hail wheel; ```. 2. Dry-run the upload-artifacts target and inspect output; ```bash; cloud_base is set to ""gs://hail-common/hailctl/dataproc/0.2.129"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:76,release,release,76,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,3,"['deploy', 'release']","['deploy', 'release']"
Deployability,"Tested by deploying it, I know it works. No one else is awake.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8710#issuecomment-624411877:10,deploy,deploying,10,https://hail.is,https://github.com/hail-is/hail/pull/8710#issuecomment-624411877,1,['deploy'],['deploying']
Deployability,"Tested by installing wheel in clean venv, importing and running basic hail commands",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14631#issuecomment-2243375019:10,install,installing,10,https://hail.is,https://github.com/hail-is/hail/pull/14631#issuecomment-2243375019,1,['install'],['installing']
Deployability,Tested by running `hailctl dev deploy -b hail-is/hail:main -s git_make_bash_image` against a CI in my namespace and seeing that it succeeded and pushed an image [here](https://console.cloud.google.com/artifacts/docker/hail-vdc/us/hail/ci-intermediate?project=hail-vdc),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211#issuecomment-1253928151:31,deploy,deploy,31,https://hail.is,https://github.com/hail-is/hail/pull/12211#issuecomment-1253928151,1,['deploy'],['deploy']
Deployability,Tested in a dev deploy'd load test that # of add_attempt_resources queries == # of jobs instead of double as you can currently observe in default.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12461:16,deploy,deploy,16,https://hail.is,https://github.com/hail-is/hail/pull/12461,1,['deploy'],['deploy']
Deployability,"Tested on https://internal.hail.is/chrisl/auth/user and comparing with previous, non-updated behavior",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14636#issuecomment-2248848361:85,update,updated,85,https://hail.is,https://github.com/hail-is/hail/pull/14636#issuecomment-2248848361,1,['update'],['updated']
Deployability,Tested the following in GCP and Azure:; 1. `hailctl auth login` on the current release (pre-OAuth2) and `hailctl auth logout` on `main`; 2. Both login/logout on `main`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13578:79,release,release,79,https://hail.is,https://github.com/hail-is/hail/pull/13578,1,['release'],['release']
Deployability,Tested this manually to make sure that the build-worker instance ran to completion (it fails on `apt-get update` currently),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10411:105,update,update,105,https://hail.is,https://github.com/hail-is/hail/pull/10411,1,['update'],['update']
Deployability,"Tested this out by running it locally and changing `aiohttp-session` in `docker/requirements.txt` from `2.70` to `2.8.0` and got the following:. ```; + pip-compile --quiet docker/requirements.txt docker/pinned-requirements.txt --output-file=new-pinned.txt; Could not find a version that matches aiohttp-session==2.7.0,==2.8.0 (from -r docker/requirements.txt (line 4)); Tried: 0.0.1, 0.0.1, 0.1.0, 0.1.0, 0.1.1, 0.1.1, 0.1.2, 0.1.2, 0.2.0, 0.2.0, 0.3.0, 0.3.0, 0.4.0, 0.4.0, 0.5.0, 0.5.0, 0.7.0, 0.7.0, 0.7.1, 0.7.1, 0.8.0, 0.8.0, 1.0.0, 1.0.0, 1.0.1, 1.0.1, 1.1.0, 1.1.0, 1.2.0, 1.2.0, 1.2.1, 1.2.1, 2.0.0, 2.0.0, 2.0.1, 2.0.1, 2.1.0, 2.1.0, 2.2.0, 2.2.0, 2.3.0, 2.3.0, 2.4.0, 2.4.0, 2.5.1, 2.5.1, 2.6.0, 2.6.0, 2.7.0, 2.7.0, 2.8.0, 2.8.0, 2.9.0, 2.9.0, 2.10.0, 2.10.0, 2.11.0, 2.11.0; Skipped pre-versions: 2.10.0a0, 2.10.0a0; There are incompatible versions in the resolved dependencies:; aiohttp-session==2.7.0 (from -r docker/pinned-requirements.txt (line 20)); aiohttp-session==2.8.0 (from -r docker/requirements.txt (line 4)); ```. and another example where I added an unrelated pip dependency in the requirements but didn't update the lock file",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11842#issuecomment-1131987651:1132,update,update,1132,https://hail.is,https://github.com/hail-is/hail/pull/11842#issuecomment-1131987651,1,['update'],['update']
Deployability,"Tested using code from [zulip]. After this change, the same pipeline went from 135 MiB peak usage per; partition, to 808 KiB. [zulip]: https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query.200.2E2.20support/topic/Hail.20off-heap.20memory/near/270245855",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11312:60,pipeline,pipeline,60,https://hail.is,https://github.com/hail-is/hail/pull/11312,1,['pipeline'],['pipeline']
Deployability,"Tested using modified code from Lindsay Liang on zulip:. ```python; import hail as hl; hl.init(log='hail.log'); rg37 = hl.get_reference('GRCh37'). rg38 = hl.get_reference('GRCh38'); rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38); gnomad_ht = hl.read_table('gs://gcp-public-data--gnomad/release/2.1.1/ht/exomes/gnomad.exomes.r2.1.1.sites.ht'). gnomad_ht = gnomad_ht.annotate(new_locus=hl.liftover(gnomad_ht.locus, 'GRCh38')); gnomad_ht = gnomad_ht.key_by(locus=gnomad_ht.new_locus, alleles=gnomad_ht.alleles); mt = hl.balding_nichols_model(3, 100, 10_000, reference_genome='GRCh38'); mt = mt.annotate_entries(AD=hl.zeros(hl.len(mt.alleles))); mt = mt.annotate_rows(gnomad_non_neuro_AF =; gnomad_ht.index(mt.row_key).freq[hl.eval(gnomad_ht.freq_index_dict[""non_neuro""])].AF); mt = mt.annotate_entries(pAB = hl.or_missing(mt.GT.is_het(),; hl.binom_test(mt.AD[1], hl.sum(mt.AD), 0.5, 'two-sided'))); mt._force_count_rows(); ```. This faithfully replicated the issue and went from pretty much every task failing at least once as they read bad state to no tasks failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10888#issuecomment-924123558:325,release,release,325,https://hail.is,https://github.com/hail-is/hail/pull/10888#issuecomment-924123558,1,['release'],['release']
Deployability,Tested with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8194#issuecomment-592560456:16,deploy,deploy,16,https://hail.is,https://github.com/hail-is/hail/pull/8194#issuecomment-592560456,1,['deploy'],['deploy']
Deployability,Tests are all passing. One question that remains is should we put a manifest file with the current Hail configurations somewhere in hail-common?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1514682815:104,configurat,configurations,104,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1514682815,1,['configurat'],['configurations']
Deployability,Tests passed rolling up,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9852#issuecomment-754182568:13,rolling,rolling,13,https://hail.is,https://github.com/hail-is/hail/pull/9852#issuecomment-754182568,1,['rolling'],['rolling']
Deployability,Tests with dev deploy worked.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7909#issuecomment-575794175:15,deploy,deploy,15,https://hail.is,https://github.com/hail-is/hail/pull/7909#issuecomment-575794175,1,['deploy'],['deploy']
Deployability,"Thank you :). Could you also make a release soon after this PR is merged, if reasonable? Would be handy to be able to try the package in our setup!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11435#issuecomment-1055996916:36,release,release,36,https://hail.is,https://github.com/hail-is/hail/pull/11435#issuecomment-1055996916,1,['release'],['release']
Deployability,"Thank you all for another round of detailed critique!. OK, I think the only remaining critical fix is to hard-code a mainclass. This is a wee bit complicated because I need to multiplex the ServiceBackendSocketAPI2 and the Worker. I hope to do this tomorrow AM. I'll then dismiss reviews. I also have a list of todos generated by this process which will feedback into some master QoB doc that integrates the two teams necessary todos.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194#issuecomment-1033173934:393,integrat,integrates,393,https://hail.is,https://github.com/hail-is/hail/pull/11194#issuecomment-1033173934,2,['integrat'],['integrates']
Deployability,"Thank you for getting back to me. I was using the solution provided by aws (https://github.com/awslabs/genomics-tertiary-analysis-and-data-lakes-using-aws-glue-and-amazon-athena/blob/master/source/GenomicsAnalysisCode/buildhail_buildspec.yml) also the main page for reference (https://docs.aws.amazon.com/solutions/latest/genomics-tertiary-analysis-and-data-lakes-using-aws-glue-and-amazon-athena/welcome.html). In order to use the latest Hail (because we have vcf format 4.3 which is not supported in Hail 0.1), we changed it to ; ```. echo 'Installing pre-reqs'; yum install -y g++ cmake git; yum install -y lz4; yum install -y lz4-devel; git clone $HAIL_REPO; cd hail/hail && git fetch && git checkout main; ./gradlew clean; make install HAIL_COMPILE_NATIVES=1; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.13 SPARK_VERSION=3.1.1; cd build; pip download decorator==4.2.1; aws s3 cp decorator-4.2.1-py2.py3-none-any.whl s3://${RESOURCES_BUCKET}/artifacts/decorator.zip; aws s3 cp distributions/hail-python.zip s3://${RESOURCES_BUCKET}/artifacts/; aws s3 cp libs/hail-all-spark.jar s3://${RESOURCES_BUCKET}/artifacts/; ``` . What else I should change in order to deploy this solution successfully? . Thank you for the help!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10844#issuecomment-914469181:543,Install,Installing,543,https://hail.is,https://github.com/hail-is/hail/issues/10844#issuecomment-914469181,7,"['Install', 'deploy', 'install']","['Installing', 'deploy', 'install']"
Deployability,"Thank you for taking a look!. > 1. I'm not opposed to adding tokens to the batches_n_jobs_in_complete_states table, but I'm not sure why this is related to the other pieces of this PR / job groups. Aren't tokens purely a performance optimization?. The issue is that I started with #13475 and after your insightful comment about keeping the batches and job groups tables in sync, I realized that rather than using the batch_after_update trigger to keep the job groups and batches table states identical, we should just go ahead and directly add a double update to the job groups and batches table wherever a batches update occurs in our current code base. Unfortunately, I got stuck with the MJC trigger with these lines of code:. ```sql; UPDATE batches_n_jobs_in_complete_states; SET n_completed = (@new_n_completed := n_completed + 1),; n_cancelled = n_cancelled + (new_state = 'Cancelled'),; n_failed = n_failed + (new_state = 'Error' OR new_state = 'Failed'),; n_succeeded = n_succeeded + (new_state != 'Cancelled' AND new_state != 'Error' AND new_state != 'Failed'); WHERE id = in_batch_id;. # Grabbing an exclusive lock on batches here could deadlock,; # but this IF should only execute for the last job; IF @new_n_completed = total_jobs_in_batch THEN; UPDATE batches; SET time_completed = new_timestamp,; `state` = 'complete'; WHERE id = in_batch_id;; END IF;; ```. We can do the double update in the IF statement to both the job groups table for job_group_id = 0 and for the batches table in #13475. However, this SQL code / approach will eventually need to be changed for the full job group implementation. I don't know how to compute `@new_n_completed` grouped by job group and then `total_jobs_in_batch` would need to be computed per job group as well. I don't think you can use for loops in SQL. It might be possible to do this with temporary tables, but I thought it would be better to take a detour from adding job groups and get rid of how we currently do the batch update in MJC to allo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:553,update,update,553,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,3,"['UPDATE', 'update']","['UPDATE', 'update']"
Deployability,Thank you for the clear and easy-to-follow instructions! I've rebased my commit and applied your patch. What do I need to do now?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/753#issuecomment-251409770:97,patch,patch,97,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-251409770,2,['patch'],['patch']
Deployability,Thanks ! ; I saw that the fix is merged on #13806 v0.2.125 !; I am able to install hail and run it using command line. I do have an issue with jupyter through... working on it,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1782294182:75,install,install,75,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1782294182,1,['install'],['install']
Deployability,Thanks @danking. A feature like this would let us clean up some parts of our pipelines significantly. Our current workarounds leave a bad taste in the mouth whenever I see them.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14354#issuecomment-1965395083:77,pipeline,pipelines,77,https://hail.is,https://github.com/hail-is/hail/issues/14354#issuecomment-1965395083,1,['pipeline'],['pipelines']
Deployability,"Thanks @jmarshall for bringing this to our attention. It looks like while we updated the upper bound here, we did not update our fully-pinned requirements which we use to test in CI, so it did not catch this incompatibility. That being said, I don't think that was necessarily a mistake, because by testing our minimum-compatible-version we make sure not to introduce incompatibilities on that end of the spectrum either.. I think I don't see a good way in which we can confidently support more than one major version of a dependency at a given point in time. Even without the bokeh issue, there could easily be places in our codebase where we use pandas 1.x functionality that has been removed in 2.0. @danking thoughts on moving the pandas pin to >= the 2.x.x version that we test with and <3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1520406276:77,update,updated,77,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1520406276,2,['update'],"['update', 'updated']"
Deployability,"Thanks @tomwhite, that's great! Unfortunately, we have a second local cluster with another distribution on Spark 1.5 that won't get upgrade until early 2017 at the earliest.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1124#issuecomment-263332983:132,upgrade,upgrade,132,https://hail.is,https://github.com/hail-is/hail/pull/1124#issuecomment-263332983,1,['upgrade'],['upgrade']
Deployability,"Thanks Shuli, and apologies for the delay! I've taken your changes to the parameters and added some additional fixes in #2377 directly on @johnc1231 branch, which should be reviewed and merged to master this week. I'll be in touch once that's in, and from there, you can make future PRs against master for our review to improve/update Nirvana in Hail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2300#issuecomment-340895757:328,update,update,328,https://hail.is,https://github.com/hail-is/hail/pull/2300#issuecomment-340895757,1,['update'],['update']
Deployability,Thanks for doing this! Should make things easier when dataproc actually releases a Spark 3 version,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9524#issuecomment-702190052:72,release,releases,72,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-702190052,1,['release'],['releases']
Deployability,"Thanks for figuring this out! For dataproc, the startup script runs the code below the first time the data is used to generate the index for GRCh37 only. I ran the same dummy VEP command when I generated the QoB data for GRCh37. But we don't do this in GRCh38 on dataproc, so I didn't run this command for QoB as well. The fix is to add something similar as below to the GRCh38 dataproc script and then independently fix the QoB data for GRCh38. ```; # Run VEP on the 1-variant VCF to create fasta.index file -- caution do not make fasta.index file writeable afterwards!; cat /vep_data/loftee_data/1var.vcf | docker run -i -v /vep_data:/root/.vep \; ${VEP_DOCKER_IMAGE} \; perl /vep/ensembl-tools-release-85/scripts/variant_effect_predictor/variant_effect_predictor.pl \; --format vcf \; --json \; --everything \; --allele_number \; --no_stats \; --cache --offline \; --minimal \; --assembly ${ASSEMBLY} \; -o STDOUT; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1832737456:697,release,release-,697,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1832737456,1,['release'],['release-']
Deployability,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:390,install,installing,390,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040,5,['install'],"['install', 'installation', 'installed', 'installing', 'installs']"
Deployability,"Thanks for merging. The wild goose chase was amusing in retrospect!. I was considering adding another commit with something like. ```patch; +++ b/hail/python/hailtop/batch/resource.py; @@ -49,7 +49,7 @@ class ResourceFile(Resource, str):; ; def __init__(self, value: Optional[str]):; super().__init__(); - assert value is None or isinstance(value, str); + assert value is None or isinstance(value, str), f'{type(value).__name__} ({value!r}) is not str'; ```. However this iterating is probably the cause of most of the unexpected values/types here, so with the `__iter__` definitions added it doesn't matter too much. (Most of the other asserts are things like `assert value is not None` so if you see it you can tell what's happened without needing to print out `value`.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14390#issuecomment-1977643581:133,patch,patch,133,https://hail.is,https://github.com/hail-is/hail/pull/14390#issuecomment-1977643581,1,['patch'],['patch']
Deployability,Thanks for the comments and the rewording in the docs @danking ! I just pushed the updated scala files.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/886#issuecomment-251185250:83,update,updated,83,https://hail.is,https://github.com/hail-is/hail/pull/886#issuecomment-251185250,1,['update'],['updated']
Deployability,"Thanks for the report, @JacobBayer! I don't believe anyone on our team uses Spyder unfortunately, but I don't think this is a Hail issue. According to [this thread](https://community.developers.refinitiv.com/questions/88895/spyder-515-erroreikon-data-api.html?childToView=89408#answer-89408) a Spyder upgrade might resolve the issue if you're on an old version?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11758#issuecomment-1099731309:301,upgrade,upgrade,301,https://hail.is,https://github.com/hail-is/hail/issues/11758#issuecomment-1099731309,1,['upgrade'],['upgrade']
Deployability,"Thanks for the review -- that's a much better approach! I've made the change. Happy to say that the patched version has just ingested a 46 million x 1200 VCF without a hitch and in just over an hour, and I'm very much looking forward to seeing what hail can do with the data tomorrow -- thanks for creating such a powerful system!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1066#issuecomment-258820833:100,patch,patched,100,https://hail.is,https://github.com/hail-is/hail/pull/1066#issuecomment-258820833,1,['patch'],['patched']
Deployability,Thanks for the update. Let me know if there is anything I can do to help with the review.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14326#issuecomment-1964827601:15,update,update,15,https://hail.is,https://github.com/hail-is/hail/pull/14326#issuecomment-1964827601,1,['update'],['update']
Deployability,Thanks! Fix worked (I just rolled a custom jar) so don't worry about integrating immediately on my account.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2456#issuecomment-345872854:69,integrat,integrating,69,https://hail.is,https://github.com/hail-is/hail/pull/2456#issuecomment-345872854,1,['integrat'],['integrating']
Deployability,Thanks! Moving over to the Xcode cc worked. **cc --version**; Apple LLVM version 8.0.0 (clang-800.0.42.1); Target: x86_64-apple-darwin16.3.0; Thread model: posix; InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274#issuecomment-274296144:163,Install,InstalledDir,163,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274296144,1,['Install'],['InstalledDir']
Deployability,"Thanks, @danking! Looks like I broke auth deploy, investigating.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9713#issuecomment-783498605:42,deploy,deploy,42,https://hail.is,https://github.com/hail-is/hail/pull/9713#issuecomment-783498605,1,['deploy'],['deploy']
Deployability,"Thanks,. The `module-info.class` thing is incredibly unlikely to break any user. I only discovered it because it broke `jdeps` when I was testing generating a bundled JRE. None of this directly impacts me or established users of Hail in my group, but I have seen Java version be the single biggest pain point for new users wanting to install and try Hail for the first time, which is why I posted this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14433#issuecomment-2030374619:334,install,install,334,https://hail.is,https://github.com/hail-is/hail/issues/14433#issuecomment-2030374619,1,['install'],['install']
Deployability,That is indeed concerning. Can you share the full pipeline back to the read of `mt` and the creation of `sample_list`? Can you also confirm this bad behavior exists in the latest 0.2.113?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12912#issuecomment-1517847031:50,pipeline,pipeline,50,https://hail.is,https://github.com/hail-is/hail/issues/12912#issuecomment-1517847031,1,['pipeline'],['pipeline']
Deployability,"That said, fixing dataproc with minimal changes seems best to me. If/when we upgrade to a newer VEP version we can change to a more sensible structure then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14071#issuecomment-1846195135:77,upgrade,upgrade,77,https://hail.is,https://github.com/hail-is/hail/pull/14071#issuecomment-1846195135,1,['upgrade'],['upgrade']
Deployability,"That will get updated once this PR goes in, while the docs and the client users use (batch not pipeline anymore) will be dependent on upgrading to the next PIP release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8453#issuecomment-612087215:14,update,updated,14,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-612087215,3,"['pipeline', 'release', 'update']","['pipeline', 'release', 'updated']"
Deployability,"That's a legitimate concern. We don't currently do this for `datasets.json` / the annotation database. For now let's leave it as it is. If the files move the user always has the option of fixing their configuration by explicitly specifying a config. Having some sort of remote configuration seems valuable, but this PR is large already. Whatever solution we come up with for remote configuration should also support the annotation database case. Let's not worry about it for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1515016056:201,configurat,configuration,201,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1515016056,3,['configurat'],['configuration']
Deployability,"That's what the literal was doing. The problem is the result is being used in two completely separate pipelines---the variants and reference. The right thing is probably to collect with `localize=False`, and use the result to annotate globals on both sides, but we still don't support referencing the local environment in MapGlobals.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13749#issuecomment-1743389871:102,pipeline,pipelines---the,102,https://hail.is,https://github.com/hail-is/hail/pull/13749#issuecomment-1743389871,1,['pipeline'],['pipelines---the']
Deployability,"The 'build' docs page implies that the only requirement for running hail is Gradle. However, I've just tried to build hail on Debian Jessie and Ubuntu 16.04, and both failed in different ways. On Jessie, I was able to figure out that the version of Gradle was too old. On Ubuntu 16.04, I get. ```; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED; ```. A quick Google around doesn't reveal any obvious answers to this. What version of Gradle is needed? Is Scala a prerequisite? It would be very useful to provide detailed instructions on how to build hail from scratch on a fresh installation of some Linux distribution.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594:435,configurat,configuration,435,https://hail.is,https://github.com/hail-is/hail/issues/594,2,"['configurat', 'install']","['configuration', 'installation']"
Deployability,"The ATGU intranet service is home for some tools we're going to build help support ATGU operations. After talking to the administrators, we started a simple tool for the admins to curate resources for members of the group. We've also talked about things related to personnel and financial and grant management. Although this will likely be a slightly different resource, we're talking to the PMs about tools we can build to help pulling together large-scale datasets like gnomAD. The complexity of doing this is becoming a blocker for producing such datasets. We're Hail. Whatever we build is part of Hail. Things that we own and operate and deploy together live in our monorepo.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9684#issuecomment-723166642:642,deploy,deploy,642,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-723166642,2,['deploy'],['deploy']
Deployability,"The CI tests were failing due to not enough disk space on the workers. I increased the size to 20 GB, but this requires a migration. I'll need to setup a VM with the ability to deploy to have this new image pre-built. The last time I did a migration from the VM, it took a long time to deploy despite pre-caching the image, so I'll want to build a new VM with more cores to see if that helps. We should wait until after Konrad's jobs are done to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8440#issuecomment-609845317:177,deploy,deploy,177,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-609845317,2,['deploy'],['deploy']
Deployability,"The CSS makes some strong statements about how the web browser should display it. I; removed all these statements which lets the web browser choose if it should wrap some; in-cell text, truncate the table, or expand the table. I fixed some bad formatting and removed tabs (😱) from some pages. Check my dev deploy: https://internal.hail.is/dking/batch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8805:306,deploy,deploy,306,https://hail.is,https://github.com/hail-is/hail/pull/8805,1,['deploy'],['deploy']
Deployability,"The CountMentions implementation was totally wrong -- it hadn't been updated to use the binding environment. I think most of the usages of Mentions were technically correct as implemented, but this is much safer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5740:69,update,updated,69,https://hail.is,https://github.com/hail-is/hail/pull/5740,1,['update'],['updated']
Deployability,"The GENCODE GTF files associated with gnomAD annotations are occasionally useful. For example, they are needed to get the gene and transcript version numbers for VEP annotations for Ensembl transcripts. Or they can be used to get an interval for a particular gene or transcript, which can then be used to efficiently filter the variants Hail tables. However, the files hosted by GENCODE are not block gzipped. Thus, they are slow to import into Hail because the import cannot be parallelized. To make working with this data in Hail easier, it would be nice if the relevant versions of GENCODE were available in [Hail's Datasets collection](https://hail.is/docs/0.2/datasets.html). It looks like GENCODE v19 and v31 are already there. https://www.gencodegenes.org/human/releases.html; https://gnomad.broadinstitute.org/help/what-version-of-gencode-was-used-to-annotate-variants. This is effectively the same request as broadinstitute/gnomad_production#1042.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11899:769,release,releases,769,https://hail.is,https://github.com/hail-is/hail/issues/11899,1,['release'],['releases']
Deployability,"The Hail python package is not a pure-Python package. Installing it from source requires building a JAR file. We have instructions [here](https://hail.is/docs/0.2/getting_started_developing.html), but I recommend waiting for the next PyPI release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12844#issuecomment-1502218484:54,Install,Installing,54,https://hail.is,https://github.com/hail-is/hail/issues/12844#issuecomment-1502218484,2,"['Install', 'release']","['Installing', 'release']"
Deployability,"The LDMatrix apply method calls collect on more or less the entire set of variants twice. Noticed this taking a lot of time in testing lmmreg pipeline, think it's worth trying to eliminate one if possible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2009:142,pipeline,pipeline,142,https://hail.is,https://github.com/hail-is/hail/issues/2009,1,['pipeline'],['pipeline']
Deployability,"The Makefile was hardcoded for GCP. Grafana is still currently able to be deployed through CI, this just changes the Makefile so it's not hardcoded to `gcp` and moves the gcp project from a Makefile-set value to one set through kubernetes so the Makefile is more cloud agnostic (doesn't reference a gcp project).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11176:74,deploy,deployed,74,https://hail.is,https://github.com/hail-is/hail/pull/11176,1,['deploy'],['deployed']
Deployability,"The Makefiles grab the `docker_prefix` from the global-config each time they are run so once we make the change in the global-config nothing else needs to change. Actually, I think this entire PR is not even needed. When CI does a deploy, the changes it applies to Kubernetes include fully qualified image names, e.g. `gcr.io/hail-vdc/batch:asdf1234`. If we were to swap out the `docker_prefix` global-config variable, CI would start to create new images that are pushed to the new repository (it would kill the cache for a single build but whatever), but the existing images would still exist and be undisrupted. The only images that need to exist in the new container registry when the switch is made are the images that we push on bootstrap which I am going to do manually anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211#issuecomment-1259517486:231,deploy,deploy,231,https://hail.is,https://github.com/hail-is/hail/pull/12211#issuecomment-1259517486,1,['deploy'],['deploy']
Deployability,"The Red Hat folks appear to have restored the package, so we could remove this workaround. However, they're not going to maintain it so we will either have to build from source or ultimately upgrade.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11859#issuecomment-1138065476:191,upgrade,upgrade,191,https://hail.is,https://github.com/hail-is/hail/pull/11859#issuecomment-1138065476,1,['upgrade'],['upgrade']
Deployability,"The SHA's are changing a bit quicker than I want to update them. I'll update when it stabilizes, but the titles are still correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10072#issuecomment-782556470:52,update,update,52,https://hail.is,https://github.com/hail-is/hail/pull/10072#issuecomment-782556470,2,['update'],['update']
Deployability,"The Zstandard version is not changing. The zstd-jni library, which wraps Zstandard and provides some interoperation with java.nio, has released 9 times since 1.5.5-2. They do not publish a changelog, but I scanned through their commits. There were some potentially relevant bug fixes:. 1. When using array-backed ByteBuffers, zstd-jni reads the wrong data if the arrayOffset is not zero. https://github.com/luben/zstd-jni/commit/355b8511a2967d097a619047a579930cac2ccd9d. 2. Perhaps a slightly faster path for array-backed ByteBuffers. https://github.com/luben/zstd-jni/commit/100c434dfcec17a865ca2c2b844afe1046ce1b10. 3. Possibly faster buffer pool. https://github.com/luben/zstd-jni/commit/2b6c3b75012dec44f8fd2dd56dd97eea0d62f19c. 4. Removed a double free during compression. https://github.com/luben/zstd-jni/commit/b2ad3834439375b12b0fd0c0b80788a2fe94f06b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14081:135,release,released,135,https://hail.is,https://github.com/hail-is/hail/pull/14081,1,['release'],['released']
Deployability,"The [Hail CI Build Configuration](https://ci.hail.is/admin/editBuildRunners.html?id=buildType:HailSourceCode_HailCi) (admin login required) now runs `gradle clean compileTestScala` against three spark versions: `1.6.2`, `1.5.2`, and `1.6.0-cdh5.7.2`. If all of those succeed, it runs `gradle clean test createDocs` against the default spark version in the gradle script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/742#issuecomment-245033280:19,Configurat,Configuration,19,https://hail.is,https://github.com/hail-is/hail/issues/742#issuecomment-245033280,1,['Configurat'],['Configuration']
Deployability,"The [TextInputFormat](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html) class clearly comes from hadoop. It's no longer in the location from which we import it. We must get it from some other dependency. OK. So, before my simplification of build.gradle, we used a configuration called `compile` and another one called `testCompile`. [Neither of those exist in modern gradle, apparently](https://docs.gradle.org/current/userguide/java_library_plugin.html#sec:java_library_configurations_graph). I found a side-note about the `compile` configuration [here](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:java_dependency_management_overview) (search for ""compile""):. > **Why no compile configuration?**; > The Java Library Plugin has historically used the compile configuration for dependencies that are required to both compile and run a project’s production code. It is now deprecated, and will issue warnings when used, because it doesn’t distinguish between dependencies that impact the public API of a Java library project and those that don’t. You can learn more about the importance of this distinction in [Building Java libraries](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:building_java_libraries). OK, so, we used to just dump everything into our runtime dependencies. I changed it so that we have three kinds of dependencies:; 1. `shadow`: these are provided by Dataproc/QoB at run-time. They are not in any JAR. They are not on the `testRuntimeClasspath` or `runtimeClasspath`. They are on the `testCompileClasspath` because I [explicitly requested](https://github.com/hail-is/hail/blob/main/hail/build.gradle#L98) that `testCompileOnly` bring in our `shadow` dependencies.; 2. `implementation`: these are included in all class paths and in shadow JARs (but not ""thin"" jars generated by `./gradlew jar`).; 3. `testImplementation`: these are included in test class paths and in shadow JARs. Our t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741:303,configurat,configuration,303,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741,8,['configurat'],['configuration']
Deployability,"The `DeployConfig.service_ns` doesn't really do anything, we always use the `_default_namespace`. This is maybe from an earlier age where some services might live in different namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13959:5,Deploy,DeployConfig,5,https://hail.is,https://github.com/hail-is/hail/pull/13959,1,['Deploy'],['DeployConfig']
Deployability,"The `HailContext.getOrCreate` method seems to have been broken in #5512. This patch fixes the issue and adds a regression test so that it won't break again. Since this test must add create a new Hail context, I had to add a gradle task that runs every suite in a separate JVM. I'm not a gradle expert, so if there's a simpler way to accomplish this execution mode, feel free to suggest :).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5871:78,patch,patch,78,https://hail.is,https://github.com/hail-is/hail/pull/5871,1,['patch'],['patch']
Deployability,"The `assert(_ptype2 == null)` check in InferPType is breaking here on; certain complex pipelines in a way I don't want to debug. There's no IR sharing within the IR (see utility I added), but there; must be subtrees that are inferred multiple times in different Compile; calls. This is a safe stop-gap.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8031:87,pipeline,pipelines,87,https://hail.is,https://github.com/hail-is/hail/pull/8031,1,['pipeline'],['pipelines']
Deployability,"The `delete_azure_batch_instances` step is failing on various PRs with the error `jq: command not found`. This appears to be because we do not pin the version for the `mcr.microsoft.com/azure-cli` image, and while that image was previously based on the Alpine image, [now it is based on the Azure Linux image](https://learn.microsoft.com/en-us/cli/azure/run-azure-cli-docker), and does not appear to have `jq` (or `kubectl`) preinstalled on it. This change updates the commands run in the `azure-cli` container for this CI step to install `jq` and `kubectl` via `curl` before running the relevant commands. The `curl` commands were tested locally by running `docker run -it mcr.microsoft.com/azure-cli` and trying them out in the image's shell. This change also adds the installation commands in the other place where this image is used (when cleaning up from `buildImage2` jobs that are run in Azure).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14677:457,update,updates,457,https://hail.is,https://github.com/hail-is/hail/pull/14677,3,"['install', 'update']","['install', 'installation', 'updates']"
Deployability,The `install-wheel` Make target was renamed to `install` in hail-is/hail@346fb67aa5f943c42981025823876272ee222183 but the documentation for building Hail still refers to the old name.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7133:5,install,install-wheel,5,https://hail.is,https://github.com/hail-is/hail/pull/7133,2,['install'],"['install', 'install-wheel']"
Deployability,"The `notebook` and `workshop` services were incorrectly pointing to the python notebook app instead of its nginx proxy, which handles proxying to notebook workers. In #10250 I added back https to the notebook python app and accidentally changed the nginx -> notebook worker connection `https`, where it should not be. These combined meant that notebook was unable to proxy to notebook workers. I deployed this into default and verified that I can get to jupyter, and also run scale tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10292:396,deploy,deployed,396,https://hail.is,https://github.com/hail-is/hail/pull/10292,1,['deploy'],['deployed']
Deployability,"The `pre-commit` hook is a little sticky because `pre-commit` installs each tool in its own isolated virtual env, which won't have the dependencies unless we tell `pre-commit` to also install all of our pinned dependencies into the pyright virtualenv. We can configure pyright to use a different virtualenv for all its dependencies, but that would require each developer specifying the name of their virtual environment in `pyproject.toml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13437#issuecomment-1681248432:62,install,installs,62,https://hail.is,https://github.com/hail-is/hail/pull/13437#issuecomment-1681248432,2,['install'],"['install', 'installs']"
Deployability,The `rename_job_groups_cancelled_column`sql file renames the `job_groups_cancelled.id` column to `job_groups_cancelled.batch_id`. The sql also updates all constraints that reference the original column to reflect the new column name. I have reviewed other tables and found no foreign keys referencing the `job_groups_cancelled` table. All queries that previously used `job_groups_cancelled.id` have been updated to reference `job_groups_cancelled.batch_id` accordingly. Resolve [#14646 ](https://github.com/hail-is/hail/issues/14646),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672:143,update,updates,143,https://hail.is,https://github.com/hail-is/hail/pull/14672,2,['update'],"['updated', 'updates']"
Deployability,The `wait` directive tells CI to wait for the service to come up before; continuing with deployment. If the service fails to come up (including; failing to respond to the readiness checks) then CI will fail the build.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8506:89,deploy,deployment,89,https://hail.is,https://github.com/hail-is/hail/pull/8506,1,['deploy'],['deployment']
Deployability,The actual release PR didn't update the patch version so didn't do a release.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14304:11,release,release,11,https://hail.is,https://github.com/hail-is/hail/pull/14304,4,"['patch', 'release', 'update']","['patch', 'release', 'update']"
Deployability,"The assumption of the bounded gather operations is that threads of; control hold the bounding semaphore. That means, when the bounded; gather operations should be called from a thread of control which; holds the semaphore and when those operations need to block internally; (by calling wait or gather, say) it should release the semaphore. I; added a ""WithoutSemaphore"" semaphore release manager to implement this; pattern and use it in the bounded gather operations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10561:317,release,release,317,https://hail.is,https://github.com/hail-is/hail/pull/10561,2,['release'],['release']
Deployability,"The breeze version packaged with Spark [was changed to 0.12](https://issues.apache.org/jira/browse/SPARK-16494) when Spark upgraded from 2.0.2 to 2.1.0. . According to [the PR](https://github.com/apache/spark/pull/14150/files#diff-06b6ad3483185a20d3095743faa5e4f0L15) linked from that JIRA issue, the breeze packaged with Spark 2.0.2 was version 0.11.2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281757910:123,upgrade,upgraded,123,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281757910,1,['upgrade'],['upgraded']
Deployability,The cancel endpoint must also be updated.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6556#issuecomment-509397642:33,update,updated,33,https://hail.is,https://github.com/hail-is/hail/issues/6556#issuecomment-509397642,1,['update'],['updated']
Deployability,"The capacity on the cache is pretty arbitrary, but given that bunches are going to get churned through very quickly and then never used again, it seemed nice to have the assertion that every layer of the cache is always small and shouldn't be an issue to search through in a blocking manner. I tested this with a dev-deployed load-test and observed the number of `get_token_start_id` queries drop from O(jobs) to ~4 per second at max throughput. No difference in profiling, this is just an attempt to reduce the number of queries we're hitting the database with.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12023:317,deploy,deployed,317,https://hail.is,https://github.com/hail-is/hail/pull/12023,1,['deploy'],['deployed']
Deployability,"The changelog for [6.0.0](https://github.com/johnrengelman/shadow/releases/tag/6.0.0) claims performance improvements. In practice,; I save maybe a few second on the `shadowJar` step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10263:66,release,releases,66,https://hail.is,https://github.com/hail-is/hail/pull/10263,1,['release'],['releases']
Deployability,"The coalesce is necessary right now because Azure has no resources so the result is None rather than 0. Since the cost is always 0, then the tests that test billing limits fail. In addition, the tests that choose whether we select the cheapest machine won't work because all machines cost $0/hr right now due to no billing setup. Instead, we get the first pool alphabetically (highcpu). . The reason for the database upgrade is because we're selecting the first pool alphabetically which is highcpu. The default settings don't leave enough disk space and thus require an external disk. To circumvent this, I just made the worker data disk size the same as in GCP -- 375Gi -- for the highcpu pool.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11087:417,upgrade,upgrade,417,https://hail.is,https://github.com/hail-is/hail/pull/11087,1,['upgrade'],['upgrade']
Deployability,"The compiler is invoking `NormalizeNames` more than once. Really, we should be normalizing IR names at most once, then generating unique names for all new names introduced through lowering and compilation. Things to do:; - Normalize names at the start of the lowering + compilation pipeline; - Add a rule to the lowering pipeline to enforce IR names are unique to catch any lowering implementations that break the invariant; - Fix those that misbehave",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13250:282,pipeline,pipeline,282,https://hail.is,https://github.com/hail-is/hail/issues/13250,2,['pipeline'],['pipeline']
Deployability,"The conceptual change here is we want to parameterize all batch related tables to have a new job group ID that I've set to **0** for the root job group. We need to make sure all future inserts / updates into the batches table are propagated to the new job groups table. When we create a batch now, we also create the corresponding entries into the job groups and job group parents tables. I chose the root job group to be 0 as I think conceptually, the client should start numbering job groups at 1 and not know there is a hidden root job group being created under the hood. I'm not wedded to this. I tried to check for all the indices that would be needed in my prototype. It's possible I missed one or two, but it's not a big deal to add it later. I don't think we need to test this on a populated database (dev deploy main, submit jobs, then run the migration), but let me know if you think that would be helpful.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13475:195,update,updates,195,https://hail.is,https://github.com/hail-is/hail/pull/13475,2,"['deploy', 'update']","['deploy', 'updates']"
Deployability,The correct thing is to expose the entrypoint in pipeline/batch_client and add it to the config for a job.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7558:49,pipeline,pipeline,49,https://hail.is,https://github.com/hail-is/hail/issues/7558,1,['pipeline'],['pipeline']
Deployability,"The currently running CI is the old one, it chooses the index file. If you want to test that the index file looks the way you want, you'd need to add a `ci/` test. It's not super straightforward, but `ci/test-in-cluster.sh` starts a CI pointing at a test repo. This eventually calls `ci/test/test-ci.py` which triggers builds and deploys on the test repo. Those builds and deploys should have the expected index file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4551#issuecomment-431458932:330,deploy,deploys,330,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-431458932,2,['deploy'],['deploys']
Deployability,"The date time changes added `-target:jvm-1.8` to build.gradle which quietly; broke SBT. It wasn't a problem for me because I wasn't hacking on Hail until; recently. Moreover, [Ensime](https://ensime.github.io) is dead, so I'm switching over to; bloop. This plugin is necessary for SBT to generate bloop configuration files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8683:303,configurat,configuration,303,https://hail.is,https://github.com/hail-is/hail/pull/8683,1,['configurat'],['configuration']
Deployability,"The default OS for dataproc instances is based on debian8, which uses g++-4.9.x; That has a libstdc++ with an old-ABI implementation of std::list and std::string. To build; a libhail.so which can link against the default libstdc++ on g++-4.x systems, we need to; avoid the use of std::string inside libhail.so (but it's ok to use it in dynamic-generated code,; which will be built with a compiler which matches the libstdc++). This commit introduces a minimal hail::hstring and hail::hstringstream with the necessary; functionality for NativeModule.cpp. Since these don't have a std::hash, I also imported the source code for the (free and uncopyrighted); MurmurHash3, a fast high-quality (but non-crypto) hash function which can give a 128bit hash. ; This simplifies the calculation of the 80bit hash used for module-keys. In addition to using these prebuilt libraries, we also need to get g++ installed on the dataproc; master node, which could be done with ""sudo apt-get install build-essential"". But I'm not yet sure where; that initialization step needs to go.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422:895,install,installed,895,https://hail.is,https://github.com/hail-is/hail/pull/4422,2,['install'],"['install', 'installed']"
Deployability,"The default namespace of the CI job is batch-pods, so; it fails to find the notebook deployment",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4674:85,deploy,deployment,85,https://hail.is,https://github.com/hail-is/hail/pull/4674,1,['deploy'],['deployment']
Deployability,"The default options are all false. If left all false, the user gets an clear error saying that they must include at least one entry field. This forces users to think about what they actually need to import, as it can make a big difference on, say, UKBB until we have better tech. I've updated the docs and tests accordingly. @cseed suggested that we remove BGEN v1.1 support if nobody is reliant on it anymore. I've asked on Slack. So I didn't add more complexity to support these options for BGEN v1.1. Rather this PR requires GT and GP set to true if any file is 1.1 (as explained in docs and error message). If nobody minds, we can rip out BGEN 1.1 and update the docs simultaneously in a subsequent PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2930:285,update,updated,285,https://hail.is,https://github.com/hail-is/hail/pull/2930,2,['update'],"['update', 'updated']"
Deployability,"The deploy job we thought was running may have been deleted for a variety of reasons. It's not an error for that to happen, especially since we're about to accept a different deploy job that was running for the same desired target sha. This can happen if an old CI starts a deploy but is then killed and this CI creates another deploy job before it hears of the old CI's deploy job (and the old one finishes first).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4979:4,deploy,deploy,4,https://hail.is,https://github.com/hail-is/hail/pull/4979,5,['deploy'],['deploy']
Deployability,"The deploy service account was unnecessarily privileged and not actively used. AFAICT, it's used only by this make file to stand up a new hail vdc from scratch. I removed the deploy service account and modified this makefile to create it, use it, and then destroy it when finished. If this is all bitrot, then I suppose it doesn't matter. I tested that these new and modified targets work as expected. What did you use to set up Konrad's project?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8178#issuecomment-599784846:4,deploy,deploy,4,https://hail.is,https://github.com/hail-is/hail/pull/8178#issuecomment-599784846,2,['deploy'],['deploy']
Deployability,The deployed-sha logic seems broken.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5042:4,deploy,deployed-sha,4,https://hail.is,https://github.com/hail-is/hail/issues/5042,1,['deploy'],['deployed-sha']
Deployability,"The dice came up Patrick for compiler team. From Masa via Zulip:. full pipeline is literally gt_to_gp then export_bgen. ```; import hail as hl; import atexit; import datetime. atexit.register(; lambda: hl.copy_log(f'gs://ukbb-hail/export_hardcall_bgen_{datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")}.log')). # cf. https://github.com/Nealelab/ukb_common/blob/master/saige/extract_vcf_from_mt.py; def gt_to_gp(mt, gt_location: str = 'GT', gp_location: str = 'GP'):; return mt.annotate_entries(; **{; gp_location:; hl.or_missing(; hl.is_defined(mt[gt_location]),; hl.map(lambda i: hl.cond(mt[gt_location] == i, 1.0, 0.0),; hl.range(0, hl.triangle(hl.len(mt.alleles))))); }). chrom = 1; mt = hl.read_matrix_table(f'gs://ukbb-hail/ukb31063.dosage.pGT.gwas_samples.chr{chrom}.mt'). # write bgen as well; mt = gt_to_gp(mt); hl.export_bgen(mt, f'gs://ukbb-hail/ukb31063.dosage.hard_call.gwas_samples.chr{chrom}', gp=mt.GP, varid=mt.rsid); ```; doesn't work with highmem, working now on ultramem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8163:71,pipeline,pipeline,71,https://hail.is,https://github.com/hail-is/hail/issues/8163,1,['pipeline'],['pipeline']
Deployability,"The divs for the two columns now span the whole screen, so there is room to breathe between the dev deploy and PR tables. Fixed a spacing bug in the PR tables by aligning them to the start of the flexbox column and standardized on header / searchbox / table per card. This should take up more of the screen, but is not explicitly ""centered"", basically the tables start at 0% and 50%. Unfortunately it's a painful process to fiddle with the test repo to get anything good to look at on a development CI. I'll start thinking automatically writing out test data to better test UI changes, but for now I just want to see this in main.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10121:100,deploy,deploy,100,https://hail.is,https://github.com/hail-is/hail/pull/10121,1,['deploy'],['deploy']
Deployability,"The docs on the new `export` method are pretty clear:; ```pycon; >>> mt.GT.export('gt.tsv'); >>> with open('gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus	alleles	0	1	2	3; 1:1	[""A"",""C""]	0/1	1/1	0/1	0/1; 1:2	[""A"",""C""]	1/1	0/1	1/1	1/1; 1:3	[""A"",""C""]	0/1	0/0	0/0	0/0; 1:4	[""A"",""C""]	1/1	1/1	0/0	1/1; ```. I also changed all the vds extensions to mt, added a new; dataset that is small enough to print the entire matrix,; and fixed a bug in `make install-editable`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6841:467,install,install-editable,467,https://hail.is,https://github.com/hail-is/hail/pull/6841,1,['install'],['install-editable']
Deployability,The document on https://hail.is/docs/devel/getting_started.html#installation seems to be garbled,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4340:64,install,installation,64,https://hail.is,https://github.com/hail-is/hail/issues/4340,1,['install'],['installation']
Deployability,"The driver will regularly update the VM state from the azure API so including the update time can block the driver from deleting runaway instances, which can subsequently halt the system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12572:26,update,update,26,https://hail.is,https://github.com/hail-is/hail/pull/12572,2,['update'],['update']
Deployability,"The errors look like this:; ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1074"", ""message"": ""update job (278, 6858, 'main') with pod batch-278-job-6858-5879db""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1087"", ""message"": ""job (278, 6858, 'main') mark complete""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6545:176,update,update,176,https://hail.is,https://github.com/hail-is/hail/issues/6545,1,['update'],['update']
Deployability,"The failure doesn't appear to be related to my changes. Installing the docker requirements, which has `setuptools>=38.6.0`, is trying to upgrade to the latest setuptools (56.0.0). Another dependency might be forcing the upgrade. However, setuptools was installed via apt, not pip, and that is causing this:. ```; Attempting uninstall: setuptools; Found existing installation: setuptools 45.2.0; Not uninstalling setuptools at /usr/lib/python3/dist-packages, outside environment /usr; Can't uninstall 'setuptools'. No files were found to uninstall.; ```. So there's two things I don't understand. I'll keep investigating. I glad I PRed this separately!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10349#issuecomment-824092733:56,Install,Installing,56,https://hail.is,https://github.com/hail-is/hail/pull/10349#issuecomment-824092733,5,"['Install', 'install', 'upgrade']","['Installing', 'installation', 'installed', 'upgrade']"
Deployability,"The first few lines of a hail log look like:; ```; 2019-12-02 13:20:36 Hail: WARN: This Hail JAR was compiled for Spark 2.4.0, running with Spark 2.4.1.; Compatibility is not guaranteed.; 2019-12-02 13:20:36 SparkContext: INFO: Running Spark version 2.4.1; 2019-12-02 13:20:36 SparkContext: INFO: Submitted application: Hail; 2019-12-02 13:20:36 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=//miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jar; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=0; spark.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=local[*]; spark.repl.local.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; ```. But the hail version string isn't here! That would be helpful. The full one with the hash. Rolled the dice, came up John.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7644:372,configurat,configuration,372,https://hail.is,https://github.com/hail-is/hail/issues/7644,2,"['configurat', 'deploy']","['configuration', 'deployMode']"
Deployability,"The fix for Safari will take effect the next time the docs are deployed. In the meantime,; the docs are indeed downloaded, but Safari tells you there was a problem. If users navigate; to the Downloads directory, the file should indeed be present.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10302:63,deploy,deployed,63,https://hail.is,https://github.com/hail-is/hail/pull/10302,1,['deploy'],['deployed']
Deployability,"The fix is somewhat subtle and relies on PruneDeadFields, which is called (a) by the optimizer and (b) by the compiler, so this is safe. The important piece is that the process of upcasting strips requiredness. This isn't a great design, but I'm comfortable with it for now since I hope physical types will solve this in The Right Way™ before 0.2 release anyway. fixes #4134",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4265:347,release,release,347,https://hail.is,https://github.com/hail-is/hail/pull/4265,1,['release'],['release']
Deployability,The fix landed after the tagged 0.2.65 release. Can you share the git commit hash that's failing?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10352#issuecomment-829188311:39,release,release,39,https://hail.is,https://github.com/hail-is/hail/issues/10352#issuecomment-829188311,1,['release'],['release']
Deployability,"The formatting does become a bit much. This is black's preferred rendering:; ```; @app.command(); def deploy(; branch: Annotated[str, typer.Option(""--branch"", ""-b"", help=""Fully-qualified branch, e.g., hail-is/hail:feature"")],; steps: Annotated[; List[str],; typer.Option(""--steps"", ""-s"", help=""Comma or space-separated list of steps to run.""),; ],; excluded_steps: Annotated[; List[str],; typer.Option(; ""--excluded_steps"",; ""-e"",; help=""Comma or space-separated list of steps to forcibly exclude. Use with caution!"",; ),; ],; extra_config: Annotated[; List[str],; typer.Option(; ""--extra-config"",; ""-e"",; help=""Comma or space-separated list of key=value pairs to add as extra config parameters."",; ),; ],; open: Annotated[; bool,; typer.Option(""--open"", ""-o"", help=""Open the deploy batch page in a web browser.""),; ],; ):; pass. ```. We can reduce the noise a bit with aliases:; ```; from typing import Annotated as Ann, List; from typer import Opt. @app.command(); def deploy(; branch: Ann[str, Opt(""--branch"", ""-b"", help=""Fully-qualified branch, e.g., hail-is/hail:feature"")],; steps: Ann[; List[str],; Opt(""--steps"", ""-s"", help=""Comma or space-separated list of steps to run.""),; ],; excluded_steps: Ann[; List[str],; Opt(; ""--excluded_steps"",; ""-e"",; help=""Comma or space-separated list of steps to forcibly exclude. Use with caution!"",; ),; ],; extra_config: Ann[; List[str],; Opt(; ""--extra-config"",; ""-e"",; help=""Comma or space-separated list of key=value pairs to add as extra config parameters."",; ),; ],; open: Ann[; bool,; Opt(""--open"", ""-o"", help=""Open the deploy batch page in a web browser.""),; ],; ):; pass; ```. It seems to me that the benefits of real sub-commands and better dead-option linting is worth the extra noise in the function definition.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13109#issuecomment-1570514400:102,deploy,deploy,102,https://hail.is,https://github.com/hail-is/hail/pull/13109#issuecomment-1570514400,4,['deploy'],['deploy']
Deployability,"The front-end accepted a boolean flag to make the a random BlockMatrix with a uniform distribution and would default to gaussian, whereas the backend method it's calling accepts a `gaussian` flag. Updated the front-end API to match the backend and added a test to check that matrices made explicitly uniform have no negative values and are (very likely) not gaussian.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5409:197,Update,Updated,197,https://hail.is,https://github.com/hail-is/hail/pull/5409,1,['Update'],['Updated']
Deployability,"The getting started, tutorial, and command documentation are all coming to python in the next week or two. We can certainly look into registering Hail on PyPI, but I'm not sure how versioning works there -- with the current pace of development, we may want to hold off on that until a stable release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218#issuecomment-270455160:292,release,release,292,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-270455160,1,['release'],['release']
Deployability,"The goal of this PR is to have all of the JVM container logs available where all the worker logs are. I tagged the entries with ""worker.log"" so they show up with the other worker log entries. However, it's plain text with no timestamp. We can improve the formatting as a separate project. Notice the two entries with ""*"" on the left instead of the normal ""I"". The design choice I made is to have the JVM containers write to a location that is static. We cannot easily change the fluentd configuration dynamically. It requires restarting the daemon which takes 1.5 seconds. Furthermore, the configuration for fluentd is on /etc/ on the host which the batch worker container cannot access. Hence, why I took the approach of specifying it in the startup script at known locations. . Before we merge this, I'd like to confirm that (a) we want these logs and (b) they don't contain any secrets.; <img width=""1585"" alt=""Screenshot 2023-06-16 at 4 06 43 PM"" src=""https://github.com/hail-is/hail/assets/1693348/0ce9f7dc-1188-4c66-ae6f-83fcc3744f95"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13190:487,configurat,configuration,487,https://hail.is,https://github.com/hail-is/hail/pull/13190,2,['configurat'],['configuration']
Deployability,"The hail-vdc-sa-key was (temporarily) used by apiserver, which is no longer being deployed. When it comes back, it should either have its own service account (mounted in the standard location) or, if @akotlar user isolation stuff is ready, not use the Hadoop connector. FYI @konradjk when this goes I'll push a new version to Docker Hub.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6283:82,deploy,deployed,82,https://hail.is,https://github.com/hail-is/hail/pull/6283,1,['deploy'],['deployed']
Deployability,"The handling for covariates in the top of linreg/logreg was broken. I updated, can you double-check?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3477#issuecomment-386075467:70,update,updated,70,https://hail.is,https://github.com/hail-is/hail/pull/3477#issuecomment-386075467,1,['update'],['updated']
Deployability,"The high-level problem: . The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10314:96,deploy,deployment,96,https://hail.is,https://github.com/hail-is/hail/pull/10314,1,['deploy'],['deployment']
Deployability,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:95,deploy,deployment,95,https://hail.is,https://github.com/hail-is/hail/pull/10279,1,['deploy'],['deployment']
Deployability,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10390:95,deploy,deployment,95,https://hail.is,https://github.com/hail-is/hail/pull/10390,1,['deploy'],['deployment']
Deployability,"The identity color scale (which treats the values of the ""color"" aesthetic mapping as literal hex color codes) was grouping by the color values, because their type ""tstr"" is normally a discrete type. This had the effect of reordering data unnecessarily, and creating a pointless noisy legend for the trivial color scale. This changes the identity color scale to be continuous. It also modifies the grouping logic to group by aesthetics with discrete scales, not with ""discrete types"". Now the identity color scale doesn't group or create a legend.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12089:365,continuous,continuous,365,https://hail.is,https://github.com/hail-is/hail/pull/12089,1,['continuous'],['continuous']
Deployability,"The image _BUILT target didn't depend on everything in the folders. I don't know how to make it work. `%` doesn't seem to get substituted inside a Make `$(...)` command, so I don't know how to `find` all the dependencies. I just made it always run `docker`. I renamed the hail image because I use `hail` locally for an image that doesn't contain a notebook. It's just hail installed in ubuntu. The `--ip` is apparently necessary for python2 Jupyter Notebook (which is used by the isia image).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7045:373,install,installed,373,https://hail.is,https://github.com/hail-is/hail/pull/7045,1,['install'],['installed']
Deployability,"The immutable approach is fine, but we should implement it properly so the user doesn't get surprised when an old binding of the object is mutated through a new binding of the object. And there's the issue of update not really being annotate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2869#issuecomment-364199693:209,update,update,209,https://hail.is,https://github.com/hail-is/hail/pull/2869#issuecomment-364199693,1,['update'],['update']
Deployability,"The integration test is failing. Otherwise, looks good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1572#issuecomment-287900602:4,integrat,integration,4,https://hail.is,https://github.com/hail-is/hail/pull/1572#issuecomment-287900602,1,['integrat'],['integration']
Deployability,"The issue appears to be something in the way spark is configured in this branch. I cannot broadcast successfully new SerilaizableHadoopConfiguration(sc.hadoopConfiguration, inside of LoadVCF. Meaning it works, but the configuration is null. Manually serializing in a test works fine. No issues on master. Minimal example:. ```scala; // LoadVCF, using master's SerializableHadoopConfiguration class ; private val fileInfo: Array[Array[String]] = externalSampleIds.getOrElse {; val shConf = new SerializableHadoopConfiguration(sc.hadoopConfiguration); val localBcFsConf = sc.broadcast(shConf); var results: Array[Array[String]] = Array(); var stuff = sc.parallelize(files, files.length).map { file =>; sc.hadoopConfiguration; }.collect(). results; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-496946496:218,configurat,configuration,218,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-496946496,1,['configurat'],['configuration']
Deployability,"The issue is we start billing for instances as soon as they're created with the API. However, if an instance is stuck in provisioning and never activates, we never update the start billing time to account for the lack of resource. This PR uses the `lastStartTimestamp` in the [Google REST API](https://cloud.google.com/compute/docs/reference/rest/v1/instances/get). This value is in RFC3339 format. I think this is the same format the timestamp in the activity logs, so I copied how we parse that value. If we delete the instance due to activation timeout, then we set the attempt start time to NULL so it's not billed. I couldn't find good documentation on this, but it seems like the `lastStartTimestamp` approximates what we care about for the purposes of checking for stuck workers. I checked it on an instance that was provisioning and the value was missing. Once the instance was in starting, the value was about 10 seconds after the `creationTimestamp`. . QUESTION: This does raise a question on whether we should be using the `lastStartTimestamp` when billing users if the difference is around 10 seconds. That will be a harder change, but is probably doable. We can't access the `lastStartTimestamp` through the metadata on the worker which would have been the easiest solution. We can get the compute client on the worker and access the `lastStartTimestamp` that way and set the job start time to the instance start time. I'd need to change the database for the attempts trigger to account for this. For the scenario where a job private job is cancelled while creating the instance, we would either need to make the additional API call or we just leave the time we created the instance. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10069:164,update,update,164,https://hail.is,https://github.com/hail-is/hail/pull/10069,1,['update'],['update']
Deployability,The jinja2 templates got moved from batch2/templates to batch2/front_end/templates but the manifest file to copy the template files wasn't updated.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7310:139,update,updated,139,https://hail.is,https://github.com/hail-is/hail/pull/7310,1,['update'],['updated']
Deployability,"The layers of wtf really seem to have no end here. Hadoop at least *appears* to [include the configuration in the cache key](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java#L3833-L3841) for its FileSystem cache, but it is actually just ignored by the constructor. Ergo, even if you stop the Hail context and try to start a new hail context with a new Hadoop Configuration, you'll get a filesystem configured by the first configuration. I'm looking for a way around this now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12133#issuecomment-1241322443:93,configurat,configuration,93,https://hail.is,https://github.com/hail-is/hail/pull/12133#issuecomment-1241322443,3,"['Configurat', 'configurat']","['Configuration', 'configuration']"
Deployability,"The lock file is auto-generated. There isn't any need to review its contents; it matches the dependency tree for the packages/versions specified in package.json. Committing it allows all future users to follow the same dependency graph during installation. Ref: https://docs.npmjs.com/files/package-lock.json , https://stackoverflow.com/questions/44297803/package-lock-json-role; - Counter-argument: https://github.com/npm/npm/issues/20603. We can accept this PR first, to make the dependent PR small.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5362:243,install,installation,243,https://hail.is,https://github.com/hail-is/hail/pull/5362,1,['install'],['installation']
Deployability,The main change here is changing a 7 to an 8. I'm not sure how any lowered array anything ever worked. We should release after this goes in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10684:113,release,release,113,https://hail.is,https://github.com/hail-is/hail/pull/10684,1,['release'],['release']
Deployability,The main offenders were:; 1. `python3-pip` installs python3.8.; 2. `gcloud` is like 1.2GiB.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12526:43,install,installs,43,https://hail.is,https://github.com/hail-is/hail/pull/12526,1,['install'],['installs']
Deployability,"The major version upgrade of the azure provider changed some defaults, so in order to keep our infrastructure the same I explicitly set these changed fields back to their previous v2.99.0 values. The `min_tls_version` now defaults to 1.2, which *should* be fine as I presume all our services are using TLS 1.3, but we should independently verify that before making that change. I will make an issue accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14385:18,upgrade,upgrade,18,https://hail.is,https://github.com/hail-is/hail/pull/14385,1,['upgrade'],['upgrade']
Deployability,"The new tar file is now in all VEP replicates for dataproc. The only change is it uses the indexed cache files and the tar file has the word ""_indexed"" in it. Otherwise, it should have the same contents / file structure as the non-indexed tar file that is there currently. I tested this as best as I could, but it would be prudent to give ourselves time when releasing this in case there is a problem in the release script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14071#issuecomment-1881444531:408,release,release,408,https://hail.is,https://github.com/hail-is/hail/pull/14071#issuecomment-1881444531,1,['release'],['release']
Deployability,"The old bucket did not use uniform access control and also was multi-regional (us). I created a new bucket using the random suffix ger0g which has uniform access control. I also switched the location to us-central1 (not pictured here because that is a variable). I copied all the JARs from `gs://hail-query/jars` to `gs://hail-query-ger0g/jars` using a GCE VM. Again, global-config is not present in our terraform, so I'll have to manually edit that to reflect this new location: `gs://hail-query-ger0g`. The deployment process is:. 1. Edit global-config to reflect new bucket.; 2. Delete batch and batch-driver pods.; 3. Delete old workers. The rollback process (if necessary) is the same. Since this requires wiping the workers, I'll wait for a time when no one is on the cluster to do it. Any users using explicit JAR URLs will need to switch to `gs://hail-query-ger0g/...`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12969:509,deploy,deployment,509,https://hail.is,https://github.com/hail-is/hail/pull/12969,2,"['deploy', 'rollback']","['deployment', 'rollback']"
Deployability,"The old code in `Transaction` to exit the transaction and release the connection back to the pool looked like this:. ```; async def _aexit(self, exc_type, exc_val, exc_tb):; try:; if self.conn is not None:; try:; if exc_type:; await self.conn.rollback(); else:; await self.conn.commit(); finally:; self.conn = None; finally:; if self.conn_context_manager is not None:; try:; await aexit(self.conn_context_manager, exc_type, exc_val, exc_tb); finally:; self.conn_context_manager = None; ```. The problem was if the current coroutine was cancelled in the call to `aexit(self.conn_context_manager, ...)`, which ultimately calls aiomysql `Pool.release`, the release never happens. This was happening when database calls in `@only_active_instances` were getting cancelled when the client timed out and terminated the request. Roughly, the solution is to shield exiting the connection, and return the connection asynchronously in a background task (using `ensure_future`). FYI @danking @jigold This is a subtle bug/pattern for managing resources that we should all be aware of.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9307:58,release,release,58,https://hail.is,https://github.com/hail-is/hail/pull/9307,4,"['release', 'rollback']","['release', 'rollback']"
Deployability,"The only noticeable change after this PR is that devs will be able to access `internal.hail.is/<PR-namespace>/<service>` while it is running and see jobs submitted by tests. Well, that and you can add another developer to a dev namespace as a developer without destroying the developer's existing namespace. Subsequent PRs will introduce on-demand dev namespaces and the ability to suspend the deletion of a test namespace. New context added to the CI pipeline is treated as optional to be backward compatible with the current CI. So devs won't be able to log in to test namespaces on *this* PR but will be able to once this PR becomes main. ### What has changed; - All developers from default are now added to all test namespaces using the `add_users` build.yaml step and removed at the end of the PR run through the `delete_users` step. These use the normal create and delete API instead of copying the user's gsa from the production namespace. This relies on / tests that the delete user endpoint is properly deleting cloud identities when the users are deleted (previously broken in GCP but fixed in this PR.; - The developer role no longer implicitly deletes and recreates a corresponding namespace. I wanted adding developers to test namespaces not to have side-effects that leaked out of the namespace. A follow-up PR will incorporate the ability for a developer to request an on-demand dev namespace, which should be made a lot easier after these changes. I think this also means that we can remove some permissions from the auth K8s ServiceAccount since it no longer needs the ability to create and delete namespaces.; - A fixed-but-sufficient number of oauth2 callbacks are hard-coded into the oauth2 secret from GCP/azure and then allocated to a given namespace. This is fairly self-contained, all that needs to happen is to tell `auth` what callback to use and rewrite those callback urls in gateway to route back to the appropriate auth. This is done only for test namespaces, production ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12751:452,pipeline,pipeline,452,https://hail.is,https://github.com/hail-is/hail/pull/12751,1,['pipeline'],['pipeline']
Deployability,The only use of the `_to_json` method I have is in a forthcoming combiner update. Theoretically I can use what I put in the `_to_json` method for tmatrix directly where I need it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12369#issuecomment-1287161147:74,update,update,74,https://hail.is,https://github.com/hail-is/hail/pull/12369#issuecomment-1287161147,1,['update'],['update']
Deployability,"The original goal of this PR was avoiding `Try` when we are not using the restartability provided by semantic hashing because I strongly suspect it is related to the loss of stacktraces in exceptions. Unrelatedly, we realized the semantic hash PR changed the semantics of Query-on-Spark even when semantic hash is disabled: previously we would abort RDD writing on the first exception. In Hail 0.2.123 through 0.2.126, the semantics were changed to only crash *after* we already ran every other partition. Two bad scenarios of which I can think:. 1. Suppose the first partition fails due to OOM. We now waste time/money on the rest of the partitions even though we cannot possibly get a valid output. 2. Suppose every partition hits a permission error. Users should get that feedback after paying for O(1) partitions run, not O(N). I created two Backend paths: the normal `parallelizeAndComputeWithIndex` with its pre-0.2.123 semantics as well as `parallelizeAndComputeWithIndexReturnAllErrors` which, as the name says, returns errors instead of raising them. While making this change, I think I found two other bugs in the ""return all errors"" path, only one of which I addressed in this PR:. 1. I'm pretty sure semantic-hash-enabled QoB batch submission is broken because it uses the logical partition ids as job indices. Suppose there are 10,000 partitions, but we only need to compute 1, 100, and 1543. 0.2.126 would try to submit a batch of size 3 but whose job indices are 1, 100, and 1543. 2. Likewise, the Query-on-Spark path returns an invalid `SparkTaskContext.partitionId` which, at best, produces confusing partition filenames. I only fixed the former because it was simple to fix. I wasn't exactly sure what to do about the latter. We should fix that separately because the changes in this PR need to urgently land in the next release to avoid unexpected cost when one partition fails.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14085:1839,release,release,1839,https://hail.is,https://github.com/hail-is/hail/pull/14085,1,['release'],['release']
Deployability,"The original report was about `gnomad.exomes.r2.1.1.sites.liftover_grch38.vcf.bgz`. That's the gnomad v2.1.1 GRCh38 liftover sites table. See [this section of the gnomAD downloads](https://gnomad.broadinstitute.org/downloads#v2-liftover). In particular it is the ""All chromosomes VCF"". That's 85GiB, so I don't want to download it. I believe the chr21 VCF should have just as many row, column, and entry fields, so I downloaded that and tested Hail's ability to import and write it. ```bash; gsutil -m cp \; gs://gcp-public-data--gnomad/release/2.1.1/liftover_grch38/vcf/exomes/gnomad.exomes.r2.1.1.sites.21.liftover_grch38.vcf.bgz \; .; ```; ```python3; import hail as hl; recode = {f""{i}"":f""chr{i}"" for i in (list(range(1, 23)) + ['X', 'Y'])}; mt = hl.import_vcf('gnomad.exomes.r2.1.1.sites.21.liftover_grch38.vcf.bgz', reference_genome='GRCh38', contig_recoding=recode); mt.write('gnomad.mt', overwrite = True); ```. With Hail 0.2.108-fc03e9d5dc08 it worked fine. It also worked fine on a recent 0.2.120 development version I had installed. Next I tried running on the first few thousand lines of the full sites table:. ```bash; curl \; https://storage.googleapis.com/gcp-public-data--gnomad/release/2.1.1/liftover_grch38/vcf/exomes/gnomad.exomes.r2.1.1.sites.21.liftover_grch38.vcf.bgz \; | bgzip -d -c\; | head -n 10000 \; | bgzip -c \; > /tmp/head-sites.vcf.bgz; ```; ```python3; import hail as hl; recode = {f""{i}"":f""chr{i}"" for i in (list(range(1, 23)) + ['X', 'Y'])}; mt = hl.import_vcf('/tmp/head-sites.vcf.bgz', reference_genome='GRCh38', contig_recoding=recode); mt.write('gnomad.mt', overwrite = True); ```. This also succeeded with Hail 0.2.108",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13249#issuecomment-1703341525:537,release,release,537,https://hail.is,https://github.com/hail-is/hail/issues/13249#issuecomment-1703341525,3,"['install', 'release']","['installed', 'release']"
Deployability,"The per-element code seems to end up in a separate method. Is that not standard? For example, the pipeline I'm using is an array of struct and the struct is always in a separate method",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13776#issuecomment-1747578071:98,pipeline,pipeline,98,https://hail.is,https://github.com/hail-is/hail/pull/13776#issuecomment-1747578071,1,['pipeline'],['pipeline']
Deployability,The pip install for pyspark installs the 2.2 version of Spark. Will that pyspark version work with Hail? The Getting Started Page only documents 2.0 and 2.1.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2003#issuecomment-330569535:8,install,install,8,https://hail.is,https://github.com/hail-is/hail/issues/2003#issuecomment-330569535,2,['install'],"['install', 'installs']"
Deployability,The pipeline I posted doesn't leave _1 lying around -- `va = va._1` overwrites `va`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/488#issuecomment-235098604:4,pipeline,pipeline,4,https://hail.is,https://github.com/hail-is/hail/pull/488#issuecomment-235098604,1,['pipeline'],['pipeline']
Deployability,"The pods currently look like this:. ```; site-deployment-5b5697d6bb-2p5rk 1/1 Running 0 6h; site-deployment-69f686bf7f-266kg 0/1 ContainerCreating 0 2h; ```. The problem is that site mounts a volume RWO with the certs. The problem is the second pod can't launch for seamless upgrade because it can't mount the volume. There is a further discussion here: https://github.com/kubernetes/kubernetes/issues/26567. Short-term fix: we could delete the pod and recreate on upgrade, which would lead to a short window of downtime. Long-term fix: Normally the certs are only read by nginx, but need to be written by the certbot renew cron job. We could keep the certs in a volume. We could put a copy of the certs in a secret which can be mounted by multiple pods (e.g. site including when it is upgraded). Then we run certbot periodically in its own pod and update the original certs stored in the volume. After it runs, we create a new secret (or upgrade it if we can) with the new certs and then do a seamless upgrade on site.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4463:46,deploy,deployment-,46,https://hail.is,https://github.com/hail-is/hail/issues/4463,8,"['deploy', 'update', 'upgrade']","['deployment-', 'update', 'upgrade', 'upgraded']"
Deployability,"The problem is it takes more than 7 minutes to schedule a trivial CI job and then a trivial deploy job, I could set the retries or try-delay higher, but what else could correct mean?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5639#issuecomment-474817396:92,deploy,deploy,92,https://hail.is,https://github.com/hail-is/hail/pull/5639#issuecomment-474817396,1,['deploy'],['deploy']
Deployability,The problem seems to be a MatrixFilterEntries from the stack trace above. can we have the full pipeline?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384667398:95,pipeline,pipeline,95,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384667398,1,['pipeline'],['pipeline']
Deployability,"The problem was query was writing the job configuration to the query bucket, but workers only get the user gsa, so they were unable to read the configuration. This worked in the tests because the query and user account are both the test service account. I can remove the query-gsa-key and the hail-query bucket after this goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8937:42,configurat,configuration,42,https://hail.is,https://github.com/hail-is/hail/pull/8937,2,['configurat'],['configuration']
Deployability,"The real change here is changing the preemptible pool config from `preemptible = true` to `spot = true`, but the `spot` config was only available in the new provider which involved a major version upgrade. The only incompatibility was the addition of an explicit `project` input to `google_project_iam_member`, as opposed to picking it up from the provider configuration. Tested just now in my own project. If one wants to apply this change without incurring downtime for preemptible deployments, they should follow the instructions outlined in the [migrating node pools dev-docs](https://github.com/hail-is/hail/blob/main/dev-docs/kubernetes-operations.md#when-using-terraform).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12127:197,upgrade,upgrade,197,https://hail.is,https://github.com/hail-is/hail/pull/12127,3,"['configurat', 'deploy', 'upgrade']","['configuration', 'deployments', 'upgrade']"
Deployability,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:30,patch,patch,30,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474,1,['patch'],['patch']
Deployability,"The recent PR #14086 was added to for a month before it was merged and the release made on Jan 12th. However the release date listed in the changelog remained the `Released 2023-12-08` date on which the PR was initiated. If there is a checklist for making releases, it may be worth adding “Update the release date in _change_log.md_ before merging the release PR” to it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14161#issuecomment-1891099203:75,release,release,75,https://hail.is,https://github.com/hail-is/hail/pull/14161#issuecomment-1891099203,7,"['Release', 'Update', 'release']","['Released', 'Update', 'release', 'releases']"
Deployability,"The remaining occurrences are not relevant to production work:; ```; /Users/dking/projects/hail/batch/batch/worker/worker.py:484: # * gcr.io/hail-vdc/hailgenetics/python-dill; /Users/dking/projects/hail/batch/batch/worker/worker.py:705: # DockerError(500, ""Head https://gcr.io/v2/genomics-tools/samtools/manifests/latest: unknown: Project 'project:genomics-tools' not found or deleted.""); /Users/dking/projects/hail/datasets/extract/extract_CADD.py:26: j.image(""gcr.io/broad-ctsa/datasets:050521""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:12: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:19: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:26: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/hail/scripts/update-terra-image.py:33:Image URL: `us.gcr.io/broad-dsp-gcr-public/{image_name}:{image_version}`; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:115: x = parse_docker_image_reference('gcr.io/hail-vdc/batch-worker:123fds312'); /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:116: assert x.domain == 'gcr.io'; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:120: assert x.name() == 'gcr.io/hail-vdc/batch-worker'; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:121: assert str(x) == 'gcr.io/hail-vdc/batch-worker:123fds312'; /Users/dking/projects/hail/hail/python/hail/docs/change_log.md:278:- (hail#12230) The python-dill Batch images in `gcr.io/hail-vdc` are no longer supported.; /Users/dking/projects/hail/hail/python/hailtop/utils/utils.py:707: # DockerError(500, ""Head https://gcr.io/v2/genomics-tools/samtools/manifests/latest: unknown: Project 'project:genomics-tools' not found or deleted.""); /Users/dking/projects/hail/hail/python/hailtop/utils/utils.py",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12963#issuecomment-1531692013:941,update,update-terra-image,941,https://hail.is,https://github.com/hail-is/hail/pull/12963#issuecomment-1531692013,1,['update'],['update-terra-image']
Deployability,The remove_tmpdir job fails with e.g.:. ```; Activated service account credentials for: [my-service-account@hail-vdc.iam.gserviceaccount.com]; CommandException: No URLs matched: gs://my-service-account/pipeline/pipeline-947753d9ef82; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7117:202,pipeline,pipeline,202,https://hail.is,https://github.com/hail-is/hail/issues/7117,2,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,"The resource names are changing to include the regions. So yes, this PR will use the latest resources. The backwards compatibility is for right when the front end starts up before the driver has the chance to fetch the latest products and their prices. Besides not getting a chance to dev deploy this change and make sure it works is whether we need to have a solution for users being able to select us-central if they don't want to pay the 10% increase in costs. I wanted to spec out how much work that would be, but got distracted by other things.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11840#issuecomment-1164545882:289,deploy,deploy,289,https://hail.is,https://github.com/hail-is/hail/pull/11840#issuecomment-1164545882,1,['deploy'],['deploy']
Deployability,"The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be rebuilt) target is dynamically generated. That target will force a execution of any dependent targets, in the example above, it will force `build` to be exec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130:2279,install,installed,2279,https://hail.is,https://github.com/hail-is/hail/pull/5130,1,['install'],['installed']
Deployability,"The root cause of large memory usage is https://github.com/hail-is/hail/issues/13748 but we should be deploying Hail in a manner that has enough RAM (in this case, 4GiB is plenty). I'm following up with GVS to determine whether the JVM indeed has enough RAM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13712#issuecomment-1743500460:102,deploy,deploying,102,https://hail.is,https://github.com/hail-is/hail/issues/13712#issuecomment-1743500460,1,['deploy'],['deploying']
Deployability,"The search bar for the batch docs is broken and just says `Searching…` forever. Tracked it down to a bug in the `sphinx_rtd_theme` dependency that was fixed in a later release. The important files to look at are the `requirements.txt` files not the `pinned-requirements.txt` files as the latter bulk updated a bunch of patch releases when I regenerated them. . In the mess of version conflicts that updating a dependency appears to do here, I also removed `google-cloud-logging` as it appears to be an unused dependency and `pre-commit` because it is optional for developers and had a hard requirement on a `importlib-metadata` version that made it incompatible with other important libraries that we use. I also explicitly pinned `protobuf` as a major version upgrade that wasn't restricted by some google libraries we use broke those same google libraries that added that dependency.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12215:168,release,release,168,https://hail.is,https://github.com/hail-is/hail/pull/12215,5,"['patch', 'release', 'update', 'upgrade']","['patch', 'release', 'releases', 'updated', 'upgrade']"
Deployability,"The serial deployment of auth and batch can lengthen the critical path of CI pipelines by a few minutes if k8s needs to spin up new nodes. While auth is necessary for batch to function correctly, it's not necessary to deploy batch, so I think it's more appropriate to not have `deploy_batch` depend on `deploy_auth` but have anything that depends on `deploy_batch` also depend on `deploy_auth`. There's already a precedent for this in that the service backend tests depend on `deploy_batch` and `deploy_memory` as opposed to `deploy_batch` being dependent on `deploy_memory`. I also removed the dependency of `upload_query_jar` on `deploy_batch`, no idea why that was there, but maybe it should be the other way around?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12354:11,deploy,deployment,11,https://hail.is,https://github.com/hail-is/hail/pull/12354,3,"['deploy', 'pipeline']","['deploy', 'deployment', 'pipelines']"
Deployability,"The setuptools thing was actually a red herring, that error isn't fatal, and there was a version conflict elsewhere. I upgraded urllib3 and requests to resolve.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10349#issuecomment-843240422:119,upgrade,upgraded,119,https://hail.is,https://github.com/hail-is/hail/pull/10349#issuecomment-843240422,1,['upgrade'],['upgraded']
Deployability,"The shuffler lives!. I'm really quite satisfied with how small this PR is. Make sure you enable ""Hide whitespace changes"". I removed a try-catch from each test which changed a bunch of formatting. This PR just deploys the shuffler and retargets all tests at the shuffler service rather than a local version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9109:210,deploy,deploys,210,https://hail.is,https://github.com/hail-is/hail/pull/9109,1,['deploy'],['deploys']
Deployability,"The site container polls regularly for new releases of hail and pulls the docs for the latest release. But we deploy site regularly enough that we can just include the latest release docs in the site build. Eventually, a deploy to PyPi should trigger a deploy of the site but this gets us part of the way there and simplifies things considerably.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9943:43,release,releases,43,https://hail.is,https://github.com/hail-is/hail/pull/9943,6,"['deploy', 'release']","['deploy', 'release', 'releases']"
Deployability,"The status parameter was being set as the scratch parameter. https://github.com/hail-is/hail/pull/5418 introduced another constructor parameter before `_status`, changing the meaning of the fifth argument. CI is currently broken as a result. I'm going to force merge this and manually re-deploy CI and batch. I'll follow up with a commit that enforced inter-project dependencies.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5462:288,deploy,deploy,288,https://hail.is,https://github.com/hail-is/hail/pull/5462,1,['deploy'],['deploy']
Deployability,"The target creates an egg in `hail/hail/build/deploy/dist/`, alongside the wheel.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8997#issuecomment-646921877:46,deploy,deploy,46,https://hail.is,https://github.com/hail-is/hail/pull/8997#issuecomment-646921877,1,['deploy'],['deploy']
Deployability,The template files weren't getting installed so the /recent endpoint was breaking. I followed the Flask instructions for installing non-Python files: http://flask.pocoo.org/docs/0.12/patterns/distribute/.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5310:35,install,installed,35,https://hail.is,https://github.com/hail-is/hail/pull/5310,2,['install'],"['installed', 'installing']"
Deployability,"The terraform plan for `gcp-broad` has 4 changes after appropriately importing. The bucket, ci_config, and hail_ci_0_1_github_oauth_token are actually unchanged. I do not know why they are listed as a change. That secret currently has three files: hail-ci-0-1.key, oauth-token, user1, user2. AFAICT, only `user1` is used, so the new secret only has `user1`. ```; # module.ci[0].google_storage_bucket.bucket will be updated in-place; ~ resource ""google_storage_bucket"" ""bucket"" {; id = ""hail-ci-bpk3h""; # Warning: this attribute value will be marked as sensitive and will not; # display in UI output after applying this change. The value is unchanged.; ~ location = (sensitive value); name = ""hail-ci-bpk3h""; # Warning: this attribute value will be marked as sensitive and will not; # display in UI output after applying this change. The value is unchanged.; ~ storage_class = (sensitive value); # (8 unchanged attributes hidden). # (2 unchanged blocks hidden); }. # module.ci[0].kubernetes_secret.ci_config will be updated in-place; ~ resource ""kubernetes_secret"" ""ci_config"" {; id = ""default/ci-config""; # (3 unchanged attributes hidden). # (1 unchanged block hidden); }. # module.ci[0].kubernetes_secret.hail_ci_0_1_github_oauth_token will be updated in-place; ~ resource ""kubernetes_secret"" ""hail_ci_0_1_github_oauth_token"" {; id = ""default/hail-ci-0-1-github-oauth-token""; # (3 unchanged attributes hidden). # (1 unchanged block hidden); }. # module.ci[0].kubernetes_secret.hail_ci_0_1_service_account_key will be updated in-place; ~ resource ""kubernetes_secret"" ""hail_ci_0_1_service_account_key"" {; ~ data = (sensitive value); id = ""default/hail-ci-0-1-service-account-key""; # (2 unchanged attributes hidden). # (1 unchanged block hidden); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12882#issuecomment-1507162756:415,update,updated,415,https://hail.is,https://github.com/hail-is/hail/pull/12882#issuecomment-1507162756,4,['update'],['updated']
Deployability,"The test deployment sets the default storage to 1GiB instead of the normal 10GiB. As a result, the PR tests passed. The deployment tests fail because the 10GiB default storage request forces a minimum core count of 0.5 CPU. This change explicitly requests less storage, thus preventing the rounding up of core count from 0.25 CPU to 0.5 CPU. The rounding up doubled the mcpu_msec time for the test, thus failing the test. Fixes this:. ```; -------------------------------- live log call ---------------------------------; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:497:submit created batch 70818; 2020-07-28T15:42:34 INFO batch_client.aioclient aioclient.py:533:submit closed batch 70818; FAILED; _____________________________ Test.test_msec_mcpu ______________________________. self = <test.test_batch.Test testMethod=test_msec_mcpu>. def test_msec_mcpu(self):; builder = self.client.create_batch(); resources = {; 'cpu': '100m',; 'memory': '375M'; }; # two jobs so the batch msec_mcpu computation is non-trivial; builder.create_job('ubuntu:18.04', ['echo', 'foo'], resources=resources); builder.create_job('ubuntu:18.04', ['echo', 'bar'], resources=resources); b = builder.submit(); ; batch = b.wait(); assert batch['state'] == 'success', batch; ; batch_msec_mcpu2 = 0; for job in b.jobs():; # I'm dying; job = self.client.get_job(job['batch_id'], job['job_id']); job = job.status(); ; # runs at 250mcpu; job_msec_mcpu2 = 250 * max(job['status']['end_time'] - job['status']['start_time'], 0); # greater than in case there are multiple attempts; assert job['msec_mcpu'] >= job_msec_mcpu2, batch; ; batch_msec_mcpu2 += job_msec_mcpu2; ; > assert batch['msec_mcpu'] == batch_msec_mcpu2, batch; E AssertionError: {'billing_project': 'test', 'closed': True, 'complete': True, 'cost': '$0.0000', ...}; E assert 2813000 == 1406500; E -2813000; E +1406500; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9166:9,deploy,deployment,9,https://hail.is,https://github.com/hail-is/hail/pull/9166,2,['deploy'],['deployment']
Deployability,"The test failure here is spurious, happening because batch is going through a transition / upgrade and things are a little broken right now. Will make sure this merges when that's resolved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10671#issuecomment-881473556:91,upgrade,upgrade,91,https://hail.is,https://github.com/hail-is/hail/pull/10671#issuecomment-881473556,1,['upgrade'],['upgrade']
Deployability,"The tests need to install a previous version of Hail and verify that files written by the current version under test can be ready by older versions. That ""older version"" might be the change itself in the case that the file format version is being bumped. I'm not 100% sure how to specify that version because we won't know the hash on master until after merge. Hmm.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8249:18,install,install,18,https://hail.is,https://github.com/hail-is/hail/issues/8249,1,['install'],['install']
Deployability,"The unoptimized pipeline doesn't have the decorator right now, but I do want to version it so it's easier to find when I decide what we want to do with it. I also think that there's an optimization that will bring it back down to ~a few mins, rather than an hour.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6482#issuecomment-505873500:16,pipeline,pipeline,16,https://hail.is,https://github.com/hail-is/hail/pull/6482#issuecomment-505873500,1,['pipeline'],['pipeline']
Deployability,The updated documents look good.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342#issuecomment-391751782:4,update,updated,4,https://hail.is,https://github.com/hail-is/hail/issues/3342#issuecomment-391751782,1,['update'],['updated']
Deployability,"The updated template renders the following in Firefox on my machine:. <img width=""1512"" alt=""Screenshot 2023-08-18 at 19 56 27"" src=""https://github.com/hail-is/hail/assets/84595986/e6e1bfcf-d554-4ee8-8054-f6b625386178"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13463:4,update,updated,4,https://hail.is,https://github.com/hail-is/hail/pull/13463,1,['update'],['updated']
Deployability,"The values of the global config are the same across namespaces, but it does feel more correct to use the `global-config` from the namespace you're targeting than the one in production. This should also enable `make` deploying into a dev namespace without having any permissions for `default`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13338:216,deploy,deploying,216,https://hail.is,https://github.com/hail-is/hail/pull/13338,1,['deploy'],['deploying']
Deployability,"The way that the off-heap-memory-fraction argument currently works; limits total memory usage in hail value heavy (like lowered) pipelines; immensely. The default settings both reserve AND and limit hail off heap; allocations to 60% of executor's memory. This behavior is almost never; what a user wants as it will reduce total memory that they can use. We; can retain some of the characteristics that these limits give us by; reserving off-heap-memory-fraction as overhead, and setting the; worker_off_heap_memory_per_core to be the total available memory per; core. This should still give good error messaging on attempts to; allocate too much memory for hail values while allowing us to use all; the memory we have available. A flag, --off-heap-memory-hard-limit, has been added to preserve the; previous behavior.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11531:129,pipeline,pipelines,129,https://hail.is,https://github.com/hail-is/hail/pull/11531,1,['pipeline'],['pipelines']
Deployability,"The website will get updated when we deploy a PyPI version. This seems; like the best approach for now -- we don't want new functions hanging; around that people can't use. That's caused trouble in the past. Also, it seems that from ci to ci2, we've started deploying the docs; by full revision, rather than short revision. The makefile reflects; that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6289:21,update,updated,21,https://hail.is,https://github.com/hail-is/hail/pull/6289,3,"['deploy', 'update']","['deploy', 'deploying', 'updated']"
Deployability,"The worker image installs `docker/requirements.txt` which pip installs `googlecloudprofiler` which requires gcc. I could probably figure out the actual requirements of the worker image and list those separately. That seems prudent anyway. However, in the long term, if we want to profiler the workers, we'd need build-essential. Installing build-essential adds 204MB.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8030#issuecomment-582661801:17,install,installs,17,https://hail.is,https://github.com/hail-is/hail/pull/8030#issuecomment-582661801,3,"['Install', 'install']","['Installing', 'installs']"
Deployability,"There are a few small cosmetic changes in here that were a result of an updated pylint, but I put those in a separate commit to hopefully make that less confusing. There are a few follow-ups after this that I want to tackle; - simplifying the images for testing query (Dockerfile.hail-build, Dockerfile.hail-base, Dockerfile.hail-run). I think these are the only things that use `base_image` so we might be able to collapse a bunch of these; - updating to python 3.8 to avoid accidentally installing that in some of our images; - trying to produce eStargz images so that buildkit can lazily pull the base image when building new images. I hope that can bring some image build times down even further by not having to localize the installed pip dependencies when making changes to our python code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548:72,update,updated,72,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548,6,"['install', 'update']","['installed', 'installing', 'updated']"
Deployability,"There are many global config fields that CI needs in order to template build.yaml jobs that are threaded through to CI with environment variables. However, these variables are never actually used by CI and they introduce some needless dependencies to run CI (you need a GCP_PROJECT, for example, even though CI doesn't care at all). Instead of setting specific environment variables for each field that build.yaml steps need, I instead mount the global-config (read-only) to the CI container and read in the whole thing. This does potentially expose more variables to the build.yaml environment than there were previously, but I argue that none of those should be sensitive anyway or maybe don't belong in the global-config (which shouldn't be sensitive). This in part makes the process of adding global config fields easier, since right now you need separate PRs to 1) introduce the field to CI and then 2) use it in a new build.yaml step. It also makes the CI deployment.yaml cloud-agnostic. . cc @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10911:962,deploy,deployment,962,https://hail.is,https://github.com/hail-is/hail/pull/10911,1,['deploy'],['deployment']
Deployability,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10869:258,deploy,deployment,258,https://hail.is,https://github.com/hail-is/hail/pull/10869,5,"['configurat', 'deploy']","['configuration', 'deployment', 'deployments']"
Deployability,"There are many things wrong here. The Hadoop configuration is not copied per HadoopRDD operation. Proof:. ```; >>> import hail as hl; >>> hl.init(min_block_size=0); >>> t = hl.import_table('test.tsv.bgz', impute=True, min_partitions=8); >>> t.n_partitions(); 8; >>> t = hl.import_table('test-bgz.tsv.gz', impute=True, min_partitions=8); >>> t.n_partitions(); 1; ```. where `test-bgz.tsv.gz` is a bgz in gz's clothing. This is compounded by the fact that SparkContext.hadoopFile is not invoked until TableIR.execute is run making HailContext.forceBGZ() completely ineffective. One option is turning on spark.hadoop.cloneConf, that appears to clone the Hadoop configuration (to avoid some multithreading issues) although the docs don't recommend it due to ""performance regressions"". I haven't tested it. The other option is stop using the Hadoop stuff so we can pass state into the file loaders. Doing that for text files/line splitting is a bit nasty, but it would mean we could properly fix this gz/bgz business once and for all (look at the GZ header to see if it is block gzip'ed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3861:45,configurat,configuration,45,https://hail.is,https://github.com/hail-is/hail/issues/3861,2,['configurat'],['configuration']
Deployability,"There are now three check steps:; - check_hail (in the sense of $HAIL_HOME/hail directory), which checks the hail and hailtop packages, using the base image, from the source via `make check-hail`; - check_services, using the services image, except the benchmark service, which installs addition dependencies, from the source via `make -k check-services`, `-k` forces make to check all packages, even if one fails; - check_benchmark_service, which checks benchmark as installed in that image",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9394#issuecomment-685253481:277,install,installs,277,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685253481,2,['install'],"['installed', 'installs']"
Deployability,"There could be a webserver with reference datasets, and local installs that use hail-based pipelines (eg. seqr-hail prototype) can avoid downloading large files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/840:62,install,installs,62,https://hail.is,https://github.com/hail-is/hail/issues/840,2,"['install', 'pipeline']","['installs', 'pipelines']"
Deployability,"There is a [known issue](https://github.com/moby/moby/issues/41792) with the official Docker deb. If you uninstall docker and re-install it later, it might fail to start. The root cause is the `docker.socket` `systemd` unit failing to start because there are ""insufficient file descriptors available"". I think this is confusing verbiage. The socket's name must be `/var/run/docker.sock`. Clearly, if that filename is already in use, we cannot create a new socket at that filename. One of Google's [""Dataproc components""](https://cloud.google.com/dataproc/docs/concepts/components/overview) is Docker. I believe Google installed and then uninstalled docker in this image, thus leaving it in the broken state. For evidence of that:. <details>; <summary> find docker on a worker node of a *non-Hail* Dataproc cluster</summary>. ```; sudo find / -iname '*docker*'; ```. ```; /opt/conda/miniconda3/pkgs/dbus-1.13.6-h5008d03_3/info/recipe/patches/0004-disable-fd-limit-tests-not-supported-in-docker.patch; /opt/conda/miniconda3/pkgs/nbclassic-0.5.6-pyhb4ecaf3_1/site-packages/nbclassic/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/pkgs/nbclassic-0.5.6-pyhb4ecaf3_1/site-packages/nbclassic/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/pkgs/notebook-6.2.0-py38h578d9bd_0/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/pkgs/notebook-6.2.0-py38h578d9bd_0/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/lib/python3.8/site-packages/nbclassic/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/lib/python3.8/site-packages/nbclassic/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile/docker",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:129,install,install,129,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,3,"['install', 'patch']","['install', 'installed', 'patches']"
Deployability,"There is more work to do to get this unified with the modern hail deployment (note that I copied the logging setup here). Nevertheless, this brings asyncio to notebook leader, which enables it to handle a helluva lot more simultaneous users. Next steps are to get this regularly deployed again and to get it on to a normal hail docker image.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6836:66,deploy,deployment,66,https://hail.is,https://github.com/hail-is/hail/pull/6836,2,['deploy'],"['deployed', 'deployment']"
Deployability,There is now just public allocate that takes an alignment and a size. Then update client code.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2568:75,update,update,75,https://hail.is,https://github.com/hail-is/hail/pull/2568,1,['update'],['update']
Deployability,"There is occasional use of this in other projects, e.g., gnomad_methods (`SimpleRichProgressBar` in this case). Do you consider these classes to be part of the API? Was it intended to rename these without any compatibility shim, e.g., having the old names as aliases for a while?. It's not the end of the world and gnomad_methods has already updated accordingly. It does however mean that older gnomad_methods is only compatible with hail ≤ 0.2.125 and newer gnomad_methods is only compatible with hail ≥ 0.2.126, which is an otherwise unnecessary lock-step restriction. ETA: gnomad_methods have now updated by removing the (apparently unused) progress bar reference, so now newer gnomad_methods is compatible with hail both ≤ 0.2.125 and ≥ 0.2.126 again. So this is no longer a significant problem for gnomad_methods, but remains FYI.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13832#issuecomment-1788106993:342,update,updated,342,https://hail.is,https://github.com/hail-is/hail/pull/13832#issuecomment-1788106993,4,['update'],['updated']
Deployability,"There is one issue for the hail alias. The alias refers to; $SPARK_HOME/python/lib/py4j-0.10.3-src.zip However, the py4j zip file; varies from Spark version to spark version. For example, these are the; different versions for spark on our system. /share/pkg/spark/1.2.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.3.1/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.4.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:270,install,install,270,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799,8,['install'],['install']
Deployability,"There remain a couple questions that a solution should answer:; 1. TCP or Unix Domain Socket? Current consensus feels that TCP is a reasonable and more portable way to go (allows for backend deployment over the web/in K8s for example); 2. Should we use just TCP or also use HTTP? If the Java backends can multiplex requests, HTTP sounds favorable, otherwise it's unclear to me what advantages it would give us over TCP + JSON. Ergonomically HTTP might be easier, but one tends have certain default expectations of HTTP servers (I would *assume* an HTTP server should be able to serve requests concurrently, are we just going to use all `POST`s?, etc.). Either way this feels like a minor adjustment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13756#issuecomment-1743481491:191,deploy,deployment,191,https://hail.is,https://github.com/hail-is/hail/issues/13756#issuecomment-1743481491,1,['deploy'],['deployment']
Deployability,"There should be one definition of a service. The router should be; the authority on service definitions. The only exceptions are; self-deployed services: gateway, internal-gateway, and router-resolver. This primarily reduces possibility of error or confusion by removing; duplication. It does not impair hand deploying of any service because; every service (except the self-deployed ones) needs the router anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8504:135,deploy,deployed,135,https://hail.is,https://github.com/hail-is/hail/pull/8504,3,['deploy'],"['deployed', 'deploying']"
Deployability,"There was a typo in the `Interpret` rule for `TableAggregate` which had it refer to the row instead of the globals inside the init op. I tried to add a test for this, but it's frustratingly difficult to force the compiler to go through this code path. Even when using an `InterpretOnly` compilation, the lowering pipeline often lifts `TableAggregate` to a `RelationalLet`, and then evaluates it, using the compiler not the interpreter. This is a deeper issue we should address, but a user is currently blocked on this bug so I don't want to hold it up.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14673:313,pipeline,pipeline,313,https://hail.is,https://github.com/hail-is/hail/pull/14673,1,['pipeline'],['pipeline']
Deployability,"There was some real bad capture/broadcast issues in VCFsReader. I made the following changes:. - import_vcfs requires the signature of all files to be the same,; - compute the type once from the first file,; - verify the types agree when parallelizing over files computing the partitions,; - always broadcast the header lines (which can be large). This reduced the DAGScheduler RDD broadcast by about 4x (6MB => 1.4MB) on a simple 10-input pipeline of import_vcfs/transform_one/combine/write_multi.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5507:440,pipeline,pipeline,440,https://hail.is,https://github.com/hail-is/hail/pull/5507,1,['pipeline'],['pipeline']
Deployability,"There will probably still be a lot of partitions, but those that remain only include rows with keys matching those in `pcloadings`. This pipeline includes the special bgen variant filtering I added.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953#issuecomment-406017529:137,pipeline,pipeline,137,https://hail.is,https://github.com/hail-is/hail/issues/3953#issuecomment-406017529,1,['pipeline'],['pipeline']
Deployability,"There's a few things happening here:. ### Node pool updates through terraform; I extended the node pool update documentation with how to deal with terraform-managed node pools. This is what I did on Azure and worked fine. The only real change in terraform other than changing the machine type is making the node pool name configurable to adhere to the naming guidelines and allow us to do a rolling migration. ### Updated the kubernetes and azurerm providers; I updated the azurerm provider without thinking much about it and even though it's a minor version had some breaking changes that after a half-successful `apply` made it hard to downgrade. So I decided just to appease the breaking change and leave us at the new version, which is what all the `blob_properties` changes are for. They are in no way related to the node pools. ### Troubleshooting; I added a section for a bug that I've seen a couple of times (and encountered again today) but never documented how I got around it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11636:52,update,updates,52,https://hail.is,https://github.com/hail-is/hail/pull/11636,5,"['Update', 'rolling', 'update']","['Updated', 'rolling', 'update', 'updated', 'updates']"
Deployability,There's a kind of unrelated thing: Fix reading of configuration information to not ignore a hailctl configuration value of `''` . The big change is to introduce 3 progress bar systems:; 1. SimpleRichProgressBar. One progress bar active at a time.; 2. RichProgressBar. More than one progress bar active at a time.; 3. BatchProgressBar. Same as RichProgressBar but with default columns good for monitoring 1 or more Hail Batch batches.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12346:50,configurat,configuration,50,https://hail.is,https://github.com/hail-is/hail/pull/12346,2,['configurat'],['configuration']
Deployability,"There's also #11428, which just merged and I forgot to write the changelog message for:. `hailtop.batch.build_python_image` now accepts a `show_docker_output` argument to toggle printing docker's output to the terminal while building container images",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11795#issuecomment-1109887634:171,toggle,toggle,171,https://hail.is,https://github.com/hail-is/hail/pull/11795#issuecomment-1109887634,1,['toggle'],['toggle']
Deployability,"There's more work to be done here. This adds a new route to the batch UI for getting a certain job group within a batch. It then, instead of listing all jobs, only lists the jobs that belong directly to the currently viewed job group and also shows the child job groups of the current job group. When picking up this PR I would make sure to go through the Batch development tutorial to make sure you are familiar with dev deploying. Then, read [this](https://github.com/hail-is/hail/blob/main/dev-docs/development-process.md#alternatives-to-dev-deploy) to learn about all the ways you can avoid dev deploying 😄 . If you are only making tweaks in the HTML templates, you don't need to keep deploying for every little change. Instead, run. ```bash; make devserver SERVICE=batch; ```. in your terminal and you'll get a local server that proxies the Batch that your `hail` installation is pointed to. You can then make changes to HTML and refresh your browser to see the results. Note that this is just rendering the HTML locally, and will have any effect on what's deployed, meaning you can't use it for python changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14600#issuecomment-2239854998:422,deploy,deploying,422,https://hail.is,https://github.com/hail-is/hail/pull/14600#issuecomment-2239854998,12,"['deploy', 'install']","['deploy', 'deployed', 'deploying', 'installation']"
Deployability,There's some nice QoL updates and new chart types,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10598:22,update,updates,22,https://hail.is,https://github.com/hail-is/hail/pull/10598,1,['update'],['updates']
Deployability,"There's something wrong here and I think it has to do with how I'm passing around the hadoop configuration. Closing this since I'm in the process of changing how spark is called, and I'll reopen once I've fixed this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5326#issuecomment-467585906:93,configurat,configuration,93,https://hail.is,https://github.com/hail-is/hail/pull/5326#issuecomment-467585906,1,['configurat'],['configuration']
Deployability,"There's something wrong with this PR. The database migration step says it's successful, but the new database is never actually created. I think this is the same thing I saw with dev deploy and attributed it to the wrong cause. ```; +------------------------------------+; | Database |; +------------------------------------+; ...; | pr-9241-auth-zdyt4a4geys7 |; | pr-9241-batch-y0qvw1vpniad |; | pr-9241-ci-nkuua31y7nxn |; | pr-9241-test-instance-zxxeu6gotctw |; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9241#issuecomment-677664900:182,deploy,deploy,182,https://hail.is,https://github.com/hail-is/hail/pull/9241#issuecomment-677664900,1,['deploy'],['deploy']
Deployability,These are no longer relevant with the advent of pip installation.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8560:52,install,installation,52,https://hail.is,https://github.com/hail-is/hail/pull/8560,1,['install'],['installation']
Deployability,"These changes are a prerequisite for introducing a mysql DB pod for every test and dev namespace. The crux of such a change is any CreateDatabase steps should use the `database-server-config` from *its own namespace* (which will come in the PR that uses this step) and not from default. There's no cleanup step required because this will be used to create DBs inside the namespace for the pipeline, so resources will get cleaned up with the namespace. The other changes in this step bring the configuration for `dev` scopes closer to that of `test` scopes, because creation of test databases should really just be idempotent and there shouldn't be a difference between deploying a database in dev and test. I would have deferred making the changes to the `dev` scope except dev deploy was the most practical way for me to test this change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13022:389,pipeline,pipeline,389,https://hail.is,https://github.com/hail-is/hail/pull/13022,4,"['configurat', 'deploy', 'pipeline']","['configuration', 'deploy', 'deploying', 'pipeline']"
Deployability,These changes should be integrated into the next cloud tools package deployment. Not sure whether the changes should be here or the old cloud tools repo.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6118:24,integrat,integrated,24,https://hail.is,https://github.com/hail-is/hail/pull/6118,2,"['deploy', 'integrat']","['deployment', 'integrated']"
Deployability,"These docstrings weren't updated since the GCP multi-regional egress changes. It is no longer free to egress from/to multi-regional buckets, so the best thing to do would be to put your storage and compute into the same region.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14508:25,update,updated,25,https://hail.is,https://github.com/hail-is/hail/pull/14508,1,['update'],['updated']
Deployability,"These functions are now only used in tests. I re-implemented computeRRM via expression language, interim to deleting entirely on the Scala side once LMM is updated and KinshipMatrix goes away. Note `realized_relationship_matrix` is tested in `test_rrm` on the Python side, and computed independently of the ComputeRRM code. I also moved RichMatrixTable to testUtils alongside RichTable, rather than with the Suites in utils.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3287:156,update,updated,156,https://hail.is,https://github.com/hail-is/hail/pull/3287,1,['update'],['updated']
Deployability,"These make targets have been more trouble than they are worth. Since both the requirements files and the pinned requirements files that they generate are checked into the repo, updates to these files through PRs don't play well with how make tracks timestamps, since my local git may update them in different orders. In general, I've never had a need to only update a single requirements file and since these are technically dependencies of say, the Batch image, building the batch image can incidentally trigger a re-generation of these files when I never actually changed the dependencies. This PR removes those targets and does a normal refresh of the requirements, as well as updates the documentation on how we handle pip dependencies.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14604:177,update,updates,177,https://hail.is,https://github.com/hail-is/hail/pull/14604,4,['update'],"['update', 'updates']"
Deployability,These should never have been read-write. Caught this because a CI job I was modifying overwrote `/gsa-key/key.json` with `/test-gsa-key/key.json` which caused the Output step to use the test credentials instead of CI credentials. I also removed an overriding definition of `secret_host_path` in `JVMJob`. I don't see why it should be different than what's defined in `Job` and using `host_path` seems quite dangerous. Added a test that we can't `mv` a secret path and updated some existing tests that assumed we can overwrite secrets. TODO: Update `build.yaml` to not `mv` any secrets or PRs will fail when this joins the mainline.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11931:468,update,updated,468,https://hail.is,https://github.com/hail-is/hail/pull/11931,2,"['Update', 'update']","['Update', 'updated']"
Deployability,These strict pins belong in the pinned-requirements.txt not as a requirement for our users. We have a different PR which updates numpy to 1.24.2 https://github.com/hail-is/hail/pull/12898,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12895#issuecomment-1515071184:121,update,updates,121,https://hail.is,https://github.com/hail-is/hail/pull/12895#issuecomment-1515071184,1,['update'],['updates']
Deployability,"These three tests call out to [Plink](https://www.cog-genomics.org/plink2). If you install it from that link and add it to your global path, those tests will pass. If that sounds annoying, you can trust that these tests pass on our end!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-229943619:83,install,install,83,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-229943619,1,['install'],['install']
Deployability,These upgrades are simple and do not represent major version upgrades.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10830:6,upgrade,upgrades,6,https://hail.is,https://github.com/hail-is/hail/pull/10830,2,['upgrade'],['upgrades']
Deployability,"These versions came from the current base image, from `pip freeze`. `pur` seems like the thing to compute updates: https://github.com/alanhamlett/pip-update-requirements. In particular, `pur -r requirements.txt` will update the requirements file with the latest version, showing what has changed. You can do `pur -o /dev/null -r requirements.txt` if you just want to see what's out of date. Here is `pur` for this change:. ```; $ pur -o /dev/null -r requirements.txt ; Updated gcsfs: 0.2.1 -> 0.2.2; Updated urllib3: 1.24.3 -> 1.25.3; ```. I'm not including updates in this PR, just pinning the current versions. I think we should have a job that regularly (weekly?) PRs an updated requirements.txt.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6436:106,update,updates,106,https://hail.is,https://github.com/hail-is/hail/pull/6436,7,"['Update', 'update']","['Updated', 'update', 'update-requirements', 'updated', 'updates']"
Deployability,They had not been updated when the module structure changed.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8464:18,update,updated,18,https://hail.is,https://github.com/hail-is/hail/pull/8464,1,['update'],['updated']
Deployability,"They weren't documented in any release yet:; https://hail.is/docs/0.2/api.html#top-level-functions. I'm OK removing them, since they won't really break pipelines, only citations. And am happy to be bothered by people asking questions about that!. If you feel strongly, let's add the code aliases back but not the documentation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6552#issuecomment-507804081:31,release,release,31,https://hail.is,https://github.com/hail-is/hail/pull/6552#issuecomment-507804081,2,"['pipeline', 'release']","['pipelines', 'release']"
Deployability,"Things that remain to be done:; - overriding repartition. This isn't super trivial because if you have a huge pipeline that ends in a repartition down to 10 partitions, you're going to get 10 cores for the whole job.; - better ordering process on VCF import; - speed up joins by skipping ahead if the 'right' is behind",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/555#issuecomment-238118077:110,pipeline,pipeline,110,https://hail.is,https://github.com/hail-is/hail/pull/555#issuecomment-238118077,1,['pipeline'],['pipeline']
Deployability,"This PR _explicitly_ changes the defaults. It optionally accepts the old values in an _attempt_ to not break current users. At worst, this would require another `hailctl auth login`. This is the patch I _wanted_ to write. ```patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 't",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:195,patch,patch,195,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,3,"['PATCH', 'patch']","['PATCH', 'patch']"
Deployability,"This PR achieves a couple of things in preparation for #11617, which requires that QoB jars be built against Spark 3.2. QoB doesn't actually *use* spark, but it will use the Azure Blob Storage client library which requires a version of jackson that conflicts with that of pyspark 3.1.1. Until such a day that we can remove pyspark from the QoB environment, we upgrade it to appease our java dependencies. Non-QoB builds of hail query remain on Spark 3.1.1/3.0.1 as it is what dataproc/hdinsight support, on which we don't need to use the Azure storage fs. This PR moves the installation of pyspark out of the base image so we can make a spark 3.2 image on which to run the scala fs suite. We also build a spark 3.2 jar to be used in that test. Also update a couple of dependencies to be compatible with spark 3.2 and stop pulling in jackson from the gcs dependency, which just so happens to match spark 3.1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11790:360,upgrade,upgrade,360,https://hail.is,https://github.com/hail-is/hail/pull/11790,3,"['install', 'update', 'upgrade']","['installation', 'update', 'upgrade']"
Deployability,"This PR adds a new column to the `resources` table (resource_id) as well as to the `attempt_resources` table. The `resource_id` represents the mapping from the string resource name to an integer identifier. To make this migration work, we first need to add the new columns to the corresponding tables along with a new trigger. The trigger makes sure any writes that occur after we start populating the older values will have the new resource_ids filled into the `attempt_resources` table. I decided to not convert the `attempt_{batch, job,billing_project}_resources` tables to have the resource_id because we'll drop them anyways in the future and it was adding 75% more time to the migration. My original script had the updates happening to all tables, hence why there's overkill for the way it's structured. I can try and refactor the code back to the way it was before if you'd like. The code to insert into the `attempt_resources` table in `job.py` needs to insert both the resource and the resource_id because we do not want to rely on the new trigger for adding the resource_id to the record so we can remove the resource column later on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12028:721,update,updates,721,https://hail.is,https://github.com/hail-is/hail/pull/12028,1,['update'],['updates']
Deployability,"This PR adds a new token and time_completed column to the `batches_n_jobs_in_complete_states` table. The reason for doing this is twofold:. 1. This will get rid of the serialization in MJC where every job needs to update the same row in the `batches_n_jobs_in_complete_states` table.; 2. We also push marking the batch complete to outside of the MJC stored procedure as a separate database query. The current code for marking a batch complete won't work for job groups (or at least not without a complicated query I don't know how to easily write) because the job group completion state check needs to be vectorized up the ancestors tree. The mechanism for how this works is there is a new `mark_batch_complete` function in Python that is executed after every MJC. There's also a periodic loop that looks for batches that are complete (n_jobs == n_completed), but have not had state = ""complete"". This makes sure we eventually update the batch state when complete even if MJC is interrupted. The new `mark_batch_complete` function optimistically tries to update the batch state and time_completed if `n_jobs == n_complete AND state != ""complete""`. If an update occurs, then the function issues a callback if specified; otherwise, it just returns. `time_completed` is tracked as a new column on the `batches_n_jobs_in_complete_states` table where we take the GREATEST timepoint when updating a row on duplicate key. Then the time_completed is just equal to the MAX of all entries in the table for that batch. I'm still proving backwards compatibility to myself without manually copying time_completed to the `batches_n_jobs_in_complete_states` table from the `batches` table. I believe that if we have a running batch:. (A) MJC in SQL uses the new token column when inserting into `batches_n_jobs_in_complete_states`, but all new entries are always inserted at token=0 so the original SQL queries currently running on the server execute the same. However, we no longer mark the batch as complete or set",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513:214,update,update,214,https://hail.is,https://github.com/hail-is/hail/pull/13513,2,['update'],['update']
Deployability,"This PR adds support for Azure SAS tokens in QoB. A SAS token is basically a blob storage URI with a short-lived credential to access that resource appended as a URL query string. In such a scenario where the FS receives a blob URI with a SAS token, that token should be used instead of the latent credentials on the system. Most of the changes to the `AzureStorageFS.scala` are to parse out a SAS token from blob names. This change brings with it a couple caveats. Unfortunately it is not possible to truly disambiguate a SAS token from a glob pattern, or even just a normal blob filename. So we take what is probably a safe assumption and look to see if there exists a query-parameter style key-value pair after the last `?` in the blob name. If this is the case, we treat everything after the last `?` as a SAS token. If this condition is not satsified, we say there is no SAS token and treat the whole path as the blob name. This logic already exists in python, but I'm open to alternatives. Introducing SAS tokens also breaks the way globbing is currently implemented, where it is deemed safe to iteratively append components to the end of a blob URI string. I added an abstract type member to `FS` and instead of a `String` have `globWithPrefix` accept that associated URL type that can properly handle path updates. I'm unclear on the best way to do this w.r.t. the type system, and wasn't quite sure what to put as the associated type for `RouterFS`, which ideally would accept a union of the URL types for the filesystems that it wraps, so some guidance on that would be great if you have any.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13178:1314,update,updates,1314,https://hail.is,https://github.com/hail-is/hail/pull/13178,1,['update'],['updates']
Deployability,"This PR adds the job groups functionality as described in this [RFC](https://github.com/hail-is/hail-rfcs/pull/5) to the Batch backend and `hailtop.batch_client`. This includes supporting nested job groups up to a maximum depth of 5. Note, that none of these changes are user-facing yet (hence no change log here). The PRs that came before this one:; - #13475 ; - #13487 ; - #13810 (note that this database migration required a shutdown). Subsequent PRs will need to implement the following:; - Querying job groups with the flexible query language (v2); - Implementing job groups in the Scala Client for QoB; - Using job groups in QoB with `cancel_after_n_failures=1` for all new stages of worker jobs; - UI functionality to page and sort through job groups; - A new `hailtop.batch` interface for users to define and work with Job Groups. A couple of nuances in the implementation came up that I also tried to articulate in the RFC:; 1. A root job group with ID = 0 does not belong to an update (""update_id"" IS NULL). This means that any checks that look for ""committed"" job groups need to do `(batch_updates.committed OR job_groups.job_group_id = %s)` where ""%s"" is the ROOT_JOB_GROUP_ID.; 2. When job groups are cancelled, only the specific job group that was cancelled is inserted into `job_groups_cancelled`. This table does **NOT** contain all transitive job groups that were also cancelled indirectly. The reason for this is we cannot guarantee that a user wouldn't have millions of job groups and we can't insert millions of records inside a single SQL stored procedure. Now, any query on the driver / front_end must look up the tree and see if any parent has been cancelled. This code looks similar to the code below [1].; 3. There used to be `DELETE FROM` statements in `commit_batch_update` and `commit_batch` that cleaned up old records that were no longer used in `job_group_inst_coll_cancellable_resources` and `job_groups_inst_coll_staging`. This cleanup now occurs in a periodic loop on",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14282:988,update,update,988,https://hail.is,https://github.com/hail-is/hail/pull/14282,1,['update'],['update']
Deployability,"This PR attempts to simplify the use of TLS and HTTP(S) in Hail. The big changes; are in `hail/python/hailtop`. In particular I removed several functions with; confusingly overlapping functionality in `tls.py`. Instead, we now have three; functions:. - `internal_server_ssl_context`; - `internal_client_ssl_context`; - `external_client_ssl_context`. The client context is configured to seek certificates from its peers. Both; internal contexts load the Hail certificate chain specified in the Hail SSL; Config. The external client context does not load the hail certificate chain. I intend all Hail's HTTP(S) requests to use `httpx.py` (so named to not conflict; with modules named `http`). Again, I have simplified the landscape. We now have; two functions:. - `httpx.client_session`: The constructor for all asynchronous, HTTPS client; sessions.; - `httpx.blocking_client_session`: The constructor for all synchronous, HTTPS; client sessions. Both sessions have the exact same configuration parameters. The API is exactly; the same except the blocking client session replaces asynchronous methods with; synchronous ones. Both sessions accept the `aiohttp.ClientSession` constructor parameters. They; support one new parameter and modify the behavior of one old parameter.; - `retry_transient`: when set to `True` this parameter will retry all transient; errors in all requests made by this session. This defaults to `True`.; - `raise_for_status`: this parameter now defaults to `True` and includes the; response body text in the error message. Both; Both parameters may be overridden on a per-request basis. - `httpx.ResponseManager` and `httpx.ClientSession` work together to enable; `retry_transient` and `raise_for_status`. Aiohttp has this unusual structure where; all the request methods are synchronous but they return an object that is both; awaitable and an async context manager. I mirror their structure exactly. The; `httpx.ResponseManager` is both awaitable and an async context manager.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9554:979,configurat,configuration,979,https://hail.is,https://github.com/hail-is/hail/pull/9554,1,['configurat'],['configuration']
Deployability,"This PR changes our vep init scripts to pull from the new `hail-us-vep` google bucket, which is requester pays. . I realized while doing this that my previous PR (#8253) really had nothing to do with supporting this, since that really just sets the configuration for the Google Cloud / Hadoop connector, which is not how we get vep data. I tried to enforce the rules from those command line arguments anyway by rejecting `--vep` flag if they don't specify they're ok with requester pays and manually checking that `hail-us-vep` was in their approved bucket list. But if a user was to specify the init scripts using `gcloud dataproc` and didn't go through `hailctl`, there'd be no catch to check if they were ok with requester pays. Perhaps there is some way we could set environment variables on the dataproc machines based on the `--requester-pays-allow....` flags and use those in the init scripts. I also expanded `make test-dataproc` to test with a GRCh38 cluster as well, as we use separate scripts to make them and I'm uncomfortable with only testing one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8268:249,configurat,configuration,249,https://hail.is,https://github.com/hail-is/hail/pull/8268,1,['configurat'],['configuration']
Deployability,"This PR changes the `addresses` function on `DeployConfig` to retry all transient errors. In particular, if the address service is temporarily down (maybe its getting redeployed), this change allows the client to repeatedly retry until the address service comes back to life. I also added some type annotations to `retry_transient_errors`. It takes a function that returns something we can `await` and then applies that function in a loop until it does not raise an error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9200:45,Deploy,DeployConfig,45,https://hail.is,https://github.com/hail-is/hail/pull/9200,1,['Deploy'],['DeployConfig']
Deployability,"This PR changes the semantics of TableMapRows in the IR and Table.select. They are now required to preserve the key ordering. In other words, applying TableMapRows to an ordered table can produce an ordered table without shuffling. I put an assert to verify that the produced OrderedRVD is really ordered, but that might be too expensive an assertion. Before, `newKey = None` meant ""keep the old key"", but that left no room to ask for the result to be unkeyed. Now `newKey` is just the key of the resulting table. If any partition key fields are modified, then even if the ordering is preserved, the result will need to be scanned to update the partition bounds. To avoid this scan when unnecessary, I added the `preservedKeyFields` parameter, which is the length of the prefix of key fields which are not modified. Thus, if the number of partition keys of the underlying OrderedRVD is less than or equal to `preservedKeyFields`, the partition bounds will remain valid. This feels pretty clunky, and I welcome better ideas. In particular, is there a way to compute this from the `newRow` IR? That's what I did on the Python side in `_select`, but it wasn't obvious to me how to do the same with the Scala IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3622:634,update,update,634,https://hail.is,https://github.com/hail-is/hail/pull/3622,1,['update'],['update']
Deployability,This PR contains some fixes to the GCP deployment scripts. LMK if you want me to pull those into a separate PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12751#issuecomment-1458579094:39,deploy,deployment,39,https://hail.is,https://github.com/hail-is/hail/pull/12751#issuecomment-1458579094,1,['deploy'],['deployment']
Deployability,"This PR does not move us closer to testing and releasing built-once binaries. This is an important goal for me, but automating the deploy process as it exists is a prerequisite.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8550#issuecomment-613603023:131,deploy,deploy,131,https://hail.is,https://github.com/hail-is/hail/pull/8550#issuecomment-613603023,1,['deploy'],['deploy']
Deployability,"This PR enables shuffling in the service. It is stacked on several other PRs, so look only at the; most recent commit. Some highlights:; - Open the public network back up. We should probably make query jobs special so that they can; access the internal network. To do that, batch would need to accept a ""acting on behalf of"" user; account: Query submits the job using its account ""acting on behalf of"" the user. Batch allows; query to use the private network, but for all other purposes, the job is owned by the user. - Allow public access to some the `gcr.io/hail-vdc/query` Docker image. - Automatically rewrite uses of `hailgenetics/` Docker images to their `gcr.io` equivalents. - Move `deploy_address` above `deploy_query` so that query can depend on address (necessary for; shufles). - Fix logging configuration. Services team wants all logs all the time to go to stdout. - Implement lowerDistributedSort using the shuffler. - Allow shuffle ids to be encoded so they can be used in `Literal`.; Unified Split",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9848:804,configurat,configuration,804,https://hail.is,https://github.com/hail-is/hail/pull/9848,1,['configurat'],['configuration']
Deployability,"This PR fixes a problem where the database state for the instance didn't match the in-memory state for the instance. For example, in-memory the state was 'inactive' while in the database it was 'deleted'. We're not sure why that happens yet. There are 4 possible states: pending, active, inactive, deleted. Rather than failing when this happens (causing an infinite retry loop), we check the return code from the database and act accordingly. . For `deactivate`, the return code will be non-zero only if the instance is inactive or deleted. I thought about making the in-memory state match the state in the database explicitly, but I think it's safer to keep the current behavior where the in-memory state is ""inactive"". The state will be fixed to deleted when the callers of deactivate eventually call `mark_deleted`. Likewise, `mark_deleted` expects the state to be inactive. I thought about handling the case where the db is pending or active and the in-memory state is inactive and realized that could never happen. Therefore, I just assert the db state must be already deleted and update the state appropriately.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8254:1086,update,update,1086,https://hail.is,https://github.com/hail-is/hail/pull/8254,1,['update'],['update']
Deployability,"This PR implements the core IBS operations in terms of vectorized C code. In particular, we use the `libsimdpp` library to take advantage of whatever the widest available register is (many modern CPUs have AVX2 256 bit integer registers; Knights Landing will introduce AVX512 512-bit integer registers). The performance improvement is massive. We can compute the full IBD matrix on 2,535 samples and ~37 million variants in just under 17 minutes. We believe the complexity of this code is `O(nSamples^2 * nVariants)`. Assuming the scaling works out well, we should be able to compute 100,000 Variants and 40,000 samples in the same time. There were a couple issues I had to workaround, but hopefully we can re-use those workarounds:. - compiling native code from gradle; - packaging native code for `test`, `installDist`, and the JARs; - building native code specialized to certain architectures. Still left to do:. - [x] break the C tests into a separate file and call from gradle `test`; - [x] maybe use a library ([libsimdpp?](https://github.com/p12tic/libsimdpp)) to do the SIMD so we're agnostic to the underlying architecture (right now if you don't have AVX, we fall all the way back to 64-bit registers, rather than 128-bit SSE registers) ; - [x] some minor clean up of the IBSFFI class. Future Work:; - implement IBSExpectations in C as well; - expand this work to KING (or other structure correcting IBD calculations)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092:808,install,installDist,808,https://hail.is,https://github.com/hail-is/hail/pull/1092,1,['install'],['installDist']
Deployability,"This PR incorporates @cseed 's changes from #3477, brings everything up to date with master, and adds/fixes the following:; - added a test for linreg with no covariates against R, and deleted old `test_linear_regression_with_no_cov` since that still had intercept.; - extended Skat to work without covariates and added test that it runs, but it’s hard to test result against R given that the latter fails with no covariates: `Error in solve.default(t(X1) %*% X1) : 'a' is 0-diml`. The result look ""reasonable"" to me.; - added req of at least one covariate for logreg in doc and code. It's going to be painful to get logistic to take no covariates, we can always come back to it if priority goes up. Related fun breeze behavior: `a(::, *) *:* b` with `a` an `(n, 0)` matrix and `b` an `n`-vector has dimensions `(0, 0)`.; - removed default value of empty list for `covariate`, both to help signal users to consider putting in the intercept (pipelines currently using intercept only with default empty `[]` will break) and because empty is not currently valid for logreg.; - noted in docs that intercept must be included explicitly.; - added comment of R code against which linreg and logreg are testing",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4067:940,pipeline,pipelines,940,https://hail.is,https://github.com/hail-is/hail/pull/4067,1,['pipeline'],['pipelines']
Deployability,"This PR introduces Search v2.0 for querying jobs in a batch with the REST interface. The UI additions will come in a separate PR as well as the upgraded search for querying batches. We can decide whether to upgrade CI to v2. I think for backwards compatibility we need to have the default remain ""1"" in perpetuity in the batch client and you have to explicitly say you want version 2 as an argument if you want the new query language. If we want to add tests for the SQL, I can do that once what I have currently is passing and we're happy with the code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12748:144,upgrade,upgraded,144,https://hail.is,https://github.com/hail-is/hail/pull/12748,2,['upgrade'],"['upgrade', 'upgraded']"
Deployability,"This PR introduces a couple of new concepts:; - InstanceCollection which is a generic class that keeps track of instances in its collection; - Pool which is a shared, growable pool of instances with a control loop and fair share scheduler and is a subclass of InstanceCollection; - InstanceCollectionManager which keeps track of all instance collections. The cancel code remains the same and operates globally and is now in canceller.py. The GCE event monitor is in gce.py. The original instance_pool.py code has been split amongst zone_monitor.py, pool.py, gce.py, create_instance.py, and instance_collection.py. The scheduler code is now in PoolScheduler in pool.py. The SQL code has vectorized user_resources by instance_collection as well as batch_cancellable_resources and batches_staging. There are also two new tables: inst_colls and pools. Each job and instance must belong to an instance collection noted by the field `inst_coll`. The job_update trigger had to be updated to insert into user_resources to the correct pool. The cancel_batch and close_batch functions changed to vectorize by instance collection. I deleted the global `ready_cores` table. The front end code does not change except looking for a `worker_type` field in the resources field of the job spec (default if undefined is standard). I added a PoolSelector class which is overkill for now, but will be used in the future for more complicated scenarios. There was an issue with our existing code for converting between memory in bytes to memory in MB in the worker_config.py code for the `resources()` function. For the highcpu case, it is impossible for the memory in bytes to be divisible by 1024**2. The utils.py code now rounds up bytes using math.ceil. The hailtop.batch library adds a `worker_type` method on Job. I didn't change the interface significantly at this time as I think this is fine for now. More significant changes will come when we change how cpu and memory and storage are interpreted by the worker. I",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9832:973,update,updated,973,https://hail.is,https://github.com/hail-is/hail/pull/9832,1,['update'],['updated']
Deployability,"This PR introduces a new test case that was failing. It fixes two problems:. 1. needed to bind `n` to `blockSize` so that it didn't serialize the whole IR. ; 2. needed to add references to the regions from producer to `targetRegion` to ensure that the filter test passes. . Question: Does this keep too much garbage in memory? Ideally, I'd release the references to the regions of my producer once I finished constructing the new `RegionValue`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7952:340,release,release,340,https://hail.is,https://github.com/hail-is/hail/pull/7952,1,['release'],['release']
Deployability,"This PR introduces a new type `Name` for representing bound variables in the IR, replacing `String`. For now, it is just an `AnyVal` wrapper around `String`, but in the future I would like to take advantage of the new type. For example, I'd like to:; * change equality of `Name` from string comparison to comparing object identity with `eq`. That way `freshName` becomes just `new Name()`, with stronger guarantees that the new name doesn't occur anywhere in the current IR, without needing to maintain global state as we do now.; * get rid of `NormalizeNames`, instead enforcing the global uniqueness of names as a basic invariant of the IR (typecheck could also check this invariant); * keep a string in the `Name`, but no longer require it to be unique. Instead it's just a suggestion for how to show the name in printouts, adding a uniqueifying suffix as needed. With `NormalizeNames` gone, this would let us preserve meaningful variable names further in the lowering pipeline.; * possibly keep other state in the `Name`, for example to allow a more efficient implementation of environments, similar to the `mark` state on `BaseIR`. This is obviously a large change, but there are only a few conceptual pieces (appologies for not managing to separate these out):; * attempt to minimize the number of locations in which the `Name` constructor is called, to make future refactorings easier; * add `freshName()`, which just wraps `genUID()`, returning a `Name`; * convert IR construction to use the convenience methods in `ir.package`, which take scala lambdas to represent blocks with bound variables, instead of manually creating new variable names; * replace uses of the magic constant variable names (`row`, `va`, `sa`, `g`, `global`) with constants (`TableIR.{rowName, globalName}`, `MatrixIR.{rowName, colName, entryName, globalName}`); * the above changes modified the names we use for bound variables in many places. That shouldn't matter, but it cought a couple bugs where it did.; * `Normal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14547:972,pipeline,pipeline,972,https://hail.is,https://github.com/hail-is/hail/pull/14547,1,['pipeline'],['pipeline']
Deployability,"This PR is a step towards a general picture for generating debugging information in bytecode. The general picture is to write a sequence of files, each corresponding to a certain point in the compile pipeline, where each line in each file includes a line number pointing to the line in the previous file from which this line was derived. (We may eventually want richer source information than just a single line number, like a range or list of ranges.) The top of each file has the file name of the previous printout, which is the target of all line numbers in the file. We could in the future also print a list of transformations that were applied to get from the previous printed state to this one. The idea to implement this picture is simple. Each IR node stores a line number in a mutable variable. When we want to generate a printed checkpoint, we walk the IR, printing a representation of each node, including the stored line number, and then overwriting the node's line number with the current line count of the file being written to. Some work will be required to preserve this source information in all IR transformations. This PR implements this idea in lir only. If the `HAIL_WRITE_IR_FILES` environment variable is set, it is hardcoded to print the lir after the first `SimplifyControl` (because before that is very hard to read), and after method splitting right before emitting bytecode. It also prints out the class files themselves. Longer term we'll want to be able to control which points in the compilation get printed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9521:200,pipeline,pipeline,200,https://hail.is,https://github.com/hail-is/hail/pull/9521,1,['pipeline'],['pipeline']
Deployability,"This PR is almost done. I need to change one of the Makefile rules to include new targets from #5791 and make sure the database is cleaned and test the deploy works. My plan is to continually delete the production tables per deployment until we're happy with the schema and everything is working. Once this is all in, then the next PR will add the actual data to the tables in `server.py` and get rid of the global dictionaries / application state. I might try and do this in 2 stages (jobs and batch), but I'm not sure it's possible. Depends on #5781, #5784",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5809:152,deploy,deploy,152,https://hail.is,https://github.com/hail-is/hail/pull/5809,2,['deploy'],"['deploy', 'deployment']"
Deployability,"This PR is important for making the query service tests run in a reasonable amount of time. 1. msec_mcpu is not used any more, so don't bother. This causes a conflict with *every other job in; the batch*, effectively serializing changes to every attempt in the same batch. 2. There is no need to eagerly take update lock on the instances table in add_attempt. A share lock; prevents other rows from mutating that value while we're looking at it. We do not care if they; read an old value for free_cores_mcpu between us reading `state` and us updating; `free_cores_mcpu`. 3. In `schedule_job` there is no need to take an update lock on the `batches` table (which; effectively serializes all scheduling on the same batch). 4. In `unschedule_job`, I made the formatting match other selects. 5. In `unschedule_job`, same thing about update locks on the `instances` table. 6. In `mark_job_creating`, same thing about update locks on the `batches` table. 7. In `mark_job_started`, same thing about update locks on the `batches` table. 8. In `mark_job_complete`, same thing about update locks on the `instances` table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10367:309,update,update,309,https://hail.is,https://github.com/hail-is/hail/pull/10367,6,['update'],['update']
Deployability,"This PR is needed for atomic batch creation where the job_id is 0 to n_jobs and is not unique across batches. - Job now has a compound key in the database (batch_id, job_id); - Job.id on the server side is `(batch_id, job_id)`; - Batch on the client side has been modified to not require the attributes since Job now needs a reference to the batch.; - Changed some routes to be '/batches/{batch_id}/jobs/{job_id}/...`; - Changed the appropriate places in ci2 and pipeline that use the job status interface; - Changed the create_job interface for parents to take the Job objects and not the id. I think this is clearer and will allow me to check the batch of the Job is the correct batch in the future.; - deleted creating batch from a file",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6183:463,pipeline,pipeline,463,https://hail.is,https://github.com/hail-is/hail/pull/6183,1,['pipeline'],['pipeline']
Deployability,This PR is stacked on #9593. The key files to look at are in `docker/hail-ubuntu`. I introduced `hail-apt-get-install` which packages up the `apt-get update` and the removal of temporary files. I also set the number of package-download retries to 5.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9594:110,install,install,110,https://hail.is,https://github.com/hail-is/hail/pull/9594,2,"['install', 'update']","['install', 'update']"
Deployability,"This PR is to enable `hail-az;` file references to contain SAS tokens to enable bearer-auth style file access to Azure storage. Basic summary of the changes:; 	- Update `AzureAsyncFS` url parsing function to look for and separate out a SAS-token-like query string. Note: made fairly specific to SAS tokens - generic detection of query string syntax interferes with glob support and '?' characters in file names; 	- Added `generate_sas_token` convenience function to `AzureAsyncFS`. Adds new `azure-mgmt-storage` package requirement.; 	- Updated `AzureAsyncFS` to use `(account, credential)` tuple as internal `BlobServiceClient` cache key; 	- Updated `AzureAsyncFSURL` and `AzureFileListEntry` to track the token separately from the name, and extend the base classes to allow returning url with or without a token ; 	- Update `RouterFS.ls` function and associated `listfiles` function to allow for trailing query strings during path traversal ; 	- Change to existing behavior: `LocalAsyncFSURL.__str__`no longer returns 'file:' prefix. Done to make `str()` output be appropriate for input to `fs` functions across all subclasses; 	- Updated `inter_cloud/test_fs.py` to generically use query-string-friendly file path building functions; - Updated InputResource to not include the SAS token as part of the destination file name . `test_fs.py` has been updated to respect the new model, where it is no longer safe to extend URLs by just appending new segments with + ""/"" because there may be a query string. But actually running those tests for the SAS case will require some new test variables to allow the test code to generate SAS tokens (`build.yaml/test_hail_python_fs`): ; ```; export HAIL_TEST_AZURE_ACCOUNT=hailtest; export HAIL_TEST_AZURE_CONTAINER=hail-test-4nxei; # Required for SAS testing on Azure; export HAIL_TEST_AZURE_RESGRP=hailms02; export HAIL_TEST_AZURE_SUBID=12ab51c6-da79-4a99-8dec-3d2decc97343; ```; So the SAS case is disabled for now (`test_fs.py`):; ```; @pytest.fixture(param",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12877:162,Update,Update,162,https://hail.is,https://github.com/hail-is/hail/pull/12877,4,['Update'],"['Update', 'Updated']"
Deployability,"This PR moves us from having thread local `RegionPool`s, to ""task local"" `RegionPool`s, based on Spark tasks. This should make it easier for us to track peak memory usage across a pipeline.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9675:180,pipeline,pipeline,180,https://hail.is,https://github.com/hail-is/hail/pull/9675,1,['pipeline'],['pipeline']
Deployability,"This PR only adds support in the worker and exposes the functionality in the batch client. Another PR will be added to make these changes user facing in the user version of batch. I added the WIP tag because I want to make sure the test passes once before I comment it out. That's because when this PR deploys, the workers will still be the old workers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8960:302,deploy,deploys,302,https://hail.is,https://github.com/hail-is/hail/pull/8960,1,['deploy'],['deploys']
Deployability,"This PR removes the `as_array` parameter on `pca` and `hwe_normalized_pca`. The scores and loadings tables are constructed to always have a field of array type, mirroring the eigenvalues array. This simplifies the interface and makes pipeline (and PC indexing) behavior more predictable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3280:234,pipeline,pipeline,234,https://hail.is,https://github.com/hail-is/hail/pull/3280,1,['pipeline'],['pipeline']
Deployability,"This PR should fail because the gsa key is not in the test namespace -- I think we should have a second user account for testing. Still to do is to expose all of the user key infrastructure in the batch `Makefile` and `test-locally` in pipeline and ci. At some point, we should consolidate the `google_storage.py` file so not duplicating with `ci`. . But first I wanted to get feedback. @danking @cseed @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5866:236,pipeline,pipeline,236,https://hail.is,https://github.com/hail-is/hail/pull/5866,1,['pipeline'],['pipeline']
Deployability,"This PR teaches gear/database.py to respect four more MySQL configuration parameters: `ssl-mode`, `ssl-ca`, `ssl-cert`, `ssl-key`. In particular, we can now turn TLS on or off and rotate keys by simply changing secrets and restarting the services. Since all sql-config secrets (except those in my namespace) currently have no certs, no keys, and no ssl parameters, after this PR merges all services will still use plaintext communications to the database. After this PR merges, I will update the root secret as well as all the service secrets (e.g. sql-auth-user-config) to have a shared client cert/key and our sql database's cert. Moreover I will set `ssl-mode` to `VERIFY_CA` which means (in our world, at least) verify the server's certificate but not the hostname (we use IPs to connect to our sql server) and present your own certificate for verification. Then I will restart all the services. Then I will ban plaintext connections to the database. Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. I also:; - updated `create_database.py` so that it will propagate these TLS settings, if present, to created secrets, and; - updated CI to use `gear/database.py` and standard sql-config locations. All these parameters are defined by MySQL. We only support three options for [`ssl-mode`](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode), the remainder are either unnecessary or not supported (e.g. we have no hostnames so `VERIFY_IDENTITY` is irrelevant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8433:60,configurat,configuration,60,https://hail.is,https://github.com/hail-is/hail/pull/8433,4,"['configurat', 'update']","['configuration', 'update', 'updated']"
Deployability,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). ## Changes from Original PR Proposal. ### Root Certificate. I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol def",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:900,deploy,deploys,900,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['deploy'],['deploys']
Deployability,"This PR updates a lot of NDArray code that was using the old `Code[T]` and emit triplet interface in favor of using code builders and `IEmitCode`. This is going to make it easier to update the `PNDArray` interface to not use `Code[Long]` everywhere, among other things. . Before this PR, there existed `NDArrayEmitter`, which did the old thing, and `NDArrayEmitter2`, which was a prototype of a new emitter. . After this PR, a tweaked version of `NDArrayEmitter2` has become the new `NDArrayEmitter`. `outputElement` now returns a `PCode`, and all the missingness problems are handled by carrying a `IEmitCodeGen[NDArrayEmitter]` around throughout the deforesting process, meaning the `NDArrayEmitter` no longer needs internal state about missingness. . I think the diff for `Emit.scala` is going to be pretty confusing. I'd at least opt for a side by side view instead of the intermingled one, as I've mostly implemented the same logic, just on top of our new code builder primitives. . All tests pass, but marking WIP since I'm noticing some slow down in the slice test that I want to look into. . This PR also adds a `get` function to `EmitValue` that gets the underlying `PValue` and moves two functions off of the `PNDArray` interface into `LinalgCodeUtils`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9824:8,update,updates,8,https://hail.is,https://github.com/hail-is/hail/pull/9824,2,['update'],"['update', 'updates']"
Deployability,"This PR updates the LocalBackend to match the behavior of the SparkBackend w.r.t. error handling. . - `handle_java_exception` and `execute` are lifted into the parent file, `Py4JBackend`. ; - Tests in `test_ndarrays` that were marked as failing local tests are now passing, since the only failure was inconsistent handling of errors. . The error handling changes in question here were introduced in #9398",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9569:8,update,updates,8,https://hail.is,https://github.com/hail-is/hail/pull/9569,1,['update'],['updates']
Deployability,"This PR:. - Introduces `StructComparisonOp`, a subclass of `ComparisonOp` that is just for comparing structs. They have an array of sort fields for this purpose.; - Extends `StreamDistribute` to take in a `ComparisonOp`, so that it doesn't just assume data sorted in ascending order.; - Updates `LowerDistributedSort.scala` to actually use the `SortFields` that get passed into it, allowing for arbitrary sort orders.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11270:287,Update,Updates,287,https://hail.is,https://github.com/hail-is/hail/pull/11270,1,['Update'],['Updates']
Deployability,"This PR:. - Switches the unused `ENDArray` to be called `EColumnMajorNDArray`, always writes out a column major thing, never writes out strides. - Makes the `fundamentalType` of `PNDArray` be `PNDArray`, and makes required patches throughout code base to facilitate this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8952:223,patch,patches,223,https://hail.is,https://github.com/hail-is/hail/pull/8952,1,['patch'],['patches']
Deployability,"This PR:. 1. Fixes a mistake in deploy.sh so that files get written to predictable paths. I manually put a file in the hail-common bucket for the purposes of testing this PR. ; 2. Replaces `make_pip_versioned_docs` with `get_pip_versioned_docs`, a step that will usually just pull prerendered docs from `hail-common`, but if it's a deploy, it will pass along the docs that it gets from its input, `make-docs`. . The effect should be that as soon as this merges, tutorials will correctly render again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11314:32,deploy,deploy,32,https://hail.is,https://github.com/hail-is/hail/pull/11314,2,['deploy'],['deploy']
Deployability,"This SLF4J warning is nothing to be worried about -- we see that in our deployment here, and it doesn't affect the running or results in any way. I'll create an issue to resolve this though. Another note about running Hail: if you're running on one machine, using the script in `hail/build/install/hail/bin/hail` created by `gradle installDist` is the thing to do. But if you're deploying on a spark cluster, you're going to need to do the following:. `gradle shadowJar` : this builds a jar in `hail/build/libs/hail-all-spark.jar`, which can be run in parallel using the `spark-submit` command as below:. ```; /usr/bin/spark-submit --executor-memory 48g --executor-cores 24 --class org.broadinstitute.hail.driver.Main ~/hail/build/libs/hail-all-spark.jar --master yarn-client <hail commands here>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-230431412:72,deploy,deployment,72,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230431412,4,"['deploy', 'install']","['deploying', 'deployment', 'install', 'installDist']"
Deployability,"This added node performs the function that appears all over Python pipelines:; ```python; mt = mt.annotate_rows(foo = ht[mt.row_key]); ```; and ; ```; mt = mt.annotate_rows(foo = ht[mt.not_a_key_field]); ```. The code inside the execution is quite horrible when joining on a non-key field, but I expect it to be much faster than the previous Python implementation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3974#issuecomment-408132885:67,pipeline,pipelines,67,https://hail.is,https://github.com/hail-is/hail/pull/3974#issuecomment-408132885,1,['pipeline'],['pipelines']
Deployability,"This adds `test_storage_uri` and `batch_logs_storage_uri` fields in the global config. In GCP, this meant just copying existing GCS bucket names and prepending `gs://`, which I've done in the terraform and manually in default. For azure, in the terraform I add two storage accounts, `batch` and `ci`, with `logs` and `test` containers, respectively. This felt like an intuitive consolidation of containers under storage accounts that would make permissioning cleaner. E.g. the batch service principal has access to the entire batch storage account, but only to the `test` container in the ci storage account. However, I've not thought about it deeper than that so it might be worth some looking into. Luckily this decision has no impact on application code. There's still more to be done in a follow-up PR to replace instances of `hail_test_gcs_bucket_name` with `test_storage_uri`, but I think this takes care of the batch deployment's dependency on GCS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11014:924,deploy,deployment,924,https://hail.is,https://github.com/hail-is/hail/pull/11014,1,['deploy'],['deployment']
Deployability,"This adds a `label` column to pools, which can be used to select a subset of pools to consider when scheduling a job. The label can be specified for each job by setting the `_pool_label` attribute, e.g. `job._pool_label = 'seqr'` will consider all pools that have the `seqr` label. Note: this incurs a DB migration. `batch/sql/add-seqr-pools.sql` is an example for adding a copy of the default preemptible pools, with an additional `seqr` label applied. CPG limits the number of instances in those dedicated pools to prevent long running seqr loading pipeline jobs from starving other batches for resources. #assign services",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11879:551,pipeline,pipeline,551,https://hail.is,https://github.com/hail-is/hail/pull/11879,1,['pipeline'],['pipeline']
Deployability,"This adds a bare-bones status page for the batch2-driver. It includes:; - a Driver dropdown from the Batch2 header item,; - the instance ID and ready cores,; - list of instances with their name, state, free cores, created and last updated time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7458:231,update,updated,231,https://hail.is,https://github.com/hail-is/hail/pull/7458,1,['update'],['updated']
Deployability,"This adds a script that runs an `aiohttp` server on localhost that by default proxies all requests to whatever batch namespace `hailctl` is pointed to but serves static assets and renders all HTML locally. This allows for local editing of HTML / CSS / SCSS files. To run, this branch needs to be deployed in the relevant namespace (to pick up the changes to `web_common.py`), and you need to have the `sass` CLI installed (`brew install sass/sass/sass`). Then run in a separate terminal `make devserver SERVICE=batch` and open up `localhost:8000`. If you want to supply fake data instead of proxying an endpoint to the k8s deployment of the service, you can override that endpoint in `dev_proxy.py`. If this looks good, I'll add the above instructions to the dev docs. Resolves #13629",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13632:296,deploy,deployed,296,https://hail.is,https://github.com/hail-is/hail/pull/13632,4,"['deploy', 'install']","['deployed', 'deployment', 'install', 'installed']"
Deployability,"This adds all WIP-labelled PRs to the Awaiting Action table on the CI's /me page. CI has automated testing in-so-far as we create a test repository on GitHub with some toy code in it and see that CI can deploy that code into Kubernetes. This stuff not so much, but I can for example put a CI in my namespace and make a PR to its test repository, add a wip tag, and see the table change at `internal.hail.is/dgoldste/ci/me`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11290:203,deploy,deploy,203,https://hail.is,https://github.com/hail-is/hail/pull/11290,1,['deploy'],['deploy']
Deployability,"This adds apiserver to CI, and modifies notebook to use the hail-jupyter image that the former defines. To ensure that it's using the appropriate image, notebook2's Makefile has been modified to read the [""hail-jupyter-image"" label that the apiserver deployment defines](https://github.com/hail-is/hail/blob/master/apiserver/deployment.yaml.in#L74); - the Makefile tests that we get a non-empty string from the kubectl command used to read this value before proceeding. Stacks on https://github.com/hail-is/hail/pull/5592, without which notebook2 wouldn't work when also defaulting to hail-jupyter. I used the ""dependencies"" property in `projects.yaml` as well as presenting the proper order, but that doesn't look like it's actually used yet. Still, it seems like a good idea, so if that is planned still I would like notebook2 to take advantage. ```; ..//env-setup.sh:for project in $(cat projects.yaml | grep '^- project: ' | sed 's/^- project: //'); ..//env-setup.sh-do; ..//env-setup.sh- if [[ -e $project/environment.yml ]]; ..//env-setup.sh- then; ..//env-setup.sh- ${CONDA_BINARY} env create -f $project/environment.yml || ${CONDA_BINARY} env update -f $project/environment.yml; ..//env-setup.sh- fi; ..//env-setup.sh-done; ```. ```; ..//hail-ci-build.sh:PROJECTS=$(cat projects.yaml | grep '^- project: ' | sed 's/^- project: //'); ..//hail-ci-build.sh-; ..//hail-ci-build.sh-for project in $PROJECTS; do; ..//hail-ci-build.sh- if [[ -e $project/hail-ci-build.sh ]]; then; ..//hail-ci-build.sh- CHANGED=$(python3 project-changed.py target/$TARGET_BRANCH $project); ..//hail-ci-build.sh- if [[ $CHANGED != no ]]; then; ..//hail-ci-build.sh- (cd $project && /bin/bash hail-ci-build.sh); ..//hail-ci-build.sh- fi; ..//hail-ci-build.sh- fi; ..//hail-ci-build.sh-done; ```. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5593:251,deploy,deployment,251,https://hail.is,https://github.com/hail-is/hail/pull/5593,3,"['deploy', 'update']","['deployment', 'update']"
Deployability,"This adds roles to the auth service. They are not used yet, but they are stored in the database and I created a simple UI for adding them. I tested with this dev deploy. FYI @danking @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9672:162,deploy,deploy,162,https://hail.is,https://github.com/hail-is/hail/pull/9672,1,['deploy'],['deploy']
Deployability,"This adds terraform modules for CI azure resources and CI k8s resources. It also many roles to the test account that the batch account has so that we can run PR and test namespaces (internal batch instances use the test account for all the services so the test account needs tons of privilege. It looks scary, but this is the model we currently have). Thanks to #11053, which is the current version of the CI deployment in Azure, this required no change to the CI application code. ### Sidebar; It might look weird that I've added a block here for the kubernetes provider. That is because up until now I've kept all the k8s and azure terraform in separate root modules, so that they need to be `terraform apply`'d separately and therefore their provider blocks were separated as well. While keeping the code isolated is good (the k8s modules can be reused for GCP), putting them in separate apply's was purely because of [this bug](https://github.com/hashicorp/terraform-provider-kubernetes/issues/1028) in the kubernetes provider. I ran into it when experimenting tearing clusters down and putting them back up again. However, it has since proven very cumbersome to manage two different terraform states where one relies heavily on the other and I've changed my mind. The bug in question has a PR forthcoming and is really only a problem when tearing a K8s cluster down and rebuilding it while preserving other terraform state, which isn't something I see us dealing with often past initial development. Thus, I've added the k8s provider block so that I can directly invoke the CI k8s module. I'll follow up with a PR that moves the other k8s module invocations in here as well. If it would help, I can first start with a dev doc detailing our terraform structure (or where I want it to be).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11084:409,deploy,deployment,409,https://hail.is,https://github.com/hail-is/hail/pull/11084,1,['deploy'],['deployment']
Deployability,"This adds the Google Cloud Monitoring and Prometheus datasources to the grafana configuration. I had done this initially by hand in the UI but this is the first step toward reproducible monitoring, and I'll eventually follow up with dashboards as code. The one ""change"" I made is I exposed the prometheus port sitting behind nginx so that grafana can talk directly to prometheus. Currently, there's an nginx sitting in front of prometheus so that the prometheus UI can be exposed at prometheus.hail.is with https and dev authentication. This hasn't changed. Currently though, grafana is piggybacking on this flow by forwarding the user's session (which I set up in the UI), but I couldn't figure out an easy way to set that in the config and it seemed unnecessarily complicated. I ended up going the simpler route of just letting grafana talk to prometheus directly and not go through nginx. The 9090 endpoint is not reachable outside of the cluster. I considered namespacing the prometheus domain (`{{ default_ns.name }}` instead of `default`), but I pretty much never find it useful to spin up my own prometheus. In the rare case I run my own grafana I just point it to the data from default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10627:80,configurat,configuration,80,https://hail.is,https://github.com/hail-is/hail/pull/10627,1,['configurat'],['configuration']
Deployability,"This adds the following pieces of infrastructure:. - Fully scripted bootstrapping process, from creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10919:935,configurat,configuration,935,https://hail.is,https://github.com/hail-is/hail/pull/10919,1,['configurat'],['configuration']
Deployability,"This adds the k8s config necessary to host the [guide browser](https://hub.docker.com/r/gneak123/guide_browser/tags) in our k8s cluster. You can see it running in dev [here](https://internal.hail.is/dgoldste/guide-analysis/). There's not much special here, a deployment with the browser app and an envoy sidecar to handle TLS. Once this merges and the `ssl-config-guide-analysis` is created in `default` I can `make -C guide deploy NAMESPACE=default` and then recreate the certs to pick up the new subdomain, after which it should be live. Resolves #14067",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14078:259,deploy,deployment,259,https://hail.is,https://github.com/hail-is/hail/pull/14078,2,['deploy'],"['deploy', 'deployment']"
Deployability,"This adds the minimal resources to k8s to allow us to modify the gateway's configuration to include redirects for https://notebook.hail.is. Currently, that domain will timeout, but there should otherwise be no errors introduces to the k8s system. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4645:75,configurat,configuration,75,https://hail.is,https://github.com/hail-is/hail/pull/4645,2,['configurat'],['configuration']
Deployability,"This adds two new Dockerfiles. The first has gsutil and pip-wheel-installed; Hail. The second builds on the first adding a number of bioinformatics tools; that were included in the CCG Tutorial. Deployment to dockerhub is not trivial because, unfortunately, I need to mount; the docker socket even to download and then upload an image (never starting a; container). I'll design and implement some extension to CI that lets me deploy; images to docker hub later. For now, I used dev deploy to build; these (https://ci.hail.is/batches/33294) and then manually pulled them and; uploaded them to PyPI as hailgenetics/hail:0.2.37 and; hailgenetics/genetics:0.2.37. I particularly appreciate criticism of the names.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8562:66,install,installed,66,https://hail.is,https://github.com/hail-is/hail/pull/8562,4,"['Deploy', 'deploy', 'install']","['Deployment', 'deploy', 'installed']"
Deployability,This all would be a lot easier to reason about if at the top of CI it showed a DEPLOY batch and a PUBLISH batch. This could all be tracked fairly easily in the CI database/bucket.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11122#issuecomment-985734503:79,DEPLOY,DEPLOY,79,https://hail.is,https://github.com/hail-is/hail/pull/11122#issuecomment-985734503,1,['DEPLOY'],['DEPLOY']
Deployability,"This allows the user to specify the cloud platform ('gcp' or 'aws') they are using when accessing datasets via the datasets API and annotation DB. A user running hail on AWS would read from the s3 bucket, and a user running on GCP would read from the gs bucket (can also still read locally from gs bucket with cloud storage connector installed). Not intended for cross-platform use like running a dataproc cluster and trying to access the s3 bucket, or trying to access the gs bucket on an EMR cluster. Will assume user on AWS has their configuration set with their credentials. . Did not have permissions to set up a cluster or EC2 instance on AWS to test, but was able to access all the datasets without issue on a dataproc cluster when using s3a:// prefixes and providing my AWS credentials in the spark config. So these changes should (hopefully) work fine on an EMR cluster with the s3:// client. Everything worked as expected on GCP. Overview of changes:; - Added missing type hints to `load_dataset()` function and methods in `db.py`.; - In `datasets.json`:; - Added AWS urls so now for each version of a dataset in dataset[""versions""], the url entry looks like:; ```; ""url"": {; ""aws"": {; ""us"": ""s3://hail-datasets-us-east-1/...""; },; ""gcp"": {; ""eu"": ""gs://hail-datasets-eu/..."",; ""us"": ""gs://hail-datasets-us/...; }; ```; - In `load_dataset()` function:; - Added `cloud` parameter, set default values to `region='us'` and `cloud='gcp`.; - In `DB` class:; - Added `cloud` parameter to constructor, set default values to `region='us'` and `cloud='gcp`.; - All datasets in `datasets.json` currently end up in the `_DB__by_name` dictionary, even if not annotation datasets. Added line 279 in `db.py` to fix this and filter out datasets that are not annotation datasets (datasets missing ""annotation_db"" key).; - In `Dataset` class:; - Added `cloud` and `custom_config` parameters to `Dataset.from_name_and_json()` to pass to `DatasetVersion.from_json()` to grab correct urls for platform.; - Refac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9605:334,install,installed,334,https://hail.is,https://github.com/hail-is/hail/pull/9605,2,"['configurat', 'install']","['configuration', 'installed']"
Deployability,"This also fixes a broken link in site from the devel => 0.2 rename. See poll-devel.sh. This is urgent! I patched it by hand for the moment, but if it redeploys, it will be broken again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4571:105,patch,patched,105,https://hail.is,https://github.com/hail-is/hail/pull/4571,1,['patch'],['patched']
Deployability,"This also fixes the currently broken `delete_*_tables` steps. In the past, all dev and test databases shared the same MySQL Server (with production), so instead of each service getting its own dedicated database, there was one database per dev/test namespace with all the tables for all the services. This made it difficult to reset the database state of a particular service -- you needed to explicitly delete only the tables for that particular service. Nowadays, dev and test databases live on their own MySQL Servers, so each service gets its own database (like in production). This makes it a lot easier to reset a service's database, we just drop the MySQL database for that service. This PR makes that change, deletes all the now unused `delete-*-tables.sql` files, and adds a dev doc explaining how to reset a dev database. The reason these steps were broken is that the sql configs in dev/test namespaces use K8s DNS for the `host`, which does not work out of the box in batch jobs because they are not in the K8s network. There's code in `database.py` that uses the K8s API to resolve the database host to an IP address that the batch jobs can access. This is why I wrote a python script instead of just using `mysql`. I tested these with the following dev deploy, which scrapped everything and I was able to log in after it was done!. ```; hailctl dev deploy -b daniel-goldstein/hail:dev-ns-delete-db -s delete_auth_tables,delete_batch_tables,deploy_batch,add_developers; ```. cc: @sjparsa, @iris-garden given your recent dev namespace woes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13456:1267,deploy,deploy,1267,https://hail.is,https://github.com/hail-is/hail/pull/13456,2,['deploy'],['deploy']
Deployability,"This basically limits the number of nodes in the worker pool for a test/dev deployment of batch to (default 3, max 4), right? This looks fine to me; should I approve this and let you take off the WIP tag when it's good to merge?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9441#issuecomment-691312517:76,deploy,deployment,76,https://hail.is,https://github.com/hail-is/hail/pull/9441#issuecomment-691312517,1,['deploy'],['deployment']
Deployability,"This benchmark is in some ways bad. The real problem is in the compiler/orchestration, not in any execution. I feel like we need a `_do_nothing()` that doesn't execute any code, but runs a pipeline through everything in the compiler and stops short before anything that would submit a spark job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6629#issuecomment-511031021:189,pipeline,pipeline,189,https://hail.is,https://github.com/hail-is/hail/pull/6629#issuecomment-511031021,1,['pipeline'],['pipeline']
Deployability,"This both avoids an extra pass to find the failing rows as well as avoiding a an extra pass if the globals depend on non-global data. In particular, this pipeline would run the column aggregations four times (IMO, at most twice is OK):. ```; mt = hl.utils.range_matrix_table(2,2); mt = mt.annotate_entries(x = mt.row_idx * mt.col_idx); mt = mt.annotate_cols(mean_x = hl.agg.mean(mt.x)); mt = mt.annotate_entries(x = mt.x - mt.mean_x); mt._same(mt); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13151:154,pipeline,pipeline,154,https://hail.is,https://github.com/hail-is/hail/pull/13151,1,['pipeline'],['pipeline']
Deployability,This branch is up to date with master's `69ba846a3`. Let me know when #6978 lands and I'll update again. Maybe nudge @iitalics to review that?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6971#issuecomment-527604266:91,update,update,91,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-527604266,1,['update'],['update']
Deployability,"This branch, notebook, ci and batch are deployed in my namespace now. Note, batch is tested pretty well, and we test CI comes up (which is about all you can do in a dev deploy).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536245619:40,deploy,deployed,40,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245619,2,['deploy'],"['deploy', 'deployed']"
Deployability,"This broke CI, it can't update from batch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5655#issuecomment-475978868:24,update,update,24,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475978868,1,['update'],['update']
Deployability,"This broke my install, which does not use Conda. I have no `pip`, just `pip3`. cc: @chrisvittal you represent the other major environment, does this break you?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6293:14,install,install,14,https://hail.is,https://github.com/hail-is/hail/pull/6293,1,['install'],['install']
Deployability,"This came to mind yesterday during our pairing. This PR introduces the following properties that our image building targets do not currently have:; 1. If your intention is only to build images, you shouldn't need `kubectl`. When `DOCKER_PREFIX` is used as a docker build arg it is because we mirror some dockerhub images inside our registry (for reliability/rate limiting reasons). But for local building there's no reason you can't use the dockerhub image. Also, other people should be able to build the hail image if they want to!; 2. One should *only* need to use `kubectl` if they are intending to use an image in a kubernetes deployment. In other words, you should only need the private registry `DOCKER_PREFIX` for pushing images.; 3. One should not need to endure image pushing if the only goal is to build the image locally; 4. No intermediate tags should end up in the private registry. If we push on every image build, the private docker registry will accumulate a lot of `hail-ubuntu:dev-xxxxxx` tags that are never used again because `hail-ubuntu` is just an intermediate used to build other images. This does *not* change the number of layers that end up in the registry, but reduces a bit of the work that the registry cleanup job needs to do to untag and delete images and just seems cleaner.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13890:631,deploy,deployment,631,https://hail.is,https://github.com/hail-is/hail/pull/13890,1,['deploy'],['deployment']
Deployability,"This can go in after we do a release and we notify the batch users about the change. After that, we should delete all the per-user buckets.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8834#issuecomment-634121647:29,release,release,29,https://hail.is,https://github.com/hail-is/hail/pull/8834#issuecomment-634121647,1,['release'],['release']
Deployability,"This catches the problem -- you can try the diff you sent me Friday. Once this is merged, new version deploys will be blocked until a fix is in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10520#issuecomment-847464819:102,deploy,deploys,102,https://hail.is,https://github.com/hail-is/hail/pull/10520#issuecomment-847464819,1,['deploy'],['deploys']
Deployability,"This change also updates the `make pylint-hail` target to include the `hail/python/(hail|test|cluster-tests)` directories, rather than the `hail/` directory. This is because the latter would require adding `hail/__init__.py` and/or `hail/python/__init__.py` files, which creates issues because some modules in `hail/python/hail` and `hail/python/hailtop` have the same name but are at different paths, which breaks the package resolution for `ruff` and `mypy`. For this reason, although `black` is run on the files in the `hail/` and `hail/python/` directories, `pylint` cannot be.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13235:17,update,updates,17,https://hail.is,https://github.com/hail-is/hail/pull/13235,1,['update'],['updates']
Deployability,"This change brings the `facet_wrap` interface more in line with [the `ggplot2` implementation](https://ggplot2-book.org/facet.html#facet-wrap) by defaulting to show axes (with tick marks) for all facets, as well as providing `scales`, `nrow` and `ncol` arguments that the user can use to specify whether the scales should be the same across facets and how many rows or columns to use. It also updates the faceting code to avoid duplicating legend entries when [grouped legends](https://github.com/hail-is/hail/pull/12254) are used, and to disable interactivity accordingly. With this update, running this code:. ```python; import hail as hl; from hail.ggplot import aes, facet_wrap, geom_point, ggplot, vars; ht = hl.utils.range_table(10); ht = ht.annotate(squared=ht.idx ** 2); ht = ht.annotate(even=hl.if_else(ht.idx % 2 == 0, ""yes"", ""no"")); ht = ht.annotate(threeven=hl.if_else(ht.idx % 3 == 0, ""good"", ""bad"")); ht = ht.annotate(fourven=hl.if_else(ht.idx % 4 == 0, ""minor"", ""major"")); fig = (; ggplot(ht, aes(x=ht.idx, y=ht.squared)); + geom_point(aes(shape=ht.even, color=ht.threeven)); + facet_wrap(vars(ht.threeven, ht.fourven), nrow=1); ); fig.show(); ```. Produces the following plot:. <img width=""1283"" alt=""Screen Shot 2022-11-23 at 12 23 21"" src=""https://user-images.githubusercontent.com/84595986/203611376-682b98e7-2915-4e2c-8f1e-ef8c74b12907.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12497:393,update,updates,393,https://hail.is,https://github.com/hail-is/hail/pull/12497,2,['update'],"['update', 'updates']"
Deployability,This change brings the site deployment naming scheme in line with; our other k8s Deployments and also adds a `wait` to build.yaml so; that we check that the site has come up successfully.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8492:28,deploy,deployment,28,https://hail.is,https://github.com/hail-is/hail/pull/8492,2,"['Deploy', 'deploy']","['Deployments', 'deployment']"
Deployability,This change can't possibly be causing the website_image issues. I'm going to force merge this and force merge a release to address the pressing hadoop connector issues. I will address the website_image issue after that is done.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11434#issuecomment-1055787996:112,release,release,112,https://hail.is,https://github.com/hail-is/hail/pull/11434#issuecomment-1055787996,1,['release'],['release']
Deployability,"This change combines cloud auth logic that was previously duplicated; between the various `FS` implementations and the `BatchClient`. . The main refactoring is to make the interface between the `ServiceBackend` more; high-level and leave json serialisation to the `BatchClient`. To do this, I've; added a bunch of case classes that resemble the python objects the batch service ; expects (or a subset of the data). To simplify the interface, I've split batch; creation from job submission (update). For QoB, the python client creates the ; batch before handing control to the query driver; batch creation is necessary; for testing only. This change has low security impact as there are minor changes to the creation; and scoping of service account credentials. Note that for each `FS`, credentials; are scoped to the default storage oauth2 scopes for each service.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14684:490,update,update,490,https://hail.is,https://github.com/hail-is/hail/pull/14684,1,['update'],['update']
Deployability,"This change creates mysql pods for test and dev namespaces, instead of sharing the CloudSQL database server. The areas of change are as follows:. ### Generation of the namespace's database-server-config; The current approach in main does a little trick. Since the current `createDatabase` step uses the `database-server-config` from default to generate admin/user sql configs, the CI pipeline creates a dummy database `test-database-instance` to create a `sql-test-instance-admin-config` that inherits the credentials from the production `database-server-config`, and then copies that within the test namespace to `database-server-config`. In this change, since we are creating the server ourselves, we can just replace these with a step that creates a `database-server-config` from scratch, and then uses that for the DB pod. Overall making these changes really gave me the heebie jeebies that the test and dev namespaces have all these credentials to the CloudSQL server. I'm glad this gets rid of that. ### Accessing the database server; We use the DB pod's service DNS name as the `host` so inside Kubernetes this Just Works. The one caveat is the CI pipeline in which we run migrations in batch jobs. Those jobs need a way to reach the DB pod. I achieve this with a NodePort and then use the job's K8s credentials to resolve the node and port that the DB is on. The code I've added to do this resolution feels a bit janky, wouldn't mind some feedback on that. In terms of security, if a user job was able to somehow resolve the address of a test db, they would still not have the credentials to access it, and this is currently also the case with the production database. Nevertheless, this does raise an action item that we should only allow traffic to the k8s and DB subnets for `network=private` jobs, but I think we should make that a separate PR. ### Database creation; In order to test this properly in a dev deploy, I needed to make some changes to `create_database.py`. In main, dev deplo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13030:384,pipeline,pipeline,384,https://hail.is,https://github.com/hail-is/hail/pull/13030,1,['pipeline'],['pipeline']
Deployability,"This change integrates the C++ code of libhail with the python code as; a python native extension exposed as _hail. It is extremely minimal for; now, containing only small wrappers for the C++ versions of hail virtual; types as python classes/objects. It is completely unused and introduces; python build time dependencies of a C++20 compiler and CMake. The LLVM dependencies should be optional for now as they add a lot to; the binary size without adding any functionality. They will still get; built if they are found. In the future, it may be easier or more desireable to maintain the _hail; module sources as Cython rather than C++, however, for both learning's; sake and ease of compilation, the module is pure C++.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10505:12,integrat,integrates,12,https://hail.is,https://github.com/hail-is/hail/pull/10505,1,['integrat'],['integrates']
Deployability,"This change is split out from a larger refactoring effort on the various Backend ; implementations. The goals of this effort are to provide query-level ; configuration to the backend that's currently tied to the lifetime of a backend,; reduce code duplication and reduce state duplication. In this change, I'm restoring references to the execute context [1] and ; decoupling them from the backend. In a future change, they'll be lifted out of ; the backend implementations altogether. This is to reduce the surface area of ; the Backend interface to the details that are actually different. Both the Local and Spark backend have state that's manipulated from python via ; various py methods. These pollute the Backend interface [2] and so have been ; extracted into the trait Py4JBackendExtensions. In future changes, this will ; become a facade that owns state set in python. Notes; [1] ""Restoring"" old behaviour I foolishly removed in fe5ed32; [2] ""Pollute"" in that they obfuscate what's different about backend query plan ; and execution",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14686:154,configurat,configuration,154,https://hail.is,https://github.com/hail-is/hail/pull/14686,1,['configurat'],['configuration']
Deployability,"This change is split out from a larger refactoring effort on the various Backend; implementations. The goals of this effort are to provide query-level; configuration to the backend that's currently tied to the lifetime of a backend,; reduce code duplication and reduce state duplication. In this change, I'm removing blockmatrix persist/unpersist from the `Backend`; interface by adding `BlockMatrixCache: mutable.Map[String, BlockMatrix]` to; `ExecuteContext`. The various reader/writer implementations simply fetch the ; block matrix from this cache. For the spark backend, this is backed by a cache; whose lifetime is tied to the spark backend. Since block matrices are not; supported in the local and service backends, the cache is an empty map. Note that block matrix persist is broken in python (#14689)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14690:152,configurat,configuration,152,https://hail.is,https://github.com/hail-is/hail/pull/14690,1,['configurat'],['configuration']
Deployability,"This change is temporary. I do not intend to keep the extra hop to `auth` on all internal-gateway requests. Once all the TLS changes go in and everything in the cluster is TLS-secured, then I can switch the internal gateway to unconditionally use HTTPS and remove the router-resolver's extra endpoint. I've already deployed this (I need it to get batch tests to pass in my namespace.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8490:315,deploy,deployed,315,https://hail.is,https://github.com/hail-is/hail/pull/8490,1,['deploy'],['deployed']
Deployability,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7178:150,configurat,configuration,150,https://hail.is,https://github.com/hail-is/hail/pull/7178,6,"['configurat', 'deploy']","['configuration', 'deploy', 'deployment']"
Deployability,"This change sorts the jobs for a PR correctly in the UI, so that all jobs are placed under a header with the correct state (see screenshot). Because jobs are split into separate tables by their state, `focusOnSlash` has been removed from the relevant CI pages, since it is unknown which tables will exist on the page until it is rendered. It also fixes the error that was causing the server to return a 500 when there were no jobs yet (e.g. when a retry had just been requested), which was caused by an assumption in the original job filtering code that there would always be at least one job to display. It also passes the list of developers through to the underlying `PR` object that a `WatchedBranch` has, which was missed in https://github.com/hail-is/hail/pull/13398 but is required for CI to display the PR page correctly in dev deploys only. <img width=""714"" alt=""Screenshot 2023-08-22 at 15 08 48"" src=""https://github.com/hail-is/hail/assets/84595986/6bcab38c-a48a-4e10-8f69-770d35ea51b7"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13254:835,deploy,deploys,835,https://hail.is,https://github.com/hail-is/hail/pull/13254,1,['deploy'],['deploys']
Deployability,"This changes the Azure database from Azure database for MySQL Single Server to Azure database for MySQL Flexible Server. The major changes are:. - Fixed several small rots across the bootstrap code; - Altered the database module in terraform to use flexible server. This configuration mostly matches what we had with single server, importantly that it is only accessible on our vnet.; - Makes the client key/certificate in the SQLConfig optional (the current use of SQLConfig is a little repetitious and could probably use a refactor).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11423:271,configurat,configuration,271,https://hail.is,https://github.com/hail-is/hail/pull/11423,1,['configurat'],['configuration']
Deployability,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:1044,install,installed,1044,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965,2,['install'],"['install', 'installed']"
Deployability,"This copies all the artifacts into place, but doesn't update the latest hash. Once we verify the files look correct after one round of deploy, I'll comment in the code to update latest hash. This script is basically copied exactly from the old hail/hail-ci-deploy.sh.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6174:54,update,update,54,https://hail.is,https://github.com/hail-is/hail/pull/6174,4,"['deploy', 'update']","['deploy', 'update']"
Deployability,"This creates another hailgenetics image with just hailtop and its python dependencies installed (no hail query, no java, no gcloud), primarily useful for using the copy tool. In contrast to hailgenetics/hail's 2.89GB on disk, hailgenetics/hailtop-slim is 289MB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11797:86,install,installed,86,https://hail.is,https://github.com/hail-is/hail/pull/11797,1,['install'],['installed']
Deployability,"This creates another hailgenetics image with just hailtop and its python dependencies installed (no hail query, no java, no gcloud), primarily useful for using the copy tool. Instead of 3GiB it is less than half a GiB. This also is the start of separating out our python dependencies into those required by separate modules. The hail dependencies now includes the hailtop dependencies. Ultimately, I want to move the services' dependencies to include those of hailtop and not all of hail (getting pyspark out of our services images).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12446:86,install,installed,86,https://hail.is,https://github.com/hail-is/hail/pull/12446,1,['install'],['installed']
Deployability,This data is from 25-35x whole genome sequencing data aligned to the hg38 reference. The VCF was generated by the GATK pipeline. The vcf used to create the pruned dataset was limited to SNVs and so it did not contain any indels or multallelic sites.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3490#issuecomment-386642665:119,pipeline,pipeline,119,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-386642665,1,['pipeline'],['pipeline']
Deployability,"This diff uses the index on `instances.removed`. ```diff; diff --git a/batch/batch/driver/canceller.py b/batch/batch/driver/canceller.py; index d438a8519b..594b180221 100644; --- a/batch/batch/driver/canceller.py; +++ b/batch/batch/driver/canceller.py; @@ -371,10 +371,9 @@ SELECT attempts.*; FROM attempts; INNER JOIN jobs ON attempts.batch_id = jobs.batch_id AND attempts.job_id = jobs.job_id; LEFT JOIN instances ON attempts.instance_name = instances.name; -WHERE attempts.start_time IS NOT NULL; - AND attempts.end_time IS NULL; +WHERE attempts.end_time IS NULL; AND ((jobs.state != 'Running' AND jobs.state != 'Creating') OR jobs.attempt_id != attempts.attempt_id); - AND instances.`state` = 'active'; + AND instances.removed = 0; ORDER BY attempts.start_time ASC; LIMIT 300;; """""",; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14460#issuecomment-2221567514:69,a/b,a/batch,69,https://hail.is,https://github.com/hail-is/hail/issues/14460#issuecomment-2221567514,2,['a/b'],['a/batch']
Deployability,"This doesn't necessarily fix the problem, but I think they are good changes in the direction of what we know. It passed test_{batch, pipeline} on the first try, so that's a good sign. What I did:. 1. Lock down Job state transitions. Now only set_state and _mark_job_task_complete change _state, and they log identically. Explicitly enumerate the valid state transitions are check them in each function. Slightly clarified the transitions around Pending. Now Pending can only go to Ready. 2. If a state update fails (the Python object is stale), throw JobStateWriteFailure. If we have a stale picture, we clearly don't want to be doing anything else. 3. Handle a few more cases in update_job_with_pod: a pod without a job or a job that shouldn't have one, and a cancellable pod that hasn't been cancelled yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707:133,pipeline,pipeline,133,https://hail.is,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707,4,"['pipeline', 'update']","['pipeline', 'update']"
Deployability,"This enables Query-on-Batch pipelines to read from requester pays buckets. @tpoterba curious for your thoughts on the flag situation. I suspect this PR will induce the Australians to start including requester pays config in their pipelines. If you describe an API you like, I can implement it for this PR. Otherwise, I think this is ready. It works, it is tested. The changes to GoogleStorageFS suck, but its due to the reality of the GCS API.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12133:28,pipeline,pipelines,28,https://hail.is,https://github.com/hail-is/hail/pull/12133,2,['pipeline'],['pipelines']
Deployability,"This failed because of a Query bug. It's ready for another look. Once you're happy with the structure of the code, I'll do one last dev deploy and check everything is good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11840#issuecomment-1138707197:136,deploy,deploy,136,https://hail.is,https://github.com/hail-is/hail/pull/11840#issuecomment-1138707197,1,['deploy'],['deploy']
Deployability,"This failed with OpenJDK 11 installed. I don't know Scala, but my read of your checker is that it's looking for older versions, not newer ones. The docs explicitly say java 8 is required.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4896#issuecomment-444556003:28,install,installed,28,https://hail.is,https://github.com/hail-is/hail/issues/4896#issuecomment-444556003,1,['install'],['installed']
Deployability,"This fixes a bug Mike W was hitting with a gnomAD pipeline. I wasn't able to replicate with a trivial example, and would prefer not to spend another half-day poring through that huge IR to try to minimize. Verified that this fixes the problem, though. https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/.E2.9C.94.20Summing.20number.20of.20entries.20meeting.20criteria.20.20by.20group",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12863:50,pipeline,pipeline,50,https://hail.is,https://github.com/hail-is/hail/pull/12863,1,['pipeline'],['pipeline']
Deployability,"This fixes a bug in the fifo sem. If we release a large job, we would only start at most 1, not as many as we could. This could actually lead to worker deadlock.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7844:40,release,release,40,https://hail.is,https://github.com/hail-is/hail/pull/7844,1,['release'],['release']
Deployability,"This fixes a few bugs, namely $(3) being not quite right, and the base image grep | head -n 1 ; sed not working (at least on some systems) due to --color=none missing. This is fixed by moving to more explicit rule declarations (at some verbosity cost). make deploy/push is also now CI specific. Caching behavior should match the last commit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5829:258,deploy,deploy,258,https://hail.is,https://github.com/hail-is/hail/pull/5829,1,['deploy'],['deploy']
Deployability,"This fixes a few of problems:; - Batch.jobs() was discarding the job json objects and returning an Job objects. This is not OK because we need the metadata from the json objects for efficiency (e.g. in CI PR/batch pages).; - We confused Job.status(), which is the job json object, with the status field of that object. In particular, we got that wrong in BatchFormatVersion.get_status_exit_code_duration in the call to Job.{exit_code, total_duration_msecs}. Add a test to check we got this right.; - Return the duration in msecs, not a string, because as an API, the string is useless. humanize it before sending it to the API. I think the batch client is (1) has a lot of legacy baggage that can be cleaned up, and (2) is massive overkill and can probably be cleaned up. It should probably have no objects, pass around job_id ints or (batch_id, job_id) tuples, and be a thin wrapper around request calls. Pipeline should be the high level API. Finally, I think we need a little document (just a comment) about the format of GET /batches/batch_id and GET /jobs/job_id. It's changed enough that it is getting a little hard to follow. I'm going to dev deploy to test the UI. Don't approve until it checks out, please. FYI @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7996:906,Pipeline,Pipeline,906,https://hail.is,https://github.com/hail-is/hail/pull/7996,2,"['Pipeline', 'deploy']","['Pipeline', 'deploy']"
Deployability,"This fixes the notebook2 deployment permission issue that was resulting in CrashLoopBackoff (no permissions for the Table class to `read_namespaced_secret('get-users', 'default')`). Already tested, works (notebook2 back up). It also fixes an apparent error in the master branch RoleBinding. This diff looks slightly weird. I fixed the existing notebook Roles/RoleBindings by deleting the `create-services` `Role` and `notebook-create-services` `RoleBinding`, and then fixing the broken `notebook-create-servivces-and-pods` `RoleBiding`, by correctly updating the `roleRef` to read `create-services-and-pods`. When notebook1 totally goes away, we can probably remove the ""services"" permission. Before:; ```yaml; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services; apiGroup: """"; ---; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGrou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5746:25,deploy,deployment,25,https://hail.is,https://github.com/hail-is/hail/pull/5746,1,['deploy'],['deployment']
Deployability,This forces CI and pipeline to retest themselves if batch changes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5464:19,pipeline,pipeline,19,https://hail.is,https://github.com/hail-is/hail/pull/5464,1,['pipeline'],['pipeline']
Deployability,"This gets ld_prune on the `get_1kg` data down to around 37s. That's still ~1000 times slower than plink.; ```; mt = hl.read_matrix_table('repartitioned.mt'); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```. Performance Wins:; - local ld prune returns an unkeyed, unsorted dataset, and `ld_prune` collects the relatively small number of variants locally instead of trying to do table joins (I'm doing the broadcast join optimization manually); - avoid `key_by` (and thus sort) of output of MIS, again we do a broadcast join; - two unnecessary writes removed (at the cost of no debugging output); - `maximal_independent_set` no longer keys by, thus avoiding a sort. Minor Changes:. - I don't set env vars anymore, so I need an easy way to pip install hail, so I added a gradle task for that and an associated file that does almost the same thing as deploy.sh. you should complain and make me consolidate these two files. ---; ## Big Data Test. I'm running a test on profile225 right now. ---. resolves #4506",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5078:841,install,install,841,https://hail.is,https://github.com/hail-is/hail/pull/5078,2,"['deploy', 'install']","['deploy', 'install']"
Deployability,"This has been unused for a long time and I wanted to rid the world of it. This changes the client to no longer send the `mount_docker_socket` field, updates the validator so it's ok for that field to be missing, and explicitly rejects any requests where `mount_docker_socket` is set to true. The reason I couldn't remove `mount_docker_socket` entirely from the codebase is older clients would break if it was not at least optional in the validator spec, which is kind of annoying. Dev deployed this and successfully submitted jobs using the client from this branch and the client from main.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12973:149,update,updates,149,https://hail.is,https://github.com/hail-is/hail/pull/12973,2,"['deploy', 'update']","['deployed', 'updates']"
Deployability,"This helps with method size if we're doing a lot of ; ```; Let(""foo"", <value>,; Let(""bar"", <value2>,; … )); ```; where the size of the code generated by the values are potentially quite large. (I think doing this will unblock one of @konradjk's pipelines for now.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7740:245,pipeline,pipelines,245,https://hail.is,https://github.com/hail-is/hail/pull/7740,1,['pipeline'],['pipelines']
Deployability,This improves a benchmark variant of a pipeline of @konradjk with six aggregators by 24% (140.6s => 106.8s) on a shard of gnomAD. The improvement will be larger with more aggregators. This change compiles the seqOp for all aggregators together into a single function and eliminates a loop over aggregators where the aggregations are invoked (e.g. MatrixMapRows). I added a SeqOp node that represents the call of a seqOp on a single region value aggregator. This replaces ApplyAggOp when extracting aggregators. I added a Begin node which just sequences collection of void-typed IRs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3420:39,pipeline,pipeline,39,https://hail.is,https://github.com/hail-is/hail/pull/3420,1,['pipeline'],['pipeline']
Deployability,This includes various changes that have been made in the course of working with DSP ; on this pipeline.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7518:94,pipeline,pipeline,94,https://hail.is,https://github.com/hail-is/hail/pull/7518,1,['pipeline'],['pipeline']
Deployability,"This installs our fully-pinned requirements deep in the docker image and then installs the hail wheel without dependencies on top. This will be a lot more consistent (and docker cache friendly) than the current approach, which will install all dependencies with the wheel and possibly upgrade some of them. Also did the same to the hail-pip-installed images used for testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12433:5,install,installs,5,https://hail.is,https://github.com/hail-is/hail/pull/12433,5,"['install', 'upgrade']","['install', 'installed', 'installs', 'upgrade']"
Deployability,"This introduces a new version of the batch worker instance: one without `docker`. Instead we bring in `podman` to cover the functions of pulling images, extracting expanding filesystems from those images, and running the worker container. `podman` by default uses `crun` as its low-level runtime so we can get rid of the independent `crun` installation in the worker image. `podman` is daemonless and can be run rootless. For the most part you can't tell the difference, except this makes `podman` easy to run under multiple users with different caches per user. So if you ssh into a worker, be sure to `sudo` before any `podman` (or `crun`) command or else you might think nothing is running when in fact the worker is running under root's podman configuration. The podman/crun state directories are now shared between the host and the worker so `sudo crun list` on the worker should reveal the running job containers without having to exec into the worker first. For the most part, `podman` is a drop-in replacement for `docker`, but there are a handful of inconsistencies that comprise most of this PR. One notable change is that we no longer persist any GCR credentials in the worker VM image so we authenticate again on start up. cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10693:340,install,installation,340,https://hail.is,https://github.com/hail-is/hail/pull/10693,2,"['configurat', 'install']","['configuration', 'installation']"
Deployability,"This introduces the necessary pieces of infrastructure to run CI in GCP and a couple of small changes such that it can run as a secondary CI. This is currently failing because one of the secrets I introduce here in the terraform does not exist in hail-vdc. If you approve of the approach I can add it in manually. . ## Terraform changes; This adds a new CI terraform module that adds a CI bucket, sets some permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053:539,deploy,deployment,539,https://hail.is,https://github.com/hail-is/hail/pull/11053,1,['deploy'],['deployment']
Deployability,"This is a bad change to a worse problem. The right solution is to redesign the IR; so that implicit init eval scopes don't exist -- but the right solution is hard to justify; going off to do right now. This change patches the Extract.scala lowering logic to track the variables bound inside a; lowered IR and find the highest node that provides all necessary free variables. This change; still makes assumptions about the structure of the IR -- namely, it is still invalid to write; an IR like:. ```; MakeTuple; Let; initBinding1; <something>; ApplyAggOp with ref to initBinding1; Let; initBinding2; <something2>; ApplyAggOp with ref to initbinding2; ```. However, this fix resolves the case where init args reference a high single binding chain, as; in the test added.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12092:214,patch,patches,214,https://hail.is,https://github.com/hail-is/hail/pull/12092,1,['patch'],['patches']
Deployability,"This is a bit of a mess so feel free to ask that I break it down or explain more in depth. In short, this:. - adds terraform for forgotten bits and pieces necessary for running PR tests like test buckets and the necessary permissions on those resources; - Adds a couple of flags that allow a CI `WatchedBranch` to be considered mergeable or not. This shouldn't change anything in default CI, but it allows you to specify that a secondary CI should run PRs, post statuses, and deploy new commits, but never actually commit anything to GitHub. Similarly there's a flag for turning off zulip notifications, but annoyingly the zulip config is still a required secret. I plan to make that nicer in the future.; - Fixes a lot of previously-unreached syntax errors in the batch tests. Following PRs will have relatively less functionality but probably a fair bit of cleanup and reorganization, e.g. getting rid of config.mk and generating it from terraform output, making scripts of the bootstrapping process etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10866:476,deploy,deploy,476,https://hail.is,https://github.com/hail-is/hail/pull/10866,1,['deploy'],['deploy']
Deployability,"This is a follow-up on #10207, which circumvents `router-resolver` and `router` and uses kube dns to route requests passing through internal gateway directly to destination namespace and service. Previously, `internal-gateway` was only using `router-resolver` to discover the scheme of the `router` it needed to forward the request to. Since we are using `https` in front of all of our services now, and are ultimately retiring router, this is no longer necessary. This is also the last use of `router-resolver` in the whole codebase, so we can remove the service entirely! (second commit). This version of `internal-gateway` has been running since yesterday evening and there is currently no `router-resolver` deployment in default!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10212:711,deploy,deployment,711,https://hail.is,https://github.com/hail-is/hail/pull/10212,1,['deploy'],['deployment']
Deployability,"This is a little confusing and not especially well documented on our end. The `--tmpdir` option set within Hail defines the scratch space for certain Hail operations, but not Spark ones. The tempdir that's blowing up on you is the `spark.local.dir` setting (see http://spark.apache.org/docs/latest/configuration.html for more info). This directory is used when Spark needs to do a data shuffle, and currently the `splitmulti` command requires this. I assume you're submitting to a cluster with a `spark-submit` invocation. You can set spark settings by passing the `--conf` argument, as here:. ``` text; /usr/bin/spark-submit \ ; --conf spark.shuffle.compress=true \ ; --conf spark.local.dir=/correct/tmp/dir \; --class org.broadinstitute.hail.driver.Main \ ; JAR \; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/902#issuecomment-251721691:298,configurat,configuration,298,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251721691,1,['configurat'],['configuration']
Deployability,"This is a merge of hail-is/batch master updating only hail-ci-{build, deploy}.sh to resolve merge conflicts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4414:70,deploy,deploy,70,https://hail.is,https://github.com/hail-is/hail/pull/4414,1,['deploy'],['deploy']
Deployability,"This is a month out of date and CI has hit the status update limit, closing. Please push a fresh commit SHA when you're ready for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8333#issuecomment-612943506:54,update,update,54,https://hail.is,https://github.com/hail-is/hail/pull/8333#issuecomment-612943506,1,['update'],['update']
Deployability,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9129:625,pipeline,pipeline,625,https://hail.is,https://github.com/hail-is/hail/pull/9129,3,"['Deploy', 'deploy', 'pipeline']","['Deploy', 'deploy', 'pipeline']"
Deployability,"This is a pod running in default that mounts deploy-config and database-server-config. `mysql` will connect to the database as root. Useful for troubleshooting. I often have it running, but figure it should be official. Doesn't run in test since it doesn't have a database-server-config.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7819:45,deploy,deploy-config,45,https://hail.is,https://github.com/hail-is/hail/pull/7819,1,['deploy'],['deploy-config']
Deployability,"This is a preparatory PR for getting rid of router-resolver. Currently, the major role of router-resolver is:; - Find the user session in a request; - Make a call to default auth to verify the session and retrieve userdata; - If the user is a developer, return the router IP in the given namespace (given as input to router-resolver). The code for this is [here](https://github.com/hail-is/hail/blob/466f3c32c7fed972dc9bf45834669fdff038224c/router-resolver/router_resolver/router_resolver.py#L24-L66). This whole process can just be replaced by an auth endpoint that returns whether or not a request is coming in from a valid developer (we can rely on kubernetes services to find IP addresses on any of our deployments). This moves over the finding of the user session in router-resolver to make an auth endpoint that can replace router-resolver. I created this auth endpoint in #10139 with this ultimate goal in mind but did not incorporate the other places we store sessions. EDIT: Second commit fixes a silly mistake that `verify_dev_credentials` makes a whole separate network call to the `userinfo` endpoint, when they should really just both use the same helper function. The semantics of both endpoints should remain unchanged.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10203:707,deploy,deployments,707,https://hail.is,https://github.com/hail-is/hail/pull/10203,1,['deploy'],['deployments']
Deployability,"This is a pretty simple change, though the diff is a little bizarre. I don't want running a ukbb server to be a requirement of running a hail instance. Since the ukbb app is a special case in the way we deploy apps in k8s, we would render the gateway configuration server blocks with the logic:. - For each service, if it's not ukbb, render it the usual way; - Render the ukbb block. This is a very simple change to instead make the logic. - For each service, if it's ukbb render it the ukbb way, else render it the usual way. Together with the separation of the ukbb terraform into its own module in #10842, it should be easier to deploy hail without the ukbb app and simplify the bootstrap process. This is currently running in hail-vdc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10845:203,deploy,deploy,203,https://hail.is,https://github.com/hail-is/hail/pull/10845,3,"['configurat', 'deploy']","['configuration', 'deploy']"
Deployability,"This is a quadratic task in releases, so it's not really feasible. We just need to be better about writing which interface is used in a post.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1384#issuecomment-349720138:28,release,releases,28,https://hail.is,https://github.com/hail-is/hail/issues/1384#issuecomment-349720138,1,['release'],['releases']
Deployability,"This is a rebased update to the closed PR #3715. Updates:. - `locus_table` is now `locus_expr`, an expression on a table or matrix table. This is more flexible, and avoids unnatural requirements on key (rather than order). - uses `global_position` and requires/checks ascending order, rather than reordering within the method. Re-ordering, if non-trivial, would silently invalidate the results with respect to the row-order of the source. This also avoids a potentially unnecessary shuffle, and addresses the issue that contig order may not be alphabetical in the reference. - keeps the size of data collected close to the minimal data necessary (in particular, no collection of contigs). When `coord_expr` is not set, its an array of int. When `coord_expr` is set, its an array of (int, float). A tuple of arrays would be better, but directly collecting two arrays in order would require two actions with the current infrastructure since agg.collect() does not preserve order.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3873:18,update,update,18,https://hail.is,https://github.com/hail-is/hail/pull/3873,2,"['Update', 'update']","['Updates', 'update']"
Deployability,"This is a total overhaul of our docker images. Though very verbose, I tried to stick to these main tenets:. - Any docker image has exactly 1 layer in it (all the way down to ubuntu) that installs pip dependencies. This primarily aims to protect the cache for this particularly large layer and also avoids a later layer silently upgrading the version of a dependency installed in an earlier layer. This pairs nicely with the following goal; - We only ever use 1 version of a dependency across the monorepo. Liberal use of pip's [constraint files](https://pip.pypa.io/en/stable/user_guide/#constraints-files) to ensure that the dependencies for a service must be compatible with dependencies from hail. The `install-dev-dependencies` target which install all our pinned requirements files would tell you if there's any incompatible versions of transitive dependencies across the repo; - The image graph is shallow and images don't contain more than they need. In order to have a single layer with requirements and hail code on top, I moved the service images to just be based on hail-ubuntu. This shortens the critical path and therefore reduces total image building time by reducing the number of times our image data needs to be downloaded and re-uploaded to the registry. I also removed a lot of unnecessary cruft like gcloud in places it wasn't used anymore, some unused/unnecessary pip requirements, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578:187,install,installs,187,https://hail.is,https://github.com/hail-is/hail/pull/12578,4,['install'],"['install', 'install-dev-dependencies', 'installed', 'installs']"
Deployability,This is an issue with deploying on spark 1.6.0. Subsumed by #667,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/669#issuecomment-242106016:22,deploy,deploying,22,https://hail.is,https://github.com/hail-is/hail/issues/669#issuecomment-242106016,1,['deploy'],['deploying']
Deployability,This is an ok thing to do since we haven't released since those 1.6 files were created in #11151,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11246#issuecomment-1020333007:43,release,released,43,https://hail.is,https://github.com/hail-is/hail/pull/11246#issuecomment-1020333007,1,['release'],['released']
Deployability,This is an outdated installation method.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4876:20,install,installation,20,https://hail.is,https://github.com/hail-is/hail/pull/4876,1,['install'],['installation']
Deployability,"This is basically #6534 except with some bugfixes. Stacked on #6535. I opened a new PR because I created this branch to do some benchmarking against @chrisvittal's combiner pipeline; running `summarize` on the mt he provided (which I believe is chr22 and 10 samples wide) yields about the same runtime as the unstaged version, currently---about 30 seconds on my laptop to read/summarize/write. @patrick-schultz @tpoterba @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6580:173,pipeline,pipeline,173,https://hail.is,https://github.com/hail-is/hail/pull/6580,1,['pipeline'],['pipeline']
Deployability,"This is better, but I want to go more extreme:. - get rid of TransformedRegionValueAggregator and ZippedRegionValueAggregator. This is a compiler backend. Too much abstraction in your output! Let's compile that shit away.; - ExtractAggregators should return an Array[AggSum]. These are expressions ending in AggSum containing aggregator operations (filter, map, etc.) defined in terms of the aggregated element and associated context.; - Add a function AggSum => RegionValueAggregator. This is the way to generalize: make AggSum into Agg(op: AggOp) where AggOp (like unary and binary op) are all the possible aggregator types, and there is a function that maps the op to the corresponding RegionValueAggregator*; - compiling the Array[AggSum] should product a function that takes the array of aggregators and a single value (with context) of the collection we're aggregating over and updates them with that element. *I think you need an array of arguments to handle things like call_stats which are evaluated in the aggregator scope (the only scope available to evaluate something). Imagine you have `gs.filter(g => g.GT.isHet()).map(g => g.DP).sum() + gs.flatMap(g => g.PL).sum()`. The Array[AggSum] will be. ```; Array(AggSum(AggMap(; AggFilter(AggIn(...), ; ""g"", g.GT.isHet()),; ""g"", (getField (Ref ""g"") ""DP""),; AggSum(AggFlatMap(AggIn(...),; ""g"", (getField (Ref ""g"") ""PL))); ```. The generated function should look like:. ```; def f(aggs: Array[AggSum], region: MemoryBuffer, g: Long, mg: Boolean, ...) {; if (g.GT.isHet()) { // RV-ified, of course; val DP = g.DP // actually fieldOffset; aggs(0).seqOp(DP); }; for (PLi in g.PL) { // actually elementOffset; aggs(1).seqOp(PLi); }; }; ```. This is straight-line and should be fast. It immediately allows you to do common subexpression elimination on aggregator prefixes which is something that is quite common, that is, if you have `gs.filter(g => g.isHet).map(g => g.DP).mean() < 10 gs.filter.map(g => g.GQ).mean() < 50` then in the aggregation fu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2555#issuecomment-350868684:884,update,updates,884,https://hail.is,https://github.com/hail-is/hail/pull/2555#issuecomment-350868684,1,['update'],['updates']
Deployability,This is caused by VariantSpark monkey patching Hail. I've asked them to [stop doing this](https://github.com/aehrc/VariantSpark/issues/228).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12859#issuecomment-1502215008:38,patch,patching,38,https://hail.is,https://github.com/hail-is/hail/issues/12859#issuecomment-1502215008,1,['patch'],['patching']
Deployability,This is developer only and helpful primarily in dev deploy.; I think its safe as long as we trust hail devs not to; accidentally hit this endpoint.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8409:52,deploy,deploy,52,https://hail.is,https://github.com/hail-is/hail/pull/8409,1,['deploy'],['deploy']
Deployability,This is for releasing the feature added by #11884. We need to merge this PR soon after #11884 goes in as it's a breaking change and the release will give users the option to revert back to the current behavior.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12412:136,release,release,136,https://hail.is,https://github.com/hail-is/hail/pull/12412,1,['release'],['release']
Deployability,This is going to be really challenging to deploy! Can you bump the instance version? The issue is the new CI build will probably get run on old batch workers that have the old copying mechanism.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10131#issuecomment-790044318:42,deploy,deploy,42,https://hail.is,https://github.com/hail-is/hail/pull/10131#issuecomment-790044318,1,['deploy'],['deploy']
Deployability,"This is great, not having to enumerate the dependencies. Hmm, this is potentially making the build 2x slower. Your branch:. > 17 | build_hail | Complete | Success 🎉 (0) | 8 minutes | log. A master deploy a few moments ago:. > 16 | build_hail | Complete | 0 | 4 minutes | log. The cluster might have been under load when your tests run. Can you do a bit of benchmarking to see if this is real?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6248#issuecomment-498529082:197,deploy,deploy,197,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498529082,1,['deploy'],['deploy']
Deployability,"This is great, thanks @cseed! I've tried the gradlew option, and it worked well on Debian Jessie with java-8-oracle. `./gradlew installDist` worked, and the majority of the tests passed in `./gradlew check` (4 failed; I can give you the details if this is useful). On Ubuntu 16.04 with java-8-openjdk (the default) I get an error:. ```; :compileJava UP-TO-DATE; :compileScala; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:80: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:98: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:424: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; three errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; ```. Is this a dependency on java-8-oracle do you think?. My immediate problem is solved, as I have hail running now, so this is just out of curiosity really. l think it would be good for new users if you could nail down the dependencies a bit more precisely. For testing and development, it's also really useful to be able to spin up a quick Ubuntu VM, apt-get install a few packages and make a fresh install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240346120:128,install,installDist,128,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240346120,3,['install'],"['install', 'installDist']"
Deployability,"This is half code cleanup and half guardrail. The key assertion here is: if the request authenticates using a cookie and is attempting a state-changing HTTP method, it should pass a CSRF check. I tested this with the following:; 1. dev deployed and loaded the billing projects page; 2. Deleted the hidden csrf input from one of the forms; 3. Submitted the form; 4. Got a 401",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13604:236,deploy,deployed,236,https://hail.is,https://github.com/hail-is/hail/pull/13604,1,['deploy'],['deployed']
Deployability,"This is in implementation of `linear_regression_rows` that does not rely on any `MatrixToTableApply` nodes. Once `TableKeyBy` is lowered, this should be executable on the service. There are lingering issues:. 1. `TableGroupWithinPartitions` is likely not the right abstraction. It forgets about keying, which forces me to rekey and scan the table even though it's already in order. 2. I don't support chained linear regression (the situation where `y` is a list of lists of phenotypes). I just throw an error there for now. . 3. It's not as fast as the current `linear_regression_rows` (addressing problem 1 should help with this). 4. I don't yet support the `pass_through` field. I want to PR this now because I would like to get the benchmark in so I can continue to measure how this performs in comparison to the current version of `linear_regression_rows`. The tests of this method also serve as useful integration tests for lots of NDArray functionality. Additionally, it'll make it easier to make a smaller PR in the future that adds the new `TableIR` that will hopefully be more suitable than `TableGroupWithinPartitions`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8757:907,integrat,integration,907,https://hail.is,https://github.com/hail-is/hail/pull/8757,1,['integrat'],['integration']
Deployability,"This is indeed tricky, but I think there are other options here. For example, we could expand the spec so that a job specifies *either* a job_id or update_relative_job_id, and same for parents. This way the relative vs absolute job id is baked into the schema of the spec, and not inferred from the sign of the job id. Though similar in concept, I think that would be much less confusing. However,. > We can simplify things if we require all updates make two requests to the server to (1) get the start id and establish the update and then (2) submit new jobs with all absolute job IDs. I'd like to try this first. I feel like if we get a really solid API and it has a couple of superfluous requests in some edge cases, we will be able to come up with good performance shortcuts that don't muddle the normal path. Since the Query Driver currently lives the full life of the batch and is likely to stay that way for a while, it will satisfy these conditions without making any extra requests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1215951685:442,update,updates,442,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215951685,4,['update'],"['update', 'updates']"
Deployability,"This is intended to solve the issue with `delete_batch_instances` running before the batch deployment has been deleted. I added a ""cleanup"" job type to RunImage jobs. The default is the current behavior.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11751:91,deploy,deployment,91,https://hail.is,https://github.com/hail-is/hail/pull/11751,1,['deploy'],['deployment']
Deployability,"This is just an update of #7951. Merged into a more recent master. . I still need to study it to see the filesystem burden. It does seem that the comb-ops are being distributed, but I ran it on gnomAD just to try it, and it failed with HDFS out of space during combops.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8370#issuecomment-604766994:16,update,update,16,https://hail.is,https://github.com/hail-is/hail/pull/8370#issuecomment-604766994,1,['update'],['update']
Deployability,"This is kind of an annoying problem to have because these pip installed versions are frozen in time. I feel like these steps are either redundant (nothing has changed), or will fail because we've updated the checks to improve on what we had at last release. What do these steps really do for us?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11502#issuecomment-1061003165:62,install,installed,62,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1061003165,3,"['install', 'release', 'update']","['installed', 'release', 'updated']"
Deployability,"This is largely working, but IIRC was facing some bug in Azure. I would start by re-running the tests, then dev deploying and running any failing tests yourself.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14076#issuecomment-2239861081:112,deploy,deploying,112,https://hail.is,https://github.com/hail-is/hail/pull/14076#issuecomment-2239861081,1,['deploy'],['deploying']
Deployability,"This is mostly cleanup reducing the amount of the codebase that depends on GSA key files (which are not available in terra or a future keyless hail-vdc). The core bit is instead of threading `credentials_file=""/gsa-key/key.json` through the batch and auth codebase we set `GOOGLE_APPLICATION_CREDENTIALS` in their deployments. I can't think of a scenario where `auth` or `batch` would need to use multiple identities so better to remove their ability to do so and always use the default credentials. I also did a bit of tidying up, using `$GOOGLE_APPLICATION_CREDENTIALS` instead of the hard-coded path and removing the credentials endpoints on the batch-driver which have been unused by workers for many months now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13596:314,deploy,deployments,314,https://hail.is,https://github.com/hail-is/hail/pull/13596,1,['deploy'],['deployments']
Deployability,"This is mostly for the australians, it means that this commit must be fully deployed before they can deploy following migrations, since we need the changes to the batch-driver that this introduces before the introduce the cleanup migration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12117#issuecomment-1245574443:76,deploy,deployed,76,https://hail.is,https://github.com/hail-is/hail/pull/12117#issuecomment-1245574443,2,['deploy'],"['deploy', 'deployed']"
Deployability,"This is mostly hooked up but not fully tested. I'd start by dev deploying this and submitting jobs, checking to see if any of the driver's requests to the worker (like in the `schedule_job` function) are rejected.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14622#issuecomment-2243018782:64,deploy,deploying,64,https://hail.is,https://github.com/hail-is/hail/pull/14622#issuecomment-2243018782,1,['deploy'],['deploying']
Deployability,"This is my original `which` patch from almost a year ago repurposed for hailctl. I have been running this locally for cloudtools. Uses `shutil.which` to find `chromium` or `chromium-browser`, falling back to the macOS default if they cannot be found.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6280:28,patch,patch,28,https://hail.is,https://github.com/hail-is/hail/pull/6280,1,['patch'],['patch']
Deployability,This is necessary for the user account to execute (call) stored procedures. We'll have to update batch2/deployed users by hand.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7397:90,update,update,90,https://hail.is,https://github.com/hail-is/hail/pull/7397,2,"['deploy', 'update']","['deployed', 'update']"
Deployability,This is needed for Siwei's recursive pipeline to work.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12662:37,pipeline,pipeline,37,https://hail.is,https://github.com/hail-is/hail/pull/12662,1,['pipeline'],['pipeline']
Deployability,"This is not because we forgot to unfreeze CI, we just have simply never added the dockerhub images to azure automatically. The couple that are there now (only 107 and 112) must have been uploaded manually. Because there are some build.yaml steps that run on deploy that are specific to the broad GCP instance (like maybe making a release), non-hail-vdc instances don't run the whole build.yaml pipeline on deploy, but a subset that are specified through terraform (this is how AUS and MS could decide to only deploy a subset of our services e.g. not monitoring. We somewhat recently added a step (separate from the `deploy` step) called `mirror_hailgenetics_images` that was entirely intended so that other hail deployments (including ourselves on Azure!) could pick up the images that we released to dockerhub. I never added that steps to the Azure CI's config. I have done that now. Somehow I had foreseen this incident happening and when it actually did any prior on it disappeared from my brain entirely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1572657390:258,deploy,deploy,258,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1572657390,16,"['deploy', 'pipeline', 'release']","['deploy', 'deployments', 'pipeline', 'release', 'released']"
Deployability,This is now available in 4.80.0 through 5.2.0. Work for this issue is:. 1. Upgrade to latest 4.x.x; 2. Encode cleanup policies in terraform.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13504#issuecomment-1773465652:75,Upgrade,Upgrade,75,https://hail.is,https://github.com/hail-is/hail/issues/13504#issuecomment-1773465652,1,['Upgrade'],['Upgrade']
Deployability,"This is now ready to be reviewed. @danking Could you please help me setup the tests to run on the CI?. @catoverdrive This is an example of the interface and the output generated. There's also a tests file in there. I'm happy to explain the design to you if you'd like. ```python3; from pipeline import Pipeline. p = Pipeline() # initialize a pipeline. # Define mapping for taking a file root to a set of output files; bfile = {'bed': '{root}.bed', 'bim': '{root}.bim', 'fam': '{root}.fam'}. # Import a file as a resource; file = p.read_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.read_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .docker('ubuntu'); .declare_resource_group(tmp1=bfile, ofile=bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:286,pipeline,pipeline,286,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282,4,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline']"
Deployability,"This is now the first PR in the deduping resource ids stack. I decided that we should directly write the new ""deduped_resource_id"" value rather than in a trigger to make things simpler to reason about later when dropping unused columns. The implication of this is this PR must now be fully deployed before #12757. We need to announce this on Zulip once this PR merges.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12786:290,deploy,deployed,290,https://hail.is,https://github.com/hail-is/hail/pull/12786,1,['deploy'],['deployed']
Deployability,This is obviated by my imminent aggregator registry changes. The L suffix change should be a separate PR. I'll integrate the tests into my branch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1051#issuecomment-262054090:111,integrat,integrate,111,https://hail.is,https://github.com/hail-is/hail/pull/1051#issuecomment-262054090,1,['integrat'],['integrate']
Deployability,This is only a problem when hand deploying batch.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9130:33,deploy,deploying,33,https://hail.is,https://github.com/hail-is/hail/pull/9130,1,['deploy'],['deploying']
Deployability,"This is part of a minor bug-fix for how CI tracks active namespaces in development environments. #12093 added a table in CI's database that tracks the namespaces that CI is currently aware exists and that it manages. Non-production CIs don't really use this table (dev CIs don't update gateway routing or anything), but it's useful in development to see that CI is properly updating its database tables as it would in prod. In #12093, the migration that adds the `active_namespaces` table explicitly adds entries for the `default` namespace. However, I'm now realizing that this is a mistake, because a CI in the `dgoldste` namespace shouldn't have an entry for `default`, it should instead have an entry for `dgoldste`. That migration should have been parametrized by the namespace the migration is happening in. To do this, I need to expose the current namespace to the migration scripts, hence this PR. This is unfortunately a change to CI that needs to be deployed before it can be used by migrations. If this seems good then I'll alert the australians that they should fully deploy this PR prior to the following ones.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12755:279,update,update,279,https://hail.is,https://github.com/hail-is/hail/pull/12755,3,"['deploy', 'update']","['deploy', 'deployed', 'update']"
Deployability,"This is primarily a condensation and formatting of what you have above. Is there more to document?. ### `filteralleles`. #### Usage; - `-c | --condition <expr>`—a hail language expression, the following table describes the variables in the scope of `<expr>`. | Name | Description |; | --- | --- |; | `v` | variant |; | `va` | variant annotations |; | `aIndex` | allele index |; - `--keep/--remove`—keep or remove the allele if the expression is true; - `-a | --annotation <expr>`—a hail language expression which may update the variant annotations based on the removed alleles, the following table describes the variables in the scope of `<expr>`. | Name | Description |; | --- | --- |; | `v` | the _new_ variant |; | `va` | the _old_ variant annotations |; | `aIndices` | an array of the old indices (such that `aIndices[newIndex] = oldIndex`) |. _NB:_ the allele indices are zero indexed and the zeroth index contains the reference. The `condition` expression will be executed for all alleles with index greater than zero. _NB2:_ if all alternate alleles are filtered the entire variant is filtered. #### Examples. ```; ... filteralleles; --keep; -c 'va.alleleQuality[aIndex] > 0.8'; -a 'va.info.AC = aIndices.map(oldIndex => va.info.AC[oldIndex]), va.info.AN = ...'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/551#issuecomment-240216457:517,update,update,517,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240216457,1,['update'],['update']
Deployability,"This is probably a memory leak. Your build is from 2 months ago, and there are several commits since then that change memory management and fix bugs. I'm nearly certain this problem won't exist in the latest build! Can you update? The current build will also be MUCH faster in many cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3901#issuecomment-403295450:223,update,update,223,https://hail.is,https://github.com/hail-is/hail/issues/3901#issuecomment-403295450,1,['update'],['update']
Deployability,"This is runIfRequested deployment that simply has hail; and ipython installed. It facilitates developmnent of; services that interact with Hail Query (i.e. the Shuffler). I do this silly thing with a tar file because:; - I do not know the hail version (which is included in the wheel filename), so; - I am unable to copy it out with a variable name, and; - python refuses to install a wheel that does not have the version in the filename. I added `make update-hail-repl` to `hail/Makefile` which updates the hail wheel on the hail-repo without changing the pod or rebuilding the image. If the pod is restarted you lose your version, but the risk is worth the immense benefit of 5s deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8194:23,deploy,deployment,23,https://hail.is,https://github.com/hail-is/hail/pull/8194,6,"['deploy', 'install', 'update']","['deployment', 'deploys', 'install', 'installed', 'update-hail-repl', 'updates']"
Deployability,"This is super cool! I am a big fan of the idea and the overall approach, particularly when it comes to setting up the tmp bucket and getting the permissions on it correct. Here's my high level thoughts. Sorry for the wall of text but I found these a little hard to articulate. ### Regarding number of prompts. I think this is my primary concern. There's a lot of great automation here, but it's a lot right off the bat. I think what this is aiming to do is make it quick and simple to start running batches and every time someone has to stop and ask someone a question as to how they should respond to some prompt that process gets longer and more complicated. I think it's worth considering what the first batch people should run might be and design for a minimal first experience. IMO, a temp bucket is an absolutely crucial piece of configuration before you can do anything interesting and configuring a temp bucket is something that `hailctl` can easily be very opinionated about. Container registry… I feel like there's harder questions there, and you can run a lot of cool batches before having to worry about provisioning your own. It's also not actually a part of the hailctl config (unless something has changed recently) so it feels a little unusual in this flow. I still think that it is helpful to set people up with an AR and keep them from footguns, but maybe that can go in a separate command that the initial init command points to once you're done? Something along the lines of ""if you get to the point where you need to upload custom container images, you can use hailctl to set up a registry""?. Another thing that gives me a little pause is the wording around google projects. I get that you need one to create a bucket, but I think we should just make sure to steer clear of the implication that you are ""selecting a GCP project to use for Hail Batch"", because that implies some link or ownership that isn't there. But I think there's a quick fix here: for a given resource that we",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012:836,configurat,configuration,836,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012,2,['configurat'],['configuration']
Deployability,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:343,configurat,configuration,343,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881,2,"['configurat', 'integrat']","['configuration', 'integrated']"
Deployability,"This is the beginning of a series of changes to support export of VDS to VCF 4.5, the version of VCF that contains the standardized form of our work that culminated in SVCR/VDS. Reference blocks were standardized with a LEN rather than an END. So, now, by default, add LEN to all VDS reads and drop END in favor of LEN on all VDS writes. Our optimizer will be able to take care of pruning away the dead field in pipelines that don't use it. We make sure that all VDS creation (other than the combiner), such as read_vds and from_merged_representation, contains both LEN and END preserving user code that depends on the presence of the END field. Furthermore, this change contains necessary combiner updates to prefer LEN over END, and to use LEN in the combiner itself.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14675:412,pipeline,pipelines,412,https://hail.is,https://github.com/hail-is/hail/pull/14675,2,"['pipeline', 'update']","['pipelines', 'updates']"
Deployability,"This is the current state of the C++ support. If you look at the tests in src/test/is/hail/nativecode/NativeCodeSuite.scala that should give a; quick overview of how it works, viz. 1. Generate C++ source code as a Scala String, then create a NativeModule which handles; the grunt work of getting it compiled, linked, and loaded, and allows you to look up functions; by name, and get a callable Scala object corresponding to the C++ function. 2. The NativeModule also allows the binary of the DLL to be passed around and instantiated; on other cluster nodes (but note that those nodes will need to have the correct versions of; the C++ runtime shared libraries in the right directories to allow symbols in the DLL to be; correctly resolved). This is not tested yet. 3. I have been using llvm-6.0.0 on Mac, and llvm-5.0 on linux. It makes a half-hearted attempt; to use whatever other compiler you have, but that may not work. We probably need to figure; out a standardized and automated way to get the right tools installed in the right place (and; get the right shared libraries on worker nodes). 4. Data which needs to be accessible to both Scala and C++ is held in C++ objects inheriting; from NativeObj, with lifetimes managed by std::shared_ptr, i.e. reference-counted. There's; some dirty under-the-hood plumbing to allow a shared_ptr to be smuggled into a Scala; object derived from NativeBase. These Scala-side object references must be managed; carefully using copyAssign/moveAssign/close in order to maintain the off-heap ref-counts. 5. There are some gnarly differences between Linux and MacOSX in the linker and dynamic; loading. I think I'm converging on the right compile/link options for each, but in getting; Linux to work it's possible that Mac is temporarily broken ... Not really expecting that we'll merge this right away, but I wanted to put it out there to get the; review process started before it grows any bigger.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3461:1013,install,installed,1013,https://hail.is,https://github.com/hail-is/hail/pull/3461,1,['install'],['installed']
Deployability,"This is the initial version of the ATGU intranet service. Currently, it has a curated list of resources which can be created, viewed, edited and deleted. Resources can have attachments, which I store on Google storage. I used the async Google Storage client and it worked very nicely with aiohttp. Right now this developers only. I may give access to the admins to start curating resources. I'll follow up with roles and add roles for atgu-viewer and atgu-editor that I'll use in the service. I think the code is mostly straightforward, but a few remarks:. This is built on Bootstrap. It doesn't share the the styling with web common (which I probably want to convert). On the resources page, for client side search I use fuse.js: https://fusejs.io/ (so fast). For a rich text box (the resource content), I use quill.js: https://quilljs.com/. Quill doesn't allow you to post its contents in a form, so I use a JS event handler to populate a hidden input with the contents on submission. I tested it with dev deploy. I suggest you do the same before reviewing to get a sense of what's here. Feedback on the UI welcome.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9684:1008,deploy,deploy,1008,https://hail.is,https://github.com/hail-is/hail/pull/9684,1,['deploy'],['deploy']
Deployability,"This is the most recently deployed version of monitoring.yaml. I'm not sure the best way to test it solves the problem that deployments faced. One thing to note is that StatefulSets don't guarantee that all of their constituent pods get deleted when the StatefulSet is deleted. To be sure the pods all get deleted, we'd have to either manually delete them or scale the StatefulSet size down to 0 before deleting it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6343:26,deploy,deployed,26,https://hail.is,https://github.com/hail-is/hail/pull/6343,2,['deploy'],"['deployed', 'deployments']"
Deployability,This is the setup for the website. Some remarks:; - We run a web server (nginx) in Kubernetes (service and deployment); - It has a volume (letsencrypt-certs) that stores our SSL certificates; - Those come from Let's Encrypt. Getting new certs is totally automated (see run-letsencrypt make target).; - certbot by default installs a cron job that runs daily to renew the certs.; - Other publicly exposed services are now encrypted and go through nginx. That's set up for ci and scorecard.; - The site pod (via a cron job) polls for a new deployment every 3m for new documentation. It's not yet integrated into the ci for automatic deployment (and testing?),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4403:107,deploy,deployment,107,https://hail.is,https://github.com/hail-is/hail/pull/4403,5,"['deploy', 'install', 'integrat']","['deployment', 'installs', 'integrated']"
Deployability,"This is useful for falling back to Dataproc clusters in Batch pipelines, until Query is feature-complete.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10161:62,pipeline,pipelines,62,https://hail.is,https://github.com/hail-is/hail/pull/10161,1,['pipeline'],['pipelines']
Deployability,"This is what `hailctl` looks like:. ```. Usage: hailctl [OPTIONS] COMMAND [ARGS]... Manage and monitor hail deployments. ╭─ Options ────────────────────────────────────────────────────────────────────────────────────────────╮; │ --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified │; │ shell. │; │ [default: None] │; │ --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified │; │ shell, to copy it or customize the │; │ installation. │; │ [default: None] │; │ --help Show this message and exit. │; ╰──────────────────────────────────────────────────────────────────────────────────────────────────────╯; ╭─ Commands ───────────────────────────────────────────────────────────────────────────────────────────╮; │ batch Manage batches running on the batch service managed by the Hail team. │; │ config Manage Hail configuration. │; │ curl Issue authenticated curl requests to Hail infrastructure. │; │ version Print version information and exit. │; ╰──────────────────────────────────────────────────────────────────────────────────────────────────────╯; ```. This is what `hailctl batch submit --help` looks like:. ```. Usage: hailctl batch submit [OPTIONS] SCRIPT [ARGUMENTS]... Submit a batch with a single job that runs SCRIPT with the arguments ARGUMENTS. ╭─ Arguments ──────────────────────────────────────────────────────────────────────────────────────────╮; │ * script PATH Path to the script [default: None] [required] │; │ arguments [ARGUMENTS]... [default: None] │; ╰──────────────────────────────────────────────────────────────────────────────────────────────────────╯; ╭─ Options ────────────────────────────────────────────────────────────────────────────────────────────╮; │ --files PATH Files or directories to add to the working directory of the │; │ job. │; │ [default: None] │; │ --name TEXT The name of the batch. │; │ --image-name TEXT Name of Docker image for the job │; │ [default: (hailgenetics/hail)] │; │ --ou",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13109#issuecomment-1561081921:108,deploy,deployments,108,https://hail.is,https://github.com/hail-is/hail/pull/13109#issuecomment-1561081921,5,"['Install', 'configurat', 'deploy', 'install']","['Install', 'configuration', 'deployments', 'install-completion', 'installation']"
Deployability,"This is why copying is so slow:. ```; ==> NOTE: You are uploading one or more large file(s), which would run; significantly faster if you enable parallel composite uploads. This; feature can be enabled by editing the; ""parallel_composite_upload_threshold"" value in your .boto; configuration file. However, note that if you do this large files will; be uploaded as `composite objects; <https://cloud.google.com/storage/docs/composite-objects>`_,which; means that any user who downloads such objects will need to have a; compiled crcmod installed (see ""gsutil help crcmod""). This is because; without a compiled crcmod, computing checksums on composite objects is; so slow that gsutil disables downloads of composite objects. / [1/1 files][ 4.1 GiB/ 4.1 GiB] 100% Done 45.8 MiB/s ETA 00:00:00; Operation completed over 1 objects/4.1 GiB.; ```. We can also set this with -o GSUtil:parallel_composite_upload_threshold on the command line. https://cloud.google.com/storage/docs/gsutil/commands/cp. We currently use `-m` which is parallel per-file:. If you have a large number of files to transfer you might want to use the; gsutil -m option, to perform a parallel (multi-threaded/multi-processing); copy:. gsutil -m cp -r dir gs://my-bucket",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024:277,configurat,configuration,277,https://hail.is,https://github.com/hail-is/hail/pull/7024,2,"['configurat', 'install']","['configuration', 'installed']"
Deployability,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:154,deploy,deploy,154,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251,3,['deploy'],['deploy']
Deployability,"This isn't as easy as I had hoped. We have to sort out how to either let the container directly create overlay mounts or figure out how to get fuse-overlay working. I think for fuse-overlay, we might need to modify the VM image to include fuse-overlay. ```; + set +x; Using GOOGLE_APPLICATION_CREDENTIALS; + export TMPDIR=/io/; + TMPDIR=/io/; + retry buildah build -t us-docker.pkg.dev/hail-vdc/hail/git-make-bash:test-deploy-j6d7pph9mlzf -f /Dockerfile --cache-from us-docker.pkg.dev/hail-vdc/hail/cache --cache-to us-docker.pkg.dev/hail-vdc/hail/cache --layers /io; + buildah build -t us-docker.pkg.dev/hail-vdc/hail/git-make-bash:test-deploy-j6d7pph9mlzf -f /Dockerfile --cache-from us-docker.pkg.dev/hail-vdc/hail/cache --cache-to us-docker.pkg.dev/hail-vdc/hail/cache --layers /io; STEP 1/2: FROM us-docker.pkg.dev/hail-vdc/hail/ubuntu:20.04; Trying to pull us-docker.pkg.dev/hail-vdc/hail/ubuntu:20.04...; Getting image source signatures; Copying blob sha256:ca1778b6935686ad781c27472c4668fc61ec3aeb85494f72deb1921892b9d39e; Copying config sha256:88bd6891718934e63638d9ca0ecee018e69b638270fe04990a310e5c78ab4a92; Writing manifest to image destination; Storing signatures; time=\""2023-05-26T14:52:12Z\"" level=error msg=\""Unmounting /var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/merged: invalid argument\""; Error: mounting new container: mounting build container \""45e0ed631d22b6e1de7945266efcf0b802aa3b919d6b6ebd529ded6fedc11cf9\"": creating overlay mount to /var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/merged, mount_data=\""lowerdir=/var/lib/containers/storage/overlay/l/ZCKOX3GV2VWHWT4DMPLYJGMJWL,upperdir=/var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/diff,workdir=/var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/work,nodev,fsync=0,volatile\"": using mount program /usr/bin/fus",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13103#issuecomment-1564774692:419,deploy,deploy-,419,https://hail.is,https://github.com/hail-is/hail/pull/13103#issuecomment-1564774692,2,['deploy'],['deploy-']
Deployability,"This isn't really our fault. k8s sends us a 400 when a container is in a funky state. Creating this issue so I can find it again later when I run into this. Somehow a container terminates without timing information, and the read logs request returns a 400 instead of a 404. Batch handles this fine (it treats all log read failures the same). Known issue: https://github.com/kubernetes/kubernetes/issues/59296. ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1145"", ""message"": ""update job (9, 1, 'main') with pod batch-9-job-1-c8b9b2""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-11 14:19:39,890"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1159"", ""message"": ""job (9, 1, 'main') mark complete""}; {""levelname"": ""WARNING"", ""asctime"": ""2019-07-11 14:19:39,899"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""mark_complete:579"", ""message"": ""job (9, 1, 'main') has pod batch-9-job-1-c8b9b2 which is terminated but has no timing information. {'api_version': 'v1',\n 'kind': 'Pod',\n 'metadata': {'annotations': None,\n 'cluster_name': None,\n 'creation_timestamp': datetime.datetime(2019, 7, 11, 14, 19, 34, tzinfo=tzlocal()),\n 'deletion_grace_period_seconds': 30,\n 'deletion_timestamp': datetime.datetime(2019, 7, 11, 14, 20, 4, tzinfo=tzlocal()),\n 'finalizers': None,\n 'generate_name': None,\n 'generation': None,\n 'initializers': None,\n 'labels': {'app': 'batch-job',\n 'batch_id': '9',\n 'hail.is/batch-instance': 'ffa5abc4607849df8e5f0036e7350bcf',\n 'job_id': '1',\n 'task': 'main',\n 'user': 'test',\n 'uuid': '291b9eed73b9433c86ff1f58624cf24d'},\n 'name': 'batch-9-job-1-c8b9b2',\n 'namespace': 'pr-6604-batch-pods-cjklalqnl5u9',\n 'owner_references': None,\n 'resource_version': '86681671',\n 'self_link': '/api/v1/namespaces/pr-6604-batch-pods-cjklalqnl5u9/pods/batch-9-job-1-c8b9b2',\n 'uid': 'e878f906-a3e6-11e9-a4bb-42010a8000af'},\n 'spec': {'active_deadline_seconds': None,\n 'affinity': ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6616:558,update,update,558,https://hail.is,https://github.com/hail-is/hail/issues/6616,1,['update'],['update']
Deployability,This isn't used / breaking anything but I noticed that `update-fast` returns the batch id not the update id. The update id makes more sense and aligns with the create-fast endpoint where it returns the id of the thing that it made.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12233:56,update,update-fast,56,https://hail.is,https://github.com/hail-is/hail/pull/12233,3,['update'],"['update', 'update-fast']"
Deployability,This latent bug was triggered by https://github.com/hail-is/hail/commit/219f7a48d5592c7f2d86cd65f4134eec9d2ac680 which was released with 0.2.114.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245#issuecomment-1924694888:123,release,released,123,https://hail.is,https://github.com/hail-is/hail/issues/14245#issuecomment-1924694888,1,['release'],['released']
Deployability,"This leverages open batches to submit all QoB stages as updates to the same batch the Query Driver is running in. Most of this implementation feels uncontroversial, but there is a backwards-incompatible change to the JVMJob that I don't love. I needed to let the Query Driver know which batch it's running in and did so by adding that as another argument to main. I initially wanted this as an environment variable, but considering that these containers are long-lived I wasn't quite sure how to do that.; I didn't stack this on open batches so this won't work until that goes in but the changes are entirely disjoint",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12222:56,update,updates,56,https://hail.is,https://github.com/hail-is/hail/pull/12222,1,['update'],['updates']
Deployability,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:203,integrat,integrating,203,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731,2,['integrat'],['integrating']
Deployability,This makes hand running the release slightly less error-prone.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14325:28,release,release,28,https://hail.is,https://github.com/hail-is/hail/pull/14325,1,['release'],['release']
Deployability,"This may be an environment specific bug, but we have showed it only occurs with releases after 0.2.42 (in our environment) and only under very specific conditions. When we run `subject_qc` on a matrix table with >= 354 partitions using an external spark cluster (i.e. specifying `master` in `hail.init`), the spark worker crashes with a SIGSEGV. The issue does not occur with `variant_qc` but we do not know know the extent of what specific operations trigger it. Below is a test that consistently triggers the issue:. Setup:. $ $SPARK_HOME/sbin/start-master.sh --host localhost --port 7077; $ $SPARK_HOME/sbin/start-shuffle-service.sh; $ $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077 --work-dir /scratch/local/. Test:. import hail; hail.init(master=""spark://localhost:7077""); P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:80,release,releases,80,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['release'],['releases']
Deployability,This means people who don't use hailctl get these configurations.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7739:50,configurat,configurations,50,https://hail.is,https://github.com/hail-is/hail/pull/7739,1,['configurat'],['configurations']
Deployability,"This means that hail (or something on which hail depends) is trying to call into BLAS. BLAS is a Fortran library and is often shipped with C bindings. The symbol `cblas_dgemv` is a C function. Your machine is likely missing `libcblas`. Can you post the output of these commands:; - `nm -g /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so` (this may say that the file doesn't exist, in which case skip the next command; - `objdump -TC /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so`. Can you also answer these questions:; - What distribution are you using?; - What version of that distribution do you have?; - What package management tool do you use?. If you're on Ubuntu, can you tell me what version of `libatlas-dev` you have installed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-238879581:770,install,installed,770,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-238879581,1,['install'],['installed']
Deployability,"This might fix the bug you're seeing. Capacity (which should have been tracked separately) wasn't getting updated correctly in copyFrom, so if you did two copyFrom's, the second one would truncate the data.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5607:106,update,updated,106,https://hail.is,https://github.com/hail-is/hail/pull/5607,1,['update'],['updated']
Deployability,"This moves the nginx proxy configuration out of router and into a sidecar in the notebook/workshop pod. This extends TLS termination from router to the notebook pod and consolidates the notebook routing logic. I didn't run a scale test but this doesn't change any functionality, and I tested in dev that I could log in to a workshop, start and open a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10204:27,configurat,configuration,27,https://hail.is,https://github.com/hail-is/hail/pull/10204,1,['configurat'],['configuration']
Deployability,This must have been skipped or lost in a rebase but keeping the resources at minimum is keeping the notebook deployment at max replicas after the workshop. Giving a bit higher request (what I set as the baseline in #10117) should encourage k8s to downscale back to 3 replicas.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10230:109,deploy,deployment,109,https://hail.is,https://github.com/hail-is/hail/pull/10230,1,['deploy'],['deployment']
Deployability,This needs to be done at least once. The root SSL/TLS key is used to sign all; other keys used in the namespace. We do not recreate it when we deploy into; default or into dev namesapces.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9107:143,deploy,deploy,143,https://hail.is,https://github.com/hail-is/hail/pull/9107,1,['deploy'],['deploy']
Deployability,"This pains me. As currently set up, there's no way to deploy the wheel that we tested because that wheel is tightly coupled with a bunch of ""temporary"" file paths. I'm not confident I can untangle all this into a clean promote-what-you-test deploy right now. This PR should get us back to correct deploys until I can take the time to get us to proper promote-what-you-test deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8606:54,deploy,deploy,54,https://hail.is,https://github.com/hail-is/hail/pull/8606,4,['deploy'],"['deploy', 'deploys']"
Deployability,"This particular issue is that I recently added a reference to an Array of IR in the JVM that is owned by python. When the owning object is destructed, the reference in Java will be released, enabling it to be garbage collected. Python won't run a GC pass inside a function without requesting it in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5658#issuecomment-475282912:181,release,released,181,https://hail.is,https://github.com/hail-is/hail/pull/5658#issuecomment-475282912,1,['release'],['released']
Deployability,"This particularly important for rapid iteration in dev deploys. I can enable the standing worker; while I am working. I disable it when I head off to a series of meetings and then re-enable it; when I return. @jigold I think this will conflict with your worker pool change because I replace ""1 means boolean true"" with a checkbox.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9844:55,deploy,deploys,55,https://hail.is,https://github.com/hail-is/hail/pull/9844,1,['deploy'],['deploys']
Deployability,"This patch and the contents of #13970 was sufficient to successfully run `make -C hail install SPARK_VERSION=3.4.0` (but is incompatible with Spark 3.3). ```diff; diff --git a/hail/build.gradle b/hail/build.gradle; index 1b65904484..d1feb0e578 100644; --- a/hail/build.gradle; +++ b/hail/build.gradle; @@ -40,7 +40,7 @@ tasks.withType(JavaCompile) {; }; ; project.ext {; - breezeVersion = ""1.1""; + breezeVersion = ""2.1.0""; ; sparkVersion = System.getProperty(""spark.version"", ""3.3.0""); ; diff --git a/hail/src/main/scala/is/hail/HailContext.scala b/hail/src/main/scala/is/hail/HailContext.scala; index 4e4063378b..4d2f9056a5 100644; --- a/hail/src/main/scala/is/hail/HailContext.scala; +++ b/hail/src/main/scala/is/hail/HailContext.scala; @@ -113,10 +113,10 @@ object HailContext {; ; {; import breeze.linalg._; - import breeze.linalg.operators.{BinaryRegistry, OpMulMatrix}; + import breeze.linalg.operators.{BinaryRegistry, HasOps, OpMulMatrix}; ; implicitly[BinaryRegistry[DenseMatrix[Double], Vector[Double], OpMulMatrix.type, DenseVector[Double]]].register(; - DenseMatrix.implOpMulMatrix_DMD_DVD_eq_DVD); + HasOps.impl_OpMulMatrix_DMD_DVD_eq_DVD); }; ; theContext = new HailContext(backend, branchingFactor, optimizerIterations); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13971#issuecomment-1792870445:5,patch,patch,5,https://hail.is,https://github.com/hail-is/hail/issues/13971#issuecomment-1792870445,2,"['install', 'patch']","['install', 'patch']"
Deployability,"This pipeline:; ```python3; # cwd = repo_root/hail; vcf2 = hl.import_vcf('src/test/resources/gvcfs/HG00268.g.vcf.gz', force_bgz=True, reference_genome='GRCh38'); vcf1 = hl.import_vcf('src/test/resources/gvcfs/HG00096.g.vcf.gz', force_bgz=True, reference_genome='GRCh38'); vcfs = [vcf1.rows().key_by('locus'), vcf2.rows().key_by('locus')]; ht = hl.Table.multi_way_zip_join(vcfs, 'data', 'new_globals'); ht._force_count(); ```; Fails with:; ```; java.lang.IllegalArgumentException: requirement failed; at scala.Predef$.require(Predef.scala:224); at is.hail.rvd.RVD.<init>(RVD.scala:46); at is.hail.rvd.RVD$.apply(RVD.scala:1411); at is.hail.expr.ir.TableMultiWayZipJoin.execute(TableIR.scala:925); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8027:5,pipeline,pipeline,5,https://hail.is,https://github.com/hail-is/hail/issues/8027,1,['pipeline'],['pipeline']
Deployability,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9066:153,configurat,configuration,153,https://hail.is,https://github.com/hail-is/hail/pull/9066,2,['configurat'],['configuration']
Deployability,"This really does look great! I have two small suggestions:. 1. I feel like you should say `read_input` and rather than `write_input`. I'm thinking these commands are from the perspective of the pipeline since they are on Pipeline. 2. Rather than building a group and then declaring it, I think you can do both at once:. ```; # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset.declare_resource_groups(tmp1={bed=""{root}.bed"", bim=""{root}.bim"", fam=""{root}.fam""}, ; ofile={...}); subset = (subset; .label('subset'); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-452846400:194,pipeline,pipeline,194,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452846400,2,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline']"
Deployability,This release bumps file version to 1.6.0. That requires #11246 to go in before releasing. This release also adds the `hail.ggplot` module. This requires #11247 to go in before releasing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11255:5,release,release,5,https://hail.is,https://github.com/hail-is/hail/pull/11255,2,['release'],['release']
Deployability,"This seems right to me. In this vein of work, there's one more thing I want: we should build this Dockerfile with the hail wheel and then execute `pylint hail && pylint hailtop`. Pylint will look for uninstalled modules. This will save us from checking in (and eventually deploying) a hail package with bad dependencies. We should probably also run the python tests against this version of hail. This is a tru, local-mode user environment. ```; FROM ubuntu:18.04. ENV LANG C.UTF-8. RUN apt-get update && \; apt-get -y install \; openjdk-8-jdk-headless \; python3 python3-pip && \; rm -rf /var/lib/apt/lists/*. COPY hail.whl pylintrc ./; RUN pip install --no-cache-dir ./hail.whl; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7031#issuecomment-530112882:272,deploy,deploying,272,https://hail.is,https://github.com/hail-is/hail/pull/7031#issuecomment-530112882,4,"['deploy', 'install', 'update']","['deploying', 'install', 'update']"
Deployability,"This seems to do it:; ```. In [1]: import hailtop.batch as hb; ...: b = hb.Batch(backend=hb.ServiceBackend()); ...: for _ in range(32):; ...: j = b.new_job(); ...: j.command(f'cat >/dev/null {"" "".join(b.read_input(""gs://danking/foo.vcf"") for _ in range(1000))}'); ...: b.run(); /Users/dking/miniconda3/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:29: UserWarning: You have specified the GCS requester pays configuration in both your spark-defaults.conf (/Users/dking/miniconda3/lib/python3.10/site-packages/pyspark/conf/spark-defaults.conf) and either an explicit argument or through `hailctl config`. For GCS requester pays configuration, Hail first checks explicit arguments, then `hailctl config`, then spark-defaults.conf.; warnings.warn(; /Users/dking/miniconda3/lib/python3.10/site-packages/hailtop/batch/backend.py:786: UserWarning: Using an image ubuntu:22.04 from Docker Hub. Jobs may fail due to Docker Hub rate limits.; warnings.warn(f'Using an image {image} from Docker Hub. '. https://batch.hail.is/batches/8090821 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 33/33 0:00:00 0:01:37; batch 8090821 complete: success; Out[1]: <hailtop.batch_client.client.Batch at 0x1086d1bd0>. In [2]: import hailtop.batch as hb; ...: b = hb.Batch(backend=hb.ServiceBackend()); ...: for _ in range(300):; ...: j = b.new_job(); ...: j.command(f'echo {""a"" * 11 * 1024}'); ...: b.run(); ```. Perhaps related to creating a fresh service backend?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14051#issuecomment-1834376467:432,configurat,configuration,432,https://hail.is,https://github.com/hail-is/hail/issues/14051#issuecomment-1834376467,2,['configurat'],['configuration']
Deployability,This seems to me like a footgun. Perhaps the methods in LowerTableIR should all; bind the CDA before handing it to the body? That will substantially change the; structure of some of the writers which assume that they receive an IR which; represents the execution of CDA which they place inside a; `RelationalWriter.scoped`. I took that approach at first and worried that it was; too large a change. This change reduces one particular pipeline I was investigating from ~20 stages; to ~5 stages.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11783:434,pipeline,pipeline,434,https://hail.is,https://github.com/hail-is/hail/pull/11783,1,['pipeline'],['pipeline']
Deployability,"This seems to save like 20 to 30 seconds out of a 2.5 minute pipeline, so like 1/6 to 1/5 saved. I'm testing this PR and the SplitMulti one (both separately and with changes combined) with:; ```; In [1]: import hail as hl; In [2]: hl.init(); In [3]: %%time ; ...: hl.split_multi_hts(hl.read_matrix_table('/Users/dking/projects/hail-data/profile.mt').select_entries('GT', 'AD', 'DP', 'GQ', 'PL'))._force_count_rows(); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3627:61,pipeline,pipeline,61,https://hail.is,https://github.com/hail-is/hail/pull/3627,1,['pipeline'],['pipeline']
Deployability,"This sets the configuration permanently -- any following commands will use the overridden codecs. Setting a global option is almost certainly better than getting this kind of leakage, I think",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/826#issuecomment-248645129:14,configurat,configuration,14,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248645129,1,['configurat'],['configuration']
Deployability,This should be fixed once https://github.com/hail-is/hail/pull/5655 goes in and the pr-builder has the standard python dependencies installed for the default python3 image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5624#issuecomment-475123834:132,install,installed,132,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-475123834,1,['install'],['installed']
Deployability,This should be split into two PRs so as not to break users pipelines until we release the new version of hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8834#issuecomment-632136987:59,pipeline,pipelines,59,https://hail.is,https://github.com/hail-is/hail/pull/8834#issuecomment-632136987,2,"['pipeline', 'release']","['pipelines', 'release']"
Deployability,This should be updated in one of my current PRs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11966#issuecomment-1169115424:15,update,updated,15,https://hail.is,https://github.com/hail-is/hail/pull/11966#issuecomment-1169115424,2,['update'],['updated']
Deployability,This should fix the deploy step.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12266:20,deploy,deploy,20,https://hail.is,https://github.com/hail-is/hail/pull/12266,1,['deploy'],['deploy']
Deployability,"This should fix the test_copy failures. The problem was, we were sending a mutable byte iterator for data when inserting a new object. If that insert read some of the data, failed, and was retried, the data that was read was lost and the retry started where the failed insert left off. To retry writes, you need to keep around the data you've sent to resend in case of failure. So you don't have to keep around an unbounded amount of data, Google Storage supports resumable uploads: https://cloud.google.com/storage/docs/resumable-uploads. This allows us to send data in chunks, and release the data after Google reports back that the data has been committed. `StorageClient.insert_object`, when the upload type is resumable (the default), takes an additional argument `bufsize`. This is the amount the amount that the writer will buffer for retries, and the size of the chunks sent to GCS. I kept around the simpler media upload type since I actually want to use it in copy (copy doesn't need to buffer because it can retry by rereading the file being copied from). This code was actually kind of hard to organize. It would be better if the code immediately wrote any incoming data instead of just buffering it until we hit the chunk size, but I didn't find a manageable way to write that. Suggestions welcome, tho it would be good to get this fix in because of the test failures.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10023:583,release,release,583,https://hail.is,https://github.com/hail-is/hail/pull/10023,1,['release'],['release']
Deployability,"This should go in. For 0-argument functions, you should support both with and without parens for now. We can make it more strict after the 0.1 release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1411#issuecomment-283861433:143,release,release,143,https://hail.is,https://github.com/hail-is/hail/pull/1411#issuecomment-283861433,1,['release'],['release']
Deployability,"This should include performance experiments we use to drive design decisions (e.g., Array vs. Vector) so we know what assumptions change when we upgrade Scala/Spark/JVM or underlying abstractions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/19#issuecomment-152269229:145,upgrade,upgrade,145,https://hail.is,https://github.com/hail-is/hail/issues/19#issuecomment-152269229,1,['upgrade'],['upgrade']
Deployability,This should probably not be in the release script. It should just run on every deploy. The original PR's title suggested this was our intention https://github.com/hail-is/hail/pull/13703,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14154#issuecomment-1889761766:35,release,release,35,https://hail.is,https://github.com/hail-is/hail/issues/14154#issuecomment-1889761766,2,"['deploy', 'release']","['deploy', 'release']"
Deployability,"This should really speed up heavy table-join pipelines, especially; ones that involve foreign-key joins, by permitting the pruner to; work on that kind of pattern.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4122:45,pipeline,pipelines,45,https://hail.is,https://github.com/hail-is/hail/pull/4122,1,['pipeline'],['pipelines']
Deployability,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:298,install,install,298,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807,1,['install'],['install']
Deployability,"This specifically pertains to the *cgroup* of the user container. This command will probably *not* work inside the user container's mount namespace as 1. they wouldn't have `gcsfuse` installed and 2. we don't give them the capabilities necessary to set up a FUSE mount. `nsenter` should allow us to invoke `gcsfuse` inside the user's cgroup to attribute any memory usage to the user and not as part of the batch container. The tricky bit is that currently we run `gcsfuse`/`blobfuse` before the container is created. Because `crun` currently creates/destroys the cgroup, it does not yet exist when these mounts are set up. It *might* be possible to use [OCI hooks](https://github.com/opencontainers/runtime-spec/blob/main/config.md#prestart) to run `gcsfuse`/`blobfuse` after the container and corresponding cgroup is created but before user code is run.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13996#issuecomment-1804219578:183,install,installed,183,https://hail.is,https://github.com/hail-is/hail/issues/13996#issuecomment-1804219578,1,['install'],['installed']
Deployability,This still needs the following modifications to `install_bootstrap_dependencies.sh`:; - Also install the `gke-gcloud-auth-plugin`; - Install the docker buildx plugin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14625#issuecomment-2239160292:93,install,install,93,https://hail.is,https://github.com/hail-is/hail/pull/14625#issuecomment-2239160292,2,"['Install', 'install']","['Install', 'install']"
Deployability,"This subsumes #4540, which I'll close once I think I can get the rest of this finished tonight. Currently I've got... something that passes tests in Scala, although I haven't gone through and cleaned up the code for the SeqOps and other CodeAggregator stuff yet. Need to go through and push the changes into python but I thought I'd open this as a status update. @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4555:355,update,update,355,https://hail.is,https://github.com/hail-is/hail/pull/4555,1,['update'],['update']
Deployability,This test takes up to fifteen minutes on QoB and can be a long tail for CI pipelines so I broke it up into smaller tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13048:75,pipeline,pipelines,75,https://hail.is,https://github.com/hail-is/hail/pull/13048,1,['pipeline'],['pipelines']
Deployability,"This test times out occasionally, as far as we can tell due to unavailability of g2 machines. We've determined that for the time being we will allow the test to time out so it does not interfere with the CI pipeline.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14435:207,pipeline,pipeline,207,https://hail.is,https://github.com/hail-is/hail/pull/14435,1,['pipeline'],['pipeline']
Deployability,"This test:. ```python3; p = Pipeline(backend=BatchBackend('https://batch.hail.is')); for _ in range(30000):; p.new_task().command('/bin/true'); p.run(); ```. Revealed a number of issues:; daniel king: Problems Found:; - [x] https://github.com/hail-is/hail/issues/6543 mysql can deadlock itself, requiring you to reissue the db request; - [x] https://github.com/hail-is/hail/issues/6545 of the 20760 pods that were successfully created before #6543 happened, about 800 could not get their logs due to not existing. That's a failure rate of ~4%. The number of failures continues to grow as I type this message (now up to 1280). I'm counting failures this way:; ```; k logs -l app=batch --tail=999999 | grep 'no logs for ' | sed -E 's/^.*no logs for ([^ ]+).*$/\1/' | sort -u | wc -l; ```; - the k8s request latency spiked to 3.47s max 0.6 s mean during this test and stayed elevated for 10 minutes.; - [ ] https://github.com/hail-is/hail/issues/6546 there was a lot of volume mount failures due to, apparently, the secrets, e.g.:; ```; 9m13s Warning FailedMount Pod Unable to mount volumes for pod ""batch-278-job-10258-a49a81_batch-pods(82ea5910-9ccb-11e9-ad88-42010a800049)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-278-job-10258-a49a81"". list of unmounted volumes=[gsa-key default-token-8h99c]. list of unattached volumes=[gsa-key default-token-8h99c]; ```; - [ ] https://github.com/hail-is/hail/issues/6548 batch takes 4 seconds to render the batch page with 20k jobs (the web browser displays it fine though), e.g. https://batch.hail.is/batches/278; - [ ] https://github.com/hail-is/hail/issues/6548 batch UI search is DOA with 20k jobs; - [ ] https://github.com/hail-is/hail/issues/6556 delete (and likely cancel) will timeout on large batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6547:28,Pipeline,Pipeline,28,https://hail.is,https://github.com/hail-is/hail/issues/6547,1,['Pipeline'],['Pipeline']
Deployability,This updates `hl.variant_qc` to compute the p-value from the one-sided HWE test for excess heterozygosity as the field `p_value_excess_het`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10993:5,update,updates,5,https://hail.is,https://github.com/hail-is/hail/pull/10993,1,['update'],['updates']
Deployability,"This was a huge pain. I think I got everything, but it would be great if you can double check. I added two new images since earlier in case you already ran the script. The things I omitted to fix didn't have Makefiles with build steps and are deployed infrequently.; - blog; - notebook/worker/Dockerfile; - notebook/images/Dockerfile; - docker/python-dill. Memory already has the jinja templating in the Makefile.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9660#issuecomment-719690473:243,deploy,deployed,243,https://hail.is,https://github.com/hail-is/hail/pull/9660#issuecomment-719690473,1,['deploy'],['deployed']
Deployability,"This was a mix of a couple of issues. #12021 should fix the infinite redirect loop caused by your accounts not being developer accounts. I've upgraded you but even so there's a chance of us accidentally picking up your old account's session instead of your new shiny broadinstitute accounts, which will again land you with a 401. If you log out and back in does it work now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11954#issuecomment-1180671583:142,upgrade,upgraded,142,https://hail.is,https://github.com/hail-is/hail/pull/11954#issuecomment-1180671583,1,['upgrade'],['upgraded']
Deployability,This was a problem with .crc files. The solution is to delete the .crc files. We need to update the renamesamples documentation explain this.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/347#issuecomment-214553591:89,update,update,89,https://hail.is,https://github.com/hail-is/hail/issues/347#issuecomment-214553591,1,['update'],['update']
Deployability,"This was fixed in #6478, which merged a week ago, but I guess the docs haven't been deployed since then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6567#issuecomment-509626765:84,deploy,deployed,84,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509626765,1,['deploy'],['deployed']
Deployability,"This was just addressed in #11828. We'll make a release asap, and please let us know if you encounter issues in the new version. Sorry for the disruption!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11827#issuecomment-1124030641:48,release,release,48,https://hail.is,https://github.com/hail-is/hail/issues/11827#issuecomment-1124030641,1,['release'],['release']
Deployability,"This was resolved by @chrisvittal in https://github.com/hail-is/hail/pull/13385 and released in 0.2.121. Use; ```python3; combiner = hl.vds.new_combiner(..., call_fields=YOUR_CUSTOM_CALL_FIELDS_ARRAY); combiner.run(); ```; The combiner also, by default, recognizes `LPGT`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13382#issuecomment-1738205715:84,release,released,84,https://hail.is,https://github.com/hail-is/hail/issues/13382#issuecomment-1738205715,1,['release'],['released']
Deployability,"This was the patch I had to apply to fix the 500s that we had yesterday in PR pages. The URI to URL rewrite enforced GCP which caused the 500. I removed the assertion but don't know yet the proper way to link to a **private** blob storage containers so I just return the URI. I looked at the [canonical URL for blob storage containers](https://docs.microsoft.com/en-us/rest/api/storageservices/naming-and-referencing-containers--blobs--and-metadata#resource-uri-syntax) but appears to only work for public containers. When I looked up how to get a URL for private containers, I get a lot of articles on SAS URLs that bake a token into the URL -- not what we want. We really just use this to link to the portal/console, but when I went to the portal the URL contains subscription-specific parameters that we don't have based on just the URI. This was the point where I figured our time was best spent elsewhere… Unless I'm missing something obvious, I can add this to the bottom of the TODO list. I also fixed the deploy_steps environment variable to match what was actually declared in the ci/deployment.yaml",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11121:13,patch,patch,13,https://hail.is,https://github.com/hail-is/hail/pull/11121,2,"['deploy', 'patch']","['deployment', 'patch']"
Deployability,This wasn't updated when I separated out building and pushing so the `build` target wasn't pushing the image. When I fixed that the cert generation failed because `memory` is no longer available. These changes got everything working again.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14064:12,update,updated,12,https://hail.is,https://github.com/hail-is/hail/pull/14064,1,['update'],['updated']
Deployability,This way if I change something in `batch/deployment.yaml` and then make deploy it only takes a couple seconds because no images need rebuilding and it just retemplates the yaml and applies it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13640:41,deploy,deployment,41,https://hail.is,https://github.com/hail-is/hail/pull/13640,2,['deploy'],"['deploy', 'deployment']"
Deployability,"This will allow you to go conda-free: https://github.com/hail-is/hail/pull/5655. In particular, I modified batch to be conda-free as part of this PR. I added the common python dependencies (async stuff, including mysql stuff) to the default python3 installation in the image. I think my PR will obviate this one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5616#issuecomment-475252257:249,install,installation,249,https://hail.is,https://github.com/hail-is/hail/pull/5616#issuecomment-475252257,1,['install'],['installation']
Deployability,"This will be fixed in the next release.; If you can't wait until then and you're comfortable patching this yourself, replace the contents of the affected file with the following:; ```yaml; dataproc:; init_notebook.py: gs://hail-common/hailctl/dataproc/0.2.129/init_notebook.py; vep-GRCh37.sh: gs://hail-common/hailctl/dataproc/0.2.129/vep-GRCh37.sh; vep-GRCh38.sh: gs://hail-common/hailctl/dataproc/0.2.129/vep-GRCh38.sh; wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl; pip_dependencies: aiodns==2.0.0|aiohttp==3.9.3|aiosignal==1.3.1|async-timeout==4.0.3|attrs==23.2.0|avro==1.11.3|azure-common==1.1.28|azure-core==1.30.1|azure-identity==1.15.0|azure-mgmt-core==1.4.0|azure-mgmt-storage==20.1.0|azure-storage-blob==12.19.0|bokeh==3.3.4|boto3==1.34.55|botocore==1.34.55|cachetools==5.3.3|certifi==2024.2.2|cffi==1.16.0|charset-normalizer==3.3.2|click==8.1.7|commonmark==0.9.1|contourpy==1.2.0|cryptography==42.0.5|decorator==4.4.2|deprecated==1.2.14|dill==0.3.8|frozenlist==1.4.1|google-auth==2.28.1|google-auth-oauthlib==0.8.0|humanize==1.1.0|idna==3.6|isodate==0.6.1|janus==1.0.0|jinja2==3.1.3|jmespath==1.0.1|jproperties==2.1.1|markupsafe==2.1.5|msal==1.27.0|msal-extensions==1.1.0|msrest==0.7.1|multidict==6.0.5|nest-asyncio==1.6.0|numpy==1.26.4|oauthlib==3.2.2|orjson==3.9.10|packaging==23.2|pandas==2.2.1|parsimonious==0.10.0|pillow==10.2.0|plotly==5.19.0|portalocker==2.8.2|py4j==0.10.9.5|pyasn1==0.5.1|pyasn1-modules==0.3.0|pycares==4.4.0|pycparser==2.21|pygments==2.17.2|pyjwt[crypto]==2.8.0|python-dateutil==2.9.0.post0|python-json-logger==2.0.7|pytz==2024.1|pyyaml==6.0.1|regex==2023.12.25|requests==2.31.0|requests-oauthlib==1.3.1|rich==12.6.0|rsa==4.9|s3transfer==0.10.0|scipy==1.11.4|six==1.16.0|sortedcontainers==2.4.0|tabulate==0.9.0|tenacity==8.2.3|tornado==6.4|typer==0.9.0|typing-extensions==4.10.0|tzdata==2024.1|urllib3==1.26.18|uvloop==0.19.0;sys_platform!=""win32""|wrapt==1.16.0|xyzservices==2023.10.1|yarl==1.9.4|; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14452#issuecomment-2045876651:31,release,release,31,https://hail.is,https://github.com/hail-is/hail/issues/14452#issuecomment-2045876651,2,"['patch', 'release']","['patching', 'release']"
Deployability,This will hopefully fix deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4756:24,deploy,deploy,24,https://hail.is,https://github.com/hail-is/hail/pull/4756,1,['deploy'],['deploy']
Deployability,"This will hopefully make it easier to develop QoB. Now, running `make -C hail install-for-qob NAMESPACE=default` will push a jar and configure hailctl to use that jar. So `make -C hail install-for-qob NAMESPACE=default && ipython` will drop you into an ipython session pointed at production where any hail queries reflects your local code. `make -C hail pytest-qob NAMESPACE=default PYTEST_ARGS='-k test_foo'` runs `test_foo` with any of your current changes. Things to note:; - Variables like `QUERY_STORAGE_URI` are lazy so this should hopefully not break any current usages of the file for non-developers; - You are at no risk of overwriting a release jar unless you specify `UPLOAD_RELEASE_JAR=true` in the make invocation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12342:78,install,install-for-qob,78,https://hail.is,https://github.com/hail-is/hail/pull/12342,3,"['install', 'release']","['install-for-qob', 'release']"
Deployability,"This will prevent optimization around the filter intervals. Given the prevalence of filter intervals in the biggest, baddest pipelines, this is a concern.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5138#issuecomment-454386209:125,pipeline,pipelines,125,https://hail.is,https://github.com/hail-is/hail/pull/5138#issuecomment-454386209,1,['pipeline'],['pipelines']
Deployability,This will probably behave better with this: https://github.com/hail-is/hail/pull/7636. The four was roughly chosen to match the k8s maximum pool size so there is space for test deployments. One problem we're seeing now is preemptible workloads get scheduled on non-preemptible nodes meaning there isn't space for non-preemptible test workloads.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7728#issuecomment-565619961:177,deploy,deployments,177,https://hail.is,https://github.com/hail-is/hail/pull/7728#issuecomment-565619961,1,['deploy'],['deployments']
Deployability,"This works better with my environment and it means that, e.g., Brandon, only needs Java and python set up correctly to run: `git clone ... && cd hail/hail && make pytest`. No environment variables, no packages to install.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6561:213,install,install,213,https://hail.is,https://github.com/hail-is/hail/pull/6561,1,['install'],['install']
Deployability,"Though I may instead modify the test first, will update here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3785#issuecomment-399807879:49,update,update,49,https://hail.is,https://github.com/hail-is/hail/issues/3785#issuecomment-399807879,1,['update'],['update']
Deployability,"Thought a bit more about our conversation. I think it would be nice, in general, to not make the local user manually create the root path secret, and it also seems better to ensure they have the necessary gcloud permissions. What do you think about. ```sh; /pipeline-secrets/pipeline-test-0-1--hail-is.key:; kubectl get secret pipeline-test-0-1--hail-is-service-account-key -o json | jq -r '.[""data""][""pipeline-test-0-1--hail-is.key""]' > $@; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5762#issuecomment-479586435:258,pipeline,pipeline-secrets,258,https://hail.is,https://github.com/hail-is/hail/pull/5762#issuecomment-479586435,4,['pipeline'],"['pipeline-secrets', 'pipeline-test-']"
Deployability,"Tim, I left the integration tests for now. I propose if you want them out, that we leave 2 cases for each of the type-combinations, so that we can inductively prove that our code can infer the correct unified type across nested IR (without the Ref shortcut)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6990#issuecomment-530868509:16,integrat,integration,16,https://hail.is,https://github.com/hail-is/hail/pull/6990#issuecomment-530868509,1,['integrat'],['integration']
Deployability,"Tim, when I run the following command I get the exception below. ~/hail/build/install/hail/bin/hail import -i ~/t2d/GoT2D.first10k.vcf filtervariants --keep -c ""true"" count. I get the following exception. ""[-80"" is not in the original vcf, so Cotton thinks it may be because htsjdk parses the info, then you converts them back to Strings, and then reparses them, so you might have trouble eating your own output. I'll share the vcf with you. I have no trouble filtering based on sample and interval lists, or doing qc or linreg. ```; Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.NumberFormatException: For input string: ""[-80""; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); at java.lang.Integer.parseInt(Integer.java:580); at java.lang.Integer.parseInt(Integer.java:615); at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272); at scala.collection.immutable.StringOps.toInt(StringOps.scala:30); at org.broadinstitute.hail.methods.AnnotationValueString$$anonfun$toArrayInt$extension$1.apply(Filter.scala:18); at org.broadinstitute.hail.methods.AnnotationValueString$$anonfun$toArrayInt$extension$1.apply(Filter.scala:18); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); at org.broadinstitute.hail.methods.AnnotationValueString$.toArrayInt$extension(Filter.scala:18); at __wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c.__wrapper$1$c1aef7d48806473ea6c7fc7ceb61989c$$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/120:78,install,install,78,https://hail.is,https://github.com/hail-is/hail/issues/120,1,['install'],['install']
Deployability,"Timing on profile.vds (1KG, 2535 samples, 25956 variants) using three covariates (isFemale, PC1, PC2) with variant QC and export is 60-63s for wald and lrt, 10-12s for score, on one core locally (with ~2s for read). Wald example:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; variantqc \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t wald \; printschema \; exportvariants -c 'variant = v, beta = va.logreg.wald.beta, se = va.logreg.wald.se, zstat = va.logreg.wald.zstat, pval = va.logreg.wald.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded, nNotCalled = va.qc.nNotCalled, nHomRef = va.qc.nHomRef, nHet = va.qc.nHet, nHomVar = va.qc.nHomVar' -o ~/data/profileHail/profile.sex.pc1.pc2.wald.tsv; ```. ```; hail: info: timing:; read: 1.922s; variantqc: 34.869ms; annotatesamples table: 317.153ms; annotatesamples fam: 60.988ms; logreg: 783.904ms; printschema: 2.034ms; exportvariants: 1m2.3s; ```. By comparison, EPACTS on 1 core of interactive node took:. ```; b.wald; real 10m45.718s; user 6m39.888s. b.lrt; real 11m59.180s; user 5m23.047s. b.score; real 5m0.636s; user 0m28.382s. b.firth; real 25m17.602s; user 19m27.214s. q.lm; real 5m17.675s; user 2m18.712s; ```. More timing info and convergence analysis to come.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/585#issuecomment-239628493:249,install,install,249,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239628493,1,['install'],['install']
Deployability,Timing on sampleqc:. Command: `./build/install/hail/bin/hail read -i profile225.vds sampleqc -o sampleqc.tsv`. cs_fastsqc:; read: 1.410s; sampleqc: 1m49.8s. master:; read: 1.449s; sampleqc: 2m48.9s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/93#issuecomment-163000784:39,install,install,39,https://hail.is,https://github.com/hail-is/hail/pull/93#issuecomment-163000784,1,['install'],['install']
Deployability,Timing:. Command: `time ~/hail/build/install/hail/bin/hail read -i ~/profile225.vds variantqc -o ~/variantqc.tsv`. With HWE:. timing: ; read: 1.449s; variantqc: 2m15.3s. real 2m19.420s; user 2m54.741s; sys 0m5.010s. Without HWE:. timing: ; read: 1.775s; variantqc: 2m11.8s. real 2m16.133s; user 2m51.038s; sys 0m5.092s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/73#issuecomment-156199705:37,install,install,37,https://hail.is,https://github.com/hail-is/hail/pull/73#issuecomment-156199705,1,['install'],['install']
Deployability,"To get this to work, I had to run the following command:; ```; hailctl --install-completion zsh; ```. And then add this line to the end of the ~/.zshrc file: `autoload -Uz compinit && compinit`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13224:73,install,install-completion,73,https://hail.is,https://github.com/hail-is/hail/pull/13224,1,['install'],['install-completion']
Deployability,"To make future modifications to the `attempt_resources_after_insert` trigger easier to follow, I have this PR which reverts the change to ignore any updates for batch format_version < 3. Stacked on #11990",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11995:149,update,updates,149,https://hail.is,https://github.com/hail-is/hail/pull/11995,1,['update'],['updates']
Deployability,To make uber jar: `mvn assembly:single`. `compile` automatically runs `make` for the NativeLib stuff.; `clean` automatically runs `make clean` for NativeLib; Not sure if I needed to incorporate `nativeLibTest` or `nativeLibPrebuilt`. Added two test configurations. One is for all tests and the other is for the set of tests with HAIL_ENABLE_CPP_CODEGEN=1. I double checked the Python tests pass with the uber jar. The test output doesn't have the nice formatting that we have in Gradle. Would be some work with listeners and reporters to do that: http://maven.apache.org/surefire/maven-surefire-plugin/examples/testng.html#. There also isn't the `check` input and some other bells and whistles we have in Gradle. I had to add `com.google.inject:guice` to get rid of some compile warnings with the test-jar. Let me know if there's other things to add/enable or if this is good enough for ci2. We should probably add some CI tests for this in a makefile somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5906:249,configurat,configurations,249,https://hail.is,https://github.com/hail-is/hail/pull/5906,1,['configurat'],['configurations']
Deployability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. Hail commit version: 5852a71e9e23. Error received:; ```; amazon-ebs: cd build/deploy; python3 setup.py -q sdist bdist_wheel; amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| xargs python3 -m pip install -U; ==> amazon-ebs: ERROR: Invalid requirement: '#'; ==> amazon-ebs: make: *** [install-on-cluster] Error 123; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10352:371,deploy,deploy,371,https://hail.is,https://github.com/hail-is/hail/issues/10352,3,"['deploy', 'install']","['deploy', 'install', 'install-on-cluster']"
Deployability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. Hello. The error has occurred during bulid. I installed all the necessary libraries and matched the jdk version and Python version.; And I also installed gcc, blas, and lapack. The error is as follows. Exception in thread ""main"" java.io.IOException: Function not implemented; at sun.nio.ch.FileDispatcherImpl.lock0(Native Method); at sun.nio.ch.FileDispatcherImpl.lock(FileDispatcherImpl.java:90); at sun.nio.ch.FileChannelImpl.tryLock(FileChannelImpl.java:1114); at java.nio.channels.FileChannel.tryLock(FileChannel.java:1155); at org.gradle.wrapper.ExclusiveFileAccessManager.access(ExclusiveFileAccessManager.java:55); at org.gradle.wrapper.Install.createDist(Install.java:48); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:107); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); make: *** [Makefile:75: build/libs/hail-all-spark.jar] Error 1. An error occurs while compilation is in progress. There seems to be an error in the 'exec ""$JAVACMD"" ""$@"" section the gradlew file at the end. My server OS is centos7. Is there a solution?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9849:339,install,installed,339,https://hail.is,https://github.com/hail-is/hail/issues/9849,4,"['Install', 'install']","['Install', 'installed']"
Deployability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. I followed [these instructions](https://hail.is/docs/0.2/install/macosx.html) for installing Hail on a Mac. After installing Java and restarting, I created a new Conda environment and used `pip install hail` to install.; Running the import in `ipython` succeeds, but initialization (in the course of running the code in 'Your First Hail Query' [example](https://hail.is/docs/0.2/install/try.html) fails with the following traceback:. ```; Exception Traceback (most recent call last); <ipython-input-2-20bea8c9bdc1> in <module>; ----> 1 hl.init(). ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/decorator.py in fun(*args, **kw); 230 if not kwsyntax:; 231 args, kw = fix(args, kw, sig); --> 232 return caller(func, *(extras + args), **kw); 233 fun.__name__ = func.__name__; 234 fun.__doc__ = func.__doc__. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578; 579 return wrapper. ~/miniconda3/envs/hail-env/lib/python3.9/site-packages/hail/context.py in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations); 244 optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3); 245; --> 246 backend = SparkBackend(; 247 idempotent, sc, spark_conf, app_name, master, local, log,; 248 quiet, append, min_block_size, branching_factor, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10524:350,install,install,350,https://hail.is,https://github.com/hail-is/hail/issues/10524,6,['install'],"['install', 'installing']"
Deployability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. Where's ""hail-python.zip"" in Hail 0.2 (latest version). we used to be able to use ```aws s3 cp distributions/hail-python.zip s3://${RESOURCES_BUCKET}/artifacts/``` on aws to install Hail but this failed when we switched from 0.1 to 0.2. Has anyone encountered similar issues before?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10844:467,install,install,467,https://hail.is,https://github.com/hail-is/hail/issues/10844,1,['install'],['install']
Deployability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------. hail v 0.2. I can't build from sources. Error below. $ make install-on-cluster HAIL_COMPILE_NATIVES=1 SPARK_VERSION=2.3.2; make -C src/main/c prebuilt; make[1]: Entering directory `/share/apps/luffy/hail/hail/src/main/c'; make[1]: *** No rule to make target `lz4.h', needed by `build/Decoder.o'. Stop.; make[1]: Leaving directory `/share/apps/luffy/hail/hail/src/main/c'; make: *** [native-lib-prebuilt] Error 2. How can I fix this?. thank you",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7464:353,install,install-on-cluster,353,https://hail.is,https://github.com/hail-is/hail/issues/7464,1,['install'],['install-on-cluster']
Deployability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; First of all, thank you for making such a highly integrated tool. . I learned that this tool could be run in two modes, on Cloud and locally. Well, I happen to have an HPC server that I can work on, so I'd love to use the tool locally. However, many annotation tools require many annotation data that need to be prepared in advance, and no one has seen the exact format of them. Plus, the annotation data sometimes is stored on a google cloud bucket that is requester paid so I don't have a chance to take a peek at them. Therefore, even I try to fill my configuration file, the annotation data needed cannot be prepared unless I have a template of them. . Pls, consider adding a feature like, if we want to run an annotation job locally, let us download package containing all the necessary annotation data in there. So we can set up the configuration file on our own and run the job on a local HPC server. Much appreciated!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9059:342,integrat,integrated,342,https://hail.is,https://github.com/hail-is/hail/issues/9059,3,"['configurat', 'integrat']","['configuration', 'integrated']"
Deployability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; Hello, I am having problems to read vcf file using hail. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz'); py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply. : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1; Any help? Best, Zillur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6747:352,install,installed,352,https://hail.is,https://github.com/hail-is/hail/issues/6747,2,['install'],['installed']
Deployability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible.; version 0.2.46-6ef64c08b000. Script is trying to merge 22 vcf.gz into a matrix table.; ```python3; import hail as hl; import sys. hl.init(default_reference='GRCh38'); vcf=""/project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz""; mt=""/project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt""; print(""Converting vcf ""+vcf+"" to mt ""+ mt); hl.import_vcf(vcf,force_bgz=True).write(mt); ```; -----------------------------------------------------------------------------; ```; $ submit ./vcf2mt_all.py; Loading modules; /share/pkg.7/gcc/8.3.0/install/lib64:/share/pkg.7/gcc/8.3.0/install/lib:/share/pkg.7/python3/3.7.7/install/lib:/usr/hdp/2.6.5.0-292/hadoop/lib/native/; Export env vars; Submitting Spark job; 20/08/17 23:24:46 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to /restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/hail-20200817-2324-0.2.46-6ef64c08b000.log; Converting vcf /project/casa/bayestyper/vcf/adsp5k.cadre.chr*.norm.ann2.ruth.fix.vcf.gz to mt /project/casa/bayestyper/mt/adsp5k.cadre.bayestyper.autosome.mt; [Stage 1:==================================================>(30467 + 1) / 30468]2020-08-17 23:59:36 Hail: INFO: Coerced almost-sorted dataset; 2020-08-17 23:59:37 Hail: INFO: Coerced dataset with out-of-order partitions.; [Stage 2:================> (9622 + 90) / 30468]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/pipelines/bayestyper/merge/./vcf2mt_all.py"", line 10, in <module>; hl.import_vcf(vcf,force_bgz=True).write(mt); File ""<decor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:776,install,install,776,https://hail.is,https://github.com/hail-is/hail/issues/9293,3,['install'],['install']
Deployability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2. ### What you did:; ```python; from gnomad_hail import *. logging.basicConfig(format=""%(levelname)s (%(name)s %(lineno)s): %(message)s""); logger = logging.getLogger(""variant_histograms""); logger.setLevel(logging.INFO). def release_ht_path(data_type: str, nested = True, with_subsets = True, temp = False):; tag = 'nested_release' if nested else 'flat_release'; tag = tag + '.with_subsets' if with_subsets else tag + '.no_subsets'; tag = tag + '.temp' if temp else tag; return f'gs://gnomad/release/2.1/ht/gnomad.{data_type}.{tag}.ht'. def main(args):; hl.init(log='/variant_histograms.log'); data_type = 'genomes' if args.genomes else 'exomes'. metrics = ['FS', 'InbreedingCoeff', 'MQ', 'MQRankSum', 'QD', 'ReadPosRankSum', 'SOR', 'BaseQRankSum',; 'ClippingRankSum', 'DP', 'VQSLOD', 'rf_tp_probability', 'pab_max']. ht = hl.read_table(release_ht_path(data_type, nested=False)); # NOTE: histogram aggregations are done on the entire callset (not just PASS variants), on raw data. # Compute median and MAD on variant metrics; medmad_dict = {}; for metric in metrics:; medmad_dict[metric] = hl.struct(median=hl.median(hl.agg.collect(ht[metric])), mad=4*1.48268*hl.median(hl.abs(hl.agg.collect(ht[metric])-hl.median(hl.agg.collect(ht[metric]))))); medmad = ht.aggregate(hl.struct(**medmad_dict)); print(medmad); print(hl.eval_expr(hl.json(medmad))); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 0:==================================================>(9853 + 93) / 10000]#; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fbeaec3ca22, pid=6662, tid=0x00007fbe3dd81700; #; # JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4418:747,release,release,747,https://hail.is,https://github.com/hail-is/hail/issues/4418,1,['release'],['release']
Deployability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2/devel hash eb1e04205793. ### What you did:; I'm trying to take PC loadings generated using Hail in one sample and project another sample into that PC space to generate scores for them. I've been using @danking's tricks developed for my PRS pipeline to speed things along and am now trying to feed the relevant inputs into gnomad Hail's [pc_project()](https://github.com/macarthur-lab/gnomad_hail/blob/master/utils/generic.py#L164) function. I've written out the function in script format for debugging purposes. Full code is below. . ```import hail as hl; import pickle; import time. generate_pcloadings_table = True; pcloadings_table_location = 'gs://ukbb_prs/sibdiff/keytables/ukb-pca-locus-allele-keyed.kt'; generate_contig_row_dict = True; contig_row_dict_location = 'gs://ukbb_prs/sibdiff/keytables/contig_row_dict-UKB'; output_location = 'gs://ukbb_prs/sibdiff/UKB_sibloadings.txt'; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}; bgen_files = 'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}_v3.bgen'. # large block size because we read very little data (due to filtering & ignoring genotypes); hl.init(branching_factor=10, min_block_size=2000). ### set up the pcloadings table; if (generate_pcloadings_table):; pcloadings = hl.import_table('gs://phenotype_31063/ukb31063.gwas.pca_loadings.tsv.gz', impute=True); pcloadings = pcloadings.annotate(locus=hl.parse_locus(hl.str(pcloadings.chr) + "":"" + hl.str(pcloadings.pos)),; alleles=[pcloadings.ref,pcloadings.alt]).key_by('locus','alleles'). pcloadings.write(pcloadings_table_location, overwrite=True). pcloadings = hl.read_table(pcloadings_table_location). ### determine the file locations of the pca ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953:497,pipeline,pipeline,497,https://hail.is,https://github.com/hail-is/hail/issues/3953,1,['pipeline'],['pipeline']
Deployability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-63d60cc. ### What you did:. Tried to run ld_prune and pc_relate on 5000 WGS samples. ```; spark-submit --verbose --master yarn --deploy-mode client \; --num-executors 14 \; --executor-cores 6 \; --jars $JAR \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; --conf ""spark.driver.extraClassPath=$JAR"" \; --conf ""spark.executor.extraClassPath=$JAR"" \; --executor-memory 90G\; --driver-memory 80g\; --conf spark.yarn.executor.memoryOverhead=8000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120\; hc_prune.py; ```. Where hc_prune.py is:. ```; import matplotlib.pyplot as plt; import seaborn. import numpy as np; import pandas as pd; from collections import Counter; from math import log, isnan; from pprint import pprint; # hail; import hail as hl; import hail.expr.aggregators as agg; import hail.expr.functions. hl.init(default_reference='GRCh38'); print(""Read in PASS SNVs""); passed=hl.read_matrix_table('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass'); print(""Filtering Common Variants""); common=passed.filter_rows(passed.variant_qc.AF > 0.01).persist(); common.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass.common'); print(""Pruning LD Variants""); pruned =hl.ld_prune(common,30,r2=0.1, memory_per_core=2048); pruned.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pruned'); print(""Sample 20% of variants for running PC-Relate""); pruned_subsample = pruned.sample_rows(0.2).persist(); print(""Running PC_Relate""); rel = hl.pc_relate(pruned_subsample.GT, 0.01, k=10); rel_df = rel.to_pandas(); rel_df.describe(); pprint(rel_df); rel_df.to_csv('gcad_5k.snv.rel.csv'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Got a memor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3463:388,deploy,deploy-mode,388,https://hail.is,https://github.com/hail-is/hail/issues/3463,1,['deploy'],['deploy-mode']
Deployability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: master build . ### What you did:; Ran python script ; [root@]cat hail/Hail_Tutorial.py; **from hail import * ; hc = HailContext()** ; import numpy as np ; import pandas as pd ; import matplotlib.pyplot as plt ; import matplotlib.patches as mpatches ; from collections import Counter ; from math import log, isnan ; from pprint import pprint ; import seaborn ; vds = hc.read('/gpfs/hail/test_hail_qnsnodes_2500c_spark2_20170515.vds') ; vds.summarize().report() ; vds.query_variants('variants.take(5)'); vds.query_samples('samples.take(5)'); vds.sample_ids[:5]; vds.query_genotypes('gs.take(5)'); #%%sh; #head data/1kg_annotations.txt | column -t; table = hc.import_table('data/1kg_annotations.txt', impute=True)\; .key_by('Sample'); print(table.schema); pprint(table.schema); table.to_dataframe().show(10); vds = vds.annotate_samples_table(table, root='sa'); pprint(vds.sample_schema); pprint(table.query('SuperPopulation.counter()')); pprint(table.query('CaffeineConsumption.stats()')); table.count(); vds.num_samples; vds.query_samples('samples.map(s => sa.SuperPopulation).counter()'); table.count(); vds.num_samples. #common_vds = (vds; # .filter_variants_expr('va.qc.AF > 0.01'); # .ld_prune(memory_per_core=256, num_cores=4)); pca = vds.pca('sa.pca', k=5, eigenvalues='global.eigen'); pprint(pca.globals). #vds = vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); #print('After filter, %d/1000 samples remain.' % vds.num_samples). df = vds.samples_table().to_pandas(); df.head(). ### What went wrong (all error messages here, including the full java stack trace):. **File ""/hail/Hail_Tutorial.py"", line 2, in <module>; hc = HailContext() ; NameError: name 'HailContext' is not defined; + /software/spark/spark-2.2.0-bin-ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3102:482,patch,patches,482,https://hail.is,https://github.com/hail-is/hail/issues/3102,1,['patch'],['patches']
Deployability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; .2, Spark 2.2. ### What you did:; Getting an error about a missing header file when I try to run ./gradlew during Hail installation. ### What went wrong (all error messages here, including the full java stack trace):. In file included from Encoder.cpp:1:0:; ../resources/include/hail/Encoder.h:3:17: fatal error: lz4.h: No such file or directory; #include ""lz4.h""; ^; compilation terminated.; make: *** [build/Encoder.o] Error 1. FAILURE: Build failed with an exception.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4651:373,install,installation,373,https://hail.is,https://github.com/hail-is/hail/issues/4651,1,['install'],['installation']
Deployability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. Running on Ubuntu 18.04. I had installed openjdk-11-jre-headless instead of openjdk-8-jre-headless. ### Hail version:; 0.2 ; ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):; 2018-12-04 22:13:57 root: ERROR: IllegalArgumentException: null; From java.lang.IllegalArgumentException: null; at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source); at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443); at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426); at org.apache.xbean.asm5.ClassReader.a(Unknown Source); at org.apache.xbean.asm5.ClassReader.b(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.xbean.asm5.ClassReader.accept(Unknown Source); at org.apache.spark.util.Closu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4896:266,install,installed,266,https://hail.is,https://github.com/hail-is/hail/issues/4896,1,['install'],['installed']
Deployability,"To reproduce:. ```; >>> import hail as hl; >>> hl._set_flags(cpp='true'); >>> mt = hl.read_table('gs://gnomad-public/release/2.1/ht/exomes/gnomad.exomes.r2.1.sites.ht'); >>> mt._force_count(); ```. gets:. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x0000000113aae924, pid=29051, tid=0x0000000000004003; #; ```. This is on OSX. Smaller examples work fine with C++ on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4816:117,release,release,117,https://hail.is,https://github.com/hail-is/hail/issues/4816,1,['release'],['release']
Deployability,"To that end, build llvm in CI using sccache. To update the upstream; branch/tag/commit used, put the git ref as the sole contents of; `query/.llvm.ref`. This will cause CI to build that reference, saving on; rebuild time through the use of sccache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12547:48,update,update,48,https://hail.is,https://github.com/hail-is/hail/pull/12547,1,['update'],['update']
Deployability,Tornado 6.0 was released 5 hours ago. https://pypi.org/project/tornado/#history,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5505#issuecomment-468797074:16,release,released,16,https://hail.is,https://github.com/hail-is/hail/issues/5505#issuecomment-468797074,1,['release'],['released']
Deployability,"Total minutes for test_hail_python went up to 162 compared to last deploy at 152. Total minutes for local was 294 compared to last deploy at 312. No change in service backend runtime (I think service backend is bottlenecked on test parallelism in default and non-preemptible cores in PR, so maybe a less interesting number). Maybe the doubling of the block size is having a negative effect? Anyway, let's benchmark on something more realistic than the Hail tests and assess.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12981#issuecomment-1536802594:67,deploy,deploy,67,https://hail.is,https://github.com/hail-is/hail/pull/12981#issuecomment-1536802594,2,['deploy'],['deploy']
Deployability,"True. I thought at least for the copy-paste tokens that this would be intentional. Looks like you can get access tokens from GCP that last up to 12 hours, but that could be insufficient for large workloads. If we need something arbitrarily long-lived, our current implementation might be our best bet short of some better integration with OIDC.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13934#issuecomment-1785516525:322,integrat,integration,322,https://hail.is,https://github.com/hail-is/hail/pull/13934#issuecomment-1785516525,1,['integrat'],['integration']
Deployability,"True`.; - `raise_for_status`: this parameter now defaults to `True` and includes the; response body text in the error message. Both; Both parameters may be overridden on a per-request basis. - `httpx.ResponseManager` and `httpx.ClientSession` work together to enable; `retry_transient` and `raise_for_status`. Aiohttp has this unusual structure where; all the request methods are synchronous but they return an object that is both; awaitable and an async context manager. I mirror their structure exactly. The; `httpx.ResponseManager` is both awaitable and an async context manager. Its; `response_coroutine` field is a coroutine that includes the retry and; raise-for-status logic. - The `HailResolver` overrides domain name resolution to first consult the Hail; `address` service. `address` is effectively a domain name server. It watches; kubernetes services and publishes the pod IPs. It supports two name styles:; `service` and `service.namespace`. The former uses the deploy config to; determine in which namespace to find the given service. Currently, the; client-side library only looks up IPs for `shuffler` and `address`. - `BlockingClientSession` and `BlockingContextManager` wrap the; aforementioned `httpx` classes. `BlockingClientResponse` wraps an; `aiohttp.ClientResponse`. ---. Examples of correct usage:. A blocking HTTPS request:. ```python3; with httpx.blocking_client_session() as session:; with session.post(url, json=config, headers=headers) as resp:; assert resp.status == 200; print(resp.text()); ```. An asynchronous HTTPS request to auth:; ```python3; async with httpx.client_session() as session:; async with session.get(; deploy_config.url('auth', '/api/v1alpha/userinfo'),; headers=headers) as resp:; assert resp.status == 200; print(await resp.json()); ```. A blocking HTTPS session with a large default timeout:. ```python3; httpx.blocking_client_session(; headers=service_auth_headers(deploy_config, 'query'),; timeout=aiohttp.ClientTimeout(total=600)); ```. cc: @cat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9554:2393,deploy,deploy,2393,https://hail.is,https://github.com/hail-is/hail/pull/9554,1,['deploy'],['deploy']
Deployability,Trying to build on ubuntu ; Welcome to Ubuntu Xenial Xerus (development branch) (GNU/Linux 4.4.0-16-generic x86_64). This tree builds fine on mac using grade installDist. ; On ubuntu it gives the below failure message. We wondered if it might have something to do with case sensitivity issues in file/path naming (mac maintaining case but being case-insensitive). Let me know if you would like more info. ; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/275:158,install,installDist,158,https://hail.is,https://github.com/hail-is/hail/issues/275,2,"['configurat', 'install']","['configuration', 'installDist']"
Deployability,Trying to debug a CI issue and this PR's status has exceeded the number of CI updates due to it being rather old. I'll reopen once I've figured out CI's issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8725#issuecomment-706201983:78,update,updates,78,https://hail.is,https://github.com/hail-is/hail/pull/8725#issuecomment-706201983,2,['update'],['updates']
Deployability,Trying to do dev install deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6284#issuecomment-499938113:17,install,install,17,https://hail.is,https://github.com/hail-is/hail/pull/6284#issuecomment-499938113,2,"['deploy', 'install']","['deploy', 'install']"
Deployability,"Trying to make it more ergonomic to simply do `python3 -m pytest batch/test/test_batch.py::test_job` (now works without any extra environment variables or configuration). This involved the following changes:; - Deleted of some env vars that are no longer used / can be easily consolidated into existing ones; - Gave defaults to those testing env variables for which there are reasonable defaults. E.g. `DOCKER_ROOT_IMAGE` and `HAIL_GENETICS_HAIL_IMAGE`.; - Pushed other environment variables for which there are not reasonable defaults into the tests that need them. If you run a test that requires `HAIL_CLOUD`, you'll still get an error that that env variable is unset and you should set it. But, if you just want to run a single test that doesn't need `HAIL_CLOUD` it won't get in the way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12862:155,configurat,configuration,155,https://hail.is,https://github.com/hail-is/hail/pull/12862,1,['configurat'],['configuration']
Deployability,"Turns out pandas dataframes have an `.attrs` field to store ""globals"". So now I group by discrete values and collect them as globals, and I get one pandas dataframe per group of discrete values. This makes things a bit less complicated I think. . As part of this change, I now create scales based on looking at all the collected data, not just one particular geom's data. This is more correct for things like continuous color, where if I have one geom that has points in color range 0 - .5, and another geom that has points in color range .5-1, there should be a continuous color gradient between 0 and 1, not two separate ones from 0 - .5 and .5 - 1. . This will also allow me to support things like `scale_color_hue`, which is the default R way of picking discrete colors by choosing evenly spaced points on a color wheel. I can't pick evenly spaced points if I don't know how many discrete values of `color` I have across all geoms. Summary of actual code changes:. 1. Don't flatten out grouped data. Stats use `group_by` to group by discrete aesthetics, but prior to this PR they would just forget that grouping information and flatten out the data, only to redetermine the grouping information later. This prevents that entirely by storing the per group information in a dataframes `attrs`. ; 2. Update geoms to reflect they are getting data in this form. This simplifies away the need to have a specified `take_one` boolean for each aesthetic a geom supports that said whether it's a per data point or per group field.; 3. Replace `Scale.transform_local_data` with `Scale.create_local_transformer`. This takes in all the data and returns a lambda to map over the data. This is part of the ""scales looking at all collect data"" change mentioned above.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11380:409,continuous,continuous,409,https://hail.is,https://github.com/hail-is/hail/pull/11380,3,"['Update', 'continuous']","['Update', 'continuous']"
Deployability,Two big changes. Catch any errors and release the semaphore. Restart failed workers in the concurrent worker pool.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6804:38,release,release,38,https://hail.is,https://github.com/hail-is/hail/pull/6804,1,['release'],['release']
Deployability,"Two of your comments deal with my elimination of redundant sources of information, so I'll address both here. I'm basically thinking of us when we have to debug this system. It's confusing if there's two sources of truth or if we're manually calling `deploy.sh` to isolate issues with that from issues with gradle, I don't want to have two separate knobs to spin. If they accidentally get out of sync that's gonna be double confusing (imagine a PyPI version that disagrees with hail's internal version). How about we put these two versions into two single-line, plain text files and have _generated... load the files to initialize the variables?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512:251,deploy,deploy,251,https://hail.is,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512,1,['deploy'],['deploy']
Deployability,Two things:; - I broke `hailctl batch init` after my changes to auth in the case where you are starting from no config (`~/.config/hail` does not exist); - The service account needs delete permissions on the temp bucket to run `remove_tmpdir` jobs. Cutting a release here so we can use this in today's workshop.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13681:259,release,release,259,https://hail.is,https://github.com/hail-is/hail/pull/13681,1,['release'],['release']
Deployability,"UPDATE:; It's not shuffling region values, but we're still doing a full shuffle to reorder instead of doing a map-side combine. This is _very_ bad. Take the case that you're aggregating to a single key -- this will shuffle ALL THE DATA into a single partition.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3641#issuecomment-391440637:0,UPDATE,UPDATE,0,https://hail.is,https://github.com/hail-is/hail/issues/3641#issuecomment-391440637,1,['UPDATE'],['UPDATE']
Deployability,"US datasets are copied and ready for use. Europe is in progress. Once everything is transitioned, we need to merge https://github.com/hail-is/hail/pull/14286 and release. Then loudly inform everyone of the loss of these buckets. Then we delete them before March 1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13009#issuecomment-1939792040:162,release,release,162,https://hail.is,https://github.com/hail-is/hail/issues/13009#issuecomment-1939792040,1,['release'],['release']
Deployability,Unclear to me exactly why (perhaps a terraform update on my machine) but running terraform against azure was complaining that it didn't know the resource group that the terraform state storage account lives in. Unclear to me why this is necessary because the storage account is globally unique but it was easy enough to add and fixed my problem *shrug*. Let me know if you'd like me to look into this further.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13125:47,update,update,47,https://hail.is,https://github.com/hail-is/hail/pull/13125,1,['update'],['update']
Deployability,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8165:26,release,release,26,https://hail.is,https://github.com/hail-is/hail/issues/8165,2,"['release', 'upgrade']","['release', 'upgraded']"
Deployability,"Under ideal conditions, I observed this code complete 12 jobs per second in a 1000 job; batch. However, it's usually closer to 6 jobs per second. There's still a deadlock in; `mark_job_complete` which hamstrings performance. I haven't thought carefully about why my metric; (jobs per second) is much lower than our usual metric (MJC per second). I will eventually squash; the `mark_job_complete` deadlock, but I haven't the time currently. The two main issues were the `batches` field `cancelled` and the `instances` field; `free_cores_mcpu`. We frequently locked the entire batch row just to prevent a batch from being; cancelled while other work was done. This effectively serialized all operation on jobs in the same; batch. We had the dual issue with free_cores_mcpu: we lock the entire instance record everywhere to; prevent the instance changing state while we update job information. This serialized updates to; free_cores_mcpu which serialized job operation for any job on the same instance. We also had some unnecessarily lock-heavy code inside `add_attempt`. This block of code is trying to; insert a new attempt record and update the associated instance's free cores. I also fixed some out of date values and bad syntax in `estimated-current.sql`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10985:867,update,update,867,https://hail.is,https://github.com/hail-is/hail/pull/10985,3,['update'],"['update', 'updates']"
Deployability,"Unfortunately, I can't dev deploy the rest of the resources branch until this is merged. I need these functions in the services-base image as that's what CreateDatabase uses to run the migration scripts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8788:27,deploy,deploy,27,https://hail.is,https://github.com/hail-is/hail/pull/8788,1,['deploy'],['deploy']
Deployability,Update 'users' page of scorecard,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4496:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/4496,1,['Update'],['Update']
Deployability,"Update AN, AC, AF when exporting vcf after filtering.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/159:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/issues/159,1,['Update'],['Update']
Deployability,Update AnnotateGlobalExpr.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1302:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1302,1,['Update'],['Update']
Deployability,Update AnnotationImpex.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1749:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1749,1,['Update'],['Update']
Deployability,"Update CHANGES for PR <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10504"">#10504</a></li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/8aacb338aa0e9e5a9823babbce90ce478eed1237""><code>8aacb33</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10504"">#10504</a> from AA-Turner/fix-kbd-findall</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/28f9ce78b0074c70a3b849c7f9eb7a4dabf6bece""><code>28f9ce7</code></a> Update CHANGES for PR <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10493"">#10493</a></li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/cb407c4024d7d63a832b87df62d1b73fb70162dc""><code>cb407c4</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10493"">#10493</a> from AA-Turner/more-css</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/d6d2a403450e022f5b91323a367fdc5539f7c525""><code>d6d2a40</code></a> Update CHANGES for PR <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10503"">#10503</a></li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/433c67effc12f23c5501f051d94296cbb1cd8282""><code>433c67e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10503"">#10503</a> from AA-Turner/gettext-catalogue-fix</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/a1ecf99b9fb75bfb74d891716958666d497bd153""><code>a1ecf99</code></a> Remove warning</li>; <li>Additional commits viewable in <a href=""https://github.com/sphinx-doc/sphinx/compare/v3.5.4...v5.0.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinx&package-manager=pip&previous-version=3.5.4&new-version=5.0.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11885:6194,Update,Update,6194,https://hail.is,https://github.com/hail-is/hail/pull/11885,1,['Update'],['Update']
Deployability,Update CI expected review count to 2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14659:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/14659,1,['Update'],['Update']
Deployability,Update Cassandra support to match Solr.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/925:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/925,1,['Update'],['Update']
Deployability,Update Context to catch throwables rather than exceptions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/955:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/955,1,['Update'],['Update']
Deployability,Update Epi25 from bioRxiv to AJHG,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6691:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/6691,1,['Update'],['Update']
Deployability,Update ErrorMessages.md,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1212:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1212,1,['Update'],['Update']
Deployability,Update FAQ ErrorMessages.md on changing SPARK_LOCAL_DIRS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1027:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1027,1,['Update'],['Update']
Deployability,Update GettingStarted.md,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1045:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1045,5,['Update'],['Update']
Deployability,Update Hail Docs types overview file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9097:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/9097,1,['Update'],['Update']
Deployability,Update Hail on AWS documentation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14738:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/14738,1,['Update'],['Update']
Deployability,Update HailExpressionLanguage.md,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/832:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/832,4,['Update'],['Update']
Deployability,Update HailExpressionLanguage.md on integer division,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1195:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1195,1,['Update'],['Update']
Deployability,Update Homebrew Java installation docs for OSX,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14729:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/14729,2,"['Update', 'install']","['Update', 'installation']"
Deployability,Update Join.scala warning,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1301:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1301,1,['Update'],['Update']
Deployability,Update LICENSE,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8932:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/8932,1,['Update'],['Update']
Deployability,Update Makefile,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6886:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/6886,1,['Update'],['Update']
Deployability,Update MatrixTable.show,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14250:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/14250,1,['Update'],['Update']
Deployability,Update RDD and types simultaneously in linreg.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2295:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/2295,1,['Update'],['Update']
Deployability,Update README.md,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/948:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/948,6,['Update'],['Update']
Deployability,Update README.md research links,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/971:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/971,1,['Update'],['Update']
Deployability,Update README.md with gnomAD link,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/993:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/993,1,['Update'],['Update']
Deployability,Update README.md with line break,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/968:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/968,1,['Update'],['Update']
Deployability,"Update README.md, lowercase gnomAD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/970:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/970,1,['Update'],['Update']
Deployability,"Update Release Scripts (<a href=""https://redirect.github.com/jupyter/jupyter_core/issues/396"">#396</a>)</li>; <li><a href=""https://github.com/jupyter/jupyter_core/commit/1c5fa3720d3d6da1b5188072c24bac095082903b""><code>1c5fa37</code></a> Enforce pytest 7 (<a href=""https://redirect.github.com/jupyter/jupyter_core/issues/393"">#393</a>)</li>; <li><a href=""https://github.com/jupyter/jupyter_core/commit/dfed905e5ce3550e1bdae60e9e9242f0d0d2faae""><code>dfed905</code></a> chore: update pre-commit hooks (<a href=""https://redirect.github.com/jupyter/jupyter_core/issues/392"">#392</a>)</li>; <li>See full diff in <a href=""https://github.com/jupyter/jupyter_core/compare/v5.7.1...v5.7.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jupyter-core&package-manager=pip&previous-version=5.7.1&new-version=5.7.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14484:4533,update,updates,4533,https://hail.is,https://github.com/hail-is/hail/pull/14484,1,['update'],['updates']
Deployability,Update SparkInfo.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1043:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1043,1,['Update'],['Update']
Deployability,Update Table method examples,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14242:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/14242,1,['Update'],['Update']
Deployability,Update Table.annotate docs to numpy style,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2669:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/2669,1,['Update'],['Update']
Deployability,Update Tutorial link to 1000 genomes sample data,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1334:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1334,1,['Update'],['Update']
Deployability,Update UK biobank line in readme,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4132:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/4132,1,['Update'],['Update']
Deployability,"Update aiohttp-session requirement from <2.8,>=2.7 to >=2.7,<2.12 in /hail/python",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11577:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/11577,1,['Update'],['Update']
Deployability,Update all make in makefiles to use $(MAKE),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9525:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/9525,1,['Update'],['Update']
Deployability,Update annotatevariants_bed.md,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1000:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1000,1,['Update'],['Update']
Deployability,Update astroid requirement from <2.5 to <2.11 in /hail/python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11463:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/11463,1,['Update'],['Update']
Deployability,Update astroid requirement from <2.5 to <2.11 in /hail/python/dev,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11469:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/11469,1,['Update'],['Update']
Deployability,"Update avro requirement from <1.11,>=1.10 to >=1.10,<1.12 in /hail/python",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11475:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/11475,1,['Update'],['Update']
Deployability,Update batch README.md,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5022:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/5022,1,['Update'],['Update']
Deployability,"Update bokeh requirement from <2.0,>1.3 to >1.3,<3.0 in /hail/python",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11540:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/11540,1,['Update'],['Update']
Deployability,"Update bokeh requirement from <2.0,>1.3 to >1.3,<4.0 in /hail/python/dev",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12454:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/12454,1,['Update'],['Update']
Deployability,Update breeze version of Spark 2.1.0,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1756:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1756,1,['Update'],['Update']
Deployability,Update build.gradle,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1480:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1480,2,['Update'],['Update']
Deployability,Update build.sbt dependencies,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8475:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/8475,1,['Update'],['Update']
Deployability,Update build.sbt for Spark 2.2.0,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3091:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/3091,1,['Update'],['Update']
Deployability,Update contributing guidelines,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9752:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/9752,1,['Update'],['Update']
Deployability,Update dataset.py,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1598:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1598,1,['Update'],['Update']
Deployability,Update decorator requirement from <5 to <6 in /hail/python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11490:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/11490,1,['Update'],['Update']
Deployability,Update defaults of import_vcfs to match import_vcf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5276:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/5276,1,['Update'],['Update']
Deployability,Update discuss posts from command line to Python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1384:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/issues/1384,1,['Update'],['Update']
Deployability,Update docker_root_image to Ubuntu 22.04,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13545:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/issues/13545,1,['Update'],['Update']
Deployability,Update docs and signatures for Concordance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3066:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/3066,1,['Update'],['Update']
Deployability,Update docs for building Hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7133:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/7133,1,['Update'],['Update']
Deployability,Update docs for import_vcf about filters and pass,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1671:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1671,1,['Update'],['Update']
Deployability,Update docs to remove 3.6 from supported versions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11219:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/11219,1,['Update'],['Update']
Deployability,Update docstring for VariantDataset.vep,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1938:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1938,1,['Update'],['Update']
Deployability,Update documentation for building Hail documentation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9458:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/9458,1,['Update'],['Update']
Deployability,Update example releaseJar to use Spark 2.4.0,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6920:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/6920,2,"['Update', 'release']","['Update', 'releaseJar']"
Deployability,Update gcsfs requirement from ==2021.* to ==2022.* in /hail/python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11575:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/11575,1,['Update'],['Update']
Deployability,Update general_advice.rst,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9085:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/9085,1,['Update'],['Update']
Deployability,Update getting started to recommend modifying ~/.profile,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2361:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/2361,2,['Update'],['Update']
Deployability,Update getting_started docs for running on a remote spark cluster in cluster mode,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4099:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/issues/4099,1,['Update'],['Update']
Deployability,Update getting_started.rst,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1234:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1234,4,['Update'],['Update']
Deployability,Update gitignore to include pycache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6187:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/6187,1,['Update'],['Update']
Deployability,Update google-cloud-storage requirement from ==1.25.* to ==2.1.* in /hail/python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11520:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/11520,1,['Update'],['Update']
Deployability,Update google-cloud-storage requirement from ==1.25.* to ==2.2.* in /hail/python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11578:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/11578,1,['Update'],['Update']
Deployability,Update grafana dashboard backup,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6636:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/6636,1,['Update'],['Update']
Deployability,Update help for isMissing and isDefined,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/374:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/issues/374,1,['Update'],['Update']
Deployability,Update hist expr docs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1500:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/1500,1,['Update'],['Update']
Deployability,Update htsjdk version in build.sbt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4870:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/4870,1,['Update'],['Update']
Deployability,Update ibd.md,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/894:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/894,1,['Update'],['Update']
