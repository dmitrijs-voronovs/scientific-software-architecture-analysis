quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Modifiability,"> I'm curious if there's a way to rewrite this to be more clear about the state machine?. Yes, this is quite confusing as written, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6581#issuecomment-509610153:34,rewrite,rewrite,34,https://hail.is,https://github.com/hail-is/hail/pull/6581#issuecomment-509610153,2,['rewrite'],['rewrite']
Modifiability,"> I'm happy to switch to debian:9.6 (that's the same as debian:stretch). But as far as I can tell, it has 1.10.3. Why not use the official docker image nginx:1.15.8? It does everything our custom Dockerfile does, pins the version, and removes 50% of the lines in our custom config. The only thing that I see it not doing that we may want is the jwt auth request module, but that isn't needed currently. https://github.com/nginxinc/docker-nginx/blob/baa050df601b5e798431a9db458e16f53b1031f6/mainline/stretch/Dockerfile",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5244#issuecomment-460363335:274,config,config,274,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460363335,1,['config'],['config']
Modifiability,"> I'm inclined to set `HAIL_QUERY_BACKEND=batch` by default, though I can see how this would also be useful to run a local-mode Spark-Hail. I'm happy to be overruled here but I like the ""just copy their config"" approach for configurations like these where both the local and batch backend could make sense, so the behavior is as consistent as is reasonable across environments. It will Just Work in the way you want if the user has the backend set to batch in their config ;)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12471#issuecomment-1323968044:203,config,config,203,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1323968044,3,['config'],"['config', 'configurations']"
Modifiability,"> I'm not sure this change is thorough enough. Is there a way for a bucket to get partially mounted but have config['mounted'] still be False?. I suppose this is exactly what happened when the user was able to create a duplicate mount. The unmount succeeded, but it did not rid the worker of the fuse process. Perhaps the more appropriate thing to do is instead of relying on the outcome of the unmount operation, we should check for the existence of the FUSE process",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12975#issuecomment-1533438375:109,config,config,109,https://hail.is,https://github.com/hail-is/hail/pull/12975#issuecomment-1533438375,1,['config'],['config']
Modifiability,"> If we ever ban old versions of Hail from the cluster, then we can also eliminate the log4j2 reconfiguration. New versions of Hail work fine without any runtime log configuration (thanks to QoBAppender). We might want to do this if we get rid of GSA keys. We can't have any more jars that presume the existence of some key file. It would also be a good time to fully delete the `memory` service, even though old jars should be able to tolerate that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1527963692:166,config,configuration,166,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1527963692,1,['config'],['configuration']
Modifiability,"> In the course of this work I also fixed a problem with the staged code generated by the copyFromType methods -- the addresses to copy from were never bound to variables, so in nested types, we ended up duplicating a lot of code (an array of Tuple10s of Tuple10s of Tuple10s would duplicate the top `loadElement` 1000x!). Could you show me where I forgot to bind address returns? This was an oversight; I understand the cost of not binding address-generating code (Need to call Code.store and load that, instead of re-running the generating function)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8099#issuecomment-586410150:161,variab,variables,161,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586410150,1,['variab'],['variables']
Modifiability,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:363,config,configuration,363,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285,1,['config'],['configuration']
Modifiability,> Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded?. This is an interesting idea. I think it would effectively require merging the database and the deploy steps. We can't re-deploy the existing config because that's the wrong (old) one.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7855#issuecomment-573739953:308,config,config,308,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573739953,1,['config'],['config']
Modifiability,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256:65,config,config,65,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256,2,['config'],['config']
Modifiability,"> Just want to verify, first cut for EType will be like the existing Type (with missingness), right? Then, Type, EType and PType will be free to evolve separately: we can remove missingness for Type, add non-encoded alternate representations to PTypes, and add alternate encodings (e.g. struct of arrays as arrays of structs) to ETypes. Yes, exactly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4734#issuecomment-437416979:145,evolve,evolve,145,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-437416979,1,['evolve'],['evolve']
Modifiability,"> Oh, I misunderstood, I thought you were suggesting changing our FROM to stretch/9.6.; > ; > I think that should be fine, but can we do it as a separate PR since it seems orthogonal to this change which we're trying to get in for the demo tomorrow? (And in general orthogonal changes should be separate PRs so discussion on one part doesn't hold up the other parts.). Yes, although the gzip settings issued in this pr will be different between the two version. 1.10.3 doesn't have gzip on by default. I understand the value of conservative updates before public demonstrations, so will do what you ask. Btw, the full config if relying on nginx:10.15.8 goes from:. ```; FROM debian:9.5. RUN apt-get update -y && \; apt-get install -y nginx && \; rm -rf /var/lib/apt/lists/*. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf. RUN ln -sf /dev/stdout /var/log/nginx/access.log; RUN ln -sf /dev/stderr /var/log/nginx/error.log. CMD [""nginx"", ""-g"", ""daemon off;""]; ```. to . ```; FROM nginx:1.15.8. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf; ```. kind of neat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467:618,config,config,618,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467,1,['config'],['config']
Modifiability,"> One option is to extend NormalizeNames to take a prefix. Yeah, or use generate uids instead of counting up from 0. I think ForwardLets should take a flag for this (or be parameterized with a name generator).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5710#issuecomment-483514236:19,extend,extend,19,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-483514236,2,"['extend', 'parameteriz']","['extend', 'parameterized']"
Modifiability,"> Sorry - one more thing I need help with. There's a cyclical import with the RouterFS in `variables.py`. Should I just pylint ignore it?. So it looks like that circular import is because of `config_variables`, but `config_variables` is only ever used in `hailctl` not elsewhere in `hailtop`:. ```; hailctl/config/cli.py; 10:from hailtop.config.variables import ConfigVariable, config_variables; 46: for var, var_info in config_variables().items():; 56: if parameter not in config_variables():; 62: config_variable_info = config_variables()[parameter]; 86: from hailtop.config import config_variables, get_user_config # pylint: disable=import-outside-toplevel; 99: config_items = {var.name: var_info.help_msg for var, var_info in config_variables().items()}. config/__init__.py; 4:from .variables import ConfigVariable, config_variables; 14: 'config_variables',. config/variables.py; 6:_config_variables = None; 30:def config_variables():; 34: global _config_variables; 36: if _config_variables is None:; 37: _config_variables = {; 112: return _config_variables; ```. Can you move `config_variables` into a file in `hailtop/hailctl/config` and keep the `ConfigVariable` enum in `hailtop/config/variables.py`? Then you can't have a circular reference because `hailtop` can't depend on `hailctl`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13224#issuecomment-1677917371:91,variab,variables,91,https://hail.is,https://github.com/hail-is/hail/pull/13224#issuecomment-1677917371,15,"['Config', 'config', 'variab']","['ConfigVariable', 'config', 'variables']"
Modifiability,> TMPDIR; > This variable shall represent a pathname of a directory made; > available for programs that need a place to create temporary; > files. http://pubs.opengroup.org/onlinepubs/9699919799/. Requested by the discuss user rca:. http://discuss.hail.is/t/hailcontext-tmp-dir/323,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2327:17,variab,variable,17,https://hail.is,https://github.com/hail-is/hail/pull/2327,1,['variab'],['variable']
Modifiability,"> Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first.; > ; > So if I understand correctly, for the different headers:; > ; > * X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I. Yep. In fact everything request to a Hail service (besides a lets-encrypt path) gets redirected to https. > Or am I misunderstanding how this works?. Nope, you have it correct. $http_x_forwarded_proto should never be absent, and would be fine to use instead of $updated_scheme (but I'd prefer one of those two, rather than https, because otherwise we're not relying on our upstream infrastructure). > * I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? . Yeah that's right. Host refers to the current server (or in the proxied case, what gateway set Host to). X-Forwarded-Host is set by gateway to be the $http_host at the time it proxies the request to router, which is going to be blog.hail.is. > I don't quite follow your comment about our use of `Host` being wrong, in this case; I _think_ I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enoug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569:555,config,config,555,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569,1,['config'],['config']
Modifiability,"> The approach in this PR doubles down on the functional Code[T] structure. I don't see anything in the design that prevents us from moving away from `Code[T]`, but it does have to support it for now. > I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. This is also the interface I would like to see for CodeBuilder. And you're right, making that change would allow methods taking a `CodeBuilder` to not need a line number argument. I agree that's better. I may have gotten a bit of tunnel vision in the middle of the giant mechanical refactoring :) I will make this change. > I think part of my concern is that I’m not entirely sold by the need to have a whole stack of IR printouts and associated line numbers — right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. I think most of this PR is necessary for debug information with the fully lowered IR line numbers. It doesn't do anything to propagate line numbers through IR lowerings, which is what would be needed to support line numbers at earlier compiler stages. > Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration. My main reason for that approach was to manage the number of changes required in this PR. We could follow up on this with making line number arguments explicit in manageable chunks. But personally this seems like the ideal use case for implicit arguments. And as `Code` goes away, the number of places with implicit line number arguments should go down significantly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975:598,refactor,refactoring,598,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975,1,['refactor'],['refactoring']
Modifiability,"> The changes around `toJSON` seem to be unrelated to the introduction of `VType`. Can you explain the motivation there?. This was added as a step towards a greater refactoring effort where I applied a number of changes to try and make various backend implementations look the same.; `Type`, `TableType` and `MatrixType` had a `toJSON` method, with the exception of `BlockMatrixType`. Since these are all virtual types, it seemed like a simple change to unify these methods.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14678#issuecomment-2341475173:165,refactor,refactoring,165,https://hail.is,https://github.com/hail-is/hail/pull/14678#issuecomment-2341475173,2,['refactor'],['refactoring']
Modifiability,"> The hailctl dataproc subcommand now has --beta, --configuration=, --dry-run, --project= and --zone=. These apply to all commands. There is a GcloudRunner object that takes these options, is set to the click context user obj field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with click, the subcommand options must go on the subcommand, so hailctl dataproc stop --dry-run is an error. Nice. It's great that these are handled at the `hailctl dataproc` level instead of having to remember to account for them in every `hailctl dataproc` subcommand. That's going to resolve a lot of inconsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:52,config,configuration,52,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393,2,['config'],['configuration']
Modifiability,> The new config addresses the requested changes on the config. I don't know what this means. You didn't answer the question.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-541104185:10,config,config,10,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541104185,2,['config'],['config']
Modifiability,"> This has significantly improved the simplicity of the parser, so much so that much of the logic therein could be simplified further, though I think that's beyond the scope of this change. Agreed. But as my follow up will be a complete rewrite of the parser, I definitely don't want to do more to simplify the current one. > I like the separation of type-checking and parsing, however I'd prefer in your implementations of `typecheck` that you assert one thing at a time. That way when things fail, it'll be clear which assertion was fired (ie if `(a && b && c)` fails, you don't know if it's `a` or `b` or `c`, whereas; > ; > ```scala; > assert(a); > assert(b); > assert(c); > ```; > ; > would give you that information. Good suggestion. I only moved assertions here, didn't add any new ones, but I don't mind splitting up some that I moved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13990#issuecomment-1809203400:237,rewrite,rewrite,237,https://hail.is,https://github.com/hail-is/hail/pull/13990#issuecomment-1809203400,2,['rewrite'],['rewrite']
Modifiability,"> This is a small refactor, right?. Yes, no problem",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7712#issuecomment-566671854:18,refactor,refactor,18,https://hail.is,https://github.com/hail-is/hail/pull/7712#issuecomment-566671854,1,['refactor'],['refactor']
Modifiability,"> This node can be used to group together multiple nodes with lowering implementations; > let us generate a TableValue that can go into the tail of relational functions that will take longer to lower. Sorry, I didn't sleep well last night and I must be slow today. I don't think I understand either of these. Can you give me examples?. In thinking about how this is intended to be used, I'm actually starting to formulate a different picture: what I think we want is. ```; case class DistributedArray(; contexts: IR, globals: IR, cname: String, gname: String, body: IR); extends TableIR; ```. where DistributedArray has the same signature as CollectDistributedArray, but is a TableIR instead of a (value)IR and should be able to be rendered as an RVD. In particular, this is something we can construct from a TableStage during the lowering process when we hit something that can't be lowered. In LowreTableIR we'd have:. ```; case TableToTableApply(child, f) =>; TableToTableApply(lower(child).toDistributedArray, f); ```. where lower(child) returns a TableStage.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8028#issuecomment-589006380:571,extend,extends,571,https://hail.is,https://github.com/hail-is/hail/pull/8028#issuecomment-589006380,1,['extend'],['extends']
Modifiability,"> To use protected var _pType2 instead I believe we need to have InferPType extend IR inside of IR.scala, e.g object InferPType extend IR, by requirement of sealed traits. You can use `protected[ir] var _pType2: PType`, which would be fine and let you keep the current structure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6594#issuecomment-515062679:76,extend,extend,76,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515062679,2,['extend'],['extend']
Modifiability,"> Two comments, and a meta-comment:; > ; > * I had looked over async http libraries and had preferred aiohttp over sanic because (1) ""aiohttp"" is a blessed aio library, (2) performance seemed comparable, (4) aiohttp seemed like a simpler solution which was attractive because the microservices are looking more and more like services and less like web servers (even more so moving all the rendering to the front end with the web app, the legacy version of scorecard using jinja is not the representative case). Did you look at aiohttp?; > * From the code:; > > Global variables that are modified ...; > ; > ; > I don't want to have to think about shared state and locking. I want a shared-nothing architecture in the microservices where the only globals are true constants and threads communication by sending immutable data through queues.; > * Finally, a meta-comment. I started reviewing this when it was just ujson, I did a bit of research about json packages to understand your choices and when I came back, the PR had expanded with all the async stuff. I would have approved the ujson stuff. The async stuff could have been a separate PR. Nobody wants to review a moving target, so the scope of a change should be roughly frozen when you assign a PR and additional changes should be minimized and restricted to that scope. You're welcome to have an open PR with no reviewer if you're still fleshing out the scope, of course. Thanks!. In response:. 1) aiohttp is an option, but appears to be generally considered slow on a per-response basis (published benchmarks, haven't had a chance to try it), even potentially slower than flask. It seems wrong to choose something slower if there are are reasonable alternatives.; 2) The globals were a feature of the initial implementation (the GitHub cache). It felt outside of the scope of my PR to change that to some queue solution. Meta comment. Ok. I didn't think it had been looked at, and expanded what it did pretty quickly, as I realized that ujso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051:568,variab,variables,568,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461191051,2,['variab'],['variables']
Modifiability,> What was the issue with the third variable?. Needed the full path: gcr.io/$(PROJECT)/$(1):$$(shell ...),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5829#issuecomment-481372325:36,variab,variable,36,https://hail.is,https://github.com/hail-is/hail/pull/5829#issuecomment-481372325,1,['variab'],['variable']
Modifiability,"> Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. We take special care to ensure our system is as efficient as possible when reading or writing to this native format. So, it's partly a sociological thing. On the practical end of things, Hail's native formats (for Tables and Matrix Tables) are a partitioned binary format. The partitioned part means Hail can use many cores in parallel to process and write the dataset. The binary part means that Hail need not use unnecessarily large (in terms of bytes) representations of values. These three things together make writing the native formats use less time, use less memory, and be more reliable. ---. > One thing I noticed is the mt_hwe_vals variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. Yes, after you aggregate you get back an MT with a different column key. ---. The `entries` method converts your matrix table from a compact and efficient matrix into a ""long"" and inefficient table. I generally recommend avoiding it if you can. However, if you only have a handful of ancestries, I wouldn't expect this to be *that* bad. You can just write the MT itself:. ```python3; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)); mt_hwe_vals = mt_hwe_vals.select_rows().select_cols() # drop irrelevant row and column fields; mt_hwe_vals.write(bucket + '/hwe.ht'); ```. ---. > I tried modifying the code to what is shown below but I'm still having the same issue. Just to be clear it's the exact same error ""Container exited with a non-zero exit code 137. ""? This makes me think we have an issue with `entries`, because, even though it's not great, it shouldn't be blowing RAM here. Can you share the log file from your previous or next attempt?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636:755,variab,variable,755,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636,2,['variab'],['variable']
Modifiability,"> Yeah, I imagine we do. `InferPType` isn't used right now, correct? I'll add the new case, just checking if there's a test that would have caught this. Yeah, I edited my comment. Totally not used right now, which is why you didn't notice. I think making an issue is also fine, like you said it would be an enhancement, not something necessary for what you wrote to work. Looks really cool btw.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7682#issuecomment-562812319:307,enhance,enhancement,307,https://hail.is,https://github.com/hail-is/hail/pull/7682#issuecomment-562812319,1,['enhance'],['enhancement']
Modifiability,"> Your tool should also examine the first word of the MAKEFLAGS variable and look for the character n. If this character is present then make was invoked with the ‘-n’ option and your tool should stop without performing any operations. Added. > Your tool should be sure to write back the tokens it read, even under error conditions. This includes not only errors in your tool but also outside influences such as interrupts (SIGINT), etc. You may want to install signal handlers to manage this write-back. I mean, I doubt anyone is sending signals other than SIGKILL to our build system, but I added some signal handlers that just `sys.exit(0)` which triggers the finally (I checked). > We also get a lot of ‘warning: jobserver unavailable: using -j1. Add +' to parent make rule.’warnings when runningmake jvm-test`. This is because our C++ backend uses make to drive compilation (wtf‽). I strip MAKEFLAGS before calling gradle now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104:64,variab,variable,64,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104,1,['variab'],['variable']
Modifiability,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170:268,config,config,268,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170,2,['config'],['config']
Modifiability,"> ah yes - the identity used to create the SAS token needs to have a control plane role on the Storage Account - Owner, Contributor, or (most specific) Storage Account Key Operator Service Role... Is that a manageable role to configure for testing or should I try to explore alternatives in the generation?. Thanks! This is totally fine, I'll just configure the SP that we use for the inter-cloud tests with the key operator role and re-run the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13140#issuecomment-1579202097:226,config,configure,226,https://hail.is,https://github.com/hail-is/hail/pull/13140#issuecomment-1579202097,2,['config'],['configure']
Modifiability,"> doSomething(r) ).catch( err => throw new Error(err) ); ```. This has one problem. Chaining promises leads to a potentially hard to follow chain of `.then` `.catch`. As in many other languages, the solution to ""transforming"" async call syntax to sync ones, is to color async functions with a ""async"" and ""await"" clauses. This can be used with any functions that return promises (but not those that just return a callback). Luckily again, JS libraries have been moving towards the Promise-land (sorry) for ~5 years, before Promises were in stdlib (bluebird). ```js; async function usePromise() {; const arg = someSyncOperation();; ; let result;; try {; result = await asyncPromise(arg);; } catch(e) {; // without wrapping catch, will just throw on reject(), unwinding the call stack; doSomethingWIthError(e) ; }; ; doStuffWithResult(result);; }; ```; ### React; What is a react component? A function that returns JSX. React components accept props (HTML attributes `<Component propName={propValue} />`); Stateless vs stateful components; ```jsx; # Stateful; class Stuff extends React.Component {; static getInitialProps() {; ; }. render() {; return <div>Hello World</div>; } ; }. # Stateless; # Note that arrow syntax has an implicit return if you don't create a function block, i. => { return <div>Hello World</div> } is valid too.; () => <div> Hello World </div> ; ```. Stateless ones are typically cheaper, but not necessarily:; * See: [PureComponent](https://reactjs.org/docs/react-api.html#reactpurecomponent); * See: [shouldComponentUpdate lifecycle method](https://reactjs.org/docs/react-component.html#shouldcomponentupdate). #### JSX differences from html; 1. `className` : ""class"" is a reserved word in JSX; used to specify the component class (every HTML element is modeled as an object). [This will go away in 2019, maybe](https://github.com/facebook/react/issues/13525); 2. There should alway be one and only one non-leaf node in the component tree. Leaf siblings are allowed; note that t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:6668,extend,extends,6668,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['extend'],['extends']
Modifiability,> elimination of default_namespace. This is also unfortunately tricky :/// as clients require `default_namespace` exist in container's deploy config,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14056#issuecomment-1879295919:142,config,config,142,https://hail.is,https://github.com/hail-is/hail/pull/14056#issuecomment-1879295919,1,['config'],['config']
Modifiability,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868:140,config,config,140,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868,2,['config'],['config']
Modifiability,"> nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):; > ; > > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode.; > ; > Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission.; > ; > See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957).; > ; > [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now. Thanks, yeah, I shared this with Cotton yesterday. We need to be careful seems to be the conclusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479640942:338,config,config,338,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640942,1,['config'],['config']
Modifiability,"> nginx should need the services (i.e. domain names) to exist, not the deployments. Ah, that makes sense. I suppose that's why the service definition of `router` used to exist along with gateway's k8s config.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10736#issuecomment-891312778:201,config,config,201,https://hail.is,https://github.com/hail-is/hail/pull/10736#issuecomment-891312778,1,['config'],['config']
Modifiability,"> otherwise we need to wait until EmitStream1 is ripped out. That will be very soon. > I will propose some code changes to deal with this reuse issue. In general, I want the picture that Code[_] cannot be placed in multiple locations. Absolutely. I have some WIP from before I shifted focus to streams that makes a `Code[_]` throw an exception at the site where it is used a second time. But it's tangled up with some other changes I was experimenting with, mostly to enable using variables which could be either locals or fields, with the decision being made after the complete `Code[_]` is assembled. I'd be happy to revisit that when lowering is unblocked, or let you do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8207#issuecomment-593417586:481,variab,variables,481,https://hail.is,https://github.com/hail-is/hail/pull/8207#issuecomment-593417586,1,['variab'],['variables']
Modifiability,> pytest-instafail is a plugin for py.test that shows failures and errors instantly instead of waiting until the end of test session. https://github.com/pytest-dev/pytest-instafail/,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6854:24,plugin,plugin,24,https://hail.is,https://github.com/hail-is/hail/pull/6854,1,['plugin'],['plugin']
Modifiability,"> should be explicitly representable in the IR. How would that work with MakeArray?. > MakeStream is most naturally a push stream. I'm not sure I see that. If you don't duplicate the consumer, the push code is the same. The question is, do you use a variable + switch to track where you are in the MakeStream, or do you use the program counter via labels, where you get the latter from the former by code duplication.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8148#issuecomment-591932796:250,variab,variable,250,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-591932796,1,['variab'],['variable']
Modifiability,"> submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; > if any request fails, raise an exception, which is caught by outer submit, which retries a configurable number of times, logging a configurable number of errors. I haven't dug into the PR yet, but will just remark I'm going to argue pretty strenuously to maintain our current model here: infinitely retry transient errors with exponential backoff and no retry of non-transient errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-574377227:188,config,configurable,188,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574377227,2,['config'],['configurable']
Modifiability,"> the TableIR doesn't define partitionCounts. statically known partition counts are used to optimize `.count()` when we know the partition sizes. Here that doesn't apply, so I don't think you need to define that method (it inherits `def partitionCounts = None`). > perhaps ""LiftLiterals"" was changed to ""LiftNonCompilable"". Yes, it was. No need to write a rule for this. Separately, I think we should delete the checklist. It'll never be correct, since it's not checked against the codebase. To add an IR node, one needs to understand the compiler, and we can't adequately document that in a bullet list right now (over time, things should get simpler). . If a node is missing from somewhere it needs to appear, then tests should catch that case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6689#issuecomment-513935766:223,inherit,inherits,223,https://hail.is,https://github.com/hail-is/hail/pull/6689#issuecomment-513935766,2,['inherit'],['inherits']
Modifiability,"> was made a bit more descriptive (<code>pattern=N,...</code> instead of <code>moduleSpec</code>). (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106090"">kubernetes/kubernetes#106090</a>, <a href=""https://github.com/pohly""><code>@​pohly</code></a>) [SIG API Machinery, Architecture, CLI, Cluster Lifecycle, Instrumentation, Node and Scheduling]</li>; <li>Introduce <code>OS</code> field in the PodSpec (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104693"">kubernetes/kubernetes#104693</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>)</li>; <li>Introduce <code>v1beta3</code> API for scheduler. This version; <ul>; <li>; <p>increases the weight of user specifiable priorities.; The weights of following priority plugins are increased</p>; <ul>; <li><code>TaintTolerations</code> to 3 - as leveraging node tainting to group nodes in the cluster is becoming a widely-adopted practice</li>; <li><code>NodeAffinity</code> to 2</li>; <li><code>InterPodAffinity</code> to 2</li>; </ul>; </li>; <li>; <p>Won't have <code>HealthzBindAddress</code>, <code>MetricsBindAddress</code> fields (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104251"">kubernetes/kubernetes#104251</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>)</p>; </li>; </ul>; </li>; <li>Introduce v1beta2 for Priority and Fairness with no changes in API spec. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104399"">kubernetes/kubernetes#104399</a>, <a href=""https://github.com/tkashem""><code>@​tkashem</code></a>)</li>; <li>JSON log output is configurable and now supports writing info messages to stdout and error messages to stderr. Info messages can be buffered in memory. The default is to write both to stdout without buffering, as before. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104873"">kuber",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:6009,plugin,plugins,6009,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['plugin'],['plugins']
Modifiability,">)</li>; <li>Add support for specifying stable version with <code>--required-version</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2832"">#2832</a>).</li>; <li>Avoid crashing when the user has no homedir (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2814"">#2814</a>)</li>; <li>Avoid crashing when md5 is not available (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2905"">#2905</a>)</li>; <li>Fix handling of directory junctions on Windows (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2904"">#2904</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Update pylint config documentation (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2931"">#2931</a>)</li>; </ul>; <h3>Integrations</h3>; <ul>; <li>Move test to disable plugin in Vim/Neovim, which speeds up loading (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2896"">#2896</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>In verbose, mode, log when <em>Black</em> is using user-level config (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2861"">#2861</a>)</li>; </ul>; <h3>Packaging</h3>; <ul>; <li>Fix Black to work with Click 8.1.0 (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2966"">#2966</a>)</li>; <li>On Python 3.11 and newer, use the standard library's <code>tomllib</code> instead of <code>tomli</code>; (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2903"">#2903</a>)</li>; <li><code>black-primer</code>, the deprecated internal devtool, has been removed and copied to a; <a href=""https://github.com/cooperlees/black-primer"">separate repository</a> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2924"">#2924</a>)</li>; </ul>; <h3>Parser</h3>; <ul>; <li>Black can now parse starred expressions in the target of <code>for</code> and <code>async for</code>; statements, e.g <code>for item in *items_1, *items_2: pass</code> (<a href=""https://github-redir",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11696:2163,config,config,2163,https://hail.is,https://github.com/hail-is/hail/pull/11696,2,['config'],['config']
Modifiability,">; <blockquote>; <h2>0.13.1 / 2022-01-28</h2>; <p>[BUGFIX] Relax some type constraints that were too strict. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/754"">#754</a>, <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/755"">#755</a>, <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/756"">#756</a>, <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/758"">#758</a>; [BUGFIX] Explicitly export functions with <code>__all__</code>. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/757"">#757</a></p>; <h2>0.13.0 / 2022-01-25</h2>; <p>[CHANGE] Drop support for Python versions 2.7, 3.4, and 3.5. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/718"">#718</a>; [FEATURE] Support adding labels when using <code>.time()</code> <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/730"">#730</a>; [ENHANCEMENT] Begin to add type hints to functions. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/705"">#705</a>; [ENHANCEMENT] Improved go-to-declaration behavior for editors. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/747"">#747</a>; [BUGFIX] Remove trailing slashes from pushgateway URLS. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/722"">#722</a>; [BUGFIX] Catch non-integer bucket/count values. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/726"">#726</a></p>; <h2>0.12.0 / 2021-10-29</h2>; <p>[FEATURE] Exemplar support (excludes multiprocess) <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/669"">#669</a>; [ENHANCEMENT] Add support for Python 3.10 <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/706"">#706</a>; [ENHANCEMENT] Restricted Registry will handle metrics added after restricting <a ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11515:1265,ENHANCE,ENHANCEMENT,1265,https://hail.is,https://github.com/hail-is/hail/pull/11515,1,['ENHANCE'],['ENHANCEMENT']
Modifiability,">; <h2>Deprecated</h2>; <ul>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11412"">#11412</a>: Emit warnings on using a deprecated Python-specific index entry type; (namely, <code>module</code>, <code>keyword</code>, <code>operator</code>, <code>object</code>, <code>exception</code>,; <code>statement</code>, and <code>builtin</code>) in the :rst:dir:<code>index</code> directive, and; set the removal version to Sphinx 9. Patch by Adam Turner.</li>; </ul>; <h2>Features added</h2>; <ul>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11415"">#11415</a>: Add a checksum to JavaScript and CSS asset URIs included within; generated HTML, using the CRC32 algorithm.</li>; <li>:meth:<code>~sphinx.application.Sphinx.require_sphinx</code> now allows the version; requirement to be specified as <code>(major, minor)</code>.</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11011"">#11011</a>: Allow configuring a line-length limit for object signatures, via; :confval:<code>maximum_signature_line_length</code> and the domain-specific variants.; If the length of the signature (in characters) is greater than the configured; limit, each parameter in the signature will be split to its own logical line.; This behaviour may also be controlled by options on object description; directives, for example :rst:dir:<code>py:function:single-line-parameter-list</code>.; Patch by Thomas Louf, Adam Turner, and Jean-François B.</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/10983"">#10983</a>: Support for multiline copyright statements in the footer block.; Patch by Stefanie Molin</li>; <li><code>sphinx.util.display.status_iterator</code> now clears the current line; with ANSI control codes, rather than overprinting with space characters.</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11431"">#11431</a>: linkcheck: Treat SSL failures as broken links.; Patch by Bénédikt Tran</li>; <li><a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13295:2353,config,configuring,2353,https://hail.is,https://github.com/hail-is/hail/pull/13295,1,['config'],['configuring']
Modifiability,">; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/da6d0d034822f66966e4a84a3a1e2f37cc83e3b0""><code>da6d0d0</code></a> Remove unneeded &quot;update order&quot; consistency test</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/e85d8659733cb3e28d539a28db0fdd71672ab2e4""><code>e85d865</code></a> Simplify &quot;update order&quot; consistency test</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/7dc426c95a0c329d5514e6198d92080f1ffc1e5e""><code>7dc426c</code></a> Fix update() ordering to be more consistent with add() ordering (<a href=""https://github-redirect.dependabot.com/grantjenks/python-sortedcontainers/issues/159"">#159</a>)</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/13d30bc654eb9e6be092282ca502967fcb7f0113""><code>13d30bc</code></a> Bump version to 2.2.2</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/4997d0e849f2275d1931772a5432163ecc20e0b0""><code>4997d0e</code></a> Refactor small slice optimization in SortedList.<strong>getitem</strong></li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/6ee5d57fc8d691fbab4972b853a60348d0f922ef""><code>6ee5d57</code></a> improve SortedList.<strong>getitem</strong>() performance for small slices</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/ac80254fb6a08045ced7d9704412878ff8000fa7""><code>ac80254</code></a> suppress warning in test of deprecated function (<a href=""https://github-redirect.dependabot.com/grantjenks/python-sortedcontainers/issues/118"">#118</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/grantjenks/python-sortedcontainers/compare/v2.1.0...v2.4.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sortedcontainers&package-manager=pip&previous-version=2.1.0&new-version=2.4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11476:2996,Refactor,Refactor,2996,https://hail.is,https://github.com/hail-is/hail/pull/11476,1,['Refactor'],['Refactor']
Modifiability,"></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/684"">jpadilla/pyjwt#684</a></li>; <li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/686"">jpadilla/pyjwt#686</a></li>; <li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/689"">jpadilla/pyjwt#689</a></li>; <li>Remove upper bound on cryptography version by <a href=""https://github.com/riconnon""><code>@​riconnon</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/693"">jpadilla/pyjwt#693</a></li>; <li>Add support for Ed448/EdDSA. by <a href=""https://github.com/dajiaji""><code>@​dajiaji</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/675"">jpadilla/pyjwt#675</a></li>; <li>Chore: inline Variables that immediately Returned by <a href=""https://github.com/yezz123""><code>@​yezz123</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/690"">jpadilla/pyjwt#690</a></li>; <li>Use timezone package as Python 3.5+ is required by <a href=""https://github.com/kkirsche""><code>@​kkirsche</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/694"">jpadilla/pyjwt#694</a></li>; <li>Bump up version to v2.2.0 by <a href=""https://github.com/jpadilla""><code>@​jpadilla</code></a> in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/697"">jpadilla/pyjwt#697</a></li>; </ul>; <h2>New Contributors</h2>; <ul>; <li><a href=""https://github.com/TPXP""><code>@​TPXP</code></a> made their first contribution in <a href=""https://github-redirect.dependabot.com/jpadilla/pyjwt/pull/664"">jpadilla/pyjwt#664</a></li>; <li><a href=""https://github.com/Klavionik""><code>@​Klavionik</code></a> made their first contribution in <a href=""https",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11457:5606,Variab,Variables,5606,https://hail.is,https://github.com/hail-is/hail/pull/11457,1,['Variab'],['Variables']
Modifiability,"><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/adc88090f341d9872e9e9b4d22a94cdadf60b3bc""><code>adc8809</code></a> Build(deps): Bump typing-extensions in /dependencies/default (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-asyncio/issues/425"">#425</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/4abf9d1df228ed8b083721d7affa73e4a08d13c3""><code>4abf9d1</code></a> Build(deps): Bump zipp from 3.8.1 to 3.9.0 in /dependencies/default (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-asyncio/issues/424"">#424</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/eb487bcb076f44dedcdb33e74972bf06c37027ee""><code>eb487bc</code></a> Build(deps): Bump hypothesis in /dependencies/default (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-asyncio/issues/423"">#423</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/907c461f172e52159a595e2592176c7feac04a43""><code>907c461</code></a> Refactor pytest_pycollect_makeitems (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-asyncio/issues/421"">#421</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/d45ab217c80117854b510edc6c9fdd457b6b07fc""><code>d45ab21</code></a> feat: Add deprecation warning for pytest &lt; 7. (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-asyncio/issues/420"">#420</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/cab20f4d346e9e52e5ffc93854de3ec881e7d342""><code>cab20f4</code></a> Build(deps): Bump importlib-metadata in /dependencies/default (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-asyncio/issues/415"">#415</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/d9f77567189c96536b39b43520f4b40895b34fb9""><code>d9f7756</code></a> Build(deps): Bump hypothesis in /dependencies/default (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-asyncio/is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12390:5637,Refactor,Refactor,5637,https://hail.is,https://github.com/hail-is/hail/pull/12390,1,['Refactor'],['Refactor']
Modifiability,"><a href=""https://github.com/sphinx-doc/sphinx/commit/ed6970311349e54ceebe24ede255378fcd9d94e5""><code>ed69703</code></a> Update CHANGES for PR <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10535"">#10535</a></li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/377d8668b5c93cc224fec46f2f3c2920b25107ca""><code>377d866</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10535"">#10535</a> from AA-Turner/css-nav-contents</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/709602437df850d5538a4fe899a50625c01a0f80""><code>7096024</code></a> Update CHANGES for PR <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10539"">#10539</a></li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/d0452276689bfb5b97ca7a3469e1afb505895cdd""><code>d045227</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10539"">#10539</a> from AA-Turner/fix-inherited-attrs</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/29edce9243046962f5f024d510315133448dd3e1""><code>29edce9</code></a> test: Add testcase for autodoc_inherit_docstring and attributes (refs: <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10539"">#10539</a>)</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/3956cf2249d27ed63e8381c07dfde36f6c96f78f""><code>3956cf2</code></a> Fix documenting inherited attributes</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/27f05328d0369ad0db85c27935d52fdadf020f6b""><code>27f0532</code></a> Move <code>aside.topic</code> into the conditional blocks</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/5806f0af2788db40661d62e5e88c2c1560ae46b6""><code>5806f0a</code></a> Add <code>nav.contents</code> everywhere that <code>div.topic</code> is used</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/8da2efb1d71ab2d384ddc90cf4fdebe5d18e91cd""><code>8da2efb</c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11925:5097,inherit,inherited-attrs,5097,https://hail.is,https://github.com/hail-is/hail/pull/11925,1,['inherit'],['inherited-attrs']
Modifiability,"><code>10a4427</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1092"">#1092</a> from PyCQA/2_9_1</li>; <li><a href=""https://github.com/PyCQA/pycodestyle/commit/c33e852a5938b823b04dd981260bd1664c643385""><code>c33e852</code></a> Release 2.9.1</li>; <li><a href=""https://github.com/PyCQA/pycodestyle/commit/c97e4f86bd60e449a64be6c0de5b5ec5bb28b8e9""><code>c97e4f8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1091"">#1091</a> from asottile/E275-yield-expression</li>; <li><a href=""https://github.com/PyCQA/pycodestyle/commit/43c5afaeef44a01b512ade340030ff4d7b0ba78e""><code>43c5afa</code></a> allow parenthesized yield (generator-coroutines)</li>; <li><a href=""https://github.com/PyCQA/pycodestyle/commit/9e6e820b269cbe39da854ae2835bd797028d22db""><code>9e6e820</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1089"">#1089</a> from PyCQA/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/PyCQA/pycodestyle/commit/9851cb692a2f824495f6bbdded03059116bb46bb""><code>9851cb6</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/PyCQA/pycodestyle/commit/44b3d2895b39b1eff8cb5048ae3464a033b4ede8""><code>44b3d28</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1087"">#1087</a> from PyCQA/2_9_0</li>; <li><a href=""https://github.com/PyCQA/pycodestyle/commit/57e39fa4f66707c305ead679e62d7ce1b7af9362""><code>57e39fa</code></a> Release 2.9.0</li>; <li><a href=""https://github.com/PyCQA/pycodestyle/commit/ab806f3f9133ca24366b6254e499f0363f6bf5ec""><code>ab806f3</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1085"">#1085</a> from PyCQA/revert-1041</li>; <li><a href=""https://github.com/PyCQA/pycodestyle/commit/c14bd2aac8e370bc84048a97f17a1ed906523bf9""><code>c14bd2a</code></a> Revert &quot;Merg",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12476:2440,config,config,2440,https://hail.is,https://github.com/hail-is/hail/pull/12476,1,['config'],['config']
Modifiability,"><code>@​akayunov</code></a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jmoiron/humanize/commit/a1514eb521c2befe40274674d61aba4f0fbf6137""><code>a1514eb</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/239"">#239</a> from hugovk/rm-3.6</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/48506d434fd315a976bbdc058a791b80086f7e7e""><code>48506d4</code></a> pre-commit autoupdate</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/8f2c8551e5e20cc6cc3bcaa241fa2c1760d07926""><code>8f2c855</code></a> Remove unused import</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/04bf8872908178b3d7d9fb4b316da8ce72916209""><code>04bf887</code></a> Drop support for Python 3.6</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/0f2ff42cbe632c47ddb6ac255c61890ab8a46fd4""><code>0f2ff42</code></a> Use actions/setup-python's pip cache and update other CI config</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/464de5965692765d29d1c3cfde1f87c4ceece440""><code>464de59</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/253"">#253</a> from hugovk/rm-VERSION</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/66b8a6322fbda9bffb2882500c6a9b6c96271401""><code>66b8a63</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/250"">#250</a> from carterbox/no-overflow-naturaldelta</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/e89c8c8e325ccb2b3ee78ef507e9d6805c47a175""><code>e89c8c8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/241"">#241</a> from samueljsb/remove-deprecated-private-function-ali...</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/ffe4bcfaa6cfbd95ba47315f8f71a206485af6ae""><code>ffe4bc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11517:5367,config,config,5367,https://hail.is,https://github.com/hail-is/hail/pull/11517,2,['config'],['config']
Modifiability,">>> hc = hail.HailContext(); log4j:ERROR setFile(null,false) call failed.; java.io.FileNotFoundException: hail.log (Permission denied); 	at java.io.FileOutputStream.open0(Native Method); 	at java.io.FileOutputStream.open(FileOutputStream.java:270); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:213); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:133); 	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294); 	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165); 	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104); 	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842); 	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768); 	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648); 	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514); 	at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:440); 	at is.hail.HailContext$.configureLogging(HailContext.scala:132); 	at is.hail.HailContext$.apply(HailContext.scala:159); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198:2120,config,configureRootCategory,2120,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198,1,['config'],['configureRootCategory']
Modifiability,">Fix handling of standalone <code>match()</code> or <code>case()</code> when there is a trailing newline or a comment inside of the parentheses. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2760"">#2760</a>)</li>; <li><code>from __future__ import annotations</code> statement now implies Python 3.7+ (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2690"">#2690</a>)</li>; </ul>; <h3>Performance</h3>; <ul>; <li>Speed-up the new backtracking parser about 4X in general (enabled when <code>--target-version</code> is set to 3.10 and higher). (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2728"">#2728</a>)</li>; <li>Black is now compiled with mypyc for an overall 2x speed-up. 64-bit Windows, MacOS, and Linux (not including musl) are supported. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/1009"">#1009</a>, <a href=""https://github-redirect.dependabot.com/psf/black/issues/2431"">#2431</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not accept bare carriage return line endings in pyproject.toml (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2408"">#2408</a>)</li>; <li>Add configuration option (<code>python-cell-magics</code>) to format cells with custom magics in Jupyter Notebooks (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2744"">#2744</a>)</li>; <li>Allow setting custom cache directory on all platforms with environment variable <code>BLACK_CACHE_DIR</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2739"">#2739</a>).</li>; <li>Enable Python 3.10+ by default, without any extra need to specify -<code>-target-version=py310</code>. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2758"">#2758</a>)</li>; <li>Make passing <code>SRC</code> or <code>--code</code> mandatory and mutually exclusive (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2804"">#2804</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>Improve error",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11468:4538,Config,Configuration,4538,https://hail.is,https://github.com/hail-is/hail/pull/11468,1,['Config'],['Configuration']
Modifiability,"@@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; in",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3316,config,config,3316,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,"@Sun-shan According to the error message you posted, Spark itself cannot find `/hail/test/BRCA1.raw_indel.vcf`:; ```; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.; : org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/hail/test/BRCA1.raw_indel.vcf; ```. Looking at that error message, it looks like Spark is interpreting your path as a local file system path, _not_ a hadoop path. Moreover, earlier in your posted output this line:; ```; 17/08/15 08:58:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; ```; suggests that you're not actually connecting to a Spark cluster with a properly configured Hadoop installation. ---. Your Spark cluster appears improperly configured. I'm not sure if `pyspark` is even connecting to your cluster. You might try looking at [this StackOverflow post](https://stackoverflow.com/questions/34642292/cant-connect-pyspark-to-master) about connecting `pyspark` to a Spark cluster. I strongly recommend running `pyspark` again and executing:; ```; spark.sparkContext.master; ```; This should print the URL of your Spark master node. If this prints a String starting with `local`, then you're definitely not connecting to a Spark cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-322539635:769,config,configured,769,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-322539635,2,['config'],['configured']
Modifiability,"@bw2 that package name is a lie, sadly. The [maven repository page](https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20_2.11/5.5.1) for `org.elasticsearch:elasticsearch-spark-20_2.11:5.5.1` lists `org.apache.spark:spark-core_2.11:2.1.0` as a dependency, which is decidedly not 2.0. We'll have to use elasticsearch-spark 5.1.2. It's a bit annoying. You'll have to extend the [spark version-specific logic](https://github.com/hail-is/hail/pull/2049/files#diff-c197962302397baf3a4cc36463dce5eaR44) in `build.gradle`. You'll want to bind a new name, something like `elasticsearchSparkVersion`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2049#issuecomment-320335957:387,extend,extend,387,https://hail.is,https://github.com/hail-is/hail/pull/2049#issuecomment-320335957,1,['extend'],['extend']
Modifiability,"@bw2, it looks like you're picking up the router_fs, which should only be used when you have the QoB backend enabled. What settings do you have for `hailctl config get query/backend` and `HAIL_QUERY_BACKEND`? You can prefer the Spark backend with: `hailctl config set query/backend spark` and `HAIL_QUERY_BACKEND=spark`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12540#issuecomment-1363220199:157,config,config,157,https://hail.is,https://github.com/hail-is/hail/issues/12540#issuecomment-1363220199,2,['config'],['config']
Modifiability,"@catoverdrive yes, but that would be normal behavior. We register a series of compression codecs when creating the spark configuration/hadoop configuration/HailContext that hadoop uses to dispatch reading of the file to the appropriate input stream class, it does this based on a method in the codec classes like so in `BGZipCodec.java`; ```java; @Override; public String getDefaultExtension() {; return "".bgz"";; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5513#issuecomment-469540554:121,config,configuration,121,https://hail.is,https://github.com/hail-is/hail/pull/5513#issuecomment-469540554,2,['config'],['configuration']
Modifiability,"@cseed ; I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. I've updated the PR description with this text so it ends up in the squashed commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061:48,config,config-hail-root,48,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061,1,['config'],['config-hail-root']
Modifiability,"@cseed @catoverdrive This is how we should do aggregators. I implemented several as examples. It's a bit repetitive, maybe `@specialized` can help here? I'm kind of afraid of it. Some open issues:. - if the result of an aggregator is missing, I can't write it into the region, maybe rewrite `result` in continuation-passing style with a missing and non missing continuation? (is that function call indirection worth avoiding an allocation of a `Some(offset)` per-aggrgator-result?). - how do I take a user supplied function, e.g. `takeBy`? I keep avoiding lambda-like constructs. Do I introduce a new binding form, like `ApplyUnaryFunAggOp`. I don't like this path, but I also think adding a `Lambda` IR that isn't a full-fledged lambda will make the compiler look annoying/ugly too. This issue is basically the continuation of me punting on how to handle lambdas. - How do y'all feel about me eliminating some type-specific aggregators that can be implemented in terms of other ones (see AggOp.scala). If y'all are happy with this, I want to solve the missingness issue, and then farm out the last few (non-lambda-taking) aggregators to the team.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2606:283,rewrite,rewrite,283,https://hail.is,https://github.com/hail-is/hail/pull/2606,1,['rewrite'],['rewrite']
Modifiability,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-252825829:58,config,configured,58,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829,1,['config'],['configured']
Modifiability,"@cseed @tpoterba . There's now an option to disable the Unsafe warnings in javac. You have to `-XDenableSunApiLintControl` and then you can `@SuppressWarnings(""sunapi"")`. The changes that are not in build.gradle and build.sbt are just me fixing warnings. There were some meaningful things, like unused variables and some places we were unnecessarily allocating a tuple.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5951:302,variab,variables,302,https://hail.is,https://github.com/hail-is/hail/pull/5951,1,['variab'],['variables']
Modifiability,"@cseed @tpoterba @jigold. I think these are most of the structural changes I was planning on making to the python aggregator stuff so that the interface change can go in. I haven't changed the existing interface yet, but I figured I'd throw this up for comments if you guys wanted to take a look this morning. Some comments:; - I don't think that the IR introspection this is the correct way to do this, long term, but I think that the correct thing to do will be easier to implement correctly with the IR changes we discussed yesterday, so I'm inclined to do it this way for now since we'll have to rewrite it when we change the IR structure (assuming we still want to get the interface change in first and think about the IR rewrite next week). - I think there are some things in group_by that won't work with the current IR structure. I'll have some more thoughts on this in a bit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4540:600,rewrite,rewrite,600,https://hail.is,https://github.com/hail-is/hail/pull/4540,2,['rewrite'],['rewrite']
Modifiability,@cseed @tpoterba I accidentally undid the caching behavior in the tests by using the name of input variables from their name in the environment instead of just using a default set of variable names. Should be much faster now.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5578:99,variab,variables,99,https://hail.is,https://github.com/hail-is/hail/pull/5578,2,['variab'],"['variable', 'variables']"
Modifiability,"@cseed All set! Still not sure why 0.0.0.0 was needed in this case, but not Dan's config; first assumption is that JupyterLab sets this as default, and not sure. why listening on localhost was insufficient (first guess is that the docker image didn't specify EXPOSE 8888?). Still need to provide finer-grained status updates, based on more than status.phase (inspect container during the MODIFIED watch event). Also. need to re-implement auth_request to deal with (ignore) the ~30 requests subsequent to the redirect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460097832:82,config,config,82,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097832,1,['config'],['config']
Modifiability,@cseed Back to you. Can you please verify the `codec` variable is being used correctly?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1458#issuecomment-283766450:54,variab,variable,54,https://hail.is,https://github.com/hail-is/hail/pull/1458#issuecomment-283766450,1,['variab'],['variable']
Modifiability,"@cseed I don't really like how this looks with lists as the inputs to the commands. To me, it's much harder to read and modify. I like the commands looking as much like writing a shell script as possible. Maybe others feel differently though... Also, you can't do something like this ` ' '.join([task.ofile for task in shapeit_tasks])` because you'll lose information about the dependencies before `command` sees the original resource inputs. What I really want is a version of f-string interpolation where I parse and detect the variables (known and unknown), handle them properly by either creating new resources or adding dependencies to the Task, and then execute the Python formatting code inside the curly braces. I'm not sure if it is possible to do this. If it is, it's probably complicated and we'll have to use the Python `ast` and `parser` modules and call `eval` ourselves. ; ```python; from pyapi import Pipeline; p = Pipeline(). bfile_root = 'gs://jigold/input'; bed = bfile_root + '.bed'; bim = bfile_root + '.bim'; fam = bfile_root + '.fam'. p.write_input(bed=bed, bim=bim, fam=fam). subset = p.new_task(); subset = (subset; .label('subset'); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--make-bed', '--out', subset.tmp1]); .command(['awk', ""'{ print $1, $2}'"", subset.tmp1 + '.fam', ""| sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }'"",; '>', subset.tmp2]); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--remove', subset.tmp2,; '--make-bed', '--out', subset.tmp2])). shapeit_tasks = []; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .command(['shapeit', '--bed-file', subset.ofile, '--chr ', contig, '--out', shapeit.ofile])); shapeit_tasks.append(shapeit). merger = p.new_task(); merger = (merger; .label('merge'); .command(['cat', ' '.join([task.ofile for task in shapeit_tasks]), '>>', merger.ofile)). p.write_output(merger.ofile + "".haps"", ""gs://jigold/final_output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039:530,variab,variables,530,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039,1,['variab'],['variables']
Modifiability,"@cseed I know you didn't approve the final design document. In summary,. 1. We print a warning to the user if they specify the memory in the batch user library that we're ignoring this parameter. Memory is deduced from CPU and the worker configuration in the front end ignoring the resource request.; 2. There's a concept of reserved vs. unreserved space. Each job gets 5 Gi per core requested in the reserved space. We try and reserve unreserved space if needed for storage needs that are bigger than the reserved space with a semaphore. If we can't get enough unreserved space, then we still give the user the reserved space at 5Gi per core for their container on the worker data disk in addition to the extra disk at /io that is the full storage request. Example:. Storage request is 375Gi and 1 CPU.; User gets 5 Gi for their container.; We spin up a 375Gi disk. For a local ssd with 16 cores, there's 16*5 Gi or 80 Gi in the reserved space. The reserved space for us is 20 Gi (I can set this back to 25 Gi, but I thought 100 Gi being the minimum disk size for persistent SSD data disks was a nice number). This means the unreserved space that is first come first serve is 275 Gi. If the data disk is a 100Gi persistent SSD, then the unreserved space is 0 Gi and any job that requests more storage than 5 Gi per core will have to spin up a new disk.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9598#issuecomment-714610813:238,config,configuration,238,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-714610813,1,['config'],['configuration']
Modifiability,@cseed I refactored the resource stream into utils. Back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1780#issuecomment-302461856:9,refactor,refactored,9,https://hail.is,https://github.com/hail-is/hail/pull/1780#issuecomment-302461856,1,['refactor'],['refactored']
Modifiability,"@cseed I've split the packages up so they can be imported from within the appropriate init method. Edit: re-running ./gradlew shadowJar fixes this. Tests are not passing, but seemingly not because of any of these changes. If I checkout last commit in master, they also fail. Furthermore, I can restore all changes from last FS PR manually, and no benefit. ```; HEAD is now at 117c365c3 [ci] also handle batch Ready state (#5909); (hail) alex:~/projects/hail/hail/python:$ pytest test/ -x; ======================================================================================= test session starts ========================================================================================; platform darwin -- Python 3.6.8, pytest-3.8.0, py-1.7.0, pluggy-0.8.1; rootdir: /Users/alex/projects/hail/hail/python, inifile:; plugins: xdist-1.22.2, metadata-1.8.0, html-1.19.0, forked-1.0.2; collected 591 items . test/hail/test_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:816,plugin,plugins,816,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928,1,['plugin'],['plugins']
Modifiability,@cseed The part I am stuck on is the authentication for the router resolver. How does the batch2 instance in a test namespace get access to the real encryption key that the router resolver is expecting? Can you also double check the nginx configuration?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6918:239,config,configuration,239,https://hail.is,https://github.com/hail-is/hail/pull/6918,1,['config'],['configuration']
Modifiability,"@cseed This PR can be merged. I ran the comparison on the cloud between current master and this branch with UKBB Wave 1 Chr21 (20GB) with the exact same cluster configuration (Liam's default settings). Ran this command:. ```; %%timeit -n 1. hc.import_bgen(bgen_file, sample_file = sample_file).count(); ```. Got 3min21sec for master and 3min24sec for my branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2239#issuecomment-337996806:161,config,configuration,161,https://hail.is,https://github.com/hail-is/hail/pull/2239#issuecomment-337996806,1,['config'],['configuration']
Modifiability,"@cseed added these print statements. I've never used the output in debugging in production except to quickly realize a job is one of a given user's when I was looking at worker performance. Early on, I might have checked it to make sure the Docker config resource options and volume mounts were correct. But I can add those back when debugging if needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9184#issuecomment-667107337:248,config,config,248,https://hail.is,https://github.com/hail-is/hail/pull/9184#issuecomment-667107337,1,['config'],['config']
Modifiability,@cseed am I right that the thinking on this has evolved with move to Python?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/614#issuecomment-279583150:48,evolve,evolved,48,https://hail.is,https://github.com/hail-is/hail/issues/614#issuecomment-279583150,1,['evolve'],['evolved']
Modifiability,@cseed unrelated but I think you've configured your local git email and name to be hail-ci-leader,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5814#issuecomment-481693314:36,config,configured,36,https://hail.is,https://github.com/hail-is/hail/pull/5814#issuecomment-481693314,1,['config'],['configured']
Modifiability,"@cseed, digesting from this and our in-person conversation:. I added a steps about job dependencies and batch closure. Regarding ""a way to refer to individual inputs/outputs"", in the original post, I gave the example of:. ```; - type: exec; name: build-jar; image: hail-pr-builder; namespace: ns; command: [""./gradlew"", ""test"", ""shadowJar""]; outputs:; - ""build/libs/hail-all-spark.jar""; - type: exec; name: pytests; image: hail-pr-builder; dependsOn:; - build-jar; command: [""./run-python-tests-using-input-jar.sh""]; ```. The outputs of a job are stated explicitly by the definition. For the `pytests` job in the example above, the input from `build-jar` is placed at `/inputs/build-jar`. Regarding ""specify a series of stacked containers to execute"", I don't see a straightforward way to implement this. It's tricky enough to have a ""anti-init""/""finalizer"" container. Inter-job I/O will be handled by batch. The user controls the image and the command and the environment variables of the build step, so they can arrange for permission to copy results to a bucket they own. Are we worried about the setting of user's wanting to run untrustworthy software? They already run arbitrary software on cloud instances that have plenty of latent credentials. I think we can at least punt on this until other functionality is in. Local disk sounds like a nice thing to add eventually. Agreed, that sounds like a nice model. I'll consider it as I envision a persistent batch system.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-459537502:973,variab,variables,973,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-459537502,1,['variab'],['variables']
Modifiability,"@cseed: I feel somewhat guilty about duplicating a large amount of `AnnotateVariantsTable` code here, almost all of it. However, it isn't totally easy to refactor. I'm also happy to rename ""GenomicIndex"" to something better.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/438#issuecomment-227626931:154,refactor,refactor,154,https://hail.is,https://github.com/hail-is/hail/pull/438#issuecomment-227626931,1,['refactor'],['refactor']
Modifiability,"@danking . This should be ready to look at. Stacks on #5452. Once that PR is merged, changes are:. 1) notebook.html: organize into form (notebook-form.html) and notebook state (notebook-state.html) components.; 2) add modified versions of notebook-api. Namely I don't use the marshaling procedure you didn't like, and refactor as much of the JS stuff as I can into synchronous http requests.; * modifies /notebook routes , adds `marshall_notebook`, `get_live_user_notebooks`, `wait_websocket`, and replace any calls to `session['pod_name']` and `session['svc_name`] with equivalent versions based on `session['notebook']`, which contains the notebook object of the existing session. Changes mainly contained within commit: https://github.com/hail-is/hail/pull/5476/commits/2f180ed0bfb3b0dfb7224df1ef6afba0e1a9cbfc (the following pr only renames notebook-obj.html to notebook-state.html). Basically feels like a synchronous / refresh-based version of what we had on app.hail.is, with less state insight (uses only the websocket-based reachability check). Upcoming PR will restore fine-grained state updates via JS/websocket. cc @cseed. Images:; <img width=""1302"" alt=""screen shot 2019-02-28 at 3 06 46 pm"" src=""https://user-images.githubusercontent.com/5543229/53595163-88d8ea00-3b6a-11e9-841b-7dbf6981c990.png"">. <img width=""1301"" alt=""screen shot 2019-02-28 at 3 06 51 pm"" src=""https://user-images.githubusercontent.com/5543229/53595148-7ced2800-3b6a-11e9-9428-5290b5ee1dc7.png"">. <img width=""1301"" alt=""screen shot 2019-02-28 at 3 09 17 pm"" src=""https://user-images.githubusercontent.com/5543229/53595276-d35a6680-3b6a-11e9-930e-5ef0757e181e.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5476#issuecomment-468419208:318,refactor,refactor,318,https://hail.is,https://github.com/hail-is/hail/pull/5476#issuecomment-468419208,1,['refactor'],['refactor']
Modifiability,"@danking @catoverdrive OK, I think it's ready for another review. I made the suggested changes, as well as the changes we talked about on Tuesday during check-in. There is a ton of indirection now, which might be confusing but is good for future expansion. The matrix and table metadata are now polymorphic `RelationalSpec`s and have a `name` field so new relational types can be added. Here is `sample.hmt/metadata.json.gz`:. ```; {; ""components"": {; ""cols"": {; ""rel_path"": ""cols\/rows"",; ""name"": ""RVDComponentSpec""; },; ""rows"": ...,; ""partition_counts"": {; ""counts"": [; 346; ],; ""name"": ""PartitionCountsComponentSpec""; }; },; ""matrix_type"": ...,; ""references_rel_path"": ""references"",; ""hail_version"": ""devel-e6ef439"",; ""file_version"": 65536,; ""name"": ""MatrixTableSpec""; }; ```. A MT has currently has five components: globals, cols, rows, entries and partition_counts. The first four are `RVDComponents`, the counts its own thing. I wanted to make the references a component, but they need to be loaded before the types are parse, so `RelationalSpec`s have a path to the references. Components are future expandable. The MT directory structure has metadata and four directories for each main component which is a Table directory. Those directory names are stored in the metadata, so they can be changed or even point elsewhere. A Table directory has two directories: globals and rows, which are RVDs. Again, the directories are stored in the metadata and I use that here: the globals RVD for rows and cols tables are the rows RVD of the globals table of the MT (if you can understand this sentence you've got a handle on this PR.). RVDs now store their own metadata, the RVDSpec. A sample rows RVD metadata for the rows table of an MT, `sample.hmt/rows/rows/metadata.json.gz` looks like:. ```; {; ""jRangeBounds"": [],; ""partFiles"": [; ""part-0""; ],; ""codecSpec"": ...,; ""orvdType"": ...,; ""name"": ""OrderedRVDSpec""; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2858#issuecomment-364814856:295,polymorphi,polymorphic,295,https://hail.is,https://github.com/hail-is/hail/pull/2858#issuecomment-364814856,1,['polymorphi'],['polymorphic']
Modifiability,"@danking Curious for your thoughts on this refactor. I was getting pretty turned around myself with the various credential classes and I think this is closer to what we want in a keyless world. Ideally the batch worker should just be able to request credentials (in the form of an access token) for a given identity with just the identity's ID. LMK if you're in favor of this or not, or if you would like to see it folded into the metadata server PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14125#issuecomment-1879054756:43,refactor,refactor,43,https://hail.is,https://github.com/hail-is/hail/pull/14125#issuecomment-1879054756,1,['refactor'],['refactor']
Modifiability,"@danking I have a mostly completed draft for SAIGE in QoB. Can you take a look? I'm mainly looking for enough feedback to get a green light to actually start testing this end to end, fill in the remaining not implemented components, add documentation, add verbosity and possibly a dry run feature, and support VEP annotations natively. There are a couple of core concepts:; 1. Phenotypes - Set of phenotypes to test. I support the ability to group phenotypes together. This is in anticipation of a new version of SAIGE that Wei is going to release soon.; 2. VariantChunks - The set of variant intervals of data to test per job. If it's SAIGE-GENE, then there's also the ""groups"" to actually test within that interval.; 3. io - There's a bunch of wrappers that handle input and output files so all of that logic combined with the checkpointing logic is abstracted away from what is actually going on.; 4. steps - These are the SAIGE modules to run. They are all dataclasses with configuration options; 5. saige - There's a class that can be instantiated in Python or I started writing the framework for a CLI. This has the code that builds the DAG end to end. All configuration happens with a yaml file that can overwrite default parameters for each step such as whether to checkpoint or where the results should be written to. For the CLI, I envision you can either give a config file and/or specify `--overrides step1_null_glmm.use_checkpoint=true`. For every Saige run, I write out the configuration used to a file in the output directory as well as information about the input data and variant chunks and the batch information.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13804:978,config,configuration,978,https://hail.is,https://github.com/hail-is/hail/pull/13804,4,['config'],"['config', 'configuration']"
Modifiability,"@danking Looks like I'm still failing to configure a couple of settings related to references on the `ServiceBackend` but you can feel free to start looking. You'll notice that I made quite a substantial refactor in `ServiceBackend.scala` in an attempt to harmonize the scala backends a bit more. The rationale behind the refactor is I was having a hard time working with the various thunks passed around there. I saw them as a bit of poor-man's-object way to capture some state from the input file while keeping the `ServiceBackend` stateless. IMO there's no harm in keeping the `ServiceBackend` just as stateful as the other backends since it is single use. So I lifted a lot of that state into backend-creation time and created a harder delineation between which part of the input is for configuring the backend and which part is for the action being performed. This made it easier to reuse a couple of methods like `tableType` and such. I'm happy to take suggestions on ways to trim down this PR, but I thought you'd want to take a look at the whole thing given the time-sensitivity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13797#issuecomment-1766521995:41,config,configure,41,https://hail.is,https://github.com/hail-is/hail/pull/13797#issuecomment-1766521995,4,"['config', 'refactor']","['configure', 'configuring', 'refactor']"
Modifiability,"@danking Thanks for taking this over! I commented out the mark_unscheduled if the sidecar fails for debugging. The sidecar is running, but I'm getting an error because it's trying to run the top level code in batch.py. Either sidecar.py needs to be separate or we need to reconfigure batch.py so it doesn't run that code. ```; Traceback (most recent call last):; File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.6/dist-packages/batch/sidecar.py"", line 14, in <module>; from .batch import REFRESH_INTERVAL_IN_SECONDS, HAIL_POD_NAMESPACE, KUBERNETES_TIMEOUT_IN_SECONDS; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 83, in <module>; db = BatchDatabase.create_synchronous('/batch-user-secret/sql-config.json'); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 23, in create_synchronous; run_synchronous(cls.__init__(db, config_file)); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 15, in run_synchronous; return loop.run_until_complete(coro); File ""uvloop/loop.pyx"", line 1451, in uvloop.loop.Loop.run_until_complete; File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 210, in __init__; await super().__init__(config_file); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 27, in __init__; with open(config_file, 'r') as f:; FileNotFoundError: [Errno 2] No such file or directory: '/batch-user-secret/sql-config.json'; ```. To see the logs `kubectl -n namespace logs pod_name cleanup`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6600#issuecomment-510272888:871,config,config,871,https://hail.is,https://github.com/hail-is/hail/pull/6600#issuecomment-510272888,2,['config'],['config']
Modifiability,"@danking The latest version is the code in hail/methods/. I'm having trouble with all of the configs and how to instantiate that properly. After that, I need to figure out what inputs `run_saige` actually needs. Then I need to write util functions for creating the testing chunks and annotating the matrix table. Then I think after testing and cleaning it up, it will be sufficient for the workshop. To get it into main is going to be a lot more work to have helpful error messages, check MT is valid for this analysis, integrate it more carefully into QoB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13588:93,config,configs,93,https://hail.is,https://github.com/hail-is/hail/pull/13588,1,['config'],['configs']
Modifiability,@danking This should be really close to having the tests passing. The only other thing left to do once it passes in GCP is to copy the data over to Azure and make a default Azure configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1339804624:179,config,configuration,179,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1339804624,1,['config'],['configuration']
Modifiability,@danking What do you think about having a version ID inside the JAR file (MANIFEST???). We already download the JAR file on the worker. Not sure how much extra time it would be to look for the version inside the JAR (maybe cache this?) and then pass the right argument configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12222#issuecomment-1258646909:269,config,configuration,269,https://hail.is,https://github.com/hail-is/hail/pull/12222#issuecomment-1258646909,1,['config'],['configuration']
Modifiability,"@danking should internal_base_url in wait-for.py be https? Do we have anything that should route through http?. As an alternative, I think it would be reasonable to always rewrite port: 443 to https:// in that function. I can PR if you're ok with that. edit: The comment states that the protocol should match hailtop.config. Not sure why that is, besides sharing gateway probably? In any case, this function doesn't match that. We're always in the `self._location == 'external'` space I think, which means https according to hailtop.config",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548098055:172,rewrite,rewrite,172,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548098055,3,"['config', 'rewrite']","['config', 'rewrite']"
Modifiability,"@danking the changes in the router configuration for the blog are causing an infinite redirect loop when you try to go to https://blog.hail.is. I'd like to change them back to how they were before (I added the X-Real-IP line that was not in the original configuration, which I think is the change you were introducing in that PR, although I have no idea how that works.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8107:35,config,configuration,35,https://hail.is,https://github.com/hail-is/hail/pull/8107,2,['config'],['configuration']
Modifiability,"@danking, @cseed An attempt to use --notebook-dir failed (Didn't understand the path). Will make another attempt to set this as a config, but if not, I think we should defer folder creation as an improvement to jgscm, I'll open an issue. Have forked jgscm, and have identified what appears a likely path to the fix (they don't specify the full blob path, gs://bucket/blob). As an aside, jgscm is effectively unmaintained. 2 of the problems I've encountered have issues dating to May & August (last accepted PR was April 2018). After we patch in the fixes needed (dependencies, folder creation), I think we should consider publishing a separate package from our fork (say jgscm2), unless we want to maintain jgscm in our repo, which may be less desirable from a licensing perspective based on our earlier convos (jgscm is MIT, but I believe you still may prefer to not mix codebases?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480335780:130,config,config,130,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480335780,1,['config'],['config']
Modifiability,"@danking. Here what I have done in my environment ( AWS EMR ); * Create EMR without installing hail; * Update PATH ( this is needed or I get an error with `hailctl not found` at the installation step); ```sh; export PATH=$PATH:/home/hadoop/.local/bin; ```; * Clone latest commit of Hail; ```sh; cd /tmp; git clone --depth 1 https://github.com/broadinstitute/hail.git; cd hail/hail/; ```; * Edit `build.gradle` and add `exclude group: 'org.scala-lang', module: 'scala-reflect'`; * Build Hail; ```sh; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.2; ```; * Symlink hail-all-spark.jar into /opt ( At the EMR creation step (before hail installation) I edit the `spark-defaults` properties in order to link `hail-all-spark.jar`... This config was needed & works successfuly for an old version of Hail (0.2.60)... can be revisit if not appropriate for recent version; ```sh; sudo mkdir /opt/hail/; sudo ln -sf /home/hadoop/.local/lib/python3.9/site-packages/hail/backend /opt/hail/backend; ```; * start pyspark; ```sh; $ pyspark; Python 3.9.18 (main, Oct 25 2023, 05:26:35) ; [GCC 7.3.1 20180712 (Red Hat 7.3.1-17)] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/. Using Python version 3.9.18",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:772,config,config,772,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,1,['config'],['config']
Modifiability,"@danking; ```; [root@mg hail]# echo $HAIL_HOME; /opt/Software/hail; [root@mg hail]# echo $PYTHONPATH; :/opt/Software/hail/python:/opt/cloudera/parcels/SPARK2/lib/spark2/python:/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip; [root@mg hail]# cd /opt/Software/hail/python; [root@mg python]# ls; hail; [root@mg python]# cd /opt/cloudera/parcels/SPARK2/lib/spark2/python; [root@mg python]# ls; docs lib MANIFEST.in pylintrc pyspark README.md run-tests run-tests.py setup.cfg setup.py test_support; [root@mg python]# cd /opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/; [root@mg lib]# ls; py4j-0.10.4-src.zip PY4J_LICENSE.txt pyspark.zip; [root@mg lib]# echo $SPARK_CLASSPATH; /opt/Software/hail/build/libs/hail-all-spark.jar; [root@mg lib]# cd /opt/Software/hail/build/libs/; [root@mg libs]# ls; hail-all-spark.jar; ```; the configuration file:; ```; export SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337442177:847,config,configuration,847,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337442177,1,['config'],['configuration']
Modifiability,"@iris-garden I think you make great points! And I agree, across many PRs we probably do want to be analyzing the security impacts at every stage, not just as a one-off ""when we're done it will be X"" analysis in the ticket... So I guess in my mind the _only_ real reason for using the issue-level review would be for tracking the impact of non-code changes (like configuration updates to production). I will try to make the templates reflect that distinction",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14666#issuecomment-2329290981:362,config,configuration,362,https://hail.is,https://github.com/hail-is/hail/pull/14666#issuecomment-2329290981,1,['config'],['configuration']
Modifiability,@iris-garden Would you be interested in collaborating on this set of changes? I know you were starting to think about environment variables and configuration as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13224#issuecomment-1663830418:130,variab,variables,130,https://hail.is,https://github.com/hail-is/hail/pull/13224#issuecomment-1663830418,2,"['config', 'variab']","['configuration', 'variables']"
Modifiability,@jbloom22 I realized that updating the length of the array after creating it doesn't work because of the variable size of the missingness bits that get allocated.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3151#issuecomment-373180986:105,variab,variable,105,https://hail.is,https://github.com/hail-is/hail/pull/3151#issuecomment-373180986,1,['variab'],['variable']
Modifiability,"@jigold I assigned to you since this essentially re-opens #3715 with your comments addressed. After experimenting, I felt that it didn't make sense at this point to design flexible ndarray checking at the level of typecheckers. The requirements vary quite a bit between these two functions. If you feel strongly, let's discuss. I'll leave Tim's review un-dismissed until we've agreed where to put these functions, but no need to wait on it w.r.t reviewing (once you're back).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3873#issuecomment-401470955:172,flexible,flexible,172,https://hail.is,https://github.com/hail-is/hail/pull/3873#issuecomment-401470955,1,['flexible'],['flexible']
Modifiability,"@jigold I stood up batch/ci in my own project from `hail-is/hail:main` and then deployed this branch, taking notes of any changes I needed to make and all seemed to work out OK. I think that's about as much as I can properly test this without trying things out in haildev/hail-vdc. The steps were as follows:. 1. Generate the configmaps used by gateway/internal-gateway. These will have the routing configuration for production services (I've edited the bootstrap instructions to match); `make -C gateway envoy-xds-config && make -C internal-gateway envoy-xds-config`; 2. … wait a few seconds for CI to quietly update these configmaps with information about testing namespaces … (can manually verify changes with `download-configmap gateway-xds-config`); 3. Deploy the new versions of gateway/internal-gateway; `make -C gateway deploy NAMESPACE=default && make -C internal-gateway NAMESPACE=default`. This worked for me in my project with no downtime, but either way I would probably do the same thing as with the previous PR where I test it in azure before making changes to hail-vdc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095#issuecomment-1293703346:326,config,configmaps,326,https://hail.is,https://github.com/hail-is/hail/pull/12095#issuecomment-1293703346,7,['config'],"['config', 'configmap', 'configmaps', 'configuration']"
Modifiability,@jigold Lint failed because the `is_developer` variable is no longer used,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12972#issuecomment-1533377221:47,variab,variable,47,https://hail.is,https://github.com/hail-is/hail/pull/12972#issuecomment-1533377221,1,['variab'],['variable']
Modifiability,"@jigold Notebook has no tests, so it doesn't have an environment.yml that needs to be updated in the hail-ci-build-image. This configuration is only for the run-time service.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5451#issuecomment-467637462:127,config,configuration,127,https://hail.is,https://github.com/hail-is/hail/pull/5451#issuecomment-467637462,1,['config'],['configuration']
Modifiability,@jigold this is a minimal adaptation of #3466 which avoids exposing RowMatrix by putting an export command on BlockMatrix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3500#issuecomment-386445838:26,adapt,adaptation,26,https://hail.is,https://github.com/hail-is/hail/pull/3500#issuecomment-386445838,1,['adapt'],['adaptation']
Modifiability,"@jigold this needs gcloud installed and configured to work in cluster, specifically 'GOOGLE_APPLICATION_CREDENTIALS'. Will work on that after some higher priority items are in, I believe it is sufficient to have this working on our local machines for now (manual user creation). Will unassign for now, and re-assign when gcloud is configured. Any suggestions on how to get that configured on the cluster would be much appreciated too :). cc @cseed, @danking",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5618#issuecomment-479316954:40,config,configured,40,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-479316954,3,['config'],['configured']
Modifiability,"@jigold. > I ran a test job with the copy tool on a 10 Gi random file and matched 1.2 Gibit / second. Does this mean something like:; ```; j = b.new_job(); j.image('hailgenetics/hail:0.2.118'); j.command('python3 -m hailtop.copy ...'); ```; Or did you use a `read_input`? I'm curious if something about how we configure the input container could affect this. I doubt it, but wanted to confirm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1599420578:310,config,configure,310,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1599420578,1,['config'],['configure']
Modifiability,"@lfrancioli look at the built docs (under TeamCity, artifacts, index):; https://ci.hail.is/repository/download/HailSourceCode_HailMainline_BuildDocs/9716:id/www/hail/types.html#set-t. There is an issue of variable naming: your a is implicit (not named), and your b is our a. So for example:; ```; add(a: T): Set[T] – Returns the result of adding the element b to Set a.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1491#issuecomment-284773672:205,variab,variable,205,https://hail.is,https://github.com/hail-is/hail/pull/1491#issuecomment-284773672,1,['variab'],['variable']
Modifiability,"@mhebrard I notice you're using `sudo make`, I suspect this means that Hail's code is running under a modified `PATH` that lacks `pip-compile`. We'll fix our install-on-cluster target to have a ""make the artifact"" and an ""install"" step that are separate (so you can install as root but build as a normal user). In the mean time, apply this patch:. ```; diff --git a/hail/Makefile b/hail/Makefile; index dabe146d3a..e12ac791c4 100644; --- a/hail/Makefile; +++ b/hail/Makefile; @@ -349,7 +349,7 @@ install: $(WHEEL); hailctl config set query/backend spark; ; .PHONY: install-on-cluster; -install-on-cluster: $(WHEEL) check-pip-lockfile; +install-on-cluster: $(WHEEL); sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$$' | tr '\n' '\0' | xargs -0 $(PIP) install -U; -$(PIP) uninstall -y hail; $(PIP) install $(WHEEL) --no-deps. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445#issuecomment-1690045571:523,config,config,523,https://hail.is,https://github.com/hail-is/hail/issues/13445#issuecomment-1690045571,1,['config'],['config']
Modifiability,"@mhebrard I would expect that `MutableSettings` to come from your Spark installation. I think there are two problems:; 1. For some reason, our gradle configuration is including `MutableSettings`. We should get rid of that, but I haven't yet figured out how to do that.; 2. Normally (1) isn't a problem because your Spark installation appears on the class path before Hail does. It seems to me that this isn't true in your case. This is probably caused by linking the Hail JAR into `/opt`. Is `/opt/hail/backend` on your class path? Why do you link the backend directory into `/opt`? The Hail JAR should be distributed automatically by Spark. You shouldn't need to do anything special after you `pip install` / `make install-on-cluster`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1777706782:150,config,configuration,150,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1777706782,1,['config'],['configuration']
Modifiability,"@mhebrard The only way I can imagine that we would mutate your environment is if we are accidentally installing `pyspark`. `install-on-cluster` takes pains to avoid that:; ```; install-on-cluster: $(WHEEL) check-pip-lockfile; 	sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$$' | tr '\n' '\0' | xargs -0 $(PIP) install -U; 	-$(PIP) uninstall -y hail; 	$(PIP) install $(WHEEL) --no-deps; 	hailctl config set query/backend spark; ```. But that is broken somehow? When you ran `install-on-cluster` did you see a `pip` output indicating that pyspark got installed?. Can you check if pyspark is installed via pip now? `pip show pyspark` (should say its not installed). If it is installed, try uninstalling it `pip uninstall pyspark`. You might also try removing the first line of `install-on-cluster` entirely. That will leave you without Hail's dependencies installed, but if `pyspark-shell` is still the right version of Scala, then I suspect the issue is that line.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1769053906:433,config,config,433,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1769053906,1,['config'],['config']
Modifiability,"@mhebrard can you try applying this diff and then building?. ```diff; diff --git a/hail/build.gradle b/hail/build.gradle; index d4bdd879f0..1b65904484 100644; --- a/hail/build.gradle; +++ b/hail/build.gradle; @@ -100,6 +100,7 @@ configurations {; hailJar.extendsFrom implementation; hailJar {; exclude group: 'org.scala-lang', module: 'scala-library'; + exclude group: 'org.scala-lang', module: 'scala-reflect'; exclude group: 'org.apache.spark'; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1777721812:229,config,configurations,229,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1777721812,2,"['config', 'extend']","['configurations', 'extendsFrom']"
Modifiability,@patrick-schultz I addressed your comments in the second commit and added sum/count aggregators. I also refactored the tests to test them; they're otherwise still the same.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6501#issuecomment-506923400:104,refactor,refactored,104,https://hail.is,https://github.com/hail-is/hail/pull/6501#issuecomment-506923400,1,['refactor'],['refactored']
Modifiability,"@patrick-schultz I'm sending this back to you because I made some pretty drastic changes trying to fix some errors. The biggest non-refactoring change that the original this introduces is that we can't parse IR for a persisted block matrix reader if the persisted block matrix doesn't exist. (This makes some amount of sense if you consider that we also can't parse the IR for a native block matrix reader if the file doesn't exist.). This led me down a rabbit hole of test failures since we're parsing IR/types a fair number of times, through the execution and after we get the result. After fiddling with it for a little bit, I removed UnpersistBlockMatrix. I'm not sure what I was thinking when I added it. I re-added an ""unpersist"" function to the backend to handle unpersisting BlockMatrices. It differs from the current Table/MatrixTable unpersist functions in that we only pass the id of the thing we want to unpersist, not the entire IR, since that's all we need.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9421#issuecomment-691300692:132,refactor,refactoring,132,https://hail.is,https://github.com/hail-is/hail/pull/9421#issuecomment-691300692,1,['refactor'],['refactoring']
Modifiability,"@patrick-schultz your assignment was totally random!. Here is the code for TableDistinct:; ```scala; case class TableDistinct(child: TableIR) extends TableIR {. ... protected[ir] override def execute(hc: HailContext): TableValue = {; val prev = child.execute(hc); prev.copy(rvd = prev.rvd.distinctByKey()); }; }; ```. I suspect that if the RVD key is longer than the table `typ.key`, this is going to produce incorrect results.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4833:142,extend,extends,142,https://hail.is,https://github.com/hail-is/hail/issues/4833,1,['extend'],['extends']
Modifiability,"@tpoterba , can you take a look as well?. Notes:. 1. Azure uses Spark 3.0.2, so I need to build and publish a wheel for Spark 3.0.2.; 2. Azure provides Jupyter Notebooks already.; 3. hail/Makefile (for manual deploys) was missing some changes for deploy.sh, so I updated it.; 4. Azure sets the `AZURE_SPARK` environment variable inside hosted Jupyter Notebooks. 5. In Azure's Jupyter, if you set `extraClassPath` you break the extant classpath (e.g. you cannot load Scala stdlib classes). However, the JARs specified in `spark.jars` are added to the classpath properly, so, in Azure, it suffices to specify `spark.jars`. 6. Azure lacks requester pays, so I require Azure users download, untar, and upload the VEP files to their own bucket. 7. Instead of ""submit"", Azure installs Livy, a Java job-queue system. I have no idea how to set environment variables in Livy and Azure does not set AZURE_SPARK in Livy jobs; therefore, I search for `hdinsight` in the CLASSPATH.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11187:320,variab,variable,320,https://hail.is,https://github.com/hail-is/hail/pull/11187,2,['variab'],"['variable', 'variables']"
Modifiability,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:126,extend,extends,126,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437,1,['extend'],['extends']
Modifiability,"@tpoterba @cseed. I ran into some issues building images that I addressed as a part of this PR. The main issue was that we needed the `beta` `gcloud` commands, so I added a line to the Dockerfile to load those. I also made a couple changes to the Docker build commands to ensure we at least reuse all images that are available in our GCR. I added `hail-pr-builder` as a cache source in an attempt to take advantage of successful local builds. This, unfortunately, does not allow us to reuse successful layers from a failing build. I noticed that if I build and fail once using `--cache-from`, then I cannot re-use the succeeding layers of the failed build, regardless of whether I supplied `--cache-from`. I also added `:latest` to the `docker images` command because I have a couple tagged images from earlier iterations of this script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4362#issuecomment-424066734:502,layers,layers,502,https://hail.is,https://github.com/hail-is/hail/pull/4362#issuecomment-424066734,2,['layers'],['layers']
Modifiability,"@tpoterba @jbloom22 a few more things to fix for the workshop. I was using a too powerful kubernetes command to look up worker pods and services for the admin page. I now use a restricted form of it that is permitted by our security policy. We also are missing the non-preemptible node pool (!), so this adds that to our gcp-config. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4862:325,config,config,325,https://hail.is,https://github.com/hail-is/hail/pull/4862,1,['config'],['config']
Modifiability,"@tpoterba @konradjk A workaround for this issue, should you encounter it again, is to disable the conscrypt library with this dataproc config:. `dataproc:dataproc.conscrypt.provider.enable: 'false'`. Capturing a core file provided a little more detail, but google support cannot explain why it happens. Cheers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053#issuecomment-444135177:135,config,config,135,https://hail.is,https://github.com/hail-is/hail/issues/3053#issuecomment-444135177,1,['config'],['config']
Modifiability,"@tpoterba @patrick-schultz Ok, I switched to elif style and made fixes so that the surrounding code also followed the elif style. I'd like to merge this and push off any further discussions to another PR. IMO, the no-else-return style is almost always nicer when I want to:; - bind a variable half way through an if chain, or; - have a complex condition (see the one with a while loop around 3400) that demands a nested if (I have to duplicate the common fall-through case)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5913#issuecomment-486809766:284,variab,variable,284,https://hail.is,https://github.com/hail-is/hail/pull/5913#issuecomment-486809766,1,['variab'],['variable']
Modifiability,"@tpoterba I added [another PR](https://github.com/hail-is/hail/pull/1613) which adds a `./configure` script which walks the user through setting a spark version (in the future we can add other parameters too). Perhaps that's the best way to manage this going forward?. If the gradle.properties file doesn't exist, our gradle script errors and asks the user to run `./configure`. The `./configure` script queries the user for sparkVersion and generates a valid `gradle.properties` file. Afterwards, the user can execute gradle normally without any `-D` parameters. Users may still override the `sparkVersion` variable on the command line by specifying `-PsparkVersion=2.1.1`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1608#issuecomment-290198582:90,config,configure,90,https://hail.is,https://github.com/hail-is/hail/pull/1608#issuecomment-290198582,4,"['config', 'variab']","['configure', 'variable']"
Modifiability,@tpoterba I just realized I forgot to propagate the FUSE config through to worker jobs. Should I be and I got lucky that the singular test is just doing everything driver-side? Or is there a test we can write to ensure that worker jobs access the FASTA data?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736#issuecomment-1478404495:57,config,config,57,https://hail.is,https://github.com/hail-is/hail/pull/12736#issuecomment-1478404495,1,['config'],['config']
Modifiability,@tpoterba I thought the conversions would lift `Int`s to `Double`s before unifying the type variables. What are struct attributes? I am a unsure that our conversions would work for struct field types.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1630#issuecomment-291173170:92,variab,variables,92,https://hail.is,https://github.com/hail-is/hail/pull/1630#issuecomment-291173170,1,['variab'],['variables']
Modifiability,"@tpoterba I was expecting we'd change the function registry to require the same type variable for left and right, i.e.:. ```scala; register(""=="", (a: Any, b: Any) => a == b, null)(TTHr, TTHr, boolHr); register(""!="", (a: Any, b: Any) => a != b, null)(TTHr, TTHr, boolHr); ```. Did that not work correctly?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1630#issuecomment-291159078:85,variab,variable,85,https://hail.is,https://github.com/hail-is/hail/pull/1630#issuecomment-291159078,1,['variab'],['variable']
Modifiability,"@tpoterba Is this any progress on this issue? We have a bunch of VCFs generated by SV programs ( Delly, GenomeStrip, Manta and Lumpy) that have imprecise SV variants that use these type of field formats. . VCF 4.2 spec; ```; 1.2.5 Alternative allele field format; Symbolic alternate alleles for imprecise structural variants:; ##ALT=<ID=type,Description=description>; The ID field indicates the type of structural variant, and can be a colon-separated list of types and subtypes. ID; values are case sensitive strings and may not contain whitespace or angle brackets. The first level type must be one; of the following:; • DEL Deletion relative to the reference; • INS Insertion of novel sequence relative to the reference; • DUP Region of elevated copy number relative to the reference; 2; • INV Inversion of reference sequence; • CNV Copy number variable region (may be both deletion and duplication); The CNV category should not be used when a more specific category can be applied. Reserved subtypes include:; • DUP:TANDEM Tandem duplication; • DEL:ME Deletion of mobile element relative to the reference; • INS:ME Insertion of a mobile element relative to the reference ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413#issuecomment-386162935:848,variab,variable,848,https://hail.is,https://github.com/hail-is/hail/issues/3413#issuecomment-386162935,1,['variab'],['variable']
Modifiability,"@tpoterba OK, this is ready for final review. Flags are now duplicated in Python so that service backend can perform all actions without starting a JVM. I have a test that verifies the flag sets, their envvars, and the default values, are all the same. I preserved the randomness behavior. We can address that in a separate PR. The flags now use the Hail `configuration_of` machinery which checks, in order:; - an explicit value (not relevant to flags); - a deprecated environment variable (these are the current flag envvars); - an environment variable with a mechanically derived name (e.g. `HAIL_QUERY_NO_WHOLE_STAGE_CODEGEN`); - the hail configuration file (usually: ""~/.config/hail/config.ini"") under the section ""query"". FWIW, hail configuration files look like this:. ```; (base) dking@wm28c-761 hail % cat ~/.config/hail/config.ini ; [query]; backend = spark; jar_url = gs://hail-query/jars/dking/0wfcw2e6sma9/f4fb19e3d387d6efc6cf0f19b95bec59c95b793a.jar. [batch]; remote_tmpdir = gs://1-day/dktmp/; billing_project = hail. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12423#issuecomment-1411200434:481,variab,variable,481,https://hail.is,https://github.com/hail-is/hail/pull/12423#issuecomment-1411200434,8,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"@tpoterba fixed the config issue and changed n_partitions to ensure workers are scheduled for the FASTA reading. I tested this on a single batch worker so the jobs overlapped and flexed the shared mount code, but we don't really have a guarantee in our test setup because batch has no way to force collocation of jobs (and even so we can't exactly force that the runtimes will overlap). I suppose if there's an issue here it will bubble up as a nondeterministic failure. Not great but perhaps good enough for now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736#issuecomment-1499151688:20,config,config,20,https://hail.is,https://github.com/hail-is/hail/pull/12736#issuecomment-1499151688,1,['config'],['config']
Modifiability,"@tpoterba there is one minor implementation difference, wanted to check if you considered it too far afield: removed `PStringOptional` and `PStringRequired`. These are used in only 3 classes, and *Optional/Required classes are inconsistently used in the codebase anyways. By removing them we have fewer legacy constructors hanging off PArray.; * Furthermore, by adding the final class modifier to PCanonicalString, I'm not sure we can implement a case object in the same way (cannot extend PCanonicalString, which means places that expect a PType, like `StagedBlockLinkedListSuite.scala:159`, won't type check, if we simply make a PStringOptional with a constructor that calls `PCanonicalStruct()`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7750:483,extend,extend,483,https://hail.is,https://github.com/hail-is/hail/pull/7750,1,['extend'],['extend']
Modifiability,"A `DependentFunCode` is a function that additionally depends on the particular type of its arguments, even if it is otherwise generic. This likely provides a path towards refactoring `TNumeric` as well. cc: @cseed @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2226:171,refactor,refactoring,171,https://hail.is,https://github.com/hail-is/hail/pull/2226,1,['refactor'],['refactoring']
Modifiability,"A few improvements:. 1. Show `n_partitions` in the ""Exploring Tables"" section. Fixes #7827 ; 2. Show `drop` in the ""Subset variables"" section (based on user question).; 3. Advertise `hl.plot.show` instead of `bokeh.io.show`. Fixes #7911",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7963:123,variab,variables,123,https://hail.is,https://github.com/hail-is/hail/pull/7963,1,['variab'],['variables']
Modifiability,"A forthcoming change to the hail ci system will introduce deployment. This change adds `hail-ci-deploy.sh` which replicates the [""Deploy Website""](https://ci.hail.is/admin/editRunType.html?id=buildType:HailSourceCode_HailMainline_DeployWebsite&runnerId=RUNNER_29) and [""Deploy Google Cloud""](https://ci.hail.is/admin/editRunType.html?id=buildType:HailSourceCode_HailMainline_DeployDocsAndGoogleCloudSpark220&runnerId=RUNNER_10) TeamCity jobs. My general thinking for deploy jobs from the CI is that, for the time being, we'll hardcode a mapping from GitHub repository to [Kubernetes Secret](https://kubernetes.io/docs/concepts/configuration/secret/). That's where this `/secret/ci.hail.is-web-updater-rsa-key` will come from. Moreover, the CI will always authorize a gcloud account (again with a baked in mapping from GitHub repository to GCP service account) before calling the deploy script. I did not retest the master branch here. Should we do that even though a PR is only merged to master if it passes the tests? Even after locking down merging, there's still the possibility of CI bugs. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4220:627,config,configuration,627,https://hail.is,https://github.com/hail-is/hail/pull/4220,1,['config'],['configuration']
Modifiability,"A long-standing fixme in the LocalBackend was to not rely on HadoopFS, which we use with the SparkBackend for compatibility with dataproc and hdfs urls. By default, the HadoopFS doesn't understand gs urls. Users need to install the gcs-hadoop-connector (preinstalled in dataproc) to communicate with google cloud storage. Spark handles supplying credentials to the connector. Issue #13904 is caused by failing to properly supply the gcs-hadoop-connector with credentials in the LocalBackend. In the absence of config, the connector hangs while trying to fetch a token form a non-existant metadata server. The LocalBackend was designed to be a testing ground for lowered and compiled code that would eventually be run on batch, where we use the RouterFS. I propose a pragmatic fix for #13904 that ditches the HadoopFS for all but local filesystem access in the LocalBackend instead of identifying and fixing the root cause. In doing so, I made a couple of changes to how the RouterFS is configured: In the absence of the `HAIL_CLOUD` environment variable, RouterFS can handle gs and az urls iff credentials are not supplied. If the user supplies creditials, we use `HAIL_CLOUD` to decide which cloud to route to. fixes #13904",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14407:510,config,config,510,https://hail.is,https://github.com/hail-is/hail/pull/14407,3,"['config', 'variab']","['config', 'configured', 'variable']"
Modifiability,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112:854,refactor,refactored,854,https://hail.is,https://github.com/hail-is/hail/pull/7112,1,['refactor'],['refactored']
Modifiability,"A simple but powerful extension requested by @alexb-3 and Christina to allow for synthetic genotypes with very general and realistic-looking PCA plots with [redacted]. Alex pointed out that BaldingNichols is special case of PritchardStephensDonnelly in a degenerate sense, just as one-hot encoded `Categorical(p_1,...,p_k)` is the distributional limit of `Dirichlet(a * p_1,..., a * p_k)` as `a` goes to 0. So the substantive changes took about 10 lines. It's turned on by the `mixture` parameter which defaults to False and is marked as experimental. `True` means treat `pop_dist` as the parameters of Dirichlet rather than Categorical. @alexb-3 , it'd be great if you and Christina could experiment with it and extend the documentation accordingly. Once we have that, I'll add tests and remove ""experimental"". The plots below are already quite convincing. ```; import hail as hl; import matplotlib.pyplot as plt. mt = hl.balding_nichols_model(3, 500, 50, pop_dist=[0.01, 0.02, 0.05], fst=[.2, .3, .5]); _, pcs, _ = hl.hwe_normalized_pca(mt, 3); plt.scatter(pcs.PC1.collect(), pcs.PC2.collect()); ```. ![ex0](https://user-images.githubusercontent.com/3201642/37743475-a470a372-2d40-11e8-894c-5ed0d74f3d14.png). ```; mt = hl.balding_nichols_model(3, 500, 50, pop_dist=[0.01, 0.02, 0.05], fst=[.2, .3, .5], mixture=True); ```. ![ex1](https://user-images.githubusercontent.com/3201642/37743104-decf0da8-2d3e-11e8-8d43-3e36f194fa8e.png). ```; mt = hl.balding_nichols_model(3, 500, 50, pop_dist=[0.1, 0.2, 0.5], fst=[.2, .3, .5], mixture=True); ```. ![ex2](https://user-images.githubusercontent.com/3201642/37743108-e2e4cfe0-2d3e-11e8-9860-724de2c6611c.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3206:713,extend,extend,713,https://hail.is,https://github.com/hail-is/hail/pull/3206,1,['extend'],['extend']
Modifiability,"A small bit of refactoring. I've moved schema and math for both LinearRegression and LinearRegressionMultiPheno to the LinearRegressionModel class, and now fit returns LinearRegressionStats which in turn have toAnnotation functions. This provides better separation of data prep and annotation from core math, in line with structure of LogisticRegression(Model), and sets stage for next step of generalizing genotype field. I've left LinearRegression3 as is for now, full consolidation may wait until 0.2.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2007:15,refactor,refactoring,15,https://hail.is,https://github.com/hail-is/hail/pull/2007,1,['refactor'],['refactoring']
Modifiability,"A test failed because `hailctl config unset` now returns an error if the config variable does not exist. Let me know if you think we should maintain the current behavior -- otherwise, I slightly modified the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13224#issuecomment-1677602623:31,config,config,31,https://hail.is,https://github.com/hail-is/hail/pull/13224#issuecomment-1677602623,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"A test timed out with this in the logs. This is some driver job. It just hangs for 2 minutes trying to talk to Azure Blob Storage presumably? Let us get some more information:. ```; 2023-06-08 20:22:28.209 JVMEntryway: INFO: Yielding control to the QoB Job.; 2023-06-08 20:22:28.210 Tokens: INFO: tokens found for namespaces {pr-12991-default-ei61x1qrplk9}; 2023-06-08 20:22:28.210 tls: INFO: ssl config file found at /batch/240df6ec091b49d8a6062b781e6700d3/secrets/ssl-config/ssl-config.json; 2023-06-08 20:24:30.873 : INFO: RegionPool: initialized for thread 10: pool-2-thread-2; 2023-06-08 20:24:31.016 ServiceBackend$: INFO: executing: iaU8w3QX6Y6hRrB9jczJds ArrayBuffer((), is.hail.utils.SerializableHadoopConfiguration@5ad5cde6); 2023-06-08 20:24:31.153 : INFO: JSON: JObject(List((name,JString(TableFilterPartitions)), (parts,JArray(List(JInt(0)))), (keep,JBool(true)))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13157:397,config,config,397,https://hail.is,https://github.com/hail-is/hail/pull/13157,3,['config'],['config']
Modifiability,"A very small PR but here's the background and context behind this change. When talking to either GCP or Azure, hail chooses credentials in the following order from highest priority to lowest priority:. 1. An explicit `credential_file` argument passed to the relevant credentials class; 2. An environment variable containing the path to the credentials (`GOOGLE_APPLICATION_CREDENTIALS` or `AZURE_APPLICATION_CREDENTIALS`) (from this you can see why the code that was here is totally redundant); 3. The latent credentials present on the machine. This might be `gcloud` or `az` credentials, or the metadata server if you're on a cloud VM. I'm trying to rid the codebase of most explicit providing of credentials file paths, for two reasons:; - Quality of life. I'm already signed into the cloud with `gcloud` and `az`. I shouldn't need to download some file and provide `AZURE_APPLICATION_CREDENTIALS` to run this test. It should just use the latent credentials.; - We are trying to phase out credentials files altogether for security reasons. These files are long-lived secrets that you really don't want to leak and are currently exposed to users in Batch jobs, so they can be easily exfiltrated. Using the latent credentials on a cloud VM (the metadata server) has the benefit of only issuing short-lived access tokens which last for hours not months, so it's basically always better to use the latent credentials when possible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13981:304,variab,variable,304,https://hail.is,https://github.com/hail-is/hail/pull/13981,1,['variab'],['variable']
Modifiability,"A very uninteresting PR but just to show you more bits of the codebase that are relevant to our earlier `python-dill` bug. We publish a small collection of images in DockerHub that users can use, like `python-dill` and a `hail` image that includes the whole hail pip package. You can use these like `j.image('hailgenetics/hail')`. However, DockerHub sets severe rate limits that would throttle a large batch from pulling those images on N workers for sufficiently large N. So, we mirror these images in our private image registry in GCP / Azure. If a user submits a job with one of these images, we instead pull from our own registry instead. This script does the mirroring from DockerHub -> internal registry. All I did in this PR is refactor the script. I don't honestly know why I used two lists instead of one, there was probably at one point some difference in how these images were handled that got deleted.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12231:735,refactor,refactor,735,https://hail.is,https://github.com/hail-is/hail/pull/12231,1,['refactor'],['refactor']
Modifiability,"A/pylint) from 2.6.0 to 2.12.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/pylint/releases"">pylint's releases</a>.</em></p>; <blockquote>; <h2>pylint-2.8.1</h2>; <ul>; <li>; <p>Add numversion back (temporarily) in <code>__pkginfo__</code> because it broke Pylama and revert the unnecessary; <code>pylint.version</code> breaking change.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/4399"">#4399</a></p>; </li>; </ul>; <h2>pylint-2.8.0</h2>; <ul>; <li>; <p>New refactoring message <code>consider-using-with</code>. This message is emitted if resource-allocating functions or methods of the; standard library (like <code>open()</code> or <code>threading.Lock.acquire()</code>) that can be used as a context manager are called without; a <code>with</code> block.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/3413"">#3413</a></p>; </li>; <li>; <p>Resolve false positives on unused variables in decorator functions</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/4252"">#4252</a></p>; </li>; <li>; <p>Add new extension <code>ConfusingConsecutiveElifChecker</code>. This optional checker emits a refactoring message (R5601 <code>confusing-consecutive-elif</code>); if if/elif statements with different indentation levels follow directly one after the other.</p>; </li>; <li>; <p>New option <code>--output=&lt;file&gt;</code> to output result to a file rather than printing to stdout.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/1070"">#1070</a></p>; </li>; <li>; <p>Use a prescriptive message for <code>unidiomatic-typecheck</code></p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/3891"">#3891</a></p>; </li>; <li>; <p>Apply <code>const-naming-style</code> to module constants annotated with; <code>typing.Final</code></p>; </li>; <li>; <p>The packaging is now done via s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11461:1045,variab,variables,1045,https://hail.is,https://github.com/hail-is/hail/pull/11461,2,['variab'],['variables']
Modifiability,"AFAIK, we do not use this file and it confuses Emacs. Emacs uses this configuration; instead of the one at the root of the repository.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9465:70,config,configuration,70,https://hail.is,https://github.com/hail-is/hail/pull/9465,1,['config'],['configuration']
Modifiability,"According to the makefile documentation https://www.gnu.org/software/make/manual/html_node/Splitting-Recipe-Lines.html:; ```; Notice how the backslash/newline pair was removed inside the string quoted with double quotes (""…""), ; but not from the string quoted with single quotes ('…'). This is the way the default shell (/bin/sh) ; handles backslash/newline pairs. If you specify a different shell in your makefiles it may treat them differently.; ```. Seems you (or `brew`) may have configured `make` to use something other than `/bin/sh`.; Quick way to verify:. ```Makefile; .PHONY: print-shell; print-shell:; 	@echo $(SHELL); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1890075303:484,config,configured,484,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1890075303,1,['config'],['configured']
Modifiability,"Actually I just went ahead and looked at this PR to make sure highcpus were indeed highcpus. They are!; <img width=""1476"" alt=""Screen Shot 2021-12-09 at 11 56 56 AM"" src=""https://user-images.githubusercontent.com/106194/145441305-fec38573-9c66-4a95-9fb7-0e6dc3a7c2e9.png"">. I also grabbed all the VM details and all the things that should be different (vm name, Nic name, etc.) are different. The userData is myseriously null, but its null for every VM in Azure currently (other PRs, namespaces, and default). ```; {; ""additionalCapabilities"": null,; ""applicationProfile"": null,; ""availabilitySet"": null,; ""billingProfile"": {; ""maxPrice"": -1.0; },; ""capacityReservation"": null,; ""diagnosticsProfile"": null,; ""evictionPolicy"": ""Delete"",; ""extendedLocation"": null,; ""extensionsTimeBudget"": null,; ""hardwareProfile"": {; ""vmSize"": ""Standard_F16s_v2"",; ""vmSizeProperties"": null; },; ""host"": null,; ""hostGroup"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/virtualMachines/batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5"",; ""identity"": {; ""principalId"": null,; ""tenantId"": null,; ""type"": ""UserAssigned"",; ""userAssignedIdentities"": {; ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.ManagedIdentity/userAssignedIdentities/batch-worker"": {; ""clientId"": ""890af904-42f1-4136-810a-c52f4e132c6b"",; ""principalId"": ""b952a3bb-1091-4f11-803b-9d5199219a27""; }; }; },; ""instanceView"": null,; ""licenseType"": null,; ""location"": ""eastus"",; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5"",; ""networkProfile"": {; ""networkApiVersion"": null,; ""networkInterfaceConfigurations"": null,; ""networkInterfaces"": [; {; ""deleteOption"": ""Delete"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Network/networkInterfaces/batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-nic"",; ""primary"": null,; ""resourceGroup"": ""dgoldste""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:738,extend,extendedLocation,738,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['extend'],['extendedLocation']
Modifiability,"Actually, v95 might be configured differently, so we do need to do something else...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1832748615:23,config,configured,23,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1832748615,1,['config'],['configured']
Modifiability,"Add `product` option to `Table.index`, and `MatrixTable.{index_rows, index_cols}`. Supports interval joins. Refactored the index methods to reduce code duplication, and make them more consistent with each other. Only case not supporting `product=True` is annotating columns of a matrix table with an interval keyed table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5937:108,Refactor,Refactored,108,https://hail.is,https://github.com/hail-is/hail/pull/5937,1,['Refactor'],['Refactored']
Modifiability,"Add a codegen method `SNDArray.coiterate`, with signature; ```; def coiterate(cb: EmitCodeBuilder, region: Value[Region], indexVars: IndexedSeq[String], arrays: IndexedSeq[(SNDArrayCode, IndexedSeq[Int], String)], body: IndexedSeq[SSettable] => Unit, deepCopy: Boolean): Unit; ```; For example, the index expression `A[i, j] += B[j]` would be written; ```; coiterate(cb, region, Seq(""i"", ""j""), Seq((A, Seq(0, 1), ""A""), (B, Seq(1), ""B"")), {; case Seq(a, b) => cb.assign(a, SCode.add(cb, a, b)); }); ```; This generates a loop nest, with one loop per variable in `indexVars`. The inner loop is `indexVars(0)`, so that column major traversal is when index variables are increasing, as in `(A, Seq(0, 1), ""A"")`. Each index variable iterates over a dimension, with the size of the dimension inferred from its use. In the inner loop, each index variable `i0, i1, ...` has a value; `body` is run, with each of the `SSettable`s bound to an element of the corresponding input in `arrays`. For example, if the first element of `arrays` is `(A, IndexedSeq(1, 3), ""A"")`, then the `SSettable` will be the element of `A` at index `(i1, i3)`. However, it avoids computing the address of each element from the indices from scratch in the inner loop. This was motivated by the need to generate operations on ndarrays in the local whitening aggregator. I replaced a few uses of `forEachIndex` with `coiterate`, which may give a performance boost since it avoids index math in the inner loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10583:549,variab,variable,549,https://hail.is,https://github.com/hail-is/hail/pull/10583,4,['variab'],"['variable', 'variables']"
Modifiability,Add convenience method to dummy code categorical variables,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14269:49,variab,variables,49,https://hail.is,https://github.com/hail-is/hail/pull/14269,1,['variab'],['variables']
Modifiability,Add domain to the deploy config.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9791:25,config,config,25,https://hail.is,https://github.com/hail-is/hail/pull/9791,1,['config'],['config']
Modifiability,Add rewrite rule to combine MatrixFilterEntries nodes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4548:4,rewrite,rewrite,4,https://hail.is,https://github.com/hail-is/hail/pull/4548,1,['rewrite'],['rewrite']
Modifiability,Add rewrite rule to optimize a common case of Literal array contains,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5660:4,rewrite,rewrite,4,https://hail.is,https://github.com/hail-is/hail/pull/5660,1,['rewrite'],['rewrite']
Modifiability,Add struct rewrite tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3675:11,rewrite,rewrite,11,https://hail.is,https://github.com/hail-is/hail/pull/3675,1,['rewrite'],['rewrite']
Modifiability,"Added a region argument, and mandated that users have either configured a region or are using `--region`. I believe older versions of gcloud assume the region is the default region for your project if you don't have anything specified, which causes users to have random errors when they update gcloud that they ask about on Zulip. This way, everyone gets a good error message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8281#issuecomment-597307671:61,config,configured,61,https://hail.is,https://github.com/hail-is/hail/pull/8281#issuecomment-597307671,1,['config'],['configured']
Modifiability,Added infrastructure for user-configured type printout.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2775:30,config,configured,30,https://hail.is,https://github.com/hail-is/hail/pull/2775,1,['config'],['configured']
Modifiability,Added required hail.vep.fasta VEP configuration property.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1435:34,config,configuration,34,https://hail.is,https://github.com/hail-is/hail/pull/1435,1,['config'],['configuration']
Modifiability,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:759,rewrite,rewrite,759,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516,4,['rewrite'],['rewrite']
Modifiability,"Adding a new compiler pass (lowering MatrixIR to TableIR) exposed a problem in Simplify. The logic for preventing some simplifications from triggering if they would introduce non-determinism was broken, and fixing it required a pretty complete overhaul. Fortunately, I think it's now a lot simpler. Besides the rewrite of the high level Simplify architecture, I also:; * Changed `testRepartitioningSimplifyRules` to something that failed in the old version.; * Changed the `copy` signature on the IR hierarchy to be more precise (to avoid unnecessary coercions).; * Grouped the Simplify rules into IR, MatrixIR, and TableIR. After the reorganization, a couple of rule redundancies became evident.; * A couple of vals in PruneSuite required running the compiler. When I had a bug in Simplify, this was causing the test runner to fail before any tests were run, on class initialization of PruneSuite, which gives very little help in diagnosing the issue. I made them lazy vals to prevent this in the future.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4564:311,rewrite,rewrite,311,https://hail.is,https://github.com/hail-is/hail/pull/4564,1,['rewrite'],['rewrite']
Modifiability,Adding configuration flag to hailctl dataproc submit,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7586:7,config,configuration,7,https://hail.is,https://github.com/hail-is/hail/pull/7586,1,['config'],['configuration']
Modifiability,Adding doc for linreg in its current form. This doc will evolve as we add more features.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/198:57,evolve,evolve,57,https://hail.is,https://github.com/hail-is/hail/pull/198,1,['evolve'],['evolve']
Modifiability,Addressed comments. ; - Refactored to a separate module and added module-level tests. ; - Cleaned up TypeChecker interface to call recursively down,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1727#issuecomment-299293790:24,Refactor,Refactored,24,https://hail.is,https://github.com/hail-is/hail/pull/1727#issuecomment-299293790,1,['Refactor'],['Refactored']
Modifiability,"Addressing what we discussed in lowering meeting yesterday. I've refactored TableStage to accumulate a bunch of let bindings that can be wrapped around the IR you want to generate with `TableStage.wrapInBindings`. See `TableCollect` for an example of this. This definitely generates the IR we wanted, but I'd be very open to suggestions about how to make this more ergonomic to use (had to generate a lot of UIDs and Refs manually for `TableParallelize`). . cc @cseed @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8627:65,refactor,refactored,65,https://hail.is,https://github.com/hail-is/hail/pull/8627,1,['refactor'],['refactored']
Modifiability,"Adds `copy_spark_log_on_error` init configuration option. When true, driver logs are copied to the remote tmpdir if an error occurs. This is useful in support cases where users cannot copy logs off the dataproc server themselves as it has already shut down.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14447:36,config,configuration,36,https://hail.is,https://github.com/hail-is/hail/pull/14447,1,['config'],['configuration']
Modifiability,"Adds functionality for CI to track which namespaces in the cluster are currently active and which services they are running. See #12095 for more context on the motivation for this change. As an added bonus, this is one step toward more flexible internal namespace management, i.e. automatic cleanup of dev namespaces or keeping PR namespaces open for X days.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12093:236,flexible,flexible,236,https://hail.is,https://github.com/hail-is/hail/pull/12093,1,['flexible'],['flexible']
Modifiability,"Adds support for using mkl, which is needed for some of the added lapack methods. To use, install mkl, then set the environment variable `HAIL_MKL_PATH` to the directory containing the mkl dylib files. In particular, it should contain `libmkl_rt.dylib`. On my laptop, I installed mkl through conda with `conda install -c intel mkl`, then set `HAIL_MKL_PATH=~/opt/miniconda3/envs/hail/lib/`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10770:128,variab,variable,128,https://hail.is,https://github.com/hail-is/hail/pull/10770,1,['variab'],['variable']
Modifiability,Adds the `docker_root_image` to the global config in `build.py` so that #10107 can pass CI tests without manually redeploying CI.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10340:43,config,config,43,https://hail.is,https://github.com/hail-is/hail/pull/10340,1,['config'],['config']
Modifiability,"Adds the ability to rerun/retry queries from the nearest `CollectDistributedArray` (`CDA`) IR site. Computes a ""Semantic Hash"" of the top-level IR, which is split and shared among the various constituent `CDA` calls in a query. The `CDA` procedure looks in an execution cache for the results of each partition for that call and uses/updates the cache with successful partition computations. . The nature of the staged- lower and execute model means we don't know how many `CDA` calls that will be generated ahead of time. Thus we treat the ""Semantic Hash"" in a similar way to an RNG state variable and generate a key from the Semantic Hash every time every time we encounter a `CDA`. Since an `ExecutionContext` is re-used for multiple queries in tests while a `SemanticHash` is coupled to one query, the two were kept separate. To minimise the amount of manual state handling, the code was transformed to use a ""State"" monad (abstracted as `MonadLower`). Since the `ExecuteContext` is used nearly everywhere the semantic hash is required, the `ExecuteContext` was absorbed into the `MonadLower` interface. `Lower` is a simple, concrete instance of `MonadLower`, and is used to adapt statements into `MonadLower` expressions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13194:589,variab,variable,589,https://hail.is,https://github.com/hail-is/hail/pull/13194,2,"['adapt', 'variab']","['adapt', 'variable']"
Modifiability,"Adds the ability to rerun/retry queries from the nearest `CollectDistributedArray` (`CDA`) `IR` site. Computes a ""Semantic Hash"" of the top-level `IR` which is used to generate a key for the various constituent `CDA` calls in a query. The implementation for CDA, `BackendUtils.collectDArray`, uses that key to look into an the execution cache for the results of each partition for that call and uses/updates the cache with successful partition computations. The nature of the staged- lower and execute model means we don't know how many `CDA` calls that will be generated ahead of time. Thus we treat the ""Semantic Hash"" in a similar way to an RNG state variable and generate a key from the Semantic Hash every time every time we encounter a `CDA`. The execution cache is implemented on-top of a local or remote filesystem (configurable via the `HAIL_CACHE_DIR` environment variable). This defaults to `{tmpdir}/hail/{pip-version}`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12954:654,variab,variable,654,https://hail.is,https://github.com/hail-is/hail/pull/12954,3,"['config', 'variab']","['configurable', 'variable']"
Modifiability,"After some reading, I am still not sure what exactly the difference is between dummy coding and one-hot encoding. Suppose there is a categorical variable with $n$ categories. The [referenced Stack Exchange question](https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn) suggests that a one-hot encoding converts the categorical variable to $n$ indicator variables (one for each category) and that a dummy coding converts the categorical variable to $n-1$ indicator variables. With these definitions, the dummy coding is the one-hot encoding without one of the indicator variables. However, from the prototype implementation in this issue, the [scikit-learn one-hot encoder documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), and the [dummy variable Wikipedia article](https://en.m.wikipedia.org/wiki/Dummy_variable_(statistics)), I get the impression that dummy coding and one-hot encoding are synonyms and that there is no real distinction. Anyway, I would like to work on this issue. I will base my implementation on the prototype, and perhaps we can add a parameter to drop one of the indicator variables similar to what the [scikit-learn one-hot encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) has.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13601#issuecomment-1932779413:145,variab,variable,145,https://hail.is,https://github.com/hail-is/hail/issues/13601#issuecomment-1932779413,16,['variab'],"['variable', 'variables']"
Modifiability,"After spending a few hours digging through log4j and trying a bunch of approaches, I wasn't able to fix our current approach of adding appenders to the consoleLog after log4j has already been configured. Instead, we set up log4j in initial configuration to have the appenders we want. Also moved logging config from HailContext to backend, where it should be. . Storing the StringSocketAppender on the static object is definitely a bit funky, but it's being allocated inside log4j and I don't see a simpler way to store it for retrieval later.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12783:192,config,configured,192,https://hail.is,https://github.com/hail-is/hail/pull/12783,3,['config'],"['config', 'configuration', 'configured']"
Modifiability,"After talking to Cotton, I'm going to refactor the client code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6245#issuecomment-498401222:38,refactor,refactor,38,https://hail.is,https://github.com/hail-is/hail/pull/6245#issuecomment-498401222,1,['refactor'],['refactor']
Modifiability,"After the first commit, lots of unrelated type errors popped up from mypy. I think this is because I have the new mypy installed and it's actually catching more errors that were there all along. It might help to see the errors that it gave me (also visible in old CI builds of the PR:. ```; ci/github.py:508: error: Incompatible types in assignment (expression has type ""bool"", variable has type ""Optional[str]""); ci/github.py:551: error: Incompatible types in assignment (expression has type ""bool"", variable has type ""Optional[str]""); ci/github.py:554: error: Incompatible types in assignment (expression has type ""bool"", variable has type ""Optional[str]""); ci/github.py:574: error: Unsupported operand types for > (""int"" and ""None""); ci/github.py:574: note: Left operand is of type ""Optional[int]""; ci/github.py:575: error: Unsupported operand types for + (""None"" and ""int""); ci/github.py:575: note: Left operand is of type ""Optional[int]""; ci/github.py:817: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:828: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:840: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:842: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:849: error: Item ""MergeFailureBatch"" of ""Union[Batch, Any, MergeFailureBatch]"" has no attribute ""id""; ci/github.py:849: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; Found 11 errors in 1 file (checked 19 source files); ```. It might be helpful to look at the first commit and last commit in isolation. Or if you'd like I can make a separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11530#issuecomment-1062214775:378,variab,variable,378,https://hail.is,https://github.com/hail-is/hail/pull/11530#issuecomment-1062214775,3,['variab'],['variable']
Modifiability,"After this, there are only 150ish warnings remaining, which shouldn't be too hard to fix by hand. Most are unused locals. Scalafix can delete unused locals, but I disable that, because there were too many cases where it left the rhs unnecessarily, e.g.; ```; ...; val idx = Symbol(genUID()); ...; ```; rewrites to; ```; ...; Symbol(genUID()); ...; ```; I'd rather just leave those as errors to be fixed manually. This is intended to replace #14103, which ended up mixing manual changes to fix warnings with scalafix changes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14156:302,rewrite,rewrites,302,https://hail.is,https://github.com/hail-is/hail/pull/14156,1,['rewrite'],['rewrites']
Modifiability,"Agreed on plan with @astheeggeggs to use this branch for his immediate needs, while pulling in pieces of this code in new PR once 0.1 and refactoring of RegressionUtils is done.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1709#issuecomment-298405321:138,refactor,refactoring,138,https://hail.is,https://github.com/hail-is/hail/pull/1709#issuecomment-298405321,1,['refactor'],['refactoring']
Modifiability,"Agreed that `hailctl init` is maybe the wrong name b/c `hailctl` also controls Dataproc. `hailctl batch init` seems right to me. I agree that `hailctl config init` seems wrong if we're creating buckets. On the issue of login, I think we can appease both points of view. If you're not logged in, have `hailctl batch init` do this:; ```; You are not currently logged in to Hail. Redirecting you to a login screen. ... user does login flow ... In the future, you can use hailctl auth login to login to Hail. ... continue with hailctl batch init ...; ```. I think we should punt on addressing domain. Broad users don't need to interact with it at all. What are the other concerns about how this is developing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1662837421:151,config,config,151,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1662837421,1,['config'],['config']
Modifiability,"Ah there is some upcast that happens before we get to case class TableUnion:. ```scala; case class TableUnion(children: IndexedSeq[TableIR]) extends TableIR {; assert(children.forall(c => c.typ.rowType == children.head.typ.rowType)); println(""ALL SAME""); ```. If this is added, ""ALL SAME"" gets printed 16 times after ""RESULT"". Not sure why that isn't 8 (maybe due to a lowering pass causing a copy?) or 24, but that's less important. edit: Nope, these tables must be interpreted as being of both non-missing types in the ""maybeNull"" column by the import function, or I'm specifying the type wrong:. ```python; def union(...):; left_key = self.key.dtype; for i, ht, in enumerate(tables):; if left_key != ht.key.dtype:; raise ValueError(...); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8008#issuecomment-580777705:141,extend,extends,141,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-580777705,1,['extend'],['extends']
Modifiability,"Ah yeah good point I forgot about that. You have to construct a string to avoid truncation a la:; ```; (base) dking@wm28c-761 /tmp % cat foo.py; def test():; assert False, 'b' * 1000; =========================================== test session starts ============================================; (base) dking@wm28c-761 /tmp % pytest foo.py; platform darwin -- Python 3.10.9, pytest-7.4.3, pluggy-1.3.0; rootdir: /private/tmp; configfile: pytest.ini; plugins: xdist-2.5.0, timeout-2.2.0, instafail-0.5.0, devtools-0.12.2, asyncio-0.21.1, timestamper-0.0.9, metadata-3.0.0, html-1.22.1, anyio-4.2.0, forked-1.6.0, accept-0.1.9, image-diff-0.0.11; asyncio: mode=strict; collected 1 item . foo.py F [100%]. ================================================= FAILURES =================================================; ___________________________________________________ test ___________________________________________________. def test():; > assert False, 'b' * 1000; E AssertionError: bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb; E assert False. foo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14151#issuecomment-1889800019:424,config,configfile,424,https://hail.is,https://github.com/hail-is/hail/pull/14151#issuecomment-1889800019,2,"['config', 'plugin']","['configfile', 'plugins']"
Modifiability,"Ah, I deleted the global-config as a part of switching to gcp-broad. Regardless, we'll need to address that as terraform is iteratively improved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12964#issuecomment-1531832089:25,config,config,25,https://hail.is,https://github.com/hail-is/hail/pull/12964#issuecomment-1531832089,1,['config'],['config']
Modifiability,"Ah, I see. If those paths point to the same location then it shouldn't make any difference. This error almost certainly means that `pyspark` cannot find your hail jar. I suspect that Spark 2.2.x has dropped support for the `SPARK_CLASSPATH` environment variable. Can you try starting `pyspark` with these options:; ```; pyspark \; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --conf=spark.driver.extraClassPath=$HAIL_HOME/build/libs/hail-all-spark.jar \; --conf=spark.executor.extraClassPath=./hail-all-spark.jar. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337639898:253,variab,variable,253,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337639898,1,['variab'],['variable']
Modifiability,"Ah, I'm actually wrong, it was defined in the Makefile itself originally. We do have a `TOKEN` variable defined in `config.mk` but `config.mk` is only used by services Makefiles. I restored the definition of TOKEN into `upload_qob_jar.sh`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12825#issuecomment-1489163821:95,variab,variable,95,https://hail.is,https://github.com/hail-is/hail/pull/12825#issuecomment-1489163821,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"Ah, OK, this is actually quite sensible. I have to tell gradle that this is a test jar, I do that by saying I want my class path to look like the test class path at runtime. A wrinkle is that I have to explicitly request our own code too (that's the first diff line). I'm actually quite pleased that our grade file has become a bit more standard and less custom.; ```diff; task shadowTestJar(type: ShadowJar) {; archiveClassifier = 'spark-test'; + from sourceSets.test.output; + configurations = [project.configurations.testRuntimeClasspath]; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1710347695:479,config,configurations,479,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1710347695,2,['config'],['configurations']
Modifiability,"Ah, thanks! This is just the gear => config rename now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9396#issuecomment-685904525:37,config,config,37,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685904525,1,['config'],['config']
Modifiability,"Ah, yeah, I can't use it as a feature yet: the running CI isn't creating global-config. Fixing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9777#issuecomment-738459457:80,config,config,80,https://hail.is,https://github.com/hail-is/hail/pull/9777#issuecomment-738459457,1,['config'],['config']
Modifiability,"Ah, yes that's right, I'll add that to the config.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10235#issuecomment-812185045:43,config,config,43,https://hail.is,https://github.com/hail-is/hail/pull/10235#issuecomment-812185045,1,['config'],['config']
Modifiability,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:333,rewrite,rewrite,333,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,2,['rewrite'],['rewrite']
Modifiability,"Ah, you must have a file at `~/.hail/.zuliprc`. This should really live inside hail-is, right? I've made it a SOPS encoded file, placed it in hail-is, and imported the secret. This is what it was before:; ```; resource ""kubernetes_secret"" ""zulip_config"" {; count = fileexists(""~/.hail/.zuliprc"") ? 1 : 0; metadata {; name = ""zulip-config""; }. data = {; "".zuliprc"" = fileexists(""~/.hail/.zuliprc"") ? file(""~/.hail/.zuliprc"") : """"; }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12991#issuecomment-1564546844:331,config,config,331,https://hail.is,https://github.com/hail-is/hail/pull/12991#issuecomment-1564546844,1,['config'],['config']
Modifiability,"Ajupyter%2Fnotebook+involves%3Ameeseeksmachine+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​meeseeksmachine</code></a></p>; <!-- raw HTML omitted -->; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jupyter/notebook/commit/80e992e9f4cfa6cc1fcf4d84cebe09d53e769790""><code>80e992e</code></a> Publish 7.0.7</li>; <li><a href=""https://github.com/jupyter/notebook/commit/089c78c48fd00b2b0d2f33e4463eb42018e86803""><code>089c78c</code></a> Update to JupyterLab 4.0.11 (<a href=""https://redirect.github.com/jupyter/notebook/issues/7215"">#7215</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/109ba7578886283ad7be54d189904132bd7bb6f0""><code>109ba75</code></a> Backport PR <a href=""https://redirect.github.com/jupyter/notebook/issues/7176"">#7176</a>: Update publish-release workflow for PyPI trusted publisher...</li>; <li><a href=""https://github.com/jupyter/notebook/commit/d252423198e3bce218fd4c370a706f373dcb4c78""><code>d252423</code></a> Update ruff config and typing (<a href=""https://redirect.github.com/jupyter/notebook/issues/7145"">#7145</a>) (<a href=""https://redirect.github.com/jupyter/notebook/issues/7186"">#7186</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/d2ef92f0b385b7ecd11cbf0f3af181ee8e494623""><code>d2ef92f</code></a> Backport PR <a href=""https://redirect.github.com/jupyter/notebook/issues/7142"">#7142</a>: Clean up lint handling (<a href=""https://redirect.github.com/jupyter/notebook/issues/7185"">#7185</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/8e9390d9af903f34bb1c8414c7e9b49d2fdec32f""><code>8e9390d</code></a> Backport PR <a href=""https://redirect.github.com/jupyter/notebook/issues/7132"">#7132</a>: Adopt ruff format (<a href=""https://redirect.github.com/jupyter/notebook/issues/7184"">#7184</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/4d07f1ee9b6d3dca2736e2bf3a1254451add8259""><code>4d07f1e</code></a> Install stable JupyterLab 4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:6653,config,config,6653,https://hail.is,https://github.com/hail-is/hail/pull/14182,1,['config'],['config']
Modifiability,"All of these things I completely agree with!. 1. I was lazy and used the Jinja template engine to parse and find the variable declarations. I need to write a custom parser, but wanted to figure out exactly what we're going to support. Which makes me worried that I don't want to implement an expr language or it should be minimal. 2. Tim suggested something similar: `%%IN bfile%%` and `%%OUT ofile%%. Requires the custom parser. See comment 1 above. 3. I was also concerned about the formatting of arrays. I tried using lambdas for comment 4 and it got complicated. I like your proposal but want to think about it more. 4. PLINK, etc. output lots of files and you specify the file root and then it outputs a bunch of files with different extensions. We must be able to support this and make it easy for users. I agree with your suggestion. I'll try that in the example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-446742931:117,variab,variable,117,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-446742931,1,['variab'],['variable']
Modifiability,"All that messy state twiddling is because Scala's `Iterator` is the wrong model for most things we use it for, which is why I made `FlipbookIterator`. Using that, what you have would become; ```scala; private class BgenRecordStateMachine(; ctx: RVDContext,; p: BgenPartition,; settings: BgenSettings; ) extends StateMachine[RegionValue] {; private[this] val bfis = p.makeInputStream; private[this] val rv = RegionValue(ctx.region); private[this] val rvb = ctx.rvb; ; def isValid: Boolean = p.isValid; def value: RegionValue = rv; def advance() { p.advance(); findNextVariant() }; private def findNextVariant() {; // same as existing advance(), but without advancing p; }. findNextVariant() // make sure iterator is initialized in first valid state; }; ```; giving `BgenPartition` a `FlipbookIterator` style interface, with `isValid`, `value`, and `advance()` instead of `hasNext()` and `next()`. Then to create a new iterator `FlipbookIterator(new BgenRecordStateMachine(...))`. But honestly, what you had was clear enough, so if you benchmarked and the allocation isn't an issue, you should do whatever you find most readable. I've been conditioned to avoid `Option` in low-level code, but I don't have a good intuition for when it is or isn't actually a problem.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3893#issuecomment-404156507:303,extend,extends,303,https://hail.is,https://github.com/hail-is/hail/pull/3893#issuecomment-404156507,2,['extend'],['extends']
Modifiability,"All the args for each genotype and struct are converted to strings with repr and stored. For certain things like genotype there's no reason to have it extend history_mixin, since we can produce a sensible `repr` without recording the args.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2209#issuecomment-328143714:151,extend,extend,151,https://hail.is,https://github.com/hail-is/hail/pull/2209#issuecomment-328143714,1,['extend'],['extend']
Modifiability,All these classes inherit from `Resource` which is an `abc.ABC`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13541:18,inherit,inherit,18,https://hail.is,https://github.com/hail-is/hail/pull/13541,1,['inherit'],['inherit']
Modifiability,"Almost every call to these methods was just casting the values to codes anyway, and using a list of values makes more sense. In the future, we are almost definitely going to want to change this so the method just directly takes in a `PTupleValue` so that we aren't making variables per dimension of the ndarray all the time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9302:272,variab,variables,272,https://hail.is,https://github.com/hail-is/hail/pull/9302,1,['variab'],['variables']
Modifiability,"Already done:; - Made a public bucket in us-central1 with requester pays on (gs://qob-vep-grch37-us-central1); - Uploaded the configuration file for grch37 in us-central1 to gs://hail-common/qob-vep/; - Documentation. To-Do items:; - Mirror the loftee base image in our artifact registry; - Replicate the data in Azure and create an Azure configuration file (can't make this requester pays, so not sure what to do here); - Add instructions for setting up a config file in the respective infra READMEs; - Write tests; - Get GRCh38 working; - Modify the cloud run functions for ACR cleanup to cleanup the vep-grch37 and eventually vep-grch38 images",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428:126,config,configuration,126,https://hail.is,https://github.com/hail-is/hail/pull/12428,3,['config'],"['config', 'configuration']"
Modifiability,"Alright, I snagged the PR namespace from the CI:. ```; pr-13135-default-u5tt5011yt5w; ```. Then I went to the Azure [Log Analytics workspace haildev-logs](https://portal.azure.com/#@haildev.onmicrosoft.com/resource/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/haildev/providers/Microsoft.OperationalInsights/workspaces/haildev-logs/logs). I went to ""Queries"", selected ""DK's AKS Pod Logs"", modified the namespace to the aforementioned one, and added a filter for ""hail-az://"". ```; let startTimestamp = ago(2h);; KubePodInventory; | where TimeGenerated > startTimestamp; | extend PodName=Name; | where Namespace == ""pr-13135-default-u5tt5011yt5w"" and PodName startswith ""batch-driver""; | distinct ContainerID, PodName, Namespace; | join (; ContainerLog; | where TimeGenerated > startTimestamp; ) on ContainerID; | project TimeGenerated, message=parse_json(LogEntry).message, LogEntry=parse_json(LogEntry); | where message contains ""hail-az://""; | order by TimeGenerated desc; ```. That revealed the batch logs path:. ```; EXAMPLE BATCH_JOB_LOGS_PATH hail-az://haildevtest/test/batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1/1/abc123/main/log; ```. In the [failing PR test job logs](https://ci.azure.hail.is/batches/3956877/jobs/152), I found the batch id:. ```; [2023-06-09 12:43:34] test/hail/methods/test_impex.py::BGENTests::test_import_bgen_row_fields; -------------------------------- live log call ---------------------------------; INFO batch_client.aioclient:aioclient.py:753 created batch 1148. INFO batch_client.aioclient:aioclient.py:770 updated batch 1148. FAILED; ```. I listed the job logs:. ```; (base) dking@wm28c-761 hail % az storage blob list --account-name haildevtest --container test --prefix batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/ -o table; Name Blob Type Blob Tier Length Content Type Last Modified Snapshot; ----------------------------------------------------------------------------- ----------- ----------- -------- ------------------------ ---------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:594,extend,extend,594,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['extend'],['extend']
Modifiability,"Alright, so the goal of this PR is to make this work:. ```; HAIL_QUERY_BACKEND=service \; python3 -c 'import hail as hl; hl.utils.range_table(10).write(""gs://foo/bar.t"")`; ```. In particular, the Hail Query JAR is stored in a well known location. If we know the desired git revision, (we do, it should be the same git revision as our wheel), then we can deduce the JAR URL for the user. Moreover, if we're pointed at a namespace, we can still determine the correct location [1]. This PR provides three escape hatches to this behavior:; 1. Specify the `jar_url` parameter to `ServiceBackend`.; 2. Specify the `HAIL_JAR_URL` environment variable.; 3. Specify a JAR url in the user config: `hailctl config set query/jar_url gs://...`. This PR is unfortunately entangled with one other minor change. In `main`, we send the git revision *and* the JAR URL to the driver and the worker as a part of the ""command string"" (the JVMEntryway passes this array of strings to the `main` method of `ServiceBackendSocketAPI2` or `Worker`. After this change, the backend does not necessarily know the git revision. That's fine. The git revision was only ever used as:; 1. a cache key for the JAR cache, and; 2. a unique name for the JAR; Both of these uses are buggy anyway! If you re-use a HAIL_SHA with a different HAIL_JAR_URL and you land on a worker that previously pulled that HAIL_SHA, you'll get the previously pulled JAR, not the newly specified one. Instead I use the JAR URL directly as a cache key and unique name. ---. [1] Odds are good that the developer has not uploaded a JAR to this location, but they can do so by dev deploying `upload_query_jar` or by running `make -C query push-jar`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11643:635,variab,variable,635,https://hail.is,https://github.com/hail-is/hail/pull/11643,3,"['config', 'variab']","['config', 'variable']"
Modifiability,Also @cseed @jigold your thoughts on this small refactor appreciated if you have time.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3783#issuecomment-398382857:48,refactor,refactor,48,https://hail.is,https://github.com/hail-is/hail/pull/3783#issuecomment-398382857,1,['refactor'],['refactor']
Modifiability,Also `WatchedBranch.config`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6142#issuecomment-494491481:20,config,config,20,https://hail.is,https://github.com/hail-is/hail/issues/6142#issuecomment-494491481,1,['config'],['config']
Modifiability,Also could someone please make sure this works on macOS and there aren't any dumb linux/glibc portability issues. I tried to keep it pretty POSIX-y.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6927#issuecomment-524446990:94,portab,portability,94,https://hail.is,https://github.com/hail-is/hail/pull/6927#issuecomment-524446990,1,['portab'],['portability']
Modifiability,"Also may need to add back annotate_global_expr. Either way include these examples which used to be un the FAQ:. **How do I access an annotation name with white-space in the Hail Expression Language?**. Put the annotation name in back ticks. ```; annotateglobal expr -c 'global.`my variable` = global.`lof count`'; ```. **How do I count the number of samples matching a phenotype annotation?**. ```; annotateglobal expr -c '; global.nMales = samples.count(sa.pheno.sex == ""Male""),; global.nFemales = samples.count(sa.pheno.sex == ""Female""),; global.nSamples = samples.count(true)'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1349:281,variab,variable,281,https://hail.is,https://github.com/hail-is/hail/issues/1349,1,['variab'],['variable']
Modifiability,Also refactor BGEN ptype logic. stacked on #7941,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7943:5,refactor,refactor,5,https://hail.is,https://github.com/hail-is/hail/pull/7943,1,['refactor'],['refactor']
Modifiability,"Also, I created `gs://hail-common/vep/vep/GRCh37`, `gs://hail-common/vep/vep/GRCh38`; directories with VEP configs and loftee data files, so you can now run ; ```; gcloud dataproc clusters create $CLUSTER; ...; --initialization-actions gs://hail-common/hail-init.sh,gs://hail-common/vep/vep/GRCh37/vep85-GRCh37-init.sh. or . --initialization-actions gs://hail-common/hail-init.sh,gs://hail-common/vep/vep/GRCh38/vep85-GRCh38-init.sh; ```; along with ; ```; gs://hail-common/vep/vep/GRCh37/vep85-GRCh37-gcloud.properties. or . gs://hail-common/vep/vep/GRCh38/vep85-GRCh38-gcloud.properties; ```. though the init.sh script ties the cluster to a particular genome build. . Also, it would be nice if hail could throw an error if trying to annotate a GRCh37 callset with GRCh38 VEP, etc. Would it make sense to put this check in the VEP command?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1779#issuecomment-299721946:107,config,configs,107,https://hail.is,https://github.com/hail-is/hail/pull/1779#issuecomment-299721946,1,['config'],['configs']
Modifiability,"Also, added hail.vep.extra_plugins for specifying additional plugins beyond the default ones (such as LoF_splice.pm for predicting variants' probability of splice junction disruption or creation)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1712:61,plugin,plugins,61,https://hail.is,https://github.com/hail-is/hail/pull/1712,1,['plugin'],['plugins']
Modifiability,"Also, wrt the `hail` alias, that only sets the environment variable for that single execution of `python`. You will need to run:; ```bash; export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python; ```; before running `./gradlew test`, otherwise it's very likely that you will see a variety of errors related to Spark. I am surprised that you saw an error about Breeze natives. An inappropriate `$PYTHON_PATH` should trigger a failure much earlier than the section of code that uses of Breeze natives.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281862423:59,variab,variable,59,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281862423,1,['variab'],['variable']
Modifiability,"Also, yes, the `registerNumeric` would require some sort of polymorphic implementation, which this is not.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1196#issuecomment-267986559:60,polymorphi,polymorphic,60,https://hail.is,https://github.com/hail-is/hail/pull/1196#issuecomment-267986559,1,['polymorphi'],['polymorphic']
Modifiability,Also... - fix inheriting from str so `join` will work; - fix Makefile for checking whether Batch files have been updated; - fix copying same input multiple times in LocalBackend,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5483:14,inherit,inheriting,14,https://hail.is,https://github.com/hail-is/hail/pull/5483,1,['inherit'],['inheriting']
Modifiability,An unfortunately large set of changes to enable `writeRows` on a `ContextRDD`. I took the chance to do a wee bit of refactoring between the write methods of `UnpartitionedRVD` and `OrderedRVD`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3311:116,refactor,refactoring,116,https://hail.is,https://github.com/hail-is/hail/pull/3311,1,['refactor'],['refactoring']
Modifiability,"And `Batch`. Again, no code changes here. That's all for today. Monday I'll do one last refactor before I'm ready to start merging the DAG functionality.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4798:88,refactor,refactor,88,https://hail.is,https://github.com/hail-is/hail/pull/4798,1,['refactor'],['refactor']
Modifiability,And a typical interaction for a current 2.0.2 user:. ```bash; dking@wmb16-359 # gradle compileScala . FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.413 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); ; using default version: 2.0.2; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613#issuecomment-290201637:353,config,configure,353,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201637,3,"['Config', 'config']","['Configuring', 'configure']"
Modifiability,"And this is on a laptop with an SSD, right? it'll be even worse on the cloud, I think. Should we parameterize this behavior?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6291#issuecomment-500455081:97,parameteriz,parameterize,97,https://hail.is,https://github.com/hail-is/hail/pull/6291#issuecomment-500455081,1,['parameteriz'],['parameterize']
Modifiability,"And use internally. This adds the bucket parameter/config setting, but doesn't require it, and falls back to getting the bucket from the user information. After this is released, I'll rip out the user bucket and make this mandatory.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8852:51,config,config,51,https://hail.is,https://github.com/hail-is/hail/pull/8852,1,['config'],['config']
Modifiability,Another small effort toward getting `gs://` out of our code. I'll follow up with a PR that puts the prod CI bucket into terraform. The `test_storage_uri` field of the global config was introduced in #11014,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11025:174,config,config,174,https://hail.is,https://github.com/hail-is/hail/pull/11025,1,['config'],['config']
Modifiability,Another thing to explore is how much the block matrix write is spending in compression. That might not be helping out overall. It might be worth modifying BlockMatrix to make the compression optional (and add some features from Matrix/Table will make the file format a bit more flexible.),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3335#issuecomment-385312301:278,flexible,flexible,278,https://hail.is,https://github.com/hail-is/hail/pull/3335#issuecomment-385312301,1,['flexible'],['flexible']
Modifiability,"Another try at #10796. It's all the same excpet I've fixed the `test_cant_submit_to_default_with_other_ns_creds` test, which had been wiping the `deploy-config.json` after trying to read AND write to it. Now the test tries to change the `default_namespace` to `""default""`. This should succeed in the default namespace but is expected to fail in other namespaces.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10854:153,config,config,153,https://hail.is,https://github.com/hail-is/hail/pull/10854,1,['config'],['config']
Modifiability,"Apologies, this spiraled into a big PR. Hopefully the large set of changes actually improves everyone's understanding of this. ---. The goal of this PR is to make this work:. ```; HAIL_QUERY_BACKEND=service \; python3 -c 'import hail as hl; hl.utils.range_table(10).write(""gs://foo/bar.t"")`; ```. In particular, a normal user should not need to know the location of a Hail Query JAR. Currently, you must specify two environment variables: `HAIL_SHA` and `HAIL_JAR_URL`. This PR takes advantage of the well known location of a Hail Query JAR [1]. We use the newly introduced `hl.revision()` to determine the SHA-1 of the currently installed Hail. This PR includes the revision in the driver job spec. The front end has been modified to convert the revision into a cloud storage URL. This PR also provides three escape hatches to the aforementioned default behavior. These escape hatches should more or less only be used by developers. They're specified from highest priority to lowest.; 1. Specify the `jar_url` parameter to `ServiceBackend`.; 2. Specify the `HAIL_JAR_URL` environment variable.; 3. Specify a JAR url in the user config: `hailctl config set query/jar_url gs://...`. While writing this PR, I decided to clean up five bits of cruft I left when I first built the service backend. First, I took the JAR URL out of the ""command"" of the job spec. This ""command"" is just an array of strings. The fact that certain parts of that array *must* be the JAR URL and the SHA-1 is confusing. Instead, there are now two keys in a JVM process specification:; 1. `jar_spec`, which may be either `{""type"": ""jar_url"", ""value"": ""gs://..../abc123....jar""}` or `{""type"":""git_revision"", ""value"": ""abc123...""}`.; 2. `argv`, an opaque list of strings which are passed, by the JVMEntryway, along with a few more args, to `is.hail.backend.service.Main`. The `Main` class dispatches to either `ServiceBackendSocketAPI2` or the `Worker` based on the first element of `argv`. Each class expects different contents in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11645:428,variab,variables,428,https://hail.is,https://github.com/hail-is/hail/pull/11645,1,['variab'],['variables']
Modifiability,"Applies the most restrictive bind and event propagation settings to job container mounts. While user jobs do not have the capabilities to create mount points, overlapping mount points in the container config can inadvertently trigger mount propagation back to the host which we just never want.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12960:201,config,config,201,https://hail.is,https://github.com/hail-is/hail/pull/12960,1,['config'],['config']
Modifiability,"Apt-get update does not use the retries parameter used by; apt-get install. In fact, I could not find any retry configuration; for apt-get update. This is a cheap hack that retries 5 times with; exponential back-off.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9722:112,config,configuration,112,https://hail.is,https://github.com/hail-is/hail/pull/9722,1,['config'],['configuration']
Modifiability,"Arcturus -- I'm assigning this to you, but please don't take off the WIP tag as merging this will cause Batch to shutdown (database migration). If we're not prepared for it, then it could cause an extended outage.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9441#issuecomment-691229759:197,extend,extended,197,https://hail.is,https://github.com/hail-is/hail/pull/9441#issuecomment-691229759,1,['extend'],['extended']
Modifiability,Are you running this locally?; ```; (cd hail/python && python3 -m mypy --config-file ../../setup.cfg hailtop); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11734#issuecomment-1090451328:73,config,config-file,73,https://hail.is,https://github.com/hail-is/hail/pull/11734#issuecomment-1090451328,1,['config'],['config-file']
Modifiability,Array.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$4(Emit.scala:644); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$4$adapted(Emit.scala:643); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$3(Emit.scala:643); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$3$adapted(Emit.scala:641); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:641); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:68); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:78); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:50); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.Lo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12531:7331,adapt,adapted,7331,https://hail.is,https://github.com/hail-is/hail/issues/12531,1,['adapt'],['adapted']
Modifiability,ArrayExpression.extend doesn't do type promotion in scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2691:16,extend,extend,16,https://hail.is,https://github.com/hail-is/hail/issues/2691,1,['extend'],['extend']
Modifiability,"ArrayFunctions:. - [ ] extend; - [ ] argF; - [ ] uniqueIndex; - [ ] ""[]"" with negative argument; - [ ] ""[*:]""; - [ ] ""[:*]""; - [ ] ""[*:*]"". DictFunctions:. - [ ] contains; - [ ] get. SetFunctions:. - [ ] contains. StringFunctions:. - [ ] ""[*:]""; - [ ] ""[:*]""; - [ ] ""[*:*]"". UtilFunctions:. - [ ] min; - [ ] max",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4766:23,extend,extend,23,https://hail.is,https://github.com/hail-is/hail/issues/4766,1,['extend'],['extend']
Modifiability,"As a step towards coralling the specification of the binding structure of the IR into one place, this rewrites `Bindings` to use only a single method of the `GenericBindingEnv` interface, `newBlock`, which therefore captures all possibilities of how a node can modify its parent's environment in a child. Later work refactors this to return an object encoding this modification, instead of returning a modified environment, which allows the caller complete flexibility in how to maintain an environment appropriately for their use case. This PR leaves in the old `Bindings` implementation, with an assertion that they agree. The PR stacked above this, #14495, deletes the old implementation. This way CI asserts that this refactoring hasn't changed any behavior.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14475:102,rewrite,rewrites,102,https://hail.is,https://github.com/hail-is/hail/pull/14475,3,"['refactor', 'rewrite']","['refactoring', 'refactors', 'rewrites']"
Modifiability,"As an aside, we should definitely have a `HAIL_BATCH_BACKEND` and associated config variables. There is no end to my annoyance that `hb.Batch()` gives me a local backend batch. It seems to me that, given the precedent of `hailctl dataproc submit`, `hailctl batch submit` conveys the intent to use QoB or Batch-in-Batch, not ""local mode Batch"" or ""local mode Hail"". It seems very reasonable to have a `--local-mode-query` override (I think we should ignore local mode Batch as much as possible since container-in-container is fraught). We need a better name for local mode Spark or Query. I'm slowly realizing that lots of people don't realize you can use Hail on a laptop. Are there other tools that have already settled on terminology here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12471#issuecomment-1324143394:77,config,config,77,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1324143394,2,"['config', 'variab']","['config', 'variables']"
Modifiability,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:49,config,config,49,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336,1,['config'],['config']
Modifiability,"As currently written, if `git clone` returns a non-zero exit code, the script; should exit immediately. I am not sure why this GnuTLS recv error (pasted below); does not trigger a non-zero exit code from git clone. This change both; explicitly echoes the exit code so we can be sure of our sanity and adds a check; that I'm confident will fail if no git repository was cloned (`git status`). ```; + date; Wed Apr 29 21:15:15 UTC 2020; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; + set -e; ++ mktemp -d; + dir=/tmp/tmp.5R5aJAlgEm; + git clone https://github.com/hail-is/hail.git /tmp/tmp.5R5aJAlgEm; Cloning into '/tmp/tmp.5R5aJAlgEm'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: early EOF; fatal: index-pack failed; ++ ls -A /tmp/tmp.5R5aJAlgEm. real	0m0.998s; user	0m0.008s; sys	0m0.017s; + git config user.email ci@hail.is; fatal: not in a git directory; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8667:930,config,config,930,https://hail.is,https://github.com/hail-is/hail/pull/8667,1,['config'],['config']
Modifiability,"As of #14056, there is ambiguity when referring to the configuration value `domain`. - In the `global-config`, this is the root domain of the entire hail system. This is the same across all namespaces.; - In the `deploy-config` of a namespace N, this refers to the root domain served by applications of that namespace. In production (namespace `default`), this is `hail.is`, the same as the root domain of the entire system. In other namespaces, this is `internal.hail.is`. Setting the `HAIL_DOMAIN` environment variable in the k8s deployments from the `global-config` overrides what should be `internal.hail.is` to `hail.is`, breaking any form of redirection. There's really no need to set this environment variable at all, as its value can be derived from the `deploy-config`. This PR removes that environment variable. I tested this in my development namespace.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14164:55,config,configuration,55,https://hail.is,https://github.com/hail-is/hail/pull/14164,8,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"As part of our work with generating All of Us datasets, we needed to copy around a million gcs objects. Our `Copier` infrastructure 'should' be able to handle that, but it kept falling with robustness issues. What finally worked was using GCS's [rewrite](https://cloud.google.com/storage/docs/json_api/v1/objects/rewrite) api. This allowed us to copy data without reading it, allowing the copies to complete in a fraction of the time while also reducing bandwidth needs. There are two components to this:; 1. Research what specific APIs we can take advantage of; 2. Update our code to use them when we can, for the `Copier`, and the new sync tool (#14248). Here's the code I used for making the rewrite requests for merging a set of matrix tables together, the progress bar code was for visibility. ```python3; async def rewrite(; gfs: GoogleStorageAsyncFS,; src: str,; dst: str,; progress: Optional[rich.progress.Progress] = None,; file_tid: Optional[rich.progress.TaskID] = None,; requests_tid: Optional[rich.progress.TaskID] = None,; ):; assert (progress is None) == (file_tid is None) == (requests_tid is None); src_bkt, src_name = gfs.get_bucket_and_name(src); dst_bkt, dst_name = gfs.get_bucket_and_name(dst); if not src_name:; raise IsABucketError(src); if not dst_name:; raise IsABucketError(dst); client = gfs._storage_client; path = (; f'/b/{src_bkt}/o/{urllib.parse.quote(src_name, safe="""")}/rewriteTo'; f'/b/{dst_bkt}/o/{urllib.parse.quote(dst_name, safe="""")}'; ); kwargs = {'json': '', 'params': {}}; client._update_params_with_user_project(kwargs, src_bkt); response = await retry_transient_errors(client.post, path, **kwargs); if progress is not None:; progress.update(requests_tid, advance=1); while not response['done']:; kwargs['params']['rewriteToken'] = response['rewriteToken']; response = await retry_transient_errors(client.post, path, **kwargs); if progress is not None:; progress.update(requests_tid, advance=1); if progress is not None:; progress.update(file_tid, advance=1)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14601:246,rewrite,rewrite,246,https://hail.is,https://github.com/hail-is/hail/issues/14601,7,['rewrite'],"['rewrite', 'rewriteTo', 'rewriteToken']"
Modifiability,"As stated in the GNU make manual, ""Recursive make commands should always; use the variable MAKE, not the explicit command name ‘make’"". https://www.gnu.org/software/make/manual/make.html#MAKE-Variable",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9525:82,variab,variable,82,https://hail.is,https://github.com/hail-is/hail/pull/9525,2,"['Variab', 'variab']","['Variable', 'variable']"
Modifiability,"Assigned John, I think @cseed is busy. John, this PR has 2 features:; 1) Remove catch-all server block, for service-specific blocks, in better keeping with router.nginx.conf. 2) Allow prefix matches only on the exact, slash-less url. Meaning /prometheusss$haxor doesn't work, but /prometheus does. This is most easily accomplished with an exact match location block, because by Nginx semantics, regex-containing locations cannot be elided with a root proxy_pass (one with a trailing /), because nginx wants a static prefix to remove, and regex prevents that. So to accomplish this with regex would require more LOC, namely a URL rewrite rule inside the location block.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-554498117:629,rewrite,rewrite,629,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-554498117,1,['rewrite'],['rewrite']
Modifiability,"Assigning @tpoterba since he (and cotton) have the most context to review this. A few preliminaries:. 1. I noticed the proxy headers were not quite right when you're testing this without SSL or on some non-standard port. `$host` does not include the port, `$http_host` does. `$scheme` returns `http` or `https` depending on how the user connected to gateway; 2. The admin privilege check was too restrictive, if `delete_worker_pod` is called by `/new` there's no need to check admin privs; 3. I realized that the timeout logic wasn't quite right because a misconfigured gateway (I was testing with a broken gateway config) will return 5xx codes, but that doesn't mean the server is alive. We probably should error here, but I'm hesitant to add new error modes so close to a tutorial. Ok, how does this work? Basically, if the gateway cannot connect to the notebook pod, we intercept the error and redirect the user to the ""create new notebook"" webpage. That webpage deletes whatever remains of the users previous notebook pod & service. Here are the pieces:. 1. `recursive_error_pages on;` the internet suggests that without this we cannot use `error_page` with an ""internal"" rule (the `@` rules are internal rules that users cannot directly access); 2. `proxy_connect_timeout` defaults to 60s which is a shit user experience if your pod dies. Honestly, I might set this to 100ms. This is all inside a datacenter.; 3. `proxy_intercept_errors` permits us to use `error_page` with 5xx errors from failing to connect to the proxy. ---. I tested this with a pile of hacks to deploy this into an anonymous namespace in `vdc`. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of `vdc/` and `gateway/` to be more modular.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4974:615,config,config,615,https://hail.is,https://github.com/hail-is/hail/pull/4974,1,['config'],['config']
Modifiability,"Assigning to Daniel 2 because the scorecard beacon is tired. This removes the workshop login option (previously agreed upon with Cotton), which makes the login.html page totally useless; so I've converted the login link to hit the old /login POST endpoint, and converted the POST to a GET. I think this is semantically fine, because no credentials (or other data) is actually sent to that endpoint (as workshop password is kaput), making that endpoint solely issue a redirect. Since login.html is gone, I also no longer redirect to it. Instead, unauthorized users are redirected to /error, and I refactored this redirect into a function since it's now used identically in 2 places. I've also imported the jwt library, so that jwt.exceptions.InvalidTokenError is in scope, and made some minor cleanup. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6078:596,refactor,refactored,596,https://hail.is,https://github.com/hail-is/hail/pull/6078,1,['refactor'],['refactored']
Modifiability,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826:54,parameteriz,parameterizations,54,https://hail.is,https://github.com/hail-is/hail/issues/7826,6,"['inherit', 'parameteriz']","['inheritors', 'parameterization', 'parameterizations']"
Modifiability,"At some point we started optimizing the MakeStruct to a SelectFields,; which is great, but not if it breaks important optimizations like the; avoid-a-shuffle rewrite rule!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7073:158,rewrite,rewrite,158,https://hail.is,https://github.com/hail-is/hail/pull/7073,1,['rewrite'],['rewrite']
Modifiability,"At this point, every job that needs the private network specifies; it (https://github.com/hail-is/hail/pull/9380). This change moves jobs with; unspecified `network` to the private network. I removed the gce-deploy-config because jobs should not, by default, assume they; are on the private network. The GCE Deploy Config instructs you to enter GKE; using the internal-gateway, which is on our private network.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9744:215,config,config,215,https://hail.is,https://github.com/hail-is/hail/pull/9744,4,"['Config', 'config']","['Config', 'config']"
Modifiability,Awesome InsFields rewrite rules,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4374:18,rewrite,rewrite,18,https://hail.is,https://github.com/hail-is/hail/pull/4374,1,['rewrite'],['rewrite']
Modifiability,"Azure already has a Jupyter system in place, so I worked within that. As a result, I took a very different approach from `hailctl dataproc`. I'm not sure how many of the configuration settings done in `hailctl dataproc` are necessary in Azure. I also do not plan to add special support for any special parameters from Azure. If a user wants to, for example, configure auto-scaling, they can use pass through arguments. There are three files that need to be hosted somewhere: two startup scripts and an Azure-specific wheel file. For the startup scripts, I just rely on GitHub tagged raw files. For the wheel file, I placed it in hail common and use the public HTTP endpoint. For development, you have to manually upload the files you want to override and invoke `hailctl hdinsight` like this:; ```; hailctl hdinsight; start \; clustername \; password \; password \; storageaccount \; --install-hail-uri https://raw.githubusercontent.com/danking/hail/dk-hdinsight-test/hail/python/hailtop/hailctl/hdinsight/resources/install-hail.sh \; --install-native-deps-uri https://raw.githubusercontent.com/danking/hail/dk-hdinsight-test/hail/python/hailtop/hailctl/hdinsight/resources/install-native-deps.sh \; --wheel-uri https://storage.googleapis.com/hail-common/dking/hail-0.2.79-py3-none-any.whl; ```; We could make this easier, but I'd rather spend that time on the query service. cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11180:170,config,configuration,170,https://hail.is,https://github.com/hail-is/hail/pull/11180,2,['config'],"['configuration', 'configure']"
Modifiability,"B .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:1987,variab,variables,1987,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935,2,['variab'],['variables']
Modifiability,"BLAS is actually installed in /usr/lib64/atlas. Spark was not finding the lib for some reason. ; ; The solution was to add the following config to the spark-submit command line. --conf spark.executor.extraClassPath=""/usr/lib64/atlas"" . It would be useful to add this to the documentation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7008#issuecomment-529740667:137,config,config,137,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529740667,1,['config'],['config']
Modifiability,"Based off of discussion in #11907, this aims to avoid separate PRs from clobbering the image cache tag and sets up PR-specific cache tags per image. Note that using `ci-intermediate` was also detrimental to the image cache and I don't think different images sharing layers under the common name holds much value. I think we should ultimately get rid of `ci-intermediate` entirely and explicitly name our images so that they don't ruin each other's caches. I tested this in my namespace's CI. Here's the image build times from two consecutive dev deploys:. Before | After; :-------------------------:|:-------------------------:; ![Screen Shot 2022-07-05 at 6 14 36 PM](https://user-images.githubusercontent.com/24440116/177426924-5d5ade8c-0cee-4a0e-b477-2156d4e01e78.png) | ![Screen Shot 2022-07-05 at 6 14 45 PM](https://user-images.githubusercontent.com/24440116/177426882-c0029760-42ae-471d-b48c-daa0eadea448.png). I don't personally see the need for adding more SHAs to the cache as mentioned in #11907, a per-PR cache seems like exactly what you would want. The one drawback I can think of here is that a deploy won't make use of the cache from the PR that resulted in the commit to main. I believe the commit SHAs would be different because we squash so other than devising a way to trace the commit back to the PR I don't see how we can easily connect the two. Still, I feel like it's not a big deal since it will still use the previously deployed commit as a cache, so most deploys will still be very fast and no one's waiting on deploys in the same way as we wait on PRs and dev deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11999:266,layers,layers,266,https://hail.is,https://github.com/hail-is/hail/pull/11999,1,['layers'],['layers']
Modifiability,"Based on #3822. Make sure you start looking at commits at `Keyed RV Aggregator`. Same PR as #3768, but rebased with some refactoring.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3824:121,refactor,refactoring,121,https://hail.is,https://github.com/hail-is/hail/pull/3824,1,['refactor'],['refactoring']
Modifiability,Based on #7681. I had to refactor `ParameterPack.newFields` a bit to get it to work with `ParameterPack.array`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8028:25,refactor,refactor,25,https://hail.is,https://github.com/hail-is/hail/pull/8028,1,['refactor'],['refactor']
Modifiability,"Basically all naming, rids these test files of linting errors. We do a lot of reassigning a `BatchBuilder` variable to a `Batch` and so I consolidated around `bb` and `b`. A couple instances where I remove debug_info from an assert statement is because the associated `Batch` object would not exist, since that assert is triggered by an error that's raised before the `Batch` object is created.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12147:107,variab,variable,107,https://hail.is,https://github.com/hail-is/hail/pull/12147,1,['variab'],['variable']
Modifiability,"Basically done, I just have to go through and rewrite the LAPACK calls to use the new interface (so far I've only done so with NDArrayInverse). If you have any thoughts on the actual reference counting @tpoterba, that can be reviewed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10001#issuecomment-775541037:46,rewrite,rewrite,46,https://hail.is,https://github.com/hail-is/hail/pull/10001#issuecomment-775541037,1,['rewrite'],['rewrite']
Modifiability,"Batch Client:; - Added new parameter max_idle_time (seconds); - Removed BatchBuilder and fused it with Batch; - Added 2 new operations:; - Commit ; - Close; - Kept submit which is the same as close for backwards compatibility reasons; - commit, close, and submit now return the batch so these methods can be chained together; - create_batch stayed the same. Tests:; - Added 3 new tests for new functionality; - Renamed a bunch of variables in the tests and cleaned up the variables. Database:; - Added time_last_updated for determining how long a batch has been idle; - Added max_idle_time; - Added a closed field and changed the possible states for a batch to created, running, and complete (removed open); - Changed close_batch to be commit_staged_jobs. Changes were made to make sure this worked even if there were 0 jobs to actually commit. ; - Changed cancel_batch to always set cancelled = 1 and closed = 1 regardless of whether the batch is actually running. The time_completed is only set if no jobs are currently running. Otherwise, the time_completed will be set in MJC. **It also commits any jobs that are pending before cancelling the batch.** I'm not sure if we want this behavior or not. Driver:; - Runs a loop every 60 seconds to close batches with max_idle_time greater than that specified. UI:; - The UI changed the batches table to be time_created instead of time_closed as the Submitted/Created column. The duration is the time from time_complete - time_created for newer batches instead of time_complete - time_closed.; - Added a close button; - An open batch (even one just in the created state) can be cancelled or closed. **This might be confusing**. Other:; - Cancel is idempotent; - Getting the batch state and time_completed correct was tricky.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10484:430,variab,variables,430,https://hail.is,https://github.com/hail-is/hail/pull/10484,2,['variab'],['variables']
Modifiability,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:78,config,configuration,78,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683,4,['config'],['configuration']
Modifiability,"Because, you know, that one is maternally inherited.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2005:42,inherit,inherited,42,https://hail.is,https://github.com/hail-is/hail/issues/2005,1,['inherit'],['inherited']
Modifiability,"Before change:. ```; {""config"": {""cores"": 1, ""version"": ""0.2.57-e20c00f05c78"", ""timestamp"": ""2020-09-24 08:27:24.863298"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""hwe_normalized_pca_blanczos_small_data_10_iterations"", ""failed"": false, ""timed_out"": false, ""times"": [54.736854666000006, 46.213391341000005, 52.75462794499998]}]}; ```. After change: . ```; {""config"": {""cores"": 1, ""version"": ""0.2.57-c013f70fe868"", ""timestamp"": ""2020-09-24 08:32:23.991129"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""hwe_normalized_pca_blanczos_small_data_10_iterations"", ""failed"": false, ""timed_out"": false, ""times"": [28.998368115000005, 40.65512770199999, 28.816323178000005]}]}; ```. Obvious improvement, nearly 2x.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9498#issuecomment-698315046:23,config,config,23,https://hail.is,https://github.com/hail-is/hail/pull/9498#issuecomment-698315046,2,['config'],['config']
Modifiability,"Before we can simplify the binding structure, we need to stop duplicating it all over the place. This PR rewrites `FreeVariables` so that it no longer needs special logic for particular nodes, hard coding binding structure (redundantly). To do this, it takes advantage of the new `Bindings`, which operates on a `GenericBindingEnv` interface. It adds a new implementation of this interface specifically for computing free variables, then simply does a generic traversal of the IR using this custom binging environment. While I find the new implementation far simpler and more obviously correct than the old, I do expect it to further simplify once I'm able to start modifying the core binding structure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14451:105,rewrite,rewrites,105,https://hail.is,https://github.com/hail-is/hail/pull/14451,2,"['rewrite', 'variab']","['rewrites', 'variables']"
Modifiability,"Before we merge this, I'd like to have the new Artifact Registry in hail-vdc setup and have configured a cloud run job for the cleanup script. I think a daily run is good enough. https://github.com/GoogleCloudPlatform/gcr-cleaner/blob/main/docs/deploy-cloud-run.md",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211#issuecomment-1255117035:92,config,configured,92,https://hail.is,https://github.com/hail-is/hail/pull/12211#issuecomment-1255117035,1,['config'],['configured']
Modifiability,"Ben came across an image in the wild with a null `Env` field in the manifest, which caused the following error:; ```; Error; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 868, in _run; timed_out = await self._run_until_done_or_deleted(self._run_container); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1010, in _run_until_done_or_deleted; return await run_until_done_or_deleted(self.deleted_event, f, *args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 680, in run_until_done_or_deleted; return step.result(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1066, in _run_container; await self._write_container_config(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1106, in _write_container_config; config = await self.container_config(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1166, in container_config; 'env': self._env(),; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1354, in _env; self.image.image_config['Config']['Env'] + self.env + CLOUD_WORKER_API.cloud_specific_env_vars_for_user_jobs; TypeError: unsupported operand type(s) for +: 'NoneType' and 'list'; ```. He fixed it by creating the following docker image:. ```docker; FROM jargene/hapice:1.0; ```. It could be that old versions of docker allowed this to be empty but have since made it `[]`, which would mean this would be unfortunately very annoying to test but nonetheless pretty trivial to fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13720:893,config,config,893,https://hail.is,https://github.com/hail-is/hail/pull/13720,2,"['Config', 'config']","['Config', 'config']"
Modifiability,"Better place to post things like this would be discuss.hail.is (because it's probably a configuration issue with your cluster and not a bug in hail). I'd guess you don't have BLAS installed on your cluster. If you check the hail.log file, do you have lines like. ```; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS; ```. ?. See here: https://hail.is/docs/0.2/getting_started.html#common-installation-issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7008#issuecomment-529008947:88,config,configuration,88,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529008947,1,['config'],['configuration']
Modifiability,"Bigger than I expected, but:; 1. Re-enable the FS tests and create a Gradle target for them so they can be run locally.; 2. Allow the FS tests to be easily used locally by not hardcoding a particular key file path.; 3. Skip GoogleStorageFSSuite when `CLOUD` is not `gcp`; 4. Remove irrelevant env vars from non-FS Scala tests.; 5. Eliminate the ""hail_repl"" image and deployment which was scoped dev anyway and never used.; 6. Add hail_pip_installed_image which can be used to execute `hailtop.aiotools.copy`.; 7. Use copy in two places in build.yaml.; 8. Add a command line argument for configuring the number of concurrent transfers which sets an upper bound on the number of open source files (and, additionally, open destination files). On my MacBook, I can't seem to open 100 local files simultaneously. I set the default low enough that local use should work by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11089:587,config,configuring,587,https://hail.is,https://github.com/hail-is/hail/pull/11089,1,['config'],['configuring']
Modifiability,"Black only supports the `pyproject.toml` configuration file. Between all our tools (mypy, flake8, black, pylint…), there's no single config file they all seem to support. I moved the black config from `.pre-commit-config.yaml` to `pyproject.toml` so black can pick up the configuration whenever it's run, so editor plugins to activate autoformatting can work by default now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10438:41,config,configuration,41,https://hail.is,https://github.com/hail-is/hail/pull/10438,6,"['config', 'plugin']","['config', 'configuration', 'plugins']"
Modifiability,BlockMatrix.write_from_entry_expr OOM without appropriate cluster config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8239:66,config,config,66,https://hail.is,https://github.com/hail-is/hail/issues/8239,1,['config'],['config']
Modifiability,Bootstrap render_config_mk: add missing variables,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11371:40,variab,variables,40,https://hail.is,https://github.com/hail-is/hail/pull/11371,1,['variab'],['variables']
Modifiability,BoxedArrayBuilder's type parameter needs to extend AnyRef to avoid; runtime matches on the type for all operations on the array.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10509:44,extend,extend,44,https://hail.is,https://github.com/hail-is/hail/pull/10509,1,['extend'],['extend']
Modifiability,Builds on #4094. Diff [here](https://github.com/patrick-schultz/hail/compare/RVD2-partitioner...patrick-schultz:RVD3-partitionKeys). * Remove partition keys from `OrderedRVDType` and `OrderedRVDPartitioner`; * Rewrite `TableKeyBy` to match specification in [design doc](https://docs.google.com/document/d/1Or3AeBvHB-6zDRKq0KHDBcKcoPedOxwlZMRdxWoblhE/edit?usp=sharing); * Add back explicit partition key arguments to `getKeyInfo` and `coerce` to keep support for local sorting path.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4119:210,Rewrite,Rewrite,210,https://hail.is,https://github.com/hail-is/hail/pull/4119,1,['Rewrite'],['Rewrite']
Modifiability,"Builds on: https://github.com/hail-is/hail/pull/4869. ExtendedOrdering on rows and containers now matches CodeOrdering by using lt instead of compare in lt, etc. FYI @tpoterba since this could potentially (e.g. comparing arrays with nans) cause a user-visible change in behavior.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4871:54,Extend,ExtendedOrdering,54,https://hail.is,https://github.com/hail-is/hail/pull/4871,1,['Extend'],['ExtendedOrdering']
Modifiability,"Builds on: https://github.com/hail-is/hail/pull/5004. Convert all operations in table.py to IR (if possible). Here are the things that remain in order to get rid of Table._jt in table.py. Rewrite in Python:; - expandTypes; - flatten; - collectJSON: use aggregate/collect (@tpoterba, do you feel this will be significantly slower now?); - showString: rewrite in Python in terms of collect. Add IR:; - intervalJoin; - same; - groupByKey. Should only work with SparkBackend:; - toDF. Hmm:; - forceCount: remove? add force option to TableCount that disables optimization?; - nPartitions; - filterPartitions; - persist, unpersist",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5015:188,Rewrite,Rewrite,188,https://hail.is,https://github.com/hail-is/hail/pull/5015,2,"['Rewrite', 'rewrite']","['Rewrite', 'rewrite']"
Modifiability,"Bumps [aiohttp](https://github.com/aio-libs/aiohttp) from 3.8.6 to 3.9.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/releases"">aiohttp's releases</a>.</em></p>; <blockquote>; <h2>3.9.0</h2>; <h2>Features</h2>; <ul>; <li>; <p>Introduced <code>AppKey</code> for static typing support of <code>Application</code> storage.; See <a href=""https://docs.aiohttp.org/en/stable/web_advanced.html#application-s-config"">https://docs.aiohttp.org/en/stable/web_advanced.html#application-s-config</a></p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/5864"">#5864</a>)</p>; </li>; <li>; <p>Added a graceful shutdown period which allows pending tasks to complete before the application's cleanup is called.; The period can be adjusted with the <code>shutdown_timeout</code> parameter. -- by :user:<code>Dreamsorcerer</code>.; See <a href=""https://docs.aiohttp.org/en/latest/web_advanced.html#graceful-shutdown"">https://docs.aiohttp.org/en/latest/web_advanced.html#graceful-shutdown</a></p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7188"">#7188</a>)</p>; </li>; <li>; <p>Added <code>handler_cancellation &lt;https://docs.aiohttp.org/en/stable/web_advanced.html#web-handler-cancellation&gt;</code>_ parameter to cancel web handler on client disconnection. -- by :user:<code>mosquito</code>; This (optionally) reintroduces a feature removed in a previous release.; Recommended for those looking for an extra level of protection against denial-of-service attacks.</p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7056"">#7056</a>)</p>; </li>; <li>; <p>Added support for setting response header parameters <code>max_line_size</code> and <code>max_field_size</code>.</p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/2304"">#2304</a>)</p>; </li>; <li>; <p>Added <code>auto_decompress</code> parameter to <code>ClientSession.request</code> to override <code>ClientSession._a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14027:468,config,config,468,https://hail.is,https://github.com/hail-is/hail/pull/14027,12,['config'],['config']
Modifiability,"Bumps [astroid](https://github.com/PyCQA/astroid) from 2.11.5 to 2.12.9.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/astroid/blob/main/ChangeLog"">astroid's changelog</a>.</em></p>; <blockquote>; <h1>What's New in astroid 2.12.9?</h1>; <p>Release date: 2022-09-07</p>; <ul>; <li>; <p>Fixed creation of the <code>__init__</code> of <code>dataclassess</code> with multiple inheritance.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7427"">PyCQA/pylint#7427</a></p>; </li>; <li>; <p>Fixed a crash on <code>namedtuples</code> that use <code>typename</code> to specify their name.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7429"">PyCQA/pylint#7429</a></p>; </li>; </ul>; <h1>What's New in astroid 2.12.8?</h1>; <p>Release date: 2022-09-06</p>; <ul>; <li>; <p>Fixed a crash in the <code>dataclass</code> brain for <code>InitVars</code> without subscript typing.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7422"">PyCQA/pylint#7422</a></p>; </li>; <li>; <p>Fixed parsing of default values in <code>dataclass</code> attributes.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7425"">PyCQA/pylint#7425</a></p>; </li>; </ul>; <h1>What's New in astroid 2.12.7?</h1>; <p>Release date: 2022-09-06</p>; <ul>; <li>; <p>Fixed a crash in the <code>dataclass</code> brain for uninferable bases.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7418"">PyCQA/pylint#7418</a></p>; </li>; </ul>; <h1>What's New in astroid 2.12.6?</h1>; <p>Release date: 2022-09-05</p>; <ul>; <li>; <p>Fix a crash involving <code>Uninferable</code> arguments to <code>namedtuple()</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7375"">PyCQA/pylint#7375</a></p>; </li>; <li>; <p>The <code>dataclass</code> brain now understands the <code>kw_only</code> keyword in dat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12161:422,inherit,inheritance,422,https://hail.is,https://github.com/hail-is/hail/pull/12161,1,['inherit'],['inheritance']
Modifiability,"Bumps [black](https://github.com/psf/black) from 22.1.0 to 22.3.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/psf/black/releases"">black's releases</a>.</em></p>; <blockquote>; <h2>22.3.0</h2>; <h3>Preview style</h3>; <ul>; <li>Code cell separators <code>#%%</code> are now standardised to <code># %%</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2919"">#2919</a>)</li>; <li>Remove unnecessary parentheses from <code>except</code> statements (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2939"">#2939</a>)</li>; <li>Remove unnecessary parentheses from tuple unpacking in <code>for</code> loops (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2945"">#2945</a>)</li>; <li>Avoid magic-trailing-comma in single-element subscripts (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2942"">#2942</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not format <code>__pypackages__</code> directories by default (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2836"">#2836</a>)</li>; <li>Add support for specifying stable version with <code>--required-version</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2832"">#2832</a>).</li>; <li>Avoid crashing when the user has no homedir (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2814"">#2814</a>)</li>; <li>Avoid crashing when md5 is not available (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2905"">#2905</a>)</li>; <li>Fix handling of directory junctions on Windows (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2904"">#2904</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Update pylint config documentation (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2931"">#2931</a>)</li>; </ul>; <h3>Integrations</h3>; <ul>; <li>Move test to disable plugin in Vim/Neovim, which speeds up loading (<a href=""https://githu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11696:938,Config,Configuration,938,https://hail.is,https://github.com/hail-is/hail/pull/11696,1,['Config'],['Configuration']
Modifiability,"Bumps [boto3](https://github.com/boto/boto3) from 1.26.6 to 1.26.9.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/boto/boto3/blob/develop/CHANGELOG.rst"">boto3's changelog</a>.</em></p>; <blockquote>; <h1>1.26.9</h1>; <ul>; <li>api-change:<code>customer-profiles</code>: [<code>botocore</code>] This release enhances the SearchProfiles API by providing functionality to search for profiles using multiple keys and logical operators.</li>; <li>api-change:<code>lakeformation</code>: [<code>botocore</code>] This release adds a new parameter &quot;Parameters&quot; in the DataLakeSettings.</li>; <li>api-change:<code>managedblockchain</code>: [<code>botocore</code>] Updating the API docs data type: NetworkEthereumAttributes, and the operations DeleteNode, and CreateNode to also include the supported Goerli network.</li>; <li>api-change:<code>proton</code>: [<code>botocore</code>] Add support for CodeBuild Provisioning</li>; <li>api-change:<code>rds</code>: [<code>botocore</code>] This release adds support for restoring an RDS Multi-AZ DB cluster snapshot to a Single-AZ deployment or a Multi-AZ DB instance deployment.</li>; <li>api-change:<code>workdocs</code>: [<code>botocore</code>] Added 2 new document related operations, DeleteDocumentVersion and RestoreDocumentVersions.</li>; <li>api-change:<code>xray</code>: [<code>botocore</code>] This release enhances GetServiceGraph API to support new type of edge to represent links between SQS and Lambda in event-driven applications.</li>; </ul>; <h1>1.26.8</h1>; <ul>; <li>api-change:<code>glue</code>: [<code>botocore</code>] Added links related to enabling job bookmarks.</li>; <li>api-change:<code>iot</code>: [<code>botocore</code>] This release add new api listRelatedResourcesForAuditFinding and new member type IssuerCertificates for Iot device device defender Audit.</li>; <li>api-change:<code>license-manager</code>: [<code>botocore</code>] AWS License Manager now supports onboarded Managem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12466:351,enhance,enhances,351,https://hail.is,https://github.com/hail-is/hail/pull/12466,1,['enhance'],['enhances']
Modifiability,"Bumps [boto3](https://github.com/boto/boto3) from 1.26.7 to 1.26.16.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/boto/boto3/blob/develop/CHANGELOG.rst"">boto3's changelog</a>.</em></p>; <blockquote>; <h1>1.26.16</h1>; <ul>; <li>api-change:<code>grafana</code>: [<code>botocore</code>] This release includes support for configuring a Grafana workspace to connect to a datasource within a VPC as well as new APIs for configuring Grafana settings.</li>; <li>api-change:<code>rbin</code>: [<code>botocore</code>] This release adds support for Rule Lock for Recycle Bin, which allows you to lock retention rules so that they can no longer be modified or deleted.</li>; </ul>; <h1>1.26.15</h1>; <ul>; <li>bugfix:Endpoints: [<code>botocore</code>] Resolve endpoint with default partition when no region is set</li>; <li>bugfix:s3: [<code>botocore</code>] fixes missing x-amz-content-sha256 header for s3 object lambda</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Adding support for Amazon AppFlow to transfer the data to Amazon Redshift databases through Amazon Redshift Data API service. This feature will support the Redshift destination connector on both public and private accessible Amazon Redshift Clusters and Amazon Redshift Serverless.</li>; <li>api-change:<code>kinesisanalyticsv2</code>: [<code>botocore</code>] Support for Apache Flink 1.15 in Kinesis Data Analytics.</li>; </ul>; <h1>1.26.14</h1>; <ul>; <li>api-change:<code>route53</code>: [<code>botocore</code>] Amazon Route 53 now supports the Asia Pacific (Hyderabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; <h1>1.26.13</h1>; <ul>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow provides a new API called UpdateConnectorRegistration to update a custom connector that customers have previously registered. With this API, customers no longer need to unregister and then",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12502:364,config,configuring,364,https://hail.is,https://github.com/hail-is/hail/pull/12502,2,['config'],['configuring']
Modifiability,"Bumps [botocore](https://github.com/boto/botocore) from 1.24.13 to 1.24.14.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/boto/botocore/blob/develop/CHANGELOG.rst"">botocore's changelog</a>.</em></p>; <blockquote>; <h1>1.24.14</h1>; <ul>; <li>api-change:<code>chime-sdk-meetings</code>: Adds support for Transcribe language identification feature to the StartMeetingTranscription API.</li>; <li>api-change:<code>ecs</code>: Amazon ECS UpdateService API now supports additional parameters: loadBalancers, propagateTags, enableECSManagedTags, and serviceRegistries</li>; <li>api-change:<code>migration-hub-refactor-spaces</code>: AWS Migration Hub Refactor Spaces documentation update.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/botocore/commit/5c6f8c8d8e6c5ed05b05302ba9ef83cc2f0c420f""><code>5c6f8c8</code></a> Merge branch 'release-1.24.14'</li>; <li><a href=""https://github.com/boto/botocore/commit/3042265ca9488b8d73c6442f703337309d6733a4""><code>3042265</code></a> Bumping version to 1.24.14</li>; <li><a href=""https://github.com/boto/botocore/commit/ba0d095eeb62a2a293abadb54111df5fc0e2f0c8""><code>ba0d095</code></a> Update to latest models</li>; <li><a href=""https://github.com/boto/botocore/commit/a8c5cc855ecb91f5f64d73f2a15dfebc9e5e20e0""><code>a8c5cc8</code></a> Merge branch 'release-1.24.13' into develop</li>; <li>See full diff in <a href=""https://github.com/boto/botocore/compare/1.24.13...1.24.14"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=botocore&package-manager=pip&previous-version=1.24.13&new-version=1.24.14)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11534:647,refactor,refactor-spaces,647,https://hail.is,https://github.com/hail-is/hail/pull/11534,2,"['Refactor', 'refactor']","['Refactor', 'refactor-spaces']"
Modifiability,"Bumps [botocore](https://github.com/boto/botocore) from 1.29.13 to 1.29.16.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/boto/botocore/blob/develop/CHANGELOG.rst"">botocore's changelog</a>.</em></p>; <blockquote>; <h1>1.29.16</h1>; <ul>; <li>api-change:<code>grafana</code>: This release includes support for configuring a Grafana workspace to connect to a datasource within a VPC as well as new APIs for configuring Grafana settings.</li>; <li>api-change:<code>rbin</code>: This release adds support for Rule Lock for Recycle Bin, which allows you to lock retention rules so that they can no longer be modified or deleted.</li>; </ul>; <h1>1.29.15</h1>; <ul>; <li>bugfix:Endpoints: Resolve endpoint with default partition when no region is set</li>; <li>bugfix:s3: fixes missing x-amz-content-sha256 header for s3 object lambda</li>; <li>api-change:<code>appflow</code>: Adding support for Amazon AppFlow to transfer the data to Amazon Redshift databases through Amazon Redshift Data API service. This feature will support the Redshift destination connector on both public and private accessible Amazon Redshift Clusters and Amazon Redshift Serverless.</li>; <li>api-change:<code>kinesisanalyticsv2</code>: Support for Apache Flink 1.15 in Kinesis Data Analytics.</li>; </ul>; <h1>1.29.14</h1>; <ul>; <li>api-change:<code>route53</code>: Amazon Route 53 now supports the Asia Pacific (Hyderabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/botocore/commit/f0dd67f9b7cc2791f301f3fd135f0c97d9c66bae""><code>f0dd67f</code></a> Merge branch 'release-1.29.16'</li>; <li><a href=""https://github.com/boto/botocore/commit/22c3cb362c0ef00c6de404140f06a14d0e195f39""><code>22c3cb3</code></a> Bumping version to 1.29.16</li>; <li><a href=""https://github.com/boto/botocore/commit/4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12503:353,config,configuring,353,https://hail.is,https://github.com/hail-is/hail/pull/12503,2,['config'],['configuring']
Modifiability,"Bumps [de.undercouch.download](https://github.com/michel-kraemer/gradle-download-task) from 5.3.0 to 5.3.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/michel-kraemer/gradle-download-task/releases"">de.undercouch.download's releases</a>.</em></p>; <blockquote>; <h2>5.3.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li>Allow <code>download</code> and <code>verify</code> extensions to be created on demand in custom tasks, so these tasks can be made compatible with Gradle's configuration cache (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/284"">#284</a>). Thanks to <a href=""https://github.com/liblit""><code>@​liblit</code></a> for testing!</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; <li>Improve documentation</li>; <li>Add integration tests for Gradle 6.9.3 and 7.6</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a0374fc7c895ae53309ea351e989571204e0ea5f""><code>a0374fc</code></a> Bump up version number to 5.3.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/612f57a382b8640cc730dc5e75d1c809e3e772bd""><code>612f57a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/291"">#291</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/53af1049f5514afe58e884d487d7c57dae47759d""><code>53af104</code></a> Bump http-cache-semantics from 4.1.0 to 4.1.1 in /screencast</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/398c14c05c6448b380ac35c6095598299c5e23c5""><code>398c14c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:593,config,configuration,593,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['config'],['configuration']
Modifiability,"Bumps [google-cloud-logging](https://github.com/googleapis/python-logging) from 1.12.1 to 3.0.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/python-logging/releases"">google-cloud-logging's releases</a>.</em></p>; <blockquote>; <h2>v3.0.0</h2>; <h2><a href=""https://github.com/googleapis/python-logging/compare/v2.7.0...v3.0.0"">3.0.0</a> (2022-01-27)</h2>; <h3>⚠ BREAKING CHANGES</h3>; <ul>; <li>make logging API more friendly to use (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/422"">#422</a>)</li>; <li>api consistency between HTTP and Gapic layers (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/375"">#375</a>)</li>; <li>support string-encoded json (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/339"">#339</a>)</li>; <li>Infer default resource in logger (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/315"">#315</a>)</li>; <li>support json logs (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/316"">#316</a>)</li>; <li>deprecate AppEngineHandler and ContainerEngineHandler (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/310"">#310</a>)</li>; </ul>; <h3>Features</h3>; <ul>; <li>add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/472"">#472</a>) (<a href=""https://github.com/googleapis/python-logging/commit/81ca8c616acb988be1fbecfc2a0b1a5b39280149"">81ca8c6</a>)</li>; <li>add json_fields extras argument for adding to jsonPayload (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/447"">#447</a>) (<a href=""https://github.com/googleapis/python-logging/commit/a760e02371a55d6262e42de9e0222fffa2c7192b"">a760e02</a>)</li>; <li>avoid importing grpc when explicitly disabled (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11574:640,layers,layers,640,https://hail.is,https://github.com/hail-is/hail/pull/11574,1,['layers'],['layers']
Modifiability,"Bumps [google-cloud-storage](https://github.com/googleapis/java-storage) from 1.106.0 to 2.16.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/java-storage/releases"">google-cloud-storage's releases</a>.</em></p>; <blockquote>; <h2>v2.16.0</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.15.1...v2.16.0"">2.16.0</a> (2022-12-06)</h2>; <h3>Features</h3>; <ul>; <li>Add {Compose,Rewrite,StartResumableWrite}Request.object_checksums and Bucket.RetentionPolicy.retention_duration (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1790"">#1790</a>) (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; <li>Added a new retention_duration field of Duration type (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; <li>Added object_checksums for compose/rewrite/startResumableWrite request (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>Removed WriteObject routing annotations (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Clarified relative resource names in gRPC IAM RPCs (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; <li>Clarified the object can be deleted via DeleteObject (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; <li>Updated the document link for <code>Naming Guidelines</code> (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12545:459,Rewrite,Rewrite,459,https://hail.is,https://github.com/hail-is/hail/pull/12545,2,"['Rewrite', 'rewrite']","['Rewrite', 'rewrite']"
Modifiability,"Bumps [jupyter-lsp](https://github.com/jupyter-lsp/jupyterlab-lsp) from 2.2.1 to 2.2.2.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter-lsp/jupyterlab-lsp/blob/main/CHANGELOG.md"">jupyter-lsp's changelog</a>.</em></p>; <blockquote>; <h3><code>jupyter-lsp 2.2.2</code></h3>; <ul>; <li>bug fixes:; <ul>; <li>address warning about renamed <code>extension_points</code> (<a href=""https://redirect.github.com/jupyter-lsp/jupyterlab-lsp/issues/1035"">#1035</a>)</li>; <li>fix compatibility with jupyter server 1.x</li>; <li>fix an authentication-related security vulnerability (see <a href=""https://github.com/jupyter-lsp/jupyterlab-lsp/security/advisories/GHSA-4qhp-652w-c22x"">the advisory</a> for details)</li>; </ul>; </li>; <li>enhancements:; <ul>; <li>add authorization support (<code>lsp</code> resource, jupyter-server v2+ only) - this allows server operators for fine grained access control, e.g. in case if specific users (such as guest or read-only users) should not be allowed to access LSP; this is in addition to authentication fixes</li>; </ul>; </li>; </ul>; <h3><code>@jupyter-lsp/jupyterlab-lsp 5.0.1</code></h3>; <ul>; <li>bug fixes:; <ul>; <li>fix false “undefined name” in <code>%%time</code> and <code>%%capture</code> magics <a href=""https://redirect.github.com/jupyter-lsp/jupyterlab-lsp/issues/1007"">#1007</a> (thanks <a href=""https://github.com/i-aki-y""><code>@​i-aki-y</code></a>!)</li>; <li>fix completion items for paths and other long items being cut off <a href=""https://redirect.github.com/jupyter-lsp/jupyterlab-lsp/issues/1025"">#1025</a></li>; <li>workaround issue with markdown lost on edit <a href=""https://redirect.github.com/jupyter-lsp/jupyterlab-lsp/issues/1016"">#1016</a></li>; <li>fix latex/Greek letters insertion and other completions which do not match prefix (do not pre-filter completions from kernel) <a href=""https://redirect.github.com/jupyter-lsp/jupyterlab-lsp/issues/1022"">#1022</a></li>; <li>fix completion",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14171:774,enhance,enhancements,774,https://hail.is,https://github.com/hail-is/hail/pull/14171,1,['enhance'],['enhancements']
Modifiability,"Bumps [kubernetes-asyncio](https://github.com/tomplus/kubernetes_asyncio) from 19.15.1 to 24.2.2.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/tomplus/kubernetes_asyncio/blob/master/CHANGELOG.md"">kubernetes-asyncio's changelog</a>.</em></p>; <blockquote>; <h1>v24.2.2</h1>; <ul>; <li>fix: config reader handles bool types (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/pull/218"">#218</a>, <a href=""https://github.com/tomplus""><code>@​tomplus</code></a>)</li>; </ul>; <h1>v24.2.1</h1>; <ul>; <li>fixed watch.stream bug of not working with apis with follow kwarg (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/pull/216"">#216</a>, <a href=""https://github.com/mcreng""><code>@​mcreng</code></a>)</li>; </ul>; <h1>v24.2.0</h1>; <p>Kubernetes API Version: v1.24.2</p>; <h3>API Change</h3>; <ul>; <li>Add 2 new options for kube-proxy running in winkernel mode. <code>--forward-healthcheck-vip</code>, if specified as true, health check traffic whose destination is service VIP will be forwarded to kube-proxy's healthcheck service. <code>--root-hnsendpoint-name</code> specifies the name of the hns endpoint for the root network namespace. This option enables the pass-through load balancers like Google's GCLB to correctly health check the backend services. Without this change, the health check packets is dropped, and Windows node will be considered to be unhealthy by those load balancers. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/99287"">kubernetes/kubernetes#99287</a>, <a href=""https://github.com/anfernee""><code>@​anfernee</code></a>)</li>; <li>Added CEL runtime cost calculation into CustomerResource validation. CustomerResource validation will fail if runtime cost exceeds the budget. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108482"">kubernetes/kubernetes#108482</a>, <a href=""https://github.com/cici37""><code>@​cici37</code><",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12196:334,config,config,334,https://hail.is,https://github.com/hail-is/hail/pull/12196,1,['config'],['config']
Modifiability,"Bumps [minimist](https://github.com/substack/minimist) from 1.2.5 to 1.2.6.; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/substack/minimist/commit/7efb22a518b53b06f5b02a1038a88bd6290c2846""><code>7efb22a</code></a> 1.2.6</li>; <li><a href=""https://github.com/substack/minimist/commit/ef88b9325f77b5ee643ccfc97e2ebda577e4c4e2""><code>ef88b93</code></a> security notice for additional prototype pollution issue</li>; <li><a href=""https://github.com/substack/minimist/commit/c2b981977fa834b223b408cfb860f933c9811e4d""><code>c2b9819</code></a> isConstructorOrProto adapted from PR</li>; <li><a href=""https://github.com/substack/minimist/commit/bc8ecee43875261f4f17eb20b1243d3ed15e70eb""><code>bc8ecee</code></a> test from prototype pollution PR</li>; <li>See full diff in <a href=""https://github.com/substack/minimist/compare/1.2.5...1.2.6"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=minimist&package-manager=npm_and_yarn&previous-version=1.2.5&new-version=1.2.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and blo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11653:590,adapt,adapted,590,https://hail.is,https://github.com/hail-is/hail/pull/11653,1,['adapt'],['adapted']
Modifiability,"Bumps [mypy](https://github.com/python/mypy) from 0.950 to 0.982.; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python/mypy/commit/1c2b899fa9029538b9b9e6d30401901d94536202""><code>1c2b899</code></a> Bump version to 0.982</li>; <li><a href=""https://github.com/python/mypy/commit/51d9858b09c82499c79023d0a80693a71baa7bed""><code>51d9858</code></a> Restore Type vs Callable special-casing that was broken in refactoring (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/13784"">#13784</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/d03f201762df7138c6da157b5cbb8e634acef45f""><code>d03f201</code></a> Suggest using upper bound for unbound tvar (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/13730"">#13730</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/5b17cc6c393280326ed15d763e599cbaeefbc0e6""><code>5b17cc6</code></a> Fix overload overlap check for UninhabitedType (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/13461"">#13461</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/c7b4714e1f5e3cb8f3fec7426b6538fe1a3dcab1""><code>c7b4714</code></a> Update version to 0.981</li>; <li><a href=""https://github.com/python/mypy/commit/2bd7da21462a59643f2aec546304db1a624ba285""><code>2bd7da2</code></a> [0.980 backport] build changes (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/13688"">#13688</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/2b2953a1392368f623331d5168ccdfd39e37bbee""><code>2b2953a</code></a> [0.980 backport] Update pos-only unit tests for Python 3.10.7 (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/13660"">#13660</a>) (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/13665"">#13665</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/ada007841f6a96f68d114769624a0f7b523814a7""><code>ada0078</code></a> Remove dev from version</li>; <li><a href=""https://github.com/python/mypy/commit/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12291:435,refactor,refactoring,435,https://hail.is,https://github.com/hail-is/hail/pull/12291,1,['refactor'],['refactoring']
Modifiability,"Bumps [notebook](https://github.com/jupyter/notebook) from 7.0.6 to 7.0.7.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/notebook/releases"">notebook's releases</a>.</em></p>; <blockquote>; <h2>v7.0.7</h2>; <h2>7.0.7</h2>; <p>(<a href=""https://github.com/jupyter/notebook/compare/@jupyter-notebook/application-extension@7.0.6...089c78c48fd00b2b0d2f33e4463eb42018e86803"">Full Changelog</a>)</p>; <h3>Enhancements made</h3>; <ul>; <li>Update to JupyterLab 4.0.11 <a href=""https://redirect.github.com/jupyter/notebook/pull/7215"">#7215</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Update ruff config and typing <a href=""https://redirect.github.com/jupyter/notebook/pull/7145"">#7145</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Clean up lint handling <a href=""https://redirect.github.com/jupyter/notebook/pull/7142"">#7142</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Adopt ruff format <a href=""https://redirect.github.com/jupyter/notebook/pull/7132"">#7132</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[7.0.x] Install stable JupyterLab 4.0 in the releaser hook <a href=""https://redirect.github.com/jupyter/notebook/pull/7183"">#7183</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; <li>Update publish-release workflow for PyPI trusted publisher <a href=""https://redirect.github.com/jupyter/notebook/pull/7176"">#7176</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/notebook/graphs/contributors?from=2023-10-17&amp;to=2024-01-19&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Abrichet+updated%3A2023-10-17..2024-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:454,Enhance,Enhancements,454,https://hail.is,https://github.com/hail-is/hail/pull/14182,2,"['Enhance', 'config']","['Enhancements', 'config']"
Modifiability,"Bumps [numpy](https://github.com/numpy/numpy) from 1.21.6 to 1.22.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/numpy/numpy/releases"">numpy's releases</a>.</em></p>; <blockquote>; <h2>v1.22.0</h2>; <h1>NumPy 1.22.0 Release Notes</h1>; <p>NumPy 1.22.0 is a big release featuring the work of 153 contributors; spread over 609 pull requests. There have been many improvements,; highlights are:</p>; <ul>; <li>Annotations of the main namespace are essentially complete. Upstream; is a moving target, so there will likely be further improvements,; but the major work is done. This is probably the most user visible; enhancement in this release.</li>; <li>A preliminary version of the proposed Array-API is provided. This is; a step in creating a standard collection of functions that can be; used across application such as CuPy and JAX.</li>; <li>NumPy now has a DLPack backend. DLPack provides a common interchange; format for array (tensor) data.</li>; <li>New methods for <code>quantile</code>, <code>percentile</code>, and related functions. The; new methods provide a complete set of the methods commonly found in; the literature.</li>; <li>A new configurable allocator for use by downstream projects.</li>; </ul>; <p>These are in addition to the ongoing work to provide SIMD support for; commonly used functions, improvements to F2PY, and better documentation.</p>; <p>The Python versions supported in this release are 3.8-3.10, Python 3.7; has been dropped. Note that 32 bit wheels are only provided for Python; 3.8 and 3.9 on Windows, all other wheels are 64 bits on account of; Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.; All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix; the occasional problems encountered by folks using truly huge arrays.</p>; <h2>Expired deprecations</h2>; <h3>Deprecated numeric style dtype strings have been removed</h3>; <p>Using the strings <code>&quot;Bytes0&",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11939:662,enhance,enhancement,662,https://hail.is,https://github.com/hail-is/hail/pull/11939,4,['enhance'],['enhancement']
Modifiability,"Bumps [protobuf](https://github.com/protocolbuffers/protobuf) from 3.19.6 to 4.21.12.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/protocolbuffers/protobuf/releases"">protobuf's releases</a>.</em></p>; <blockquote>; <h2>Protocol Buffers v3.20.3</h2>; <h1>Java</h1>; <ul>; <li>Refactoring java full runtime to reuse sub-message builders and prepare to; migrate parsing logic from parse constructor to builder.</li>; <li>Move proto wireformat parsing functionality from the private &quot;parsing; constructor&quot; to the Builder class.</li>; <li>Change the Lite runtime to prefer merging from the wireformat into mutable; messages rather than building up a new immutable object before merging. This; way results in fewer allocations and copy operations.</li>; <li>Make message-type extensions merge from wire-format instead of building up; instances and merging afterwards. This has much better performance.</li>; <li>Fix TextFormat parser to build up recurring (but supposedly not repeated); sub-messages directly from text rather than building a new sub-message and; merging the fully formed message into the existing field.</li>; <li>This release addresses a <a href=""https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-h4h5-3hr4-j3g2"">Security Advisory for Java users</a></li>; </ul>; <h2>Protocol Buffers v3.20.2</h2>; <h1>C++</h1>; <ul>; <li>Reduce memory consumption of MessageSet parsing</li>; <li>This release addresses a <a href=""https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-8gq9-2x98-w8hf"">Security Advisory for C++ and Python users</a></li>; </ul>; <h2>Protocol Buffers v3.20.1</h2>; <h1>PHP</h1>; <ul>; <li>Fix building packaged PHP extension (<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9727"">#9727</a>)</li>; <li>Fixed composer.json to only advertise compatibility with PHP 7.0+. (<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12563:324,Refactor,Refactoring,324,https://hail.is,https://github.com/hail-is/hail/pull/12563,1,['Refactor'],['Refactoring']
Modifiability,"Bumps [protobuf](https://github.com/protocolbuffers/protobuf) from 3.20.2 to 4.21.9.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/protocolbuffers/protobuf/releases"">protobuf's releases</a>.</em></p>; <blockquote>; <h2>Protocol Buffers v3.20.3</h2>; <h1>Java</h1>; <ul>; <li>Refactoring java full runtime to reuse sub-message builders and prepare to; migrate parsing logic from parse constructor to builder.</li>; <li>Move proto wireformat parsing functionality from the private &quot;parsing; constructor&quot; to the Builder class.</li>; <li>Change the Lite runtime to prefer merging from the wireformat into mutable; messages rather than building up a new immutable object before merging. This; way results in fewer allocations and copy operations.</li>; <li>Make message-type extensions merge from wire-format instead of building up; instances and merging afterwards. This has much better performance.</li>; <li>Fix TextFormat parser to build up recurring (but supposedly not repeated); sub-messages directly from text rather than building a new sub-message and; merging the fully formed message into the existing field.</li>; <li>This release addresses a <a href=""https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-h4h5-3hr4-j3g2"">Security Advisory for Java users</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/protocolbuffers/protobuf/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=protobuf&package-manager=pip&previous-version=3.20.2&new-version=4.21.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12518:323,Refactor,Refactoring,323,https://hail.is,https://github.com/hail-is/hail/pull/12518,1,['Refactor'],['Refactoring']
Modifiability,"Bumps [psutil](https://github.com/giampaolo/psutil) from 5.8.0 to 5.9.0.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/giampaolo/psutil/blob/master/HISTORY.rst"">psutil's changelog</a>.</em></p>; <blockquote>; <h1>5.9.0</h1>; <p>2021-12-29</p>; <p><strong>Enhancements</strong></p>; <ul>; <li>1851_, [Linux]: <code>cpu_freq()</code>_ is slow on systems with many CPUs. Read current; frequency values for all CPUs from <code>/proc/cpuinfo</code> instead of opening many; files in <code>/sys</code> fs. (patch by marxin)</li>; <li>1992_: <code>NoSuchProcess</code>_ message now specifies if the PID has been reused.</li>; <li>1992_: error classes (<code>NoSuchProcess</code><em>, <code>AccessDenied</code></em>, etc.) now have a better; formatted and separated <code>__repr__</code> and <code>__str__</code> implementations.</li>; <li>1996_, [BSD]: add support for MidnightBSD. (patch by Saeed Rasooli)</li>; <li>1999_, [Linux]: <code>disk_partitions()</code>_: convert <code>/dev/root</code> device (an alias; used on some Linux distros) to real root device path.</li>; <li>2005_: <code>PSUTIL_DEBUG</code> mode now prints file name and line number of the debug; messages coming from C extension modules.</li>; <li>2042_: rewrite HISTORY.rst to use hyperlinks pointing to psutil API doc.</li>; </ul>; <p><strong>Bug fixes</strong></p>; <ul>; <li>1456_, [macOS], <strong>[critical]</strong>: <code>cpu_freq()</code>_ <code>min</code> and <code>max</code> are set to; 0 if can't be determined (instead of crashing).</li>; <li>1512_, [macOS]: sometimes <code>Process.connections()</code>_ will crash with; <code>EOPNOTSUPP</code> for one connection; this is now ignored.</li>; <li>1598_, [Windows]: <code>disk_partitions()</code>_ only returns mountpoints on drives; where it first finds one.</li>; <li>1874_, [SunOS]: swap output error due to incorrect range.</li>; <li>1892_, [macOS]: <code>cpu_freq()</code>_ broken on Apple M1.</li>; <li>1901_, [macOS]: diff",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11459:299,Enhance,Enhancements,299,https://hail.is,https://github.com/hail-is/hail/pull/11459,1,['Enhance'],['Enhancements']
Modifiability,"Bumps [pylint](https://github.com/PyCQA/pylint) from 2.13.3 to 2.13.4.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/pylint/blob/main/ChangeLog"">pylint's changelog</a>.</em></p>; <blockquote>; <h1>What's New in Pylint 2.13.4?</h1>; <p>Release date: 2022-03-31</p>; <ul>; <li>; <p>Fix false positive regression in 2.13.0 for <code>used-before-assignment</code> for; homonyms between variable assignments in try/except blocks and variables in; a comprehension's filter.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6035"">#6035</a></p>; </li>; <li>; <p>Include <code>testing_pylintrc</code> in source and wheel distributions.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6028"">#6028</a></p>; </li>; <li>; <p>Fix crash in <code>super-init-not-called</code> checker when using <code>ctypes.Union</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6027"">#6027</a></p>; </li>; <li>; <p>Fix crash for <code>unneccessary-ellipsis</code> checker when an ellipsis is used inside of a container or a lambda expression.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6036"">#6036</a>; Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6037"">#6037</a>; Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6048"">#6048</a></p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/pylint/commit/14ae9e8985a70af1b04aa996c04a1a8c3fa8f463""><code>14ae9e8</code></a> Bump pylint to 2.13.4, update changelog</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/9009189c06dd326b7a4f5b9911d0246976f64509""><code>9009189</code></a> Fix crash in <code>super-init-not-called</code> checker (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6043"">#6043</a>)</li>; <li><a href=""https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11723:432,variab,variable,432,https://hail.is,https://github.com/hail-is/hail/pull/11723,2,['variab'],"['variable', 'variables']"
Modifiability,"Bumps [pylint](https://github.com/PyCQA/pylint) from 2.13.4 to 2.13.5.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/pylint/blob/main/ChangeLog"">pylint's changelog</a>.</em></p>; <blockquote>; <h1>What's New in Pylint 2.13.5?</h1>; <p>Release date: 2022-04-06</p>; <ul>; <li>; <p>Fix false positive regression in 2.13.0 for <code>used-before-assignment</code> for; homonyms between variable assignments in try/except blocks and variables in; subscripts in comprehensions.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6069"">#6069</a>; Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6136"">#6136</a></p>; </li>; <li>; <p><code>lru-cache-decorating-method</code> has been renamed to <code>cache-max-size-none</code> and; will only be emitted when <code>maxsize</code> is <code>None</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6180"">#6180</a></p>; </li>; <li>; <p>Fix false positive for <code>unused-import</code> when disabling both <code>used-before-assignment</code> and <code>undefined-variable</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6089"">#6089</a></p>; </li>; <li>; <p>Narrow the scope of the <code>unnecessary-ellipsis</code> checker to:</p>; <ul>; <li>functions &amp; classes which contain both a docstring and an ellipsis.</li>; <li>A body which contains an ellipsis <code>nodes.Expr</code> node &amp; at least one other statement.</li>; </ul>; </li>; <li>; <p>Fix false positive for <code>used-before-assignment</code> for assignments taking place via; nonlocal declarations after an earlier type annotation.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5394"">#5394</a></p>; </li>; <li>; <p>Fix crash for <code>redefined-slots-in-subclass</code> when the type of the slot is not a const or a string.</p>; <p>Closes <a href=""https://github-redi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11739:432,variab,variable,432,https://hail.is,https://github.com/hail-is/hail/pull/11739,2,['variab'],"['variable', 'variables']"
Modifiability,"Bumps [pylint](https://github.com/PyCQA/pylint) from 2.13.5 to 2.14.3.; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/pylint/commit/680edebc686cad664bbed934a490aeafa775f163""><code>680edeb</code></a> Bump pylint to 2.14.3, update changelog</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/b05ac51ad2e3785b6b9b071b8cb241993c914105""><code>b05ac51</code></a> Pin <code>colorama</code> to lowest supported version (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6970"">#6970</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/417e8c3560bcb733a08dbdc8a0d33d5e3cb4a1b0""><code>417e8c3</code></a> Fix <code>bad-super-call</code> for non-direct parents (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6956"">#6956</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/fb6be5933ab270d542d80589be6fdea8abc82665""><code>fb6be59</code></a> Fix <code>undefined-variable</code> for <code>__class__</code> in inner methods (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6957"">#6957</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/b9ecb4d70d23f7a6d05cc14e94c26fd8d3261d0f""><code>b9ecb4d</code></a> Fix false positive for <code>useless-super-delegation</code> for variadics (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6949"">#6949</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/f881219a66deaf9cef6467ba27c3385bc98dad82""><code>f881219</code></a> Bump pylint to 2.14.2, update changelog</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/988d882b56f9eca8ba1825b86b59e92b824ca1c3""><code>988d882</code></a> Treat <code>--errors-only</code> as a disable, not a paired enable/disable (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6937"">#6937</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/386e7782b78a6e1baf0edd57cff893f3a08fb33c""><code>386e778</code></a> Mix incorrect parsing of multi-line optio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11971:952,variab,variable,952,https://hail.is,https://github.com/hail-is/hail/pull/11971,1,['variab'],['variable']
Modifiability,"Bumps [pylint](https://github.com/PyCQA/pylint) from 2.13.5 to 2.14.4.; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/pylint/commit/bf29a5520e8d0e432ca715e0614a62052b3809e2""><code>bf29a55</code></a> Bump pylint to 2.14.4, update changelog</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/15470d10f74adb8fd3fab599097a8da8c10ec515""><code>15470d1</code></a> Fix recognition of config files named <code>setup.cfg</code> (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/3630"">#3630</a>) (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6577"">#6577</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/e8202000e046e286816375f5887110cacda4d11b""><code>e820200</code></a> Normalize path before checking if path should be ignored (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7080"">#7080</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/c82d08c8433de92433b9b555dd2eb50a7987060f""><code>c82d08c</code></a> Don't report <code>import-private-name</code> for relative imports (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7079"">#7079</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/f8f05f12522c0036668f9a0da86fa0d3456ed795""><code>f8f05f1</code></a> Don't emit <code>modified-iterating-dict</code> when updating existing keys (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7037"">#7037</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/bee24cd55af4f1231e787aed5a1cc072492adee6""><code>bee24cd</code></a> Avoid hangs on many-core Windows machines (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7035"">#7035</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/b379ef3acc2a983140994c93a2ea2c99e260c9c1""><code>b379ef3</code></a> Fix handling of quoted <code>init-hook</code> (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7010"">#7010</a>)</li>; <li><a href=""https",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11980:420,config,config,420,https://hail.is,https://github.com/hail-is/hail/pull/11980,1,['config'],['config']
Modifiability,"Bumps [pylint](https://github.com/PyCQA/pylint) from 2.13.5 to 2.15.3.; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/pylint/commit/403dac602ee01e317a22800e0d63bdeb0c2faa7e""><code>403dac6</code></a> Bump pylint to 2.15.3, update changelog</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/38e278401a66218fba26308fbce56740761a2003""><code>38e2784</code></a> Bump astroid to 2.12.10</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/f5e168e867799013fb380aa9fe8a0c1516a651c8""><code>f5e168e</code></a> Fix <code>undefined-loop-variable</code> with <code>NoReturn</code> and <code>Never</code> (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7476"">#7476</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/fbc9e663473fa0416779f1d71109b4123f6c3365""><code>fbc9e66</code></a> Accept a comma-separated list of messages IDs in <code>--help-msg</code> (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7490"">#7490</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/fe3436efb0ec10677ba1539ac02e26cb3f852cbb""><code>fe3436e</code></a> False positive <code>global-variable-not-assigned</code> (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7479"">#7479</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/52cf631d732f7b39a879adf7e617e0aa7059a83a""><code>52cf631</code></a> [invalid-class-object] Fix crash when <strong>class</strong> is defined with a tuple</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/8e05ff6acf30deae5d83ea3847ec47ed0bf049a4""><code>8e05ff6</code></a> Fix a crash in the <code>modified-iterating-dict</code> checker involving instance attri...</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/9b359ad676dff97a35321976c19ca0f6c4fc44ad""><code>9b359ad</code></a> Fix <code>unhashable-member</code> crash when <code>lambda</code> used as a dict key (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/7454"">#7454</a>)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12240:575,variab,variable,575,https://hail.is,https://github.com/hail-is/hail/pull/12240,1,['variab'],['variable']
Modifiability,"Bumps [pylint](https://github.com/PyCQA/pylint) from 2.6.0 to 2.12.2.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/pylint/releases"">pylint's releases</a>.</em></p>; <blockquote>; <h2>pylint-2.8.1</h2>; <ul>; <li>; <p>Add numversion back (temporarily) in <code>__pkginfo__</code> because it broke Pylama and revert the unnecessary; <code>pylint.version</code> breaking change.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/4399"">#4399</a></p>; </li>; </ul>; <h2>pylint-2.8.0</h2>; <ul>; <li>; <p>New refactoring message <code>consider-using-with</code>. This message is emitted if resource-allocating functions or methods of the; standard library (like <code>open()</code> or <code>threading.Lock.acquire()</code>) that can be used as a context manager are called without; a <code>with</code> block.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/3413"">#3413</a></p>; </li>; <li>; <p>Resolve false positives on unused variables in decorator functions</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/4252"">#4252</a></p>; </li>; <li>; <p>Add new extension <code>ConfusingConsecutiveElifChecker</code>. This optional checker emits a refactoring message (R5601 <code>confusing-consecutive-elif</code>); if if/elif statements with different indentation levels follow directly one after the other.</p>; </li>; <li>; <p>New option <code>--output=&lt;file&gt;</code> to output result to a file rather than printing to stdout.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/1070"">#1070</a></p>; </li>; <li>; <p>Use a prescriptive message for <code>unidiomatic-typecheck</code></p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/3891"">#3891</a></p>; </li>; <li>; <p>Apply <code>const-naming-style</code> to module constants annotated with; <code>typing.Final</code></p>; </li>; <li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11461:591,refactor,refactoring,591,https://hail.is,https://github.com/hail-is/hail/pull/11461,2,['refactor'],['refactoring']
Modifiability,"Bumps [pytest](https://github.com/pytest-dev/pytest) from 6.2.5 to 7.0.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pytest-dev/pytest/releases"">pytest's releases</a>.</em></p>; <blockquote>; <h2>7.0.1</h2>; <h1>pytest 7.0.1 (2022-02-11)</h1>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9608"">#9608</a>: Fix invalid importing of <code>importlib.readers</code> in Python 3.9.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9610"">#9610</a>: Restore [UnitTestFunction.obj]{.title-ref} to return unbound rather than bound method.; Fixes a crash during a failed teardown in unittest TestCases with non-default [__init__]{.title-ref}.; Regressed in pytest 7.0.0.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9636"">#9636</a>: The <code>pythonpath</code> plugin was renamed to <code>python_path</code>. This avoids a conflict with the <code>pytest-pythonpath</code> plugin.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9642"">#9642</a>: Fix running tests by id with <code>::</code> in the parametrize portion.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9643"">#9643</a>: Delay issuing a <code>~pytest.PytestWarning</code>{.interpreted-text role=&quot;class&quot;} about diamond inheritance involving <code>~pytest.Item</code>{.interpreted-text role=&quot;class&quot;} and; <code>~pytest.Collector</code>{.interpreted-text role=&quot;class&quot;} so it can be filtered using <code>standard warning filters &lt;warnings&gt;</code>{.interpreted-text role=&quot;ref&quot;}.</li>; </ul>; <h2>7.0.0</h2>; <h1>pytest 7.0.0 (2022-02-03)</h1>; <p>(<strong>Please see the full set of changes for this release also in the 7.0.0rc1 notes below</strong>)</p>; <h2>Deprecations</h2>; <ul>; <li>; <p><a href=""https://github-redirect.dependabot.com/pytest-dev/pyte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:932,plugin,plugin,932,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['plugin'],['plugin']
Modifiability,"Bumps [rich](https://github.com/Textualize/rich) from 12.6.0 to 13.7.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/releases"">rich's releases</a>.</em></p>; <blockquote>; <h2>The &quot;It's a wrap&quot; release</h2>; <h2>[13.7.0] - 2023-11-15</h2>; <h3>Added</h3>; <ul>; <li>Adds missing parameters to Panel.fit <a href=""https://redirect.github.com/Textualize/rich/issues/3142"">Textualize/rich#3142</a></li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Some text goes missing during wrapping when it contains double width characters <a href=""https://redirect.github.com/Textualize/rich/issues/3176"">Textualize/rich#3176</a></li>; <li>Ensure font is correctly inherited in exported HTML <a href=""https://redirect.github.com/Textualize/rich/issues/3104"">Textualize/rich#3104</a></li>; <li>Fixed typing for <code>FloatPrompt</code>.</li>; </ul>; <h2>The Python 3.12 release</h2>; <p>Mostly a meta update in readiness for the release of Python3.12</p>; <h2>[13.6.0] - 2023-09-30</h2>; <h3>Added</h3>; <ul>; <li>Added Python 3.12 to classifiers.</li>; </ul>; <h2>Markdown fixes</h2>; <h2>[13.5.3] - 2023-09-17</h2>; <h3>Fixed</h3>; <ul>; <li>Markdown table rendering issue with inline styles and links <a href=""https://redirect.github.com/Textualize/rich/issues/3115"">Textualize/rich#3115</a></li>; <li>Fix Markdown code blocks on a light background <a href=""https://redirect.github.com/Textualize/rich/issues/3123"">Textualize/rich#3123</a></li>; </ul>; <h2>v13.5.2</h2>; <p>Bugfix</p>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>v13.5.1</h2>; <p>Very minor update to URL highlighting</p>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not included in link regex when printing to console <a href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>Mostly cake, one or two puppies</h2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14012:714,inherit,inherited,714,https://hail.is,https://github.com/hail-is/hail/pull/14012,2,['inherit'],['inherited']
Modifiability,"Bumps [rich](https://github.com/Textualize/rich) from 12.6.0 to 13.7.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/releases"">rich's releases</a>.</em></p>; <blockquote>; <h2>v13.7.1</h2>; <p>Fixes some character widths</p>; <h2>[13.7.1] - 2023-02-28</h2>; <h3>Fixed</h3>; <ul>; <li>Updated the widths of some characters <a href=""https://redirect.github.com/Textualize/rich/pull/3289"">Textualize/rich#3289</a></li>; </ul>; <h2>The &quot;It's a wrap&quot; release</h2>; <h2>[13.7.0] - 2023-11-15</h2>; <h3>Added</h3>; <ul>; <li>Adds missing parameters to Panel.fit <a href=""https://redirect.github.com/Textualize/rich/issues/3142"">Textualize/rich#3142</a></li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Some text goes missing during wrapping when it contains double width characters <a href=""https://redirect.github.com/Textualize/rich/issues/3176"">Textualize/rich#3176</a></li>; <li>Ensure font is correctly inherited in exported HTML <a href=""https://redirect.github.com/Textualize/rich/issues/3104"">Textualize/rich#3104</a></li>; <li>Fixed typing for <code>FloatPrompt</code>.</li>; </ul>; <h2>The Python 3.12 release</h2>; <p>Mostly a meta update in readiness for the release of Python3.12</p>; <h2>[13.6.0] - 2023-09-30</h2>; <h3>Added</h3>; <ul>; <li>Added Python 3.12 to classifiers.</li>; </ul>; <h2>Markdown fixes</h2>; <h2>[13.5.3] - 2023-09-17</h2>; <h3>Fixed</h3>; <ul>; <li>Markdown table rendering issue with inline styles and links <a href=""https://redirect.github.com/Textualize/rich/issues/3115"">Textualize/rich#3115</a></li>; <li>Fix Markdown code blocks on a light background <a href=""https://redirect.github.com/Textualize/rich/issues/3123"">Textualize/rich#3123</a></li>; </ul>; <h2>v13.5.2</h2>; <p>Bugfix</p>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>v13.5.1</h2>; <p>Very minor update to URL highlighting</p>; <!-- raw HTML omitted -->; </blockquot",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14376:966,inherit,inherited,966,https://hail.is,https://github.com/hail-is/hail/pull/14376,2,['inherit'],['inherited']
Modifiability,"Bumps [sphinx-rtd-theme](https://github.com/readthedocs/sphinx_rtd_theme) from 1.3.0 to 2.0.0.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/readthedocs/sphinx_rtd_theme/blob/master/docs/changelog.rst"">sphinx-rtd-theme's changelog</a>.</em></p>; <blockquote>; <h1>2.0.0</h1>; <h2>Added</h2>; <ul>; <li>Support for Sphinx versions <code>6.x</code> and <code>7.x</code></li>; <li>Support for docutils <code>&lt;=0.20</code></li>; </ul>; <h2>Deprecations</h2>; <ul>; <li>The HTML4 writer is now officially deprecated. An error will be thrown if your; project configuration still uses the HTML4 writer.</li>; <li>Support for Sphinx versions &lt; 5.0 was removed.</li>; <li>In addition, our supported dependencies will match the dependencies from our; lowest supported Sphinx release, version 5.0: Python &gt;= 3.6 and docutils &gt; 0.14 and &lt; 0.19</li>; </ul>; <p>.. _release-1.3.0:</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/7c9b1b5d391f6d7fae72274393eb25d1df96e546""><code>7c9b1b5</code></a> Release 2.0 final (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1544"">#1544</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/c1044107602faf9be43e4358bc4f8b6abff9b420""><code>c104410</code></a> Bump for next potential release, 2.0.0rc5 (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1539"">#1539</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/53ca116ef64123735e5e445258b8b103ad31a26e""><code>53ca116</code></a> Release 2.0.0rc4 (<a href=""https://redirect.github.com/readthedocs/sphinx_rtd_theme/issues/1538"">#1538</a>)</li>; <li><a href=""https://github.com/readthedocs/sphinx_rtd_theme/commit/4498e97b462688bac2ff3615ac1da1b867b21842""><code>4498e97</code></a> Fix AttributeError when one of <code>css_files</code> is a string (<a href=""https://redire",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14502:600,config,configuration,600,https://hail.is,https://github.com/hail-is/hail/pull/14502,1,['config'],['configuration']
Modifiability,"Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.5 to 1.26.8.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/releases"">urllib3's releases</a>.</em></p>; <blockquote>; <h2>1.26.8</h2>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a>.</strong></p>; <p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <p>:warning: <strong>This release will be the last release supporting Python 3.5. Please upgrade to a non-EOL Python version.</strong></p>; <ul>; <li>Added extra message to<code>urllib3.exceptions.ProxyError</code> when urllib3 detects that a proxy is configured to use HTTPS but the proxy itself appears to only use HTTP.</li>; <li>Added a mention of the size of the connection pool when discarding a connection due to the pool being full.</li>; <li>Added explicit support for Python 3.11.</li>; <li>Deprecated the <code>Retry.MAX_BACKOFF</code> class property in favor of <code>Retry.DEFAULT_MAX_BACKOFF</code> to better match the rest of the default parameter names. <code>Retry.MAX_BACKOFF</code> is removed in v2.0.</li>; <li>Changed location of the vendored <code>ssl.match_hostname</code> function from <code>urllib3.packages.ssl_match_hostname</code> to <code>urllib3.util.ssl_match_hostname</code> to ensure Python 3.10+ compatibility after being repackaged by downstream distributors.</li>; <li>Fixed absolute imports, all imports are now relative.</li>; </ul>; <h2>1.26.7</h2>; <p>:warning: <strong>IMPORTANT: urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed a bug with HTTPS hostname verification involving IP addresses and lack of SNI</li>; <li>Fixed a bug ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11532:856,config,configured,856,https://hail.is,https://github.com/hail-is/hail/pull/11532,1,['config'],['configured']
Modifiability,"Bumps [zipp](https://github.com/jaraco/zipp) from 3.17.0 to 3.18.1.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jaraco/zipp/blob/main/NEWS.rst"">zipp's changelog</a>.</em></p>; <blockquote>; <h1>v3.18.1</h1>; <p>No significant changes.</p>; <h1>v3.18.0</h1>; <h2>Features</h2>; <ul>; <li>Bypass ZipFile.namelist in glob for better performance. (<a href=""https://redirect.github.com/jaraco/zipp/issues/106"">#106</a>)</li>; <li>Refactored glob functionality to support a more generalized solution with support for platform-specific path separators. (<a href=""https://redirect.github.com/jaraco/zipp/issues/108"">#108</a>)</li>; </ul>; <h2>Bugfixes</h2>; <ul>; <li>Add special accounting for pypy when computing the stack level for text encoding warnings. (<a href=""https://redirect.github.com/jaraco/zipp/issues/114"">#114</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jaraco/zipp/commit/bfae83474a730e8cc9b8a71027fb859b46b3875c""><code>bfae834</code></a> Finalize</li>; <li><a href=""https://github.com/jaraco/zipp/commit/487066ec9757c3c82e96014d0b30906996c6280d""><code>487066e</code></a> Merge changelog into last release.</li>; <li><a href=""https://github.com/jaraco/zipp/commit/4584ee2dcfb10d5314ad319d9d5b140c90bc2951""><code>4584ee2</code></a> Move changelog entry, saved to the wrong location :(</li>; <li><a href=""https://github.com/jaraco/zipp/commit/3c06d30b91b37a118536d9d424e0a8b893e78a6e""><code>3c06d30</code></a> Finalize</li>; <li><a href=""https://github.com/jaraco/zipp/commit/48b72b8db6ae5f7712323aca6b340744db15f576""><code>48b72b8</code></a> Merge pull request <a href=""https://redirect.github.com/jaraco/zipp/issues/113"">#113</a> from jaraco/feature/glob-perf</li>; <li><a href=""https://github.com/jaraco/zipp/commit/171fa98236a1adfc316c3bc5cdc5eaa4b9548424""><code>171fa98</code></a> Add news fragment.</li>; <li><a href=""https://github.com/jaraco/zipp/commit/ac8ea7a5",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14473:471,Refactor,Refactored,471,https://hail.is,https://github.com/hail-is/hail/pull/14473,1,['Refactor'],['Refactored']
Modifiability,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:833,extend,extends,833,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682,2,['extend'],['extends']
Modifiability,But let's solve for Aus and Azure as a separate thread of work that doesn't block with PR. I imagine whatever we decide will either make this command need to take a domain or enhance this command to create a profile per-domain.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1663144322:175,enhance,enhance,175,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1663144322,1,['enhance'],['enhance']
Modifiability,"But only when ExtendedOrdering is used, which I think is pretty rare, now, just in `Interpret` (so eval, top-level expressions) and aggregators.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4871#issuecomment-443787719:14,Extend,ExtendedOrdering,14,https://hail.is,https://github.com/hail-is/hail/pull/4871#issuecomment-443787719,1,['Extend'],['ExtendedOrdering']
Modifiability,"Buy it, use it, break it, fix it; Trash it, change it, mail - upgrade it; Charge it, point it, zoom it, press it; Snap it, work it, quick - erase it; Write it, cut it, paste it, save it; Load it, check it, quick - rewrite it; Plug it, play it, burn it, rip it; Drag and drop it, zip - unzip it; Lock it, fill it, call it, find it; View it, code it, jam - unlock it; Surf it, scroll it, pause it, click it; Cross it, crack it, switch - update it; Name it, read it, tune it, print it; Scan it, send it, fax - rename it; Touch it, bring it, pay it, watch it; Turn it, leave it, start - format it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2036#issuecomment-318518817:214,rewrite,rewrite,214,https://hail.is,https://github.com/hail-is/hail/pull/2036#issuecomment-318518817,2,['rewrite'],['rewrite']
Modifiability,C5Compiled.__m7split_Let(Emit.scala); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:163); 	at is.hail.backend.service.ServiceBackend.lowerDistributedSort(ServiceBackend.scala:356); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:100); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$4(RewriteBottomUp.scala:26); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringP,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:7756,Rewrite,RewriteBottomUp,7756,https://hail.is,https://github.com/hail-is/hail/issues/12983,4,['Rewrite'],['RewriteBottomUp']
Modifiability,CF$extension(VariantDataset.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:11546,Config,Configuration,11546,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,"CHANGELOG: ABS blob URIs in the format of `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>` are now supported. The `hail-az` scheme for referencing blobs in ABS is now deprecated and will be removed in an upcoming release. This PR introduces the https addressing of blobs in ABS and phases out hail-az. The test suite converts completely to testing `https`, but both schemes are still supported. We can have confidence that this did not break completely break the `hail-az` scheme because our test bucket configuration is still using `hail-az` (and must until this PR is merged. So some of the test suite + all the service backend tests are flexing the `https` code path, and then the inter_cloud tests are flexing the `hail-az` code path. After this merges, we'll need the following PRs. - Update the azure terraform to use `https` instead of `hail-az` and apply the changes; - Remove support for the hail-az scheme. This will be a breaking change as the copy tool and batch worker will stop being able to transfer files and logs for that scheme. It seems like there is large support for dropping this scheme entirely though so I'd rather make this change while there is little use on azure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12917:526,config,configuration,526,https://hail.is,https://github.com/hail-is/hail/pull/12917,1,['config'],['configuration']
Modifiability,"CHANGELOG: Add `hl.pgenchisq` the cumulative distribution function of the generalized chi-squared distribution. The [Generalized Chi-Squared; Distribution](https://en.wikipedia.org/wiki/Generalized_chi-squared_distribution) arises from weighted sums of sums of squares of independent normally distributed variables and is used by `hl.skat` to generate p-values. The simplest formulation I know for it is this:. w : R^n; k : Z^n; lam : R^n; mu : R; sigma : R. x ~ N(mu, sigma^2); y_i ~ NonCentralChiSquared(k_i, lam_i). Z = x + w y^T; = x + sum_i{ w_i y_i }; Z ~ GeneralizedNonCentralChiSquared(w, k, lam, mu, sigma). The non-central chi-squared distribution arises from a sum of independent normally distributed variables with non-zero mean and unit variance. The non-centrality parameter, lambda, is defined as the sum of the squares of the means of each component normal random variable. Although the non-central chi-squared distribution has a closed form implementation (indeed, Hail implements this CDF: `hl.pchisqtail`), the generalized chi-squared distribution does not have a closed form. There are at least four distinct algorithms for evaluating the CDF. To my knowledge, the oldest one is by Robert Davies:. Davies, Robert. ""The distribution of a linear combination of chi-squared; random variables."" Applied Statistics 29 323-333. 1980. The [original publication](http://www.robertnz.net/pdf/lc_chisq.pdf) includes a Fortran implementation in the publication. Davies' [website](http://www.robertnz.net/QF.htm) also includes a C version. Hail includes a copy of the C version as `davies.cpp`. I suspect this code contains undefined behavior. Moreover, it is not supported on Apple M1 machines because we don't ship binaries for that platform. It seemed to me that the simplest solution is to port this algorithm to Scala. This PR is that port. I tested against the 39 test cases provided Davies with the source code. I also added some doctests based on the CDF plots from Wikipedia. The same",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12605:305,variab,variables,305,https://hail.is,https://github.com/hail-is/hail/pull/12605,3,['variab'],"['variable', 'variables']"
Modifiability,"CHANGELOG: Added `Job.run_when()` which allows you to specify under what conditions a job should run based on the state of its parent dependencies and whether the job is cancellable. The gist of this PR is that we now have an extra variable `run_condition` which specifies under what conditions a job should run (any, all, always). The default is to maintain the current behavior that a job runs when all its parent dependencies succeeded. The semantics of `always_run` should stay the same. Can you check over the jobs_after_update trigger and make sure I'm not crazy and nothing needs to be modified there since the job state where the cancelled state matters is a Ready or Running job and the run_condition / cancelled interaction applies to Pending jobs with parent dependencies. Also, I did this crazily fast, so maybe I'm missing something and it shouldn't be this easy????",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12365:232,variab,variable,232,https://hail.is,https://github.com/hail-is/hail/pull/12365,1,['variab'],['variable']
Modifiability,"CHANGELOG: Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to specify which cloud regions a job can run in. The default value is a job can run in any available region. Stacked on #12212 . This PR threads through region requests from the user and feeds that information into the scheduler. The architecture of a pool per machine type has not changed. We explicitly chose not to have a new pool per region x machine_type. Instead, the control loop looks at the front of the job queue and tries to predict which jobs are likely to be scheduled. From those jobs, we then find which regions the jobs can run in and create the number of corresponding instances. We use the fair share calculation to estimate how many jobs per user can be scheduled in 2.5 minutes assuming the scheduling loop runs once per second. We then grab this many jobs from the queue for each user and estimate the ""scheduling iteration"" at which each iteration of the scheduler each chunk of user jobs would be scheduled. We sort the overall set of jobs that we've chosen by the ""scheduling iteration"". We also include the regions as part of the sorting queries with None (any region) being sorted last. This is to compact the free cores across jobs so as to avoid fragmentation of instances created and for jobs with no region specifications to fill in the remaining cores in any region. For the hailtop.batch client, I added a new setting in `~/.config/hail` to set the default regions for all jobs in the ServiceBackend and a new method on `Job` that sets the list of regions to run in. Things to double check once everything is working is the sort orders on the scheduling queries are correct. . Once this PR goes in, then we can merge #11840 with some minor changes. There will also be a follow-up PR that gets rid of the CI-specific code in the scheduler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221:57,config,configurable,57,https://hail.is,https://github.com/hail-is/hail/pull/12221,2,['config'],"['config', 'configurable']"
Modifiability,CHANGELOG: Added hailctl config setting 'batch/backend' to specify the default backend to use in batch scripts when not specified in code. This is more consistent now with `HAIL_QUERY_BACKEND` and makes it less boilerplate-y to use the service. cc: @danking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12522:25,config,config,25,https://hail.is,https://github.com/hail-is/hail/pull/12522,1,['config'],['config']
Modifiability,"CHANGELOG: Adds close, default implementation pass, to Backend as an abstract method. This reduces the number of conditional statements needed by 1 when using variable backends. Variable backends (Local or Service) are useful when prototyping batches, or when the user will know in advance that a particular batch routine will work locally (since in future work it will be much easier to consume dockerized methods using Batch than anything else). Additionally, as provided, the only Batch tutorial, GWAS clumping will not work with LocalBackend without this. Use case:. ```python; parser.add_argument('--local', required=False, action=""store_true""); if is_local:; backend = hb.LocalBackend(); run_opts = {}; else:; backend = hb.ServiceBackend(); run_opts = {open: True, wait: True}. # do a bunch of Batch stuff to ; batch.run(**run_opts); backend.close(); ```. In a similar vein, I'd like to allow LocalBackend to ignore unused run opts. Again, GWAS tutorial would not work with LocalBackend without this (or an opts dict as above).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9191:159,variab,variable,159,https://hail.is,https://github.com/hail-is/hail/pull/9191,2,"['Variab', 'variab']","['Variable', 'variable']"
Modifiability,"CHANGELOG: Batch ServiceBackend now requires a bucket for intermediate files, either explicitly or through the batch/bucket config setting. Other changes:; - moved notebook/user to auth/user (more appropriate there); - auth no longer creates bucket during user creation",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8834:124,config,config,124,https://hail.is,https://github.com/hail-is/hail/pull/8834,1,['config'],['config']
Modifiability,CHANGELOG: Changed cost per instance from $0.02170 to $0.021935 from switching to using local SSDs. - Added 1 local SSD (375 GB) and formatted it in the worker run script.; - Changed the resource for boot-disk to just disk and modified the worker config. I figured there was no reason to have a separate boot disk in the resources as long as all disks are assumed to be fractions of the instance based on the number of cores being used.; - Changed the worker boot disk from 100 GB to 20 GB; - Changed the worker to move all docker files and batch files to the Local SSD from the boot disk. Can you double check my math for the documentation?. Is it possible it takes longer for an instance to boot up with a local SSD? One of my earlier tests had workers stuck in STAGING. This resolved itself later on so I'm assuming it was a Google error.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8844:247,config,config,247,https://hail.is,https://github.com/hail-is/hail/pull/8844,1,['config'],['config']
Modifiability,"CHANGELOG: Fixed bug where making NDArrays of non-numeric types would fail. Non-numeric ndarrays still cannot be collected to python though. . NDArrays of non numeric types are broken, have been for a while. No one seems to use them for that currently, so it hasn't been an issue, but I suspect with `dndarray` or BlockedMatrixTable experiments it's going to be desirable. . This PR starts to address that problem by doing the following:. 1. `checkedConvertFrom`, which only supported primitive arrays, is replaced with the more flexible `copyFromType`. As this was the only use of `checkedConvertFrom`, I removed it altogether. . 2. Add tests that show that it's now possible to make an ndarray of non-numeric types, so long as the only things that get returned in python are numbers. The remaining problems all involve conversions to numpy. If you never convert to numpy, things should be fine:. 1. I need to get strides out of the Java ndarray representation. Strides make no sense for non-numeric objects after converting from Java to Python. We say the size of a required tuple of 3 int32's is 12 bytes, but that's not going to be the size of the python object. 2. Strings are tricky too, since the numpy string dtype comes with a max length, so we'll have to do a pass over the strings to figure out how large the largest one is before converting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9503:529,flexible,flexible,529,https://hail.is,https://github.com/hail-is/hail/pull/9503,1,['flexible'],['flexible']
Modifiability,"CHANGELOG: Fixed bugs in the identity by descent implementation for Query on Batch. This PR fixes #14052. There were two bugs in how we compute IBD. In addition, the tests weren't running in QoB and the test dataset we were using doesn't have enough variability to catch errors. I used Balding Nichols generated data instead. Do we need to set the seed in the tests here?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14062:250,variab,variability,250,https://hail.is,https://github.com/hail-is/hail/pull/14062,1,['variab'],['variability']
Modifiability,"CHANGELOG: Hail Query-on-Batch previously used Class A Operations for all interaction with blobs. This change ensures that QoB only uses Class A Operations when necessary. Inspired by @jigold 's file system improvement campaign, I pursued the avoidance of ""list"" operations. I anticipate this reduces flakiness in Azure (which is tracked in #13351) and cost in Azure. I enforced aiotools.fs terminology on hail.fs and Scala:. 1. `FileStatus`. Metadata about a blob or file. It does not know if a directory exists at this path. 2. `FileListEntry`. Metadata from a list operation. It knows if a directory exists at this path. Variable names were updated to reflect this distinction:. 1. `fileStatus` / `fileStatuses`. 2. `fle`/ `fles` / `fileListEntry` / `fileListEntries`, respectively. `listStatus` renamed to `listDirectory` for clarity. In both Azure and Google, `fileStatus` does not use a list operation. `fileListEntry` can be used when we must know if a directory exists. I just rewrote this from first principles because:; 1. In neither Google nor Azure did it check if the path was a directory and a file.; 2. In Google, if the directory entry wasn't in the first page, it would fail (NB: there are fifteen non-control characters in ASCII before `/`, if the page size is 15 or fewer, we'd miss the first entry with a `/` at the end).; 3. In Azure, we issued both a get and a list. There are now unit tests for this method. ---. 1. `copyMerge` and `concatenateFiles` previously used `O(N_FILES)` list operations, they now use `O(N_FILES)` get operations.; 2. Writers that used `exists` to check for a _SUCCESS file now use a get operation.; 3. Index readers, import BGEN, and import plink all now check file size with a get operation. That said, overall, the bulk of our Class A Operations are probably writes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13885:624,Variab,Variable,624,https://hail.is,https://github.com/hail-is/hail/pull/13885,1,['Variab'],['Variable']
Modifiability,"CHANGELOG: Hail Query-on-Batch previously used Class A Operations for all interaction with blobs. This change ensures that QoB only uses Class A Operations when necessary. Inspired by @jigold 's file system improvement campaign, I pursued the avoidance of ""list"" operations. I anticipate this reduces flakiness in Azure (which is tracked in #13351) and cost in Azure. I enforced aiotools.fs terminology on hail.fs and Scala:. 1. `FileStatus`. Metadata about a blob or file. It does not know if a directory exists at this path. 2. `FileListEntry`. Metadata from a list operation. It knows if a directory exists at this path. Variable names were updated to reflect this distinction:. 1. `fileStatus` / `fileStatuses`. 2. `fle`/ `fles` / `fileListEntry` / `fileListEntries`, respectively. `listStatus` renamed to `listDirectory` for clarity. In both Azure and Google, `fileStatus` does not use a list operation. `fileListEntry` can be used when we must know if a directory exists. I just rewrote this from first principles because:; 1. In neither Google nor Azure did it check if the path was a directory and a file.; 2. In Google, if the directory entry wasn't in the first page, it would fail (NB: there are fifteen non-control characters in ASCII before `/`, if the page size is 15 or fewer, we'd miss the first entry with a `/` at the end).; 3. In Azure, we issued both a get and a list. There are now unit tests for this method. ---. 1. `copyMerge` and `concatenateFiles` previously used `O(N_FILES)` list operations, they now use `O(N_FILES)` get operations.; 2. Writers that used `exists` to check for a _SUCCESS file now use a get operation.; 3. Index readers, import BGEN, and import plink all now check file size with a get operation. That said, overall, the bulk of our Class A Operations are probably writes. fix test failures. passes tests. fixes. fix tests to not use fileStatus for folders. only file vs directory status matters. fix azure. azure dislikes %. finally get azure right. nix e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13883:624,Variab,Variable,624,https://hail.is,https://github.com/hail-is/hail/pull/13883,1,['Variab'],['Variable']
Modifiability,"CHANGELOG: Hail Query-on-Batch previously used Class A Operations for all interaction with blobs. This change ensures that QoB only uses Class A Operations when necessary. Inspired by @jigold 's file system improvement campaign, I pursued the avoidance of ""list"" operations. I anticipate this reduces flakiness in Azure (which is tracked in #13351) and cost in Azure. I enforced aiotools.fs terminology on hail.fs and Scala:. 1. `FileStatus`. Metadata about a blob or file. It does not know if a directory exists at this path. 2. `FileListEntry`. Metadata from a list operation. It knows if a directory exists at this path. Variable names were updated to reflect this distinction:. 1. `fileStatus` / `fileStatuses`. 2. `fle`/ `fles` / `fileListEntry` / `fileListEntries`, respectively. `listStatus` renamed to `listDirectory` for clarity. In both Azure and Google, `fileStatus` does not use a list operation. `getFileListEntry` can be used when we must know if a directory exists. I just rewrote this from first principles because:; 1. In neither Google nor Azure did it check if the path was a directory and a file.; 2. In Google, if the directory entry wasn't in the first page, it would fail (NB: there are fifteen non-control characters in ASCII before `/`, if the page size is 15 or fewer, we'd miss the first entry with a `/` at the end).; 3. In Azure, we issued both a get and a list. There are now unit tests for this method. ---. 1. `copyMerge` and `concatenateFiles` previously used `O(N_FILES)` list operations, they now use `O(N_FILES)` get operations.; 2. Writers that used `exists` to check for a _SUCCESS file now use a get operation.; 3. Index readers, import BGEN, and import plink all now check file size with a get operation. That said, overall, the bulk of our Class A Operations are probably writes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13452:624,Variab,Variable,624,https://hail.is,https://github.com/hail-is/hail/pull/13452,1,['Variab'],['Variable']
Modifiability,"CHANGELOG: Hail `frozenlist` now has an eval-able `repr`. `hailtop.hail_frozenlist.frozenlist` previously inherited the `repr` of the `frozenlist` library:. > frozenlist([1, 2, 3]); <FrozenList(frozen=True, [1, 2, 3])>. With this change, I both use the fact that `frozen=True` for Hail frozenlists and use a printed form that is actually eval-able:. > frozenlist([1, 2, 3]); frozenlist([1, 2, 3]); > eval(repr(frozenlist([1, 2, 3]))); frozenlist([1, 2, 3])",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13422:106,inherit,inherited,106,https://hail.is,https://github.com/hail-is/hail/pull/13422,1,['inherit'],['inherited']
Modifiability,"CHANGELOG: Introduce `hailtop.fs` that makes public a filesystem module that works for local fs, gs, s3 and abs. This is now used as the `Backend.fs` for hail query but can be used standalone for Hail Batch users by `import hailtop.fs as hfs`. Still have to do the docs but a couple questions remain:. I create a hidden singleton `RouterFS` object so that is used by functions in `hailtop.fs`. Should this singleton also be used by the Hail Query backends when they are initialized? How do we propagate configuration information such as `requester_pays_bucket` to the FS?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12731:503,config,configuration,503,https://hail.is,https://github.com/hail-is/hail/pull/12731,1,['config'],['configuration']
Modifiability,CHANGELOG: Introduce `hl.fs.fast_stat` and `hl.hadoop_fast_stat` which use cheaper Class B Operations in Google Cloud Storage rather than Class A Operations. Users of `hl.hadoop_stat` and `hl.fs.stat` should consider switching. This PR extends https://github.com/hail-is/hail/pull/13883 into the public API.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13884:236,extend,extends,236,https://hail.is,https://github.com/hail-is/hail/pull/13884,1,['extend'],['extends']
Modifiability,CHANGELOG: Introduce `hl.fs.fast_stat` and `hl.hadoop_fast_stat` which use cheaper Class B Operations in Google Cloud Storage rather than Class A Operations. Users of `hl.hadoop_stat` and `hl.fs.stat` should consider switching. This PR extends https://github.com/hail-is/hail/pull/13885 into the public API.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13886:236,extend,extends,236,https://hail.is,https://github.com/hail-is/hail/pull/13886,1,['extend'],['extends']
Modifiability,"CHANGELOG: Mitigate #12936 in which VEP Dataproc clusters fail to start. The root cause is complex. Docker has a bug which prevents it from cleanly starting if it is *re* installed. Whatever Google is doing in Dataproc to configure their Docker ""component"" appears to trigger this bug. See for details: https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751. The basic fix is to sleep to allow the system to coalesce a bit and then to restart Docker.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13580:222,config,configure,222,https://hail.is,https://github.com/hail-is/hail/pull/13580,1,['config'],['configure']
Modifiability,CHANGELOG: Refactored VCF combiner to support other GVCF schemas.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8942:11,Refactor,Refactored,11,https://hail.is,https://github.com/hail-is/hail/pull/8942,1,['Refactor'],['Refactored']
Modifiability,"CHANGELOG: Remove memory leak in `BlockMatrix.to_matrix_table_row_major` and `BlockMatrix.to_table_row_major`. Also, make the cache size used in both methods configurable. The `ref` variable was holding entire blocks in memory for no reason. It was a; vestiage of debugging. Moreover, the configurable cache permits users to fine tune; memory usage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9501:158,config,configurable,158,https://hail.is,https://github.com/hail-is/hail/pull/9501,3,"['config', 'variab']","['configurable', 'variable']"
Modifiability,"CHANGELOG: Requester pays buckets now work in `hailtop.fs` and `hl.hadoop_*`. This has been broken since at least 0.2.115. I first check for an explicit argument or the standard hailctl configuration. If neither of those exist, I try to parse spark-defaults.conf with lots of error handling and warning.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13089:186,config,configuration,186,https://hail.is,https://github.com/hail-is/hail/pull/13089,1,['config'],['configuration']
Modifiability,"CHANGELOG: `hailctl batch submit` now propagates configuration environment variables to the submitted job. Copying the user's config file into the job feels hacky and rude. Now that everything is guaranteed to be using the `configuration_of` mechanism we can set the entire config through environment variables. Also, if the intention is that the job's environment should magically reflect what the user set up locally, we should use `configuration_of` so that `HAIL_BILLING_PROJECT=foo hailctl batch submit` actually works.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13506:49,config,configuration,49,https://hail.is,https://github.com/hail-is/hail/pull/13506,5,"['config', 'variab']","['config', 'configuration', 'variables']"
Modifiability,"CHANGELOG: `hl.Table.parallelize` is much more flexible and now successfully imports most Hail-compatible data. I really wanted to load the hail-is/hail pull requests into Hail. I did not want to specify; the types of all 271 fields. I souped up Hail's `impute_type`:. - If an empty array, set, dict or `None` appears at any nesting level, but a ""peer"" is non-empty and; non-missing, we accept the peer's type.; - We take the union of two struct types as long as they agree on their intersection.; - If we discover a dict that cannot be imputed as a Hail dict, we try to impute it as a struct. If you like this change, I'll add tests. Note: I had to change `HailType` to include `None`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10045:47,flexible,flexible,47,https://hail.is,https://github.com/hail-is/hail/pull/10045,1,['flexible'],['flexible']
Modifiability,"CHANGELOG: `hl.import_table` is up to twice as fast for small tables. The big change is optimizing for the single file, no filters case in which; we need not scan for the first extant row, that row *must* be in the first; partition, if it exists at all. Unfortunately there is no zero-RPC way to; determine the number of partitions in a table, so I must catch an error; about the lack of a zeroth partition. I also did some refactoring:. 1. Move some functions to a utility file and add lots of indents and newlines to make them readable.; 2. Use `hl.format` for constructing strings.; 3. Make `should_filter_line` into `should_remove_line` for clarity of name.; 4. Modify `should_remove_line` to use short-circuiting and/or instead of array folds.; 5. Modify `should_remove_line` to indicate (via returning None) when there are no filters enabled.; 6. Add types.; 7. Fix a bug where we assumed that `.collect()[0]` would be `None` if there were no values in the table. (It raises an error); 8. Deduplicate `hail.utils.deduplicate` (haha: I mean, there is already code for doing field dedupe)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11782:424,refactor,refactoring,424,https://hail.is,https://github.com/hail-is/hail/pull/11782,1,['refactor'],['refactoring']
Modifiability,"CHANGELOG: `hl.logistic_regression_rows`, `hl.poisson_regression_rows`, and `hl.skat` all now support configuration of the maximum number of iterations and the tolerance. I had to make some fixes to how we count the number of iterations. It was quite screwy. Now it should reliably report the correct number of iterations regardless of failure, non-convergence, or explosion. This was requested [on Zulip](https://hail.zulipchat.com/#narrow/stream/127634-Feature-Requests/topic/Convergence.20issues.20with.20hl.2Elogistic_regression_rows).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11759:102,config,configuration,102,https://hail.is,https://github.com/hail-is/hail/pull/11759,1,['config'],['configuration']
Modifiability,"CHANGELOG: make hail's optimization rewriting filters to interval-filters smarter and more robust. Completely rewrites ExtractIntervalFilters. Instead of matching against very specific patterns, and failing completely for things that don't quite match (e.g. an input is let bound, or the fold implementing ""locus is contained in a set of intervals"" is written slightly differently), this uses a standard abstract interpretation framework, which is almost completely insensitive to the form of the IR, only depending on the semantics. It also correctly handles missing key fields, where the previous implementation often produced an unsound transformation of the IR. Also adds a much more thorough test suite than we had before. At the top level, the analysis takes a boolean typed IR `cond` in an environment where there is a reference to some `key`, and produces a set `intervals`, such that `cond` is equivalent to `cond & intervals.contains(key)` (in other words `cond` implies `intervals.contains(key)`, or `intervals` contains all rows where `cond` is true). This means for instance it is safe to replace `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond)`. Then in a second pass it rewrites `cond` to `cond2`, such that `cond & (intervals.contains(key))` is equivalent to `cond2 & intervals.contains(key)` (in other words `cond` implies `cond2`, and `cond2 & intervals.contains(key)` implies `cond`). This means it is safe to replace the `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond2)`. A common example is when `cond` can be completely captured by the interval filter, i.e. `cond` is equivant to `intervals.contains(key)`, in which case we can take `cond2 = True`, and the `TableFilter` can be optimized away. This all happens in the function; ```scala; def extractPartitionFilters(ctx: ExecuteContext, cond: IR, ref: Ref, key: IndexedSeq[String]): Option[(IR, IndexedSeq[Interval])] = {; if (key.isEmpty) None; else {; val e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13355:110,rewrite,rewrites,110,https://hail.is,https://github.com/hail-is/hail/pull/13355,1,['rewrite'],['rewrites']
Modifiability,"CVE-2019-11245 is a vulnerability in k8s that causes some (all?); containers without a runAsUser configuration to run; as user id 0, i.e. root. Jupyter refuses to start as root.; This change enables Jupyter to start successfully.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6678:97,config,configuration,97,https://hail.is,https://github.com/hail-is/hail/pull/6678,1,['config'],['configuration']
Modifiability,"Can confirm that at least locally, the environment variable caused a table to be written with packed integers, and the tests passed with the flag on (I assume).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7821#issuecomment-595913767:51,variab,variable,51,https://hail.is,https://github.com/hail-is/hail/pull/7821#issuecomment-595913767,1,['variab'],['variable']
Modifiability,Can we combine the Hail Documentation Build Configuration with the regular Hail CI configuration?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/735:44,Config,Configuration,44,https://hail.is,https://github.com/hail-is/hail/issues/735,2,"['Config', 'config']","['Configuration', 'configuration']"
Modifiability,"Can we make it a job and/or batch level configuration? The user obviously can do whatever they like with their tokens. However, since most users don't need them, I prefer our default to be to not expose them. I think we just need a new method and attribute on `Job` and `Batch` that mirrors, say, `image` or `memory`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-767739020:40,config,configuration,40,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-767739020,1,['config'],['configuration']
Modifiability,"Can you double check we don't need to explicitly tell nginx to use more than 1 core? I'm looking here:. https://www.nginx.com/blog/thread-pools-boost-performance-9x/#Configuring-Thread-Pools; https://www.nginx.com/blog/thread-pools-boost-performance-9x/#Benchmarking. Otherwise, I think this change is fine, although we do already have a minimum of two copies of internal-gateway at any time. Is this change better than increasing the number of copies of internal-gateway? I assume that the response time to increases in load will be faster with your proposed change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11876#issuecomment-1145004769:166,Config,Configuring-Thread-Pools,166,https://hail.is,https://github.com/hail-is/hail/pull/11876#issuecomment-1145004769,1,['Config'],['Configuring-Thread-Pools']
Modifiability,Can you respond to my concerns in the initial commit message? Specifically some math questions when computing resources and the AsyncWorkerPool usage and the temp variable in the SQL code.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9832#issuecomment-758019254:163,variab,variable,163,https://hail.is,https://github.com/hail-is/hail/pull/9832#issuecomment-758019254,1,['variab'],['variable']
Modifiability,Case class equals does a reference equality check as you expected. I switched to structural equality in the rewrite functions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1778#issuecomment-302164339:108,rewrite,rewrite,108,https://hail.is,https://github.com/hail-is/hail/pull/1778#issuecomment-302164339,1,['rewrite'],['rewrite']
Modifiability,Changed to use the environment variable directly. I also updated the PR description with details on how to install mkl and make Hail load it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10770#issuecomment-897042440:31,variab,variable,31,https://hail.is,https://github.com/hail-is/hail/pull/10770#issuecomment-897042440,1,['variab'],['variable']
Modifiability,"Changes since last review:; - Method now takes expressions for call and (optionally) scores.; - Block matrix and table of scores annotated and collected from source.cols() sent to Python, processed using int indices, column names restored on python side (thanks @tpoterba); - Fixed bug that silently dropped `n_samples / block_size` proportion of pairs, Python test checks it; - Extended Python tests to compare k and scores paths, test counts, min_kinship, maf, block_size; - Tuned tolerances on comparison with R from Python; - Extended to general column key, removing unique key check, noted in docs; - MEMORY_AND_DISK caching as default (thanks @konradjk) on Scala side; - The diagonal fix meant phi is computed with parallelism up to the number of diagonal blocks, rather than parallelism 1. But that's still likely a bottleneck as phi requires computing and point-wise dividing two big gram matrices. I now write phi to disk and read it back in, which squares the parallelism up to the number of blocks in phi. I think this should also improve the stability of the many downstream calculations derived from phi, esp. if pre-emptibles are used. No longer cacheing phi, but I left caching on the other matrices. @konradjk let us know how this version compares next time you run it.; - Noted in FIXME room for further improvement when fusing blocks: `replace join with zipPartitions, throw away lower triangular blocks sooner, avoid the nulls`; - Updated docs accordingly; - Deleted a bunch of code in PCRelate and PCRelateSuite",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3211#issuecomment-376725104:379,Extend,Extended,379,https://hail.is,https://github.com/hail-is/hail/pull/3211#issuecomment-376725104,2,['Extend'],['Extended']
Modifiability,"Changes the first argument that Emit expects to be a SparkFunctionContext, which currently holds a region and a SparkEnv (currently a stub; will be fleshed out as we start writing code to call back into Spark.) This should let us be more flexible in our ability to pass other necessary (non-IR-value) inputs, such as a hadoop configuration, to the function without relying on function argument ordering and accounting. builds on #5457.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5459:238,flexible,flexible,238,https://hail.is,https://github.com/hail-is/hail/pull/5459,2,"['config', 'flexible']","['configuration', 'flexible']"
Modifiability,"Changes to make sure that only the annotation datasets are visible on the docs page, now that the `datasets.json` config file contains all available datasets. Overview:. - In `datasets.json`, moved ""key_properties"" inside an ""annotation_db"" field, like `""annotation_db"": {""key_properties"": []}`, so that only the datasets with the ""annotation_db"" key are shown in the annotation DB docs page. Removed ""key_properties"" from non-annotation datasets. - Minor reformatting changes to docs page, added a reference genome column to the HTML table. - Updated deploy script to reflect the filename change from `annotation_db.json` to `datasets.json`. - Modified checks for keys in dicts from `assert key in doc, doc` to `assert key in doc` in `DatasetVersion.from_json()` and `Dataset.from_name_and_json()`. Since the `doc` that is passed to these methods from the checked in JSON file is just a dict like `doc = {""annotation_db"": {""key_properties"": [...]}, ""description"": ..., ""url"": ..., ""versions"": [...]}` this seems to work fine. Let me know if `key in doc, doc` form was used for other reasons I've overlooked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9546:114,config,config,114,https://hail.is,https://github.com/hail-is/hail/pull/9546,1,['config'],['config']
Modifiability,"Changes:; - removed unused py4jVersion from build.gradle; - pin breeze native version to version required by spark. This was not easy! I do it by creating a configuration that just depends on Spark, and then a resolution rule for all configurations that says only accept the breeze natives version corresponding to the version requested by spark.; - determine sparkMajorVersion from sparkVersion (strip patch version); - in docs, everywhere we use SPARK_VERSION, also specify SCALA_VERSION. I verified 2.4.0.cloudera is built against Scala 2.11.; - added SCALA_VERSION to hail/Makefile; - make Makefile versions match build.gradle defaults (somewhat annoying they are duplicated)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8680:157,config,configuration,157,https://hail.is,https://github.com/hail-is/hail/pull/8680,2,['config'],"['configuration', 'configurations']"
Modifiability,Ci is currently failing because this variable is not defined.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6641:37,variab,variable,37,https://hail.is,https://github.com/hail-is/hail/pull/6641,1,['variab'],['variable']
Modifiability,Citation for log4j1 programmatic configuration breaking log4j2: https://logging.apache.org/log4j/2.x/manual/migration.html#limitations-of-the-log4j-1-x-bridge,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1524328047:33,config,configuration,33,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1524328047,1,['config'],['configuration']
Modifiability,Closed because I want to rewrite it to take an environment VDS,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/929#issuecomment-253989005:25,rewrite,rewrite,25,https://hail.is,https://github.com/hail-is/hail/pull/929#issuecomment-253989005,1,['rewrite'],['rewrite']
Modifiability,Closes hail-is/hail-tasks#2. @danking Can you take a look at this before I start testing? I think the query/log4j.properties file still needs to be there to configure the logs that show up for the JVM and not the user's jobs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471:157,config,configure,157,https://hail.is,https://github.com/hail-is/hail/pull/11471,1,['config'],['configure']
Modifiability,"Closes https://github.com/hail-is/hail/issues/14485. Tested that this works by deploying the branch to my dev namespace and pointing my dev config at it:. ```bash; hailctl dev deploy -b iris-garden/hail:batch/deprecated-apis -s deploy_batch,add_developers; hailctl dev config set default_namespace irademac; ```. And then running the following:. ```python; from hailtop.batch import ServiceBackend; batch_client = await ServiceBackend(billing_project='test', remote_tmpdir='gs://irademac/test/')._batch_client(); # one of the deprecated endpoints; await batch_client._get(""/api/v1alpha/batches/402/jobs/1/log""); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14621:140,config,config,140,https://hail.is,https://github.com/hail-is/hail/pull/14621,2,['config'],['config']
Modifiability,Closing as this will evolve in separate branch for a while yet.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1697#issuecomment-298819643:21,evolve,evolve,21,https://hail.is,https://github.com/hail-is/hail/pull/1697#issuecomment-298819643,1,['evolve'],['evolve']
Modifiability,Closing this for now. Didn't realize the bootstrap dependence of not having a letsencrypt config. I don't think we can entirely delete this just yet (but could trivially move it to envoy),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12096#issuecomment-1226092951:90,config,config,90,https://hail.is,https://github.com/hail-is/hail/pull/12096#issuecomment-1226092951,1,['config'],['config']
Modifiability,"Closing this for now. I'll redo it in a way I think will be acceptable (or not, it's really just a small hill I care about, but I would like to point out that `gcloud` stores its credential and config data in `.config/`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-540740392:194,config,config,194,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-540740392,2,['config'],['config']
Modifiability,Closing this in favor of refactoring `TableStage` to do the whole right thing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8599#issuecomment-618680964:25,refactor,refactoring,25,https://hail.is,https://github.com/hail-is/hail/pull/8599#issuecomment-618680964,1,['refactor'],['refactoring']
Modifiability,Closing to PR build config with enabled ci2.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5842#issuecomment-487309837:20,config,config,20,https://hail.is,https://github.com/hail-is/hail/pull/5842#issuecomment-487309837,1,['config'],['config']
Modifiability,"Closing to refactor, will open updated PR shortly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3397#issuecomment-386605198:11,refactor,refactor,11,https://hail.is,https://github.com/hail-is/hail/pull/3397#issuecomment-386605198,1,['refactor'],['refactor']
Modifiability,Closing until Eigen PR is ready again and this is refactored accordingly.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2177#issuecomment-326646865:50,refactor,refactored,50,https://hail.is,https://github.com/hail-is/hail/pull/2177#issuecomment-326646865,1,['refactor'],['refactored']
Modifiability,Closing until I have time to rewrite using recursively passed EvalContexts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1637#issuecomment-298932795:29,rewrite,rewrite,29,https://hail.is,https://github.com/hail-is/hail/pull/1637#issuecomment-298932795,1,['rewrite'],['rewrite']
Modifiability,Closing while I refactor.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1011#issuecomment-257051213:16,refactor,refactor,16,https://hail.is,https://github.com/hail-is/hail/pull/1011#issuecomment-257051213,1,['refactor'],['refactor']
Modifiability,Closing while this evolves.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2965#issuecomment-369418966:19,evolve,evolves,19,https://hail.is,https://github.com/hail-is/hail/pull/2965#issuecomment-369418966,1,['evolve'],['evolves']
Modifiability,"Closing, will incorporate into refactor of linreg on dev.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2007#issuecomment-318852681:31,refactor,refactor,31,https://hail.is,https://github.com/hail-is/hail/pull/2007#issuecomment-318852681,1,['refactor'],['refactor']
Modifiability,"Closing, will make new PR after refactoring against dev.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2037#issuecomment-318852593:32,refactor,refactoring,32,https://hail.is,https://github.com/hail-is/hail/pull/2037#issuecomment-318852593,1,['refactor'],['refactoring']
Modifiability,"Code looks good. Can you update the VariantDataset.vep function documentation? In particular, note that plugin overrides human_ancestor and conservation file. plugin in cleaner. We should remove the latter when we start version 0.2: https://github.com/hail-is/hail/issues/1728",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1712#issuecomment-297784358:104,plugin,plugin,104,https://hail.is,https://github.com/hail-is/hail/pull/1712#issuecomment-297784358,2,['plugin'],['plugin']
Modifiability,Code looks good. It looks like there's something wrong with the gradle cpp test configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4594#issuecomment-433123340:80,config,configuration,80,https://hail.is,https://github.com/hail-is/hail/pull/4594#issuecomment-433123340,1,['config'],['configuration']
Modifiability,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468:176,extend,extend,176,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468,2,['extend'],['extend']
Modifiability,Configuration can now be stored in the metadata top-level JSON (e.g. split).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/368#issuecomment-236621515:0,Config,Configuration,0,https://hail.is,https://github.com/hail-is/hail/issues/368#issuecomment-236621515,1,['Config'],['Configuration']
Modifiability,Configure feature flags with environment variables,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8256:0,Config,Configure,0,https://hail.is,https://github.com/hail-is/hail/pull/8256,2,"['Config', 'variab']","['Configure', 'variables']"
Modifiability,Configure git user name and email in deploy.sh,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11351:0,Config,Configure,0,https://hail.is,https://github.com/hail-is/hail/pull/11351,1,['Config'],['Configure']
Modifiability,"Configure spark version once, re-use in future builds",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613:0,Config,Configure,0,https://hail.is,https://github.com/hail-is/hail/pull/1613,1,['Config'],['Configure']
Modifiability,Confirmed that the hailctl bit works:; ```; (base) dking@wm28c-761 hail % hailctl config set http/timeout_in_seconds 1234s; Error: bad value '1234s' for parameter <ConfigVariable.HTTP_TIMEOUT_IN_SECONDS: 'http/timeout_in_seconds'> should be a float or an int like 42.42 ; or 42; (base) dking@wm28c-761 hail % hailctl config set http/timeout_in_seconds 42 ; (base) dking@wm28c-761 hail % hailctl config set http/timeout_in_seconds 42.0; (base) dking@wm28c-761 hail % hailctl config set http/timeout_in_seconds 60 ; (base) dking@wm28c-761 hail % cat ~/.config/hail/config.ini ; [query]; backend = spark; jar_url = gs://hail-query-ger0g/jars/dking/uk4prwgezgva/5fc88d5a4b614454004226f5c77ea72efee1e38f.jar. [batch]; remote_tmpdir = gs://1-day/; billing_project = hail; backend = service. [aiocloudflare]. [global]; domain = hail.is. [gcs_requester_pays]; project = broad-ctsa. [http]; timeout_in_seconds = 60. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14206#issuecomment-1915030577:82,config,config,82,https://hail.is,https://github.com/hail-is/hail/pull/14206#issuecomment-1915030577,7,"['Config', 'config']","['ConfigVariable', 'config']"
Modifiability,"Contains one piece of a fix for #5262. This refactor was initially undertaken to resolve the above issue, which; manifested due to the nested stack frames created by the IR renderer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5465:44,refactor,refactor,44,https://hail.is,https://github.com/hail-is/hail/pull/5465,1,['refactor'],['refactor']
Modifiability,"Context.java:343); 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911); 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); 	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); 	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); 	at java.lang.Thread.run(Thread.java:748); </details>. <details>; <summary>Working hail.log</summary>. ```; 2018-10-09 15:04:33 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 15:04:33 Hail: INFO: Running Hail version devel-17a988f2a628; 2018-10-09 15:04:33 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 15:04:33 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 15:04:33 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@16ba3696{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2780d0b8{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7cea1161{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@696b1f0{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:13610,config,config,13610,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['config'],['config']
Modifiability,Correct website URL in Zenodo config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8998:30,config,config,30,https://hail.is,https://github.com/hail-is/hail/pull/8998,1,['config'],['config']
Modifiability,"Cotton, mostly looks great, I haven't taken a look at the deployment configs yet. I would like to do that, and if you're ok with this from a time standpoint, spin up a locally deployed version to play with. . Nice work!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-528373663:69,config,configs,69,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-528373663,1,['config'],['configs']
Modifiability,"Created IR nodes for reading, writing and adding BlockMatrix objects as well as a BlockMatrixLiteral node to interop with functionality that doesn't use the IR. Refactored `blockmatrix.py` to stay fully functional while methods are converted into the IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5148:161,Refactor,Refactored,161,https://hail.is,https://github.com/hail-is/hail/pull/5148,1,['Refactor'],['Refactored']
Modifiability,"Creating network namespaces can often take hundreds of milliseconds (and sometimes seconds with `iptables` contention), so Batch takes this off the job hot path by pre-allocating namespaces. All job namespaces are configured identically and there is a fixed number of ""slots"" on any batch worker (`CORES * 4`), so pre-allocation and asynchronous recycling of namespaces is fairly straight-forward so long as we never attempt to run more containers on a worker than the number of slots (which the scheduling system should prohibit). However, since we started running long-lived JVM containers (#11397), the number of containers running on a given worker can easily be *greater* than `N_SLOTS`. On a 16-core machine, we create 30 JVMs that sit idle waiting for JVMJobs all the while occupying a precious network namespace. I thought for the longest time that #13402 was a race condition so was trying to trigger it through a barrage of quick jobs. Turns out all it took was running >34 long-running jobs on a single 16-core worker. In a dev deploy of `main`, running a batch with 35 quarter-core `sleep 150` jobs fails with a single job timing out waiting for a network. On this branch, I am able to run the same 35 job batch as well as a batch with 64 quarter-core jobs. Unfortunately, we don't have a great way to test ""run all these jobs at once on the same worker"". Resolves #13402",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13678:214,config,configured,214,https://hail.is,https://github.com/hail-is/hail/pull/13678,1,['config'],['configured']
Modifiability,"Crosslink to dataproc docs that explain how to start a VEP cluster, mention that the config info is not necessary if you're using dataproc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8885:85,config,config,85,https://hail.is,https://github.com/hail-is/hail/pull/8885,1,['config'],['config']
Modifiability,"Current implementation allows to specify covariates, but does not output coefficients for the covariates.; I suggest implementing a linereg, where the X variable of interest can be specified.; E.g. . linreg -y sa.pheno.height -c sa.cov.age,sa.cov.isMale -x sa.isblueEyes. In this case, it just run 1 linear regression without using the genotype data. another important addition would be to specify the link function.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/375:153,variab,variable,153,https://hail.is,https://github.com/hail-is/hail/issues/375,1,['variab'],['variable']
Modifiability,"Currently `TableRead.execute` always produces a `TableValueIntermediate`, even though almost all `TableReader`s are lowerable, so could produce a `TableStageIntermediate`. This pr refactors `TableReader` to allow producing a `TableStageIntermediate` in most cases, and to make it clearer which readers still need to be lowered (only `TableFromBlockMatrixNativeReader`, `MatrixVCFReader`, and `MatrixPLINKReader`). It also deletes some now dead code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13229:180,refactor,refactors,180,https://hail.is,https://github.com/hail-is/hail/pull/13229,1,['refactor'],['refactors']
Modifiability,"Currently the auth service uses its own authentication decorators instead of using those that the other services do (because those make a request to auth which feels a bit circular), but now we have two sets of decorators which we want to mostly keep in step aside from where they get their `UserData` from. This PR creates a small abstraction with an `Authenticator` base class and a `AuthServiceAuthenticator` subclass that extends it and fetches userdata from the auth service. The auth service instead uses a `LocalAuthenticator` which instead of making an rpc directly fetches the userdata from its database. (I would recommend using the split diff)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13672:426,extend,extends,426,https://hail.is,https://github.com/hail-is/hail/pull/13672,1,['extend'],['extends']
Modifiability,"Currently the` nvidia-container-toolkit` is installed both in the vm startup script and in the worker docker image. The toolkit must be installed in the startup script to be able to configure docker with the command `nvidia-ctk runtime configure --runtime=docker`. This command cannot be run from Dockerfile.worker because it gets the error `""unable to flush config: unable to open /etc/docker/daemon.json for writing: open /etc/docker/daemon.json: no such file or directory""`.; The toolkit also has to be installed in Dockerfile.worker since that is where crun is invoked from. To execute the nvidia hook, the toolkit needs to be installed in that container. We could probably find a workaround to this if you would like but it only increased the worker image from 1.47Gb to 1.55Gb so it seems pretty small.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13430#issuecomment-1710796481:182,config,configure,182,https://hail.is,https://github.com/hail-is/hail/pull/13430#issuecomment-1710796481,3,['config'],"['config', 'configure']"
Modifiability,"Currently you can't do a dev deploy that only builds an image, because the `deployed_services` method breaks if there are no `deploy_*` steps in the config. This change allows a dev deploy to work with just steps that don't transitively depend on the create namespace step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12619:149,config,config,149,https://hail.is,https://github.com/hail-is/hail/pull/12619,1,['config'],['config']
Modifiability,"Currently, `hail.ggplot.geom_point` generates cross-product-style legends when the `shape` and `color` aesthetics each have multiple groups. This change makes that behavior consistent with `ggplot2`'s implementation, producing legends where the groups are factored out. Because the strategy used to accomplish this does not preserve the interactive features of the legends, such as toggling groups' visibility, this change also makes all plots generated via `hail.ggplot.geom_point` [static](https://plotly.com/python/configuration-options/#making-a-static-chart). For example, before this change, this code:. ```python; import hail as hl; from hail.ggplot import ggplot, aes, geom_point; ht = hl.utils.range_table(10); ht = ht.annotate(squared=ht.idx ** 2); ht = ht.annotate(even=hl.if_else(ht.idx % 2 == 0, ""yes"", ""no"")); ht = ht.annotate(threeven=hl.if_else(ht.idx % 3 == 0, ""good"", ""bad"")); fig = (; ggplot(ht, aes(x=ht.idx, y=ht.squared)); + geom_point(aes(color=ht.even, shape=ht.threeven)); ); fig.show(); ```. Generates this legend:. <img width=""107"" alt=""Screen Shot 2022-09-29 at 12 22 57"" src=""https://user-images.githubusercontent.com/84595986/193085964-e4545e78-473f-46a3-8c8c-7d6189eb7adc.png"">. After the change, this legend is generated:. <img width=""102"" alt=""Screen Shot 2022-10-03 at 13 56 01"" src=""https://user-images.githubusercontent.com/84595986/193645748-b02ec35c-37c0-400e-b4d6-5c11a5d8df8c.png"">. Custom labels can be used by updating the code like so:; ```python; ...; from hail.ggplot import ggplot, aes, geom_point, labs; ...; fig = (; ggplot(ht, aes(x=ht.idx, y=ht.squared)); + geom_point(aes(color=ht.even, shape=ht.threeven)); + labs(color=""Even"", shape=""Threeven""); ); ...; ```. Generating this legend:. <img width=""106"" alt=""Screen Shot 2022-10-03 at 13 58 34"" src=""https://user-images.githubusercontent.com/84595986/193646267-f935b880-94fe-4a1e-a8a8-b0f850b54a86.png"">. For more information on the current behavior, see #12244 and #12207.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12254:518,config,configuration-options,518,https://hail.is,https://github.com/hail-is/hail/pull/12254,1,['config'],['configuration-options']
Modifiability,"Currently, `hailctl curl` uses `external_url` instead of `url`. As a result,; if `hailctl curl` is used inside a GCE VM or on a k8s pod, the url will always; be `....hail.is` to which GCE VMs and k8s pods likely lack credentials. This was a mistake when I first wrote curl. At that time, I was only using it for; local testing. It will still work for local testing because our deploy configs are; all `external`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8584:384,config,configs,384,https://hail.is,https://github.com/hail-is/hail/pull/8584,1,['config'],['configs']
Modifiability,"Currently, attempting to start a Dataproc cluster without either a region argument or a configured `dataproc/region` results in a long error message `subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'clusters', 'create', ... ]' returned non-zero exit status 1` with the actual cause obscured above the traceback. That cause is:; ```; Failed to find attribute [region]. The attribute can be set in the following ways:; - provide the argument [--region] on the command line; - set the property [dataproc/region]; ```. There is some logic to show a nicer error message if no region is provided. However, that is only shown if `gcloud config get-value dataproc/region` fails. When `dataproc/region` is not set, that command succeeds and outputs an empty string. This change handles that case and shows the nicer error message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8791:88,config,configured,88,https://hail.is,https://github.com/hail-is/hail/pull/8791,2,['config'],"['config', 'configured']"
Modifiability,"Currently, jobs in hail batch can only be run on n1 machines but with the rise of deep learning in bioinformatics, the ability to run jobs on g2 machines, as well as other GPU supported machines, is an important and exciting addition to hail batch. This PR highlights the steps needed to add new machine types into hail batch and could be used as a template for further development support. . The changes in this PR can broadly be divided into additions to the job crun container and insertion of g2 resources (CPU, RAM, L4 Accelerator) into the resources table for billing. This PR uses the NVIDIA Container Toolkit, which allows the creation of GPU accelerated containers. This toolkit is integrated with docker via the parameters —runtime=nvidia and the specification of GPUs is made through —gpus all. The toolkit is installed in the batch worker VM startup script and the corresponding docker parameters are configured if the machine type is g2, so there is no change to the docker configuration for n1 machines. For the toolkit to work there is a nvidia hook that needs to be injected into the crun config. These modifications are also done based on machine type. On the billing side, the existing pricing setup was expanded to include g2 machines. The g2 instance cores and RAM are inserted into the database, and the SKUs are hard coded. For future machine type incorporation or updates, [https://cloud.google.com/skus/?currency=USD&filter=](https://cloud.google.com/skus/?currency=USD&filter=) may serve as a useful resource to identify relevant SKU ids. A new resource type was also added for the accelerator, including preemptible and non-preemtible. Finally, g2 machines mount the worker data disk under the name nvme0n2 so the code is updated to reflect this. Future work may want to investigate a way to automatically detect what the proper disk name is or make the disk naming logic more robust.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13430:913,config,configured,913,https://hail.is,https://github.com/hail-is/hail/pull/13430,3,['config'],"['config', 'configuration', 'configured']"
Modifiability,"Currently, multiple users updating the pool config on the pool pages on the batch-driver website can have unexpected behavior. For instance, if two users simultaneously go to the batch-driver website and update the pool config one after the other, the net result is that the second config update will override the first with neither user knowing that two updates happened. This PR addresses this issue by ensuring that the current pool config that a client sees is consistent with what the pool config that the server has, before processing an update. - Add hidden input to pool.html to store the current client config json; - Change config-update route to get current client pool config and proposed pool config from POST request body; - Compare current client config with current server config and if they're different, then reject the proposed pool config",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11798:44,config,config,44,https://hail.is,https://github.com/hail-is/hail/pull/11798,12,['config'],"['config', 'config-update']"
Modifiability,"Currently, tasks to schedule new instances are put on the event loop inside the `Pool` and `JobPrivateInstanceManager` constructors. `Pool.create` and `JobPrivateInstanceManager.create` first instantiate an object of their respective type and then load existing instances from the database into the in-memory instance collection. This could potentially cause the create instances loop to trigger while we're drawing ""existing"" instances, which causes the assertion error in https://github.com/hail-is/hail-tasks/issues/24 when the create instances loop and load instances query race to add the instance to the in-memory data structure. This change moves the task creation from the constructor to the `create` method, so we don't start creating instances until all existing instances are accounted for. I think I would have liked to simply pass the constructor a list of instances, but we can't create an `Instance` without an `InstanceCollection`. Resolves hail-is/hail-tasks#24. I also threw in a bit of cleanup, i.e. removing some variable assignments that didn't seem very helpful and resolving a lint issue where we used `items` where we could just use `values`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11766:1033,variab,variable,1033,https://hail.is,https://github.com/hail-is/hail/pull/11766,1,['variab'],['variable']
Modifiability,"Currently, the Grafana service deployed with the Hail environment is behind two layers of authentication, since the Grafana NGINX configuration proxies requests to it through the `/auth` route, and the login screen built into Grafana also displays. This change removes the second login screen. Demo at https://internal.hail.is/irademac/grafana.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12192:80,layers,layers,80,https://hail.is,https://github.com/hail-is/hail/pull/12192,2,"['config', 'layers']","['configuration', 'layers']"
Modifiability,"Currently, the k8s namespace field is used both for routing internal requests inside kubernetes but also external requests over the internet. It also has special logic based on whether the namespace indicates a production or dev environment. For example, if `namespace == 'default'`, then we route external `batch` requests to `batch.<domain>/path`, but if `namespace == foo_dev`, we route external `batch` requests to `internal.<domain>/foo_dev/path`. This PR decouples the namespace field from routing. Aside from being overall more straightforward in my opinion, this is necessary for batch on azure terra where batch is served out of a subpath it does not control and is unrelated to whatever namespace it might reside in. The guiding principle for routing is then as follows: If the config has no subpath, use a subdomain, otherwise put everything under domain + subpath. For example:; - `{'domain': 'hail.is', 'subpath': null}` => `batch.hail.is`; - `{'domain': 'internal.hail.is', 'subpath': '/foo_dev'}` => `internal.hail.is/foo_dev/batch`. Since the CI pipeline runs on current production instances, there is a minor need to stay compatible with old deploy configs (or else hack up the CI build.yaml). It's quite a simple translation though, because if there is no subpath provided we can infer one based on the `default_namespace`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14056:788,config,config,788,https://hail.is,https://github.com/hail-is/hail/pull/14056,2,['config'],"['config', 'configs']"
Modifiability,"Currently, the test_batch fails for local users. These changes enable test_batch to work for local users with minimal configuration. The only necessary step is for a user to execute:; ```; hailctl config set batch/billing_project hail; ```; All other steps are handled by the test suite, including uploading test data if it does not already exist. I believe this obsoletes `hail-services` bucket. Is that correct?. The main change necessary to support this was a Hail configuration system. There is now a file stored in an [XDG acceptable](https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html) location to which we can read and write sectioned key-value pairs. The `ServiceBackend` looks in this configuration file if the billing_project is unspecified. The file format is defined by the INI-like configuration file library, [`configparser`](https://docs.python.org/3/library/configparser.html#). `configparser` is included in Python. cc: @cseed your thoughts on `hailctl config` appreciated. I think we'll use this for the query service as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8559:118,config,configuration,118,https://hail.is,https://github.com/hail-is/hail/pull/8559,9,['config'],"['config', 'configparser', 'configuration']"
Modifiability,DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RVD.scala:688); 			at is.hail.expr.ir.Interpret$.run(Interpret.scala:804); 			at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:39); 			at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 			at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 			at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 			at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 			at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 			at is.hail.expr.ir.lowering.InterpretNonCom,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:12416,rewrite,rewrite,12416,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['rewrite'],['rewrite']
Modifiability,DDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029); 	at is.hail.backend.spark.SparkBackend.parallelizeAndComputeWithIndex(SparkBackend.scala:355); 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:43); 	at __C793Compiled.__m916begin_group_0(Emit.scala); 	at __C793Compiled.__m794split_Let(Emit.scala); 	at __C793Compiled.apply(Emit.scala); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$3(CompileAndEvaluate.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:57); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:9207,rewrite,rewrite,9207,https://hail.is,https://github.com/hail-is/hail/issues/13287,1,['rewrite'],['rewrite']
Modifiability,"Dan's OOO this week. It's a Python lint failure:; ```; + cd /io/repo; + make check-hail; make -C hail/python check; make[1]: Entering directory '/io/repo/hail/python'; python3 -m flake8 --config ../../setup.cfg hail; python3 -m flake8 --config ../../setup.cfg hailtop; hailtop/aiogoogle/auth/credentials.py:43:1: W293 blank line contains whitespace; hailtop/aiogoogle/auth/credentials.py:47:5: E303 too many blank lines (2); hailtop/aiogoogle/auth/credentials.py:105:1: E302 expected 2 blank lines, found 1; make[1]: Leaving directory '/io/repo/hail/python'; make[1]: *** [Makefile:12: check] Error 1; make: *** [Makefile:13: check-hail] Error 2; Status; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10647#issuecomment-876439207:188,config,config,188,https://hail.is,https://github.com/hail-is/hail/pull/10647#issuecomment-876439207,2,['config'],['config']
Modifiability,"DatabaseStep(Step):; self._name = database_name; self.admin_username = f'{database_name}-admin'; self.user_username = f'{database_name}-user'; + elif params.scope == 'dev':; + dev_username = params.code.config()['user']; + self._name = f'{dev_username}-{database_name}'; + self.admin_username = f'{dev_username}-{database_name}-admin'; + self.user_username = f'{dev_username}-{database_name}-user'; else:; assert params.scope == 'test'; self._name = f'{params.code.short_str()}-{database_name}-{self.token}'; @@ -1030,7 +1032,7 @@ class CreateDatabaseStep(Step):; @staticmethod; def from_json(params: StepParameters):; json = params.json; - return CreateDatabaseStep(; + return CreateDatabase2Step(; params,; json['databaseName'],; json['namespace'],; @@ -1111,12 +1113,12 @@ EOF; attributes={'name': self.name},; secrets=[; {; - 'namespace': self.database_server_config_namespace,; + 'namespace': self.namespace,; 'name': 'database-server-config',; 'mount_path': '/sql-config',; }; ],; - service_account={'namespace': DEFAULT_NAMESPACE, 'name': 'ci-agent'},; + service_account={'namespace': self.namespace, 'name': 'admin'},; input_files=input_files,; parents=[self.create_passwords_job] if self.create_passwords_job else self.deps_parents(),; network='private',; @@ -1125,42 +1127,4 @@ EOF; ); ; def cleanup(self, batch, scope, parents):; - if scope in ['deploy', 'dev'] or self.cant_create_database:; - return; -; - cleanup_script = f'''; -set -ex; -; -commands=$(mktemp); -; -cat >$commands <<EOF; -DROP DATABASE IF EXISTS \\`{self._name}\\`;; -DROP USER IF EXISTS '{self.admin_username}';; -DROP USER IF EXISTS '{self.user_username}';; -EOF; -; -until mysql --defaults-extra-file=/sql-config/sql-config.cnf <$commands; -do; - echo 'failed, will sleep 2 and retry'; - sleep 2; -done; -; -'''; -; - self.cleanup_job = batch.create_job(; - CI_UTILS_IMAGE,; - command=['bash', '-c', cleanup_script],; - attributes={'name': f'cleanup_{self.name}'},; - secrets=[; - {; - 'namespace': self.database_ser",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600:2408,config,config,2408,https://hail.is,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600,2,['config'],['config']
Modifiability,Dataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFun,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:7713,Config,Configuration,7713,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,"Defines a TStream/PStream type stub. I've omitted some number of things that other types need to define, as the purpose of the stream type is going to be to ensure that we're never fully instantiating collections where we shouldn't be, e.g. all the rows in a table partition. To that end, I've omitted definitions for ordering since I don't forsee a need for ordering on the entire stream (as opposed to on the element, or a subset thereof), as well as generators for annotations, etc. It basically otherwise mimics the PArray/TArray definitions, but I've made it extend Type/PType directly since most of the extra methods on containers seem irrelevant to streams, having mostly to do with e.g. length and loading specific elements. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5610:564,extend,extend,564,https://hail.is,https://github.com/hail-is/hail/pull/5610,1,['extend'],['extend']
Modifiability,"Deleted the `gsutil cat` line, it wasn't doing anything because of that erroneous `/vep_data/Plugins.tar`. I don't think plugin needs to be included here, as it's in the docker image. . Deleted the `gsutil cp`, as the file it referenced did not exist. Grabbed the 1var.vcf from the already copied loftee_data. . It's not clear to me why we don't do the `1var.vcf` vep run stuff when using `GRCh38`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10370:93,Plugin,Plugins,93,https://hail.is,https://github.com/hail-is/hail/pull/10370,2,"['Plugin', 'plugin']","['Plugins', 'plugin']"
Modifiability,Depends -- we can extend the HWE aggregator to incorporate sample phenotype information or we can leave it to the user to filter out male samples.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/200#issuecomment-279521097:18,extend,extend,18,https://hail.is,https://github.com/hail-is/hail/issues/200#issuecomment-279521097,1,['extend'],['extend']
Modifiability,"Deploying grafana in our GKE cluster gives us instant and easy access to the stackdriver backend with the same querying capabilities of our current front-end, but without the clutter and insanely slow load times. See [here](https://internal.hail.is/dgoldste/grafana/d/TVkleyLMk/detailed-service-resource-utilization?orgId=1) for some example dashboards I set up to look at resources across our services (credentials are the default admin/admin). This alleviates the immediate pain of using the console (for metrics only, not logging), but my longer aim is that getting more regular use out of our metrics can reveal deeper pain points of our monitoring stack and if/where we need to eat up more responsibility from google monitoring. This is a StatefulSet, so configuration through the UI will persist and is done manually. If we find that our dashboards are stable and boilerplate enough, I'd like to move to a code-based dashboard configuration. Sadly, `check-yaml` does not appreciate our jinja templating in yaml, so I've removed it for now. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10013:760,config,configuration,760,https://hail.is,https://github.com/hail-is/hail/pull/10013,2,['config'],['configuration']
Modifiability,Dice came up @patrick-schultz. - Parameterize (de/en)coders by InputStream type which is either a Java input stream or an array of bytes.; - add `compileComparison` which produces the bytes of a system-specific executable for comparing two values of types `l` and `r` encoded in `codec`; - experimental python API to `compileComparison` to enable experimentation. The end goal is to run `compileComparison` and ship the bytes to the shuffle service which will use it as a comparison operation on the encoded keys.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6505:33,Parameteriz,Parameterize,33,https://hail.is,https://github.com/hail-is/hail/pull/6505,1,['Parameteriz'],['Parameterize']
Modifiability,Disable apiserver deploy for the moment. Remove everything related to Spark. Saved existing deploy configuration in apiserver/build.yaml_disabled.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6195:99,config,configuration,99,https://hail.is,https://github.com/hail-is/hail/pull/6195,1,['config'],['configuration']
Modifiability,Do not create hail/hailctl config file if it does not exist,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11671:27,config,config,27,https://hail.is,https://github.com/hail-is/hail/pull/11671,1,['config'],['config']
Modifiability,"Docs and a bunch of other updates are done. Ready for review!. Once it's in, I'll refactor linreg, logreg, and lmmreg commands to put the phenotype and covariate extraction logic in one place under stats.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1064#issuecomment-260219957:82,refactor,refactor,82,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-260219957,1,['refactor'],['refactor']
Modifiability,"Does not extend into modifying TableValue, as this affects the IR, relates to the boundary you spoke of I believe, and will affect much more besides IBD. Happy to start digging into that in this PR or the next PR if desired. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6415:9,extend,extend,9,https://hail.is,https://github.com/hail-is/hail/pull/6415,2,['extend'],['extend']
Modifiability,Does not/should not change current behavior; I'm just refactoring some code in anticipation of future changes needing the individual pieces instead of the whole.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5457:54,refactor,refactoring,54,https://hail.is,https://github.com/hail-is/hail/pull/5457,1,['refactor'],['refactoring']
Modifiability,"Does the version of spark matter? such as apache spark 2.0.2 and the cloudera spark?; We use the cloudera hadoop,but for hail, the cloudera'spark can't work,so in the configuration we replaced the cloudera spark with the apache spark2.0.2,and this works in local mode,but have errors in cluster mode",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321241969:167,config,configuration,167,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321241969,2,['config'],['configuration']
Modifiability,"Don't split variable-length encoded ints across compression blocks. Save a comparison in the inner loop. As usual, has seemingly negligible effect on performance. I'm sure it would be significant on the C side. I don't encode as signed yet. It makes the termination condition more complicated and I want to think on it a bit more.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2430:12,variab,variable-length,12,https://hail.is,https://github.com/hail-is/hail/pull/2430,1,['variab'],['variable-length']
Modifiability,Done in scalacheck rewrite.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/25#issuecomment-179404081:19,rewrite,rewrite,19,https://hail.is,https://github.com/hail-is/hail/issues/25#issuecomment-179404081,1,['rewrite'],['rewrite']
Modifiability,"Drat, it appears that pyspark doesn't work with Python 3.8. https://stackoverflow.com/a/58849063/342839. A simpler reproduction to demonstrate that this is a pyspark issues:. ```; snafu$ python -m pyspark.cloudpickle; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 185, in _run_module_as_main; mod_name, mod_spec, code = _get_module_details(mod_name, _Error); File ""/usr/lib/python3.8/runpy.py"", line 111, in _get_module_details; __import__(pkg_name); File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py"", line 51, in <module>; from pyspark.context import SparkContext; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py"", line 31, in <module>; from pyspark import accumulators; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py"", line 97, in <module>; from pyspark.serializers import read_int, PickleSerializer; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py"", line 71, in <module>; from pyspark import cloudpickle; File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py"", line 145, in <module>; _cell_set_template_code = _make_cell_set_template_code(); File ""/home/reece/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py"", line 126, in _make_cell_set_template_code; return types.CodeType(; TypeError: an integer is required (got type bytes); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197#issuecomment-800647452:500,sandbox,sandbox,500,https://hail.is,https://github.com/hail-is/hail/issues/10197#issuecomment-800647452,12,['sandbox'],['sandbox']
Modifiability,"E.g., replace spaces with underscores. Use regex rewrite.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/945:49,rewrite,rewrite,49,https://hail.is,https://github.com/hail-is/hail/issues/945,1,['rewrite'],['rewrite']
Modifiability,"EDIT2:. OK, so, I'm not sure when this behavior changed but Make 4.0 wants a `\` to indicate that the recipe continues on the next line *but also* passes that backslash and newline to the shell. In Make 3.81, the `\` was also required but the newline and backslash *are not passed* to the shell. In other words: in 3.81, backslash-newline is always replaced with a space and in 4.0, backslash-newline is replaced with a space *except on recipe lines in which case it is necessary to indicate the recipe continues but it is also passed literally to the shell*. The docs page you linked directly addresses our use case and suggests we put the command inside of a make variable (thus triggering normal backslash-newline rules rather than the special recipe line rules). > Sometimes you want to split a long line inside of single quotes, but you don’t want the backslash/newline to appear in the quoted content. This is often the case when passing scripts to languages such as Perl, where extraneous backslashes inside the script can change its meaning or even be a syntax error. One simple way of handling this is to place the quoted string, or even the entire command, into a make variable then use the variable in the recipe. In this situation the newline quoting rules for makefiles will be used, and the backslash/newline will be removed. If we rewrite our example above using this method:; > ; > ```; > HELLO = 'hello \; > world'; > ; > all : ; @echo $(HELLO); > ```; > ; > we will get output like this:; > ; > ```; > hello world; > ```; >; > If you like, you can also use target-specific variables (see [Target-specific Variable Values](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html)) to obtain a tighter correspondence between the variable and the recipe that uses it. It seems to me like there are not any great choices. Putting the JSON into a Make variable seems too magical and likely to confuse a newbie editing this file. Using escaped double quotes is less leg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324:666,variab,variable,666,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324,2,['variab'],['variable']
Modifiability,"ES.html#release-0-19-2022-07-05"">https://docutils.sourceforge.io/RELEASE-NOTES.html#release-0-19-2022-07-05</a></p>; <h2>Deprecated</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10467"">#10467</a>: Deprecated <code>sphinx.util.stemmer</code> in favour of <code>snowballstemmer</code>.; Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9856"">#9856</a>: Deprecated <code>sphinx.ext.napoleon.iterators</code>.</li>; </ul>; <h2>Features added</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10444"">#10444</a>: html theme: Allow specifying multiple CSS files through the <code>stylesheet</code>; setting in <code>theme.conf</code> or by setting <code>html_style</code> to an iterable of strings.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10366"">#10366</a>: std domain: Add support for emphasising placeholders in :rst:dir:<code>option</code>; directives through a new :confval:<code>option_emphasise_placeholders</code> configuration; option.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10439"">#10439</a>: std domain: Use the repr of some variables when displaying warnings,; making whitespace issues easier to identify.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10571"">#10571</a>: quickstart: Reduce content in the generated <code>conf.py</code> file. Patch by; Pradyun Gedam.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10648"">#10648</a>: LaTeX: CSS-named-alike additional :ref:<code>'sphinxsetup' &lt;latexsphinxsetup&gt;</code>; keys allow to configure four separate border-widths, four paddings, four; corner radii, a shadow (possibly inset), colours for border, background, shadow; for each of the code-block, topic, attention, caution, danger, error and warning; directives.</li>; <li><a href=""https:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12165:3458,config,configuration,3458,https://hail.is,https://github.com/hail-is/hail/pull/12165,1,['config'],['configuration']
Modifiability,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:1404,extend,extend,1404,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307,1,['extend'],['extend']
Modifiability,Elasticsearch/Kibana/Fluentd Configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6413:29,Config,Configuration,29,https://hail.is,https://github.com/hail-is/hail/pull/6413,1,['Config'],['Configuration']
Modifiability,"Emit was taking two MethodBuilder arguments, one directly and one embedded in an EmitRegion. This PR replaces the EmitRegion argument with a Value[Region]. It also makes a couple other simple refactorings that were personally helpful in understanding the structure of Emit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8449:192,refactor,refactorings,192,https://hail.is,https://github.com/hail-is/hail/pull/8449,1,['refactor'],['refactorings']
Modifiability,Emit.scala); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:163); 	at is.hail.backend.service.ServiceBackend.lowerDistributedSort(ServiceBackend.scala:356); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:100); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$4(RewriteBottomUp.scala:26); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:151); 	at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:7790,Rewrite,RewriteBottomUp,7790,https://hail.is,https://github.com/hail-is/hail/issues/12983,4,['Rewrite'],['RewriteBottomUp']
Modifiability,"Envoy by default responds with a 403 if it fails to make an authorization check. We should treat the failure to contact `auth` as a transient error, so that clients can know to retry whatever request they were trying to make instead of failing. In particular, in dev namespaces the current behavior can trigger a QoB client to cancel an ongoing pipeline while polling for completion. [status_on_error](https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/http/ext_authz/v3/ext_authz.proto#extensions-filters-http-ext-authz-v3-extauthz) allows us to configure that default behavior to return a 503 unavailable. I deployed this in Azure by running `make -C gateway deploy NAMESPACE=default`. I poked around to see that I could access my dev namespace and production pages through the browser but did not otherwise try to prove that this failure mode no longer exists.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13735:566,config,configure,566,https://hail.is,https://github.com/hail-is/hail/pull/13735,1,['config'],['configure']
Modifiability,"Erm, sorry, the right place to look for a diff is: https://github.com/hail-is/hail/compare/main...danking:batch-cloud-agnostic-rewrite-all",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956506343:127,rewrite,rewrite-all,127,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956506343,1,['rewrite'],['rewrite-all']
Modifiability,ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2861); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2792); 	at org.apache.spark.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:10129,adapt,adapted,10129,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['adapt'],['adapted']
Modifiability,Evaluation of relational lets is an explicit pass. Executing and rewriting shuffles is an explicit pass.; * lowerDistributedSort executes the shuffle and produces a TableReader. Higher-level passes that recursively lower and execute are parameterized; by the contained pipeline.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12587:237,parameteriz,parameterized,237,https://hail.is,https://github.com/hail-is/hail/pull/12587,1,['parameteriz'],['parameterized']
Modifiability,Example error:. ```; Caused by: is.hail.relocated.org.json4s.MappingException: No usable value for value_parameter_names; No usable value for str; Did not find value which can be converted into java.lang.String; 	at is.hail.relocated.org.json4s.reflect.package$.fail(package.scala:53); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:638); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:689); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:688); 	at scala.PartialFunction.$anonfun$runWith$1$adapted(PartialFunction.scala:145); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at scala.collection.TraversableLike.collect(TraversableLike.scala:407); 	at scala.collection.TraversableLike.collect$(TraversableLike.scala:405); 	at scala.collection.AbstractTraversable.collect(Traversable.scala:108); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:688); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:767); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$extract$10(Extraction.scala:462); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$customOrElse$1(Extraction.scala:780); 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127); 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126); 	at scala.PartialFunction$$anon$1.applyOrElse(PartialFunction.scala:257); 	at is.hail.relocated.org.json4s.Extraction$.customOrElse(Extraction.scala:780); 	at is.hail.relocated.org.json4s.Extraction$.extract(Extraction.scala:454); 	at is.hail.relocated.org.json4s.Extraction$.org$json4s$Extraction$$extractDete,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14579#issuecomment-2163457890:698,adapt,adapted,698,https://hail.is,https://github.com/hail-is/hail/pull/14579#issuecomment-2163457890,2,['adapt'],['adapted']
Modifiability,Example stack trace:; ```; 2022-02-08 18:09:30 root: ERROR: IllegalArgumentException: requirement failed; From java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.rvd.RVDPartitioner.<init>(RVDPartitioner.scala:52); 	at is.hail.rvd.RVDPartitioner.extendKeySamePartitions(RVDPartitioner.scala:141); 	at is.hail.expr.ir.LoweredTableReader$$anon$2.coerce(TableIR.scala:382); 	at is.hail.expr.ir.GenericTableValue.toTableStage(GenericTableValue.scala:162); 	at is.hail.io.vcf.MatrixVCFReader.lower(LoadVCF.scala:1790); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:581); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:561); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1304); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:561); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1035); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:394); 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:547); 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:69); 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:18); 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:77). ```. called from here:; https://github.com/hail-is/hail/blob/d2f87d81dd1af43617740309e354d4bac8c672e0/hail/src/main/scala/is/hail/expr/ir/TableIR.scala#L382,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11335:307,extend,extendKeySamePartitions,307,https://hail.is,https://github.com/hail-is/hail/issues/11335,1,['extend'],['extendKeySamePartitions']
Modifiability,"Existence of directories is filesystem dependent, and they exist on Google if there is an object with that directory name a prefix. This required a little refactoring that changed fs.listfiles to return a coroutine that gives an iterator, rather than returning an iterator directly. This is because, if you use the `async def foo ... yield ...` syntax, it is not possible to write any code that will run between calling the async genreator function `foo` and the first call to `__anext__`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9818:155,refactor,refactoring,155,https://hail.is,https://github.com/hail-is/hail/pull/9818,1,['refactor'],['refactoring']
Modifiability,Expecting this to fail since I haven't applied the terraform yet and the config values set there are still using `hail-az`. After applying terraform this should pass.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14209:73,config,config,73,https://hail.is,https://github.com/hail-is/hail/pull/14209,1,['config'],['config']
Modifiability,Expose Genome Reference as global variable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1789:34,variab,variable,34,https://hail.is,https://github.com/hail-is/hail/pull/1789,1,['variab'],['variable']
Modifiability,Extend Let to work with BaseIR,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4905:0,Extend,Extend,0,https://hail.is,https://github.com/hail-is/hail/issues/4905,1,['Extend'],['Extend']
Modifiability,Extend PruneDeadFields to prune key fields (woohoo!),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5864:0,Extend,Extend,0,https://hail.is,https://github.com/hail-is/hail/pull/5864,1,['Extend'],['Extend']
Modifiability,Extend PruneDeadFields to prune tuples,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5851:0,Extend,Extend,0,https://hail.is,https://github.com/hail-is/hail/pull/5851,1,['Extend'],['Extend']
Modifiability,Extend Python methods that print to take arbitrary handler functions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4303:0,Extend,Extend,0,https://hail.is,https://github.com/hail-is/hail/pull/4303,1,['Extend'],['Extend']
Modifiability,Extend Simplify to remove more operations before TableCount,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3676:0,Extend,Extend,0,https://hail.is,https://github.com/hail-is/hail/issues/3676,1,['Extend'],['Extend']
Modifiability,"Extend `FunctionChecker` to assert that at least the number of non-default parameters are specified, up to the maximum number of parameters.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12814:0,Extend,Extend,0,https://hail.is,https://github.com/hail-is/hail/pull/12814,1,['Extend'],['Extend']
Modifiability,Extend min/max functions.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2935:0,Extend,Extend,0,https://hail.is,https://github.com/hail-is/hail/pull/2935,1,['Extend'],['Extend']
Modifiability,Extend option to skip Scala tests requiring plink/Rscript executables,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5126:0,Extend,Extend,0,https://hail.is,https://github.com/hail-is/hail/pull/5126,1,['Extend'],['Extend']
Modifiability,Extending the idea of better error messages from #9398 to include `NDArrayRef`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9444:0,Extend,Extending,0,https://hail.is,https://github.com/hail-is/hail/pull/9444,1,['Extend'],['Extending']
Modifiability,"FO: Getting 1 non-empty blocks out of 1 blocks; 2018-10-09 14:46:42 ShuffleBlockFetcherIterator: INFO: Started 0 remote fetches in 0 ms; 2018-10-09 14:46:42 Executor: INFO: Finished task 0.0 in stage 4.0 (TID 4). 1539 bytes result sent to driver; 2018-10-09 14:46:42 TaskSetManager: INFO: Finished task 0.0 in stage 4.0 (TID 4) in 7 ms on localhost (executor driver) (1/1); 2018-10-09 14:46:42 TaskSchedulerImpl: INFO: Removed TaskSet 4.0, whose tasks have all completed, from pool ; 2018-10-09 14:46:42 DAGScheduler: INFO: ResultStage 4 (collect at utils.scala:197) finished in 0.007 s; 2018-10-09 14:46:42 DAGScheduler: INFO: Job 2 finished: collect at utils.scala:197, took 0.053572 s; 2018-10-09 14:46:42 CodeGenerator: INFO: Code generated in 5.955541 ms; 2018-10-09 14:46:42 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:42 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table7e606a8b83f4` AS `zzz1`; WHERE (0 = 1); 2018-10-09 14:46:42 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:42 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table7e606a8b83f4`; 2018-10-09 14:46:43 root: INFO: optimize: before:; (TableCount; (TableKeyBy () False; (TableLiteral))); 2018-10-09 14:46:43 root: INFO: optimize: after:; (TableCount; (TableLiteral)); 2018-10-09 14:46:43 SparkContext: INFO: Starting job: fold at RVD.scala:361; 2018-10-09 14:46:43 DAGScheduler: INFO: Got job 3 (fold at RVD.scala:361) with 1 output partitions; 2018-10-09 14:46:43 DAGScheduler: INFO: Final stage: ResultStage 5 (fold at RVD.scala:361); 2018-10-09 14:46:43 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 14:46:43 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 14:46:43 DAGScheduler: INFO: Submitting ResultStage 5 (MapPartitionsRDD[28] at mapPartitions at ContextRDD.scala:137), which has no missing parents; 2018-10-09 14:46:43 MemoryStore: ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:46192,config,configuration,46192,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['config'],['configuration']
Modifiability,"FWIW you don't actually need to parse it at the moment. This is my code that works:. ```; temp_file = 'hdfs:/kt.txt.bgz'; types_file = 'hdfs:/kt.types.txt'; kt.export(temp_file, types_file=types_file). with hail.hadoop_read(types_file) as f:; types = f.read(); kt = hc.import_keytable(temp_file, config=hail.TextTableConfig(types=types)); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1743#issuecomment-299075862:296,config,config,296,https://hail.is,https://github.com/hail-is/hail/issues/1743#issuecomment-299075862,1,['config'],['config']
Modifiability,"FWIW, when I added the MySQL pods I made sure to install the client config in them so you don't need the admin-pod for test namespaces, just `kssh db <NAMESPACE>` and `mysql` should work",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13150#issuecomment-1581414024:68,config,config,68,https://hail.is,https://github.com/hail-is/hail/pull/13150#issuecomment-1581414024,1,['config'],['config']
Modifiability,"FYI @cseed @danking: I found a bad bug in the Kubernetes deserializer that we use to deserialize the pod spec from the request from the client. Any variable with a compound name like `generate_name` or `cluster_name` was being set to None regardless of what the input was. This is because their code has a mapping from the Python style with underscores to camel case and it was using the camel case to look up the attributes instead of the underscore names. @akotlar was going to make a PR to correct it in their repo. For now, I'm including the correct code in our repo. Without it, I was getting errors trying to deserialize pod templates with metadata from the MySQL database back to Python. The other horrible behavior I found with the deserializer is if you pass `None` to the deserialize function, you get a dictionary with all attributes set to `None` instead of just `None`. Something to be aware of or we should fix it in this PR to return None instead. @akotlar: I tried to make everything just adhere to Python3, but can you make sure I did the conversions correctly?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5738:148,variab,variable,148,https://hail.is,https://github.com/hail-is/hail/pull/5738,1,['variab'],['variable']
Modifiability,"FYI @danking There was also a Makefile bug: make creates make variables from the environment variables by default. Also, when make runs a rule, it makes the make variables available in the environment. That means PYTHONPATH in those Makefiles was getting set to some literal ${...} string, not the right value. I solved this by not assigning PYTHONPATH in the Makefile. https://www.gnu.org/software/make/manual/html_node/Environment.html. ""Every environment variable that make sees when it starts up is transformed into a make variable with the same name and value. However, an explicit assignment in the makefile, or with a command argument, overrides the environment."". ""When make runs a recipe, variables defined in the makefile are placed into the environment of each shell.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9394#issuecomment-685254706:62,variab,variables,62,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685254706,6,['variab'],"['variable', 'variables']"
Modifiability,"FYI, I had to add a default/gce-deploy-config for the deploy config for tasks running on batch2 workers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7507#issuecomment-553959196:39,config,config,39,https://hail.is,https://github.com/hail-is/hail/pull/7507#issuecomment-553959196,2,['config'],['config']
Modifiability,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/303:105,config,config,105,https://hail.is,https://github.com/hail-is/hail/issues/303,1,['config'],['config']
Modifiability,Fair point. I think asserting this behavior works on a local FS is good. I think a follow up to this PR that parameterized the other tests by filesystem is also great. That follow up PR can just leave the local-only test unchanged.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11227#issuecomment-1015881708:109,parameteriz,parameterized,109,https://hail.is,https://github.com/hail-is/hail/pull/11227#issuecomment-1015881708,1,['parameteriz'],['parameterized']
Modifiability,"Few more variables are expected to be in `config.mk` for manual bootstrap:. * `DOCKER_ROOT_IMAGE` used to build batch workers and benchmark; * `HAIL_TEST_GCS_BUCKET` used to build query; * `KUBERNETES_SERVER_URL` used to build amundsen; * `PROJECT`, `ZONE`, `REGION` are probably not need, but might make sense to add for consistency. Also match the order from `global-config`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11371:9,variab,variables,9,https://hail.is,https://github.com/hail-is/hail/pull/11371,3,"['config', 'variab']","['config', 'variables']"
Modifiability,"Finally add BGEN filtering at the level of bytes. We add a non-public, non-documented `_variants_per_file` argument that lists the variants to keep by their on-disk indices. It's a pretty groady interface, but it works with the current infrastructure and provides a _massive_ improvement. We go from decoding all the variants in the BGEN to decoding only those relevant to the computation at hand. In the case of Caitlin's PRS scoring method, that's about 1% or less of the original variant set. It's a bit janky. The file paths need to be the fully qualified ones that are seen by the hadoop reader. So `file:/full/path/to/file.bgen` or `gs://full/path/to/bgen.file`, which is annoying. I don't have any better way to generically uniquely identify files though. ---; ### Calc Depth Bug. I also had to fix a bug in the indices. Neither my `OnDiskBTreeIndexToValue` nor the existing `IndexBTree` correctly calculated the sizes of the given trees. Recall that a b-tree is a series of layers. Layer 0 is at most `branchingFactor` in size. Layer i is at most `branchingFactor ^ (i+1)` in size. The total size of the b-tree is the sum of the layer sizes. Here's a few max sizes for a branchingFactor of 1024:. - 1 layer tree: 1024; - 2 layer tree: 1024^2 + 1024; - 3 layer tree: 1024^3 + 1024^2 + 1024. If you look carefully at the old `calcDepth` method, it incorrectly concludes that fully populated 3 layer trees have four layers because they have more than 1024^3 total (internal+leaf) elements. This issue rears it's head on an exponentially small number of trees (at depth `i`, the number of leaf elements must lie in `[1024^i-1024^(i-1), 1024^i]`. This discrepancy is what lead to my confusion for the last few days. It shows up quite quickly with very small branching factors (e.g. 3) but with a large branching factor (the default of 1024 and what all the tests were written against) it's fairly rare. ---; ### Summary of Changes. - add `_variants_per_file` which is a map from absolute file paths",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3813:982,layers,layers,982,https://hail.is,https://github.com/hail-is/hail/pull/3813,1,['layers'],['layers']
Modifiability,"First step in RVD changes. Rewrites `Interval` to support endpoints that are `Row`s of different lengths. Hopefully comments and test suite are enough to make the semantics clear. If not, let me know what is unclear and I'll add documentation and/or test cases.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4072:27,Rewrite,Rewrites,27,https://hail.is,https://github.com/hail-is/hail/pull/4072,1,['Rewrite'],['Rewrites']
Modifiability,"First, @cristinaluengoagullo, thank you for your contribution! This is awesome. Second, I think this PR will need a little work before it can go in. Let me describe the situation:. We have now stopped work on Hail 0.1 and are now making only critical bug fixes. I think we can accept small feature additions, but we're optimizing for stability over features now. All new development has moved to master/0.2 beta. If you do make changes to 0.1, they should be forward ported to 0.2 if you want them to be carried forward. In addition, there are two problems with your PR:. 1. It is quite large. We prefer contributions to be single conceptual units. For example, a change to VEP should be separate from additions to the function registry. 2. The diff is somewhat confusing and I'm not 100% sure what is going on. It appears to include a large number of our own changes, it looks like from this commit: https://github.com/hail-is/hail/pull/3172/commits/e6f0b7f3a854f0fd64857876ab04375e570ba09f. However, given that the commit is under your name with a new commit message, I can't tell where those changes originated. Also, at least some (all?) of those changes already appear in 0.1, so I'm not sure why Github is displaying them, for example: https://github.com/hail-is/hail/pull/3172/files#diff-f11d07953ac5cd8bd8d4d3fd135a3efbR11. I think squashing your changes (and just your changes) and rebasing them onto the current 0.1 HEAD will fix the problem. Then we can take a closer look at the changes. Finally, it looks like your changed the VEP schema because you're invoking it with extra plugins. I think that's a no-go for us, but you could modify the VEP command (through an argument or in the properties file) to specify an alternate schema.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3172#issuecomment-377559347:1589,plugin,plugins,1589,https://hail.is,https://github.com/hail-is/hail/pull/3172#issuecomment-377559347,1,['plugin'],['plugins']
Modifiability,"Fix `IEmitCode.map`, Rewrite `MakeNDArray` in `CodeBuilder` style",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8516:21,Rewrite,Rewrite,21,https://hail.is,https://github.com/hail-is/hail/pull/8516,1,['Rewrite'],['Rewrite']
Modifiability,"Fix bad error message in Table.order_by, extend functionality",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4927:41,extend,extend,41,https://hail.is,https://github.com/hail-is/hail/pull/4927,1,['extend'],['extend']
Modifiability,Fix configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1162:4,config,configuration,4,https://hail.is,https://github.com/hail-is/hail/pull/1162,1,['config'],['configuration']
Modifiability,Fix simplification for integer subtraction; Extend scope of simplifier to more numeric types.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754:44,Extend,Extend,44,https://hail.is,https://github.com/hail-is/hail/pull/12754,1,['Extend'],['Extend']
Modifiability,Fix the overriding of the gcs_requester_pays/project config variable through using 'hailctl describe -u'. Closes #13793,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13826:53,config,config,53,https://hail.is,https://github.com/hail-is/hail/pull/13826,2,"['config', 'variab']","['config', 'variable']"
Modifiability,Fixed bed annotator to be more flexible,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1627:31,flexible,flexible,31,https://hail.is,https://github.com/hail-is/hail/pull/1627,1,['flexible'],['flexible']
Modifiability,Fixed getting started cloudera configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2021:31,config,configuration,31,https://hail.is,https://github.com/hail-is/hail/pull/2021,1,['config'],['configuration']
Modifiability,Fixed inlining of ArrayAgg nodes without aggs in the body. Also added; simplify rule to rewrite these nodes to just the body.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7219:88,rewrite,rewrite,88,https://hail.is,https://github.com/hail-is/hail/pull/7219,1,['rewrite'],['rewrite']
Modifiability,"Fixed, config and status now displayed in job page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7524#issuecomment-562856309:7,config,config,7,https://hail.is,https://github.com/hail-is/hail/issues/7524#issuecomment-562856309,1,['config'],['config']
Modifiability,"Fixed. Two outstanding issues:; 1. `RichVDS` cannot extend `AnyVal` due to implementation restrictions on value extensions nesting with other implicit conversions. Right now it is a standalone class.; 2. What do with `VSMSuite`? This seemed like it was designed to make sure all the implementations were synced, but that is no longer necessary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/83#issuecomment-160690390:52,extend,extend,52,https://hail.is,https://github.com/hail-is/hail/pull/83#issuecomment-160690390,1,['extend'],['extend']
Modifiability,"Fixes #11335. @tpoterba made the replicating test, with that it was easy to find the source of the bug. The error was in; ```; pkPartitioned; .strictify(); ...; .changePartitionerNoRepartition(partitioner.extendKeySamePartitions(keyType)); ```; where `pkPartitioned` is keyed by the partition key. In the test case, all rows have the same partition key, so the partitioner looks like `[x, x], [x, x], ...`. In that case, `strictify` correctly collapses all those partitions into one, but `partitioner.extendKeySamePartitions(keyType)` tries to extend the key type without changing the partitioning, which in this case creates an invalid partitioner. The fix is to use `pkPartitioned.extendKeyPreservesPartitioning(key)`, which does the `strictify` and creates the correct partitioner.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11355:205,extend,extendKeySamePartitions,205,https://hail.is,https://github.com/hail-is/hail/pull/11355,4,['extend'],"['extend', 'extendKeyPreservesPartitioning', 'extendKeySamePartitions']"
Modifiability,"Fixes #3729. The problem with the If in the array emitting logic was that the lengths were being stored in two different local variables, which were only being evaluated/stored depending on which branch was taken. Because we store that information in another local variable, `xvcond`, and check it again when we're actually consuming the array, the analyzer was checking both branches again for that step, and being unhappy that the local variable where the length is stored could have been unpopulated. I have fixed this by only having one branch and doing both the length calculation and the rest of the stuff there.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4234:127,variab,variables,127,https://hail.is,https://github.com/hail-is/hail/pull/4234,3,['variab'],"['variable', 'variables']"
Modifiability,Fixes a bug where identifiers for bound variables were not being printed by the sexpr style pretty printer.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11603:40,variab,variables,40,https://hail.is,https://github.com/hail-is/hail/pull/11603,1,['variab'],['variables']
Modifiability,"Fixes the problem where if there is a bad secret / job config fails, we didn't update the attempt id in the jobs table in the database.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8760:55,config,config,55,https://hail.is,https://github.com/hail-is/hail/pull/8760,1,['config'],['config']
Modifiability,Fixes this problem:. ```; + xargs -r gcloud -q compute instances delete --zone; ERROR: (gcloud.compute.instances.list) The required property [project] is not currently set.; You may set it for your current workspace by running:. $ gcloud config set project VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_CORE_PROJECT]; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7520:238,config,config,238,https://hail.is,https://github.com/hail-is/hail/pull/7520,2,"['config', 'variab']","['config', 'variable']"
Modifiability,Fixes/enhancements to HTSGenotypeView.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2296:6,enhance,enhancements,6,https://hail.is,https://github.com/hail-is/hail/pull/2296,1,['enhance'],['enhancements']
Modifiability,"Flags now use the same user configuration machinery we use for Batch and QoB. I am not certain this is the right choice. Feedback very welcome. The configuration_of function lets us uniformly treat any configuration by checking, in order: explicit argument, envvar, config file, or a fallback. I added a bit of code to allow us to support the envvars which do not conform to the new envvar scheme. I also removed a few flags that are no longer used. I kind of think these flags should actually be under a new section like ""query_compiler"" or something. @tpoterba, thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12423:28,config,configuration,28,https://hail.is,https://github.com/hail-is/hail/pull/12423,3,['config'],"['config', 'configuration']"
Modifiability,"For `hail` we’ve tried to be flexible, but We can move the lower bound or add an upper bound if necessary. I’d this is services stuff that’s just a mistake, definitely pin if you’d like.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10349#issuecomment-824032425:29,flexible,flexible,29,https://hail.is,https://github.com/hail-is/hail/pull/10349#issuecomment-824032425,1,['flexible'],['flexible']
Modifiability,"For clarity: I removed PCanonicalBaseStruct because most method implementations were moved back to PBaseStruct, and I found its now-limited benefit to not be worth the drawback of a significantly more complex inheritance structure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7733#issuecomment-566778522:209,inherit,inheritance,209,https://hail.is,https://github.com/hail-is/hail/pull/7733#issuecomment-566778522,1,['inherit'],['inheritance']
Modifiability,"For dockerd, we probably need to use this: https://github.com/fluent-plugin-systemd/fluent-plugin-systemd",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9189#issuecomment-667117637:69,plugin,plugin-systemd,69,https://hail.is,https://github.com/hail-is/hail/pull/9189#issuecomment-667117637,2,['plugin'],['plugin-systemd']
Modifiability,"For flag style env variables, I think we should use present and non-empty. We could also go with a reasonable set of 'affirmative' values, like say (all case insensitive) `true`, `t`, `yes`, `y`, `1`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11746#issuecomment-1104272760:19,variab,variables,19,https://hail.is,https://github.com/hail-is/hail/pull/11746#issuecomment-1104272760,1,['variab'],['variables']
Modifiability,"For linreg, logrem, lmmreg, and skat:; - changed Python implementation to annotate or select on `x` if not a field and always pass `x_field`, which must be float64 but may have missing values.; - changed Scala linreg, logrem, lmmreg, and skat to take `xField` rather than `xExpr`. Updated Scala tests with selectEntry accordingly.; - replaced RegressionUtils `inputVector` with `setMeanImputedDoubles`; - removed `dataset` parameter from Python. Now all methods that take a dataset and one or more required expressions on that dataset now only take the expressions. Updated docs, tests, tutorial accordingly.; - added `req_tstring` to linear_mixed_regression and `We plan to change the interface to this method in Hail 0.2 while maintaining its functionality.` The constraint is due to string assumption made when comparing and filtering column keys against keys on KinshipMatrix. Since the latter is going away (and marked as such), I don't think it's worth more changes to remove the constraint.; - made docs more consistent and variable names more generic (sample=>col, variant=>row, etc)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3289:1031,variab,variable,1031,https://hail.is,https://github.com/hail-is/hail/pull/3289,1,['variab'],['variable']
Modifiability,"For my SAIGE implementation, it would be nice to be able to use the `{BATCH_TMPDIR}` environment variable within a file name so that I can give user-specified file path names for output files to write that can be used downstream in globs when importing temporary files in Hail without having to localize all of the files (could be 4K+ files). I also thought this could solve Konrad's region bucket request where we copy data from a region-specific location.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14340:97,variab,variable,97,https://hail.is,https://github.com/hail-is/hail/pull/14340,1,['variab'],['variable']
Modifiability,"From Cotton:. I think the request/s and request latency metrics in Grafana are not actually the metrics for the Kubernetes service as we'd hoped. In particular, the reqs/s makes no sense. This post:. https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-4-the-kubernetes-api-server-72f1e1210770. indicates we should have an apiserver_request_count and apiserver_request_latencies_bucket which sound like what we want. They give ""the Prometheus configuration for getting metrics from the Kubernetes API server, even in environments where the masters are hosted for you"" (and I think our master is hosted for us in GKE). In particular, we have no analogous apiserver scrape config with ""role: endpoints"" in our setup. Here is the apiserver code with all the metrics they collect: https://github.com/kubernetes/apiserver/blob/master/pkg/endpoints/metrics/metrics.go",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6495:459,config,configuration,459,https://hail.is,https://github.com/hail-is/hail/issues/6495,2,['config'],"['config', 'configuration']"
Modifiability,From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:2298,adapt,adapted,2298,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['adapt'],['adapted']
Modifiability,"From the man page:. ```; -t Don't run, just test the configuration file. The nginx; checks configuration for correct syntax and then tries; to open files referred in configuration.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4429:53,config,configuration,53,https://hail.is,https://github.com/hail-is/hail/pull/4429,3,['config'],['configuration']
Modifiability,Future enhancements to batch will necessitate the proper use of modules.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4787:7,enhance,enhancements,7,https://hail.is,https://github.com/hail-is/hail/pull/4787,1,['enhance'],['enhancements']
Modifiability,"Gah, OK, I think I have it now, but there was one more detail:. The gradle configuration `testCompileOnly` [1] *does not* inherit from the `shadow` configuration (as evidence see [this search](https://github.com/search?q=repo%3Ajohnrengelman%2Fshadow%20extendsFrom&type=code) of the shadow repo). We must explicitly request that `shadow` dependencies are included in the compile-time class path of the tests. This is as it should be: the things in `shadow` are things which are provided to us by our runtime environment. That's true of both the *test* runtime environment and the normal runtime environment. The Gradle Shadow plugin takes a different perspective by default, it suggests that `shadow` dependencies shouldn't be used in the tests at all. [1] NB: `testCompile` does not exist but you don't get an error if you try to use it, thanks for nothing gradle.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1710414563:75,config,configuration,75,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1710414563,4,"['config', 'inherit', 'plugin']","['configuration', 'inherit', 'plugin']"
Modifiability,"Gateway receives the user IP (thanks to #8045). However, gateway is an HTTP; proxy, so packets from gateway necessarily come from gateway's IP. Gateway; places the user IP into the HTTP header `X-Real-IP`. All downstream servers; must: log `X-Real-IP` and forward `X-Real-IP` unadulterated. This PR makes that; change for `router`. - fix router Makefile (`domain` is now in `global`); - add `proxy.conf` which configures the standard proxy headers (importantly:; forwards `X-REAL-IP`); - for non-notebook servers, `include` the `proxy.conf`; - for notebook, update to include proxy headers; - override default `access_log` (which required checking in the default; `nginx.conf`); - lift other `http` directives into `nginx.conf` now that it is checked in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8058:410,config,configures,410,https://hail.is,https://github.com/hail-is/hail/pull/8058,1,['config'],['configures']
Modifiability,GenericTableValue.toTableStage(GenericTableValue.scala:162); at is.hail.io.vcf.MatrixVCFReader.lower(LoadVCF.scala:1798); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:717); at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:697); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:903); at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:467); at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:472); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:73); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:18); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:77); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:53); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOpt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280:3284,rewrite,rewrite,3284,https://hail.is,https://github.com/hail-is/hail/issues/12280,1,['rewrite'],['rewrite']
Modifiability,Genome reference as global variable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1889:27,variab,variable,27,https://hail.is,https://github.com/hail-is/hail/pull/1889,1,['variab'],['variable']
Modifiability,"Getting the following errors when compiling on a Mac. Any suggestions?. ./gradlew shadowJar ; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/farrell/github/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/darwin; c++ -fvisibility=hidden -dynamiclib -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/darwin/libibs.dylib; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:23:no such instruction: `vmovd %xmm0, %rax'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:38:no such instruction: `vpextrq $1, %xmm0,%rax'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:79:no such instruction: `vpcmpeqd %xmm5, %xmm5,%xmm5'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:96:no such instruction: `vpxor %xmm1, %xmm0,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:114:no such instruction: `vpxor %xmm5, %xmm1,%xmm1'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:135:no such instruction: `vpand %xmm3, %xmm2,%xmm3'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:150:no such instruction: `vpsrlq $1, %xmm1,%xmm0'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:167:no such instruction: `vpxor %xmm5, %xmm3,%xmm3'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:193:no such instruction: `vpor LC1(%rip), %xmm3,%xmm3'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:212:no such instruction: `vpand %xmm1, %xmm0,%xmm4'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:225:no such instruction: `vpandn %xmm4, %xmm3,%xmm4'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:241:no such instruction: `vmovd %xmm4, %rax'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:255:no such instruction: `vpxor %xmm1, %xmm0,%xmm2'; /var/folders/gx/t1q6w73n4pn8jzfc4_k3lm0m0000gn/T//ccuBKQk1.s:277:no such instruction: `v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274:169,Config,Configuring,169,https://hail.is,https://github.com/hail-is/hail/issues/1274,1,['Config'],['Configuring']
Modifiability,"Getting this with current master on the cloud:. ```; Use of uninitialized value in hash element at /vep/ensembl-tools-release-85/scripts/variant_effect_predictor/Bio/EnsEMBL/Variation/Utils/VEP.pm line 4255, <VARS> line 1.; [Stage 18:=> (273 + 410) / 13592]Traceback (most recent call last):; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 75, in <module>; main(args, pops); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 51, in main; 'va.rf').write(args.output + "".autosomes.vds"", overwrite=True); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/utils.py"", line 452, in post_process_vds; vds = vds.vep(config=vep_config, csq=True, root='va.info.CSQ', force=True); File ""<decorator-gen-110>"", line 2, in vep; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/pyhail-attr.zip/hail/java.py"", line 93, in handle_py4j; hail.java.FatalError: NoSuchElementException: None.get; [Stage 18:=> (277 + 409) / 13592]java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@2a632cbb rejected from java.util.concurrent.ThreadPoolExecutor@974d518[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2913]; ```. Lmk if you need more log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1518:639,config,config,639,https://hail.is,https://github.com/hail-is/hail/issues/1518,1,['config'],['config']
Modifiability,Ghost does not respect the `X-Forwarded` headers. It should not have a `url` parameter but a `pathPrefix` and the protocol should be set from `X-Forwarded-Proto`. Thanks to this design bug we cannot test connectivity to the blog in PRs until the internal gateway is configured to use TLS. I'll revisit this PR when that happens.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8117#issuecomment-589741905:266,config,configured,266,https://hail.is,https://github.com/hail-is/hail/pull/8117#issuecomment-589741905,1,['config'],['configured']
Modifiability,"Good idea, I'll check. I feel like I initially found this in deep in a redhat tutorial, but ultimately found it again at the bottom of the [man page](https://man7.org/linux/man-pages/man8/xfs_quota.8.html). I was following this example:; ```; Enabling project quota on an XFS filesystem (restrict files in; log file directories to only using 1 gigabyte of space). # mount -o prjquota /dev/xvm/var /var; # echo 42:/var/log >> /etc/projects; # echo logfiles:42 >> /etc/projid; # xfs_quota -x -c 'project -s logfiles' /var; # xfs_quota -x -c 'limit -p bhard=1g logfiles' /var. Same as above without a need for configuration files. # rm -f /etc/projects /etc/projid; # mount -o prjquota /dev/xvm/var /var; # xfs_quota -x -c 'project -s -p /var/log 42' /var; # xfs_quota -x -c 'limit -p bhard=1g 42' /var; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10467#issuecomment-834771396:607,config,configuration,607,https://hail.is,https://github.com/hail-is/hail/pull/10467#issuecomment-834771396,1,['config'],['configuration']
Modifiability,"Good questions:. > How does Terraform know to import the module gsa_k8s_secret? I assume it just scans the directory. In short, yes. Every module is defined by a `main.tf`, `variables.tf`, and `outputs.tf`, and it finds it through the `source` path in the module blocks. When you run `terraform init` in the `infra` directory, it scans `main.tf`, sees that there are module references to the local filesystem, and sets up a watch on that directory (you can still make changes to `gsa_k8s_secret` without re-running `init`). > What does ${module.ci_gsa_secret....} do? Does module refer to that terraform file?. The way references work in terraform is a little bizarre. There are basically three classes of value that we use right now in terraform: resources, modules and variables. You declare instances of them like so (note that I'm using class and instance colloquially):. ```; resource <resource_class_name> <resource_instance_name> {; …; }. # I like to think of modules as being essentially unnamed collections of resources; module <module_instance_name> {; source = '/path/to/module'; …; }. variable <variable_name> {; …; }; ```. and then reference them like so:. ```; <resource_class_name>.<resource_instance_name>; module.<module_instance_name>; var.<variable_name>; ```. So module.ci_gsa_secret.... is how you would access `output`s of the module instance called `ci_gsa_secret`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10785#issuecomment-900398381:174,variab,variables,174,https://hail.is,https://github.com/hail-is/hail/pull/10785#issuecomment-900398381,3,['variab'],"['variable', 'variables']"
Modifiability,"Got annoyed with the constant re-tagging of images that don't need to be rebuilt, and decided to play a little make golf along the way. cc @jigold This should dramatically reduce the number of tags for hail-ubuntu from make-deployed images, though the number of layers in the container registry should not change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12841:262,layers,layers,262,https://hail.is,https://github.com/hail-is/hail/pull/12841,1,['layers'],['layers']
Modifiability,"Grace requested this for gnomAD purposes. In Azure, we can determine this; using the classpath or envionrment variables (the env var is only available; inside Jupyter). In GCP, I added an environment variable to our Spark env; / Jupyter env.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11230:110,variab,variables,110,https://hail.is,https://github.com/hail-is/hail/pull/11230,2,['variab'],"['variable', 'variables']"
Modifiability,"Great feedback. Addressed comments, back to you. In the scope lists, I use the format: `variable (*Type*): description`, where the type is in italics but not a hyperlink, but I put a hyperlink in the description when it seemed appropriate. ```; *:ref:`foo`; ```. didn't format the hyperlink. Also, I don't think we can put hyperlinks in double-back-quote literal/code blocks. I didn't address the math stuff. I think we can merge this (and other doc migrations) when it is ready and fix that separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1225#issuecomment-271382041:88,variab,variable,88,https://hail.is,https://github.com/hail-is/hail/pull/1225#issuecomment-271382041,2,['variab'],['variable']
Modifiability,"Great, thank you! I've just checked out your branch and will do some manual testing this morning and let you know how that goes. I noticed that the VEP logic didn't have test coverage right now, but had a few ideas for some modest refactoring so unit tests are possible. I'll see if I can get that working!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3790#issuecomment-400314076:231,refactor,refactoring,231,https://hail.is,https://github.com/hail-is/hail/issues/3790#issuecomment-400314076,1,['refactor'],['refactoring']
Modifiability,HAIL_QUERY_BACKEND is unset. $ hailctl config get query/backend; batch. I will set it to spark,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12540#issuecomment-1363231471:39,config,config,39,https://hail.is,https://github.com/hail-is/hail/issues/12540#issuecomment-1363231471,1,['config'],['config']
Modifiability,"Had to refactor some things to deal with subtle importing graphs. I also left the hl.scan stuff using functools, because I'm not sure that; we can fix that as easily. fixes #4112",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4139:7,refactor,refactor,7,https://hail.is,https://github.com/hail-is/hail/pull/4139,1,['refactor'],['refactor']
Modifiability,"Hail does not support heterogeneous arrays: found list with elements of types [dtype('int32'), dtype('str')] . The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 584, in arg_check; return checker.check(arg, function_name, arg_name); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_typecheck.py"", line 80, in check; raise TypecheckFailure from e; hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.pyenv/versions/3.8.16/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>; cli.main(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main; run(); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file; runpy.run_path(target, run_name=""__main__""); File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path; return _run_module_code(code, init_globals, run_name,; File ""/home/edmund/.vscode/extensions/ms-python.python-2023.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code; _run_code(code, mod_globals, init_globals,; File ""/home/edmund/.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:3044,adapt,adapter,3044,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['adapt'],['adapter']
Modifiability,"Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:20764,Plugin,Plugins,20764,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,"Hail's benchmarks were kind of their own thing and a little neglected.; This change moves the benchmarks into the `hail/python` folder and updates them to use pytest with a custom plugin/set of pytest hooks.; Now, benchmarks can be run from the command line like any pytest.; This change removes the `benchmark-hail` (or `hailbench`) utility. Benchmarks are marked by `pytest.mark.benchmark` (via the `@benchmark` decorator).; By convention, benchmarks are python tests whose names are prefixed by `benchmark_` and are located in files with the same prefix.; Nothing enforces this, however, so you could name your benchmarks `test_*` and put them in files named `test_*.py`.; Benchmarks may import and use any test code or utilities defined in `test/`.; The results of each benchmark are outputted as json lines (`.jsonl`) to the file specified by the `--output` pytest arg or stdout. The folder structure should be familiar, resembling our `test/` directory.; I believe this is flexible enough to add `hailtop` benchmarks should we so wish:; ```; pytest.ini - hoisted from `test/` to include benchmark marks; benchmark/; - conftest.py for custom pytest command line args ; - hail/; - confest.py for custom plugin that runs hail benchmarks; - benchmark_*.py hail query benchmark code; - tools/; - shared utilites, including the `@benchmark`; ```; Supporting pytest fixtures required writing a custom plugin to run benchmarks, as using off-the-shelf; solutions like `pytest-benchmark` would forbid method level fixtures like `tmp_path` etc.; The plugin is designed to run ""macro-benchmarks"" (ie long-running tests) and fully supports pytest parameterisation.; For each benchmark, the plugin initialises hail and then repeats (for a number of iterations defined by the pytest mark); acquiring fixtures, timing invocation and tearing-down fixtures, finally stopping hail. It is therefore unsuitable for; microbenchmarks, for which we currenly have none in python. If we add them we'd need to tweak this s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14565:180,plugin,plugin,180,https://hail.is,https://github.com/hail-is/hail/pull/14565,1,['plugin'],['plugin']
Modifiability,"HailContext initialization overrides any existing log4j configuration, which can lead to the logs ending up in an unexpected location. This PR adds an option to HailContext initialization to skip this configuration step. I also included two unrelated changes to this PR:; - Not bundling the transitive dependencies for `com.indeed:lsmtree-core:1.0.7`, which don't seem to be needed and can lead to classpath conflicts.; - Allowing the `quiet` option during initialization to silence the warning issued when initializing with pip-installed Hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8571:56,config,configuration,56,https://hail.is,https://github.com/hail-is/hail/pull/8571,2,['config'],['configuration']
Modifiability,Handling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.variant.ReferenceGenome.addLiftover(ReferenceGenome.scala:407); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$2(SparkBackend.scala:613); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$2$adapted(SparkBackend.scala:612); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$1(SparkBackend.scala:347); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$1(SparkBackend.scala:612); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$1$adapted(SparkBackend.scala:611); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.spark.SparkBackend.pyAddLiftover(SparkBackend.scala:611); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13993:4488,adapt,adapted,4488,https://hail.is,https://github.com/hail-is/hail/issues/13993,1,['adapt'],['adapted']
Modifiability,"Happened on VDS with small number of partitions (18) but large number of variants (~150mio). [Stage 0:=============================> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/430:180,config,config,180,https://hail.is,https://github.com/hail-is/hail/issues/430,1,['config'],['config']
Modifiability,"Happy to hear suggestions as to better ways to do this, but I've made too many emitter typos with while loops. I keep either doing something like. ```; i := 0,; j := 0,; Code.whileLoop(i < M,; Code.whileLoop(j < N, ; ???,; j := j + 1; ); i := i + 1; ); ```; (j should be inside the outer whileLoop),; forgetting one of the increments, or forgetting to set the variables to 0 at the beginning. `Code.forLoop` is to force me to include all parts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7587:360,variab,variables,360,https://hail.is,https://github.com/hail-is/hail/pull/7587,1,['variab'],['variables']
Modifiability,Have a new test to target that will verify correct code generation. Also refactor parameterpack a bit to be more traceable. High prio because this is blocking ptypes work.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8084:73,refactor,refactor,73,https://hail.is,https://github.com/hail-is/hail/pull/8084,1,['refactor'],['refactor']
Modifiability,"Having run through a test myself, it looks like we'll actually need to add back all the deleted config aside from the last bit that starts up the jupyter server. Aside from the last block that starts up jupyter, the rest is important logic to configure extensions and content management.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12788#issuecomment-1472586228:96,config,config,96,https://hail.is,https://github.com/hail-is/hail/pull/12788#issuecomment-1472586228,2,['config'],"['config', 'configure']"
Modifiability,"Heh. At least in my version of Docker, those are implicitly relative to the root not the WORKDIR:; ```; (base) dking@wm28c-761 /tmp % cat Dockerfile ; FROM ubuntu:20.04; WORKDIR /foo/bar; VOLUME baz; (base) dking@wm28c-761 /tmp % docker build -t foo . ; [+] Building 0.1s (6/6) FINISHED ; => [internal] load build definition from Dockerfile 0.0s; => => transferring dockerfile: 34B 0.0s; => [internal] load .dockerignore 0.0s; => => transferring context: 2B 0.0s; => [internal] load metadata for docker.io/library/ubuntu:20.04 0.0s; => [1/2] FROM docker.io/library/ubuntu:20.04 0.0s; => CACHED [2/2] WORKDIR /foo/bar 0.0s; => exporting to image 0.0s; => => exporting layers 0.0s; => => writing image sha256:217748640e5c53f72b8de9917010e5742fb8bef99a37dcb13ec59a903cb5834c 0.0s; => => naming to docker.io/library/foo 0.0s. Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them; (base) dking@wm28c-761 /tmp % docker run foo /bin/sh -c 'pwd && ls -l . && ls -l / && ls -l /baz'; /foo/bar; total 0; total 56; drwxr-xr-x 2 root root 4096 May 9 15:06 baz; lrwxrwxrwx 1 root root 7 Oct 19 2022 bin -> usr/bin; drwxr-xr-x 2 root root 4096 Apr 15 2020 boot; drwxr-xr-x 5 root root 340 May 9 15:06 dev; drwxr-xr-x 1 root root 4096 May 9 15:06 etc; drwxr-xr-x 3 root root 4096 May 9 15:01 foo; drwxr-xr-x 2 root root 4096 Apr 15 2020 home; lrwxrwxrwx 1 root root 7 Oct 19 2022 lib -> usr/lib; drwxr-xr-x 2 root root 4096 Oct 19 2022 media; drwxr-xr-x 2 root root 4096 Oct 19 2022 mnt; drwxr-xr-x 2 root root 4096 Oct 19 2022 opt; dr-xr-xr-x 238 root root 0 May 9 15:06 proc; drwx------ 2 root root 4096 Oct 19 2022 root; drwxr-xr-x 5 root root 4096 Oct 19 2022 run; lrwxrwxrwx 1 root root 8 Oct 19 2022 sbin -> usr/sbin; drwxr-xr-x 2 root root 4096 Oct 19 2022 srv; dr-xr-xr-x 13 root root 0 May 9 15:06 sys; drwxrwxrwt 2 root root 4096 Oct 19 2022 tmp; drwxr-xr-x 10 root root 4096 Oct 19 2022 usr; drwxr-xr-x 11 root root 4096 Oct 19 2022 var; total 0; (base) dki",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12990#issuecomment-1540332989:667,layers,layers,667,https://hail.is,https://github.com/hail-is/hail/pull/12990#issuecomment-1540332989,2,['layers'],['layers']
Modifiability,"Here is a Hail log.... I will work on getting the YARN container logs next. . more /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:513,config,configuration,513,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['config'],['configuration']
Modifiability,"Here is an example of the trailing slash issue:. What kibana sees:. ""GET /monitoring/kibana/ui/fonts/inter_ui/Inter-UI-Bold.woff2 HTTP/1.1"" 200 94840 ""https://internal.hail.is/monitoring/kibana/app/kibana"". config:. ```; location /monitoring/kibana/ {; proxy_pass http://kibana/;; }; ```. It may not be inconsistent with trailing / on proxy_pass stripping the url, but it sure is confusing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540622757:207,config,config,207,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540622757,1,['config'],['config']
Modifiability,"Here's a larger rewrite of Github readme, ready for feedback. The gitter links reflect hail and hail-dev as we want them to be, so before merging we should rename hail to hail-dev and create hail. I also think it'd be good to give a bit more context for users on what ""pre-alpha, very active dev"" does and does not mean. In particular, that Hail is usable and tested now, but liable to change in non backward-compatible ways. Thoughts on including / wording this?. We should also consider moving the Roadmap somewhere on the forum. I think the development forum is a good place for more detailed instructions on collaboration (forking, etc) and best practices.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/699#issuecomment-243136925:16,rewrite,rewrite,16,https://hail.is,https://github.com/hail-is/hail/pull/699#issuecomment-243136925,2,['rewrite'],['rewrite']
Modifiability,"Here's a link with an absolute time window: https://cloudlogging.app.goo.gl/gXAWZpZtUiV8jphXA. This is the assertion's stack trace:; ```; at scala.Predef$.assert(Predef.scala:208); at is.hail.QoBOutputStreamManager.createOutputStream(QoBAppender.scala:38); at org.apache.logging.log4j.core.appender.OutputStreamManager.getOutputStream(OutputStreamManager.java:165); at org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStreamManager.java:250); at org.apache.logging.log4j.core.appender.OutputStreamManager.flushBuffer(OutputStreamManager.java:283); at org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:294); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.directEncodeEvent(AbstractOutputStreamAppender.java:217); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.tryAppend(AbstractOutputStreamAppender.java:208); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:199); at org.apache.logging.log4j.core.config.AppenderControl.tryCallAppender(AppenderControl.java:161); ```. And the line of our code that triggers the logger appender:; ```; is.hail.JVMEntryway$2.run(JVMEntryway.java:139); ```. On that line, we should have already evaluated line 97:; ```; QoBOutputStreamManager.changeFileInAllAppenders(logFile);; ```; Which updates the filename for all `QoBOutputStreamManager`s. We should be the only ones allocating `QoBOutputStreamManager` (it has no magic annotations, we don't pass its constructor anywhere). We should only allocate `QoBOutputStreamManager` in its associated object. We always put it into the map in `getInstance`. We don't synchronize the other methods though, so that could be the issue? If we have a stale version of that map?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13242#issuecomment-1703383030:1083,config,config,1083,https://hail.is,https://github.com/hail-is/hail/issues/13242#issuecomment-1703383030,1,['config'],['config']
Modifiability,"Here's a typical interaction for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:377,config,configure,377,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020,2,['config'],['configure']
Modifiability,"Here's an example:. https://github.com/hail-is/hail/blob/6f5d1d9b511a1b5c91908a70f1afde909ec3226e/hail/src/main/scala/is/hail/expr/types/physical/PBaseStruct.scala#L346. We use `srcStructOffset` multiple times without binding, and then don't bind a variable before calling a copyFromType. I think the invariant is basically that if you get a Code[T], you must not use it more than once, since you're inlining arbitrary code. I've tried to enforce this model in my implementations",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8099#issuecomment-586413200:249,variab,variable,249,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586413200,1,['variab'],['variable']
Modifiability,"Here's my proposed interface (names to be changed, I'm terrible at those). ```; case class WithSource[T](value: T, source: InputSource) {; def map[U](f: T => U): WithSource[U] = {; try {; copy[U](value = f(value)); } catch {; case e: Exception => source.wrapError(e); }; }; }. abstract class InputSource {; def wrapError(e: Exception): Nothing; }. case class TextSource(line: String, file: String, position: Option[Int]) extends InputSource {; def wrapError(e: Exception): Nothing = {; val msg = e match {; case _: FatalException => e.getMessage; case _ => s""caught $e""; }; val lineToPrint =; if (line.length > 62); line.take(59) + ""...""; else; line. log.error(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $line"""""".stripMargin); fatal(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $lineToPrint"""""".stripMargin); }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233012302:421,extend,extends,421,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233012302,1,['extend'],['extends']
Modifiability,Here's the output of grepping for `dmk9z` in the root of the hail repo:; ```; config.mk:HAIL_TEST_GCS_BUCKET := hail-test-dmk9z. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10807#issuecomment-905867503:78,config,config,78,https://hail.is,https://github.com/hail-is/hail/pull/10807#issuecomment-905867503,1,['config'],['config']
Modifiability,"Here's the terraform configurations in GCP and Azure:. - GCP: Batch has admin storage permissions, as granted here https://github.com/hail-is/hail/blob/1f5e1540c04abfde58ead1084841fec5aa6e0ed3/infra/gcp/main.tf#L415-L424. We also grant it a Viewer role on the query bucket after that which seems redundant. We should really not grant it global storage admin and instead give it admin for just the query bucket and other associated batch buckets. I checked in hail-vdc and batch does not have the global storage admin role, and it has the Viewer role on the query bucket. I've changed that role now to admin on the query bucket. - Azure: Story is simpler. The `query` storage container is part of the `batch` storage account. The batch SP has ownership over the `batch` storage account and by extension all of the containers inside it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011:21,config,configurations,21,https://hail.is,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011,2,['config'],['configurations']
Modifiability,"Hey @danking,; Thanks so much for the advice. Your team has been very helpful and responsive. - I made the adjustment to my `create_intervals` function; - Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. I tried your suggestion of writing the data to a table first, but my `cols` field doesn't contain the computed HWE values. These are contained within the `entries` field. Table description is below. I tried modifying the code to what is shown below but I'm still having the same issue. Also tried increasing the RAM to max available per CPU. One thing I noticed is the `mt_hwe_vals` variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'ancestry': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'filters': set<str>; 'variant_qc': struct {; gq_stats: struct {; mean: float64, ; stdev: float64, ; min: float64, ; max: float64; }, ; call_rate: float64, ; n_called: int64, ; n_not_called: int64, ; n_filtered: int64, ; n_het: int64, ; n_non_ref: int64, ; het_freq_hwe: float64, ; p_value_hwe: float64, ; p_value_excess_het: float64; }; 'info': struct {; AC: array<int32>, ; AF: array<float64>, ; AN: int32, ; homozygote_count: array<int32>; }; 'a_index': int32; 'was_split': bool; ----------------------------------------; Entry fields:; 'hwe': struct {; het_freq_hwe: float64, ; p_value: float64; }; ----------------------------------------; Column key: ['ancestry']; Row key: ['locus', 'alleles']; ----------------------------------------; ```. ```python; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)). # T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1674895492:649,variab,variable,649,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1674895492,2,['variab'],['variable']
Modifiability,"Hi @nawatts, your comments are very welcome, and I appreciate your perspective as a hailctl user. OK, mulling over your comments, I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. I think this also addresses the issue `hailctl dataproc submit` not supporting `--`, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: `hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters`. What do you think?. > it would be nice if the --configuration/--gcloud-configuration argument was consistent across hailctl dataproc commands; > It would also be nice to standardize on kebab case for all arguments. Agree on both accounts, will fix. Thanks again!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-758080935:703,config,configuration,703,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758080935,2,['config'],['configuration']
Modifiability,"Hi Cotton,. Interesting that this is during tablet creation not while inserting data.; Looks like this is a known issue, but with no fix or workaround yet that I; can see:. https://issues.cloudera.org/plugins/servlet/mobile#issue/KUDU-383. Does it work if you retry, or delete the table and retry? I successfully; imported chr1 from 1k genomes on a 6 node cluster. This would create fewer; tablets though as it only covers one chromosome, so I should try with the; full dataset - I'll do that in the next few days when I'm back from; travelling. Thanks for trying it out. Do you have any more review comments for the PR?. Cheers,; Tom; On 11 Apr 2016 21:29, ""cseed"" notifications@github.com wrote:. Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to; 40m to fix a The requested number of tablets is over the permitted maximum; (100) error. I was able to write a small table. When I tried to write a; larger file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208722298:201,plugin,plugins,201,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298,1,['plugin'],['plugins']
Modifiability,"Hi Tim,. What's the problem with this implementation? I've tested it and it works... On Wed, Sep 21, 2016 at 11:07 AM, Tim Poterba notifications@github.com; wrote:. > Laurent, I was totally wrong about being able to do this per-command --; > I'm really sorry. I thought that it would be possible to create a new; > configuration just for this command and use that, but this is only possible; > for HadoopConfigurations and not SparkContexts. Can you reopen the old; > PR? That model is our only option.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/826#issuecomment-248641543, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgcPW4xK16W3DlZfdE5U6RTcVmJthks5qsUhMgaJpZM4KC1O-; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/826#issuecomment-248643185:315,config,configuration,315,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248643185,1,['config'],['configuration']
Modifiability,"Hi Tim,; Sorry for the bother again. I am following this short tutorial as described [here](https://gnomad.broadinstitute.org/news/2021-09-using-the-gnomad-ancestry-principal-components-analysis-loadings-and-random-forest-classifier-on-your-dataset/). The code snippet was working properly with earlier. Now that I have installed hail from pip, I have this error while running RF model. ```; ht, rf_model = assign_population_pcs(; ... ht,; ... pc_cols=ht.scores,; ... fit=fit,; ... ). 2022-09-29 14:55:46 Hail: INFO: Coerced sorted dataset (0 + 1) / 1]; INFO (gnomad.sample_qc.ancestry 224): Found the following sample count after population assignment: sas: 1; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/bioinfoRD/ARCdata/Projects_AMT/conda_envs/hail/lib/python3.10/site-packages/gnomad/sample_qc/ancestry.py"", line 235, in assign_population_pcs; min_assignment_prob=min_prob, error_rate=error_rate; UnboundLocalError: local variable 'error_rate' referenced before assignment; ```. Could you please suggest what might be happening here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759:963,variab,variable,963,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759,1,['variab'],['variable']
Modifiability,"Hi Vlad, thanks for the PR! I'm afraid there are some internal migrations we're making that are probably not clear from just looking at the codebase. Are you up to date on our `main`? We've found working with `config.mk` cumbersome because it can be stale if you switch between different instances of Batch (e.g. one deployed in azure and the other in GCP). > DOCKER_ROOT_IMAGE used to build batch workers and benchmark. I've recently updated the scripts for building the batch worker VM image to query kubernetes directly and we should probably do the same for benchmark. > HAIL_TEST_GCS_BUCKET used to build query; KUBERNETES_SERVER_URL used to build amundsen. These services are both currently deleted in our `main`. > PROJECT, ZONE, REGION are probably not need, but might make sense to add for consistency. These will fail in an Azure deployment, and while we want to move away from `config.mk` entirely, we would at least want it to contain configurations that are valid across clouds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11371#issuecomment-1041941055:210,config,config,210,https://hail.is,https://github.com/hail-is/hail/pull/11371#issuecomment-1041941055,6,['config'],"['config', 'configurations']"
Modifiability,"Hi!. Trying to calculate polygenic risk score with code from the [Polygenic Score Calculation](https://hail.is/docs/0.2/guides/genetics.html#polygenic-score-calculation), getting error with stacktrace:. `2022-05-14 12:09:07 Hail: INFO: Running Hail version 0.2.94-f0b38d6c436f; 2022-05-14 12:09:08 SparkContext: WARN: Using an existing SparkContext; some configuration may not take effect.; 2022-05-14 12:09:08 root: INFO: RegionPool: initialized for thread 30: Thread-4; 2022-05-14 12:09:09 MemoryStore: INFO: Block broadcast_0 stored as values in memory (estimated size 34.3 KiB, free 434.4 MiB); 2022-05-14 12:09:09 MemoryStore: INFO: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 434.4 MiB); 2022-05-14 12:09:09 BlockManagerInfo: INFO: Added broadcast_0_piece0 in memory on 10.40.3.21:33951 (size: 3.2 KiB, free: 434.4 MiB); 2022-05-14 12:09:09 SparkContext: INFO: Created broadcast 0 from broadcast at SparkBackend.scala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:355,config,configuration,355,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['config'],['configuration']
Modifiability,"Hi, I am using hail in spark, but encounter some problem.; I followed the ""Getting Started"" to deploy hail , and build Hail from source; (https://hail.is/docs/stable/getting_started.html). I set the environmental variables as follows:; ```; export SPARK_HOME=/opt/Software/spark/spark-2.0.2-bin-hadoop2.6; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```; I put the vcf file in hadoop， as follows:; ```; [hdfs@tele-1 root]$ hdfs dfs -ls /hail/test; Found 1 items; -rw-r--r-- 3 hdfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076:213,variab,variables,213,https://hail.is,https://github.com/hail-is/hail/issues/2076,1,['variab'],['variables']
Modifiability,"Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. ERROR:; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/550095/splitmulti_1_1.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1003:40,config,configured,40,https://hail.is,https://github.com/hail-is/hail/issues/1003,1,['config'],['configured']
Modifiability,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584:145,config,config,145,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584,13,"['Config', 'config']","['Config', 'config', 'configuration']"
Modifiability,"Hi, when we executed the command above, the results are as follows:; ```; [root@tele-1 ~]# PYSPARK_PYTHON=""ipython"" pyspark --conf spark.sql.files.openCostInBytes=1099511627776 --conf spark.sql.files.maxPartitionBytes=1099511627776 --conf spark.hadoop.parquet.block.size=1099511627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer; /usr/local/lib/python3.5/site-packages/IPython/core/history.py:228: UserWarning: IPython History requires SQLite, your history will not be saved; warn(""IPython History requires SQLite, your history will not be saved""); Python 3.5.2 (default, Jul 12 2017, 14:00:23) ; Type ""copyright"", ""credits"" or ""license"" for more information. IPython 5.1.0 -- An enhanced Interactive Python.; ? -> Introduction and overview of IPython's features.; %quickref -> Quick reference.; help -> Python's own help system.; object? -> Details about 'object', use 'object??' for extra details.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/09 12:51:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/09 12:51:55 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 12:51:55 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 12:51:55 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 12:51:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Welcome to; ____ __; / __/__ ___ _____/ /",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609:703,enhance,enhanced,703,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609,1,['enhance'],['enhanced']
Modifiability,"Hi,. I'm just curious if you tried black on a code that uses Hail query. As far as I see, PEP8 conflicts with the code style adopted in the Hail docs, e.g. black would remove spaces in named function arguments:. ```; - mt = mt.annotate_entries(GT = lgt_to_gt(mt.LGT, mt.LA)); + mt = mt.annotate_entries(GT=lgt_to_gt(mt.LGT, mt.LA)); ```. On the other hand, query can be seen as a DSL on top of Python, so the same guidelines probably don't need to be applied to it. Wondering if you had thoughts about lining the query code? We will be writing a lot of that in the nearest future in the Centre for Population Genomics, and would love to set up some style checks, or even automate that with a tool like black. And on black - are you considering automating code refactoring with black as part of the CI? Or you wanted to just do checks, alongside with pylint?. Vlad",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9931#issuecomment-768677633:760,refactor,refactoring,760,https://hail.is,https://github.com/hail-is/hail/pull/9931#issuecomment-768677633,2,['refactor'],['refactoring']
Modifiability,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-250697347:20,config,configured,20,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347,1,['config'],['configured']
Modifiability,"Hmm, looks like it's something like `spark.executorEnv.FOO=...` based on this: https://spark.apache.org/docs/latest/configuration.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583434753:116,config,configuration,116,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583434753,1,['config'],['configuration']
Modifiability,"Hmm. I trust the code now. I test against several R SKAT runs. I'm not sure I understand how we derive that Q is generalized chi-squared distributed. We use the residual phenotypes in the calculation of Q, but those are inverse-logit transformed normal variables. The derivation for the linear case doesn't apply, as far as I can tell. I assume the residuals are Bernoulli distributed? Maybe not. I guess the phenotypes are Bernoulli but the errors aren't? I'm not sure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12643#issuecomment-1419295599:253,variab,variables,253,https://hail.is,https://github.com/hail-is/hail/pull/12643#issuecomment-1419295599,1,['variab'],['variables']
Modifiability,Hmm. I’ll have to sort this out tomorrow. Not sure what’s going on with that. It seems like the shadowTestJar target is probably not correctly pulling in the testImolemebtation configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1709453126:177,config,configuration,177,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1709453126,1,['config'],['configuration']
Modifiability,"Hmmm... I guess we could just have it look for an environment variable, instead of asking for a flag on `HailContext`. Easy enough to set the environment variable in the `hailctl` init scripts. . You should maybe name it `HAIL_MKL_PATH` instead of `MKL_PATH`, so people know what this variable is for / doesn't conflict with other tools variables.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10770#issuecomment-897020574:62,variab,variable,62,https://hail.is,https://github.com/hail-is/hail/pull/10770#issuecomment-897020574,4,['variab'],"['variable', 'variables']"
Modifiability,"Hmph, ya this seems annoyingly complicated, and I'd prefer to make one command with opinionated but configurable defaults than have different commands. One thing that feels inconsistent here is what we do in the Batch interface. We don't have the equivalent of `HAIL_QUERY_BACKEND` and a user specifically has to create a `ServiceBackend` as opposed to relying on the environment dictating which model to use. I feel like it would be OK if we documented `hailctl batch submit` as ""distribute everything on batch by default""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12471#issuecomment-1324131496:100,config,configurable,100,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1324131496,1,['config'],['configurable']
Modifiability,"How's this?; ```; info(s""Running linear regression per row on $n samples for ${ y.cols } response ${ plural(y.cols, ""variable"") } y,\n""; + s"" with input variable x, intercept, and ${ k - 1 } additional ${ plural(k - 1, ""covariate"") }...""); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2777#issuecomment-359417991:117,variab,variable,117,https://hail.is,https://github.com/hail-is/hail/issues/2777#issuecomment-359417991,2,['variab'],['variable']
Modifiability,I *think* I've addressed all concerns. It would help if @danking could test all functions; I only have a partially configured environment.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4994#issuecomment-448388700:115,config,configured,115,https://hail.is,https://github.com/hail-is/hail/pull/4994#issuecomment-448388700,1,['config'],['configured']
Modifiability,I accidentally copy pasted the line. If you look a few lines down you can see the actual name/value pair for that environment variable.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13628:126,variab,variable,126,https://hail.is,https://github.com/hail-is/hail/pull/13628,1,['variab'],['variable']
Modifiability,"I actually think moving `root` into a `location` at the bottom of the file does not matter, but it seems easier to spot this way. I think the actual issue is that this server has never heard of `https://hail.is`, so the rewrite doesn't work?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4726#issuecomment-435889718:220,rewrite,rewrite,220,https://hail.is,https://github.com/hail-is/hail/pull/4726#issuecomment-435889718,1,['rewrite'],['rewrite']
Modifiability,I added `organization_domain` to the global-config in hail-vdc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11147#issuecomment-992708508:44,config,config,44,https://hail.is,https://github.com/hail-is/hail/pull/11147#issuecomment-992708508,1,['config'],['config']
Modifiability,"I added a `PropertySuite` and deleted the `check` business. I feel this is an improvement, but also that we can do better. You can still have an orphaned `Prop` by writing:. ```; class MyProperties extends PropertySuite {; forAll ... // no property(""name"") = ...; }; ```. I think better would be for `PropretySuite` to declare `forAll` and make `forAll` take a name. `PropertySuite` extends `SparkSuite`. I was seeing some strange behavior that I don't fully understand if I made it extend `TestNGSuite` and then mixed `PropertySuite` and `SparkSuite` in a test suite. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/732:198,extend,extends,198,https://hail.is,https://github.com/hail-is/hail/pull/732,3,['extend'],"['extend', 'extends']"
Modifiability,"I added a new field to the global config that is gs:// + hail_test_gcs_bucket named test_blob_storage_uri and use that wherever it doesn't matter that the backend be google storage, which is essentially everywhere except for the FS/copy tests, where we specifically want a test gcs bucket.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10960:34,config,config,34,https://hail.is,https://github.com/hail-is/hail/pull/10960,1,['config'],['config']
Modifiability,"I added the capability for the deploy config to find the domain from setting it in the config.ini file. This way users only use `hailctl config set domain` rather than `hailctl dev config set domain`. In addition, we use this new capability to make a test in Batch work on Azure. CC: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11113:38,config,config,38,https://hail.is,https://github.com/hail-is/hail/pull/11113,4,['config'],['config']
Modifiability,"I added the configuration option for the minimum number of workers that should be present at any time. I tested this in my namespace. I'd like you to double check the logic is correct for the number of workers needed as I derived it by working through examples:. ```python3; n_live_instances = self.n_instances_by_state['pending'] + self.n_instances_by_state['active']; n_standing_instances_needed = max(0, self.min_instances - self.n_instances); n_standing_instances_needed = min(; n_standing_instances_needed,; self.max_live_instances - n_live_instances,; self.max_instances - self.n_instances,; remaining_instances_per_autoscaler_loop,; # 20 queries/s; our GCE long-run quota; 300,; ); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12742:12,config,configuration,12,https://hail.is,https://github.com/hail-is/hail/pull/12742,1,['config'],['configuration']
Modifiability,"I addressed some comments. I still need to:; - expose hts_genotype_schema in python, and; - figure out what to rewrite instead of ""genotype"" in the VariantDataset docs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2480#issuecomment-347072699:111,rewrite,rewrite,111,https://hail.is,https://github.com/hail-is/hail/pull/2480#issuecomment-347072699,1,['rewrite'],['rewrite']
Modifiability,"I agree that we never carefully considered switching domains at the user-level. I'm mostly concerned with Broadies using the GCP instance for now. I think how to deal with domains really gets into the idea of having config profiles, which just makes all of this more complicated. Ergo, my gut is: if you're not a Broadie using Google, you have to do some extra work, but that set of users is very small.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1663143699:216,config,config,216,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1663143699,1,['config'],['config']
Modifiability,"I also added factories for Google and Azure so that we only; check the global-config cloud setting once, not repeatedly; every time we create an AsyncFS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11201:78,config,config,78,https://hail.is,https://github.com/hail-is/hail/pull/11201,1,['config'],['config']
Modifiability,"I also did a bit of refactoring in lmmreg to make this change more organic. I will add a test asap, but want to simultaneously give @alexb-3 a chance to look over the math.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1720:20,refactor,refactoring,20,https://hail.is,https://github.com/hail-is/hail/pull/1720,1,['refactor'],['refactoring']
Modifiability,I also eliminated a useless internal function. I think this was an artifact leftover from a refactor.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6902#issuecomment-522672211:92,refactor,refactor,92,https://hail.is,https://github.com/hail-is/hail/pull/6902#issuecomment-522672211,1,['refactor'],['refactor']
Modifiability,I also made a bit of a restructuring to all `BlockMatrix` `Gen`erators because you can't use default arguments with overloaded methods but I wanted to parameterize everything by an element `Gen`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2366#issuecomment-339832499:151,parameteriz,parameterize,151,https://hail.is,https://github.com/hail-is/hail/pull/2366#issuecomment-339832499,1,['parameteriz'],['parameterize']
Modifiability,"I also prefer the second option in https://github.com/hail-is/hail/pull/9842#issuecomment-758128554. The more I think about this, the more problematic the notion of having an opaque list of ""arguments to pass through to gcloud"" seems. For example, `hailctl dataproc start` may run multiple gcloud commands: one to start the cluster and another to apply tags to the master node. In that case, we'd want to pass through extra args to the cluster start command, but not the apply tags command. Or `hailctl dataproc modify`, where we might want to accept extra args for `gcloud dataproc clusters update`. Those args shouldn't be passed through to the `gcloud compute ssh` commands, but options like `--project` or `--configuration` should. It seems like pass through arguments would be best handled on a case by case basis for each hailctl command. That would probably make any approach that required parsing those pass through options more cumbersome to use.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-758143032:713,config,configuration,713,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758143032,1,['config'],['configuration']
Modifiability,I also reduced the layers and size of the notebook a bit. It's still ~8GB. I added `time` to the make command for curiosity's sake.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4976:19,layers,layers,19,https://hail.is,https://github.com/hail-is/hail/pull/4976,1,['layers'],['layers']
Modifiability,"I based this on the row store in 0.2, in order to preserve partitioning on block matrices under read / write. @danking this should adapt to HailBlockMatrix with basically no change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2276:131,adapt,adapt,131,https://hail.is,https://github.com/hail-is/hail/pull/2276,1,['adapt'],['adapt']
Modifiability,I believe that even a local cluster (2+ jvms) would be sufficient to reproduce this error. I just have no idea how to configure such a thing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5033#issuecomment-449493275:118,config,configure,118,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449493275,1,['config'],['configure']
Modifiability,"I believe the code is behaving as designed. The error message, however, leaves a lot to be desired. A few take-aways:; 1. Hail doesn't support heterogeneous arrays. In situations like these, using an array of tuples has the desired outcome.; 2. The variable `x` in `lambda x:` is already a hail expression and so you don't need to explictly capture it as a `literal`.; a. While support for using hail expressions with `literal` was added in https://github.com/hail-is/hail/pull/4086 (see the issue for motivation), it can only be used when that expression is self-contained (ie it's not dependent on another hail expression, eg referencing an element of a hail array expression or tuple expression etc).; b. Our evaluation strategy is to `eval` the expression, then broadcast the result in a `literal`.; c. `eval` correctly complains that that expression has free variables and so can't be evaluated.; d. This error is ugly and has little to do with what the user wanted to achieve. Off the top of my head, a couple of ways to proceed:; 1. The hardest (but backwards compatible) fix is to somehow provide a good error message that the `x` in `lambda x:` in this particular context is a hail expression containing a reference that you should not use with `literal`.; 2. Remove support for using hail expressions with literal.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624335413:249,variab,variable,249,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624335413,2,['variab'],"['variable', 'variables']"
Modifiability,"I believe the next step should be the minimal changes to move the global variables in `server/globals.py` to a database table(s). We also need to think about sql configuration. For now, I suppose we can just create a table manually, but we need a longer term strategy for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-456848381:73,variab,variables,73,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456848381,2,"['config', 'variab']","['configuration', 'variables']"
Modifiability,"I borrowed the MissingArrayBuilder from #3458. I tried to unify this with ArrayBuilder with inheritance / trait, but was having trouble doing so.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3503#issuecomment-386721940:92,inherit,inheritance,92,https://hail.is,https://github.com/hail-is/hail/pull/3503#issuecomment-386721940,1,['inherit'],['inheritance']
Modifiability,"I broke this when I separated out the dependencies for hail into two layers:. 1. hailtop dependencies; 2. hail dependencies, which builds on top of the hailtop dependencies. This fix does two things:; - Use the full dependencies in 1 & 2; - Use fully pinned dependencies when installing on clusters which seems better than using our wide-range dependencies. I left the `install-deps` and `install-dev-deps` as the normal requirements files as those are meant for development (I think?) but am happy to take opinions on whether we should use fully pinned deps there as well. I have so far been going by the rule of thumb of fully-pinned for CI and production environments, more lax rules for dev environments. See [here](https://github.com/hail-is/hail/pull/12446#discussion_r1030986069) for additional context. cc: @tpoterba, any idea why the test dataproc test succeeded?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12510:69,layers,layers,69,https://hail.is,https://github.com/hail-is/hail/pull/12510,1,['layers'],['layers']
Modifiability,I can break this up further if you want. Big changes:. - change batch.py to support multi-line commands (use `{\n...\n}`); - change batch.py and job.py to support per-job environment variables (and add tests to test_batch.py); - add `partition` to hail top mirroring the implementation in Scala; - implement BatchPoolExecutor which attempts to faithfully implement the interface of concurrent.futures.Executor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9035:183,variab,variables,183,https://hail.is,https://github.com/hail-is/hail/pull/9035,1,['variab'],['variables']
Modifiability,I completely refactored this. You'll probably want to review the auth.py code from scratch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9658#issuecomment-720219339:13,refactor,refactored,13,https://hail.is,https://github.com/hail-is/hail/pull/9658#issuecomment-720219339,1,['refactor'],['refactored']
Modifiability,I could not get the `\d` to work for me on an Ubuntu machine. The `[0-9]`; range seems to work with extended regexps.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11568:100,extend,extended,100,https://hail.is,https://github.com/hail-is/hail/pull/11568,1,['extend'],['extended']
Modifiability,"I created a new multi-branch configuration that should be better for what we are trying to accomplish. This should fix issues 2 and 3. . For the reproducibility of errors, that will probably take both setting the random seed parameter in Hail for all random tests and getting Jenkins to give better error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/335#issuecomment-214377125:29,config,configuration,29,https://hail.is,https://github.com/hail-is/hail/issues/335#issuecomment-214377125,1,['config'],['configuration']
Modifiability,"I deleted the LinearRegressionFromHcsCommand and associated tests as it'd fallen out of sync with how hcs evolved for T2D, it's independent of the rest of the PR and I can add this functionality back later (at which point I imagine there will be other changes both to hcs and to the stats interfaces more generally).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/517#issuecomment-236385122:106,evolve,evolved,106,https://hail.is,https://github.com/hail-is/hail/pull/517#issuecomment-236385122,1,['evolve'],['evolved']
Modifiability,I develop accross a couple of OSes and my username isn't always the same. I'd like to expose this make variable so that I can push images to the same location.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12846:103,variab,variable,103,https://hail.is,https://github.com/hail-is/hail/pull/12846,1,['variab'],['variable']
Modifiability,"I didn't give very good names to all the introduced `memoize`s, but those will all go away as we push the refactoring through.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10848:106,refactor,refactoring,106,https://hail.is,https://github.com/hail-is/hail/pull/10848,1,['refactor'],['refactoring']
Modifiability,"I didn't mean disable all optimization, I meant optimizations that specifically transform `TableCount` (e.g. the TableCount rewrite rules, the pruner can't prune the input to TableCount, etc.). As you note, it will also have to modify the implementation to run the RDD. Except that regression returns a table but force count returns a number. We could have set of 6 opaque operations: {Table, MatrixTable} => {Table, MatrixTable, Value} (skipping ones that don't actually appear).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5015#issuecomment-448765387:124,rewrite,rewrite,124,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448765387,1,['rewrite'],['rewrite']
Modifiability,"I didn't set that to a value,and kept it by default.; I have no idea about which variables should be set to some value, is there a guide to show all the variables I should set ? I didn't see something like this in the hail website?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338462427:81,variab,variables,81,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338462427,4,['variab'],['variables']
Modifiability,"I didn't want to put too much in this PR but I dislike reassigning the loop variable, it only feels like a footgun. So will happily approve the follow-up PR that fixes violations of that rule. There a couple of rules that I would like to gradually follow more. w.r.t. convincing it is ok, I suppose I just trust the ruff developers (and the big projects that have already converted) that given we don't use any niche flake8 plugins we are not dropping any rules by moving over.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12967#issuecomment-1535600667:76,variab,variable,76,https://hail.is,https://github.com/hail-is/hail/pull/12967#issuecomment-1535600667,2,"['plugin', 'variab']","['plugins', 'variable']"
Modifiability,"I discovered [issue forms](https://github.blog/changelog/2021-06-23-issues-forms-beta-for-public-repositories/) the other day and thought it might be helpful for directing users to the discussion forum / Zulip chatroom. With this configuration, when someone opens an issue, they'll be presented with some options:; ![Screen Shot 2023-01-13 at 8 01 11 AM](https://user-images.githubusercontent.com/1156625/212326189-214fb8b2-e210-4c96-8b52-7000d5025148.png). If they choose to report a bug, they'll be presented with a form prompting for Hail version and log output.; ![Screen Shot 2023-01-13 at 8 01 46 AM](https://user-images.githubusercontent.com/1156625/212326274-affeaa80-adec-45c9-b436-73059c6fc841.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12595:230,config,configuration,230,https://hail.is,https://github.com/hail-is/hail/pull/12595,1,['config'],['configuration']
Modifiability,I dismissed them as won’t fix because they’re leaking secrets as intended into config files.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12794#issuecomment-1505797153:79,config,config,79,https://hail.is,https://github.com/hail-is/hail/pull/12794#issuecomment-1505797153,1,['config'],['config']
Modifiability,"I do not know why we do this, but we return a `Code` here, not a `Value`, so it seems silly to store in a variable. In practice, the LIR is littered with invocations that store their value in a local which is then read and stored in a different local beause we `memoize` the function call.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13777:106,variab,variable,106,https://hail.is,https://github.com/hail-is/hail/pull/13777,1,['variab'],['variable']
Modifiability,"I don't know the exact rules about java/scala protected variables -- but `protected[ir]` means anything in `is.hail.expr.ir` should be able to see it. Which is the whole compiler, basically, so it doesn't do a lot. But I think that's preferable to requiring that InferPType needs to be in the IR file, or extend it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6594#issuecomment-515073290:56,variab,variables,56,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515073290,2,"['extend', 'variab']","['extend', 'variables']"
Modifiability,"I don't love what I've had to do with the deploy config stuff. That's in my opinion the most finicky part of this (has already broken multiple times) and it's mostly our fault, because we overload the `namespace` parameter with both identifying the namespace in Kubernetes and signifying whether the environment is prod or not. All I want really is to change the `domain` to a domain and path prefix, and not have the namespace have such an impact on routing. Like what if `namespace` didn't affect routing, but if the deploy config only gave a domain with no path e.g. `hail.is`, we use subdomains so `batch.hail.is`, but if we provided a domain with a path prefix like `internal.hail.is/dgoldste`, we make the batch root `internal.hail.is/dgoldste/batch`?. Alternative: Actually have and use a `base_path` in the deploy config. This would be used in dev and terra environments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13944#issuecomment-1785575616:49,config,config,49,https://hail.is,https://github.com/hail-is/hail/pull/13944#issuecomment-1785575616,3,['config'],['config']
Modifiability,"I don't think I actually understand how artifact and snapshot dependencies work in TeamCity. I thought a build by the main build configuration (the regular CI) would trigger a build of the docs build configuration. This was not the case and I'm not sure why. I've set up the docs build to trigger on any change to master. Unfortunately, we have to `compileScala` twice because these are separate builds. I'll add an issue to clean this up and make it more sensible. There's got to be a right way to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/733#issuecomment-244475645:129,config,configuration,129,https://hail.is,https://github.com/hail-is/hail/issues/733#issuecomment-244475645,2,['config'],['configuration']
Modifiability,"I feel a bit like a cheat here since there's been a fair bit of work since you last reviewed. Most of it was fixes of tiny bugs that the CI revealed. There was [one, kind of notable, change](https://github.com/hail-is/hail/pull/5194/commits/f95e4e0ff1cdd2865cf703aa27f780c7f162316c). I removed Spark from the Dockerfile. It is no longer necessary because the pip install will pull the correct version of Spark. To avoid pulling Spark on each PR build, I cache 2.2.0 (and all our other pip dependencies) in the hail conda env in the docker image. Doing this required that I move our requirements into a requirements file which is parameterized by spark version. A good follow up PR would be to either a) entirely remove dependency on conda or b) generate the conda `environment.yml` from `requirements.txt.in`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333:629,parameteriz,parameterized,629,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333,1,['parameteriz'],['parameterized']
Modifiability,"I finally figured out how to get the authorization bearer token for the Grafana robot into Grafana automatically. The problem I'm running into right now is when we load a datasource from a configuration file, we can not edit any of the settings in the UI. We'd want to make sure all the prometheus settings we want are inside the new config file. I also don't want to accidentally overwrite any of the existing configuration.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10772:189,config,configuration,189,https://hail.is,https://github.com/hail-is/hail/pull/10772,3,['config'],"['config', 'configuration']"
Modifiability,"I fixed sql-config.cnf in the last change, not sql-config.json. I just changed everything to treat a key being `null` in JSON the; same as the key not existing. The situation now:; - sql-config.cnf will not have the string None (seems right, cnf is for MySQL); - sql-config.json might have `null` which is treated the same as a missing key",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8500:12,config,config,12,https://hail.is,https://github.com/hail-is/hail/pull/8500,4,['config'],['config']
Modifiability,I fixed the code so that there's one interpolation step. What I did was use the job name plus a random token as the job directory name and the batch tmp directory is an environment variable. I'm happy with this setup. Then it will be easier for users to tell where files came from if they give jobs unique names.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10716#issuecomment-895316665:181,variab,variable,181,https://hail.is,https://github.com/hail-is/hail/pull/10716#issuecomment-895316665,1,['variab'],['variable']
Modifiability,"I forgot that we still had cron jobs running gcr-cleaner daily. This could have been conflicting with the new cleanup policy deletion settings. Let's reopen if this occurs again. Posting the job configurations here before I delete the jobs. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:195,config,configurations,195,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,1,['config'],['configurations']
Modifiability,I forgot the Java tests were split in the same way as the batch tests and had forgot to add the new env variable in each split.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10807#issuecomment-905458230:104,variab,variable,104,https://hail.is,https://github.com/hail-is/hail/pull/10807#issuecomment-905458230,1,['variab'],['variable']
Modifiability,I forgot to include the changes in #14056 to the scala code as well. This favors using `basePath` in the Scala deploy config over the `defaultNamespace`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14195:118,config,config,118,https://hail.is,https://github.com/hail-is/hail/pull/14195,1,['config'],['config']
Modifiability,"I got a bad local variable compilation error without the fields for the bindings. I suspect this is a locals/fields problem elsewhere but don't want to debug right now, so reverted.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7830#issuecomment-574725641:18,variab,variable,18,https://hail.is,https://github.com/hail-is/hail/pull/7830#issuecomment-574725641,1,['variab'],['variable']
Modifiability,I got a timeout!; ```; SocketTimeoutException: connect timed out. Java stack trace:; java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:254); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:823,config,configure,823,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,1,['config'],['configure']
Modifiability,"I guess I'm OK accepting this as-is, since I expect us to have the real fix, adaptive branching, in soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11784#issuecomment-1105467838:77,adapt,adaptive,77,https://hail.is,https://github.com/hail-is/hail/pull/11784#issuecomment-1105467838,1,['adapt'],['adaptive']
Modifiability,"I had a choice on how to implement this and I decided to add a JobTask class that takes care of a single pod and the Job changes to just be a manager of the pods. However, I could have done it all within the Job if you think that is clearer. Happy to refactor if needed. Please look and see if I have enough tests. The tests are passing, but I'm getting this error message. Is this expected or a bug in my code? . ```; INFO	| 2019-02-22 11:48:48,126 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,210 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; INFO	| 2019-02-22 11:48:48,833 	| server.py 	| mark_complete:190 | wrote log for job 61, main task to logs/job-61-main.log; INFO	| 2019-02-22 11:48:48,845 	| server.py 	| set_state:272 | job 61 changed state: Created -> Complete; INFO	| 2019-02-22 11:48:48,851 	| server.py 	| parent_new_state:287 | parent 61 successfully complete for 63; INFO	| 2019-02-22 11:48:48,857 	| server.py 	| parent_new_state:292 | all parents successfully complete for 63, creating pod; INFO	| 2019-02-22 11:48:48,918 	| server.py 	| create_pod:135 | created pod name: job-63-main-qqwb2 for job 63, main task; INFO	| 2019-02-22 11:48:48,929 	| server.py 	| mark_complete:330 | job 61 complete, exit_code 0; INFO	| 2019-02-22 11:48:48,995 	| _internal.py 	| _log:87 | 127.0.0.1 - - [22/Feb/2019 11:48:48] ""POST /pod_changed HTTP/1.1"" 204 -; [2019-02-22 11:48:49,043] ERROR in app: Exception on /test [POST]; Traceback (most recent call last):; File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1982, in wsgi_app; response = self.full_dispatch_request(); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1615, in full_dispatch_request; return self.finalize_request(rv); File ""//anaconda/envs/hail-batch/lib/python3.6/site-packages/flask/app.py"", line 1630, in finalize_request",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5418:251,refactor,refactor,251,https://hail.is,https://github.com/hail-is/hail/pull/5418,1,['refactor'],['refactor']
Modifiability,"I had exactly the same idea, but it causes problems with inheritance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1826#issuecomment-301948641:57,inherit,inheritance,57,https://hail.is,https://github.com/hail-is/hail/pull/1826#issuecomment-301948641,1,['inherit'],['inheritance']
Modifiability,I had problems with calling the Make rules that create batch worker images; but I see that you already remove them and created a script that doesn't depend on config.mk: https://github.com/hail-is/hail/blob/main/batch/gcp-create-worker-image.sh. Thanks for a response! I guess I don't have to worry about config.mk :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11371#issuecomment-1045522380:159,config,config,159,https://hail.is,https://github.com/hail-is/hail/pull/11371#issuecomment-1045522380,2,['config'],['config']
Modifiability,"I hardcoded us-central1, which is the only thing we're using right now. Otherwise, we'd have to change CI before deploy. Also, clearly a fixed global zone is naive, so I think we have to reconsider the GCP configuration going forward. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8016:206,config,configuration,206,https://hail.is,https://github.com/hail-is/hail/pull/8016,1,['config'],['configuration']
Modifiability,"I have commit `a451e1aaa5d1dd4cc055f8e7c1e261aa59eabeca`, I built the jar as `cd hail && ./gradlew shadowJar`. I have this file:; ```; (foo) # cat /tmp/failure.R ; data(mtcars); hail_jar <- ""/Users/bking/projects/hail/hail/build/libs/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sessionInfo(); sparklyr::invoke(ht, ""count""); ```. it generates this output:; ```; (foo) # Rscript /tmp/failure.R; R version 3.5.1 (2018-07-02); Platform: x86_64-apple-darwin17.6.0 (64-bit); Running under: macOS High Sierra 10.13.6. Matrix products: default; BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib; LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib. locale:; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8. attached base packages:; [1] stats graphics grDevices utils datasets methods base . loaded via a namespace (and not attached):; [1] Rcpp_0.12.19 dbplyr_1.2.2 compiler_3.5.1 pillar_1.3.0 ; [5] later_0.7.5 bindr_0.1.1 r2d3_0.2.2 base64enc_0.1-3 ; [9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977:439,config,config,439,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977,3,['config'],['config']
Modifiability,"I have diagnosed the root cause of the issue observed by; both Patrick and Chris: incorrect spilling of method parameter variables. This patch makes the bug impossible to replicate using proper interfaces,; though does not fix the underlying issue in LIR. Here's a way to replicate:. ```; val mb = kb.genEmitMethod(""btree_foo"", FastIndexedSeq[ParamType](typeInfo[Long]), typeInfo[Unit]); mb.voidWithBuilder { cb =>; val arg = mb.getCodeParam[Long](1).asInstanceOf[Settable[Long]]. cb.assign(arg, arg + 1L). (0 until 100).foreach { i =>; cb.println(s""i=$i, arg="", arg.toS); }. }; cb.invokeVoid(mb,const( 0L)); ```. called with `0`, this prints `1` until i=84, then starts printing 0 again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9946:121,variab,variables,121,https://hail.is,https://github.com/hail-is/hail/pull/9946,1,['variab'],['variables']
Modifiability,I have found a solution. It is necessary to set the environmental variable SPARK_LOCAL_DIRS to the desired temp location. e.g. ```; export SPARK_LOCAL_DIRS=/local; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/902#issuecomment-251923422:66,variab,variable,66,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251923422,1,['variab'],['variable']
Modifiability,"I have the FET in the expr language done. Also in this branch is the linear regression code refactoring. I removed the docs for the group tests, but left the code for creating groups and the FET and linear regression group tests in this branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/423#issuecomment-226914280:92,refactor,refactoring,92,https://hail.is,https://github.com/hail-is/hail/pull/423#issuecomment-226914280,1,['refactor'],['refactoring']
Modifiability,I haven't tested the refactored code yet -- would like to see if this was the refactoring you had in mind with Enums.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13224#issuecomment-1664685352:21,refactor,refactored,21,https://hail.is,https://github.com/hail-is/hail/pull/13224#issuecomment-1664685352,2,['refactor'],"['refactored', 'refactoring']"
Modifiability,"I haven't tested this yet because I wanted to see if you liked the idea. Not as flexible as a python loop, but possibly the easiest way to get rid of *a lot* of yaml.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11771:80,flexible,flexible,80,https://hail.is,https://github.com/hail-is/hail/pull/11771,1,['flexible'],['flexible']
Modifiability,"I knew I was missing tests of greater than two layers. Sure enough there were more bugs lurking. I think >3 layers won't find any new bugs given that there are basically three kinds of b-trees:. - 1 layer: all leaf nodes, <=1024 elements; - 2 layers: one internal/key layer, one leaf layer [1025, 1024^2] elements; - n layers: n-1 internal/key layers, one leaf layer [1024^(n-1)+1, 1024^n] elements. The last case is the first case where we have to do two levels of internal layers. This traversal is defined inductively, so I suspect succeeding on 3 layers tests all the functionality of >3 layers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3807:47,layers,layers,47,https://hail.is,https://github.com/hail-is/hail/pull/3807,8,['layers'],['layers']
Modifiability,I left in google_storage.py in for now because Benchmark uses it and I didn't want to rewrite that right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10331#issuecomment-820662160:86,rewrite,rewrite,86,https://hail.is,https://github.com/hail-is/hail/pull/10331#issuecomment-820662160,1,['rewrite'],['rewrite']
Modifiability,"I left the changes to Query and Batch in separate commits for ease of review. I put these in the same PR because we don't really have standalone testing for JVM Jobs outside of Query-on-Batch so the FASTA use-case serves as a test here that cloudfuse is working properly for JVM Jobs. Would be great if Jackie you could review the batch commit and Tim could review the query commit. ## Hail Query; - Added support for the `FROM_FASTA_FILE` rpc and the service backend now passes sequence file information from RGs in every rpc; - Refactored the liftover handling in service_backend to not redundantly store liftover maps and just take them from the ReferenceGenome objects like I did for sequence files. This means that add/remove liftover/sequence functions on the Backend are just intended to sync up the backend with python, which is a no-op for the service backend.; - Don't localize the index file on fromFASTAFile/addSequence before creating the index object. `FastaSequenceIndex` just loads the whole file on construction so might as well stream it in from whatever storage it's in.; - FASTA caching is left alone because those files will be mounted and unmounted from the jvm container over the life of the job. JVM doesn't have to worry about disk usage because that's handled by Batch XFS quotas, so long as the service backend requests enough storage to fit the FASTA file. Batch will make sure that a given bucket (and therefore a given FASTA file) is mounted once per-user on a batch worker. ## Hail Batch; - Added support for read-only cloudfuse mounts for JVM jobs; - These mounts are shared between jobs on the same machine from the same user; - I did not change DockerJobs, but they could be very easily adapted to use this new mount-sharing code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736:530,Refactor,Refactored,530,https://hail.is,https://github.com/hail-is/hail/pull/12736,2,"['Refactor', 'adapt']","['Refactored', 'adapted']"
Modifiability,"I like this plan overall. Some comments to consider:. - I don't think batch should persist job intermediates which I think more as a pipe (|) or a temporary file in a traditional script. This means the downstream batch clients (CI, Pipeline) should be copying files they want to persist to something like gs. One thing we should be sure here is that the solution doesn't involve excessive copies, e.g. we don't want to generate batch steps that just copy from a temporary persisted location in gs to a permanent location in gs.; - We need a way to refer to individual inputs/outputs. A bioinformatics command might output a massive data file and a report file, and we want to run a command to process or format the report file, but we don't want to copy the (unused) massive data file unnecessarily. Copying everything is a fine start.; - This might be covered by ""parse and exec series of commands"", but we want to be able to specify a series of stacked containers to execute, e.g. in the case of Pipeline, a user command to execute followed by a pipeline-controlled container with Pipeline gs credentials to copy the output to Pipeline (or user) controlled bucket. Same for CI.; - In the long run, we're going to want to be able to control the size of the local disk for user jobs (e.g. a bioinformatics command that needs 1TB of scratch space) separate from the host node's local file system. This will go into the job configuration. We probably don't need this for CI.; - We should separate the database management from the job execution. Probably easiest to make this a monolithic service, but we could separate them. One will interact with batch users to update the database and watch the database, and the other to reconcile the database and k8s by running jobs and updating results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739:1422,config,configuration,1422,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739,1,['config'],['configuration']
Modifiability,"I looked over the code and it looks fine, but I'm having trouble understanding the bigger picture of what you're trying to accomplish. I see that you have a new step that creates a test database in the default namespace in the test scope. Then you create the database config secret from this new database. And then deploy_ci depends on it, which makes sense because it needs the secret to be able to create new databases. And this is all only in the test scope. It looks like you cleaned up the build database in the case of dev deploy, which is fine too. > we also create a ""test_instance"" database that will be used as the database instance inside the tests. I don't understand what you wrote here because test_instance database doesn't seem to be used at all. Aren't we still creating the same batch and ci databases? I don't see what the test_instance database is buying you except to be able to make the database config secret that doesn't have the root username and password. I also don't quite understand what's going on in the build_cant_create_database build step. Shouldn't those secrets already exist? Won't this fail?. I'm sorry if I'm missing something obvious.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906:268,config,config,268,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906,2,['config'],['config']
Modifiability,"I made `define_function` go through the CSE path, and it broke because CSE was assuming an IR with no free variables. I fixed CSE to take a list of free variables.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7009#issuecomment-531331340:107,variab,variables,107,https://hail.is,https://github.com/hail-is/hail/pull/7009#issuecomment-531331340,2,['variab'],['variables']
Modifiability,"I made a branch which I think fixes this (and makes the 'root' variable actually used), but will PR once I have testing going.; https://github.com/jbloom22/hail/tree/vep_csq_global",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4050#issuecomment-409401666:63,variab,variable,63,https://hail.is,https://github.com/hail-is/hail/issues/4050#issuecomment-409401666,1,['variab'],['variable']
Modifiability,I made some changes. I had to rewrite the audits to not fill up the temp disk space and account for a bug in billing that was fixed for job private instances #10069. I'll test this afternoon after I figure out how to revert my first attempt.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11996#issuecomment-1201421379:30,rewrite,rewrite,30,https://hail.is,https://github.com/hail-is/hail/pull/11996#issuecomment-1201421379,1,['rewrite'],['rewrite']
Modifiability,"I made the name change `gce-deploy-config` to `worker-deploy-config`. Feel free to change the name. Also, I'm pretty sure this name change won't break the deploy because it's created as part of the deploy process. But it has been 198 days since the secret was updated... FYI: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11002:35,config,config,35,https://hail.is,https://github.com/hail-is/hail/pull/11002,2,['config'],['config']
Modifiability,"I manually added a `hail_test_gcs_bucket` field to the k8s global config and use that value wherever we have our current test bucket hard coded. I also added the necessary terraform to make that in a new cluster, though I have not done a new terraform run in my project. Once this and a couple more refactoring PRs go in I'll be able to run ci tests in a separate cluster and validate that the terraform is working correctly. cc: @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10807:66,config,config,66,https://hail.is,https://github.com/hail-is/hail/pull/10807,2,"['config', 'refactor']","['config', 'refactoring']"
Modifiability,I manually added a field to the global-config for the requester pays bucket used in batch tests. Adding it here to build.py's view of global fields so that CI can template #10866 in the future and actually test it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10870:39,config,config,39,https://hail.is,https://github.com/hail-is/hail/pull/10870,1,['config'],['config']
Modifiability,"I manually changed the Cloud SQL automated backups storage from the `us` multi-region to `us-central1`. There's no reason to store it in a multi-region and it's more expensive. What I didn't realize is that this configuration is actually owned by the terraform that we have managing the lifecycle of the database and other infra, so we need to update the terraform to reflect the desired (and current) state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14568:212,config,configuration,212,https://hail.is,https://github.com/hail-is/hail/pull/14568,1,['config'],['configuration']
Modifiability,"I meant type `TEXT` compared to `VARCHAR`. The reason I did not pick a BIGINT representation is there are 34 regions in GCP right now. If there's ever more than 64, then we can't support that with a BIGINT representation. A varchar seemed more flexible as we can change the type later on and we're not restricted to 64 regions. However, restricting to 64 regions as just a fundamental batch limitation is always fine as well. I'm ambivalent on which way we choose as long as the field can be used in an index. Apparently, `TEXT` can be indexed, but you have to specify the first N characters that are indexable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1274923740:244,flexible,flexible,244,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1274923740,1,['flexible'],['flexible']
Modifiability,"I merged these already approved, likely passing, branches together for CI efficiency. - [daniel-goldstein/drop-user-if-exists](/daniel-goldstein/hail/tree/drop-user-if-exists); - [daniel-goldstein/mysql-flexible](/daniel-goldstein/hail/tree/mysql-flexible); - [daniel-goldstein/blame-ignore-isort-pr](/daniel-goldstein/hail/tree/blame-ignore-isort-pr); - [jigold/timings](/jigold/hail/tree/timings); - [CDiaz96/find_spark_home_type](/CDiaz96/hail/tree/find_spark_home_type); - [daniel-goldstein/close-db-pool-last-batch-driver](/daniel-goldstein/hail/tree/close-db-pool-last-batch-driver); - [chrisvittal/lowering/fix-export-table-separate-header](/chrisvittal/hail/tree/lowering/fix-export-table-separate-header); - [chrisvittal/vds/n_partitions](/chrisvittal/hail/tree/vds/n_partitions). FYI @chrisvittal , @jigold, @daniel-goldstein, @CDiaz96",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11482:203,flexible,flexible,203,https://hail.is,https://github.com/hail-is/hail/pull/11482,2,['flexible'],['flexible']
Modifiability,"I misunderstood the issue originally. The exit status was set *in the sub-shell*, so; it did not affect the parent shell's environment. Instead, I run the command in; a sub-shell and update the variable in the parent shell. I also had to fix the issues that arose while the check wasn't honored.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9377:194,variab,variable,194,https://hail.is,https://github.com/hail-is/hail/pull/9377,1,['variab'],['variable']
Modifiability,"I must have broken this during a recent refactor of the decorators. When making requests to an internal namespace, the default namespace's auth token is set in the `X-Hail-Internal-Authorization` header and the dev namespace's token is in the `Authorization` header. The dev namespace needs to know *not* to pick up the `X-Hail-Internal-Authorization` header.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13515:40,refactor,refactor,40,https://hail.is,https://github.com/hail-is/hail/pull/13515,1,['refactor'],['refactor']
Modifiability,"I need to get GKE costs down further. The driver is now 3 CPU anyway. @daniel-goldstein, I am not sure how to modify the Azure terraform. It appears this; is controlled by a variable which is already set to a 4 core machine?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11985:174,variab,variable,174,https://hail.is,https://github.com/hail-is/hail/pull/11985,1,['variab'],['variable']
Modifiability,"I noticed something else, really long running jobs don't look great. That second blue line is a 6 minute job and it extends past the end of the page. Can we maybe put all of these divs into something that scrolls? Maybe a `div` with an `overflow: scroll`? I'm not sure exactly what should happen or how to do it, but I think we need a solution for this situation.; ![Screen Shot 2021-02-26 at 1 57 37 AM](https://user-images.githubusercontent.com/106194/109244152-17be8280-77d6-11eb-8f00-c124f9c2a5c7.png). I created this example with:; ```; import hailtop.batch as hb ; b = hb.Batch(backend=hb.ServiceBackend('test')) ; for i in range(1): ; b.new_job().command('sleep 360') ; b.run() ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10060#issuecomment-786356423:116,extend,extends,116,https://hail.is,https://github.com/hail-is/hail/pull/10060#issuecomment-786356423,1,['extend'],['extends']
Modifiability,"I observed a cluster with it set to the default idle time of 30 seconds in Azure and the workers were continuously thrashing leading up to 49 instances being created over the course of a PR. With an idle time of 120 seconds, there was no thrashing and 28 instances were created over the course of the PR (16 standard + job private etc.). The cluster nicely scaled down at the end of the PR. It looked like a couple of times the `standard-np` pool scaled up and then scaled down so I assume the `standard` pool wasn't at full capacity while that was happening. It might be worth configuring the `standard-np` pool to be 4 or 5 standing instances with 16 cores and see what happens -- that might help as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13314:578,config,configuring,578,https://hail.is,https://github.com/hail-is/hail/pull/13314,1,['config'],['configuring']
Modifiability,"I picked this refactor off of my terra branch and took it the last ten yards such that now you can run `make batch-db` and get a full local instance of the batch database that you can access through `mysql -h 127.0.0.1 -u root -ppw` or. ```ipython; (hail) dgoldste@wmce3-cb7 hail % HAIL_SQL_DATABASE=local-batch ipython; Python 3.9.17 (main, Jul 5 2023, 16:17:03); Type 'copyright', 'credits' or 'license' for more information; IPython 8.15.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: from gear import Database; ...: db = Database(); ...: await db.async_init(); ...: await db.select_and_fetchone('SELECT * FROM globals'); ...:; ...:; Out[1]:; {'instance_id': 'XXXXXXXX',; 'internal_token': 'XXXXXXX',; 'n_tokens': 200,; 'frozen': 0}; ```. If you add a migration, just run `make batch-db` again.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13670:14,refactor,refactor,14,https://hail.is,https://github.com/hail-is/hail/pull/13670,2,"['enhance', 'refactor']","['enhanced', 'refactor']"
Modifiability,"I played with a few options. I liked this one the best. Downside to `Value[T] extends Code[T]`:; - A bunch of code (using Array) assumes Code[T] is monomorphic. Either way I fixed those here (by switching to polymorphic IndexedSeq[T]); - Can't use the analogous setup for PValue since the hierarchy is more complicated. This is why I picked this solution. Downside to this solution: ; - Scala won't apply stacked implicits, so need to add additional implicits for e.g. Value[Int] to CodeInt. I do that here. In the end, `Value[T]` is a thing that can produce multiple `Code[T]`, which can then only be emitted once. I used `Value.get: Code[T]` over `load()` and renamed a few field accessors get => getField. If we like how this goes I can rip out `load()`. I fixed some know multiple emission of Code[T] in ETypes buildEncoder. I will slowly convert over the necessary stuff to `Value[T]` in later PRs. `Code.markEmitted` (not called) can be used to find Code[T] that are emitted multiple times.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8229:78,extend,extends,78,https://hail.is,https://github.com/hail-is/hail/pull/8229,2,"['extend', 'polymorphi']","['extends', 'polymorphic']"
Modifiability,I propose saving the configuration question to a different PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9769#issuecomment-738084194:21,config,configuration,21,https://hail.is,https://github.com/hail-is/hail/pull/9769#issuecomment-738084194,1,['config'],['configuration']
Modifiability,"I rage programmed here a bit. I was pretty frustrated with this class when; trying to debug some other issues in the Shuffler IR. I have two aims with this; PR:; 1. Use a clear, consistent naming scheme throughout the file; 2. Use concise definitions. In particular, I unified the terminology to use these words:; - name: the function's name; - valueParameterTypes: the type of each value-level parameter; - typeParameters: the type-level parameters, these are always type variables, I; considered calling them typeVariables, but I like the symmetry with valueParameters; - returnType and returnPType; - typeArguments and valueArguments: these refer to the concrete values to which the parameters are bound. I also simplified some anonymous class definitions by providing constructor arguments; instead of methods that are always overridden by `val`s.; Oh, and I changed `IRFunction` to `JVMFunction` because there are already ""IR"" functions; and an ""irRegistry"" and it was super confusing to not have the IRFunctions inside the; ""irRegistry"". I did not use `CodeFunction` because these are actually implemented by; a number of different JVM Bytecode building tools: `Code`, `PCode`, `EmitCode`, and; `IEmitCode`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8718:473,variab,variables,473,https://hail.is,https://github.com/hail-is/hail/pull/8718,1,['variab'],['variables']
Modifiability,"I realize this looks like a lot of code changes, but it's mostly copying and pasting two SQL procedures and changing one line in each. This adds 4 bits of metadata to requests that then can be queried as extra metadata:; - batch_id; - job_id; - batch_operation; - job_queue_time. Should be self-explanatory except job_queue time is the time in which the job is first set to ready to when it was scheduled on the worker (exact moment is when the job config is made to send to the worker). Example logging query. Note that the search on ""batch_id"" is not optimized so you definitely want to add some kind of time limit that's short on the window to search. I can add my Python script that scrapes these logs and makes a Plotly figure in a separate PR once this goes in. ```; (; resource.labels.container_name=""batch""; resource.labels.namespace_name=""{namespace}""; ) OR (; resource.labels.container_name=""batch-driver""; resource.labels.namespace_name=""{namespace}""; ) OR (; resource.type=""gce_instance""; logName:""worker.log""; labels.""compute.googleapis.com/resource_name"":""{namespace}""; ); jsonPayload.batch_id=""{batch_id}""; timestamp >= ""{start_timestamp}"" {end_timestamp}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13219:449,config,config,449,https://hail.is,https://github.com/hail-is/hail/pull/13219,1,['config'],['config']
Modifiability,I refactored the client code. Should be better now!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6245#issuecomment-498414073:2,refactor,refactored,2,https://hail.is,https://github.com/hail-is/hail/pull/6245#issuecomment-498414073,1,['refactor'],['refactored']
Modifiability,"I renamed RichProgressBar and SimpleRichProgressBar to ...CopyToolProgressBar because that is more accurate. I enhanced both to now include a count and a rate with the right units based on the description. It is a bit flaky because I need the descriptions to be exactly ""files"" or exactly ""bytes"" to pick the right units, but this seems fine for the specific case of th CopyToolProgressBar. There is probably a better way to build these UIs. I am sure we will start to figure that out as we use rich more. Before:; <img width=""830"" alt=""Screenshot 2023-10-16 at 18 28 14"" src=""https://github.com/hail-is/hail/assets/106194/95f8828e-beb3-46d2-9403-18ff7aa60256"">. After:; <img width=""830"" alt=""Screenshot 2023-10-16 at 18 27 53"" src=""https://github.com/hail-is/hail/assets/106194/01186b7c-d59f-4a0e-a1f6-9279fb50ae7e"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13832:111,enhance,enhanced,111,https://hail.is,https://github.com/hail-is/hail/pull/13832,1,['enhance'],['enhanced']
Modifiability,I see now what you are saying about the Structs -- the issue is the attributes on each Field object. I am tempted to go back to my original implementation where the types inherited from `HistoryMixin` and the `__init__` method was recorded when the object was initialized. @cseed didn't think it was necessary to have a Type have a history and suggested using `repr` to print how the object was created.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2060#issuecomment-322284636:171,inherit,inherited,171,https://hail.is,https://github.com/hail-is/hail/pull/2060#issuecomment-322284636,1,['inherit'],['inherited']
Modifiability,I still don't see what's wrong. TSet inherits from TIterable which does override it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2006#issuecomment-317562287:37,inherit,inherits,37,https://hail.is,https://github.com/hail-is/hail/issues/2006#issuecomment-317562287,1,['inherit'],['inherits']
Modifiability,"I suppose this is where I started getting entangled with the domain issue. If the Australians run a workshop, what should happen if their users run `hailctl batch init`? Should they have to supply some additional argument so that _if_ they're not authenticated they get sent somewhere other than `hail.is`? Maybe that's ok, seems kind of awkward though. While it doesn't work this way today, I imagine that we should ultimately configure `hailctl auth login` to accept a domain. I feel like that would make the AUS scenario slightly less awkward, and the tool more consistent, though I admit it is conceding some of our own convenience.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1663110567:428,config,configure,428,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1663110567,1,['config'],['configure']
Modifiability,I tested the config comes through correctly in my namespace.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10839:13,config,config,13,https://hail.is,https://github.com/hail-is/hail/pull/10839,1,['config'],['config']
Modifiability,I think I addressed most of the comments. I haven't tested the new code -- I don't want to do that until we're happy with it. I don't know that I like how this is turning out. I think we're conflating what `hailctl config init` should be which is intitializing an environment configuration file versus a quick start to using Hail Batch and QoB. My intention for this feature was to idiot proof the latter especially for the ATGU workshop and make it as few commands as possible. I worry that needing to run `hailctl auth login` before this is not a just run this single command and then you can get going with little effort. I don't think `hailctl config init` is the right place for what I have written. Maybe it should be `hailctl batch quick-start`???,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1662716382:215,config,config,215,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1662716382,3,['config'],"['config', 'configuration']"
Modifiability,"I think I'm on board with the RVDSpec stuff you've proposed. I still want to rewrite the names of the MatrixTableSpec and TableSpec, since these should have been abstract from the beginning.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4734#issuecomment-436689558:77,rewrite,rewrite,77,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-436689558,1,['rewrite'],['rewrite']
Modifiability,I think it's safe. I was more worried about what the namespace should be and whether for developers it should have the service_namespace. I think the deploy config only tells you how to get the correct URL. There's nothing special about it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9437#issuecomment-691241075:157,config,config,157,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691241075,1,['config'],['config']
Modifiability,"I think prometheus got forcibly shutdown and its volume got corrupted, which caused it to fail on startup. I had to wipe the volume. Starting with a new volume introduced permissions issues, I think because that volume claim might still have been there from the original time we were running prometheus.. Either way I was able to get this up and running in default. I had wanted to extend the retention time so we can see a full month of metrics instead of just two weeks, so I'm being extra nice with the volume and moving prometheus off of preemptibles in default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10271:382,extend,extend,382,https://hail.is,https://github.com/hail-is/hail/pull/10271,1,['extend'],['extend']
Modifiability,"I think the clear default answer is referential transparency. Whether you bind something in a python variable, or you inline that definition, should be semantically equivalent. Unless we come up with a compelling reason to break that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7418#issuecomment-548485790:101,variab,variable,101,https://hail.is,https://github.com/hail-is/hail/issues/7418#issuecomment-548485790,2,['variab'],['variable']
Modifiability,"I think the only two things I'm stuck on are:; (a) Do we want users to pass a VEPConfig instead of a `config` dictionary (and add documentation)?; (b) What is the best way to expose the VEP command interface so a user can customize it to their setup? I wanted to do something like this, but I don't see how to do this with the bash script being called with an argument `/bin/bash -c ""...."" csq` or `/bin/bash -c ""..."" vep`. ```python3; vep_85_grch37_command = '''; #!/bin/bash. if [ $VEP_CONSEQUENCE -ne 0 ]; then; vcf_or_json=""--vcf""; else; vcf_or_json=""--json""; fi. export VEP_COMMAND=/vep/vep \; ${VEP_INPUT_FILE:+--input_file $VEP_INPUT_FILE} \; --format vcf \; ${vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir=${VEP_DATA_DIR} \; --plugin LoF,human_ancestor_fa:${VEP_DATA_DIR}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:${VEP_DATA_DIR}/loftee_data/phylocsf_gerp.sql,gerp_file:${VEP_DATA_DIR}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT. exec vep.py ""$@""; '''. supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfig(; 'hail-qob-vep-grch37-us-central1',; ['us-central1'],; HAIL_GENETICS_QOB_VEP_GRCH37_IMAGE,; '/vep_data/',; {},; VEPConfig.default_vep_json_typ,; [""/bin/bash"", ""-c"", vep_85_grch37_command, ""vep""],; [""/bin/bash"", ""-c"", vep_85_grch37_command, ""csq_header""],; True,; 'gcp',; ),; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1498124947:102,config,config,102,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1498124947,2,"['config', 'plugin']","['config', 'plugin']"
Modifiability,"I think this could interact badly with Spark's partitioning logic. That would appear in the text file imports. We should probably rewrite the text file stuff the same way we rewrote import_bgen to use our own, sensible, partitioning logic. I guess the worst thing that happens is a small text file is broken into one partition per-line.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5698#issuecomment-476796791:130,rewrite,rewrite,130,https://hail.is,https://github.com/hail-is/hail/pull/5698#issuecomment-476796791,1,['rewrite'],['rewrite']
Modifiability,"I think this is a bad idea in the current model. However, we should explore IR visualization / execution enhancements",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1616#issuecomment-422389477:105,enhance,enhancements,105,https://hail.is,https://github.com/hail-is/hail/issues/1616#issuecomment-422389477,1,['enhance'],['enhancements']
Modifiability,"I think this is a race condition with another process trying to pull the same image after the current process has pulled the image. That would mostly be solved by a per user Docker cache, but I think this solution is still needed as you could have a race condition on the cache timeout boundaries. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8193:733,config,config,733,https://hail.is,https://github.com/hail-is/hail/pull/8193,1,['config'],['config']
Modifiability,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:295,config,config,295,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626,3,['config'],['config']
Modifiability,"I think this is ready for another look but I put a WIP label on it because I don't want to merge it today while there's still workshop things happening. After it goes in I'll run another scale test. The nginx config is mostly just lifted over from router, with a couple small changes. I wasn't able to just proxy to localhost because notebook couldn't figure out which subdomain the request was going to and everything would 404. Adding a Host header fixed the 404, but messed with the requests to notebook pods so instead I added `workshop.local` and `notebook.local` to `/etc/hosts` on the pod, which I kind of like, which did the trick. I needed two ssl configs, one for nginx and one for aiohttp to use internally, and ended up changing the `ssl-config-notebook` secret to `nginx` and creating a second `ssl-config-notebook-python`, but let me know if this seems off.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10204#issuecomment-806087347:209,config,config,209,https://hail.is,https://github.com/hail-is/hail/pull/10204#issuecomment-806087347,4,['config'],"['config', 'config-notebook', 'config-notebook-python', 'configs']"
Modifiability,"I think this is what was wrong with the `git_make_bash_image` taking a minute each time. Since every image without a `publishAs` uses `ci-intermediate`, the `ci-intermediate:cache-PR-X` tag is left pointing to whichever anonymous image built last in the PR run. This is certainly never `git_make_bash_image`, so every time it gets rebuilt, the cache-from that it is using points to an an image whose layers do not include a layer that is `RUN apt-get update && apt-get install -y git make bash`. If this PR runs twice, hopefully we'll see the first step go super quick.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12285:400,layers,layers,400,https://hail.is,https://github.com/hail-is/hail/pull/12285,1,['layers'],['layers']
Modifiability,"I think this may help users discover `group_by` and also help users; who are comfortable with the idea of a counter but not with `group_by`. I added a new dataset for doctests and I realized a couple things:; - doctest_write_data.py is not deterministic; - if I add/change one dataset, my commit explodes with changes to all the datasets (see above); - doctest_write_data.py has to be run by *me*, it's not run by CI. I also noticed that when you specify no `row_key` to `import_matrix_table` you get a row key called `row_id`, which is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6856:856,config,configurable,856,https://hail.is,https://github.com/hail-is/hail/pull/6856,1,['config'],['configurable']
Modifiability,"I think we should continue with another review and then a load test. I'm still a bit hesitant about the query change, but we can keep an eye on it. I'm still get errors with the typing:. ```; (venv) jigold@wm349-8c4 hail % make -C hail/python check; python3 -m flake8 --config ../../setup.cfg hail; python3 -m flake8 --config ../../setup.cfg hailtop; python3 -m pylint --rcfile ../../pylintrc hailtop --score=n; python3 -m mypy --config-file ../../setup.cfg hailtop; hailtop/batch/backend.py:481: error: Incompatible types in assignment (expression has type ""Union[str, List[str], None]"", variable has type ""Optional[List[str]]""); Found 1 error in 1 file (checked 146 source files); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1271807464:270,config,config,270,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1271807464,4,"['config', 'variab']","['config', 'config-file', 'variable']"
Modifiability,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:594,config,config,594,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887,8,"['config', 'variab']","['config', 'variables']"
Modifiability,"I think we should just always mount the deploy config. That might trigger some latent bugs where we don't do quite the right thing, but we can fix those.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-767891514:47,config,config,47,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-767891514,1,['config'],['config']
Modifiability,"I think we should totally remove these tutorials, and rewrite them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6745#issuecomment-515479734:54,rewrite,rewrite,54,https://hail.is,https://github.com/hail-is/hail/issues/6745#issuecomment-515479734,1,['rewrite'],['rewrite']
Modifiability,"I think we're okay in terms of not having errors currently. This was a backwards compatibility code path. I will double check again, but I think we first create the config on the driver `GCPSlimInstanceConfig.create()`. This config is sent to the worker which deserializes it, but it's on the last part of that if/else and runs `resources = [gcp_resource_from_dict(data) for data in resources]`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13539#issuecomment-1703188825:165,config,config,165,https://hail.is,https://github.com/hail-is/hail/pull/13539#issuecomment-1703188825,2,['config'],['config']
Modifiability,"I think:. ```; return hl.experimental.loop(; lambda f, array, idx: hl.if_else(idx > 10, array, f(array.extend(hl.str(idx)), idx + 1),; hl.tarray(hl.tstr), ['foo'], 1). ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10695#issuecomment-886215032:103,extend,extend,103,https://hail.is,https://github.com/hail-is/hail/pull/10695#issuecomment-886215032,1,['extend'],['extend']
Modifiability,I thought I'd proposed this env variable but then we ended up going with [this](https://github.com/hail-is/hail/pull/12246)?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12662#issuecomment-1420873523:32,variab,variable,32,https://hail.is,https://github.com/hail-is/hail/pull/12662#issuecomment-1420873523,1,['variab'],['variable']
Modifiability,"I thought about this over the weekend, and I think it would be better if we didn't have a separate data structure (JobTask). I'm going to try and refactor this without that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5418#issuecomment-467080179:146,refactor,refactor,146,https://hail.is,https://github.com/hail-is/hail/pull/5418#issuecomment-467080179,1,['refactor'],['refactor']
Modifiability,"I thought the config file was better for QoB where versioning and backwards compatibility is important and there could be substantially more configuration in the future. I don't think we mount that config file in the worker regardless for DockerJobs, but I could be wrong.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12662#issuecomment-1420895778:14,config,config,14,https://hail.is,https://github.com/hail-is/hail/pull/12662#issuecomment-1420895778,3,['config'],"['config', 'configuration']"
Modifiability,"I thought the purpose of the cache was to cache the latest version in production. Let's take service-base as an example. There's the deployment in production that we care about. But every PR is now going to change the cache each time to what it thinks service-base is. This means that the last 4 layers for service-base will change for every time we run a test PR and it changes hailtop, gear, or web-common. If you don't like this change, then feel free to close it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152568213:296,layers,layers,296,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152568213,1,['layers'],['layers']
Modifiability,"I tracked down why this is happening. The old code stored the (compressed) genotype data per variant in a buffer and decoded it in BgenRecord.getValue. The new code decodes eagerly, but only if the entries are needed. I assume the intention was to mark the entries as unneeded during the scan, but not when decoding the actual values, but this wasn't done. It isn't done easily, either, since we can't set a per-Hadoop import configuration, see: https://github.com/hail-is/hail/issues/3861. Options:. - go back to the old code that stashes the compressed value and evaluates lazily,; - have separate InputFormat/RecordReader for scan and decode,; - stop using Hadoop InputFormat to load BGEN and just code it in directly in Spark, where it is trivial to pass different parameters to scan and decode. I personally vote for the latter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3862:426,config,configuration,426,https://hail.is,https://github.com/hail-is/hail/issues/3862,1,['config'],['configuration']
Modifiability,I tried again:. hail-new-vep importvcf /user/satterst/DILI/DILI_controls.vcf.bgz repartition -n 1000 splitmulti vep --config /psych/genetics_data/working/cseed/vep.properties write -o /user/satterst/DILI/DILI_split_vep.vds. It's taking wayyyy too long. log here: /humgen/atgu1/fs03/satterst/hail.log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/302#issuecomment-210941562:118,config,config,118,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-210941562,1,['config'],['config']
Modifiability,"I tried it in both raw python and pyspark and I got a new error. Seem to be a problem with the profile having too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:925,config,configuration,925,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978,2,['config'],"['config', 'configuration']"
Modifiability,"I tried to run Hail with Spark 2.4.4 built for Scala 2.12, and it did not work. It does work with Spark 2.4.4 built for Scala 2.11. Here's the error I got with Scala 2.12:; > Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; > : java.lang.NoSuchMethodError: scala/Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps; (loaded from file:/home/hammer/codebox/spark-2.4.4-bin-without-hadoop-scala-2.12/jars/scala-library-2.12.8.jar by sun.misc.Launcher$AppClassLoader@ac1080fa) called from class is.hail.HailContext$ (loaded from file:/home/hammer/anaconda3/lib/python3.7/site-packages/hail/hail-all-spark.jar by sun.misc.Launcher$AppClassLoader@ac1080fa).; > 	at is.hail.HailContext$.majorMinor$1(HailContext.scala:71); > 	at is.hail.HailContext$.checkSparkCompatibility(HailContext.scala:73); > 	at is.hail.HailContext$.createSparkConf(HailContext.scala:84); > 	at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:134); > 	at is.hail.HailContext$.apply(HailContext.scala:270); > 	at is.hail.HailContext.apply(HailContext.scala); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); > 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); > 	at py4j.Gateway.invoke(Gateway.java:282); > 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); > 	at py4j.commands.CallCommand.execute(CallCommand.java:79); > 	at py4j.GatewayConnection.run(GatewayConnection.java:238); > 	at java.lang.Thread.run(Thread.java:819); >",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8009:934,config,configureAndCreateSparkContext,934,https://hail.is,https://github.com/hail-is/hail/issues/8009,1,['config'],['configureAndCreateSparkContext']
Modifiability,"I updated every call site to match the new parameters. This refactoring removes some code duplication, changes a stack trace in `hailctl auth user` to a nice print message, and adds a parameter (`client_session`) which I will use in my forthcoming TCP Proxy PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9715:60,refactor,refactoring,60,https://hail.is,https://github.com/hail-is/hail/pull/9715,1,['refactor'],['refactoring']
Modifiability,"I updated terraform but. 1. GCP Terraform state is still local on my laptop. 2. GCP Terraform appears to not configure global-config. As such, I cannot thread the name of the bucket through to the tests the way we do with TEST_STORAGE_URI. For now, I've hardcoded the name (which is what we were doing previously). When we eventually get to testing recreation of GCP in a new project we'll have to address the global config then.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12964:109,config,configure,109,https://hail.is,https://github.com/hail-is/hail/pull/12964,3,['config'],"['config', 'configure']"
Modifiability,"I updated the HTTP to HTTPS redirect in `/etc/apache2/sites-enabled/000-default.conf` to preserve the sub-domains:. ``` apache; RewriteEngine on; RewriteCond %{HTTPS} off; RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI} [END,QSA,R=permanent]; ```. and I modified the `VirtualHost` set up to use [name-based VirtualHost discrimination](https://httpd.apache.org/docs/2.4/vhosts/name-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsenc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:128,Rewrite,RewriteEngine,128,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,3,['Rewrite'],"['RewriteCond', 'RewriteEngine', 'RewriteRule']"
Modifiability,I verified that I can now run the batch tests on my laptop from the hail directory with:; ```; hailctl config set batch/billing_project hail # only needed once; make pytest PYTEST_ARGS='-k BatchTests'; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8559#issuecomment-614205652:103,config,config,103,https://hail.is,https://github.com/hail-is/hail/pull/8559#issuecomment-614205652,1,['config'],['config']
Modifiability,"I verified this works on 1.5.2, 1.6.3 and 2.0.2, but fails if we remove the relevant configuration settings (maxPartitionBytes, parquet.blocks.size).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1170#issuecomment-266622157:85,config,configuration,85,https://hail.is,https://github.com/hail-is/hail/pull/1170#issuecomment-266622157,1,['config'],['configuration']
Modifiability,"I want cancel_after_n_failures to be on a job group. The things a job group doesn't have which maybe it should is:; - callback; - attributes; - updates. I think updates should be on a batch and not part of a job group. An update can add jobs to multiple job groups. Otherwise, the batches table should only have static fields that apply to the entire batch. I think we can do callbacks and attributes on a job group. I added a PATCH endpoint to be able to update a job group's cancel_after_n_attributes as the hailtop.batch interface was going to automatically generate job groups without any configuration settings. As for the full text search, I think prefix searches are faster with full text search than with a regular index, but I could be wrong. We'd have to benchmark it. > If we made batches simpler, does that ease complexity and decrease code duplication? In particular, what if batches didn't contain jobs at all? Instead, a batch contains exactly one job group. That job group contains zero or more job groups. Job groups manage: resource aggregation, cancellation, etc. I believe my plan is basically already doing this. It might not be clear because I didn't put the migrations in. But basically all of the current batches tables are now indexed by batch_id, job_group_id where the current ""batch"" has job_group_id = 1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12697#issuecomment-1450603163:593,config,configuration,593,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1450603163,2,['config'],['configuration']
Modifiability,"I want to get rid of TableCount, too. I think we can do that with a rewrite rule that rewrites TableCount(tir) if tir.partitionCounts.isDefined.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5015#issuecomment-448843839:68,rewrite,rewrite,68,https://hail.is,https://github.com/hail-is/hail/pull/5015#issuecomment-448843839,2,['rewrite'],"['rewrite', 'rewrites']"
Modifiability,"I want to make ordering a val instead of a def so it doesn't have to be recreated, but that means I need a way to pass missingGreatest as a parameter in compare. Hence, ExtendedOrdering. There are variants of compare, etc. that default missingGreatest = true since that is the default. All the overrides are necessary because lt, etc. can't always be determined from compare (e.g. nan compares false with everything for all comparison operations). This is used by a future PR for generic interval comparison.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2675:169,Extend,ExtendedOrdering,169,https://hail.is,https://github.com/hail-is/hail/pull/2675,1,['Extend'],['ExtendedOrdering']
Modifiability,"I want to preempt before you go much further and suggest that we not change the *implementation* of the decoders/encoders right now. Instead, we should just be copying code from EmitPackEncoder/EmitPackDecoder into the ETypes. I think there should be an internal method on EType that roughly looks like ; ```; def emitDecoder(; t: PType,; mb: MethodBuilder,; in: Code[InputBuffer],; srvb: StagedRegionValueBuilder): Code[Unit] = {; ```. And probably a top level thing on EType that mimics EmitPackDecoder.decode. I don't think I'm OK doing a rewrite of all the codegen in the same PR as a reorganization.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7060#issuecomment-533674187:542,rewrite,rewrite,542,https://hail.is,https://github.com/hail-is/hail/pull/7060#issuecomment-533674187,1,['rewrite'],['rewrite']
Modifiability,"I want to respond to some of your objections to unique_ptr. > If you buy into using std::unique_ptr, then everyone who writes or reads the code has to get; their head around the massively confusing and counter-intuitive concept of move semantics (a; form of assignment which modifies the source) and the somewhat bizarre terminology and syntax; used to express that in C++. I agree that move semantics takes getting used to, but I think it is much too integrated into modern C++ to ignore, going far beyond unique_ptr. Writing interfaces that take advantage of move semantics requires understanding rvalue-references in more detail, but for users of those move-enabled interfaces I think the guidelines are easy to teach: a variable will only be modified by moving if it is explicitly tagged with a `std::move`, so all you have to remember is ""after a `std::move(foo)`, the variable `foo` may only be assigned to or deleted."". > And then you get into a whole host of associated design decisions (I'm holding this as a unique_ptr,; but I want to pass it to a function - should I pass it as a raw pointer ? a raw reference ? a reference; to the unique_ptr ?). Keeping in mind the model that letting a function/class `foo` hold a `unique_ptr<Widget>` means explicitly ""`foo` owns this Widget, and is responsible for deleting it or passing ownership somewhere else"", these questions have pretty clear answers. * If a function `bar` takes a `Widget` but isn't concerned with its lifetime management, it should take its argument as a `Widget*` or `Widget&`, with the usual reasoning to choose between them. The caller owns the widget, and the lifetime of `bar` is nested inside that of its caller, so lifetime management isn't an issue.; * If `bar` takes a `Widget` and needs to take ownership, it should take its argument as a `unique_ptr<Widget>`. This serves as documentation that the function is taking over responsibility for deleting the Widget, in a way enforced by the compiler.; * A `unique_ptr<Wid",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3718#issuecomment-396669638:724,variab,variable,724,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396669638,4,['variab'],['variable']
Modifiability,"I wanted `WriteValue` to be able to essentially generate filenames exactly like we currently do in crdd.writePartitions, so I added a `HailTaskContext` concept to be able to mimic that behavior. It'll currently generate files named `prefix` if run on the master, and files named `prefix-stage-partition-…` if run in a distributed setting. We may eventually want to parameterize the behavior of `WriteValue` to have it do different things with the filename, but I think this will be enough to start moving over some of the writers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8180:365,parameteriz,parameterize,365,https://hail.is,https://github.com/hail-is/hail/pull/8180,1,['parameteriz'],['parameterize']
Modifiability,"I wanted to chime in on this briefly because I think it a good example use case and its design will influence many future methods, so it is important to get the design right. Thoughts:. - the underscore stuff is a non-starter in my opinion, and too clever by half. A lot of my feedback on your stuff is guided by the general heuristic that you should start by writing down the code you want, and then decide how to implement. You'd never want to write this _ stuff if you didn't have to. - I'm still not quite sure what tablify does (in part because the name is too clever by half and in part because it doesn't appear to always return tables). - But I think the idea of tablify is something we want, which is to convert (possibly indexed expressions) back into relational objects (Table, MatrixTable) because the latter support a wider set of operations and don't have the ""source mismatch problem"". Tim and I discussed this yesterday and we suggest the following interface:. ```; t = build_table(); .set_globals(x = 5, batch = batch); .set_rows(locus = locus, aaf = aaf); .build(); ```. and. ```; mt = build_table_matix(); .set_globals(dataset = dataset); .set_rows(locus = locus, aaf = aaf); .set_entries(GT = GT); .build(); ```. where the input expressions for each part must all come from the same source (or be compatible, e.g., constants) and the resulting (matrix) table inherits the keys from the original table. I think there is an unresolved question about how to handle potential name conflicts (e.g. a column key named locus).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2852#issuecomment-363841964:1379,inherit,inherits,1379,https://hail.is,https://github.com/hail-is/hail/pull/2852#issuecomment-363841964,2,['inherit'],['inherits']
Modifiability,I wanted to get feedback on the code I've written thus far before I start testing everything. I'm worried it might be too complicated / brittle to maintain. I also chose to blow away the entire existing environment rather than resetting the variables with new values. Not sure if that's what we want.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279:241,variab,variables,241,https://hail.is,https://github.com/hail-is/hail/pull/13279,1,['variab'],['variables']
Modifiability,"I was assigned:; ```; size(dict<T, U>): int32; size(set<T>): int32; isEmpty(dict<T, U>): bool; isEmpty(set<T>): bool; isEmpty(array<T>): bool; head(set<T>): T; head(array<T>): T; tail(set<T>): set<T>; tail(array<T>): array<T>; sum(set<tnum>): tnum; product(set<tnum>): tnum; map(set<T>,(T) => U): set<U>; exists(set<T>,(T) => bool): bool; forall(set<T>,(T) => bool): bool; filter(set<T>,(T) => bool): set<T>; length(array<T>): int32; sort(array<T>,bool): array<T>; sort(array<T>): array<str>; append(array<T>,T): array<T>; extend(array<T>,array<T>): array<T>; ```; I didn't do `head` or `tail`, because they weren't being used. I'm not sure how/where to test the new functions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3554:523,extend,extend,523,https://hail.is,https://github.com/hail-is/hail/pull/3554,1,['extend'],['extend']
Modifiability,"I was hoping that a greater restructure would lead to a more elegant solution, but after so many footguns it just felt worth making this change. The main point is shelling out in the Makefiles to ask Kubernetes directly for what the global config values are instead of hard-coding them. Specifically, the changes are:. - `KUBERNETES_SERVER_URL` was simply not used anymore in the Makefiles so I deleted it.; - `DOCKER_ROOT_IMAGE`, `INTERNAL_IP`, `IP` and `CLOUD` were only used in a couple Makefiles so I moved the `kubectl` invocation into where they're used so that Makefiles that don't depend on those variables won't incur the cost of querying them; - `DOCKER_PREFIX` and `DOMAIN` were used pretty widely, so I kept them in config.mk because most Makefiles will need to query those values anyway. The added startup time for `make deploy` feels pretty insignificant.; - With this change in place, there's no need to render config.mk anymore so I deleted the shell function for doing so",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11414:240,config,config,240,https://hail.is,https://github.com/hail-is/hail/pull/11414,4,"['config', 'variab']","['config', 'variables']"
Modifiability,"I was surprised to see this didn't fail by fixing some fails_local_backend tests. It turns out that we're writing out invalid files with the lowered matrix writer:; ```; def test_indexed_read(self):; mt = hl.utils.range_matrix_table(2000, 100, 10); f = new_temp_file(extension='mt'); mt.write(f); mt2 = hl.read_matrix_table(f, _intervals=[; hl.Interval(start=150, end=250, includes_start=True, includes_end=False),; hl.Interval(start=250, end=500, includes_start=True, includes_end=False),; ]); self.assertEqual(mt2.n_partitions(), 2); self.assertTrue(mt.filter_rows((mt.row_idx >= 150) & (mt.row_idx < 500))._same(mt2)). mt2 = hl.read_matrix_table(f, _intervals=[; hl.Interval(start=150, end=250, includes_start=True, includes_end=False),; hl.Interval(start=250, end=500, includes_start=True, includes_end=False),; ], _filter_intervals=True); self.assertEqual(mt2.n_partitions(), 3); self.assertTrue(mt.filter_rows((mt.row_idx >= 150) & (mt.row_idx < 500))._same(mt2)). E Java stack trace:; E is.hail.utils.HailException: `intervals` specified on an unindexed matrix table.; E This matrix table was written using an older version of hail; E rewrite the matrix in order to create an index to proceed; E 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:11); E 	at is.hail.utils.package$.fatal(package.scala:77); E 	at is.hail.expr.ir.MatrixNativeReader$.apply(MatrixIR.scala:166); E 	at is.hail.expr.ir.MatrixNativeReader$.fromJValue(MatrixIR.scala:184); ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10111#issuecomment-790698282:1142,rewrite,rewrite,1142,https://hail.is,https://github.com/hail-is/hail/pull/10111#issuecomment-790698282,1,['rewrite'],['rewrite']
Modifiability,I wasn't sure if we should print at least one Docker config or not. I'm worried about the test for out_of_memory where that prints out a bunch of the letter 'a' in the script.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9184:53,config,config,53,https://hail.is,https://github.com/hail-is/hail/pull/9184,1,['config'],['config']
Modifiability,"I wonder if this occurs with the Hadoop-BAM codec too? I think the issue before that led to it being reverted (https://github.com/hail-is/hail/issues/566) was because the split size was too small, which would be fairly easy to fix by putting a limit in place (so that -b/-n are hints, in other words). I'd be happy to work on such a change. From a maintainability point of view it would be best if we could have a single codec that is shared (GATK is using the Hadoop-BAM codec, for example) so we don't duplicate work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/869#issuecomment-251128189:348,maintainab,maintainability,348,https://hail.is,https://github.com/hail-is/hail/issues/869#issuecomment-251128189,1,['maintainab'],['maintainability']
Modifiability,"I would prefer that there was some way to abstract over this behavior, but I don't see any ""mutually exclusive"" tag in Args4j. A common `FilterOptions` class might help, but we'd still have to call a ""check options"" method in each `Command` and linear inheritance limits the scalability of this approach to other common options.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/622#issuecomment-240831961:252,inherit,inheritance,252,https://hail.is,https://github.com/hail-is/hail/pull/622#issuecomment-240831961,1,['inherit'],['inheritance']
Modifiability,"I'll keep an eye out for that, but I nix'ed the circularity by taking the vm_config off of the instance & instance config. Every instance still has an instance_config which I made non-optional. It basically has exactly one function now: tell me how much this instance costs per hour.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956517576:115,config,config,115,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956517576,1,['config'],['config']
Modifiability,"I'll make a quick Discuss post when this goes in. Here is a quick example of renaming as before with a mapping file:. ```; m2 = {r._0: r._1 for r in hc.import_keytable(test_resources + '/sample2_rename.tsv',; config=TextTableConfig(noheader=True)); .collect()}; self.assertEqual(sample2.join(sample2.rename_samples(m2)); .count()['nSamples'], 200); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1463#issuecomment-283826241:209,config,config,209,https://hail.is,https://github.com/hail-is/hail/pull/1463#issuecomment-283826241,1,['config'],['config']
Modifiability,"I'm attempting to build hail from a clone of this repository's master branch, as a local install on my laptop, under Debian GNU/Linux version 8. The gradle script successfully downloaded and installed the various Java dependencies, but gcc chokes on the C source code. I get:; $ ./gradlew shadowJar; :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /home/rmk/package_sources/hail/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h: In function ‘uint64_t vector_popcnt(uint64vector)’:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:14:48: error: called from here; uint64_t count = _mm_popcnt_u64(extract<0>(x));; ^; In file included from ibs.cpp:1:0:; /usr/lib/gcc/x86_64-linux-gnu/4.9/include/popcntintrin.h:42:1: error: inlining failed in call to always_inline ‘long long int _mm_popcnt_u64(long long unsigned int)’: target specific option mismatch; _mm_popcnt_u64 (unsigned long long __X); ^; ibs.cpp:16:41: error: called from here; count += _mm_popcnt_u64(extract<1>(x));; ^; make: *** [lib/linux-x86-64/libibs.so] Error 1; Makefile:52: recipe for target 'lib/linux-x86-64/libibs.so' failed; :nativeLib FAILED. Tim Poterba suggested defining `CXXFLAGS='-DHAIL_OVERRIDE_ARCH -DSIMDPP_ARCH_X86_SSE2'`, but to no avail. So, he suggested I open this issue. I'm running gcc version 4.9.2. Possibly relevant might be the processor I'm running,; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1520:375,Config,Configuring,375,https://hail.is,https://github.com/hail-is/hail/issues/1520,1,['Config'],['Configuring']
Modifiability,I'm beginning to think the configuration stuff is confusing because it ignores the distinction between clients and servers.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513#issuecomment-611765252:27,config,configuration,27,https://hail.is,https://github.com/hail-is/hail/pull/8513#issuecomment-611765252,1,['config'],['configuration']
Modifiability,"I'm currently expanding BlockMatrix binary ops to work naturally between BlockMatrix and 2d ndarray, with the useful ones for LMM being broadcasting ndarray over rows or columns of BlockMatrix. That'll round out the linear algebra, so that next week I'll begin refactoring the pipeline, with the interesting bits being the form of Python form of the small ""global model"" (initially using Scala side as black box) and then applying the local model to the pair or RowMatrices `X` and `PX` (read from BlockMatrices) to get per variant results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3114#issuecomment-373451854:261,refactor,refactoring,261,https://hail.is,https://github.com/hail-is/hail/pull/3114#issuecomment-373451854,1,['refactor'],['refactoring']
Modifiability,I'm definitely here for a refactor that structures these things using an exit stack or just plain try-finallies.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12692#issuecomment-1432253012:26,refactor,refactor,26,https://hail.is,https://github.com/hail-is/hail/pull/12692#issuecomment-1432253012,1,['refactor'],['refactor']
Modifiability,I'm getting circular import problems in later branches. I'm going to try some major refactoring and might put the rest of the changes in this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-951271454:84,refactor,refactoring,84,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-951271454,1,['refactor'],['refactoring']
Modifiability,"I'm getting this warning:. ```; /Users/jigold/hail/src/main/scala/is/hail/io/vcf/ExportVCF.scala:199: non-variable type argument String in type pattern scala.collection.immutable.Map[String,Any] (the underlying of Map[String,Any]) is unchecked since it is eliminated by erasure; case Some(x: Map[String, Any]) => getAttributes(path.tail, Some(x)); ```. If you have suggestions on how to improve the `getAttributes` function, I'd appreciate it!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2496:106,variab,variable,106,https://hail.is,https://github.com/hail-is/hail/pull/2496,1,['variab'],['variable']
Modifiability,I'm getting ~13 changes when I `terraform plan` now. Most are related to terraform wanting to replace the network because `auto_create_subnetworks` is false and it wants it to be true. It's also complaining about no `name` label on `hail_test_requester_pays_bucket` and it is not currently aware of `zulip_config` and wants to recreate it. I'm guessing the zulip config is also a file on your machine that needs to go through SOPS and be committed?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12991#issuecomment-1563417950:363,config,config,363,https://hail.is,https://github.com/hail-is/hail/pull/12991#issuecomment-1563417950,1,['config'],['config']
Modifiability,I'm going to need to do a bit of manual work with our global config and submit a prequel PR but most of the context is here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10866#issuecomment-918331563:61,config,config,61,https://hail.is,https://github.com/hail-is/hail/pull/10866#issuecomment-918331563,1,['config'],['config']
Modifiability,I'm having trouble with finding examples of codegen to improve for these types. It seems that a lot of our way of generating code here is to apply functions from our function registry and adapting them to use PCode seems to be a much larger project. I think I need to change how those work before I can do the rest of the PCode changes in such a way that benefits our code changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8348#issuecomment-603354428:188,adapt,adapting,188,https://hail.is,https://github.com/hail-is/hail/pull/8348#issuecomment-603354428,1,['adapt'],['adapting']
Modifiability,"I'm hitting default & my local build is latest hi/main (`15a45cfb9b0f8da01b2d0408993556f8391749e3`), still broke. I started hail this way:; ```; hailctl config set batch/billing_project hail; hailctl dev config set default_namespace default; HAIL_QUERY_BACKEND=service ipython; ```; ```ipython; In [1]: In [1]: import hail as hl ; ...: ...: ; ...: ...: temp = hl.utils.range_table(100) ; ...: ...: temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True) ; ...: ; Initializing Hail with default parameters...; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.61-15a45cfb9b0f; LOGGING: writing to /Users/dking/projects/hail/hail-20210202-1642-0.2.61-15a45cfb9b0f.log; Traceback (most recent call last):; File ""<ipython-input-1-92be8dd8c99f>"", line 4, in <module>; temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True); File ""</Users/dking/miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-1094>"", line 2, in write; File ""/Users/dking/projects/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/hail/python/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 116, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 395, in ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:153,config,config,153,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601,2,['config'],['config']
Modifiability,I'm just going to close this and make separate tickets that aren't assigned to anyone in particular. The cheat sheet has evolved to a place where all of these are just nice to haves.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7358#issuecomment-555048507:121,evolve,evolved,121,https://hail.is,https://github.com/hail-is/hail/issues/7358#issuecomment-555048507,1,['evolve'],['evolved']
Modifiability,I'm nearly through a modular refactoring as discussed that I'll revise on this and make PR back to the branch tomorrow.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1984#issuecomment-320542160:29,refactor,refactoring,29,https://hail.is,https://github.com/hail-is/hail/pull/1984#issuecomment-320542160,1,['refactor'],['refactoring']
Modifiability,"I'm not seeing the leak. `MemoryBuffer.clear` only zeroes the `pos` and `end` variables, and all the allocated memory is in the java heap. If anything, maybe you want to do `cb.assign(lazyBuffer, Code._null)`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12040#issuecomment-1191668397:78,variab,variables,78,https://hail.is,https://github.com/hail-is/hail/pull/12040#issuecomment-1191668397,2,['variab'],['variables']
Modifiability,I'm not sure how to set env variables for the Hail java process from Python. I'm sure it can be done,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583418657:28,variab,variables,28,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583418657,1,['variab'],['variables']
Modifiability,"I'm not sure if this is the right place to drop this, but is there a reason for building the new functionality on a pretty old VEP build? 95 dates from January 2019... It may not be related, but I had to do a chunk of overwriting of the LOFTEE plugin, because the default bundled with VEP was/is broken for GRCh38 on 105 - https://github.com/populationgenomics/images/pull/46. . The VEP 109 Docker image claims to 'include all VEP plugins', so I'm not sure if that variability would interfere with parsing VEP data into Hail data structures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12946#issuecomment-1542982780:244,plugin,plugin,244,https://hail.is,https://github.com/hail-is/hail/issues/12946#issuecomment-1542982780,3,"['plugin', 'variab']","['plugin', 'plugins', 'variability']"
Modifiability,I'm not sure this change is thorough enough. Is there a way for a bucket to get partially mounted but have config['mounted'] still be False?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12975:107,config,config,107,https://hail.is,https://github.com/hail-is/hail/pull/12975,1,['config'],['config']
Modifiability,"I'm not sure what the best/easiest thing to do is. I think we can accomplish the same thing by copying and pasting the delete tables step (runImage) into a dev branch while testing. . The other easiest thing I can think of is to add a `run_if_requested=True` option to each build step config and modify ci to skip over steps that aren't specifically requested in dev deploy. I don't think a new step is a good idea because what if I want an optional runImage step or an optional Deploy step, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7705#issuecomment-565009061:285,config,config,285,https://hail.is,https://github.com/hail-is/hail/pull/7705#issuecomment-565009061,1,['config'],['config']
Modifiability,I'm pretty sure this is why the session messages were gone when trying to configure the pools pages.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11094:74,config,configure,74,https://hail.is,https://github.com/hail-is/hail/pull/11094,1,['config'],['configure']
Modifiability,"I'm rather inclined to wait for @cseed's commentary on this before merging it. I'm not sure if this is the best solution, but it seems like a viable way to hold necessary configuration parameters.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613#issuecomment-290197728:171,config,configuration,171,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290197728,1,['config'],['configuration']
Modifiability,"I'm sorry, I was unclear when I suggested that we ""fix the ENTRYPOINT bug"". In my view, the bug is that we do not *unset* the entry point when we invoke Docker (locally and in the service backend). Locally this means including the argument `--entrypoint """"`. In the service we need to modify `worker.py` to set the `config`'s `Entrypoint` to, I think, the empty string. I do not want `entrypoint` in the job spec at all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9219#issuecomment-670707543:316,config,config,316,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670707543,1,['config'],['config']
Modifiability,"I'm still figuring out what the abstractions should be here. This refactoring has resulted in the `apply_to_figure` method of most geoms looking very similar, but still with little differences. Reluctant to commit to a very stringent class hierarchy yet and didn't want to further complicate this PR, but other than `hline` and `vline`, feels close to a world where each geom just overrides `plot_one_group` and the general looping over groups is handled by the parent class.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11317#issuecomment-1030102740:66,refactor,refactoring,66,https://hail.is,https://github.com/hail-is/hail/pull/11317#issuecomment-1030102740,1,['refactor'],['refactoring']
Modifiability,"I'm thinking something like `parseExprsAtTypes`:. ```scala; def parseExprsAtTypes[A](e: String, ec: EvalContext, hole: TypeHole[A]): Array[(BaseType, A, () => Option[Any])]; ```. ```scala; // function throws Exception if type is unacceptable; trait TypeHole[A] extends Iterable[Type => A] { }; sealed case class RepeatingTypeHole[A](f: Type => A) extends TypeHole[A] {; def iterator() = repeat(f).iterator; }; sealed case class ExactTypeHole(types: Type*) extends TypeHole[Unit] {; def iterator() =; types.iterator.map(x => y => if (x == y) () else fatal(s""$x should be of type $y""); }. def repeat[A](it: Iterable[A]) = Stream.continually(it.toStream).flatten; ```. Then the new LMMReg check for covariance types would be:; ```scala; val compiledExprs = parseExprsAtTypes(options.covSA, ec, RepeatingTypeHole(toDouble)); val covSA = vds.sampleIdsAndAnnotations.map { case (s, sa) =>; ec.setAll(s, sa); compiledExprs.map { case (typ, convert, query) =>; query().map(convert); }; }; ```. And AnnotationImpex would be something like:. ```scala; val fs = Parser.parseExprs(variantFields, ec, ExactTypeHole(TString, TInt, TString, TArray(TString))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1113:261,extend,extends,261,https://hail.is,https://github.com/hail-is/hail/issues/1113,3,['extend'],['extends']
Modifiability,"I'm thinking that I want to have this:. Basic setup for the domain, remote_tmpdir, billing_project, and only select one region; ```; hailctl init; ```. Which will prompt for:; 1. GCP project; 2. Hail domain; 3. region; 4. remote tmpdir location. And set:; - domain; - batch/billing_project; - batch/regions; - batch/remote_tmpdir; - batch/backend; - query/backend. And then print out a message with the default settings with instructions on how to change any of the settings and warnings about the configuration by using `hailctl config set ...`. I'll have the autocomplete PR merged (hopefully) by then so we can make that nice. The trial billing project is automatically used and we don't prompt for that. Then we'll have optional flags for components to configure; ```; hailctl init --container-registry --requester-pays-buckets --extra-query-settings --extra-batch-settings; ```; This would prompt for the same as above as well as advanced regions settings, query settings, and create the container registry. And lastly; ```; hailctl config check; ```; which prints warnings about misspecified regions and tmpdirs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1650549451:498,config,configuration,498,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1650549451,4,['config'],"['config', 'configuration', 'configure']"
Modifiability,"I'm using it locally (installed using `./gradlew installDist`) but working on our cluster, rather than with `spark-submit`. I have not loaded the spark module on the cluster. Is Hail installing its own spark libraries? Is there a way to configure the tmp dir for these?. Thanks",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/902#issuecomment-251732459:237,config,configure,237,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251732459,1,['config'],['configure']
Modifiability,"I've added real tests, that check that setup and finalization are correctly paired. If it's okay to wait on refactoring COption (which I don't think will be any harder to do later), then this should be ready for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8129#issuecomment-589715873:108,refactor,refactoring,108,https://hail.is,https://github.com/hail-is/hail/pull/8129#issuecomment-589715873,1,['refactor'],['refactoring']
Modifiability,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:815,adapt,adapt,815,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141,1,['adapt'],['adapt']
Modifiability,I've already applied this change to the cluster since k8s-config.yaml isn't actually auto-deployed yet.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5518:58,config,config,58,https://hail.is,https://github.com/hail-is/hail/pull/5518,1,['config'],['config']
Modifiability,"I've changed BlockMatrix.from(lm: BDM[Double]) so that each executor is transmitted only the blocks it needs (~num_blocks/num_executors) rather than all of them: ""[TorrentBroadcast](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-TorrentBroadcast.html) uses a BitTorrent-like protocol for block distribution (that only happens when tasks access broadcast variables on executors)."". In another branch, I've verified on GCP that distributing and then localizing a 10k x 10k matrix is twice as fast (about 15s vs 30s). Distributing and then writing a 25k by 25k matrix (5GB) with 10+2 standard 8-core workers takes about 30s with the new method but fails for every partition with the old method (months ago I believe I sometimes got the old method to work at this scale using high mem. It's needed for LMM). Note the matrix only has 49 partitions at the new default blockSize so I had more cores than needed for the experiment. I've also added a method to write a local matrix as a block matrix. I use ParRange to parallelize writing from master. Writing and then reading should be the safest way to distribute a big local matrix at the beginning of complex pipelines, and I think it avoids some of the memory overhead associated with broadcast.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2848:371,variab,variables,371,https://hail.is,https://github.com/hail-is/hail/pull/2848,1,['variab'],['variables']
Modifiability,"I've merged a stack of changes to this branch including:; - Cleaned up tests, including refactoring, making Balding-Nichols covariates deterministic and removing lots of extra test code; - Reorder args in Scala to match Python, related bug fixes; - Improved large N performance by using single array D rather than A and B; - Moved dense versus sparse matching outside of loop; - Improved Python docs and Scala remarks; - Debugged test failure only occurring in Spark 2.1.0, which turned out to be related to accuracy of Davies. I've increased accuracy to 1e-8 which is enough to make current tests pass. Once this goes in, I'll make PRs to:; - Allow users to set accuracy and iterations on Davies, will use same defaults as R: 1e-6 and 10k.; - Add number of variants per key as column.; - Fix behavior to finish running even if some groups are too big upper bound, or if Cholesky fails. Document this behavior. Less urgently, but to keep in mind:; - If bottleneck, improve performance of Gramian computation in large N case using blocking; - Improve Davies C code",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2153#issuecomment-325388707:88,refactor,refactoring,88,https://hail.is,https://github.com/hail-is/hail/pull/2153#issuecomment-325388707,1,['refactor'],['refactoring']
Modifiability,"I've merged fixes to two old bugs and one new bug:; - `typecheck_method` => `typecheck`; - an extra `rvb.startStruct()`; - match error for `alleles: Array[String]` to `IndexedSeq[_]` in `RegionValueBuilder.addAnnotation`. (apologies, I meant to PR rather rather than merge directly but forgot to change cseed to origin). Adding; ```; --init gs://hail-common/nirvana/nirvana-init-GRCh37.sh; ```; at cluster startup and running; ```; import hail as hl; mt = hl.import_vcf(path='gs://jbloom/profile225.vcf.bgz'); mt = mt.filter_rows(hl.len(mt.alleles) == 2); ht = hl.nirvana(mt, config='/nirvana/nirvana-cloud-GRCh37.properties', block_size=10000).rows(); (ht.filter((ht.locus.position > 24430000) & (ht.locus.position < 24580000)); .export(output='gs://jbloom/nirvana_cabin1_3.tsv')); ```; yields the same output as before modulo superficial changes to our representation of variant and flattening of `va` to `rsid	qual	filters	info	nirvana`. Here's an examplar now:; ```; locus	alleles	rsid	qual	filters	info	nirvana; 22:24468386	[""G"",""A""]	NA	3.8351e+04	NA	{""AC"":[306],""AF"":[0.061],""AN"":5018,""BaseQRankSum"":26.807,""ClippingRankSum"":-0.538,""DP"":22432,""DS"":null,""FS"":1.203,""HaplotypeScore"":null,""InbreedingCoeff"":0.0335,""MLEAC"":[309],""MLEAF"":[0.062],""MQ"":59.13,""MQ0"":0,""MQRankSum"":16.406,""QD"":14.9,""ReadPosRankSum"":-0.637,""set"":null}	{""chromosome"":""22"",""refAllele"":""G"",""position"":24468386,""altAlleles"":[""A""],""cytogeneticBand"":""22q11.23"",""quality"":null,""filters"":null,""jointSomaticNormalQuality"":null,""copyNumber"":null,""strandBias"":null,""recalibratedQuality"":null,""variants"":[{""altAllele"":""A"",""refAllele"":""G"",""chromosome"":""22"",""begin"":24468386,""end"":24468386,""phylopScore"":3.457,""isReferenceMinor"":null,""variantType"":""SNV"",""vid"":""22:24468386:A"",""isRecomposed"":null,""regulatoryRegions"":null,""clinvar"":null,""cosmic"":[{""id"":""COSM3759087"",""isAlleleSpecific"":true,""refAllele"":""G"",""altAllele"":""A"",""gene"":""CABIN1"",""sampleCount"":2,""studies"":[{""id"":376,""histology"":""carcinoma"",""primarySite"":""large intestine""},{""id",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3122#issuecomment-372137231:576,config,config,576,https://hail.is,https://github.com/hail-is/hail/pull/3122#issuecomment-372137231,1,['config'],['config']
Modifiability,"I've raised the dead whale and rebased against master. Also reverted some of the refactoring changes, so the diff is ~600 lines shorter.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6748#issuecomment-527900542:81,refactor,refactoring,81,https://hail.is,https://github.com/hail-is/hail/pull/6748#issuecomment-527900542,1,['refactor'],['refactoring']
Modifiability,"I've rebased John's branch, added the default block_size change by @shulik7 at Nirvana, and made a few additional small changes including adding the @typecheck_method and @record_method decorators (the latter is now required, else history goes histrionic). The configuration file, init script, and resource files are in`gs://hail-common/nirvana`. The init script `gs://hail-common/nirvana/nirvana-init-GRCh37.sh` makes local copies of the resource files and .net: ; ```; #!/bin/bash. mkdir -p /nirvana/Data/Cache; mkdir -p /nirvana/Data/References; mkdir -p /nirvana/Data/SupplementaryDatabase. #Data is copied for use with Nirvana 1.6.2 as of June 19 2017; gsutil -m cp -r gs://hail-common/nirvana/Data/Cache/24/GRCh37 /nirvana/Data/Cache; gsutil -m cp gs://hail-common/nirvana/Data/References/5/Homo_sapiens.GRCh37.Nirvana.dat /nirvana/Data/References; gsutil -m cp -r gs://hail-common/nirvana/Data/SupplementaryDatabase/39/GRCh37 /nirvana/Data/SupplementaryDatabase; gsutil -m cp -r gs://hail-common/nirvana/netcoreapp1.1 /nirvana; gsutil -m cp gs://hail-common/nirvana/nirvana-cloud-GRCh37.properties /nirvana. chmod -R 777 /nirvana. apt-get -y install curl libunwind8 gettext; curl -sSL -o dotnet.tar.gz https://go.microsoft.com/fwlink/?linkid=843453; mkdir -p /opt/dotnet && sudo tar zxf dotnet.tar.gz -C /opt/dotnet; ln -s /opt/dotnet/dotnet /usr/local/bin; ```. The properties file `nirvana-cloud-GRCh37.properties` points Nirvana to these local resources:; ```; hail.nirvana.location = /nirvana/netcoreapp1.1/Nirvana.dll; hail.nirvana.cache = /nirvana/Data/Cache/GRCh37/Ensembl84; hail.nirvana.reference = /nirvana/Data/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.supplementaryAnnotationDirectory = /nirvana/Data/SupplementaryDatabase/GRCh37; ```. I started a cluster with the init script and ran Nirvana on all of `profile225.vcf`, and later exported results for just a region bounding the gene CABIN1:; ```; from hail import *; hc = (HailContext()). (hc; .import_vcf(path='gs:/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701:261,config,configuration,261,https://hail.is,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701,1,['config'],['configuration']
Modifiability,"I've rebased `jbloom22:lmm_getthisin` onto `jbloom22:py_reg` (which should go in first) and made the parallel modifications (LinearMixedModelCommand is gone, command line relics gone, refactored using RegressionUtils, tests modified to accommodate changing interface, etc). I'll do some command line testing next to be safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1064#issuecomment-272765058:184,refactor,refactored,184,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-272765058,1,['refactor'],['refactored']
Modifiability,I've updated the docs. ; One remaining issue is how to allow multiple `--plugin` options (http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html) ?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1712#issuecomment-298847712:73,plugin,plugin,73,https://hail.is,https://github.com/hail-is/hail/pull/1712#issuecomment-298847712,1,['plugin'],['plugin']
Modifiability,"I've verified that the latest devel Python zip doesn't have hail2. When I `gradle archiveZip` locally, I do have hail2. Odd... I'll look into CI config.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2604#issuecomment-354512643:145,config,config,145,https://hail.is,https://github.com/hail-is/hail/issues/2604#issuecomment-354512643,1,['config'],['config']
Modifiability,"IIRC, it took about an hour of effort to do the first time. Repeat invocations are nearly free, once you've configured it all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4038#issuecomment-433967898:108,config,configured,108,https://hail.is,https://github.com/hail-is/hail/issues/4038#issuecomment-433967898,1,['config'],['configured']
Modifiability,"INFO	2022-03-02 19:06:30,199	hail_logging.py	log:40	https GET /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/credentials done in 0.005999999999858119s: 200; INFO	2022-03-02 19:06:30,226	main.py	activate_instance_1:237	activating instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q; INFO	2022-03-02 19:06:30,991	base.py	check:335	checking on instance batch-worker-pr-11438-default-g6cibyji6520-highcpu-z0idl, last updated 60.151s ago; INFO	2022-03-02 19:06:31,526	pool.py	schedule_loop_body:371	schedule pool standard: starting; INFO	2022-03-02 19:06:31,583	job.py	schedule_job:443	schedule job (94, 2) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,584	job.py	schedule_job:443	schedule job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,585	job.py	schedule_job:443	schedule job (93, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (90, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (94, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,598	job.py	schedule_job:443	schedule job (97, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,656	job.py	schedule_job:443	schedule job (99, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,662	job.py	schedule_job:443	schedule job (100, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,663	job.py	schedule_job:443	schedule job (98, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:1442,config,config,1442,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['config'],['config']
Modifiability,"IO's <code>ExceptionGroup</code> class; causing <code>AttributeError: 'NonBaseMultiError' object has no attribute '_exceptions'</code>; (AnyIO 4 is unaffected)</li>; </ul>; <p><strong>3.6.1</strong></p>; <ul>; <li>Fixed exception handler in the asyncio test runner not properly handling a context; that does not contain the <code>exception</code> key</li>; </ul>; <p><strong>3.6.0</strong></p>; <ul>; <li>; <p>Fixed <code>TypeError</code> in <code>get_current_task()</code> on asyncio when using a custom <code>Task</code> factory</p>; </li>; <li>; <p>Updated type annotations on <code>run_process()</code> and <code>open_process()</code>:</p>; <ul>; <li><code>command</code> now accepts accepts bytes and sequences of bytes</li>; <li><code>stdin</code>, <code>stdout</code> and <code>stderr</code> now accept file-like objects; (PR by John T. Wodder II)</li>; </ul>; </li>; <li>; <p>Changed the pytest plugin to run both the setup and teardown phases of asynchronous; generator fixtures within a single task to enable use cases such as cancel scopes and; task groups where a context manager straddles the <code>yield</code></p>; </li>; </ul>; <p><strong>3.5.0</strong></p>; <ul>; <li>Added <code>start_new_session</code> keyword argument to <code>run_process()</code> and <code>open_process()</code>; (PR by Jordan Speicher)</li>; <li>Fixed deadlock in synchronization primitives on asyncio which can happen if a task acquiring a; primitive is hit with a native (not AnyIO) cancellation with just the right timing, leaving the; next acquiring task waiting forever (<code>[#398](https://github.com/agronholm/anyio/issues/398) &lt;https://github.com/agronholm/anyio/issues/398&gt;</code>_)</li>; <li>Added workaround for bpo-46313_ to enable compatibility with OpenSSL 3.0</li>; </ul>; <p>.. _bpo-46313: <a href=""https://bugs.python.org/issue46313"">https://bugs.python.org/issue46313</a></p>; <p><strong>3.4.0</strong></p>; <ul>; <li>; <p>Added context propagation to/from worker threads in <code>to_t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12362:1386,plugin,plugin,1386,https://hail.is,https://github.com/hail-is/hail/pull/12362,1,['plugin'],['plugin']
Modifiability,Ideally:; - Add a flag to state whether to force rewrite or not,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/859:49,rewrite,rewrite,49,https://hail.is,https://github.com/hail-is/hail/issues/859,1,['rewrite'],['rewrite']
Modifiability,"If the gradle.properties file doesn't exist, our gradle script errors and asks the user to run ./configure. The ./configure script queries the user for sparkVersion and generates a valid gradle.properties file. Afterwards, the user can execute gradle normally without any -D parameters. Users may still override the sparkVersion variable on the command line by specifying -PsparkVersion=2.1.1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613:97,config,configure,97,https://hail.is,https://github.com/hail-is/hail/pull/1613,3,"['config', 'variab']","['configure', 'variable']"
Modifiability,"If these tests are being run, I can't find them. Also, rename incorrectly named gear => config in hailtop tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9396:88,config,config,88,https://hail.is,https://github.com/hail-is/hail/pull/9396,1,['config'],['config']
Modifiability,"If we always mount a deploy config (which sounds like a great idea), I think it would be nice to also always mount the user tokens too (similar to how the GSA key is always mounted). That way, nested batches ""just work"". But happy to make it a configurable option, if you prefer that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-768605979:28,config,config,28,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-768605979,2,['config'],"['config', 'configurable']"
Modifiability,"If we're binding a variable and using it in an array expression, and then transforming that array expression, we should be able to deforest the entire thing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5667:19,variab,variable,19,https://hail.is,https://github.com/hail-is/hail/pull/5667,1,['variab'],['variable']
Modifiability,If you can make this IR rewrite pass determinism with/without optimization I will be totally confident in this change; https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/ir/Simplify.scala#L294,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4104#issuecomment-411794524:24,rewrite,rewrite,24,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411794524,1,['rewrite'],['rewrite']
Modifiability,"If you follow the call path of a `org.apache.hadoop.fs.LocalFileSystem.mkdirs`,; you'll find that it's actually implemented on; `org.apache.hadoop.fs.FilterFileSystem` which implements it in terms of some; internal `fs` variable, which in `LocalFileSystem`'s case is; `org.apache.hadoop.fs.RawLocalFileSystem`. This class has `mkOneDirWithMode`,; which delegates to `java.io.File`. Now we're at ground truth. This is a well; documented Java API. It returns true if the directory was created, false; otherwise. No `IOException`s. Going back up through the callstack, we find that; `IOException`s are thrown for a variety of unusual circumstances (like if; you're creating `/foo/bar/baz` and `/foo/bar` is a file that isn't a; directory), but, ultimately, if the directory *already exists* `mkdirs`; returns `false`, it does *not* throw an `IOException`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1359:220,variab,variable,220,https://hail.is,https://github.com/hail-is/hail/pull/1359,1,['variab'],['variable']
Modifiability,"If you look in the body of create_database, you will see that I copy the; keys from an existing sql-config using `get`. This propagates the `None`s; rather than leaving the keys missing. This fix changes `write_user_config`; to filter out keys set to None. Such keys should not appear in normal; configs because we never use `null` in our configs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8498:100,config,config,100,https://hail.is,https://github.com/hail-is/hail/pull/8498,3,['config'],"['config', 'configs']"
Modifiability,"If you start a HailContext with no arguments:; ```python; HailContext(); ```; then Hail does not require you to set any Spark variables. This error:; ```; hail.java.FatalError: SparkException: Failed to get broadcast_4_piece0 of broadcast_4; ```; is caused by a failure in your Spark cluster. I suggested investigating `spark.cleaner.ttl` due to [Spark bug 5594](https://issues.apache.org/jira/browse/SPARK-5594). This also seems to happen when [you're running more than one spark context at once](https://github.com/spark-jobserver/spark-jobserver/issues/147). You might also be encountering [Spark bug 116599](https://issues.apache.org/jira/browse/SPARK-16599). I think the most productive use of your time is to:; 1. restart your spark cluster; 2. ensure there are no pending jobs and no one will submit any jobs while you run the next steps; 3. start a fresh `pyspark` session; 4. execute your hail commands. If this _still_ fails, then I suspect your Spark cluster is misconfigured in some way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338716796:126,variab,variables,126,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338716796,1,['variab'],['variables']
Modifiability,"If/when the need is pressing, we can extend parsing to deal with both unnamed and named and optional args in full generality. With sort and sortBy, I handled the possibilities more directly and think the documentation is clear as is. I also assume the Boolean parameter is a constant rather than an expression handling null etc, but I think that covers the use cases of these functions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/511#issuecomment-236266169:37,extend,extend,37,https://hail.is,https://github.com/hail-is/hail/pull/511#issuecomment-236266169,2,['extend'],['extend']
Modifiability,"Implemented image untagging for image cleanup steps (like is done in GCR) for Azure. Since old layers still should be used for caching, this just removes the tag used for an image in a test build. We can then do something like [here](https://docs.microsoft.com/en-us/azure/container-registry/container-registry-auto-purge#run-in-an-on-demand-task) where you can purge untagged layers that are older than some number of weeks where we believe they're no longer relevant to the layer cache. I also switched out the `registry-push-credentials` that CI uses to build images from the ACR admin login to CI's service principal and eliminated the admin login from the ACR terraform resource. I dev deployed CI and manually verified after a deploy that a tag that was cleaned up no longer showed up in acr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11100:95,layers,layers,95,https://hail.is,https://github.com/hail-is/hail/pull/11100,2,['layers'],['layers']
Modifiability,"Implemented the transmission disequilibrium test (TDT) in hail. TDT tests for variants that are inherited more or less than what would be expected by chance (i.e., 50%).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/753:96,inherit,inherited,96,https://hail.is,https://github.com/hail-is/hail/pull/753,1,['inherit'],['inherited']
Modifiability,Importing from `batch_configuration` means that for this cache to be used you must define all environment variables that batch depends on. I severed this connection and fixed a use of the k8s cache in bootstrap.py,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11162:106,variab,variables,106,https://hail.is,https://github.com/hail-is/hail/pull/11162,1,['variab'],['variables']
Modifiability,"In PR namespaces, it creates the database-server-config which is the root user which we use to set up everyone else. test_database_instance is cooked up by the createDatabase for the test database. In non-PR namespaces, it doesn't exist because that secret already exists",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11441#issuecomment-1056027209:49,config,config,49,https://hail.is,https://github.com/hail-is/hail/pull/11441#issuecomment-1056027209,1,['config'],['config']
Modifiability,"In R, when you load a data table, it auto-detects whether each column is a character vs. numeric type. It would be super if this could be implemented in Hail. I'm guessing it would take the form of ""if none of the fields in the column contain special characters or letters, then it's numeric, else it's character,"" (but maybe it's not so straight forward, not so sure...). . Anyways, when you have over 30 annotations that are numeric, it's a bit of a pain to have to go through writing all the -t flag options in Hail, so if it could be auto-detected, that would be super! . In the case where 'dummy' variables are used (like 1-5 for Batch), then the user should be able to say that that's a string or a ""factor"" as it is in R (or a character/string, which is essentially the same), for the purposes of analysis in linear regression.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/463:602,variab,variables,602,https://hail.is,https://github.com/hail-is/hail/issues/463,1,['variab'],['variables']
Modifiability,"In a new environment,; ```; cd hail; make install; make pytest; ```; fails with; ```; ...; ERROR: usage: setup.py [options] [file_or_dir] [file_or_dir] [...]; setup.py: error: unrecognized arguments: --instafail --self-contained-html --html=../build/reports/pytest.html; inifile: None; rootdir: /path/to/hail/hail/python; ```. because the pytest plugins in hail/python/dev-requirements.txt are not installed. This documents the need to install them before running tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7942:346,plugin,plugins,346,https://hail.is,https://github.com/hail-is/hail/pull/7942,1,['plugin'],['plugins']
Modifiability,"In a world without gsa keys and hail auth tokens in secrets, the only other k8s secrets that we sometimes add to the job spec are the user ssl config which I believe to be unused (will address in a separate PR) and the `worker-deploy-config` secret. I don't really get why this is in a secret though, this is information already known to the worker and we can effortlessly add this to every job. Not only that, we can write the deploy-config once and mount it as read-only in all containers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13056:143,config,config,143,https://hail.is,https://github.com/hail-is/hail/pull/13056,3,['config'],['config']
Modifiability,"In addition to the main change:; - I made all the global configuration options editable now that we have flexible billing,; - We now check the worker cores and standing worker cores against the worker type to make sure they are valid (there is no highmem/highcpu-1).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9285#issuecomment-674977850:57,config,configuration,57,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674977850,2,"['config', 'flexible']","['configuration', 'flexible']"
Modifiability,"In anticipation of changes to the CI system, I've moved the local tutorial files to `/usr/local/hail-tutorial-files` and updated the CI configuration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1374#issuecomment-279443114:136,config,configuration,136,https://hail.is,https://github.com/hail-is/hail/pull/1374#issuecomment-279443114,1,['config'],['configuration']
Modifiability,"In https://github.com/hail-is/hail/pull/9113, I forced the auth driver to use; the modern, TLS-required, SQL config format. I incorrectly forgot to specify the; TLS file paths. Luckily, when I tried to create a user account for Patrick; Cummings, instead of creating a broken secret, the auth driver; error'ed. Moreover, the clean up code was broken. As a result, Patrick's account; was stuck in `creating`. This PR fixes both the clean up code issue (I set `self.namespace` in; `K8sSecretResource`) and specifies the TLS file paths (see driver.py near; line 217). In addition, this PR attempts to avoid future problems with the sql; configuration by codifying the required components as a NamedTuple, `SQLConfig`. I also; co-located all the parsing and transformation logic between JSON, dicts, and CNF; in the `SQLConfig` class. I traced back all the users of `create_secret_data_from_config` to ensure they; all now use SQLConfig. I added lots of type annotations, but those won't do; anything right now because we don't have mypy enabled for hailtop.auth. ---. There's a separate issue of us not getting notified that Patrick's account was; not being created due to an error. The relevant logs are linked below. I'm glad; we're starting work on better monitoring. Hopefully error logs like these will; trigger emails to services team. https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22auth-driver%22;timeRange=2020-08-11T15:44:00.000Z%2F2020-08-11T23:55:00.000Z?project=hail-vdc&query=%0A. Moreover, the infinite retry of his account created tens of google service; accounts that were not cleaned up. I do not yet understand why the google; service account clean up code failed. The clean up code bug that I *do* fix in; this PR addresses the GSA secret and the tokens secret.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9259:109,config,config,109,https://hail.is,https://github.com/hail-is/hail/pull/9259,2,['config'],"['config', 'configuration']"
Modifiability,"In order to rewrite `linear_regression_rows`, I'm going to need a way to get data out of ndarray type. Need something like https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tolist.html . I'm going to break with the numpy matching though and call it `to_array`, since hail doesn't use lists.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7979:12,rewrite,rewrite,12,https://hail.is,https://github.com/hail-is/hail/issues/7979,1,['rewrite'],['rewrite']
Modifiability,"In particular, isn't it possible that you have the n-1th and nth job racing to complete. Everyone else is already done. Call the n-1th job's transaction T1 and the nth job's transaction T2. Both race down to this statement in MJC:; ```; UPDATE batches; SET time_completed = new_timestamp,; `state` = 'complete'; WHERE id = in_batch_id AND n_completed = batches.n_jobs;; ```. That will now need to have a sum(n_completed) over all tokens. The isolation level is repeatable read. Assume T1 and T2 generate non equal tokens. T1 and T2 may both snapshot the state of the database before either T1 or T2 executes. T1 and T2 will necessarily see the changes they've made (which affect distinct rows because they have distinct tokens), but neither is required to see the changes the other has made. I think the only way to guarantee that at least one of T1 or T2 sees the database with sum(n_completed) == n_jobs is for both of them to LOCK IN SHARE MODE when doing the sum(n_completed). That will cause lock contention. Maybe that's OK? In the worst case you could have this happen:. 1. Job 1 executes all the way to just before the sum(n_completed).; 2. Job 2 executes all the way to modifying the volatile state.; 3. Job 1 blocks waiting for Job 2 to modify the volatile state.; 4. Job 3 executes all the way to modifying the volatile state.; 5. Job 1 and 2 now wait for Job 3 to modifying the volatile state.; 6. ...; 7. Job 1, 2, 3, n-1 now all wait for Job n to modify the volatile state.; 8. Job 1...n finally execute the sum, all in parallel. I guess that's not terrible, it just means that the latency of Job 1 is extended as long as other jobs can race in before it grabs a shared lock on all the rows.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039642916:1616,extend,extended,1616,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039642916,1,['extend'],['extended']
Modifiability,In the cloudtools config it doesn’t actually appear that a bucket is being mounted.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480364182:18,config,config,18,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480364182,1,['config'],['config']
Modifiability,"In the course of this work I also fixed a problem with the staged code generated by the copyFromType methods -- the addresses to copy from were never bound to variables, so in nested types, we ended up duplicating a lot of code (an array of Tuple10s of Tuple10s of Tuple10s would duplicate the top `loadElement` 1000x!)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8099#issuecomment-586380409:159,variab,variables,159,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586380409,1,['variab'],['variables']
Modifiability,"In the spirit of consolidation, this is just translating the current nginx config that is handling TLS for the batch driver to envoy. Doesn't have any special dependencies or deployment complications. Note that this time running Envoy as root is required to listening on the same port (443) that nginx currently listens on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12383:75,config,config,75,https://hail.is,https://github.com/hail-is/hail/pull/12383,1,['config'],['config']
Modifiability,"In this PR, I rewrite `linear_regression_rows_nd` to use `_map_partitions` instead of `_group_within_partitions`. By doing this, I've eliminated the need to do a `key_by` at the end of `linear_regression_rows_nd`. I also think this makes the code clearer. . This PR also makes a few seemingly random changes that are actually bug fixes:. 1. When emitting `Apply` nodes, we were grabbing the `Code[Region]` from the first argument to the `MethodBuilder`. However, the assumption that the first argument will always be a `Region` seems to no longer be true. As such, we just construct a `CodeParam` from the `StagedRegion` we have available. . 2. In the NDArrayEmitter, I want to make sure I call the local `emit` method that passes off to `emitWithRegion`, for the same reason as 1: (Can't trust first argument to be a `Region`). 3. In `EmitStream`, I need to use `memoizeField` instead of `memoize`, because regular `memoize` saves to a `LocalRef`, and that will get reset to 0 when `next` is called on a stream. Lesson: don't trust locals for things that must live between elements of a stream. I feel like you have a better idea of how the Stream stuff gets emitted than I do Patrick. I'm curious if what I wrote in `process_block` could be written in a way that would lead to better code getting emitted, as I still need to figure out how to squeeze more performance out of this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9469:14,rewrite,rewrite,14,https://hail.is,https://github.com/hail-is/hail/pull/9469,1,['rewrite'],['rewrite']
Modifiability,Incorporates grafana which we did not have at the time of previous revision and removes the nginx config that is no longer used. We now use Envoy as our load balancer. I'll make a separate dev doc explaining the gateways.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14400:98,config,config,98,https://hail.is,https://github.com/hail-is/hail/pull/14400,1,['config'],['config']
Modifiability,"Indeed, `pyspark/conf/spark-defaults.conf` defines the following:; ```; spark.hadoop.google.cloud.auth.service.account.enable true; spark.hadoop.google.cloud.auth.service.account.json.keyfile $HOME/.config/gcloud/application_default_credentials.json; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1986237607:199,config,config,199,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1986237607,1,['config'],['config']
Modifiability,"Initializing Spark and Hail with default parameters...; using hail jar at /home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.2.3; SparkUI available at http://10.200.100.39:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.11-cf54f08305d1; LOGGING: writing to /home/unix/dking/hail-20190307-1908-0.2.11-cf54f08305d1.log; 2019-03-07 19:08:30 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 100 samples, and 100 variants...; ^[[A; In [3]: t = hl.linear_regression_rows(x=mt.GT.n_alt_alleles(), y=mt.pop, covariates=[1]) ; [Stage 0:============================================> (6 + 1) / 8]2019-03-07 19:08:39 Hail: INFO: Coerced sorted dataset; 2019-03-07 19:08:40 Hail: INFO: linear_regression_rows: running on 100 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; /broad/software/free/Linux/redhat_7_x86_64/pkgs/jdk1.8.0_181/bin/java: symbol lookup error: /tmp/jniloader1327638724610654731netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemm; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/home/unix/dking/.conda/envs/hail/lib/python3.7/site-pa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5559:1361,variab,variable,1361,https://hail.is,https://github.com/hail-is/hail/issues/5559,2,['variab'],['variable']
Modifiability,"Instead of using `gsutil` we use hailtop.aiotools.copy from the new `HAILGENETICS_HAIL_IMAGE`. Previously, deploying the pip-versioned image was a manual asynchronous step that mostly happened in response to user requests. 1. Actually build and test hailgenetics/hail and hailgenetics/genetics on every build.; 2. On deploy, push the newly built hailgenetics/hail and hailgenetics/genetics images to both docker hub and gcr.io/hail-vdc/; 3. Provide the built-for-this-PR hailgenetics/hail as an env var to the tests.; 4. By default use the hailgenetics/hail image for the currently published pip version for FS operations. Allow overriding by environment variable.; 5. Remove now unnecessary publish-public-images.sh.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11091:655,variab,variable,655,https://hail.is,https://github.com/hail-is/hail/pull/11091,1,['variab'],['variable']
Modifiability,"Intention is two-fold:. - get ready to move the k8s cluster from broad-ctsa to hail-vdc; - automate as much of our infrastructure build out as possible. Ultimately, changing GCP or k8s infrastructure should involve pushing to something like vdc/. We should regularly test rebuild from scratch. Outline of changes:. - added a new project directory, vdc/; - parameterize projects by GCP project for GCR, set from gcloud project config; - parameterize site by domain name and IP address; - GCP resources are deployed using the Google Deployment Manager; - added a MySQL 5.6 instance (to be used by upload, others); - ugprades gke to latest version. Doesn't handle CI yet. I think we need a setting for CI where it runs the tests and tracks its internal state but doesn't do anything on Github.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4545:356,parameteriz,parameterize,356,https://hail.is,https://github.com/hail-is/hail/pull/4545,3,"['config', 'parameteriz']","['config', 'parameterize']"
Modifiability,"Interesting, [PEP 8](https://peps.python.org/pep-0008/#string-quotes) does have an opinion about triple-quoted strings. FWIW, I'm happy to go stricter in the future and allow string normalization and all that (assuming other folks are on board), but I appreciate you aligning the configs for this PR!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14132#issuecomment-1883692445:280,config,configs,280,https://hail.is,https://github.com/hail-is/hail/pull/14132#issuecomment-1883692445,1,['config'],['configs']
Modifiability,"Interface change:. ``` scala; abstract class Type[T] extends BaseType {; def coerce(a: Any): T; // ...; }; ```. Note the two major changes:; - Every `Type` now must correspond to a Scala type; - Every `Type` must know how to convert appropriate values to their associated Scala type. We may then naturally modify methods like `evalCompose`:. ``` scala; def evalCompose[T](ec: EvalContext, typ: Type[T])(subexpr: AST); (g: (T) => Any): () => Any = {; val f = subexpr.eval(ec); () => {; val x = f(); if (x != null); g(typ.coerce(x)); else; null; }; }; ```. which will hopefully induce or enable downstream simplifications.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/624:53,extend,extends,53,https://hail.is,https://github.com/hail-is/hail/issues/624,1,['extend'],['extends']
Modifiability,Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded? Maybe have k8s get the deployment yaml configuration before deleting each deployment and then reapply the yamls for each shutdown step in order. I'm worried this PR will break all dev deploys with migrations in them as you can't just do `make -c batch deploy`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7855#issuecomment-573683956:203,config,configuration,203,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573683956,1,['config'],['configuration']
Modifiability,"Issue the [rewrite] request. To save on reading and writing entire; blobs, as well as transferring data to the machine running the copy; rather than letting google storage move the data. This operation should be effectively free when copying from one location; in a bucket to a different location in the same bucket, as the linked; docs say, when source and destination have the same location and storage; class, the blob is copied in a single request and returns immediately. [rewrite]: https://cloud.google.com/storage/docs/json_api/v1/objects/rewrite",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11922:11,rewrite,rewrite,11,https://hail.is,https://github.com/hail-is/hail/pull/11922,3,['rewrite'],['rewrite']
Modifiability,"Issues to address. - [ ] Type rewriter for CastRename: https://github.com/hail-is/hail/pull/6912/files/5257ca5eead8da3c470932e61944c715a5293913#r317669438. - [x] NA, Die: https://github.com/hail-is/hail/pull/6912#discussion_r317669761, https://github.com/hail-is/hail/pull/6912/files#r317671912. - [x] MakeArray: https://github.com/hail-is/hail/pull/6912#discussion_r317670311; - [x] Literal: walk values: https://github.com/hail-is/hail/pull/6990#discussion_r323810812 ; * Literal appears done, marking completed, but lets verify @tpoterba . - [x] MakeTuple: same as MakeArray; - [x] Coalesce: https://github.com/hail-is/hail/pull/6912#discussion_r317670382; - [x] If; - [ ] Upcast pass in Emit (and Interpret). cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6952:30,rewrite,rewriter,30,https://hail.is,https://github.com/hail-is/hail/issues/6952,1,['rewrite'],['rewriter']
Modifiability,"It adds the last applied configuration annotation which is used to know if a subsequent configuration is different. ```; (base) # kubectl -n default create secret generic foo \ ; --save-config --dry-run=client -o yaml; apiVersion: v1; kind: Secret; metadata:; annotations:; kubectl.kubernetes.io/last-applied-configuration: |; {""kind"":""Secret"",""apiVersion"":""v1"",""metadata"":{""name"":""foo"",""creationTimestamp"":null}}; creationTimestamp: null; name: foo; namespace: default; (base) # kubectl -n default create secret generic foo \; --dry-run=client -o yaml ; apiVersion: v1; kind: Secret; metadata:; creationTimestamp: null; name: foo; namespace: default. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10217#issuecomment-809406141:25,config,configuration,25,https://hail.is,https://github.com/hail-is/hail/pull/10217#issuecomment-809406141,4,['config'],"['config', 'configuration']"
Modifiability,It also looks like we might be able to set datasource as [editable](https://grafana.com/docs/grafana/latest/administration/provisioning/#example-data-source-config-file) in case we need to make any immediate fixes after this merges.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10772#issuecomment-901466830:157,config,config-file,157,https://hail.is,https://github.com/hail-is/hail/pull/10772#issuecomment-901466830,1,['config'],['config-file']
Modifiability,"It is possible this addresses #12950 for two reasons:; 1. In 2.25.0, they rewrote the BlobWriteChannel entirely. [commit](https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b).; 2. Also in 2.25.0, after the rewrite, they fixed in issue with tracking offsets after incremental flushes. Maybe the issue existed in the old code too?. [commit](https://github.com/googleapis/java-storage/commit/c099a2f4f8ea9afa6953270876653916b021fd9f).; 3. In 2.22.4 they modified the BlobWriteChannel to use a different retry method. [commit](https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b). Anyway, all circumstantial.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551:245,rewrite,rewrite,245,https://hail.is,https://github.com/hail-is/hail/pull/13551,1,['rewrite'],['rewrite']
Modifiability,"It is probably sufficient to verify it runs successfully and produces an expected output. https://github.com/hail-is/hail/pull/3872 makes the VEP invocation much more flexible, including running dockerized VEP (although there's still the question of installing the data files). We should probably make similar changes to Nirvana. The Nirvana [wiki](https://github.com/Illumina/Nirvana/wiki/Getting-Started) says it runs ""in Docker"" but I a quick Google didn't turn up any images.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4021:167,flexible,flexible,167,https://hail.is,https://github.com/hail-is/hail/issues/4021,1,['flexible'],['flexible']
Modifiability,"It looks like nose is a dead project. There's nose2, but judging by https://www.reddit.com/r/Python/comments/50nqlp/is_nose_still_relevant_how_about_unittest/ it isn't very widely used. Pytest seems to be most popular, and it has a coverage plugin https://pypi.python.org/pypi/pytest-cov. But if nose is working, maybe it's fine to go with it for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3096#issuecomment-371534813:241,plugin,plugin,241,https://hail.is,https://github.com/hail-is/hail/pull/3096#issuecomment-371534813,1,['plugin'],['plugin']
Modifiability,"It looks like the configuration files for `xfs_quota` serve mostly to persist mappings from project name -> project id, and project id -> filesystem path. We keep that information in the worker anyway, and `xfs_quota` (way deep in its documentation) allows you to specify a path and project id in the project-creation command and then use a project id in the command that sets limits. This avoids locking on configuration files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10467:18,config,configuration,18,https://hail.is,https://github.com/hail-is/hail/pull/10467,2,['config'],['configuration']
Modifiability,"It sounds like this is the kind of configuration change that needs a separate tracker issue, so I've created https://github.com/hail-is/hail/issues/14718 to encompass all the various actions we'll need to take. That issue can also cover the impact assessment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14717#issuecomment-2400754176:35,config,configuration,35,https://hail.is,https://github.com/hail-is/hail/pull/14717#issuecomment-2400754176,1,['config'],['configuration']
Modifiability,"It turns out that Kryo serialization is extra sneaky and will often just try to serialize the parts of a class if the class itself doesn't implement the KryoSerializable interface. I made a trait, `UnKryoSerializable`, that extends KryoSerializable but throws errors on read and write to try to weed out the rest of the places where UnsafeRows are being serialized. The biggest place this popped up was with colValues. For now, they're just being broadcast as safe Annotations everywhere. This depends on a change in #3258 and I'll rebase when that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3288:224,extend,extends,224,https://hail.is,https://github.com/hail-is/hail/pull/3288,1,['extend'],['extends']
Modifiability,"It was previously called config.yaml, when it was actually a python; configparser config file, which is a subset of [ini syntax]. [ini syntax]: https://docs.python.org/3/library/configparser.html. Check to see if the .yaml file exists and the .ini file does not exist.; If so, silently rename the config file to the new name.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9493:25,config,config,25,https://hail.is,https://github.com/hail-is/hail/pull/9493,5,['config'],"['config', 'configparser']"
Modifiability,"It's always bothered me that it's `hailctl config list` and `hailctl dev config show`. I deprecated `show` here and added `list`. I'm also open to not deprecating `show` and aliasing them, but mainly I just want the same subcommand to work in both contexts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12019:43,config,config,43,https://hail.is,https://github.com/hail-is/hail/pull/12019,2,['config'],['config']
Modifiability,It's fine except for the `max_idle_time_msecs` in JPIM and the backwards compatibility with the instance config in the database.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-957973573:105,config,config,105,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-957973573,1,['config'],['config']
Modifiability,"It's interesting, this reading problem is basically a little configuration language, but somehow it's really hard to write the performance we want without lots of code duplication.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1778#issuecomment-300169255:61,config,configuration,61,https://hail.is,https://github.com/hail-is/hail/pull/1778#issuecomment-300169255,1,['config'],['configuration']
Modifiability,"It's somewhat low-prio because we imagine that GroupedMatrixTable as pretty much just an intermediate between `group_{rows, cols}_by` and `aggregate` that probably shouldn't be bound to a variable and inspected. Its structure is the same as the parent MT, but just records group expressions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7410#issuecomment-585213036:188,variab,variable,188,https://hail.is,https://github.com/hail-is/hail/issues/7410#issuecomment-585213036,1,['variab'],['variable']
Modifiability,"JGSCM has inconsistent handling of buckets, folders. Because of this, combined with the fact that both folder and files are blobs in the Google Cloud storage world, that Jupyter's ContentsManager class strips slashes](https://jupyter-notebook.readthedocs.io/en/stable/extending/contents.html#api-paths), and that JGSCM behaves differently in the root folder depending on whether or not `default_path` is set (where without it, the root folder is effectively a listing of buckets, and ""folders"" created within are buckets rather than folder-blobs), means that some operations fail. Currently this appears to be seen only with file moving operations, but may occur under other circumstances. cc @cseed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5820:268,extend,extending,268,https://hail.is,https://github.com/hail-is/hail/issues/5820,1,['extend'],['extending']
Modifiability,"JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); > raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; E hail.utils.java.FatalError: AssertionError: assertion failed; E ; E Java stack trace:; E java.lang.AssertionError: assertion failed; E 	at scala.Predef$.assert(Predef.scala:208); E 	at is.hail.expr.ir.BlockMatrixMap.execute(BlockMatrixIR.scala:269); E 	at is.hail.expr.ir.BlockMatrixMap2.execute(BlockMatrixIR.scala:393); E 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:875); E 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:59); E 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:20); E 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); E 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:53); E 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); E 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.low",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229:3305,rewrite,rewrite,3305,https://hail.is,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229,1,['rewrite'],['rewrite']
Modifiability,JZJZLis/hail/io/OutputBuffer;; 	at scala.Predef$.require(Predef.scala:281); 	at is.hail.asm4s.MethodBuilder.<init>(ClassBuilder.scala:531); 	at is.hail.asm4s.ClassBuilder.newMethod(ClassBuilder.scala:324); 	at is.hail.expr.ir.EmitClassBuilder.newEmitMethod(EmitClassBuilder.scala:584); 	at is.hail.expr.ir.EmitClassBuilder.genEmitMethod(EmitClassBuilder.scala:754); 	at is.hail.expr.ir.EmitClassBuilder.$anonfun$getOrGenEmitMethod$1(EmitClassBuilder.scala:747); 	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86); 	at is.hail.expr.ir.EmitClassBuilder.getOrGenEmitMethod(EmitClassBuilder.scala:746); 	at is.hail.types.encoded.EType.buildEncoderMethod(EType.scala:57); 	at is.hail.types.encoded.EType.buildEncoder(EType.scala:49); 	at is.hail.expr.ir.PartitionNativeWriter$StreamConsumer.consumeElement(TableWriter.scala:294); 	at is.hail.expr.ir.PartitionNativeWriter.$anonfun$consumeStream$1(TableWriter.scala:334); 	at is.hail.expr.ir.PartitionNativeWriter.$anonfun$consumeStream$1$adapted(TableWriter.scala:332); 	at is.hail.expr.ir.streams.StreamProducer.$anonfun$memoryManagedConsume$1(EmitStream.scala:113); 	at is.hail.expr.ir.streams.StreamProducer.$anonfun$memoryManagedConsume$1$adapted(EmitStream.scala:112); 	at is.hail.expr.ir.streams.StreamProducer.unmanagedConsume(EmitStream.scala:100); 	at is.hail.expr.ir.streams.StreamProducer.memoryManagedConsume(EmitStream.scala:112); 	at is.hail.expr.ir.PartitionNativeWriter.consumeStream(TableWriter.scala:332); 	at is.hail.expr.ir.Emit.$anonfun$emit$21(Emit.scala:2629); 	at is.hail.expr.ir.IEmitCodeGen.flatMap(Emit.scala:351); 	at is.hail.expr.ir.Emit.$anonfun$emit$20(Emit.scala:2628); 	at is.hail.expr.ir.EmitCode$.fromI(Emit.scala:445); 	at is.hail.expr.ir.Emit.emit(Emit.scala:2627); 	at is.hail.expr.ir.Emit.emitFallback$1(Emit.scala:811); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:2476); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:786); 	at is.hail.expr.ir.Emit.$anonfun$emitI$241(Emit.scala:2386); 	at is.hail.expr.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:5608,adapt,adapted,5608,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['adapt'],['adapted']
Modifiability,"Jobs that are configured with `mount_tokens=True` will have their Hail tokens mounted into the main container. However, now that we are using access tokens from cloud identities, the tokens are no longer used. This removes the default behavior of mounting the `tokens.json` files since they aren't used by our codebase anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14059:14,config,configured,14,https://hail.is,https://github.com/hail-is/hail/pull/14059,1,['config'],['configured']
Modifiability,Johnc LD Matrix Rewrite Without Matrix Multiplies,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1884:16,Rewrite,Rewrite,16,https://hail.is,https://github.com/hail-is/hail/pull/1884,1,['Rewrite'],['Rewrite']
Modifiability,"Just a bit of background: until now, to satisfy the JVM bytecode verifier, in certain (many) situations we had to initialize local variables to default values. The recent PR https://github.com/hail-is/hail/pull/8312 added a local initializer as part of lir (the bytecode-level IR) that uses a dataflow analysis to compute the minimal required set of initializations at the top of the function.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8317#issuecomment-600572344:131,variab,variables,131,https://hail.is,https://github.com/hail-is/hail/pull/8317#issuecomment-600572344,1,['variab'],['variables']
Modifiability,"Just a refactor, simplifies the interactions between `Worker`, `CloudWorkerAPI` and `CloudUserCredentials` and hopefully makes this code safer and easier to work with. Instead of the following occuring in worker.py:. 1. get credentials string from `CloudUserCredentials`; 2. tell `CloudWorkerAPI` to write credentials string to `path` owned by the job; 3. tell `CloudWorkerAPI` to mount cloudfuse using the credentials stored at `path`. we instead just do. 1. tell `CloudWorkerAPI` to mount cloudfuse using `CloudUserCredentials`. On its own I think this change makes the codepath simpler and easier to think about in terms of where credentials are stored, but this also gets rid of the requirement from `worker.py`'s point of view that credentials must be stored on the filesystem. This will make it easier to transition off of key files and over to metadata server tokens. In order to make the new statement sound in terms of types, we can't have `CloudWorkerAPI.mount_cloudfuse` just accept a `CloudUserCredentials` argument, because that means `GCPWorkerAPI` would need to be able to support an argument of type `AzureUserCredentials`, which would never happen and doesn't make sense. What we can do here is make `CloudWorkerAPI` generic over the subtype of `CloudUserCredentials` that it both produces and consumes. This allows us to use stricter types like `GCPUserCredentials` and `AzureUserCredentials` inside of `GCPWorkerAPI` and `AzureWorkerAPI` respectively and now the type system is happy. It also relaxes the restriction that both of the `GCPUserCredentials` and `AzureUserCredentials` need to conform to the same `cloudfuse_credentials` interface.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12962:7,refactor,refactor,7,https://hail.is,https://github.com/hail-is/hail/pull/12962,1,['refactor'],['refactor']
Modifiability,"Just for reference, I had to make the following changes:; 1. There's a new table that tracks attempts that have been already aggregated. The reason I can't put this variable into the attempts table is due to a circular update that's not allowed.; 2. I added a dummy variable to the attempts table that is just a way to get the update trigger to run. We find all complete attempts from the batches state and update the dummy variable for the attempts table in chunks. Within the new trigger, if the attempt has not been aggregated yet, then we use `NEW.end - NEW.start` as the time to aggregate with. If the attempt has been aggregated, then we use the original way which is just to take the difference between New and Old. At the end, we find all attempts that have not been aggregated (not in the new table that keeps track of whether an attempt has been aggregated) and update the dummy variable to run the trigger. I don't think we need to take any locks on the attempts table because any future writes that occur while we do the last processing will already be aggregated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11996#issuecomment-1178118437:165,variab,variable,165,https://hail.is,https://github.com/hail-is/hail/pull/11996#issuecomment-1178118437,4,['variab'],['variable']
Modifiability,"Just refactoring. VSM is now parameterized by a `MatrixT` that has the row, column and cell types.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1851:5,refactor,refactoring,5,https://hail.is,https://github.com/hail-is/hail/pull/1851,2,"['parameteriz', 'refactor']","['parameterized', 'refactoring']"
Modifiability,"Just renamed a variable, now really back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/153#issuecomment-176331224:15,variab,variable,15,https://hail.is,https://github.com/hail-is/hail/pull/153#issuecomment-176331224,1,['variab'],['variable']
Modifiability,"Just saw this. > adds job.entrypoint method. I'm going to argue against this. The goal is not to make Batch a Docker wrapper, in fact, maybe that's an anti-goal. The goal is to build on docker (and other container tools) to create a natural interface for the problems Batch is trying to solve. The expectation is that the commands to be executed will be specified as part of the Job. In particular, I think if you don't specify the command, the command set is empty and no commands get run. It doesn't default to the docker command by default when no command is specified. So I would solve this by just always invoking the image with `--entrypoint <shell>`. > Long term, I think it would be better to use docker engine for LocalBackend, so that container config/create/start between ServiceBackend and LocalBackend could have a common interface. While code sharing often sounds nice, it also has the effect of entangling otherwise separate pieces of code. That means they cannot evolve separately and makes development more difficult. We have lots of plans to evolve the service backend that won't have analogues in the local backend, and I feel this proposal would make those changes harder and make the code more complex to handle two use cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9219#issuecomment-670667174:755,config,config,755,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670667174,3,"['config', 'evolve']","['config', 'evolve']"
Modifiability,Just to make sure I understand -- the variable rename is to make sure it is clear that `HAIL_PRODUCTION_DOMAIN` means something different than `HAIL_DOMAIN` and is only applicable for CI? This is because the other services will have the correct deploy config?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14164#issuecomment-1898843580:38,variab,variable,38,https://hail.is,https://github.com/hail-is/hail/pull/14164#issuecomment-1898843580,4,"['config', 'variab']","['config', 'variable']"
Modifiability,Just use plugin.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1728:9,plugin,plugin,9,https://hail.is,https://github.com/hail-is/hail/issues/1728,1,['plugin'],['plugin']
Modifiability,"Just want to verify, first cut for EType will be like the existing Type (with missingness), right? Then, Type, EType and PType will be free to evolve separately: we can remove missingness for Type, add non-encoded alternate representations to PTypes, and add alternate encodings (e.g. struct of arrays as arrays of structs) to ETypes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4734#issuecomment-437405727:143,evolve,evolve,143,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-437405727,1,['evolve'],['evolve']
Modifiability,"Key changes:. - Remove old VCF combiner; - Add StreamZipJoinProducers an IR that takes an array, and a function from array.elementType to stream and zip joins the result of calling that function on each member of the array.; - Combine Table._generate and this new stream zip operation to rewrite the gvcf merge stage of the vds combiner in O(1) IR",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13206:288,rewrite,rewrite,288,https://hail.is,https://github.com/hail-is/hail/pull/13206,1,['rewrite'],['rewrite']
Modifiability,KeyboardInterrupt inherits from BaseException and not Exception so we couldn't kill waiting on batches etc. in the terminal. I decided to not catch BaseException and instead add the separate handler case because I wasn't sure we want to actually catch BaseException rather than Exception.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12495:18,inherit,inherits,18,https://hail.is,https://github.com/hail-is/hail/pull/12495,1,['inherit'],['inherits']
Modifiability,Kill text table config.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1197#issuecomment-290858742:16,config,config,16,https://hail.is,https://github.com/hail-is/hail/issues/1197#issuecomment-290858742,1,['config'],['config']
Modifiability,"Large collection of interface improvements to asm4s and Emit {Emit}{Module, Class, Method, Function}Builder. The main goal here is to make generating arbitrary classes a first class activity. Here is a summary of changes:; - newLocal now has parameters: newLocal(). This is a step towards requiring names on all generated objects.; - {Emit}{Class, Method, Function}Builder now takes a type parameter that represents (a supertype) of the class being built: e.g. MethodBuilder[C] is a builder for a method on a class of type C. Note, we can't have a type parameter that represents the actual class type because that doesn't exist yet.; - {Emit}FunctionBuilder all but gone: {Emit}FunctionBuilder is now just a {Emit}MethodBuilder is an apply method. Most functionality moved to {Emit}ClassBuilder.; - Added EmitClassBuilder.; - It is convenient to have e.g. MethodBuilder support the ClassBuilder interface: this is what the Wrapped traits are for: MethodBuilder extends WrappedClassBuilder and ClassBuilder extends WrappedModuleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedMo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8335:961,extend,extends,961,https://hail.is,https://github.com/hail-is/hail/pull/8335,2,['extend'],['extends']
Modifiability,"Last thing before I give up for now. There's this comment in the BlobWriterRetry code:. ```; /** Write channel implementation to upload Google Cloud Storage blobs. */; class BlobWriteChannel extends BaseWriteChannel<StorageOptions, BlobInfo> {. private final ResultRetryAlgorithm<?> algorithmForWrite;; // Detect if flushBuffer() is being retried or not.; // TODO: I don't think this is thread safe, and there's probably a better way to detect a retry; // occuring.; private boolean retrying = false;; private boolean checkingForLastChunk = false;; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1564809125:191,extend,extends,191,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1564809125,1,['extend'],['extends']
Modifiability,"Latest build for spark failing. -- Performing Test CAN_COMPILE_POWER_ALTIVEC - Failed; -- Configuring done; -- Generating done; -- Build files have been written to: /gpfs/home/tpathare/hail_new/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/linux-x86-64/libibs.so; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [lib/linux-x86-64/libibs.so] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 12.153 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-276938635:90,Config,Configuring,90,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-276938635,1,['Config'],['Configuring']
Modifiability,"Laurent, I was totally wrong about being able to do this per-command -- I'm really sorry. I thought that it would be possible to create a new configuration just for this command and use that, but this is only possible for `HadoopConfiguration`s and not `SparkContext`s. Can you reopen the old PR? That model is our only option.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/826#issuecomment-248641543:142,config,configuration,142,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248641543,1,['config'],['configuration']
Modifiability,"Let's build it from scratch, but better, faster, ... Philosophy: Minimal magic, minimal reliance on outside work, don't use it unless we understand it. Goal: <16ms interactions, including <16ms page transitions. Should feel identical to a desktop app in terms of performance, but maintain state like a website (i.e `get` variables). TODO:; - [ ] Profile/logout should be responsive: no user icon / dropdown until narrow view; - [x] Default to redirect rather than popup; - [x] Clicking on login should clear state if auth failed; - [ ] Write test for token verification on backend; - [ ] Add profile page; - [ ] Finish auth/redirect notebook logic in gateway; - [ ] Add notebook state endpoints in gateway; - [ ] Add notebook state view in frontend; - [ ] Break this up into ~10 commits, targeting <= 200 LOC each (with first commit being checking in package-lock.json); - [ ] Deal with cross-origin tracking issues in Safari. This may require using the ""custom domains"" feature of auth0, paid. Workaround could be to poll/websocket request to api server to refresh tokens. . To run:; ```sh; cd packages/web-client; docker build . -t blah; docker run --env-file=env-example -p 3000:3000 blah npm run start; ```; then navigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:321,variab,variables,321,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['variab'],['variables']
Modifiability,Let's make this a config parameter in the future so we can specify which migrations need more cores. This also makes it self-documenting for our future selves.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13822#issuecomment-1777724435:18,config,config,18,https://hail.is,https://github.com/hail-is/hail/pull/13822#issuecomment-1777724435,1,['config'],['config']
Modifiability,"Let's rewrite to use this style. I think that will be both simple and performant, to make us all happy!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7646#issuecomment-561448218:6,rewrite,rewrite,6,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561448218,2,['rewrite'],['rewrite']
Modifiability,"Lines 381-390 had a real bug: we re-used the ""b1"" and ""b2"" variables for the rest of wait which is a dictionary, not a true handle to the batch. I went ahead and fixed all the mypy / pylons issues I found in this file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11203:59,variab,variables,59,https://hail.is,https://github.com/hail-is/hail/pull/11203,1,['variab'],['variables']
Modifiability,"Long awaited, this change prompts the batch driver to only schedule jobs on workers with the most recent instance version, i.e. matches the `INSTANCE_VERSION` global variable. This way we can make backwards incompatible changes between the worker and driver without having to manually kill the whole fleet. This will allow pre-existing workers to finish gracefully, as they will just stop receiving work when the new batch driver is deployed and eventually die off. ### Scheduler changes; Just skips instances where the instance version doesn't match `INSTANCE_VERSION`. ### Autoscaler changes; Cluster stats like free mcpu and live instances are tracked per instance version. The autoscaler now only looks at instances of the latest version when deciding whether it needs more workers. This way we don't get stuck unable to schedule new jobs until the old workers die off because there technically are enough cores available to meet demand but they are from old workers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13055:166,variab,variable,166,https://hail.is,https://github.com/hail-is/hail/pull/13055,1,['variab'],['variable']
Modifiability,"Long term, I think it would be better to use docker engine for LocalBackend, so that container config/create/start between ServiceBackend and LocalBackend could have a common interface. If you're interested, after regenie is finished, I can PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9219#issuecomment-670291415:95,config,config,95,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670291415,1,['config'],['config']
Modifiability,"Look over this correction:; ```; registerMethod(""append"", (a: IndexedSeq[Any], b: Any) => a :+ b, ""Returns the result of adding the element `a` to the end of this Array."")(arrayHr(TTHr), TTHr, arrayHr(TTHr)); registerMethod(""extend"", (a: IndexedSeq[Any], b: IndexedSeq[Any]) => a ++ b, ""Returns the concatenation of this Array followed by Array `a`."")(arrayHr(TTHr), arrayHr(TTHr), arrayHr(TTHr)). registerMethod(""add"", (a: Set[Any], b: Any) => a + b, ""Returns the result of adding the element `a` to this Set."")(setHr(TTHr), TTHr, setHr(TTHr)); registerMethod(""union"", (a: Set[Any], b: Set[Any]) => a ++ b, ""Returns the union of this Set and Set `a`."")(setHr(TTHr), setHr(TTHr), setHr(TTHr)); registerMethod(""intersection"", (a: Set[Any], b: Set[Any]) => a & b, ""Returns the intersection of this Set and Set `a`."")(setHr(TTHr), setHr(TTHr), setHr(TTHr)); registerMethod(""difference"", (a: Set[Any], b: Set[Any]) => a &~ b, ""Returns the elements of this Set that are not in Set `a`."")(setHr(TTHr), setHr(TTHr), setHr(TTHr)); registerMethod(""issubset"", (a: Set[Any], b: Set[Any]) => a.subsetOf(b), ""Returns true if this Set is a subset of Set `a`."")(setHr(TTHr), setHr(TTHr), boolHr); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1491#issuecomment-284774704:225,extend,extend,225,https://hail.is,https://github.com/hail-is/hail/pull/1491#issuecomment-284774704,1,['extend'],['extend']
Modifiability,"Looking into this further. Python abstract methods are similar to Scala's in that they can have implementation, but less similar than my test had seemed to state. Python doc on abc:. """"""; Note Unlike Java abstract methods, these abstract methods may have an implementation. This implementation can be called via the super() mechanism from the class that overrides it. This could be useful as an end-point for a super-call in a framework that uses cooperative multiple-inheritance.; """""". However unlike Scala, even with an implementation, these need to be implemented by the subclass. I hadn't noticed the issue because our Backend didn't inherit from ABC. Fixed in https://github.com/hail-is/hail/pull/9192. ```python; In [25]: class Backend(abc.ABC): ; ...: """""" ; ...: Abstract class for backends. ; ...: """""" ; ...: @abc.abstractmethod ; ...: def close(self): ; ...: """""" ; ...: Close a Hail Batch backend. ; ...: """""" ; ...: return ""Parent"" ; ...: ; ...: ; ...: class LocalBackend(Backend): ; ...: """""" ; ...: Backend that executes batches on a local computer. ; ...: ; ...: Examples ; ...: -------- ; ...: ; ...: >>> local_backend = LocalBackend(tmp_dir='/tmp/user/') ; ...: >>> b = Batch(backend=local_backend) ; ...: ; ...: Parameters ; ...: ---------- ; ...: tmp_dir: :obj:`str`, optional ; ...: Temporary directory to use. ; ...: gsa_key_file: :obj:`str`, optional ; ...: Mount a file with a gsa key to `/gsa-key/key.json`. Only used if a ; ...: job specifies a docker image. This option will override the value set by ; ...: the environment variable `HAIL_BATCH_GSA_KEY_FILE`. ; ...: extra_docker_run_flags: :obj:`str`, optional ; ...: Additional flags to pass to `docker run`. Only used if a job specifies ; ...: a docker image. This option will override the value set by the environment ; ...: variable `HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS`. ; ...: """""" ; ...: ; ...: . In [26]: n = LocalBackend() ; ---------------------------------------------------------------------------; TypeError Traceback ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9191#issuecomment-667235187:468,inherit,inheritance,468,https://hail.is,https://github.com/hail-is/hail/pull/9191#issuecomment-667235187,2,['inherit'],"['inherit', 'inheritance']"
Modifiability,"Looks like the `get_remote_tmpdir` function was made when the Query ServiceBackend needed this config and didn't want to copy code directly from the batch ServiceBackend. Howevr, the batch ServiceBackend was never changed to use the new functions so got left behind. Aside from deleting a lot of duplicate code, the only change is that now the batch ServiceBackend will pick up the following environment variables `HAIL_BATCH_REMOTE_TMPDIR` and `HAIL_BATCH_BILLING_PROJECT`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12799:95,config,config,95,https://hail.is,https://github.com/hail-is/hail/pull/12799,2,"['config', 'variab']","['config', 'variables']"
Modifiability,"Looks like this:; ```; (py37) dking@wmb16-359 # ./install-gcs-connector.sh . To set the active account, run:; $ gcloud config set account `ACCOUNT`. created key [bd10c2da666d327144166cc71ba13075dbd7ea26] of type [json] as [/Users/dking/.hail/gcs-keys/gcs-connector-key.json] for [842871226259-compute@developer.gserviceaccount.com]; mkdir: /Users/dking/anaconda2/envs/py37/lib/python3.7/site-packages/pyspark/conf: File exists; success; ```; I tested it by running `python -c 'import hail as hl; hl.read_table(""gs://danking/gnomad-test.mt"").describe()'`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4500:119,config,config,119,https://hail.is,https://github.com/hail-is/hail/pull/4500,1,['config'],['config']
Modifiability,"Looks like we probably want to add the following to benchmark somewhere:. ```; export MKL_NUM_THREADS=1; export NUMEXPR_NUM_THREADS=1; export OPENBLAS_NUM_THREADS=1; export OMP_NUM_THREADS=1; export VECLIB_MAXIMUM_THREADS=1; ```. Trying to test if setting veclib lower fixes things, but apparently Apple caches the result of the environment variable somewhere so it's unclear whether me setting it is working",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583412384:341,variab,variable,341,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583412384,1,['variab'],['variable']
Modifiability,"Lost task 8.19 in stage 1.0 (TID 2899) (hail-test-w-1.australia-southeast1-a.c.pb-dev-312200.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:4505,Plugin,Plugins,4505,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,"Made the following changes:. - Disabled dataproc tests; - Moved dataproc tests to Makefile, to be run before manual deploys; - Add back VEP cluster test script; - Removed cloudtools config files; - Removed the latest-build functionality; - Added VEP scripts to hailctl/dataproc/resources; - Changed init_notebook to pip install hail wheels, picking up; dependencies automatically; - add out-of-date check (once per day) to hailctl startup",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6250:182,config,config,182,https://hail.is,https://github.com/hail-is/hail/pull/6250,1,['config'],['config']
Modifiability,Make TabixReader take a hadoop configuration in the constructor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5033:31,config,configuration,31,https://hail.is,https://github.com/hail-is/hail/pull/5033,1,['config'],['configuration']
Modifiability,Make split_multi_hts more flexible,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3625:26,flexible,flexible,26,https://hail.is,https://github.com/hail-is/hail/pull/3625,1,['flexible'],['flexible']
Modifiability,"Makes some further progress on simplifying the `PruneDeadFields` pass, with the primary goal of decoupling it from the details of the binding structure. The primary change is to `memoizeValueIR`. Before, it passed in only the requested type of the node, and returned and environment containing all free variables and their requested types. Any bound variables would then need to be removed, and the environments of all children then merged. This low-level manipulation of environments made it closely tied to the binding structure, essentially redundantly encoding everything in `Binds.scala`. Now we pass an environment down into the children, which maps variables to a mutable state tracking the requested type. Each `Ref` node unions the requested type at the reference with the state in the environment. This lets us use the general environment infrastructure. I didn't do an assertion directly comparing the old and new implementations, as I've done with some other pass rewrites. But `PruneDeadFields` has pretty good test coverage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14509:303,variab,variables,303,https://hail.is,https://github.com/hail-is/hail/pull/14509,4,"['rewrite', 'variab']","['rewrites', 'variables']"
Modifiability,Memory refactor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14536:7,refactor,refactor,7,https://hail.is,https://github.com/hail-is/hail/pull/14536,1,['refactor'],['refactor']
Modifiability,Mendel error computation need to be adapted to multi-allelic sites,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/45:36,adapt,adapted,36,https://hail.is,https://github.com/hail-is/hail/issues/45,1,['adapt'],['adapted']
Modifiability,"Minor cleanups, remove unused/unnecessary variables, force/forceBgz have; no use, all files must be bgzip compressed and tabix indexed for; import_gvcfs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11260:42,variab,variables,42,https://hail.is,https://github.com/hail-is/hail/pull/11260,1,['variab'],['variables']
Modifiability,Modify compiler arguments to emit warnings required for scalafix.; Fix failures that arise from the new build configuration.; Run scalafix.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14103:110,config,configuration,110,https://hail.is,https://github.com/hail-is/hail/pull/14103,1,['config'],['configuration']
Modifiability,"Modify hailctl dev config to let you set the domain. Note, this is an; interface change since I changed `hailctl dev config` to act like; gcloud/kubectl `... set property value`. The hardest part of this was getting a doubly-nested subcommand to work in argprase. The existing code is wack, but I will change everything to work like hailctl dev/dev config does below.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9791:19,config,config,19,https://hail.is,https://github.com/hail-is/hail/pull/9791,3,['config'],['config']
Modifiability,Modify the [scorecard.py](https://github.com/hail-is/hail/blob/master/scorecard/scorecard/scorecard.py#L40)'s `user` variable to include Daniel Goldstein's GitHub handle so that he shows up in the random user list.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5087:117,variab,variable,117,https://hail.is,https://github.com/hail-is/hail/issues/5087,1,['variab'],['variable']
Modifiability,"Modulo the exception/`Either` suggestion in `LinearMixedModel.scala`, I'm happy with this. We could push that refactoring into a follow-up pull request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1064#issuecomment-267979500:110,refactor,refactoring,110,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-267979500,1,['refactor'],['refactoring']
Modifiability,More details at #8058. - Include the user's IP in the site logs.; - Fix out of date Makefile. I recognize there's duplication of log format. Abstracting over that doesn't seem *that* valuable and requires putting the shared configuration into a file in the root of hail and then arranging for the shared config file to be in the docker context. It's all kind of annoying and seems low value.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8059:224,config,configuration,224,https://hail.is,https://github.com/hail-is/hail/pull/8059,2,['config'],"['config', 'configuration']"
Modifiability,"Most of the changes here are totally fine, but yes, I think we should hold off on changing paths. I have a to-do item to look into configuring / monkey patching the Sphinx function that's trying to generate links for the signatures. Dan is also OOO this week so not high prio.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9403#issuecomment-688855000:131,config,configuring,131,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-688855000,1,['config'],['configuring']
Modifiability,"Most of the failures were coming from `SUnreachableValue`s inheriting from both `SValue` and `SCode`, which has caused me problems before too. Since `SCode` will be going away, I just gave in and duplicated them all for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10848#issuecomment-914601076:59,inherit,inheriting,59,https://hail.is,https://github.com/hail-is/hail/pull/10848#issuecomment-914601076,1,['inherit'],['inheriting']
Modifiability,"Mostly I wanted ArrayEmitter to inherit from EmitTriplet since it is a type of EmitTriplet, just deforested when allowed. Also ArrayEmitter -> EmitStream, which I feel like is a better name for what it's actually doing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5609:32,inherit,inherit,32,https://hail.is,https://github.com/hail-is/hail/pull/5609,1,['inherit'],['inherit']
Modifiability,"Mostly code reorg. Also:. moved rewriters to ir objects; call Optimize before intepreting; removed Filter{Rows, Cols} rules (non-IR), those should get folded back into the MT methods like other AST-based rules; re-enabled Fitler{Rows, Cols}IR fusion rules since logical and/or is back",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3330:32,rewrite,rewriters,32,https://hail.is,https://github.com/hail-is/hail/pull/3330,1,['rewrite'],['rewriters']
Modifiability,"Mostly infrastructure. Added NewAST base class for Matrix and KeyTable ASTs with a primitive term rewriting engine. This should eventually be a base for AST, too (because we'll want to rewrite value expressions, too). I broke VariantMetadata into two parts: VSMMetadata (static types/metdata for VSM) and VSMLocalValue (part of MatrixValue that is computed/stored on master and broadcast). Added MatrixRead, FilterSamples and FilterVariants matrix AST nodes. Simple optimizer that pushes filters into read and some minor optimizations of filters.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1778:185,rewrite,rewrite,185,https://hail.is,https://github.com/hail-is/hail/pull/1778,1,['rewrite'],['rewrite']
Modifiability,"Mostly random bugs that didn't get flexed until trying to run CI jobs and batch tests within jobs.; - The IP addresses used for jobs immediately got out of sync with GCP and I needed to add an `internal.hail` entry to the worker and job's `/etc/hosts` so that default batch could submit to dev batch.; - GCP's metadata server and DNS nameserver are both 169.254.169.254. Azure has a separate IP address for the latter, so I added this configuration to the CloudWorkerAPI. Something that's not addressed here is that I needed to comment out the resource requirements for build image jobs to make them run on standards. The common 2 vCPU / 10 Gi storage / 7.5 Gi Mem lands on standards in GCP but highcpu on azure, which doesn't have disks implemented yet. I'm not sure what the correct step forward on that front is. Otherwise, dev deploying batch should be possible! I ran into multiple issues where my user's sql config was messed up because it was created from a buggy branch. I tried to fix these for the other dev namespaces (dan's which was made later was fine) but there may be some bits I missed. I got as far as running `test_batch_0` and the tests start (!) but fail quickly because of a blob permission issue on the dev driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11071:435,config,configuration,435,https://hail.is,https://github.com/hail-is/hail/pull/11071,2,['config'],"['config', 'configuration']"
Modifiability,"Motivation for this change: I want to keep the global configuration information in one place, and that's going to be the K8s default/global-config secret. In particular, I want to get rid of config.mk and just pull the relevant information from K8s. The secret currently like this:. ```; $ k get -o json secret global-config | jq '.data | map_values(@base64d)'; {; ""default_namespace"": ""..."",; ""docker_root_image"": ""..."",; ""domain"": ""..."",; ""gcp_project"": ""..."",; ""gcp_region"": ""..."",; ""gcp_zone"": ""..."",; ""gsuite_organization"": ""..."",; ""internal_ip"": ""..."",; ""ip"": ""..."",; ""kubernetes_server_url"": ""...""; }; ```. default_namespace will always be the name of the namespace the secret is in. This adds gsuite_organization which will be used by auth to restrict logins to a fixed GSuite organization, e.g. broadinstitute.org. I have created this secret on our production K8s cluster. The Terraform script will also create it. With this change, CI creates global-config in test and dev ""default"" namespaces based on the one from where CI is operating. The only field that currently needs to be updated is default_namespace. The plan is to pull from global-config in deployments instead of threading these global configuration(s) through CI. I made this change in the CI tests. FYI @lgruen. I'm going to break up the infra-1 work into a few separate PRs to keep it all clear and manageable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9777:54,config,configuration,54,https://hail.is,https://github.com/hail-is/hail/pull/9777,7,['config'],"['config', 'configuration']"
Modifiability,"My changes [are in `main`](https://github.com/hail-is/hail/blob/main/infra/azure/variables.tf#L16), so I think as long as you don't have acr_sku specifically set it should be fine. I anticipate that you'll hit the same issues I hit where it wants to recreate everything though. It seemed to be OK when I did that. We really gotta get the remote state tracking working.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11329#issuecomment-1032033650:81,variab,variables,81,https://hail.is,https://github.com/hail-is/hail/pull/11329#issuecomment-1032033650,1,['variab'],['variables']
Modifiability,"My concern is if we release the code as is now, then we cannot change the buckets where the VEP data is stored without breaking backwards compatibility. Therefore, one idea I had was to keep a single up-to-date hail manifest file in GCS with the current information on what configurations are supported, where the data lives, what VEP version etc as a way of versioning the supported configurations. I did not try and implement this yet. I wanted to run the idea by you first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1514997203:274,config,configurations,274,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1514997203,2,['config'],['configurations']
Modifiability,"My team is pretty excited about hail being released with support for Spark 3.5. One thing I noticed is that it looks like the plan is to [restrict to Spark 3.5.0](https://github.com/hail-is/hail/pull/14158/files#diff-7e9fff5f09cc109665f7fe9baa107affaac24f5dc5a0aa8bc3769221a4c6c328R53) - would it be possible to allow some wiggle room for minor releases? Spark has been beginning to release upgrades much more often than in the past, so restricting to 3.5.0 will prevent access to bug fixes, feature enhancements, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1900997582:500,enhance,enhancements,500,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1900997582,1,['enhance'],['enhancements']
Modifiability,"NFO: Getting 1 non-empty blocks out of 1 blocks; 2018-10-09 15:04:37 ShuffleBlockFetcherIterator: INFO: Started 0 remote fetches in 0 ms; 2018-10-09 15:04:37 Executor: INFO: Finished task 0.0 in stage 4.0 (TID 4). 1539 bytes result sent to driver; 2018-10-09 15:04:37 TaskSetManager: INFO: Finished task 0.0 in stage 4.0 (TID 4) in 7 ms on localhost (executor driver) (1/1); 2018-10-09 15:04:37 TaskSchedulerImpl: INFO: Removed TaskSet 4.0, whose tasks have all completed, from pool ; 2018-10-09 15:04:37 DAGScheduler: INFO: ResultStage 4 (collect at utils.scala:197) finished in 0.008 s; 2018-10-09 15:04:37 DAGScheduler: INFO: Job 2 finished: collect at utils.scala:197, took 0.051042 s; 2018-10-09 15:04:37 CodeGenerator: INFO: Code generated in 5.011153 ms; 2018-10-09 15:04:37 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:37 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table8508c46074` AS `zzz1`; WHERE (0 = 1); 2018-10-09 15:04:37 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:37 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table8508c46074`; 2018-10-09 15:04:38 root: INFO: optimize: before:; (TableCount; (TableKeyBy () False; (TableLiteral))); 2018-10-09 15:04:38 root: INFO: optimize: after:; (TableCount; (TableLiteral)); 2018-10-09 15:04:38 SparkContext: INFO: Starting job: fold at RVD.scala:361; 2018-10-09 15:04:38 DAGScheduler: INFO: Got job 3 (fold at RVD.scala:361) with 1 output partitions; 2018-10-09 15:04:38 DAGScheduler: INFO: Final stage: ResultStage 5 (fold at RVD.scala:361); 2018-10-09 15:04:38 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 15:04:38 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 15:04:38 DAGScheduler: INFO: Submitting ResultStage 5 (MapPartitionsRDD[28] at mapPartitions at ContextRDD.scala:137), which has no missing parents; 2018-10-09 15:04:38 MemoryStore: INF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:28667,config,configuration,28667,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['config'],['configuration']
Modifiability,"Namely, you can solve this. > I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control. without making your new configuration the default, right? The 2nd change you made restored the default behavior for the case of tokens.json, which, it seems to me, gives you inconsistent behavior for 'deploy-config.json', or if we ever changes tokens.json to some other name.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535660700:204,config,configuration,204,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535660700,2,['config'],"['config', 'configuration']"
Modifiability,NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:11677,Config,Configuration,11677,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,"New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in the trust chains, but once incoming trust is; fixed, I a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:7218,config,configuration,7218,https://hail.is,https://github.com/hail-is/hail/pull/8561,2,['config'],"['configuration', 'configures']"
Modifiability,"New PR for NativeModule etc. - NativeModule now has a single big_mutex, so that it is single-threaded (releasing big_mutex; only while sleeping between polling file state). - The run_shell_get_first_line() has been removed, moving almost all configuration into the; module-build makefile; ; - Simplified Makefile. - Remove some historical code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4211:242,config,configuration,242,https://hail.is,https://github.com/hail-is/hail/pull/4211,1,['config'],['configuration']
Modifiability,"New config should be correct. I have left the trailing slash on the location path, because without it requests don't appear to make it to the internal router. I think the issue is upstream.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540628849:4,config,config,4,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540628849,1,['config'],['config']
Modifiability,Next step is refactoring KeyTable.keyedRDD to no longer separate the value columns completely - just pull out the key columns as a separate thing. Then we can fix the bug where outer joins strip the right-hand key.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2111#issuecomment-322775027:13,refactor,refactoring,13,https://hail.is,https://github.com/hail-is/hail/pull/2111#issuecomment-322775027,1,['refactor'],['refactoring']
Modifiability,"No no, I reset the codecs afterwards. I tested and it works as intended; (loading a .gz annotation file with the Gzip codec). I'm trying to fix the; small letter / capital issue (thanks Daniel), but it Git seems to be; case-insensitive when it comes to files... On Wed, Sep 21, 2016 at 11:19 AM, Tim Poterba notifications@github.com; wrote:. > This sets the configuration permanently -- any following commands will use; > the overridden codecs. Setting a global option is almost certainly better; > than getting this kind of leakage, I think; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/826#issuecomment-248645129, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgYRNZnsCXFQnDx9z5wRR1WD4rr0cks5qsUr_gaJpZM4KC1O-; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/826#issuecomment-248646084:358,config,configuration,358,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248646084,1,['config'],['configuration']
Modifiability,No worker deploy config secret,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13211:17,config,config,17,https://hail.is,https://github.com/hail-is/hail/pull/13211,1,['config'],['config']
Modifiability,"Note Unlike Java abstract methods, these abstract methods may have an implementation. This implementation can be called via the super() mechanism from the class that overrides it. This could be useful as an end-point for a super-call in a framework that uses cooperative multiple-inheritance.; """""". However unlike Scala, even with an implementation, these need to be implemented by the subclass. I hadn't noticed the issue because our Backend didn't inherit from ABC. Fixed in https://github.com/hail-is/hail/pull/9192. ```python; In [25]: class Backend(abc.ABC): ; ...: """""" ; ...: Abstract class for backends. ; ...: """""" ; ...: @abc.abstractmethod ; ...: def close(self): ; ...: """""" ; ...: Close a Hail Batch backend. ; ...: """""" ; ...: return ""Parent"" ; ...: ; ...: ; ...: class LocalBackend(Backend): ; ...: """""" ; ...: Backend that executes batches on a local computer. ; ...: ; ...: Examples ; ...: -------- ; ...: ; ...: >>> local_backend = LocalBackend(tmp_dir='/tmp/user/') ; ...: >>> b = Batch(backend=local_backend) ; ...: ; ...: Parameters ; ...: ---------- ; ...: tmp_dir: :obj:`str`, optional ; ...: Temporary directory to use. ; ...: gsa_key_file: :obj:`str`, optional ; ...: Mount a file with a gsa key to `/gsa-key/key.json`. Only used if a ; ...: job specifies a docker image. This option will override the value set by ; ...: the environment variable `HAIL_BATCH_GSA_KEY_FILE`. ; ...: extra_docker_run_flags: :obj:`str`, optional ; ...: Additional flags to pass to `docker run`. Only used if a job specifies ; ...: a docker image. This option will override the value set by the environment ; ...: variable `HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS`. ; ...: """""" ; ...: ; ...: . In [26]: n = LocalBackend() ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-26-6524b19a5d4e> in <module>; ----> 1 n = LocalBackend(); TypeError: Can't instantiate abstract class LocalBackend with abstract methods close; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9191#issuecomment-667235187:1546,variab,variable,1546,https://hail.is,https://github.com/hail-is/hail/pull/9191#issuecomment-667235187,2,['variab'],['variable']
Modifiability,"Note some highlights from the log:; ```; #12 42.27 ./Bio/tmp/Bio-DB-HTS-2.9 - moving files to ./biodbhts; #12 42.27 - making Bio::DB:HTS; #12 42.40 Checking prerequisites...; #12 42.40 requires:; #12 42.40 ! Bio::Root::Version is not installed; #12 42.40 ; #12 42.40 ERRORS/WARNINGS FOUND IN PREREQUISITES. You may wish to install the versions; #12 42.40 of the modules indicated above before proceeding with this installation; #12 42.40 ; #12 42.40 Run 'Build installdeps' to install missing prerequisites.; ```; ```; #13 138.3 Building and testing Test2-Suite-0.000152 ... ! Installing Test2::V0 failed. See /root/.cpanm/work/1682614674.13506/build.log for details. Retry with --force to force install it.; #13 150.9 FAIL; #13 150.9 --> Working on FFI::CheckLib; #13 150.9 Fetching http://www.cpan.org/authors/id/P/PL/PLICEASE/FFI-CheckLib-0.31.tar.gz ... OK; #13 150.9 Configuring FFI-CheckLib-0.31 ... OK; #13 151.1 ==> Found dependencies: Test2::V0, Test2::Require::EnvVar, Test2::Require::Module; #13 151.1 ! Installing the dependencies failed: Module 'Test2::Require::EnvVar' is not installed, Module 'Test2::V0' is not installed, Module 'Test2::Require::Module' is not installed; #13 151.1 ! Bailing out the installation for FFI-CheckLib-0.31. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12946#issuecomment-1526412230:872,Config,Configuring,872,https://hail.is,https://github.com/hail-is/hail/issues/12946#issuecomment-1526412230,1,['Config'],['Configuring']
Modifiability,"Note that currently Hail Batch does add the `HAIL_REGION` environment variable, but this does not include information about the zone in GCP. If there is a metadata server implementation in the cloud exposing such an endpoint could be a reasonable approach over adding environment variables.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14189#issuecomment-1904445987:70,variab,variable,70,https://hail.is,https://github.com/hail-is/hail/issues/14189#issuecomment-1904445987,2,['variab'],"['variable', 'variables']"
Modifiability,"Note this PR replaces the previous [Feature/sas token merge](https://github.com/hail-is/hail/pull/12877) because the original PR branch got jacked up beyond repair. All the comments on the earlier PR are responded to there and addressed in the code for this one. This PR is to enable `hail-az/https` Azure file references to contain SAS tokens to enable bearer-auth style file access to Azure storage. Basic summary of the changes:; - Update `AzureAsyncFS` url parsing function to look for and separate out a SAS-token-like query string. Note: made fairly specific to SAS tokens - generic detection of query string syntax interferes with glob support and '?' characters in file names; - Added `generate_sas_token` convenience function to `AzureAsyncFS`. Adds new azure-mgmt-storage package requirement.; - Updated `AzureAsyncFS` to use `(account, container, credential)` tuple as internal `BlobServiceClient` cache key; - Updated `AzureAsyncFSURL` and `AzureFileListEntry` to track the token separately from the name, and extend the base classes to allow returning url with or without a token; - Update `RouterFS.ls` function and associated listfiles function to allow for trailing query strings during path traversal; - Update `AsyncFS.open_from` function to handle query-string urls in zero-length case; - Change to existing behavior: `LocalAsyncFSURL.__str__` no longer returns 'file:' prefix. Done to make `str()` output be appropriate for input to `fs` functions across all subclasses; - Updated `InputResource` to not include the SAS token as part of the destination file name; - Updated `inter_cloud/test_fs.py` to generically use query-string-friendly file path building functions to respect the new model, where it is no longer safe to extend URLs by just appending new segments with `+ ""/""` because there may be a query string, and added `'sas/azure-https'` test case to the fixture. Running tests for the SAS case requires some new test variables to allow the test code to generate SAS toke",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13140:1022,extend,extend,1022,https://hail.is,https://github.com/hail-is/hail/pull/13140,1,['extend'],['extend']
Modifiability,"Notebook still does auth on incoming requests, which uses the ssl config to talk to `auth`. Potentially a little confusing with a proxy in front of it too, but that felt out of scope for this PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10204#issuecomment-808520954:66,config,config,66,https://hail.is,https://github.com/hail-is/hail/pull/10204#issuecomment-808520954,1,['config'],['config']
Modifiability,"Notebook2 was _literally_ unusable (no favicon). Instead of copying and pasting the favicon link 5 times, I also extracted out the shared elements into a template, and extended it in all other views. How this works:; `layout.html`: contains all shared elements, and marks places where children can insert content (`{% block title %}{% endblock %}`, `{% block head %}{% endblock %}`, `{% block content %}{% endblock %}`). Every other file extends this. The 2 templates that weren't updated (admin-login.html, and workers.html) are placeholders from notebook1 that haven't been updated for notebook 2 yet; they should work, but don't use notebook2 styles, and therefore don't have shared elements to wrap in layout.html. This all works. cc @cseed, @jigold, @danking, @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5827:168,extend,extended,168,https://hail.is,https://github.com/hail-is/hail/pull/5827,2,['extend'],"['extended', 'extends']"
Modifiability,"Notice that this script is working with spark 1.6, the error appears with spark 2. ```; exac_vds_split = hc.read(root + 'andrea_subset_splitmulti_hard.vds'); dbNSFP_vds = hc.read(root + 'dbNSFP_3.2a_variant.filtered.allhg19_nodup.vds'); discovEHR_vds = hc.read(root + 'discoverEHR.vds'); exacV2_vds = hc.read(root + 'exacV2_split_variants_non_in_andrea_subset.vds'); fin_vds = hc.read(root + 'finnish_noexac_subset.vds'). (exac_vds_split; # .filter_variants_expr('v.contig==""7"" && v.start > 75013221 && v.start < 76253221', keep=True); # .filter_variants_expr('v.contig==""20""', keep=True); .annotate_variants_expr('va = drop(va, vep)'); .vep(config='/vep/vep-gcloud.properties', root='va.vep', force=True); .write(stroot + '/andrea_subset_splitmulti_hard_vep.vds', overwrite=True)). exac_vds_split_vep = hc.read(stroot + 'andrea_subset_splitmulti_hard_vep.vds'). (exac_vds_split_vep; .annotate_global_list(root + 'all_scores_reduced.scores', root='global.allgenes'); .annotate_global_expr_by_sample('global.allgenes = global.allgenes.map(x => x.split(""\\t""))'); .variant_qc(); .annotate_variants_vds(discovEHR_vds,root='va.EHR'); .annotate_variants_vds(exacV2_vds,root='va.EXACV2'); .annotate_variants_vds(fin_vds,root='va.FIN'); .annotate_variants_table(root + 'clinvar_clean.txt','Variant(Variant)', root='va.clinvar'); .annotate_variants_table(root + 'lethal_clean.txt','Variant(Variant)', root='va.lethal'); .annotate_variants_expr(; 	""""""; 	va.clinvar.yes=if(isMissing(va.clinvar.Variant)) 0 else 1,; 	va.lethal.yes=if(isMissing(va.lethal.Variant)) 0 else 1,; 	va.nNonRef = gs.filter(g => g.isCalledNonRef).count(); 	""""""); .annotate_variants_expr(; 	""""""; 	va.freq.AF01 = (va.qc.AF < 0.01),; 	va.freq.AF001 = (va.qc.AF < 0.001),; 	va.freq.DOUBLE = (va.qc.AC == 2),; 	va.freq.SING = (va.nNonRef == 1),; 	va.freq.URVEXACV2 = (va.nNonRef == 1 && isMissing(va.EXACV2.qc.AC)),; 	va.freq.URVEXACV2EHR = (va.nNonRef == 1 && isMissing(va.EXACV2.qc.AC) && isMissing(va.EHR.info.AF) && isMissing(va.FIN.qc.AC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1284#issuecomment-274678541:642,config,config,642,https://hail.is,https://github.com/hail-is/hail/issues/1284#issuecomment-274678541,1,['config'],['config']
Modifiability,"Now get:; ```; E AttributeError: ArrayStructExpression instance has no field, method, or property 'select'; E Did you mean:; E ArrayStructExpression inherited method: 'collect'; ```. instead of. ```; AttributeError: ArrayStructExpression instance has no field, method, or property 'select'; Did you mean:; ArrayStructExpression method: 'select'; ArrayStructExpression inherited method: 'collect'. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10608:149,inherit,inherited,149,https://hail.is,https://github.com/hail-is/hail/pull/10608,2,['inherit'],['inherited']
Modifiability,"Now image fetcher asks the running notebook image what worker image its using and pulls that. Also, add a five second sleep after the service's endpoints are configured. Hopefully that prevents these intermittent gateway errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4690:158,config,configured,158,https://hail.is,https://github.com/hail-is/hail/pull/4690,1,['config'],['configured']
Modifiability,"Now the `HAIL_SSL_CONFIG_DIR` environment variable can point to any absolute path. It must contain a `ssl-config.json` file that contains relative paths to `HAIL_SSL_CONFIG_DIR` for the rest of the ssl config files. Also allows `HAIL_TOKENS_FILE` in python, which was already implemented in Scala.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10275:42,variab,variable,42,https://hail.is,https://github.com/hail-is/hail/pull/10275,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"Now we try, in order:; ```; $XDG_CONFIG_HOME/hail; ~/.config/hail; ~/.hail; ```. The [XDG Base Directory Specification] is a freedesktop spec inteded to; define where applications should look for files they need to run. [XDG Base Directory Specification]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. If ~/.hail already exists on your system, this should not break you as; long as $XDG_CONFIG_HOME/hail or ~/.config/hail also do not exist. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125:54,config,config,54,https://hail.is,https://github.com/hail-is/hail/pull/7125,2,['config'],['config']
Modifiability,"Now, as in, after this change, or now as in generally?. MatrixRead is now a parameterized IR. It has three public parameters: it has a requested type (typ) and it can drop the rows or columns. It also has a reader that produces a matrix value based on those parameters. There are currently two readers: MatrixRangeReader and MatrixNativeReader (reading a .mt file). MatrixIR.{read, range} produce these MatrixRead nodes from suitable inputs. I didn't follow this in Python, so Python has MatrixRead (corresponding to reading a .mt) and MatrixRange, which have the ""suitable"" inputs and the parser calls MatrixIR.{read, range} when parsing them. Although TableRead isn't organized this way, I did add a TableIR.read. The problem here is that MatrixNativeReader has the spec that comes from reading the metadata, so either I need to (1) open and read the metadata in Python (duplicating the metadata parsing logic), (2) call into Scala to read the metadata, but then re-encode it when I serialize the IR node, or (3) don't include the spec in the serialized form and let Scala read the metadata when parsing. I chose (3).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3896#issuecomment-403049982:76,parameteriz,parameterized,76,https://hail.is,https://github.com/hail-is/hail/pull/3896#issuecomment-403049982,1,['parameteriz'],['parameterized']
Modifiability,"Numpy both supports this and prominently uses it in documentation. We should support this for people translating Numpy code to Hail code. Note that I don't extend this feature to Hail TupleExpressions, because Hail tuple types are distinct from Hail array types (in that the latter has a single element_type, and the former has a collection of possible types). Python tuples are distinct from lists only in their immutability. Also, we support tuples in concatenate/NDArrayConcat, which makes the constructor difference from Numpy extra confusing. Example Numpy use: ; https://numpy.org/doc/stable/reference/generated/numpy.hstack.html?highlight=hstack#numpy.hstack; `a = np.array((1,2,3))` (this is the first example, which would fail if translated to hail); https://numpy.org/doc/stable/reference/generated/numpy.array.html#numpy.array; `x = np.array([(1,2),(3,4)])`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9117:156,extend,extend,156,https://hail.is,https://github.com/hail-is/hail/pull/9117,1,['extend'],['extend']
Modifiability,"OK, I added a pair of simple tests and got everything working. There's one wrinkle: I don't want to ship jars around so I need the executors to have the same jar as the client. That means the need the test jar for the tests. I'm not quite sure how to properly parameterize that in the build system yet, so I'm just leaving it with the test jar for the moment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6221#issuecomment-499365037:260,parameteriz,parameterize,260,https://hail.is,https://github.com/hail-is/hail/pull/6221#issuecomment-499365037,2,['parameteriz'],['parameterize']
Modifiability,"OK, I gave you maximum spicy. I don't think it's so bad, but let me know if you want me to cut it up. Some remarks:; - This PR successfully tests (and it passes!) and then cleans up this branch: https://github.com/hail-is/hail/pull/5842. See `build.yaml`. It's a thing of beauty (I think).; - That branch has everything but Scala tests and dataproc/cloudtools tests. The latter are easy, the former are a little messy since I want to test against a test jar, and I've decided to switch to maven for that.; - No support for publish or deploy yet.; - There are synchronous calls it `git` in various places which can make the UI sluggish. I'll fix those in another PR.; - Work remains to validate build.yaml and the deploy step yaml.; - I currently run jinja2 if the file (Dockerfile or deployment yaml) ends in `.in`, but I think I'm going to make it unconditional. `.in` just seem error prone.; - In CreateDatabaseStep, I put secret credentials in the pod configuration. That's not ideal, but I don't think it is a serious problem, because nobody who isn't privileged can read the pods, and I can fix it in a later PR (the create database step should generate the passwords, not ci2).; - I disabled the fixme pylint message (on # FIXME comments), since are fixmes are longer lived than a single change sometimes.; - I'm slightly confused about runImage (which generates a batch job) and deploy of a pod spec (which runs kubectl apply as a batch job). Right now, runImage always runs in batch-pods, and a deploy job runs in whatever namespace you specify. Fixes https://github.com/hail-is/hail/issues/5903",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5891:955,config,configuration,955,https://hail.is,https://github.com/hail-is/hail/pull/5891,1,['config'],['configuration']
Modifiability,"OK, I have a better answer for why we can't rewrite the AggLets in place -- . we can have IRs like:. ```; TableAggregate; AggLet foo; row; ApplyAggOp(... Ref row); ```. If we extract in place, the `AggLet` becomes a `Let` in the local post-aggregation operation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5479#issuecomment-479660187:44,rewrite,rewrite,44,https://hail.is,https://github.com/hail-is/hail/pull/5479#issuecomment-479660187,1,['rewrite'],['rewrite']
Modifiability,"OK, I have a version of this branch that's nearly ready and addresses the add_instance issue as well as some other issues I hit along the way to fixing that. Unfortunately, it's not compatible with hail-is/main? What happened to batch_configuration.py? It seems to be using the global config even though the main branch batch_configuration does not.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-955880381:285,config,config,285,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-955880381,1,['config'],['config']
Modifiability,"OK, I reverted the retry on apt-get install, but kept the refactoring that makes apt-get update and pip use the same retry script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9906#issuecomment-767084684:58,refactor,refactoring,58,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-767084684,1,['refactor'],['refactoring']
Modifiability,"OK, I think this is ready. The goal was to fix the problem of client and server components being mixed together inside hailtop.gear. Changes:. - break out the client part of hailtop.gear into hailtop.{utils, config, auth}; - move the server parts of hailtop.gear to a toplevel gear module; - added service_base_image with hailtop and gear; - moved async_to_blocking and blocking_to_async to hailtop.utils; - also some bits of cleanup. In particular this fixes the aiomysql issue by moving the database stuff into gear.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7031#issuecomment-529671940:208,config,config,208,https://hail.is,https://github.com/hail-is/hail/pull/7031#issuecomment-529671940,1,['config'],['config']
Modifiability,"OK, I won't be able to fix this. @ehigham @patrick-schultz @daniel-goldstein some combo of you three can probably figure it out. The local backend tests that hit requester pays buckets are failing with new Spark. New Spark needs new GCS hadoop connector (see the Dockerfiles). New GCS hadoop connector has [brand new configuration parameters](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/v3.0.0/gcs/INSTALL.md). Somehow I managed to make the normal Spark backend work correctly but the Local backend (which still, afaik, uses Spark & Hadoop for filesystems) is still trying to pick up CI's credentials instead of the test account's credentials. ```; E hail.utils.java.FatalError: GoogleJsonResponseException: 403 Forbidden; E GET https://storage.googleapis.com/storage/v1/b/hail-test-requester-pays-fds32/o/zero-to-nine?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata&userProject=hail-vdc; E {; E ""code"": 403,; E ""errors"": [; E {; E ""domain"": ""global"",; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist)."",; E ""reason"": ""forbidden""; E }; E ],; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist).""; E }; E ; E Java stack trace:; E java.io.IOException: Error accessing gs://hail-test-requester-pays-fds32/zero-to-nine; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1986); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1882); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloud",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236:317,config,configuration,317,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236,1,['config'],['configuration']
Modifiability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an [`ignore` condition](https://docs.github.com/en/code-security/supply-chain-security/configuration-options-for-dependency-updates#ignore) with the desired `update_types` to your config file. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13576#issuecomment-1709450787:456,config,configuration-options-for-dependency-updates,456,https://hail.is,https://github.com/hail-is/hail/pull/13576#issuecomment-1709450787,510,['config'],"['config', 'configuration-options-for-dependency-updates']"
Modifiability,"OK, I'm not sure how to fix this but the work is to explain to the GCS Hadoop Connector which credentials we want it to use. See the failure here: https://batch.hail.is/batches/8136069/jobs/49 . It uses CI's credentials instead of the test credentials. We use core-site.xml to do this in Spark <3.5, but the GCS connector is different in Spark 3.5 and it uses different configuration parameters. My most recent change did not successfully configure it. Daniel G can help you a bit with credentials in Batch if that's necessary but the real work is to figure out how to tell the GCS Hadoop Connector to use the /gsa-key/key.json file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1961672844:370,config,configuration,370,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1961672844,2,['config'],"['configuration', 'configure']"
Modifiability,"OK, I've made most of the changes and I'd appreciate some feedback before I finalize the PR. Notable changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:168,config,configuration,168,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,2,['config'],['configuration']
Modifiability,"OK, should be resolved now. I had a lint error and needed to mount the global config into hello and website.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12380#issuecomment-1297763402:78,config,config,78,https://hail.is,https://github.com/hail-is/hail/pull/12380#issuecomment-1297763402,1,['config'],['config']
Modifiability,"OK, so the big insight is that ""InstanceConfig"" is really just ""ResourcesForAParticularInstance"" (well, and, sometimes, ""ResourcesOfARepresentativeInstance""). I trimmed the InstanceConfig down *significantly* removing the ""vm_config"". Now the InstanceConfig is cheap and easy to create and there's no circularity between vm_config and instance config. I pushed that through everywhere and then abstracted the common create_instance logic for pool and job-private into InstanceCollection. With both of those changes, I was able to modify the ResourceManager's API to expose methods for constructing instance configs. However, the instance config isn't critical to the operation of the ResourceManager. It's just an interface for communicating an instance's resources to the rest of the code base.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956500279:344,config,config,344,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956500279,3,['config'],"['config', 'configs']"
Modifiability,"OK, so, this is apparently an issue where browsers have not yet really implemented the standard correctly. From my reading of [HTML 2.6.4](https://html.spec.whatwg.org/#cors-settings-attributes), `crossorigin=""anonymous""` ought to be sufficient for requests to the same origin as the page containing the script tag. Jake Archibald has an informative [blog post](https://jakearchibald.com/2017/es-modules-in-browsers/) about modules. He links to a [demo](https://module-script-tests-sreyfhwvpq.now.sh/cookie-page) of three cross origin configurations. The three options are:; ```; <script type=""module"" src=""cookie-script""></script>; <script type=""module"" crossorigin="""" src=""cookie-script?1""></script>; <script type=""module"" crossorigin=""use-credentials"" src=""cookie-script?2""></script>; ```; I'm using Safari Version 13.1 (14609.1.20.111.8). I usually only see the very last script working. However, inexplicably, I have seen the first one very rarely work. All I've been doing is refreshing here and there as I try to understand this. The Safari inspector confirms that the cookie is only sent with thee last option. So, anyway, I'm adding `crossorigin=""use-credentials""`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8928#issuecomment-639218007:535,config,configurations,535,https://hail.is,https://github.com/hail-is/hail/pull/8928#issuecomment-639218007,1,['config'],['configurations']
Modifiability,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:1314,config,config,1314,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146,2,['config'],['config']
Modifiability,"OK, so. Dev deployed `site` uses the same docs and www as non-dev-deployed site. Unfortunately, this means we have a root-URL issue. I use [an nginx `sub_filter`](https://github.com/hail-is/hail/blob/master/site/site.sh) rule to fix links and anchors. I didn't add any rules for javascript modules. The right fix is to move to a system that can build for different environments and set the root URL properly. Until we switch to that hypothetical new system, I'll add a rule that properly rewrites JS module imports. We can't fix this with symlinks or any kind of redirection. The root issue is that we, the users, are outside of the system and the way we specify to whom we're talking is `internal.hail.is/NAMESPACE/SERVICE/`. Cotton wanted to do `service.namespace.internal.hail.is` but there were some TLS wildcard issues. It is perhaps worth revisiting this at some point because it is a perennial issue for us.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8923#issuecomment-639077704:488,rewrite,rewrites,488,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639077704,1,['rewrite'],['rewrites']
Modifiability,"OK, sure.; Step1 : when start pyspark with the cons; ```; [root@tele-1 ~]# PYSPARK_PYTHON=""ipython"" pyspark --conf spark.sql.files.openCostInBytes=1099511627776 --conf spark.sql.files.maxPartitionBytes=1099511627776 --conf spark.hadoop.parquet.block.size=1099511627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer; /usr/local/lib/python3.5/site-packages/IPython/core/history.py:228: UserWarning: IPython History requires SQLite, your history will not be saved; warn(""IPython History requires SQLite, your history will not be saved""); Python 3.5.2 (default, Jul 12 2017, 14:00:23) ; Type ""copyright"", ""credits"" or ""license"" for more information. IPython 5.1.0 -- An enhanced Interactive Python.; ? -> Introduction and overview of IPython's features.; %quickref -> Quick reference.; help -> Python's own help system.; object? -> Details about 'object', use 'object??' for extra details.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/10 08:41:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/10 08:41:32 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/10 08:41:32 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/10 08:41:32 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 3.5.2 (default, Jul 1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321420160:687,enhance,enhanced,687,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321420160,1,['enhance'],['enhanced']
Modifiability,"OK, switched to no pip installs. the hailjwt error was due to using python instead of python3. Makefile now defines PYTHON variable that sets the path correctly before invoking python3. Addressed other comments as well. ---. Don't approve yet, I discovered a race condition wrt pod creation and updates from k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312:123,variab,variable,123,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312,1,['variab'],['variable']
Modifiability,"OK, there was a `justSpark` ""configuration"". I dunno what that means, it wasn't referenced by the other configurations. I initially made it `implementation` which includes it in the shadow JAR. I switched it to `shadow` b/c mllib (the only thing in the justSpark) should be provided by Spark at runtime anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1708500255:29,config,configuration,29,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1708500255,2,['config'],"['configuration', 'configurations']"
Modifiability,"OK, this should fix routing from internal.hail.is. The gateway routes internal.hail.is/ns/svc to router.ns with Host: svc.internal so the ns router can dispatch to the right server block off the Host. We could dispatch off the URL, but that would mean the default and private namespaces dispatch different, doubling the complexity of the router NGINX configuration. Change the host back for grafana and prometheus which generate a redirect otherwise. The monitoring and gateway changes are deployed and everything seems to be working.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6928:351,config,configuration,351,https://hail.is,https://github.com/hail-is/hail/pull/6928,1,['config'],['configuration']
Modifiability,"ONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2905,config,config,2905,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,"Observed again https://batch.hail.is/batches/7942284/jobs/203. This method isn't threaded at all: https://github.com/hail-is/hail/blob/5d545c8d9fb474ea211e350c5dc48c599db6f5ea/hail/src/test/scala/is/hail/fs/FSSuite.scala#L339-L368. The temporary location is drawn as 62 choose 22. So, odds of collision are 3 * 10^16. ~~I can't find the referenced case analysis in Google's latest code. [It is present in this fork](https://github.com/leogamas/java-storage/blob/2af8dfd95cdebc9e4d8252b0bbe3f092844d9f2c/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java#L68-L198) from a few years ago.~~. Here's the [referenced case analysis in 2.17.1](https://github.com/googleapis/java-storage/blame/v2.17.1/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java). There seems to have been a rewrite [two months ago](https://github.com/googleapis/java-storage/blame/main/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java) (here's [the main commit](https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b)). That landed in [2.25.0](https://github.com/googleapis/java-storage/releases/tag/v2.25.0) which was released in July. ```; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:836,rewrite,rewrite,836,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['rewrite'],['rewrite']
Modifiability,"Oh yeah, sorry, this is confusing. . #6322 (which went in yesterday) changed the `pip` definition to `PIP` to indicate it's supposed to be a user-parameterizable variable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6318#issuecomment-501295237:146,parameteriz,parameterizable,146,https://hail.is,https://github.com/hail-is/hail/pull/6318#issuecomment-501295237,2,"['parameteriz', 'variab']","['parameterizable', 'variable']"
Modifiability,"Oh, I didn't realize we're running pre- and post-condition checks on every pass. I would think the main reason to require the post-condition of one pass to match the pre-condition of the next (besides general hygiene) is to only have to do the check once. Anyways, I agree this seems fine for now. I think the root of the issue is that we're assuming that `Optimize` preserves all possible `IRState`s, which is a pretty bold claim. Eventually we should probably refactor `Optimize` to separate out rules that apply to different `IRState`s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9030#issuecomment-651982433:462,refactor,refactor,462,https://hail.is,https://github.com/hail-is/hail/pull/9030#issuecomment-651982433,1,['refactor'],['refactor']
Modifiability,Ok @tpoterba I added the environment variables where I think you originally wanted them in the pipeline task.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583557445:37,variab,variables,37,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583557445,1,['variab'],['variables']
Modifiability,Ok so something like this I guess. Happy to change the env variable name to something more generic if you think it appropriate.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11771#issuecomment-1100418974:59,variab,variable,59,https://hail.is,https://github.com/hail-is/hail/pull/11771#issuecomment-1100418974,1,['variab'],['variable']
Modifiability,"Ok there's another change in here now. `tls/config.yaml` now allows multiple domains listed for a single cert, which are added as alternate names. The first domain listed is used as the Common Name (CN), though that should hopefully not matter. So for any k8s service, the name of that service must be one of the domains listed in the corresponding cert.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10222#issuecomment-807669744:44,config,config,44,https://hail.is,https://github.com/hail-is/hail/pull/10222#issuecomment-807669744,1,['config'],['config']
Modifiability,"Ok, I finally figured out why the test wasn't picking up the tokens. This is going to be fairly useless to users unless/until we also bundle in a dev config, although it definitely should suffice for what I need to do. Thoughts on doing that? (either here or in a separate PR?) I imagine the config would just be like {location: gce, namespace: $default_ns} but I'm not sure if that would leads to issues?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9437#issuecomment-691234229:150,config,config,150,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691234229,2,['config'],['config']
Modifiability,"Ok, I had to fix some shit with gradle so that it doesn't try to call git when configuring tasks. Should pass tests now. I _hate_ gradle.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4066#issuecomment-410318525:79,config,configuring,79,https://hail.is,https://github.com/hail-is/hail/pull/4066#issuecomment-410318525,1,['config'],['configuring']
Modifiability,"Ok, I've downloaded JSON for the batch, default, and CI dashboards (I don't think any of the other ones are really used?), and recorded the configurations for the GCP and prometheus datasources, so I think I should be able to quickly reconfigure grafana if everything is lost.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10772#issuecomment-906539923:140,config,configurations,140,https://hail.is,https://github.com/hail-is/hail/pull/10772#issuecomment-906539923,1,['config'],['configurations']
Modifiability,"Ok, IntIterator no longer extends Iterator[Int], and I added the necessary methods for all the regression methods. Is this what you had in mind?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1425#issuecomment-281864240:26,extend,extends,26,https://hail.is,https://github.com/hail-is/hail/pull/1425#issuecomment-281864240,1,['extend'],['extends']
Modifiability,"Okay, I have a branch that rewrites `TableOrderBy` to use the same mechanism we use for key ordering. It's based on #4354, since there I factored out the pieces that I'm reusing in `TableOrderBy`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4367#issuecomment-423307651:27,rewrite,rewrites,27,https://hail.is,https://github.com/hail-is/hail/pull/4367#issuecomment-423307651,1,['rewrite'],['rewrites']
Modifiability,"On Oct 21, 2017, at 1:27 PM, Sun-shan <notifications@github.com> wrote:. > hail: info: SparkUI: http://10.131.101.159:4040; > Welcome to; > __ __ <>__; > / // /__ __/ /; > / __ / _ `/ / /; > // //_,/// version 0.1-f69b497; > ; > print sc; > ; > >>> print hc >>> hc.import_vcf() Traceback (most recent call last): File """", line 1, in TypeError: import_vcf() takes at least 2 arguments (1 given) >>> hc.import_vcf('/hail/sample.vcf') [Stage 0:> (0 + 1) / 2]Traceback (most recent call last): File """", line 1, in File """", line 2, in import_vcf File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)) hail.java.FatalError: SparkException: Failed to get broadcast_4_piece0 of broadcast_4. This type of Spark exception seems to be related to the configuration option spark.cleaner.ttl. Have you set that to a value other than the default?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338442661:822,config,configuration,822,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338442661,1,['config'],['configuration']
Modifiability,"On Ubuntu 20.10, with Python 3.8.6 and hail 0.2.64 installed from pip, I get: ​I get `TypeError: an integer is required (got type bytes)` immediately upon importing hail. A full transcript is below. (pyve is an alias to create a python virtual env and activate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:933,sandbox,sandbox,933,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['sandbox'],['sandbox']
Modifiability,"Once the current auth overhaul is through and we go ""keyless"", the only secret left that we mount into user jobs is the deploy config, which at that point feels kind of silly. It's also nearly always the same deploy config value that we serialize and write to a file for every job. It seems cleaner and simpler to me that we create one deploy config for the worker, and the worker readonly mounts that config into every job. Note that this is overridable so that any pre-existing jobs that specify a deploy-config secret and all the special deploy-config stuff that we do in build.yaml should not be affected",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13203:127,config,config,127,https://hail.is,https://github.com/hail-is/hail/pull/13203,6,['config'],['config']
Modifiability,"Once we fold in cloud tools, the CI will only be testing hail-is/hail, so all these questions of coupling between CI and hail-is/hail are moot. (Currently, these changes will introduce a few broken object links to the cloud tools repo. Which is mostly untidy rather than bad in anyway.). I created `artifacts/index.html` to work around a GCS limitation. Serving a bucket directly doesn't generate an `ls -l` style index.html for directories. nginx and apache are happy to do this. There are two layers:. - ci's ""job"" information (a log and a directory of artifacts); - project-specific artifacts. I think the friction is caused by CI & batch lacking a way to express a DAG of jobs. We've hacked this on via background processes, but now the ci's ""job"" page doesn't neatly correspond to one process log. Cotton's proposed batch DAG (& CI's use of it) resolves this by restoring the separation of ""job"" (or, now, ""jobs"") information and artifacts (which are truly just artifacts). All that said, if this makes your life easier, I don't mind the mild ugliness on cloud tools while we wait for batch DAG to land. Let me know and I'll approve.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4667#issuecomment-433954860:97,coupling,coupling,97,https://hail.is,https://github.com/hail-is/hail/pull/4667#issuecomment-433954860,2,"['coupling', 'layers']","['coupling', 'layers']"
Modifiability,"One final comment, the goal here was separate the normal user notebook flow from the workshop guest notebook flow, while sharing the main logic without impacting logic outside notebook. I think that was largely successful. I think the only impact outside was to layout.html in web_common, it checks a `workshop` variable to load the workshop header instead of the default one. This is necessary because you can't override a block in a included file from the file that includes it. The other design I considered was have auth support a guest user for workshops which was represented just like any other user, but this seemed both more complicated and more error prone from the security perspective. As we have other use cases for guest users (e.g. free tier), let's revisit this decision.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842:312,variab,variable,312,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842,1,['variab'],['variable']
Modifiability,"One of the tests failed because I messed up some variable names, but if we compare the times:. https://storage.googleapis.com/hail-ci-0-1/ci/7cd5eec6ab8e669fe1adc3061039573c969eb16e/7eac2a154140017f6623db20f277563537b3e075/artifacts/test-report/index.html. vs . https://storage.googleapis.com/hail-ci-0-1/ci/abf0f7d9eebed9774b9b24f3e96fd74683becff4/9b150225332bdfca222715cf5cf6bb483192b54d/artifacts/test-report/index.html. there's a difference of about 9 minutes, most of which is in InterpretSuite (which contains a lot of duplicate tests now and is removed in #5579), so I expect the difference to be not so large once both of these are in. The C++ compile times are definitely a thing we need to keep thinking about, but 20 minutes for the scala tests (hopefully < 15 once the other PR lands) seems workable for now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5578#issuecomment-471622174:49,variab,variable,49,https://hail.is,https://github.com/hail-is/hail/pull/5578#issuecomment-471622174,1,['variab'],['variable']
Modifiability,"One option is to extend NormalizeNames to take a prefix, and use a uid as a prefix when calling NormalizeNames inside ForwardLets. . Another option is to add an option to NOT run NormalizeNames inside the optimizer, when we known that there cannot be name collisions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5710#issuecomment-483001807:17,extend,extend,17,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-483001807,1,['extend'],['extend']
Modifiability,Only create serializable and broadcasted HadoopConf once in HailContext and use everywhere else. I was seeing pipelines with MANY duplicate broadcasts the Hadoop configuration.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5422:162,config,configuration,162,https://hail.is,https://github.com/hail-is/hail/pull/5422,1,['config'],['configuration']
Modifiability,"Oops, quite right. I pulled this diff from the other install, I just fixed the pieces I deployed there. I didn't intend this to be a common include for all Makefiles (should it be?), but just pull together the relevant defines used in services Makefiles. I grep'ed to make sure the variables defined config.mk are now only defined there.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9339#issuecomment-679385861:282,variab,variables,282,https://hail.is,https://github.com/hail-is/hail/pull/9339#issuecomment-679385861,2,"['config', 'variab']","['config', 'variables']"
Modifiability,"Options for nested `forAll`:. ``` scala; toProp(for (; j <- forAll(Gen.choose(0, 10000));; k <- forAll(Gen.choose(0, 10000));; ) yield {; val gt = if (j < k) GTPair(j, k) else GTPair(k, j); Genotype.gtPair(Genotype.gtIndex(gt)) == gt; }).check(); ```. ``` scala; forAll(Gen.choose(0, 10000)) { (j: Int) =>; forAll(Gen.choose(0, 10000)) { (k: Int) =>; val gt = if (j < k) GTPair(j, k) else GTPair(k, j); Genotype.gtPair(Genotype.gtIndex(gt)) == gt; }; }.check(); ```. I think I can ditch the `toProp` on the do notation with an implicit conversion. I might be able to support either syntax in a unified way, but I haven't found the time to think about it. There's a little bit of weirdness because you only want `check` to be callable on things that are `Boolean`-valued. The difference between this monad and the `Gen[T]` monad is that this one is a reader monad, collecting a stack of ""read"" variables that can be used by the inner most `forAll` to generate a useful check-failure message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/400#issuecomment-244517801:893,variab,variables,893,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-244517801,1,['variab'],['variables']
Modifiability,"Options</code> mimic the following:; <pre><code> StorageOptions.grpc(); .setAttemptDirectPath(true); .build(); </code></pre>; </li>; <li>Internally the default host endpoint <code>https://storage.googleapis.com:443</code> will be transformed to the applicable <code>google-c2p-experimental:///storage.googleapis.com</code></li>; </ol>; </li>; <li>; <p>Support for <code>java.time</code> types on model classes</p>; <ol>; <li>Points in time are now represented with <code>java.time.OffsetDateTime</code>, while durations are represented with <code>java.time.Duration</code></li>; <li>All existing <code>Long</code> centric methods are still present, but have been deprecated in favor of their corresponding <code>java.time</code> variant</li>; <li>At the next major version, these deprecated methods will be replaced with types from <code>java.time</code> and the <code>java.time</code> variant methods will be deprecated</li>; </ol>; </li>; <li>; <p><code>com.google.cloud.storage.Storage</code> now extends <code>java.lang.AutoClosable</code> thereby allowing it to be used in a try-with-resource block.</p>; </li>; </ol>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/java-storage/blob/main/CHANGELOG.md"">google-cloud-storage's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.14.0...v2.15.0"">2.15.0</a> (2022-11-07)</h2>; <h3>Features</h3>; <ul>; <li>Add Autoclass support and sample (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1697"">#1697</a>) (<a href=""https://github.com/googleapis/java-storage/commit/82aacd7922573d6f4779f21cdc83de10616d7a08"">82aacd7</a>)</li>; <li>Update retries for Notifications (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1734"">#1734</a>) (<a href=""https://github.com/googleapis/java-storage/commit/0fb2f1823f9e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12456:5415,extend,extends,5415,https://hail.is,https://github.com/hail-is/hail/pull/12456,1,['extend'],['extends']
Modifiability,"Options</code> mimic the following:; <pre><code> StorageOptions.grpc(); .setAttemptDirectPath(true); .build(); </code></pre>; </li>; <li>Internally the default host endpoint <code>https://storage.googleapis.com:443</code> will be transformed to the applicable <code>google-c2p-experimental:///storage.googleapis.com</code></li>; </ol>; </li>; <li>; <p>Support for <code>java.time</code> types on model classes</p>; <ol>; <li>Points in time are now represented with <code>java.time.OffsetDateTime</code>, while durations are represented with <code>java.time.Duration</code></li>; <li>All existing <code>Long</code> centric methods are still present, but have been deprecated in favor of their corresponding <code>java.time</code> variant</li>; <li>At the next major version, these deprecated methods will be replaced with types from <code>java.time</code> and the <code>java.time</code> variant methods will be deprecated</li>; </ol>; </li>; <li>; <p><code>com.google.cloud.storage.Storage</code> now extends <code>java.lang.AutoClosable</code> thereby allowing it to be used in a try-with-resource block.</p>; <ol>; <li>When using gRPC transport be sure to call <code>Storage#close()</code> when complete so it can clean up the gRPC middleware and resources.</li>; <li>When using HTTP transport calling <code>Storage#close()</code> will gracefully no-op, allowing for the same style of use regardless of transport.</li>; </ol>; </li>; </ol>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/googleapis/java-storage/commit/48bb8ae1cd1362f44a94132c4903fb185b767728""><code>48bb8ae</code></a> chore(main): release 2.15.0 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1746"">#1746</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/67db4c4de60e49825c843afebd08ef0ac47e2b0d""><code>67db4c4</code></a> chore(java): update dependencies in java req",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12456:10909,extend,extends,10909,https://hail.is,https://github.com/hail-is/hail/pull/12456,1,['extend'],['extends']
Modifiability,OrderBy => KeyBy rewrite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4367:17,rewrite,rewrite,17,https://hail.is,https://github.com/hail-is/hail/pull/4367,1,['rewrite'],['rewrite']
Modifiability,Other changes:. - stop supporting python 2; - remove support for 0.1; - formatting. Coming soon to a PR near you:. - use google client library instead of `gsutil cat` (huge speedup); - upload configuration in deploy; include paths in hailctl package,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6136:192,config,configuration,192,https://hail.is,https://github.com/hail-is/hail/pull/6136,1,['config'],['configuration']
Modifiability,"Other things to add in separate PRs:; - logging; - concept of a ResourceDirectory where you want to copy the files in/out from a directory; - change the temp dir to be per task; - Support environment variables, cpu, memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-453621583:200,variab,variables,200,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453621583,1,['variab'],['variables']
Modifiability,"Our CI service should really be logging a JSON format the way batch does. Easy to change, need to use configure logging in Gear.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6701:102,config,configure,102,https://hail.is,https://github.com/hail-is/hail/issues/6701,1,['config'],['configure']
Modifiability,"Our current HTML display of tables flattens the table and concatenates the field; names to produce table headers. This leads to long, unreadable headers. This change reproduces the nesting structure of the types with several table; header layers. The essential activity is to convert:. ```; bing; foo:; bar:; baz; quux; fizzle:; fazz:; fazz1; fazz2; fezz; quork; bang; ```. into. ```; foo; -------------------------------; fizzle; ----------------; bar fazz; -------- -----------; bing baz quux fazz1 fazz2 fezz quork bang; ```. The bottom layer are the names of the leaves of this tree. Working from the; bottom, a name appears when the row corresponds to that name's tree height. For; this reason `bar` appears lower than `fizzle`. This frustrates finding peer; fields. However, I prefer it. I think I have some sense of visual gravity that; wants bar to fall down. Anyway, I implement this with some html grunginess in `Table._Show` and a new; class named `PlacementTree`. We construct a `PlacementTree` from a type. It is a; tree whose internal and leaf nodes contain a name, width, and height. It has a; method `to_grid` which converts it to an HTML-table-like structure with ""spacer""; elements. Our above example looks like:. ```python3; [[(None, 1), ('foo', 6), (None, 1)],; [(None, 1), (None, 2), ('fizzle', 3), (None, 1), (None, 1)],; [(None, 1), ('bar', 2), ('fazz', 2), (None, 1) (None, 1), (None, 1)],; [('bing', 1), ('baz', 1), ('quux', 1), ('fazz1', 1), ('fazz2', 1), ('fezz', 1) ('quork', 1), ('bang', 1)]]; ```. The code in `Table._Show` converts this to HTML table rows that looks like:; ```html; <tr><td></td><td colspan=""6"">foo</td><td></td></tr>; <tr><td></td><td colspan=""2""></td><td colspan=3>fizzle</td><td></td><td></td></tr>; <tr><td></td><td colspan=""2"">bar</td><td colspan=2>fazz</td><td></td><td></td><td></td></tr>; <tr><td>bing</td><td>baz</td><td>quux</td><td>fazz1</td><td>fazz2</td><td>fezz</td><td>quork</td><td>bang</td></tr>; ```. Which looks like:. <table>; <tr><t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8811:239,layers,layers,239,https://hail.is,https://github.com/hail-is/hail/pull/8811,1,['layers'],['layers']
Modifiability,"Our team is currently trying to run kinship analysis with [king()](https://hail.is/docs/0.2/methods/relatedness.html#hail.methods.king) on just under 110k samples. We have run this successfully in the past on 10k samples using a google cloud cluster with the following configuration. ```; hailctl dataproc start cluster --vep GRCh38 \; 	--requester-pays-allow-annotation-db \; 	--packages gnomad --requester-pays-allow-buckets gnomad-public-requester-pays \; 	--master-machine-type=n1-highmem-8 --worker-machine-type=n1-highmem-8 \; 	--num-workers=300	--num-secondary-workers=0 \; 	--worker-boot-disk-size=1000 \; 	--properties=dataproc:dataproc.logging.stackdriver.enable=true,dataproc:dataproc.monitoring.stackdriver.enable=true; ```; We are currently receiving a spark error when using this cluster for our larger dataset. ```; [Stage 10:=====> (69 + 656) / 729]; raise err; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 98, in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); File ""/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1304, in __call__; File ""/opt/conda/default/lib/python3.8/site-packages/hail/backend/py4j_backend.py"", line 31, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.project-.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.gbsc-project.internal executor 3568): ExecutorLostFailure (executor 3568 e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:269,config,configuration,269,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['config'],['configuration']
Modifiability,Overwrite ssl-config-hail-root secret for dev,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10188:14,config,config-hail-root,14,https://hail.is,https://github.com/hail-is/hail/pull/10188,1,['config'],['config-hail-root']
Modifiability,"P data is not copied into the dataproc cluster, and when trying to run VEP I get the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:1809,Plugin,Plugins,1809,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,"PBaseStruct becomes an interface with only implementations that are re-parameterizations of its abstract methods. PStruct and PTuple inherit. PCanonicalStruct gets the PBaseStruct implementations, and PCanonicalTuple implements its concrete methods by calling PCanonicalStruct's.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7928:71,parameteriz,parameterizations,71,https://hail.is,https://github.com/hail-is/hail/issues/7928,2,"['inherit', 'parameteriz']","['inherit', 'parameterizations']"
Modifiability,PR.config has a different signature from the base class Code.config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6142:3,config,config,3,https://hail.is,https://github.com/hail-is/hail/issues/6142,2,['config'],['config']
Modifiability,Pandas only releases [breaking changes in major versions](https://pandas.pydata.org/docs/development/policies.html). It seems safe; to be flexible on patch version. Just today I had an issue where I ran `pip install pandas` to; upgrade from an old pandas version and I landed on 1.1.5.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9819:138,flexible,flexible,138,https://hail.is,https://github.com/hail-is/hail/pull/9819,1,['flexible'],['flexible']
Modifiability,Parameterise docker registry in the kaniko config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10444:43,config,config,43,https://hail.is,https://github.com/hail-is/hail/pull/10444,1,['config'],['config']
Modifiability,Parameterize spark version in gradle install commands,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/399:0,Parameteriz,Parameterize,0,https://hail.is,https://github.com/hail-is/hail/issues/399,1,['Parameteriz'],['Parameterize']
Modifiability,Parameterized the key type of TDict,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1323:0,Parameteriz,Parameterized,0,https://hail.is,https://github.com/hail-is/hail/pull/1323,1,['Parameteriz'],['Parameterized']
Modifiability,Part 1 of chipping away at config.mk. This puts the two make targets for building the vm image in GCP into a single script. It loads variables that used to come from config.mk from kubernetes. Added a convenience function to offer a confirmation prompt before running the script. Here's an example:. ```; (hailenv) dgoldste@wmce3-cb7 hail % $HAIL/batch/gcp-create-worker-image.sh; Building image with properties:; Version: 12; Project: hail-vdc; Zone: us-central1-a; Are you sure? [y/N] n; (hailenv) dgoldste@wmce3-cb7 hail %; ```. Tested by running with a high image version number (3010 to be precise),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11327:27,config,config,27,https://hail.is,https://github.com/hail-is/hail/pull/11327,3,"['config', 'variab']","['config', 'variables']"
Modifiability,"Part of a refactoring effort. I need to break the model that a `TNDArray` is represented by an underlying `TStruct` and `TArray`. This adds an `UnsafeNDArray` Java representation, rather than the old model which was just to use`UnsafeRow` like it was a struct. . cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9854:10,refactor,refactoring,10,https://hail.is,https://github.com/hail-is/hail/pull/9854,1,['refactor'],['refactoring']
Modifiability,Pass Spark config options through HailContext(),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1336:11,config,config,11,https://hail.is,https://github.com/hail-is/hail/issues/1336,1,['config'],['config']
Modifiability,"Performance is close, if slightly worse. Could be laptop load differences. Insignificant, this is a great balance. {""config"": {""cores"": 1, ""version"": ""0.2.28-42f5ab7d9617"", ""timestamp"": ""2019-12-04 19:20:11.757847"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [25.609369775000005, 25.694102771999994, 26.285334770000006]}]}",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7646#issuecomment-561908098:117,config,config,117,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561908098,1,['config'],['config']
Modifiability,Please keep in mind this PR is part of a much larger refactor of the code and this bad interface will go away.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6238#issuecomment-498324907:53,refactor,refactor,53,https://hail.is,https://github.com/hail-is/hail/pull/6238#issuecomment-498324907,1,['refactor'],['refactor']
Modifiability,Point vep check to new config and reference files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5315:23,config,config,23,https://hail.is,https://github.com/hail-is/hail/pull/5315,1,['config'],['config']
Modifiability,Points 2 and 3 in the description are still remaining. It should be safe to manually modify the global-config in `hail-vdc` but might want to just poke around CI afterward to check that the change is picked up successfully and that PRs are succeeding.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13545#issuecomment-1766805213:103,config,config,103,https://hail.is,https://github.com/hail-is/hail/issues/13545#issuecomment-1766805213,1,['config'],['config']
Modifiability,"Pretty sure this is all dead code because we don't currently speak TLS between the worker and internal gateway. I think what's ultimately the right approach here is to start sending the hail root cert to workers and have them mount that at a well known location to all containers, not have it be part of the job spec. Then we should be able to talk https to internal gateway. I'm not exactly sure what to do in this PR specifically. I want to get rid of this hard dependency on the ssl-config because; 1. Doesn't exist in terra as everything goes through the relay listener; 2. These ssl configs aren't used anyway; but I'd need to put in a bit more work to start sending the root cert to workers and add https to internal-gateway",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13187:486,config,config,486,https://hail.is,https://github.com/hail-is/hail/pull/13187,2,['config'],"['config', 'configs']"
Modifiability,"Prevents us from being able to concatenate a bunch of NDArrays using a map over an expression. Note that np.array(hl.eval(a)) would work. ```python; >>> a = hl.literal([hl.nd.array((1,2,3)), hl.nd.array((4,5,6))]) ; >>> hl.eval(a); [array([1, 2, 3], dtype=int32), array([4, 5, 6], dtype=int32)]. >>> hl.eval(hl.nd.array(a)); Java stack trace:; java.lang.NullPointerException: null; 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:305); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:768,Rewrite,RewriteBottomUp,768,https://hail.is,https://github.com/hail-is/hail/issues/9128,8,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,"Previously, I read the tmpdir, billingProject, and remoteTmpdir arguments; separately for each comand in its branch of the switch. Then, I called; a method for that command, passing the three aformentioned arguments. The; method then constructs an execute context. Every method constructed the; context in the exact same way. This was just naughty copy paste job on my; part. I rectify that here by abstracting over the creation of the execute; context and moving the reading of the three execute context relevant; parameters to outside of the switch. I did not change any functionality. This strictly a refactor.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11646:604,refactor,refactor,604,https://hail.is,https://github.com/hail-is/hail/pull/11646,1,['refactor'],['refactor']
Modifiability,"Previously, the BlockMatrix IR had nodes for reading and writing that only covered the BlockMatrix part file format. Implemented readers and writers for both native and binary file formats (compatible with numpy) refactored read/write nodes, and implemented `tofile` and `fromfile` BlockMatrix methods in terms of the IR. Also hardcoded the front end default block size so now tests running IO/basic algebra should be able to run on the service.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5392:213,refactor,refactored,213,https://hail.is,https://github.com/hail-is/hail/pull/5392,1,['refactor'],['refactored']
Modifiability,"Prior to this PR, when someone specified they'd like to color a plot by a discrete variable, we used a predetermined list of 10 colors to assign colors. If something used more than 10 colors, it'd wrap around. . This PR changes `scale_color_discrete` to be an alias for `scale_color_hue`, as it is in ggplot. `scale_color_hue` works by sampling evenly spaced points around a color wheel to create a set of maximally distant colors. . The old behavior is now achievable by using `scale_color_manual`, which takes in a list of colors and assigns colors based on that list.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11613:83,variab,variable,83,https://hail.is,https://github.com/hail-is/hail/pull/11613,1,['variab'],['variable']
Modifiability,"Prometheus storage was only 10Gb, so it filled up after 14 days. By default, Prometheus deletes logs after 15 days. I increased the storage size to 50Gb accordingly. I also decided to use this opportunity to switch Prometheus from a Deployment to a StatefulSet. This meant turning the PersistentVolume for PrometheusStorage in the monitoring.yaml file to a PersistentVolumeClaim within the Prometheus StatefulSet spec. However, the claim was configured to mount at the same location as the previous PersistentVolume, and I did not first delete the old PersistentVolume. As a result, the new 50Gb disk was not initially allocated. I resolved this by deleting the StatefulSet and the old PersistentVolume, then redeploying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6483#issuecomment-505884681:442,config,configured,442,https://hail.is,https://github.com/hail-is/hail/issues/6483#issuecomment-505884681,1,['config'],['configured']
Modifiability,Provide option to skip log4j configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8571:29,config,configuration,29,https://hail.is,https://github.com/hail-is/hail/pull/8571,1,['config'],['configuration']
Modifiability,"Put **Notes** after example. Suggested rewrite:. ""This method registers new global annotations in the VDS. These annotations can then be accessed through expressions in downstream operations. The Hail data type must be provided and match the type of the Python object.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1501#issuecomment-284859085:39,rewrite,rewrite,39,https://hail.is,https://github.com/hail-is/hail/pull/1501#issuecomment-284859085,1,['rewrite'],['rewrite']
Modifiability,"Python CLI tools like `hailctl` suffer from slow startup times, which infuriate me. This is in part because the first thing that happens is python has to recursively load all imported packages, since imports are traditionally done at the top-level. Very conveniently, setting the `PYTHONPROFILEIMPORTTIME` environment variable will cause python to emit a profile to stderr, which you can visualize with tools like [tuna](https://github.com/nschloe/tuna). So running. ```; PYTHONPROFILEIMPORTTIME=1 hailctl dev config show 2> profile.log; tuna profile.log; ```. gave me this. <img width=""1576"" alt=""Screen Shot 2022-01-28 at 2 58 28 PM"" src=""https://user-images.githubusercontent.com/24440116/151614364-d57a4478-1516-4397-ac72-4f2b9c6c081b.png"">. showing that importing `aiohttp` is responsible for half the time it takes me to run `hailctl dev config show`, which is literally just printing a local file!! There's no reason this shouldn't be instantaneous, but reducing it to ~300ms, which this change did, is fine enough for me for now. Generally people don't care about import time because most applications are long-lived and what does a few seconds matter, so `pylint` by default wants us to put imports at the top level. I would say this is a valid exception.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11293:318,variab,variable,318,https://hail.is,https://github.com/hail-is/hail/pull/11293,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"Python integration tests often fail waiting to allocate highmem instances for worker jobs.; Since we control both APIs, it seems reasonable to move the testing burdon for vm allocation onto batch and use contract testing on the query driver side. These contract tests cover:; - uploading the the ServiceBackendRPConfig to remote storage in python; - reading that config and forwarding the relevant sections to the batch service in scala. Admittedly these are fairly busy tests and make bare a lot of lower-level implementation details. While I believe these tests are good to have, they perhaps don't warrant the time investment to properly refactor for cleaner mocking. Should details of the main implementation change, these will likely break. I've made tweaks to the python unittest annotations for backend test filtering. The old system skipped tests after all required fixtures had been acquired. Using `@pytest.mark.{feature}` allows us to exclude tests before fixtures are setup as well as add additional setup/teardown code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14512:363,config,config,363,https://hail.is,https://github.com/hail-is/hail/pull/14512,2,"['config', 'refactor']","['config', 'refactor']"
Modifiability,REMINDER TO SELF: Change the database configuration before merging in both GCP and Azure!!!!!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12813#issuecomment-1509030448:38,config,configuration,38,https://hail.is,https://github.com/hail-is/hail/pull/12813#issuecomment-1509030448,1,['config'],['configuration']
Modifiability,"REPORT_THRESHOLD: 512.0M allocated (111.9M blocks / 400.1M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:42.730 root: INFO: RegionPool: REPORT_THRESHOLD: 1.2G allocated (439.1M blocks / 781.5M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1""}, 'service_backend_debug_info': {'batch_attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'driver_cores': None, 'driver_memory': None, ...}} or 'batch.worker.jvm_entryway_protocol.EndOfStream' in {'batch_status': {'attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'closed': True, 'complete': True, ...}, 'job_status': {'attributes': {'name': 'driver'}, 'batch_id': 6627669, 'billing_project': 'test', 'cost': 0.0015413897092729028, ...}, 'log': {'main': ""2022-11-15 20:30:18.004 Tokens: INFO: tokens found for namespaces {default}\n2022-11-15 20:30:18.004 tls: INFO: ssl config file found at /batch/2bbb233e4e3c4a96bbffb515019daac9/secrets/ssl-config/ssl-config.json\n2022-11-15 20:30:18.006 GoogleStorageFS$: INFO: Initializing google storage client from service account key\n2022-11-15 20:30:18.114 root: INFO: RegionPool: initialized for thread 8: pool-1-thread-1\n2022-11-15 20:30:18.114 ServiceBackend$: INFO: executing: cEPZ5IV9gUtSnCiAiHXOPs None\n2022-11-15 20:30:18.127 root: INFO: optimize optimize: darrayLowerer, initial IR: before: IR size 17: \n(Let __rng_state\n (RNGStateLiteral (0 0 0 0))\n (MakeTuple (0)\n (TableAggregate\n (TableMapRows\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (InsertFields\n (SelectFields () (SelectFields (idx) (Ref row)))\n None\n (idx (GetField idx (Ref row)))))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row)))))))))\n2022-11-15 20:30:18.146 root: INFO: optimize optimize: darrayLowerer, initial IR: after: IR size 8:\n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:64985,config,config,64985,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,3,['config'],['config']
Modifiability,RR: https://github.com/hail-is/hail/issues/13045; RR: https://github.com/hail-is/hail/issues/13046 ; Support symmetric comparison of structs and struct expressions.; Provide better error messages when attempting to construct literals from expressions with free variables.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13226:261,variab,variables,261,https://hail.is,https://github.com/hail-is/hail/pull/13226,1,['variab'],['variables']
Modifiability,RR: https://github.com/hail-is/hail/issues/13261. Grouping asserts of distributed `BlockMatrix` queries via `BatchAssert` lead to repeated timeout failures during tests that used the batch-service backend.; This change removes all `BatchAssert`s from `test_linalg.py`. It uses `pytest.mark.parameterize` to gain parallelism in test execution from the test driver.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13348:290,parameteriz,parameterize,290,https://hail.is,https://github.com/hail-is/hail/pull/13348,1,['parameteriz'],['parameterize']
Modifiability,"Rate limiting is a bit too restrictive methinks. That said, there are a lot of deadlocks when the rate limits kick in. Perhaps worth diving more deeply into the deadlocks at some point. The critical path service backend test is #35. Here are its slowest ones. I think we should probably checkpoint the import VCF, but, regardless, pc_relate just needs to be made faster. I know how to double the speed of PCA. It's in a dead branch of mine. Patrick will incorporate those ideas into his rewrite using the new math. If #35 was the same speed as the next slowest, we'd save 3 minutes. I think we can save ~7 minutes by cutting all these slow tests down so that the distribution of runtimes is more uniform. ```; 256.68s call hail/methods/relatedness/test_pc_relate.py::test_pc_relate_against_R_truth; 178.28s call hail/methods/test_pca.py::test_spectra_2[triplet0]; 102.60s call hail/vds/test_vds.py::test_truncate_reference_blocks; 82.86s call hail/backend/test_service_backend.py::test_tiny_driver_has_tiny_memory; ```. f1ac37dbeb3625cbf91f1f9df5399f3723843029 (40 minutes) (https://ci.hail.is/batches/7484187):. <img width=""2032"" alt=""Screen Shot 2023-05-25 at 12 01 55"" src=""https://github.com/hail-is/hail/assets/106194/902a0624-46a0-4beb-ae03-6c419350ca41"">; <img width=""542"" alt=""Screen Shot 2023-05-25 at 12 01 28"" src=""https://github.com/hail-is/hail/assets/106194/3cfa366d-5719-428a-9f4f-5bd07caaf6ca"">; <img width=""521"" alt=""Screen Shot 2023-05-25 at 12 01 07"" src=""https://github.com/hail-is/hail/assets/106194/da778828-9f9e-46cc-b574-68b9835e6589"">; <img width=""522"" alt=""Screen Shot 2023-05-25 at 12 01 03"" src=""https://github.com/hail-is/hail/assets/106194/3074a6f9-06d5-487e-941c-995b47177181"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1563163790:487,rewrite,rewrite,487,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1563163790,1,['rewrite'],['rewrite']
Modifiability,"Rather than letting Breeze throw a SingularMatrixException, we should check for dependence and give an informative error message. The most common mistakes leading to dependence are accidentally including the same covariate twice (identical columns) or encoding a categorical variable with n categories using n rather than n - 1 covariates (since the model has an intercept term, this creates linear relation. We might also consider automating this encoding).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1156:275,variab,variable,275,https://hail.is,https://github.com/hail-is/hail/issues/1156,1,['variab'],['variable']
Modifiability,"Rather than linking out to Numpy, I added an identity function. In a followup PR I will add the ""k"" argument to `eye` (I have no need for it now, feel free to PR if you want). Until then. `eye` differs from `identity` in that it can generate arbitrary rectangular matrices. Adding ""k"" will extend that to enable non-main diagonals",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9105#issuecomment-661129687:290,extend,extend,290,https://hail.is,https://github.com/hail-is/hail/pull/9105#issuecomment-661129687,1,['extend'],['extend']
Modifiability,"Re-implement export vcf in generated code. There is a fair amount of 'duplicated' code here between table export; and vcf export, however, I belive this to be fine. We can always; refactor VCFPartitionWriter to be a subclass of SimplePartitionWriter,; but that would require a little special casing as VCF export needs; access to the column values and SimplePartitionWriter assumes such; a thing is not necessary. As far as VCF export itself, we simply duplicate the logic present in; ExportVCF but with generated code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11450:180,refactor,refactor,180,https://hail.is,https://github.com/hail-is/hail/pull/11450,1,['refactor'],['refactor']
Modifiability,"Re: your review @danking . We can make the HailContext available on the workers. As far as I can tell, we don't right now because we would need to serialize all the values of HailContext that aren't serializable, broadcast it, and change get to grab the broadcasted value. I could do that. It probably wouldn't take me that long, but this change reverts TabixReader to a behavior that it had during development due to Tim's concern that the hadoop configuration is not serializable. We thought the original version would be okay because TabixReader was only ever constructed on the driver. We were wrong, and considering that we intend to use this to read hundreds of thousands of files at a time, the parallelization is probably a good thing. This change fixes the bug I had in a way consistent with much of our codebase, without making larger changes to how we handle HailContext.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5033#issuecomment-449490579:448,config,configuration,448,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449490579,1,['config'],['configuration']
Modifiability,"Ready for review. The regex is working, though not sure where to place it in our code base. To properly match against nonNumeric() with no variables, there must be no groups (logically!)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/261:139,variab,variables,139,https://hail.is,https://github.com/hail-is/hail/pull/261,1,['variab'],['variables']
Modifiability,"Ready to look at. . Abstracts file system functionality. We no longer pass around a Hadoop Configuration w/ implicit methods defined in RichHadoopConfiguration. Instead we define an abstract FS class (could be a trait as well) to serve as our file system interface, and provide one Hadoop implementation to maintain existing functionality. The PR has many lines, but should hopefully be relatively easy to follow; mostly involves renaming. . cc @cseed , thanks @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083:91,Config,Configuration,91,https://hail.is,https://github.com/hail-is/hail/pull/6083,1,['Config'],['Configuration']
Modifiability,"Recommended changes to doc strings (I had an issue making PR against yours in browser, so you can just cut and paste into your code if you agree with changes):. ```; registerMethod(""append"", (a: IndexedSeq[Any], b: Any) => a :+ b, ""Returns the result of adding the element `b` to the end of Array `a`."")(arrayHr(TTHr), TTHr, arrayHr(TTHr)); registerMethod(""extend"", (a: IndexedSeq[Any], b: IndexedSeq[Any]) => a ++ b, ""Returns the concatenation of Array `a` and Array `b`."")(arrayHr(TTHr), arrayHr(TTHr), arrayHr(TTHr)). registerMethod(""add"", (a: Set[Any], b: Any) => a + b, ""Returns the result of adding the element `b` to Set `a`."")(setHr(TTHr), TTHr, setHr(TTHr)); registerMethod(""union"", (a: Set[Any], b: Set[Any]) => a ++ b, ""Returns the union of Sets `a` and `b`."")(setHr(TTHr), setHr(TTHr), setHr(TTHr)); registerMethod(""intersection"", (a: Set[Any], b: Set[Any]) => a & b, ""Returns the intersection of Sets `a` and `b`."")(setHr(TTHr), setHr(TTHr), setHr(TTHr)); registerMethod(""difference"", (a: Set[Any], b: Set[Any]) => a &~ b, ""Returns the elements of Set `a` that are not in Set `b`."")(setHr(TTHr), setHr(TTHr), setHr(TTHr)); registerMethod(""issubset"", (a: Set[Any], b: Set[Any]) => a.subsetOf(b), ""Returns true if Set `a` is a subset of Set `b`."")(setHr(TTHr), setHr(TTHr), boolHr); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1491#issuecomment-284720278:357,extend,extend,357,https://hail.is,https://github.com/hail-is/hail/pull/1491#issuecomment-284720278,1,['extend'],['extend']
Modifiability,Reduce. Reuse. Refactor.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6170:15,Refactor,Refactor,15,https://hail.is,https://github.com/hail-is/hail/pull/6170,1,['Refactor'],['Refactor']
Modifiability,Refactor AggSignature,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3890:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/3890,1,['Refactor'],['Refactor']
Modifiability,Refactor Die to take a string IR child,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4845:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/4845,1,['Refactor'],['Refactor']
Modifiability,Refactor LoadVCF to use MatrixRead,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3840:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/3840,1,['Refactor'],['Refactor']
Modifiability,Refactor MatrixLiteral to delete unnecessary parameter,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3963:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/3963,1,['Refactor'],['Refactor']
Modifiability,Refactor MatrixRead and MatrixReader to be cleaner,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3926:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/3926,1,['Refactor'],['Refactor']
Modifiability,Refactor MatrixReader JSON serialization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3880:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/3880,1,['Refactor'],['Refactor']
Modifiability,Refactor MatrixWrite and use in Python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4864:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/4864,1,['Refactor'],['Refactor']
Modifiability,Refactor Python IR to reduce code duplication and complexity,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5465:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/5465,1,['Refactor'],['Refactor']
Modifiability,Refactor Python MatrixRead,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4918:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/4918,1,['Refactor'],['Refactor']
Modifiability,Refactor ResultOp to return a single element instead of a tuple.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10894:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/10894,1,['Refactor'],['Refactor']
Modifiability,Refactor SBaseStruct.isFieldMissing and dependent methods to `CodeBuilder => Value` style.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10893:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/10893,1,['Refactor'],['Refactor']
Modifiability,Refactor SeqOp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3850:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/3850,1,['Refactor'],['Refactor']
Modifiability,Refactor TableWrite to take a TableWriter,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5775:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/5775,1,['Refactor'],['Refactor']
Modifiability,Refactor _select methods in python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4042:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/4042,1,['Refactor'],['Refactor']
Modifiability,Refactor `_emitStream` to a `EmitCodeBuilder -> IEmitCode` signature.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9926:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/9926,1,['Refactor'],['Refactor']
Modifiability,Refactor annotateVariantsTable. Annotate global table. Added extra key table method. checkpoint. Checkpoint. checkpoint before tests. Some of the docs. Passing tests. Fixed tutorial. Fix tutorial styling. Finish rebase. Fix rebase errors. Fix tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1687:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/1687,2,['Refactor'],['Refactor']
Modifiability,Refactor code so that job private instances look up memory from a table rather than calculating from number of cores and mib memory per core,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14536:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/14536,1,['Refactor'],['Refactor']
Modifiability,Refactor export argument parser. Cleans up exportCass / exportSolr c…,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/443:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/443,1,['Refactor'],['Refactor']
Modifiability,Refactor expr code to VSM,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/932:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/932,1,['Refactor'],['Refactor']
Modifiability,Refactor genotype readers with more abstraction,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1673:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/issues/1673,1,['Refactor'],['Refactor']
Modifiability,Refactor rDeletionInsertion to rInsertionDeletion,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/851:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/851,1,['Refactor'],['Refactor']
Modifiability,Refactor reorder_columns to choose_cols,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3228:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/3228,1,['Refactor'],['Refactor']
Modifiability,Refactor resource usage,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2103:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/2103,1,['Refactor'],['Refactor']
Modifiability,"Refactor uniqueMinIndex and uniqueMaxIndex into argmin and argmax,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2990:0,Refactor,Refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/2990,1,['Refactor'],['Refactor']
Modifiability,Refactored OrderedRDD2 to take a RegionValueRDD,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2517:0,Refactor,Refactored,0,https://hail.is,https://github.com/hail-is/hail/pull/2517,1,['Refactor'],['Refactored']
Modifiability,"Refactored RDD[(Variant, Annotation, Iterable[Genotype])] to; RDD[(Variant, (Annotation, Iterable[Genotype]))]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/526:0,Refactor,Refactored,0,https://hail.is,https://github.com/hail-is/hail/pull/526,1,['Refactor'],['Refactored']
Modifiability,"Refactored VariantRecord to RecordDecoder, made genotype decoding; lazy. This allows us to get out fastKeys and the genotypes with the; same abstraction. Refactored multiple BGEN file handling to BgenLoader, where it should be. Added assertions that we silently relied on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/693:0,Refactor,Refactored,0,https://hail.is,https://github.com/hail-is/hail/pull/693,2,['Refactor'],['Refactored']
Modifiability,Refactored sampleQC to run against HTSGenotypeView,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2120:0,Refactor,Refactored,0,https://hail.is,https://github.com/hail-is/hail/pull/2120,1,['Refactor'],['Refactored']
Modifiability,Refactored table reader coercion and caching mechanism. ### What changed?. - Removed `shouldCacheQueryInfo` method from `Backend` class; - Introduced `CoercerCache` in `ExecuteContext`; - Refactored `LoweredTableReader.makeCoercer` to return a function instead of a class; - Removed local caching in `GenericTableValue` and `LoweredTableReader`; - Added `NoCaching` utility . ### Why make this change?. This change aims to optimize table reader coercion by:; - Centralizing caching logic in `ExecuteContext`; - Allowing more flexible caching strategies across different backend implementations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14696:0,Refactor,Refactored,0,https://hail.is,https://github.com/hail-is/hail/pull/14696,3,"['Refactor', 'flexible']","['Refactored', 'flexible']"
Modifiability,"Refactoring RVD interface, pt. 1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4392:0,Refactor,Refactoring,0,https://hail.is,https://github.com/hail-is/hail/pull/4392,1,['Refactor'],['Refactoring']
Modifiability,"Refactoring RVD interface, pt. 2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4395:0,Refactor,Refactoring,0,https://hail.is,https://github.com/hail-is/hail/pull/4395,1,['Refactor'],['Refactoring']
Modifiability,"Refactoring RVD interface, pt. 3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4398:0,Refactor,Refactoring,0,https://hail.is,https://github.com/hail-is/hail/pull/4398,1,['Refactor'],['Refactoring']
Modifiability,"Refactoring RVD interface, pt. 4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4407:0,Refactor,Refactoring,0,https://hail.is,https://github.com/hail-is/hail/pull/4407,1,['Refactor'],['Refactoring']
Modifiability,"Refactoring RVD interface, pt. 5",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4409:0,Refactor,Refactoring,0,https://hail.is,https://github.com/hail-is/hail/pull/4409,1,['Refactor'],['Refactoring']
Modifiability,"Refactoring RVD interface, pt. 6",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4412:0,Refactor,Refactoring,0,https://hail.is,https://github.com/hail-is/hail/pull/4412,1,['Refactor'],['Refactoring']
Modifiability,"Refactors `Bindings` to return an object encoding the change to the environment (any new bindings, whether the agg/scan env is promoted, etc). This allows the deletion of `SegregatedBindingEnv`. Follow up work will use this to replace the other specializations of `GenericBindingEnv`, and to greatly simplify compiler passes, such as `NormalizeNames` and `PruneDeadFields`, which currently need to redundantly encode the binding structure of every node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14496:0,Refactor,Refactors,0,https://hail.is,https://github.com/hail-is/hail/pull/14496,1,['Refactor'],['Refactors']
Modifiability,"Refactors `_blanczos_pca` into reusable and composable pieces, and uses those pieces to implement spectral moments estimators. # Krylov factorization; The core iteration of `_blanczos_pca` is factored out into `_krylov_factorization`. `_krylov_factorization(A, V0, p)` takes a matrix `A` (represented as a table of ndarrays) and a starting block `V0` (a local ndarray), and computes matrices (for now local) `U`, `R`, and `V`, such that:; * `U` and `V` are orthonormal matrices (i.e. `U'U = V'V = I`); * the columns of `V` are a basis for the block Krylov subspace `K_p(A'A, V_0)`, where `K_p(X, Y) = span(Y, XY, ... X^pY)`; * `UR = AV`, and hence `U` is a basis for the block Krylov subspace `A K_p(A'A, V_0) = K_p(AA', AV_0)`; * `V` is an extension of `V_0`, i.e. `V = hcat(V_0, ...)`; * `R` is upper triangular. # Reduced SVD; From a Krylov factorization, a reduced SVD can be easily computed: If `R = U_1 S V'_1` is a full SVD of `R` (which is small and easily computable), then `(U U_1[:, :k]) S[:k, :k] (V V_1[:, :k])'` is a reduced SVD of `A`. This is implemented in `KrylovFactorization.reduced_svd`. # Spectral moments; We can also easily compute estimates of spectral moments, i.e. moments of the set of all eigenvalues of `A'A`. The estimator exploits the following key facts:; * If `v` is a random vector of independent entries with mean 0, std. dev. 1 (equivalently `E(v) = 0`, `E(vv') = I`), then `E(v'Xv) = tr(X)`; * `tr(X)` equals the sum of the eigenvalues of `X`, `∑_i 𝜆_i`. More generally, if `f` is any matrix function, `tr(f(X)) = ∑_i f(𝜆_i)`.; * If `w` is a unit-norm vector, and `UR = AV` is the factorization `_krylov_factorization(A, w, p)`, then `w' f(A'A) w` is well-approximated by `w' f(VV'A'AVV') w = w'V f(R'U'UR) V'w = w'V f(R'R) V'w`, and is exact if `f` is a degree `2p+1` polynomial. Moreover, since `w` is the first column of `V`, i.e. `w = Ve_1`, the above further simplifies `w'V f(R'R) V'w = e'_1 f(R'R) e_1`. Finally, if `R = U_1 S V'_1` is an SVD, this reduces",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11045:0,Refactor,Refactors,0,https://hail.is,https://github.com/hail-is/hail/pull/11045,1,['Refactor'],['Refactors']
Modifiability,"Regarding the X-Forwarded-Host. This is why the current setup works: ; (from express request lib); https://github.com/iconoeugen/express/blob/master/lib/request.js#L432. If X-Forwarded-Host is not set, the Host header is checked. So while not strictly necessary, until Ghost stops recommending X-Forwarded-Host, I would keep it, except with a slight change. Also, I think our use of Host is slightly wrong. From what I can tell, Host really should be the target/internal domain, and X-Forwarded-Host the host of the original recipient. By having $updated_host take either of these values (Host when X-Forwarded-Host isn't set, else X-Forwarded-Host) we slightly confuse the issue. This can also lead to bugs, when X-Forwarded-Host contains multiple values (which is in spec). Here is Kubernetes struggling with this issue https://github.com/kubernetes/ingress-nginx/issues/2463. So I would use the following config for Host:. ```; proxy_set_header Host $http_host; proxy_set_header X-Forwarded-Host $http_x_forwarded_host; ```. However, I'm also ok with you keeping it as you have it, because that's what is done elsewhere, which will increase the likelihood that we find that solution insufficient if it truly isn't.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548040720:908,config,config,908,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548040720,1,['config'],['config']
Modifiability,"Regex isn't used to detect the optional slash because doing so doesn't really save any lines of code, because nginx does not directly allow proxy_pass with a trailing slash inside of regex-containing locations, without a rewrite rule (and that rewrite rule costs 1 line). https://serverfault.com/questions/649151/nginx-location-regex-doesnt-work-with-proxy-pass",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-541331762:221,rewrite,rewrite,221,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541331762,2,['rewrite'],['rewrite']
Modifiability,RegionValue doesn't extend Serializable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3305:20,extend,extend,20,https://hail.is,https://github.com/hail-is/hail/pull/3305,1,['extend'],['extend']
Modifiability,"Remaining is that found in Etypes, in the _buildSkip function. This is slightly tricky, because there is a place in the code where there is no corresponding PType, and the solution to fix that is a bit involved, or if straightforward, beyond my current understanding of ETypes. I made an issue here: https://github.com/hail-is/hail/issues/7701. Stacked on https://github.com/hail-is/hail/pull/7687. edit: I removed the ETypes issue, by creating a packBitsToBytes function on UnsafeUtils. We may not want this change however, because I think array packing may needs to be the same as the array implementation (I think readBytes fills the allocated memory with the InputBuffer's encoded missingness data, which needs same number of bytes as what is encoded), in which case that coupling becomes less clear if the utility function is on UnsafeUtils. I could move it back to PContainer, or may _buildSkip take a ptype. . There are other places where (n + 7) >>> 3 are used, so this seems pretty general, hence UnsafeUtils (where we have some other bitwise ops, happy to move elsewhere). PTuple is one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7702:776,coupling,coupling,776,https://hail.is,https://github.com/hail-is/hail/pull/7702,1,['coupling'],['coupling']
Modifiability,Remove annotation methods from PType:. - ordering (ExtendedOrdering); - str; - typeCheck; - valuesSimilar; - query; - queryTyped; - gen,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4280:51,Extend,ExtendedOrdering,51,https://hail.is,https://github.com/hail-is/hail/pull/4280,1,['Extend'],['ExtendedOrdering']
Modifiability,"Remove some (now) unnecessary local variable initializations. newEmit{Local, Field}: don't store missingness variable for required types. Split up zip cases assert same length and extend na. Otherwise, the same length code ended up assigning a missing element to a required field.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8317:36,variab,variable,36,https://hail.is,https://github.com/hail-is/hail/pull/8317,3,"['extend', 'variab']","['extend', 'variable']"
Modifiability,"Remove the setup of Jupyter/JupyterLab from hailctl to enable use of Dataproc's [Component Gateway](https://cloud.google.com/dataproc/docs/concepts/accessing/dataproc-gateways) feature, which eliminates the need to use an ssh tunnel to reach the various web UIs on the Dataproc cluster. To test this pull request:; 1. run `hailctl dataproc start` as usual, but add parameters `--enable-component-gateway --optional-components JUPYTER --dry-run` to generate the `gcloud dataproc clusters create` command that will setup JupyterLab and eliminate the need for an ssh tunnel. For example:; ```; hailctl dataproc start my-cluster-name \; --region us-central1 \; --enable-component-gateway \; --optional-components JUPYTER \; --bucket name-of-my-staging-gcs-bucket-where-notebook-files-will-live \; --temp-bucket name-of-my-gcs-bucket-with-a-lifecycle-rule-to-autodelete-cruft-after-two-weeks \; --max-idle 60m \; --dry-run; ```; 2. In the generated `gcloud dataproc clusters create` command, replace the value of `--initialization-actions` with the path of the GCS location to the script in this pull request. Also replace the value of `--temp-bucket`, since hailctl appears to stomp on the user specified value. Then run the command to create your cluster with component gateway enabled.; 3. To obtain the URL to JupyterLab, run `gcloud dataproc clusters describe my-cluster-name --region=us-central1 --format=""yaml(config.endpointConfig.httpPorts)""`; 4. Run Hail notebooks to test the setup of JupyterLab provided by Dataproc!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12788:1412,config,config,1412,https://hail.is,https://github.com/hail-is/hail/pull/12788,1,['config'],['config']
Modifiability,"Renamed and moved `datasets/annotation_db.json` config file to `hail/experimental/datasets.json` and modified urls in `dataset[path]` to use a region parameter to load datasets from bucket in the appropriate region. Modified `load_datasets()` function to no longer use the `config_file` parameter, and to require user to specify `region` parameter. The checked-in `hail/experimental/datasets.json` file will now be used as the config file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9411:48,config,config,48,https://hail.is,https://github.com/hail-is/hail/pull/9411,2,['config'],['config']
Modifiability,Replaced VariantDataset ..._to_pandas with ..._keytable.; Added expand_types and flatten to KeyTable. These probably need to be more configurable but are a start.; Added KeyTable.toDF. This allows KeyTables to be easily written to Parquet.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1157:133,config,configurable,133,https://hail.is,https://github.com/hail-is/hail/pull/1157,1,['config'],['configurable']
Modifiability,"Resolves issue #763.; ### Simple Types. The Function Registry distinguishes between fields and functions because they were distinguished in the existing `AST.scala`. Moreover, for unary functions, there are registration methods for both pure functions and computations in the `Option` monad. Registration requires only a name and an implementation. Unfortunately, the Scala compiler fails to infer the type parameters from an expression like `_.isHomRef`. . ``` scala; registerOptionField(""dosage"", { (x: Genotype) => x.dosage.map(a => a: IndexedSeq[Double]) }); registerField(""isHomRef"", { (x: Genotype) => x.isHomRef }); ```. ``` scala; register(""Variant"", { (x: String) =>; val Array(chr, pos, ref, alts) = x.split("":""); Variant(chr, pos.toInt, ref, alts.split("","")); }); register(""Variant"", { (x: String, y: Int, z: String, a: String) => Variant(x, y, z, a) }); ```. The `HailRep` type class associates Scala types with Hail expression types. For example, the function registry knows that `Variant` returns a `TVariant` because of this implicit:. ``` scala; implicit object variantHr extends HailRep[Variant] {; def typ = TVariant; }; ```; ### Polymorphic Types. I don't have an answer for the various kinds of polymorphism present in the Hail expression language. There is unbounded polymorphism:. ``` scala; case (t: TArray, ""length"") => TInt; ```. as well as bounded polymorphism:. ``` scala; case (""pow"", _) => TDouble; args.map(_.`type`) match {; case Array(a: TNumeric, b: TNumeric) => TDouble; // ...; }; ```. Both of these are still handled by explicit case matching.; ### Struct Types. Functions returning structs can use `registerAnn` to specifically provide a return type. ``` scala; registerAnn(""foo"", TStruct((""bar"", TDouble)), { (x: Int) => Annotation(x / 2.0) } ; ```. In general, the `register` `HailRep` implicits can be overridden as well, but this case is common enough to merit a concise alternative.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/807:1088,extend,extends,1088,https://hail.is,https://github.com/hail-is/hail/pull/807,5,"['Polymorphi', 'extend', 'polymorphi']","['Polymorphic', 'extends', 'polymorphism']"
Modifiability,"Resource groups are permitted to use dashes, underscores, uppercase letters,; and probably other characters not permitted in storage account names. This; PR cahanges `bootstrap.sh` to:. 1. Ignore invalid characters in the resouce group. 2. Ensure (via randomness) that the generated name is unique. 3. Do not try to create a new storage account if `backend-config.tfvars` exists. I lightly tested this. Here is an example of how it sanitizes a resource group name:. ```; RESOURCE_GROUP=bu__ild-batch-worker-i32mage; possibly_invalid_storage_account_name=""$(cat /dev/urandom | LC_ALL=C tr -dc 0-9 | head -c 4)${RESOURCE_GROUP}""; STORAGE_ACCOUNT_NAME=$(LC_ALL=C tr -dc a-z0-9 <<< ""${possibly_invalid_storage_account_name}"" | head -c 24); echo $STORAGE_ACCOUNT_NAME; 7241buildbatchworkeri32m; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11313:357,config,config,357,https://hail.is,https://github.com/hail-is/hail/pull/11313,1,['config'],['config']
Modifiability,"Revert ""[batch] Mount worker deploy config instead of using k8s secret""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13209:36,config,config,36,https://hail.is,https://github.com/hail-is/hail/pull/13209,1,['config'],['config']
Modifiability,"Revert ""[combiner] use a single variable rather than a map for merge …",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5597:32,variab,variable,32,https://hail.is,https://github.com/hail-is/hail/pull/5597,1,['variab'],['variable']
Modifiability,"Reverted the interface for `select`, `drop`, and `key_by` to requiring a str or list of str rather than varargs. This is because having default values specified by a kwarg was interacting poorly with the varargs. Python3 supports this better. https://stackoverflow.com/questions/13821877/function-call-with-named-unnamed-and-variable-arguments-in-python",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2279#issuecomment-334795157:325,variab,variable-arguments-in-python,325,https://hail.is,https://github.com/hail-is/hail/pull/2279#issuecomment-334795157,1,['variab'],['variable-arguments-in-python']
Modifiability,Rewrite IR array sorting to use scala's sortWith,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3719:0,Rewrite,Rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/3719,1,['Rewrite'],['Rewrite']
Modifiability,"Rewrite VariantQC in Python, support multiallelics",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3629:0,Rewrite,Rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/3629,1,['Rewrite'],['Rewrite']
Modifiability,"Rewrite invocations of `hl.cond()` to `hl.if_else()`, `hl.null()` to `hl.missing()`, and `hl.zip_with_index()` to `hl.enumerate()`. Very minor, but a few of these appear in our test logs (and probably yours as well), which makes for noise when you're tracking down other problems in the logs:. ```; hail/methods/misc.py:437: DeprecationWarning: Call to deprecated function (or staticmethod) cond. (Replaced by hl.if_else) -- Deprecated since version 0.2.59.; hail/vds/methods.py:79: DeprecationWarning: Call to deprecated function (or staticmethod) zip_with_index. (Replaced by hl.enumerate) -- Deprecated since version 0.2.56.; hail/vds/methods.py:75: DeprecationWarning: Call to deprecated function (or staticmethod) null. (Replaced by hl.missing) -- Deprecated since version 0.2.62.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13349:0,Rewrite,Rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/13349,1,['Rewrite'],['Rewrite']
Modifiability,Rewrite the staged aggregator interface to:; - allow primitive values in aggregator without associated region; - create Scala Region instance once and reuse by setting different underlying native Region object to avoid overhead of object creation. Also includes some interface changes to AggregatorState (formerly RVAState) to clean up the region dependencies.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6652:0,Rewrite,Rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/6652,1,['Rewrite'],['Rewrite']
Modifiability,"Right now the Grafana is exposed as a k8s service speaking http with only the grafana auth. This puts an nginx sidecar in front of Grafana to bring TLS all the way through the the Grafana pod and perform dev authentication. This required adding an api endpoint to auth that can verify a connection based on the session and not an Authorization header. Other services like `router-resolver` have gotten away with not having this since they construct the Authorization header in python before hitting the `userinfo` endpoint, but this seems like a straightforward addition that will make it easier for internal authentication such as this case. This does another deviant thing which is using a `runImage` step to template the nginx config instead of templating inside the container at container start time (like router and site currently do). It is a little janky, because there are essentially two jinja passes, one in CI to render the shell script for the job, and then the jinja line in the job itself to render the nginx config. But this looks to be the most straightforward way I could figure out without adding another `build.py` Step type and even in that case it would have to be some sort of no-op job.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10139:730,config,config,730,https://hail.is,https://github.com/hail-is/hail/pull/10139,2,['config'],['config']
Modifiability,"Right now this spins up K8s, a database, some of the networking stuff,; and creates a default/global-config secret that includes the; information stored in $HAIL/config.mk. This isn't used yet and will; probably change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9713:101,config,config,101,https://hail.is,https://github.com/hail-is/hail/pull/9713,2,['config'],['config']
Modifiability,"Right. Overlapping genes is handled by the `single_key=False` case that I used in my extended 0.1 doc example for linreg burden, but you said you were thinking to remove that case and use explode so I didn't think it worth using that exact example. People will want to use explode, groupBy and linreg / logreg together to test genes for association, so that may be a nice example in groupVariantsBy. So I'm fine with holding off on more examples until this and explode are merged, and then we can revisit together.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2497#issuecomment-348369590:85,extend,extended,85,https://hail.is,https://github.com/hail-is/hail/pull/2497#issuecomment-348369590,1,['extend'],['extended']
Modifiability,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:3184,config,configured,3184,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,4,['config'],['configured']
Modifiability,Save it to a variable and pass it to standard input using a heredoc; instead of a gnarly command line argument. This still outputs the; config to the logs/job output since the variable assignment expression; will be printed with `set -x`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9533:13,variab,variable,13,https://hail.is,https://github.com/hail-is/hail/pull/9533,3,"['config', 'variab']","['config', 'variable']"
Modifiability,Scorecard should use a readiness probe to prevent traffic from being sent to scorecard before it has fully updated itself and is ready to serve traffic. https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/. An HTTP readiness probe that hits `GET /` should be sufficient.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6648:186,config,configure-pod-container,186,https://hail.is,https://github.com/hail-is/hail/issues/6648,2,['config'],"['configure-liveness-readiness-probes', 'configure-pod-container']"
Modifiability,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:414,config,config,414,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527,1,['config'],['config']
Modifiability,"Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed services. I include the; one-deploy-ago certificates in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:7189,config,config-proxy,7189,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['config'],['config-proxy']
Modifiability,"See #6370 . > Could you open an issue, to explore changing this to a header-specified token, or randomizing the name field.; > ; > https://security.stackexchange.com/questions/211352/does-owasp-recommend-to-include-a-csrf-token-in-a-header-or-to-use-it-as-a-param; > ; > Need to take care with logging in this case.; > https://github.com/OWASP/CheatSheetSeries/blob/master/cheatsheets/Cross-Site_Request_Forgery_Prevention_Cheat_Sheet.md; > ; > To further enhance the security of this proposed design, consider randomizing the CSRF token parameter name and/or value for each request. Implementing this approach results in the generation of per-request tokens as opposed to per-session tokens.; > doing both seems identical to implementing 2 CSRF tokens",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6417:456,enhance,enhance,456,https://hail.is,https://github.com/hail-is/hail/issues/6417,1,['enhance'],['enhance']
Modifiability,"See `pyproject.toml` for the isort configuration. This adds isort as a part of the `check` target for the services. To manually run isort, just run `isort .` anywhere in the hail repo. It will properly find the config file and ignore non-services code. isort has a `black` setting so it should be compatible with subsequent format checks. It doesn't immediately play well with pre-commit (ignores file excludes and runs where it shouldn't e.g. migrations) so I will separately have to look into running it there. When run, it does the following:. - Inserts line breaks to keep long imports in the desired line length; - Sorts items imported from a given module, it appears first by data type (classes before functions) and then alphabetically; - Groups and orders imports by: stdlib, third party, first party (set in pyproject.toml), and local (inferred from imports that start with `.`). I like this a lot; - Sorts imports within each group by `import`vs`from … import`, then alphabetically by module",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11231:35,config,configuration,35,https://hail.is,https://github.com/hail-is/hail/pull/11231,2,['config'],"['config', 'configuration']"
Modifiability,"See the transcript below. This is particularly confusing for users because python often elides the non-printable characters. A small wrinkle of confusion is that the UTF-8 BOM, `ef bb bf`, is converted by Java into the UTF-16 BOM, `fe ff`. This is apparently [a well known Java bug](https://stackoverflow.com/questions/1835430/byte-order-mark-screws-up-file-reading-in-java)? This looks pretty annoying to fix in Scala/Java because we'd have to muck around with Spark's `hadoopFile` infrastructure to figure out where it is actually reading from a file. ```; # hexdump /tmp/bar; 0000000 ef bb bf 73 61 6d 70 6c 65 5f 69 64 0a 66 6f 6f; 0000010 0a ; 0000011; # ipython; import hail asPython 3.7.3 (default, Mar 27 2019, 09:23:15) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl ; hl.import_; In [2]: t = hl.import_table('/tmp/bar') ; ...: t.describe() ; ...: t = t.key_by('sample_id') ; Initializing Spark and Hail with default parameters...; using hail jar at /usr/local/lib/python3.7/site-packages/hail/hail-all-spark.jar; 19/06/13 14:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.1; SparkUI available at http://wm06b-953.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.14-5cb00c115421; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; --------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6342:814,enhance,enhanced,814,https://hail.is,https://github.com/hail-is/hail/issues/6342,1,['enhance'],['enhanced']
Modifiability,Seems the `testImplementation` doesn't configure the classpath for tests correctly - javatests are currently failing with `Error: Could not find or load main class org.testng.TestNG`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1708671249:39,config,configure,39,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1708671249,1,['config'],['configure']
Modifiability,"Set and Dict used an inconsistent definition in the JVM backend, and what's more, it is different from the Scala code. This fixes that, and in particular, it is technically a breaking change. There are two orderings on types, the default coming from <, <=, etc. and a total ordering coming from `compare`. The default can compare ""strangely"", e.g. for Doubles every comparison with nan returns false. This code changes Set and Dict to use the total ordering on types for comparison of elements and keys. The representation of Set and Dict in Java are now SortedSet and SortedMap, which are implemented as TreeSet and TreeMap, which is always parameterized to take the total ordering. Note, I left the tests disabled because there's another comparison bug related to intervals I'm sorting out with @patrick-schultz.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6100:642,parameteriz,parameterized,642,https://hail.is,https://github.com/hail-is/hail/pull/6100,1,['parameteriz'],['parameterized']
Modifiability,"Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 14:46:38 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@28f0ac7{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@49a30f89{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4495af6e{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6baf9f3b{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@562ad221{/static/sql,null,AVAILABLE,@Spark}; 2018-10-09 14:46:39 StateStoreCoordinatorRef: INFO: Registered StateStoreCoordinator endpoint; 2018-10-09 14:46:39 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:39 SparkSqlParser: INFO: Parsing command: SHOW TABLES; 2018-10-09 14:46:40 SparkContext: INFO: Starting job: collect at utils.scala:44; 2018-10-09 14:46:40 DAGScheduler: INFO: Got job 0 (collect at utils.scala:44) with 1 output partitions; 2018-10-09 14:46:40 DAGScheduler: INFO: Final stage: ResultStage 0 (collect at utils.scala:44); 2018-10-09 14:46:40 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 14:46:40 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 14:46:40 DAGScheduler: INFO: Submitting ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:41), which has no missing parents; 2018-10-09 14:46:40 MemoryStore: INFO: Block broadcast_0 stored as values in memory (estimated size 6.0 KB, free 366.3 MB); 2018-10-09 14:46:41 MemoryStore: INFO: Block broadcast_0_piece0 stored as bytes in memory (estimated ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:32357,config,configuration,32357,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['config'],['configuration']
Modifiability,"Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 15:04:33 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@16ba3696{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2780d0b8{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7cea1161{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@696b1f0{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@14d32b0c{/static/sql,null,AVAILABLE,@Spark}; 2018-10-09 15:04:34 StateStoreCoordinatorRef: INFO: Registered StateStoreCoordinator endpoint; 2018-10-09 15:04:34 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:34 SparkSqlParser: INFO: Parsing command: SHOW TABLES; 2018-10-09 15:04:36 SparkContext: INFO: Starting job: collect at utils.scala:44; 2018-10-09 15:04:36 DAGScheduler: INFO: Got job 0 (collect at utils.scala:44) with 1 output partitions; 2018-10-09 15:04:36 DAGScheduler: INFO: Final stage: ResultStage 0 (collect at utils.scala:44); 2018-10-09 15:04:36 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 15:04:36 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 15:04:36 DAGScheduler: INFO: Submitting ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:41), which has no missing parents; 2018-10-09 15:04:36 MemoryStore: INFO: Block broadcast_0 stored as values in memory (estimated size 6.0 KB, free 366.3 MB); 2018-10-09 15:04:36 MemoryStore: INFO: Block broadcast_0_piece0 stored as bytes in memory (estimated ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:14842,config,configuration,14842,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['config'],['configuration']
Modifiability,Should actually figure out how to unify all these variables in one file since they're at least used in both hail-ci-deploy.sh and get-deployed-sha.sh right now,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5009:50,variab,variables,50,https://hail.is,https://github.com/hail-is/hail/pull/5009,1,['variab'],['variables']
Modifiability,"Should be fixed there. Also, the organization of site vs docs is super confusing imo. We have a site folder, which contains the Nginx configuration of site, and also the kube definition of the site deployment. Which makes a lot of sense. However, it also needs files in ../hail/build/www. Those files are built using a script in /hail/python/docs, which grabs www files from the working directory, which in our case should be /hail and not /site, copies those to its ./build/www, merges them with a bunch of files from /hail/python/docs/... , but not only that, it also compiles all of the templates for our cwd ./www. Oh and we also test hail import during doc build, which seems outside of what a documentation / static html build process should do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5282#issuecomment-463307913:134,config,configuration,134,https://hail.is,https://github.com/hail-is/hail/issues/5282#issuecomment-463307913,1,['config'],['configuration']
Modifiability,Should maybe use `spark.local.dir` if that variable is set.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8277:43,variab,variable,43,https://hail.is,https://github.com/hail-is/hail/issues/8277,1,['variab'],['variable']
Modifiability,"Significantly cleaned up code by refactoring; and deleting org.broadinstitute.hail.Utils:; 1. Rich classes are moved to utils.richUtils._,; and implicit conversions are held in a trait; utils.richUtils.Implicits; 2. Standalone classes in Utils have been made; independent classes under utils; 3. Miscellaneous methods have been moved to; utils package object",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/740:33,refactor,refactoring,33,https://hail.is,https://github.com/hail-is/hail/pull/740,1,['refactor'],['refactoring']
Modifiability,"SigningRequest API conditions were updated:; <ul>; <li>a <code>status</code> field was added; this field defaults to <code>True</code>, and may only be set to <code>True</code> for <code>Approved</code>, <code>Denied</code>, and <code>Failed</code> conditions</li>; <li>a <code>lastTransitionTime</code> field was added</li>; <li>a <code>Failed</code> condition type was added to allow signers to indicate permanent failure; this condition can be added via the <code>certificatesigningrequests/status</code> subresource.</li>; <li><code>Approved</code> and <code>Denied</code> conditions are mutually exclusive</li>; <li><code>Approved</code>, <code>Denied</code>, and <code>Failed</code> conditions can no longer be removed from a CSR (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/90191"">kubernetes/kubernetes#90191</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>) [SIG API Machinery, Apps, Auth, CLI and Node]</li>; </ul>; </li>; <li>Cluster admins can now turn off /logs endpoint in kubelet by setting enableSystemLogHandler to false in their kubelet configuration file. enableSystemLogHandler can be set to true only when enableDebuggingHandlers is also set to true. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/87273"">kubernetes/kubernetes#87273</a>, <a href=""https://github.com/SaranBalaji90""><code>@​SaranBalaji90</code></a>) [SIG Node]</li>; <li>Custom Endpoints are now mirrored to EndpointSlices by a new EndpointSliceMirroring controller. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91637"">kubernetes/kubernetes#91637</a>, <a href=""https://github.com/robscott""><code>@​robscott</code></a>) [SIG API Machinery, Apps, Auth, Cloud Provider, Instrumentation, Network and Testing]</li>; <li>CustomResourceDefinitions added support for marking versions as deprecated by setting <code>spec.versions[*].deprecated</code> to <code>true</code>, and for optionally overriding the defaul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:5167,config,configuration,5167,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['config'],['configuration']
Modifiability,"Since #13211, all jobs by default have a deploy config mounted into the container. The `worker-deploy-config` secret is no longer necessary, so long as we properly configure the namespace that CI jobs need to talk to.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13343:48,config,config,48,https://hail.is,https://github.com/hail-is/hail/pull/13343,3,['config'],"['config', 'configure']"
Modifiability,"Since my interface package isn't ready for release yet, here's a reproducible example using just R and sparklyr, with a Hail jar somewhere. Again, this is happening on the Mac, sparklyr version 0.8.4.9004 (there is probably a newer one on CRAN, I doubt that it matters). ```; data(mtcars); hail_jar <- ""/path/to/your/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sparklyr::invoke(ht, ""count""); ```. Thanks a lot for your continued attention to this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-429475190:522,config,config,522,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-429475190,3,['config'],['config']
Modifiability,"Since recently adding metadata server support for batch jobs in GCP, `gcloud` should now ""Just Work"" using the CI service account in CI jobs without explicitly configuring it with a key file, so we no longer need this line. I tested that this succeeds with a dev deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14308:160,config,configuring,160,https://hail.is,https://github.com/hail-is/hail/pull/14308,1,['config'],['configuring']
Modifiability,"So I did:. hail-new-vep importvcf /user/satterst/DILI/DILI_controls.vcf.bgz repartition -n 100 splitmulti vep --config /psych/genetics_data/working/cseed/vep.properties write -o /user/satterst/DILI/DILI_split_vep.vds . It almost immediately advanced to the write, then it sat there having tasks fail for two hours, then it said:; [Stage 1:> (0 + 35) / 100]; hail: write: caught exception: org.apache.spark.SparkException: Job aborted. log here: /humgen/atgu1/fs03/satterst/hail.jobaborted.log",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/302:112,config,config,112,https://hail.is,https://github.com/hail-is/hail/issues/302,1,['config'],['config']
Modifiability,"So I think we should fix this is in a slightly different way. First, I want to unify the log and status page. Second, I want the status to actually include the whole job configuration, of which status is just a sub-field, so you can look at detail to what you submitted. Then it would make sense for these links to always be present. That said, they obviously shouldn't be broken. Instead of 404, we should just say ""job is pending, no logs"" or wahtever. Let me make pass on the batch2 UI and then revisit this, OK?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7449#issuecomment-549809632:170,config,configuration,170,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549809632,1,['config'],['configuration']
Modifiability,"So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I don't have a strong feeling if you want to also add a Python-inspired while loop (although I personally would find the similarities misleading given the required differences, I understand others might feel differently). Your while loop should be naturally implementable in terms of mine, so I also suggest we focus on that first. Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Is Patrick's proposal for extra types written up anywhere? I don't like the idea of complicating the type hierarchy for internal bookkeeping like this. So I'm going to remark that in the code generator it is often natural to build data structures to aid the organization of the code generator, and those data structures need not need to be types/IRs. Given that Recur has to be in tail position, and you know exactly when you're existing the loop (branches that don't contain recur nodes). So the compilation looks like:. ```; set initial loop variables; # fall through into loop; Lloop:; ...; # recur; loop variables = new values; goto Lloop; Lan_exit_branch:; result = compile(branch); goto Lafter; Lanother_exit_branch:; result = compile(other_branch); goto Lafter; Lafter:; use result ...; ```. What I would do is ""peel"" off the ifs and lets (anything else?) that can sit in tail position and build a separate data structure for those nodes which I then traverse to emit the above code. Using the stream interface seems wrong to me also. What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. I'll comment more once I've looked over the code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125:1212,variab,variables,1212,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125,2,['variab'],['variables']
Modifiability,"So the problem is probably that CI is running on master, and it creates the global config in the PR namespace and it won't have docker_root_image. We'll also need to update our production global-config since we're not applying Terraform updates to our cluster. You should be able to do this now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10107#issuecomment-799714090:83,config,config,83,https://hail.is,https://github.com/hail-is/hail/pull/10107#issuecomment-799714090,2,['config'],['config']
Modifiability,"So, certain versions of bokeh require certain versions of pandas. I don't think we can simultaneously support bokeh 1.4 and pandas 2 in the Hail code base (because old bokeh is broken on new pandas). I think the fix is to just forcibly upgrade everyone to latest bokeh (3.x) and update Hail to support latest bokeh. I have a PR coming for this. I haven't checked if new bokeh supports old pandas. Nor do I know if we have old pandas usage lurking in the codebase. Can we make our `pinned-requirements.txt` use pandas 2.0, fix whatever issues arise, but leave `requirements.txt` flexible for folks?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1520463643:578,flexible,flexible,578,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1520463643,1,['flexible'],['flexible']
Modifiability,"Some light refactoring of TableStage to change how context/body is defined; this is pretty similar to how BlockMatrixStage currently handles contexts. I prefer this way of defining the body of a partition because I think it will make writing table joins more natural and lead to less boilerplate ref generation; I think BlockMatrixDot provides a pretty good illustration of what a join body will look like, in general. (The context handling is different, and I don't think we want to do what BlockMatrixStage does w.r.t contexts right now.) This is slightly different from the code in master; I've rewritten `blockBody` to make use of `bindIR` and (currently non-existent) `foldIR` instead of manually generating refs to better illustrate flow. ```; case x@BlockMatrixDot(leftIR, rightIR) =>; val left = lower(leftIR); val right = lower(rightIR); val newCtxType = TArray(TTuple(left.ctxType, right.ctxType)); new BlockMatrixStage(left.globalVals ++ right.globalVals, newCtxType) {; def blockContext(idx: (Int, Int)): IR = {; val (i, j) = idx; MakeArray(Array.tabulate[Option[IR]](leftIR.typ.nColBlocks) { k =>; if (leftIR.typ.hasBlock(i -> k) && rightIR.typ.hasBlock(k -> j)); Some(MakeTuple.ordered(FastSeq(; left.blockContext(i -> k), right.blockContext(k -> j)))); else None; }.flatten[IR], newCtxType); }. def blockBody(ctxRef: Ref): IR = {; def blockMultiply(elt: Ref) =; bindIR(GetTupleElement(elt, 0)) { leftElt =>; bindIR(GetTupleElement(elt, 1)) { rightElt =>; NDArrayMatMul(left.blockBody(leftElt), right.blockBody(rightElt)); }; }; foldIR(ToStream(invoke(""sliceRight"", ctxType, ctxRef, I32(1))),; bindIR(ArrayRef(ctxRef, 0))(blockMultiply)) { (sum, elt) =>; NDArrayMap2(sum, blockMultiply(elt), ""l"", ""r"",; Ref(""l"", x.typ.elementType) + Ref(""r"", x.typ.elementType)); }; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8658:11,refactor,refactoring,11,https://hail.is,https://github.com/hail-is/hail/pull/8658,1,['refactor'],['refactoring']
Modifiability,Some links:. http://findbugs.sourceforge.net/; https://docs.gradle.org/current/userguide/findbugs_plugin.html; https://github.com/sksamuel/scalac-scapegoat-plugin; https://stackoverflow.com/questions/22617713/whats-the-current-state-of-static-analysis-tools-for-scala; https://stackoverflow.com/questions/1598882/are-there-any-tools-for-performing-static-analysis-of-scala-code,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/81:156,plugin,plugin,156,https://hail.is,https://github.com/hail-is/hail/issues/81,1,['plugin'],['plugin']
Modifiability,"Something still isn't right with my configuration. The service and deployment are all up. I can curl to the internal gateway and it shows up in the logs. However, I'm getting 404 with this query: `http://hail.internal/jigold/batch2/healthcheck`. I set up cloud dns to route hail.internal to the internal gateway. I verified no traffic is recorded in the jigold router. I tried adding and removing the `gateway` service account but it made no difference.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6918#issuecomment-524103114:36,config,configuration,36,https://hail.is,https://github.com/hail-is/hail/pull/6918#issuecomment-524103114,1,['config'],['configuration']
Modifiability,"Somewhat related to this is the deployment configuration. That's a little easier to handle on the user's side by including it in the job's image directly, but it would be pretty convenient if it could be mounted automatically as well (as `/deploy-config/deploy-config.json`, with `""location"": ""gce""` and the `domain` set accordingly). Should that be another method on `Job` / `Batch` or should all of this functionality be hidden behind something more abstract to enable nested batches (that's really the main use case, I suppose).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-767883386:43,config,configuration,43,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-767883386,3,['config'],"['config', 'configuration']"
Modifiability,Somewhat surprising that no one has written a shade plugin that renames SO symbols 🤷 . That's clearly the correct answer here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8576#issuecomment-616208715:52,plugin,plugin,52,https://hail.is,https://github.com/hail-is/hail/pull/8576#issuecomment-616208715,2,['plugin'],['plugin']
Modifiability,"Soon, I will add certificates and keys to the secrets and I want; to add configuration parameters that specify the paths to those; certificates and keys. Therefore, the mount locations of the; secrets must be the same everywhere so the paths are valid.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8416:73,config,configuration,73,https://hail.is,https://github.com/hail-is/hail/pull/8416,1,['config'],['configuration']
Modifiability,Sorry - one more thing I need help with. There's a cyclical import with the RouterFS in `variables.py`. Should I just pylint ignore it?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13224#issuecomment-1677892406:89,variab,variables,89,https://hail.is,https://github.com/hail-is/hail/pull/13224#issuecomment-1677892406,1,['variab'],['variables']
Modifiability,"Sorry for a fairly late comment on this PR, but I was wondering about the default configuration:. > CHANGELOG: Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to specify which cloud regions a job can run in. The default value is a job can run in any available region. We're looking forward to the functionality in this PR particularly because we're hoping that it'll allow us to schedule workers in the US, while our Batch deployment is in Australia. However, by default we really need to make sure that workers won't be scheduled in the US, to avoid accidental egress charges, as all our datasets are located in Australia. For processing gnomAD data (which is located in the US), spinning up workers colocated with the data would be fantastic though. Hence we'd really need a configurable default value on the deployment level, I believe:. - Generally allow scheduling in AU + US regions (specifically `australia-southeast1` and `us-central1`).; - By default, pick any region in AU only (in practice `australia-southeast1`).; - Allow jobs to explicitly specify to run in the US (in practice `us-central1`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1275427218:82,config,configuration,82,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1275427218,3,['config'],"['configurable', 'configuration']"
Modifiability,"Sorry for the large diff, it's a conceptually small change that required a lot of refactoring!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2886#issuecomment-364619280:82,refactor,refactoring,82,https://hail.is,https://github.com/hail-is/hail/pull/2886#issuecomment-364619280,1,['refactor'],['refactoring']
Modifiability,"Sorry this got missed! We should have responded, at least. This is generally intended -- scientific notation is one of the least error-prone way to represent floating-point values, and the format we use is a standard one that most tools should handle. However, we do intend to expose an option to parameterize the format of floating point values in export_vcf (though scientific notation will probably always be the default).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6963#issuecomment-563994619:297,parameteriz,parameterize,297,https://hail.is,https://github.com/hail-is/hail/issues/6963#issuecomment-563994619,1,['parameteriz'],['parameterize']
Modifiability,"Sorry, I should have explained the rationale for returning true for empty strings. I was thinking that you should be able to do something like this:; ```python; from urllib.parse import urlparse. url = ""whatever:///path/to/file""; if hl.hadoop_supports_scheme(urlparse(url).scheme):; with hl.hadoop_open(url) as f:; ...; ```. For paths without a scheme, the ParseResult's scheme is """".; ```python; from urllib.parse import urlparse. urlparse(""/path/to/file.txt"").scheme; ''; ```. > All file systems should support file:// scheme. HadoopFS is the only one that will actually work correctly if passed a `file://` URL. For example, both LocalFS and GoogleCloudStorageFS use `os.path.exists` to implement `exists`. On macOS at least, `os.path.exists(""file:///test.txt"")` looks for `test.txt` in a directory named `file:` in the current working directory, not the root directory. > I'm not sure if it makes sense for this function to return true for empty string (which I read as meaning no specified scheme, and therefore read file from local disk). I think those should maybe just be 'file'? I could be convinced it should support empty string and 'file' as inputs if you want it to work on empty string. For LocalFS and GoogleCloudStorageFS, an empty scheme will correspond to a path to a local file. For HadoopFS, an empty scheme causes Hadoop to use its configured default file system. When running Hail locally, this would be the local file system. On a Dataproc cluster, this would be HDFS. Returning true for the empty string in `HadoopFS.supports_scheme` is based on an assumption that there would always be a working default file system.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10555#issuecomment-856148043:1353,config,configured,1353,https://hail.is,https://github.com/hail-is/hail/pull/10555#issuecomment-856148043,1,['config'],['configured']
Modifiability,"Spark doesn't have per-type requiredness but a nullable flag on array elements and struct fields. You just needed to set our required status based on their struct fields. Here is a patch that fixes it:. ```; diff --git a/src/main/scala/is/hail/expr/AnnotationImpex.scala b/src/main/scala/is/hail/expr/AnnotationImpex.scala; index 5039471..4cedfd8 100644; --- a/src/main/scala/is/hail/expr/AnnotationImpex.scala; +++ b/src/main/scala/is/hail/expr/AnnotationImpex.scala; @@ -45,11 +45,11 @@ object SparkAnnotationImpex extends AnnotationImpex[DataType, Any] {; case DoubleType => TFloat64(); case StringType => TString(); case BinaryType => TBinary(); - case ArrayType(elementType, _) => TArray(importType(elementType)); + case ArrayType(elementType, containsNull) => TArray(importType(elementType).setRequired(!containsNull)); case StructType(fields) =>; TStruct(fields.zipWithIndex; .map { case (f, i) =>; - (f.name, importType(f.dataType)); + (f.name, importType(f.dataType).setRequired(!f.nullable)); }: _*); }; ; diff --git a/src/test/scala/is/hail/methods/KeyTableSuite.scala b/src/test/scala/is/hail/methods/KeyTableSuite.scala; index 8a46826..dbb3485 100644; --- a/src/test/scala/is/hail/methods/KeyTableSuite.scala; +++ b/src/test/scala/is/hail/methods/KeyTableSuite.scala; @@ -380,9 +380,9 @@ class KeyTableSuite extends SparkSuite {; .flatten(); ; val df = kt.toDF(sqlContext); -// df.printSchema(); -// df.show(); - val kt2 = KeyTable.fromDF(hc, df); + df.printSchema(); + df.show(); + val kt2 = KeyTable.fromDF(hc, df, key = Array(""v"")); assert(kt2.same(kt)); }; ; ```. We should require the KeyTable row type to be required. We've always worked internally with the invariant that rows themselves can't be missing. (If they were, they wouldn't be in the table.) I commented in the print statements to see what was going on. You should probably delete them. I also had to set the key in `fromDF` the key isn't represented in DataFrames.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2417#issuecomment-343800938:517,extend,extends,517,https://hail.is,https://github.com/hail-is/hail/pull/2417#issuecomment-343800938,2,['extend'],['extends']
Modifiability,"SparkUI' could not bind on port 4040. Attempting port 4041.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Aug 4 2017 00:39:18); SparkSession available as 'spark'.; >>> import hail; >>> hc = hail.HailContext(); log4j:ERROR setFile(null,false) call failed.; java.io.FileNotFoundException: hail.log (Permission denied); 	at java.io.FileOutputStream.open0(Native Method); 	at java.io.FileOutputStream.open(FileOutputStream.java:270); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:213); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:133); 	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294); 	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165); 	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104); 	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842); 	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768); 	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648); 	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514); 	at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:440); 	at is.hail.HailContext$.configureLogging(HailContext.scala:132); 	at is.hail.HailContext$.apply(HailContext.scala:159); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198:1840,config,config,1840,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198,1,['config'],['config']
Modifiability,Stacked on #10876. Refactors all the function registry interfaces to take/return (S)Values.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10877:19,Refactor,Refactors,19,https://hail.is,https://github.com/hail-is/hail/pull/10877,1,['Refactor'],['Refactors']
Modifiability,"Stacked on #10920 and #10965. # Summary of Changes. ## hailtop.aiocloud.aioazure; - Renamed AzureResourcesClient to AzureResourceManagementClient; - Added AzureResourcesClient that hits a different API than before; - Added `AzureBaseClient.get_next_link` and `AzureBaseClient.delete_and_wait`. ## Batch; - Added `batch.azure` which mirrors the functionality of `batch.gcp`; - Renamed `worker_local_ssd_data_disk` to `local_ssd_data_disk` in the PoolConfig; - Renamed `worker_pd_ssd_data_disk_size_gb` to `external_data_disk_size_gb` in the PoolConfig; - Added {Azure,GCP}UserCredentials to the worker to abstract away the names of environment variables and the mount paths of credentials in containers. ## Auth; - Added new fields in the auth database for `azure service principal name` and `azure_credentials_secret_name`; - Made `auth` only create `GSAResource` if CLOUD == 'gcp'. ## Gear; - Added `azure-vm` to the location options for `DeployConfig`. # Assumptions:; - Mapped `{'lowmen': 'F', 'standard': 'D', 'highmem': 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10970:643,variab,variables,643,https://hail.is,https://github.com/hail-is/hail/pull/10970,1,['variab'],['variables']
Modifiability,"Stacked on #11240. Rewrite all `SSettable.store` implementations to take an `SValue`. With that, `SCode` and subclasses are safe to delete.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11241:19,Rewrite,Rewrite,19,https://hail.is,https://github.com/hail-is/hail/pull/11241,1,['Rewrite'],['Rewrite']
Modifiability,Stacked on #11996. This change gets rid of the extra dummy variable and temp table used for bookkeeping the previous migration to populate the aggregated billing tables.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12006:59,variab,variable,59,https://hail.is,https://github.com/hail-is/hail/pull/12006,1,['variab'],['variable']
Modifiability,"Stacked on #12210, this alters `build.py` to push images to `docker_prefix` and to `new_docker_prefix` if present in the global config. This will make sure that when we switch `docker_prefix` to the new Artifact Registry that any in-use images are ready to go. Once we are ready to switch over, we can; 1. change `docker_prefix` to the new AR in the global-config; 2. remove `new_docker_prefix` from the global-config; 3. Revert this change",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211:128,config,config,128,https://hail.is,https://github.com/hail-is/hail/pull/12211,3,['config'],['config']
Modifiability,"Stacked on #9769 . I tried to make the code changes as small as possible and this is just a refactoring. I split the current instance_pool into the instance_monitor and the instance_pool. The main difference is the instance pool and instance monitor are recording two exact copies of the instances by state and live total cores etc which are linked via `adjust_for_*_instance`. Eventually with multiple pools, these numbers won't be identical and one will be for all instances and the other will be pool specific.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9772:92,refactor,refactoring,92,https://hail.is,https://github.com/hail-is/hail/pull/9772,1,['refactor'],['refactoring']
Modifiability,"Stacked on #9774 . This PR leaves the system in a state that has multiple pools in the SQL code, but the instance pool is hard coded as standard and there's only one of them. The global variables of local ssd, standing worker cores, etc. are still in globals and not per pool yet. This is because I need to write the code for the instance pool manager which will be in subsequent PRs so that we can configure each pool individually. The goal of this PR is to make sure the incremental data structures are correct for supporting multiple pools.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9779:186,variab,variables,186,https://hail.is,https://github.com/hail-is/hail/pull/9779,2,"['config', 'variab']","['configure', 'variables']"
Modifiability,Stacked on: https://github.com/hail-is/hail/pull/5509. Broadcast once and reuse the same broadcast where. Never serialize (except via broadcast). Same pattern as RVDPartitioner and Hadoop configuration.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5512:188,config,configuration,188,https://hail.is,https://github.com/hail-is/hail/pull/5512,1,['config'],['configuration']
Modifiability,Stacked on: https://github.com/hail-is/hail/pull/5891. I found getting .in (or not) consistent between the configuration and the files was just error prone. I think this is just simpler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5907:107,config,configuration,107,https://hail.is,https://github.com/hail-is/hail/pull/5907,1,['config'],['configuration']
Modifiability,"Stacked on: https://github.com/hail-is/hail/pull/7031. Changes:; - primary change was to add `Tokens.namespace_token_or_error` which prints a friendly error of the user doesn't have the necessary authentication; - added `hailctl auth list`, and made `hailctl dev config` with no options print out the current configuration; - implemented @danking's suggestion: change some natural entrypoints (BatchClient, get_userinfo, etc.) to take optional `deploy_config` argument and load the default config if not given",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7035:263,config,config,263,https://hail.is,https://github.com/hail-is/hail/pull/7035,3,['config'],"['config', 'configuration']"
Modifiability,"Stacked on: https://github.com/hail-is/hail/pull/7208. Make app, db and v1 not global variables in batch.py. I'm doing this because I want to break Batch, Job into separate files and use them in both the scheduler and the front end. To do that, they need to be parameterized by the app (or the services that are carried on the app).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7210:86,variab,variables,86,https://hail.is,https://github.com/hail-is/hail/pull/7210,2,"['parameteriz', 'variab']","['parameterized', 'variables']"
Modifiability,"Stacks on #5430. Once #5430 is in, the changes here will be limited to: 1) notebook.py: login/logout routes, the provision of authorized users, auth0 lib, 2) index.html 3) header.html: update lines 12 and 13 to read user from session. Provides basic login page. Below are a few images of it in action. Looks like app.hail.is. Handles authorized and workshop-only login. Handles login only; future PR will extend to checking, refreshing the session. cc @cseed . screenshots (notebook create button not yet PR'd , auth0 page not yet styled). <img width=""1141"" alt=""screen shot 2019-02-25 at 11 17 37 pm"" src=""https://user-images.githubusercontent.com/5543229/53387218-d62f3e80-3953-11e9-8653-e4c6b0e8294a.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 00 pm"" src=""https://user-images.githubusercontent.com/5543229/53387219-d62f3e80-3953-11e9-8595-d7f1ea58a243.png"">; <img width=""1139"" alt=""screen shot 2019-02-25 at 11 18 18 pm"" src=""https://user-images.githubusercontent.com/5543229/53387220-d62f3e80-3953-11e9-9fba-e4a93b0374ee.png"">; <img width=""1141"" alt=""screen shot 2019-02-25 at 11 18 33 pm"" src=""https://user-images.githubusercontent.com/5543229/53387221-d62f3e80-3953-11e9-9527-7c4589846a29.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5437:405,extend,extend,405,https://hail.is,https://github.com/hail-is/hail/pull/5437,1,['extend'],['extend']
Modifiability,StagedExtractedAggregators had a lot of duplicated code from ExtractAggregators. This just parameterizes the internal functions so that we don't have two identical copies of the aggregator logic.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6161:91,parameteriz,parameterizes,91,https://hail.is,https://github.com/hail-is/hail/pull/6161,1,['parameteriz'],['parameterizes']
Modifiability,"Started playing with [typer](https://typer.tiangolo.com/) on the plane. It's a somewhat thin wrapper around `click`, and I'm not entirely sold on one vs the other, but either seems a lot neater than argparse (I remember Cotton tried this years ago but I haven't looked at that PR). Click gives us the easy composition of CLIs, function name as the command name, the use of docstring as the description, help output, and most of the core functionality you see. Typer is the one translating python type hints into click type-checking, default setting, etc. Typer also uses rich if it's installed and allows you to install shell completion which is pretty neat, not sure if you can get that through click or if you need to use a click plugin. I replaced a couple of the hailctl commands here with the typer/click version. If you like what you see I can keep going on the rest of the commands (you can look to `batch/cli.py` and `hailctl/__main__.py` as an example).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13109:732,plugin,plugin,732,https://hail.is,https://github.com/hail-is/hail/pull/13109,1,['plugin'],['plugin']
Modifiability,StatAggregator inherits NaN and Infinities from StatCounter,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1636:15,inherit,inherits,15,https://hail.is,https://github.com/hail-is/hail/issues/1636,1,['inherit'],['inherits']
Modifiability,Still need to verify the audience and probably configure our clients to specifically use the hail oauth client id as the audience.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13084:47,config,configure,47,https://hail.is,https://github.com/hail-is/hail/pull/13084,1,['config'],['configure']
Modifiability,"Strange, I can't reply to directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414:831,config,configuration,831,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414,2,['config'],['configuration']
Modifiability,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:344,config,config,344,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662,2,['config'],['config']
Modifiability,"Suggested fix:. ```scala; case class UpdatedRow(orig: Row, i: Int, update: Any) extends Row {; ...; }. ```; This gets you the update with one allocation and no copy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1629#issuecomment-290846410:80,extend,extends,80,https://hail.is,https://github.com/hail-is/hail/issues/1629#issuecomment-290846410,1,['extend'],['extends']
Modifiability,"Suggested rewrite:. While this method parallelizes over a list of BGEN files, each file is indexed serially by one core. So indexing several BGEN files on a large cluster is a waste of resources, and indexing should generally be done separately from large analyses.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1499#issuecomment-284842421:10,rewrite,rewrite,10,https://hail.is,https://github.com/hail-is/hail/pull/1499#issuecomment-284842421,1,['rewrite'],['rewrite']
Modifiability,"Summary of changes:; - Overhaul tmpdir handling. Remove most of the old code. Added local_tmpdir to `init`. tmpdir is the networked tmpdir. local_tmpdir is the tmpdir used for local files on both the driver and the executors. Added tmpdir and localTmpdir to ExecuteContext. ExecuteContext removes tmp files on close. Tmp file base is now required, try to give good base names. Tmp file names are now generated by being sufficiently random.; - Removed fs from HailContext. This involved threading ctx and fs through lots of code (most of the changes).; - Added ExecuteContext to EmitModuleBuilder and friends. This is necessary because EmitMethodBuilder gives generated code access to backend, fs, etc. which are carried by the ctx.; - Some IR (mostly readers, but also VEP, which needs to load the VEP configuration to determine its type) have overall parameters that control their behavior (e.g. the VCF reader path) but have to do IO to determine other state (like the matrix type, determined from the VCF header). This complicates pretty printing, serialization, and equality. I clarified this. In particular, I seperate the parameters (see, for example, MatrixVCFReaderParameters) which are specified on creation and used for serialization and equality from other derived state. IR no longer close over ctx or fs and they don't need to do IO after their intiial construction.; - MatrixSpec has subspecs for the marginal tables, and TableSpec has the global and rows RVD. These are now loaded on construction, so lowering no longer neesd to do IO.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8581:802,config,configuration,802,https://hail.is,https://github.com/hail-is/hail/pull/8581,1,['config'],['configuration']
Modifiability,"Summary of changes:; - add is.hail.lir, a low-level IR for emitting JVM bytecode; - lir handles local variable initialization. It uses dataflow analysis to compute which variables must be initialized. It will no longer be necessary to initialize locals to satisfy the JVM bytecode verifier.; - Modify Code[T] to use lir instead of asm directly. Code[T] can only be used once and this is now checked.; - Remove joinpoint and ParameterPack. This primarily involved making EmitStream use Labels instead of joinpoint, and specializing routines that required ParameterPack to work over EmitCode instead.; - Because Code[T] can only be used once, push Value[T], PValue and EmitValue throughout the code base. For example, the Emit environment is now an Emit[EmitValue]. This was mostly a lot of tedious changes: remove `.load()` in places, add calls to `memoize`, and change `Code[T] => Value[T]` in various places.; - EmitMetholdBuilder has newEmit{Local, Field} for creating places to store EmitCodes. I think there are two main issues to clean up before this goes in, or soon after:. This code doesn't try to optimize short-circuit boolean operations (||, &&, etc.) like the old code did, tho it seems the old code wasn't always working. Either way, this should get fixed. It is relatively easy to handle in `Code[T]`. I will fix this before the final version. I left jointpoint.Ctrl and have implicit conversions that freely convert between `Code[Unit]` and `Code[Ctrl]`. This is a bit tedious, but I guess `Code[T]` should support `Code[Nothing]` for type checking user code, although it will still treat it like a `Code[Unit]`. I will fix this later.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8312:102,variab,variable,102,https://hail.is,https://github.com/hail-is/hail/pull/8312,2,['variab'],"['variable', 'variables']"
Modifiability,"Summing a block-sparse matrix may result in a block-dense vector, in which case maybeBlocks should be None (otherwise the `bis.length < maxNBlocks` assert fails...when rebuilding BlockMatrix in Python/C++ I may change the invariants). Also extended test cases to serve as regression test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4888:240,extend,extended,240,https://hail.is,https://github.com/hail-is/hail/pull/4888,1,['extend'],['extended']
Modifiability,"Support [Artifact Registry](https://cloud.google.com/artifact-registry) to store images. It's useful for our deployment in Australia, as GCR does not support our continent. With AR, we would save on network egress. The PR adds a `DOCKER_PREFIX` variable, which is passed along in the code together with `GCP_PROJECT` and others. To switch to AR, one would need to modify `DOCKER_PREFIX` in `config.mk`:. ```; REGION := us-central1; DOCKER_PREFIX := gcr.io/$(PROJECT); ```. ```; REGION := australia-southeast1; DOCKER_PREFIX := $(REGION)-docker.pkg.dev/$(PROJECT)/hail; ```. Also, when making an initial deployment with Terraform, there is an extra variable `use_artifact_registry = false` that controls whether to use AR or GCR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10107:245,variab,variable,245,https://hail.is,https://github.com/hail-is/hail/pull/10107,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"Suppose you have two branches with the same base commit. Neither branch has Scala changes. Consider `make shadowJar` when switching between these branches: it thinks there's nothing to do because the Scala code hasn't changed. This of course doesn't work because the python version *is* changing (note: make install-editable refreshes the version files) and Hail refuses to use an out of date jar. This adds a tiny make macro that lets make targets depend on variables that depend on the latent environment, like git SHAs. To create a target for such a variable add this line: `$(eval $(call ENV_VAR,VARIABLE_NAME))`. Any rule that depends on the value of `VARIABLE_NAME` should depend on the target `env/VARIABLE_NAME`. I also split `BUILD_INFO` into the scala parts and the python parts and moved the scala dependency down to the shadow jar rule, where it belongs. This bug was hidden because build.gradle still regenerates the build info every time shadowJar is called. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6867:459,variab,variables,459,https://hail.is,https://github.com/hail-is/hail/pull/6867,2,['variab'],"['variable', 'variables']"
Modifiability,Syntax: hailctl config list [section]. Output (for each value):; ```; with no section specified:; {section}/{key}={value}; with a section specified:; {key}={value}; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9529:16,config,config,16,https://hail.is,https://github.com/hail-is/hail/pull/9529,1,['config'],['config']
Modifiability,"TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:20266,Plugin,Plugins,20266,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Plugin'],['Plugins']
Modifiability,Tested locally. Renames `config.yaml` to `config.ini` and then otherwise operates as normal for set/get/unset.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9493#issuecomment-697063777:25,config,config,25,https://hail.is,https://github.com/hail-is/hail/pull/9493#issuecomment-697063777,2,['config'],['config']
Modifiability,Tested the UI part manually in my dev namespace. The `/envoy-configs/gateway` endpoint is nice for inspecting the generated config,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14609#issuecomment-2221187830:61,config,configs,61,https://hail.is,https://github.com/hail-is/hail/pull/14609#issuecomment-2221187830,2,['config'],"['config', 'configs']"
Modifiability,Tests are all passing. One question that remains is should we put a manifest file with the current Hail configurations somewhere in hail-common?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1514682815:104,config,configurations,104,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1514682815,1,['config'],['configurations']
Modifiability,"Tests are passing now. I am going to mark WIP because I want to run benchmarks on Monday before this merges. You can look whenever though. . Some things to look at:. 1. Does `ServiceTaskContext` have a notion of stages or retries, or should those also always be 0? ; 2. Some of the logic with combOps getting region pools is a little different, now depends on where combOp is running . For the most part though, PR is just refactoring to pass `RegionPool` everywhere",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9675#issuecomment-738330547:423,refactor,refactoring,423,https://hail.is,https://github.com/hail-is/hail/pull/9675#issuecomment-738330547,1,['refactor'],['refactoring']
Modifiability,"Thank you for the above suggestions, I was originally getting the :nativeLib FAILED error. But I resolved it by using the most recent gcc 7.1.0 version. However, even I stumble upon this error while compiling :compileScala step.; `[nroak@compute-0-19 hail]$ ./gradlew shadowJar; Picked up _JAVA_OPTIONS: -Xmx4g; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /mount/pcgp/resources/hail/src/main/c/libsimdpp-2.0-rc2; :compileScala; Picked up _JAVA_OPTIONS: -Xmx4g; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2544: value floorDiv is not a member of object Math; register(""//"", (x: Int, y: Int) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2545: value floorDiv is not a member of object Math; register(""//"", (x: Long, y: Long) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2549: value floorMod is not a member of object Math; register(""%"", (x: Int, y: Int) => java.lang.Math.floorMod(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2550: value floorMod is not a member of object Math; register(""%"", (x: Long, y: Long) => java.lang.Math.floorMod(x, y), null); ^; four errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 52.396 secs; `",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-302833404:407,Config,Configuring,407,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302833404,1,['Config'],['Configuring']
Modifiability,Thanks a lot for the great comments Tim! Refactored and so much cleaner now!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1324#issuecomment-276567536:41,Refactor,Refactored,41,https://hail.is,https://github.com/hail-is/hail/pull/1324#issuecomment-276567536,1,['Refactor'],['Refactored']
Modifiability,"Thanks for pointing that out. The spec is ambiguous, though in this case I think we can hack around it if we just set ; ```python; image_ref.path = path_prefix + [image_ref.name()]); ```; **before** resetting the domain, and maybe leave a comment explaining the rewrite.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10382#issuecomment-828493098:262,rewrite,rewrite,262,https://hail.is,https://github.com/hail-is/hail/pull/10382#issuecomment-828493098,1,['rewrite'],['rewrite']
Modifiability,"Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first. So if I understand correctly, for the different headers:; - X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I'm having trouble seeing when `$http_x_forwarded_proto` would ever be absent, although if it is, isn't $scheme always `http` since all traffic from gateway to router is via http? Or am I misunderstanding how this works?; - I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? I don't quite follow your comment about our use of `Host` being wrong, in this case; I *think* I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enough to know what happens if I change it to `proxy_set_header Host $http_host;` instead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548078429:540,config,config,540,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548078429,1,['config'],['config']
Modifiability,"Thanks for the review!. The point of this code is to allow optimization across bindings. The `MaximizeLets` pass is ""let lifting"", and is the thing that would push the `ArrayLen` into the body. > Also, what's the point of pushing Lets back down again?. The MinimizeLets pass was what I used to implement single-use let forwarding in a principled way. We could also do it your way, that seems much nicer! I'll rewrite the MinimizeLets pass as `ForwardLets` and write an IR analysis function that asks the right questions. I'd like to talk about implementing a use-def chain. Should that be part of the initial PR, or would you feel OK merging this optimizer pass without that piece?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5041#issuecomment-453180790:409,rewrite,rewrite,409,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-453180790,1,['rewrite'],['rewrite']
Modifiability,"Thanks, I added the IntIterator to Utils and switched hardcallIterator to extend it. Back to you. I definitely switched branches and used shadowJar (I went back and forth twice because I didn't believe the results).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1312#issuecomment-275855290:74,extend,extend,74,https://hail.is,https://github.com/hail-is/hail/pull/1312#issuecomment-275855290,1,['extend'],['extend']
Modifiability,"That's a legitimate concern. We don't currently do this for `datasets.json` / the annotation database. For now let's leave it as it is. If the files move the user always has the option of fixing their configuration by explicitly specifying a config. Having some sort of remote configuration seems valuable, but this PR is large already. Whatever solution we come up with for remote configuration should also support the annotation database case. Let's not worry about it for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1515016056:201,config,configuration,201,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1515016056,4,['config'],"['config', 'configuration']"
Modifiability,"The 'build' docs page implies that the only requirement for running hail is Gradle. However, I've just tried to build hail on Debian Jessie and Ubuntu 16.04, and both failed in different ways. On Jessie, I was able to figure out that the version of Gradle was too old. On Ubuntu 16.04, I get. ```; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED; ```. A quick Google around doesn't reveal any obvious answers to this. What version of Gradle is needed? Is Scala a prerequisite? It would be very useful to provide detailed instructions on how to build hail from scratch on a fresh installation of some Linux distribution.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594:435,config,configuration,435,https://hail.is,https://github.com/hail-is/hail/issues/594,1,['config'],['configuration']
Modifiability,The GCP terraform got into a bit of an invalid state during an ambitious but ultimately fragmented migration I was trying to make to modularize the terraform code. The `sql_config` module assumed by the terraform code no longer exists (!) and I've reinstated the database server config resource for the time being until the GCP terraform code is ready to use the new `infra/k8s` module. This also includes the following fixes/cleanup:. - A GSA key/secret for grafana that is required for grafana/create_accounts to work correctly; - Deleting resources related to the `gcr_pull` service account that no longer exists since it isn't used in our codebase.; - Added the cluster role/binding for batch that it needs to use to access developer/test namespaces. This will become relevant soon when I introduce the rest of the changes from #10866 that I now intend to do more gradually. I tested this by applying my changes to my own cluster and restarting auth/auth-driver to validate that the sql config works as intended and using the admin-pod to verify that the `sql-config.cnf` is also correct.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11009:279,config,config,279,https://hail.is,https://github.com/hail-is/hail/pull/11009,3,['config'],['config']
Modifiability,"The Makefiles grab the `docker_prefix` from the global-config each time they are run so once we make the change in the global-config nothing else needs to change. Actually, I think this entire PR is not even needed. When CI does a deploy, the changes it applies to Kubernetes include fully qualified image names, e.g. `gcr.io/hail-vdc/batch:asdf1234`. If we were to swap out the `docker_prefix` global-config variable, CI would start to create new images that are pushed to the new repository (it would kill the cache for a single build but whatever), but the existing images would still exist and be undisrupted. The only images that need to exist in the new container registry when the switch is made are the images that we push on bootstrap which I am going to do manually anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211#issuecomment-1259517486:55,config,config,55,https://hail.is,https://github.com/hail-is/hail/pull/12211#issuecomment-1259517486,4,"['config', 'variab']","['config', 'variable']"
Modifiability,The PR adds support for skipping Scala `SchedulerSuite` unit tests by setting a `HAIL_TEST_SKIP_SCHEDULER` environment variable.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6768:119,variab,variable,119,https://hail.is,https://github.com/hail-is/hail/pull/6768,1,['variab'],['variable']
Modifiability,The TOKEN variable is defined in config.mk which is included into hail/Makefile.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12825:10,variab,variable,10,https://hail.is,https://github.com/hail-is/hail/pull/12825,2,"['config', 'variab']","['config', 'variable']"
Modifiability,"The [Hail CI Build Configuration](https://ci.hail.is/admin/editBuildRunners.html?id=buildType:HailSourceCode_HailCi) (admin login required) now runs `gradle clean compileTestScala` against three spark versions: `1.6.2`, `1.5.2`, and `1.6.0-cdh5.7.2`. If all of those succeed, it runs `gradle clean test createDocs` against the default spark version in the gradle script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/742#issuecomment-245033280:19,Config,Configuration,19,https://hail.is,https://github.com/hail-is/hail/issues/742#issuecomment-245033280,1,['Config'],['Configuration']
Modifiability,"The [TextInputFormat](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html) class clearly comes from hadoop. It's no longer in the location from which we import it. We must get it from some other dependency. OK. So, before my simplification of build.gradle, we used a configuration called `compile` and another one called `testCompile`. [Neither of those exist in modern gradle, apparently](https://docs.gradle.org/current/userguide/java_library_plugin.html#sec:java_library_configurations_graph). I found a side-note about the `compile` configuration [here](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:java_dependency_management_overview) (search for ""compile""):. > **Why no compile configuration?**; > The Java Library Plugin has historically used the compile configuration for dependencies that are required to both compile and run a project’s production code. It is now deprecated, and will issue warnings when used, because it doesn’t distinguish between dependencies that impact the public API of a Java library project and those that don’t. You can learn more about the importance of this distinction in [Building Java libraries](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:building_java_libraries). OK, so, we used to just dump everything into our runtime dependencies. I changed it so that we have three kinds of dependencies:; 1. `shadow`: these are provided by Dataproc/QoB at run-time. They are not in any JAR. They are not on the `testRuntimeClasspath` or `runtimeClasspath`. They are on the `testCompileClasspath` because I [explicitly requested](https://github.com/hail-is/hail/blob/main/hail/build.gradle#L98) that `testCompileOnly` bring in our `shadow` dependencies.; 2. `implementation`: these are included in all class paths and in shadow JARs (but not ""thin"" jars generated by `./gradlew jar`).; 3. `testImplementation`: these are included in test class paths and in shadow JARs. Our t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741:303,config,configuration,303,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741,9,"['Plugin', 'config']","['Plugin', 'configuration']"
Modifiability,"The [`flake8` check job](https://ci.hail.is/batches/536128/jobs/11) is also complaining about whitespace in struct.py on a few lines:; ```; make -C hail/python check; make[1]: Entering directory '/io/repo/hail/python'; python3 -m flake8 --config ../../setup.cfg hail; hail/utils/struct.py:120:1: W293 blank line contains whitespace; hail/utils/struct.py:122:1: W293 blank line contains whitespace; hail/utils/struct.py:124:1: W293 blank line contains whitespace; hail/utils/struct.py:126:1: W293 blank line contains whitespace; hail/utils/struct.py:129:1: W293 blank line contains whitespace; hail/utils/struct.py:131:1: W293 blank line contains whitespace; hail/utils/struct.py:134:1: W293 blank line contains whitespace; hail/utils/struct.py:136:1: W293 blank line contains whitespace; make[1]: *** [Makefile:11: check] Error 1; make[1]: Leaving directory '/io/repo/hail/python'; make: *** [Makefile:13: check-hail] Error 2; ```. I recommend enabling ""show whitespace"" in your editor. That ensure you see these issues before you make a commit. For PyCharm you can [try this](https://intellij-support.jetbrains.com/hc/en-us/community/posts/206349049-How-to-make-show-whitespace-turn-on-as-default-).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11394#issuecomment-1049169544:239,config,config,239,https://hail.is,https://github.com/hail-is/hail/pull/11394#issuecomment-1049169544,1,['config'],['config']
Modifiability,"The `--extra-config` short option used to be `-c` and I accidentally made it `-e`, which likely shadows the previous option. This broke using `-e` for `--excluded_steps`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13182:13,config,config,13,https://hail.is,https://github.com/hail-is/hail/pull/13182,1,['config'],['config']
Modifiability,The `fails_in_azure` decorator uses the `HAIL_CLOUD` environment variable and it wasn't supplied to tests. The decorator itself looks fine.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11132:65,variab,variable,65,https://hail.is,https://github.com/hail-is/hail/pull/11132,1,['variab'],['variable']
Modifiability,The `logging_queries` variable is always *defined* but sometimes `None`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13824:22,variab,variable,22,https://hail.is,https://github.com/hail-is/hail/pull/13824,1,['variab'],['variable']
Modifiability,"The `pre-commit` hook is a little sticky because `pre-commit` installs each tool in its own isolated virtual env, which won't have the dependencies unless we tell `pre-commit` to also install all of our pinned dependencies into the pyright virtualenv. We can configure pyright to use a different virtualenv for all its dependencies, but that would require each developer specifying the name of their virtual environment in `pyproject.toml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13437#issuecomment-1681248432:259,config,configure,259,https://hail.is,https://github.com/hail-is/hail/pull/13437#issuecomment-1681248432,1,['config'],['configure']
Modifiability,"The `to_dense_mt` and `to_merged_sparse_representation` methods required; localizing into an array all of the split rows at each locus when the; variant data is on the right side of the join. We flip the order so that; the variant data are on the left and the reference data, known to be; locus-distinct, are on the right. I also fix a few stream mis-parameterizations that led to a slightly; higher (but similarly scaling) memory usage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11755:351,parameteriz,parameterizations,351,https://hail.is,https://github.com/hail-is/hail/pull/11755,1,['parameteriz'],['parameterizations']
Modifiability,"The assumption is that the default_ns namespace has a database-server-config that has credentials for the database instance which can be used to create various databases. This is present in default. We will require this is also present for dev namespaces, database-server-config will be the user's private database. devs shouldn't have access to the root database credentials. When we create a test default_ns when running the tests, we also create a ""test_instance"" database that will be used as the database instance inside the tests. database-server-config is only used by CI. Also, there's no reason to use the credentials from batch-pods anymore, so I use the one from default. This will need to go in before I can finish https://github.com/hail-is/hail/pull/7674",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7683:70,config,config,70,https://hail.is,https://github.com/hail-is/hail/pull/7683,3,['config'],['config']
Modifiability,"The asyncio event loop only keeps weak references to tasks, so wherever we call `asyncio.create_task` we need to ensure that we keep a strong reference to its result. Specifically, `BackgroundTaskManager` needs to keep strong references not weak references to the tasks it creates. This is easy to do without accumulating garbage by using a done callback on the task to remove itself from the set. However, this felt iffy with the threadsafe futures, which were only used in sync.py anyway, so I pushed that functionality directly into sync.py and removed it from the `BackgroundTaskManager`. To simplify the ownership story for tasks, this changes `BackgroundTaskManager` to *not* return the task and instead hold onto strong references. If a client wants a reference to the task it creates, it should call `asyncio.create_task` directly and manage the lifecycle of the spawned task. This required only a few small changes in worker.py since most of the codebase does not assign the result of `task_manager.ensure_future`. The only change that gave me pause was the handling of `mjs_fut`, whose lifetime is a little tricky since it is potentially passed to yet another task. I think this shows a general weakness in the handling of ownership and lifetimes in between the Job and Worker classes and think a larger refactor can make this less error-prone but is out of scope for this fix. So I'd appreciate an especially scrutinizing look at that piece of the code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12692:1314,refactor,refactor,1314,https://hail.is,https://github.com/hail-is/hail/pull/12692,1,['refactor'],['refactor']
Modifiability,"The bad thing being that it will have log_2 layers rather than log_64 layers, right?. The real issue here is that for tables with one extremely tiny partition (the usual in our tests), this creates 64 nearly no-op jobs, which has unavoidable cost. Is there any number less than 64 that you would accept? Maybe 8?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11784#issuecomment-1105465160:44,layers,layers,44,https://hail.is,https://github.com/hail-is/hail/pull/11784#issuecomment-1105465160,2,['layers'],['layers']
Modifiability,"The conceptual change here is we want to parameterize all batch related tables to have a new job group ID that I've set to **0** for the root job group. We need to make sure all future inserts / updates into the batches table are propagated to the new job groups table. When we create a batch now, we also create the corresponding entries into the job groups and job group parents tables. I chose the root job group to be 0 as I think conceptually, the client should start numbering job groups at 1 and not know there is a hidden root job group being created under the hood. I'm not wedded to this. I tried to check for all the indices that would be needed in my prototype. It's possible I missed one or two, but it's not a big deal to add it later. I don't think we need to test this on a populated database (dev deploy main, submit jobs, then run the migration), but let me know if you think that would be helpful.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13475:41,parameteriz,parameterize,41,https://hail.is,https://github.com/hail-is/hail/pull/13475,1,['parameteriz'],['parameterize']
Modifiability,The config flag batch/tmp_dir is now batch/remote_tmpdir,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13258:4,config,config,4,https://hail.is,https://github.com/hail-is/hail/pull/13258,1,['config'],['config']
Modifiability,The contents of ssl-config are not named after the service in question. See create_certs.py,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9257:20,config,config,20,https://hail.is,https://github.com/hail-is/hail/pull/9257,1,['config'],['config']
Modifiability,The correct thing is to expose the entrypoint in pipeline/batch_client and add it to the config for a job.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7558:89,config,config,89,https://hail.is,https://github.com/hail-is/hail/issues/7558,1,['config'],['config']
Modifiability,"The current main version of the Query Service uses a fresh class loader for every query. This means each driver job and worker job starts with uncompiled classes for any class in Hail. This change uses a shared class loader for all jobs with the same SHA. This enables use of previously JIT'ed Hail classes. This noticeably improves no-op performance from ~8 seconds to ~3 seconds. Most of that remaining 3 seconds is due to Query-on-Batch and Batch, not Query. Currently, Hail generates classes using a counter. When a driver or worker re-uses an old class loader, it would mistakenly re-use classes generated by a previous Hail Query-on-Batch job because they share the same name. This PR avoids that entirely by using a fresh class loader per job for *generated* classes. This PR parameterizes the entire Hail Query system by a class loader. This class loader is passed in from the initiator of the driver or worker job. We could, eventually, re-use class loaders:; - across jobs for a single batch; - across jobs for a single user; - across jobs for a single billing project; - across all jobs. I think the first three are somewhat uncontroversial but we need to fix the class naming problem. The fourth introduces a new security risk. I think we have a lot of performance to squeeze out of QoB before we need to take that step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11212:783,parameteriz,parameterizes,783,https://hail.is,https://github.com/hail-is/hail/pull/11212,1,['parameteriz'],['parameterizes']
Modifiability,"The date time changes added `-target:jvm-1.8` to build.gradle which quietly; broke SBT. It wasn't a problem for me because I wasn't hacking on Hail until; recently. Moreover, [Ensime](https://ensime.github.io) is dead, so I'm switching over to; bloop. This plugin is necessary for SBT to generate bloop configuration files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8683:257,plugin,plugin,257,https://hail.is,https://github.com/hail-is/hail/pull/8683,2,"['config', 'plugin']","['configuration', 'plugin']"
Modifiability,"The diff in `deforestNDArray` is terrible. I just changed; ```; // in Emit[C].emit(); def emitDeforestedNDArray(ir: IR): EmitCode =; deforestNDArray(ir, mb, region, env).emit(mb, coerce[PNDArray](ir.pType)); ; // in Emit[C]; def deforestNDArray(x: IR, mb: EmitMethodBuilder[C], region: Value[Region], env: E): NDArrayEmitter[C] = {; def deforest(nd: IR): NDArrayEmitter[C] = deforestNDArray(nd, mb, region, env); x match { ... }; }; ; // in NDArrayEmitter[C]; def emit(mb: EmitMethodBuilder[C], targetType: PNDArray): EmitCode; ```; to; ```; // in Emit[C].emit(); def emitDeforestedNDArray(ir: IR): EmitCode =; deforestNDArray(ir, mb, region, env); ; // in Emit[C]; def deforestNDArray(x0: IR, mb: EmitMethodBuilder[C], region: StagedRegion, env: E): EmitCode = {; def deforest(x: IR): NDArrayEmitter[C] = {; x match { ... }; }; deforest(x0).emit(mb, coerce[PNDArray](x0.pType), region.code); }; ; // in NDArrayEmitter[C]; def emit(mb: EmitMethodBuilder[C], targetType: PNDArray, region: Value[Region]); ```; Before, `NDArrayEmitter.emit` was always building the NDArray in the region argument to `mb`, no matter what region was used in `deforestNDArray`. This passes the region into `NDArrayEmitter.emit`, and refactors things to better enforce that the same region is used in both parts.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106#issuecomment-665034873:1211,refactor,refactors,1211,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-665034873,1,['refactor'],['refactors']
Modifiability,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:733,layers,layers,733,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358,2,['layers'],['layers']
Modifiability,The environment variable was getting assigned after we setup the fluentd log parameters.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13726:16,variab,variable,16,https://hail.is,https://github.com/hail-is/hail/pull/13726,1,['variab'],['variable']
Modifiability,"The environment variables should be affecting `pytest`, not `cd`. `test_import_bed` just needs to use an `open` that can speak `gs://`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12536:16,variab,variables,16,https://hail.is,https://github.com/hail-is/hail/pull/12536,1,['variab'],['variables']
Modifiability,"The error about a log file is an unfortunate red herring, the main container appears to fail. `/vep/vep` is returning exit code -9 but providing no further debug information. We need to assess who is generating the -9. Based on the contents of `docker/hailgenetics/vep/grch38/95/Dockerfile`, `/vep/vep` appears to be; ```; #!/bin/bash; export PERL5LIB=$PERL5LIB:/vep/ensembl-vep/Plugins/; exec perl /vep/ensembl-vep/vep \""\$@\""""; ```; It seems likely that `/vep/esnembl-vep/vep` is returning exit code -9. We need to determine under what conditions that happens. Main container output:; ```; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 241. Traceback (most recent call last):; File ""/hail-vep/vep.py"", line 218, in <module>; main(action, consequence, tolerate_parse_error, block_size, input_file, output_file, part_id, vep_cmd); File ""/hail-vep/vep.py"", line 199, in main; results = run_vep(vep_cmd, input_file, block_size, consequence, tolerate_parse_error, part_id, os.environ); File ""/hail-vep/vep.py"", line 127, in run_vep; raise ValueError(f'VEP command {vep_cmd} failed with non-zero exit status {proc.returncode}\n'; ValueError: VEP command ['/vep/vep', '--input_file', '/io/input', '--format', 'vcf', '--json', '--everything', '--allele_number', '--no_stats', '--cache', '--offline', '--minimal', '--assembly', 'GRCh38', '--fasta', '/vep_data//homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz', '--plugin', 'LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:/vep_data//gerp_conservation_scores.homo_sapiens.GRCh38.bw",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224:379,Plugin,Plugins,379,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224,6,['Plugin'],['Plugins']
Modifiability,"The first few lines of a hail log look like:; ```; 2019-12-02 13:20:36 Hail: WARN: This Hail JAR was compiled for Spark 2.4.0, running with Spark 2.4.1.; Compatibility is not guaranteed.; 2019-12-02 13:20:36 SparkContext: INFO: Running Spark version 2.4.1; 2019-12-02 13:20:36 SparkContext: INFO: Submitted application: Hail; 2019-12-02 13:20:36 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=//miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jar; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=0; spark.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=local[*]; spark.repl.local.jars=file:///miniconda3/envs/hail/lib/python3.7/site-packages/hail/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; ```. But the hail version string isn't here! That would be helpful. The full one with the hash. Rolled the dice, came up John.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7644:372,config,configuration,372,https://hail.is,https://github.com/hail-is/hail/issues/7644,1,['config'],['configuration']
Modifiability,"The formatting does become a bit much. This is black's preferred rendering:; ```; @app.command(); def deploy(; branch: Annotated[str, typer.Option(""--branch"", ""-b"", help=""Fully-qualified branch, e.g., hail-is/hail:feature"")],; steps: Annotated[; List[str],; typer.Option(""--steps"", ""-s"", help=""Comma or space-separated list of steps to run.""),; ],; excluded_steps: Annotated[; List[str],; typer.Option(; ""--excluded_steps"",; ""-e"",; help=""Comma or space-separated list of steps to forcibly exclude. Use with caution!"",; ),; ],; extra_config: Annotated[; List[str],; typer.Option(; ""--extra-config"",; ""-e"",; help=""Comma or space-separated list of key=value pairs to add as extra config parameters."",; ),; ],; open: Annotated[; bool,; typer.Option(""--open"", ""-o"", help=""Open the deploy batch page in a web browser.""),; ],; ):; pass. ```. We can reduce the noise a bit with aliases:; ```; from typing import Annotated as Ann, List; from typer import Opt. @app.command(); def deploy(; branch: Ann[str, Opt(""--branch"", ""-b"", help=""Fully-qualified branch, e.g., hail-is/hail:feature"")],; steps: Ann[; List[str],; Opt(""--steps"", ""-s"", help=""Comma or space-separated list of steps to run.""),; ],; excluded_steps: Ann[; List[str],; Opt(; ""--excluded_steps"",; ""-e"",; help=""Comma or space-separated list of steps to forcibly exclude. Use with caution!"",; ),; ],; extra_config: Ann[; List[str],; Opt(; ""--extra-config"",; ""-e"",; help=""Comma or space-separated list of key=value pairs to add as extra config parameters."",; ),; ],; open: Ann[; bool,; Opt(""--open"", ""-o"", help=""Open the deploy batch page in a web browser.""),; ],; ):; pass; ```. It seems to me that the benefits of real sub-commands and better dead-option linting is worth the extra noise in the function definition.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13109#issuecomment-1570514400:589,config,config,589,https://hail.is,https://github.com/hail-is/hail/pull/13109#issuecomment-1570514400,4,['config'],['config']
Modifiability,"The goal of this PR is to have all of the JVM container logs available where all the worker logs are. I tagged the entries with ""worker.log"" so they show up with the other worker log entries. However, it's plain text with no timestamp. We can improve the formatting as a separate project. Notice the two entries with ""*"" on the left instead of the normal ""I"". The design choice I made is to have the JVM containers write to a location that is static. We cannot easily change the fluentd configuration dynamically. It requires restarting the daemon which takes 1.5 seconds. Furthermore, the configuration for fluentd is on /etc/ on the host which the batch worker container cannot access. Hence, why I took the approach of specifying it in the startup script at known locations. . Before we merge this, I'd like to confirm that (a) we want these logs and (b) they don't contain any secrets.; <img width=""1585"" alt=""Screenshot 2023-06-16 at 4 06 43 PM"" src=""https://github.com/hail-is/hail/assets/1693348/0ce9f7dc-1188-4c66-ae6f-83fcc3744f95"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13190:487,config,configuration,487,https://hail.is,https://github.com/hail-is/hail/pull/13190,2,['config'],['configuration']
Modifiability,"The idea is to allow the execution on both `SparkBackend` and `ServiceBackend` without code changes, simply switching to the Query service by setting the environment variables `HAIL_QUERY_BACKEND`, `HAIL_BILLING_PROJECT`, and `HAIL_BUCKET`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10189:166,variab,variables,166,https://hail.is,https://github.com/hail-is/hail/pull/10189,1,['variab'],['variables']
Modifiability,"The issue appears to be something in the way spark is configured in this branch. I cannot broadcast successfully new SerilaizableHadoopConfiguration(sc.hadoopConfiguration, inside of LoadVCF. Meaning it works, but the configuration is null. Manually serializing in a test works fine. No issues on master. Minimal example:. ```scala; // LoadVCF, using master's SerializableHadoopConfiguration class ; private val fileInfo: Array[Array[String]] = externalSampleIds.getOrElse {; val shConf = new SerializableHadoopConfiguration(sc.hadoopConfiguration); val localBcFsConf = sc.broadcast(shConf); var results: Array[Array[String]] = Array(); var stuff = sc.parallelize(files, files.length).map { file =>; sc.hadoopConfiguration; }.collect(). results; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-496946496:54,config,configured,54,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-496946496,2,['config'],"['configuration', 'configured']"
Modifiability,"The json output now has ""config"" and ""benchmarks"" top level fields",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6826:25,config,config,25,https://hail.is,https://github.com/hail-is/hail/pull/6826,1,['config'],['config']
Modifiability,"The layers of wtf really seem to have no end here. Hadoop at least *appears* to [include the configuration in the cache key](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java#L3833-L3841) for its FileSystem cache, but it is actually just ignored by the constructor. Ergo, even if you stop the Hail context and try to start a new hail context with a new Hadoop Configuration, you'll get a filesystem configured by the first configuration. I'm looking for a way around this now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12133#issuecomment-1241322443:4,layers,layers,4,https://hail.is,https://github.com/hail-is/hail/pull/12133#issuecomment-1241322443,5,"['Config', 'config', 'layers']","['Configuration', 'configuration', 'configured', 'layers']"
Modifiability,"The main goal of this PR was to remove some of the vector/scalar logic from the BlockMatrixMap2 node, and to support the scalar operations on BlockMatrixMap. I basically accomplished this by taking the cases that are matched on in BlockMatrixMap2 and lifting them into the Simplify rules. The only endpoint that I believe I needed to cover was the BlockMatrix.pyExecute() one; all the others will go through the usual CompileAndEvaluate. There's another part of the PR that fixes the variable bindings, which are currently hard-coded and unchecked. I needed this to construct the right expressions for the IR expressions, so I changed it to handle variable bindings with the rest of our IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7566:484,variab,variable,484,https://hail.is,https://github.com/hail-is/hail/pull/7566,2,['variab'],['variable']
Modifiability,The match on method identity caused mismatches between the variable; stored in loaded across splits.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9947:59,variab,variable,59,https://hail.is,https://github.com/hail-is/hail/pull/9947,1,['variab'],['variable']
Modifiability,"The old bucket did not use uniform access control and also was multi-regional (us). I created a new bucket using the random suffix ger0g which has uniform access control. I also switched the location to us-central1 (not pictured here because that is a variable). I copied all the JARs from `gs://hail-query/jars` to `gs://hail-query-ger0g/jars` using a GCE VM. Again, global-config is not present in our terraform, so I'll have to manually edit that to reflect this new location: `gs://hail-query-ger0g`. The deployment process is:. 1. Edit global-config to reflect new bucket.; 2. Delete batch and batch-driver pods.; 3. Delete old workers. The rollback process (if necessary) is the same. Since this requires wiping the workers, I'll wait for a time when no one is on the cluster to do it. Any users using explicit JAR URLs will need to switch to `gs://hail-query-ger0g/...`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12969:252,variab,variable,252,https://hail.is,https://github.com/hail-is/hail/pull/12969,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"The only real bitrot in the GCP terraform was the leftover `gsuite_organization` variable. This fixes that and also makes the zuliprc secret and ukbb module optional. The extra `fileexists` in the data block for the zuliprc is janky, but functions such as `file` are evaluated before the creation of the resource, so terraform will complain that about the file not existing without that extra guard there. I don't love it but ultimately want these to come from a cloud secret vault so it might look different soon enough anyway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11319:81,variab,variable,81,https://hail.is,https://github.com/hail-is/hail/pull/11319,1,['variab'],['variable']
Modifiability,"The only thing that may be a good change is to add a more descriptive name for variable creation and initialization than `memoize`. Because you're right, it doesn't really memoize. That's outside the scope of this PR however.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8446#issuecomment-608498258:79,variab,variable,79,https://hail.is,https://github.com/hail-is/hail/pull/8446#issuecomment-608498258,1,['variab'],['variable']
Modifiability,"The picture is from asm4s:; - Value[T] is convertible to Code[T] and can be used multiple times: it is a primitive value (constant, variable ref, etc.); - Code[T] can be used once; - Settable[T] extends Value[T] and has a store operation. Changes:; - rename PValue => PCode; - add PValue which is multi-use, PSettable extends PValue; - rename EmitTriplet => EmitCode; - add EmitValue and EmitSettable; - Removed type parameter from PValue and introduced downcast operators. I'm not really happy with either option. Will revisit this again in the future. Changes that are coming:; - add EmitMethodBuilder.newEmit{Local, Field} that return EmitSettables; - Emit.E will become Env[EmitSettable]. The goal here is to rip out jointpoint and ParameterPack. EmitSettables will replace the funtionality of ParameterPack for TypedTriplets (which will go away in favor of EmitTriplet/EmitCode). Removing joinpoint will cause problems when Code[T] are reused, so the Value types must be pushed throughout the codebase. I will put out what infrastructure I can as separate PRs, but I'm having a hard time finding way to do this incrementally.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8263:132,variab,variable,132,https://hail.is,https://github.com/hail-is/hail/pull/8263,3,"['extend', 'variab']","['extends', 'variable']"
Modifiability,"The problem was query was writing the job configuration to the query bucket, but workers only get the user gsa, so they were unable to read the configuration. This worked in the tests because the query and user account are both the test service account. I can remove the query-gsa-key and the hail-query bucket after this goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8937:42,config,configuration,42,https://hail.is,https://github.com/hail-is/hail/pull/8937,2,['config'],['configuration']
Modifiability,"The real change here is changing the preemptible pool config from `preemptible = true` to `spot = true`, but the `spot` config was only available in the new provider which involved a major version upgrade. The only incompatibility was the addition of an explicit `project` input to `google_project_iam_member`, as opposed to picking it up from the provider configuration. Tested just now in my own project. If one wants to apply this change without incurring downtime for preemptible deployments, they should follow the instructions outlined in the [migrating node pools dev-docs](https://github.com/hail-is/hail/blob/main/dev-docs/kubernetes-operations.md#when-using-terraform).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12127:54,config,config,54,https://hail.is,https://github.com/hail-is/hail/pull/12127,3,['config'],"['config', 'configuration']"
Modifiability,The reducer should be parameterized as a keyword-only arg. `filter_missing` should become a keyword-only arg.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7107:22,parameteriz,parameterized,22,https://hail.is,https://github.com/hail-is/hail/issues/7107,1,['parameteriz'],['parameterized']
Modifiability,"The refactor in #14524, ironically to add more testing, accidentally dropped the query arguments when production CI filters by live namespaces. I'll follow up with more testing but CI is currently borked without this",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14549:4,refactor,refactor,4,https://hail.is,https://github.com/hail-is/hail/pull/14549,1,['refactor'],['refactor']
Modifiability,"The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be rebuilt) target is dynamically generated. That target will force a execution of any dependent targets, in the example above, it will force `build` to be exec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130:2369,variab,variable,2369,https://hail.is,https://github.com/hail-is/hail/pull/5130,1,['variab'],['variable']
Modifiability,"The results follow. We seem to get better bandwidth from GCP VMs and/or out of S3. In either case, we aggregate bandwidth substantially decreases at 40k files of size ~100kB. Theoretical egress from the GCP VM I used is 2 GB/s. Ingress is ~3 GB/s. S3 claims as much as ~12 GB/s of aggregate bandwidth. We seem to have room for improvement, but this seems good enough for now. # On AWS, GCS -> S3. | Files | Bytes | Time | Rate |; | ----- | ----- | ---- | ---- |; | 1 | 5.4 GB | 34 seconds | 154.5 MB/s |; | 1 | 42.9 GB | 4 minutes | 161.6 MB/s |; | 200 | 5.4 GB | 35 seconds | 151.1 MB/s |; | 40000 | 5.4 GB | 4 minutes | 22.0 MB/s |. # On GCP, S3 -> GCS. | Files | Bytes | Time | Rate |; | ----- | ----- | ---- | ---- |; | 1 | 5.4 GB | 17 seconds | 304.2 MB/s |; | 1 | 42.9 GB | 3 minutes | 235.5 MB/s |; | 200 | 5.4 GB | 20 seconds | 267.8 MB/s |; | 40000 | 5.4 GB | 6 minutes | 13.3 MB/s |. # machine parsable form; ```; [{'config': 'one',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/one',; 'times': [34.76],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/one'},; {'config': 'some',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/some',; 'times': [35.527],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/some'},; {'config': 'many',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/many',; 'times': [244.154],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/many'},; {'config': 'huge',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/huge',; 'times': [265.719],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/huge'},; {'config': 'one',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/one',; 'times': [17.65],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/one'},; {'config': 'some',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/some',; 'times': [20.048],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/some'},; {'config': 'many',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/many',; 't",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10752#issuecomment-897651697:927,config,config,927,https://hail.is,https://github.com/hail-is/hail/pull/10752#issuecomment-897651697,1,['config'],['config']
Modifiability,The return value of these functions indicates if the containing loop; should wait or if we should immediately re-call the function. This; is intended to be used to allow functions which *know* they have more; work to eagerly invoke themselves again. The use of this variable seems to have been changed to basically always; eagerly re-run during the Azure work. This change restores the original behavior:; 1. Do not wait in job private if we saw 300 records (seems likely there were; 301 or more records in the db).; 2. Do not wait in pool scheduler if we exhaust a user's share. I do not; fully follow the pool scheduler's logic. There might be something; smarter we can do. I think we should really only re-call if we believe; the db contains more ready jobs and we have available cores.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11384:266,variab,variable,266,https://hail.is,https://github.com/hail-is/hail/pull/11384,1,['variab'],['variable']
Modifiability,"The root issue here was that sometimes exc.args[0] was a string and sometimes it was a dict. When it was a string the `in` condition worked fine. When it was a dict, it was looking at the keys of the dict and not finding the error message (which is buried under a few layers). The code was unnecessarily complex. I reworked the yaml printer to be simpler and work for any multiline string. I removed the regular expression that was used to discover the worker batch when the worker jobs were in a different batch from the driver jobs. I remove all specialized debugging information in favor of the general `debug_info` methods on `Batch` and `ServiceBackend`. I also have two clear error cases: if the driver does not write its output file, then something went horribly wrong. We dump all the debug info. If we do not receive valid JSON from the driver, again, something went horribly wrong. We dump all the debug info. The only remaining exceptional case is an error purposely serialized by the QoB driver to us (with or without an error id). In particular, note that we now completely ignore the number of failing or successful jobs. That doesn't matter. If the driver sends us an output file, we use the data found there. If the driver does not send us an output file or sends us an output file without valid JSON, we dump as much debug info as possible. cc: @tpoterba for visibility on your end; cc: @iris-garden because you're in this space (albeit, the bug you're fixing is in the QoB *driver* whereas this is the *client* [nb: *client* is the Python code which starts a batch with a *driver*. A *driver* adds zero or more *worker* jobs to its batch. You're addressing an issue with how the *driver* handles errors from the *workers*. This PR simplifies the logic for how the *client* handles errors from the *driver*.]).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470:268,layers,layers,268,https://hail.is,https://github.com/hail-is/hail/pull/12470,1,['layers'],['layers']
Modifiability,"The semantics of setting a variable in make are kinda weird, example:. ```make; FOO := foo; BAR ?= bar; .PHONY: echo; echo:; 	echo $(FOO) $(BAR); ```. ```sh; $ FOO=baz make; echo foo bar; foo bar; $ make FOO=baz; echo baz bar; baz bar; $ BAR=baz make; echo foo baz; foo baz; ```. This will fix an issue where ci wasn't actually building with spark 3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9556:27,variab,variable,27,https://hail.is,https://github.com/hail-is/hail/pull/9556,1,['variab'],['variable']
Modifiability,"The simpler version is slower, but in the python test, not by a large amount (previous version was, in this test 23.8s or so, although Scala benches may show a larger difference). {""config"": {""cores"": 1, ""version"": ""0.2.28-7888aeb97570"", ""timestamp"": ""2019-12-04 02:07:13.182303"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [28.613776744999996, 28.361242108, 28.481231283]}]}. So 20% slower. I would prefer to use longs, because it doesn't feel right to me to leave performance on the table, however I'm ok with this tradeoff if you find it aligns with your goals better. ; - Regarding longs, to deal with alignment: right now we assume we're int aligned. To read longs, could we read the first 4 bytes as an int, then switch to longs, then do bits for the remaining length. Should be as terse. edit: I propose to put the unstaged version for a later time, but can do now as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7646#issuecomment-561510056:182,config,config,182,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561510056,2,['config'],['config']
Modifiability,"The terraform plan for `gcp-broad` has 4 changes after appropriately importing. The bucket, ci_config, and hail_ci_0_1_github_oauth_token are actually unchanged. I do not know why they are listed as a change. That secret currently has three files: hail-ci-0-1.key, oauth-token, user1, user2. AFAICT, only `user1` is used, so the new secret only has `user1`. ```; # module.ci[0].google_storage_bucket.bucket will be updated in-place; ~ resource ""google_storage_bucket"" ""bucket"" {; id = ""hail-ci-bpk3h""; # Warning: this attribute value will be marked as sensitive and will not; # display in UI output after applying this change. The value is unchanged.; ~ location = (sensitive value); name = ""hail-ci-bpk3h""; # Warning: this attribute value will be marked as sensitive and will not; # display in UI output after applying this change. The value is unchanged.; ~ storage_class = (sensitive value); # (8 unchanged attributes hidden). # (2 unchanged blocks hidden); }. # module.ci[0].kubernetes_secret.ci_config will be updated in-place; ~ resource ""kubernetes_secret"" ""ci_config"" {; id = ""default/ci-config""; # (3 unchanged attributes hidden). # (1 unchanged block hidden); }. # module.ci[0].kubernetes_secret.hail_ci_0_1_github_oauth_token will be updated in-place; ~ resource ""kubernetes_secret"" ""hail_ci_0_1_github_oauth_token"" {; id = ""default/hail-ci-0-1-github-oauth-token""; # (3 unchanged attributes hidden). # (1 unchanged block hidden); }. # module.ci[0].kubernetes_secret.hail_ci_0_1_service_account_key will be updated in-place; ~ resource ""kubernetes_secret"" ""hail_ci_0_1_service_account_key"" {; ~ data = (sensitive value); id = ""default/hail-ci-0-1-service-account-key""; # (2 unchanged attributes hidden). # (1 unchanged block hidden); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12882#issuecomment-1507162756:1096,config,config,1096,https://hail.is,https://github.com/hail-is/hail/pull/12882#issuecomment-1507162756,1,['config'],['config']
Modifiability,"The terraform sets `gcp_project` in the global-config not `project`. A few weeks ago I did a manual copy & rename of `project -> gcp_project`, `zone -> gcp_zone` etc. This is the follow-through for that conversion.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11036:47,config,config,47,https://hail.is,https://github.com/hail-is/hail/pull/11036,1,['config'],['config']
Modifiability,"The type of this RDD is rather complicated because we prepend a String to; an RDD of a universally quantified variable. `writeTextTable` works with any subtypes of AnyRef because it only requires; that the object defines `toString`. Ergo, we don't actually need to know; anything about the type parameter of the RDD. The explicit annotation; that I've added prevents Scala from inferring a complex type that cannot; be expressed without `forSome`. Such types trigger a warning from the; Scala compiler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2705:110,variab,variable,110,https://hail.is,https://github.com/hail-is/hail/pull/2705,1,['variab'],['variable']
Modifiability,"The values of the global config are the same across namespaces, but it does feel more correct to use the `global-config` from the namespace you're targeting than the one in production. This should also enable `make` deploying into a dev namespace without having any permissions for `default`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13338:25,config,config,25,https://hail.is,https://github.com/hail-is/hail/pull/13338,2,['config'],['config']
Modifiability,The workload identity config is already enabled. Using TF to do that would've destroyed the cluster for reasons unknown.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13017#issuecomment-1540864886:22,config,config,22,https://hail.is,https://github.com/hail-is/hail/pull/13017#issuecomment-1540864886,1,['config'],['config']
Modifiability,"There are many global config fields that CI needs in order to template build.yaml jobs that are threaded through to CI with environment variables. However, these variables are never actually used by CI and they introduce some needless dependencies to run CI (you need a GCP_PROJECT, for example, even though CI doesn't care at all). Instead of setting specific environment variables for each field that build.yaml steps need, I instead mount the global-config (read-only) to the CI container and read in the whole thing. This does potentially expose more variables to the build.yaml environment than there were previously, but I argue that none of those should be sensitive anyway or maybe don't belong in the global-config (which shouldn't be sensitive). This in part makes the process of adding global config fields easier, since right now you need separate PRs to 1) introduce the field to CI and then 2) use it in a new build.yaml step. It also makes the CI deployment.yaml cloud-agnostic. . cc @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10911:22,config,config,22,https://hail.is,https://github.com/hail-is/hail/pull/10911,8,"['config', 'variab']","['config', 'variables']"
Modifiability,"There are many of occurences of k8s templating of variables like GCP project or domain that never change and exist in the global config. The process of adding a field to the global config sometimes then requires adding it to `config.mk`, then the jinja of a deployment template that needs it, and then templating that in the deployment.yaml. These are nearly always environment variables (but not always), which can and sometimes are read from kubernetes secrets. This is a sweep of every such occurence I could find so that these variables are just read directly from the k8s secret. Though it adds lines to the deployments, it reduces the complexity of our Makefile process and makes adding variables to the global config much easier. This also *dramatically* reduces the dependencies on `config.mk` and most of its variables. I think I'll address config.mk specifically in another PR, but I believe keeping it from ballooning with multi-cloud configuration will be valuable in keeping the complexity of our build/deployment system in check.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10869:50,variab,variables,50,https://hail.is,https://github.com/hail-is/hail/pull/10869,12,"['config', 'variab']","['config', 'configuration', 'variables']"
Modifiability,"There are many things wrong here. The Hadoop configuration is not copied per HadoopRDD operation. Proof:. ```; >>> import hail as hl; >>> hl.init(min_block_size=0); >>> t = hl.import_table('test.tsv.bgz', impute=True, min_partitions=8); >>> t.n_partitions(); 8; >>> t = hl.import_table('test-bgz.tsv.gz', impute=True, min_partitions=8); >>> t.n_partitions(); 1; ```. where `test-bgz.tsv.gz` is a bgz in gz's clothing. This is compounded by the fact that SparkContext.hadoopFile is not invoked until TableIR.execute is run making HailContext.forceBGZ() completely ineffective. One option is turning on spark.hadoop.cloneConf, that appears to clone the Hadoop configuration (to avoid some multithreading issues) although the docs don't recommend it due to ""performance regressions"". I haven't tested it. The other option is stop using the Hadoop stuff so we can pass state into the file loaders. Doing that for text files/line splitting is a bit nasty, but it would mean we could properly fix this gz/bgz business once and for all (look at the GZ header to see if it is block gzip'ed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3861:45,config,configuration,45,https://hail.is,https://github.com/hail-is/hail/issues/3861,2,['config'],['configuration']
Modifiability,"There remain a couple questions that a solution should answer:; 1. TCP or Unix Domain Socket? Current consensus feels that TCP is a reasonable and more portable way to go (allows for backend deployment over the web/in K8s for example); 2. Should we use just TCP or also use HTTP? If the Java backends can multiplex requests, HTTP sounds favorable, otherwise it's unclear to me what advantages it would give us over TCP + JSON. Ergonomically HTTP might be easier, but one tends have certain default expectations of HTTP servers (I would *assume* an HTTP server should be able to serve requests concurrently, are we just going to use all `POST`s?, etc.). Either way this feels like a minor adjustment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13756#issuecomment-1743481491:152,portab,portable,152,https://hail.is,https://github.com/hail-is/hail/issues/13756#issuecomment-1743481491,1,['portab'],['portable']
Modifiability,"There was a bug converting Call to/from Java, in this line:. if annotation:. A call is represented by a 0 integer which is False in Python. I; changed all of these to `if annotation is not None`. Also, there was a bit of confusion in the call interface inherited; from genotype. A not call is just a missing Call value (in python); and a null Call = java.lang.Integer (in Java).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2474:253,inherit,inherited,253,https://hail.is,https://github.com/hail-is/hail/pull/2474,1,['inherit'],['inherited']
Modifiability,There was an extra check in the where statement for the token matching on the jobs billing table. The jobs billing table is the only one not parameterized with a token.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12942:141,parameteriz,parameterized,141,https://hail.is,https://github.com/hail-is/hail/pull/12942,1,['parameteriz'],['parameterized']
Modifiability,"There's a few things happening here:. ### Node pool updates through terraform; I extended the node pool update documentation with how to deal with terraform-managed node pools. This is what I did on Azure and worked fine. The only real change in terraform other than changing the machine type is making the node pool name configurable to adhere to the naming guidelines and allow us to do a rolling migration. ### Updated the kubernetes and azurerm providers; I updated the azurerm provider without thinking much about it and even though it's a minor version had some breaking changes that after a half-successful `apply` made it hard to downgrade. So I decided just to appease the breaking change and leave us at the new version, which is what all the `blob_properties` changes are for. They are in no way related to the node pools. ### Troubleshooting; I added a section for a bug that I've seen a couple of times (and encountered again today) but never documented how I got around it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11636:81,extend,extended,81,https://hail.is,https://github.com/hail-is/hail/pull/11636,2,"['config', 'extend']","['configurable', 'extended']"
Modifiability,There's a kind of unrelated thing: Fix reading of configuration information to not ignore a hailctl configuration value of `''` . The big change is to introduce 3 progress bar systems:; 1. SimpleRichProgressBar. One progress bar active at a time.; 2. RichProgressBar. More than one progress bar active at a time.; 3. BatchProgressBar. Same as RichProgressBar but with default columns good for monitoring 1 or more Hail Batch batches.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12346:50,config,configuration,50,https://hail.is,https://github.com/hail-is/hail/pull/12346,2,['config'],['configuration']
Modifiability,"There's a pretty sizeable amount of changes that had to be made manually, so the merge conflicts might be a lot more painful with this one than https://github.com/hail-is/hail/pull/14119; not sure if that's a case for delaying this or not. I think many of those manual changes could also be made using the `--unsafe-fixes` flag to `ruff`, but it seemed better to try doing them manually to try and avoid introducing too many issues. The lint config changes, automatic changes from `ruff`, and manual changes required to make the `ruff` check pass are split into separate commits, so hopefully that will make the review easier",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14128#issuecomment-1883315070:442,config,config,442,https://hail.is,https://github.com/hail-is/hail/pull/14128#issuecomment-1883315070,1,['config'],['config']
Modifiability,"There's something wrong here and I think it has to do with how I'm passing around the hadoop configuration. Closing this since I'm in the process of changing how spark is called, and I'll reopen once I've fixed this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5326#issuecomment-467585906:93,config,configuration,93,https://hail.is,https://github.com/hail-is/hail/pull/5326#issuecomment-467585906,1,['config'],['configuration']
Modifiability,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:128,config,config,128,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393,2,['config'],['config']
Modifiability,"These are the alterations that Jackie and I made to be able to add a developer account to a dev namespace. If the namespace isn't default then it grabs the credentials for the user from default instead of creating a new credential. I don't like that the extra config lives on the branch but this is how it is otherwise supported in bootstrap.py, and ultimately we should just use the auth account to hit the auth endpoint and delete the `create_initial_account` script entirely.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11115:260,config,config,260,https://hail.is,https://github.com/hail-is/hail/pull/11115,1,['config'],['config']
Modifiability,"These changes are a prerequisite for introducing a mysql DB pod for every test and dev namespace. The crux of such a change is any CreateDatabase steps should use the `database-server-config` from *its own namespace* (which will come in the PR that uses this step) and not from default. There's no cleanup step required because this will be used to create DBs inside the namespace for the pipeline, so resources will get cleaned up with the namespace. The other changes in this step bring the configuration for `dev` scopes closer to that of `test` scopes, because creation of test databases should really just be idempotent and there shouldn't be a difference between deploying a database in dev and test. I would have deferred making the changes to the `dev` scope except dev deploy was the most practical way for me to test this change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13022:184,config,config,184,https://hail.is,https://github.com/hail-is/hail/pull/13022,2,['config'],"['config', 'configuration']"
Modifiability,"These changes are mostly because mypy now needs all `None` parameters to be explicit optionals, whereas before it would let something like `def foo(arg: str = None)` slide. The `type: ignore` on the azure client call is an azure library bug, I looked into the source for that function and there's a comment saying that the input is an `Optional[int]` but the type stubs that mypy is picking up say `int`. the `check_untyped_defs` config flag tells mypy to typecheck the bodies of functions that do not have a type signature, which apparently it wasn't doing on its own.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12447:430,config,config,430,https://hail.is,https://github.com/hail-is/hail/pull/12447,1,['config'],['config']
Modifiability,"These changes enable hailctl clusters to work correctly in an Broad GCP Security Best Practices configured project, with minor hail-specific set-up. See further details in team chat.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7978:96,config,configured,96,https://hail.is,https://github.com/hail-is/hail/pull/7978,1,['config'],['configured']
Modifiability,"These improvements would help for QC on trio data, and for exploring new variant quality models exploiting inheritance. .mendelI (nError per individual) should be an integer-valued sample annotation. We might also be interested in nError per trio, as sample annotation on child of trio. .mendelL (nError per locus, really variant) should be an integer-valued variant annotation. .mendel lists all errors. Each error has a unique (trio, variant). It's natural to ask for all variants where a trio has errors, and also for all trios in which a variant has an error. Perhaps the list of errors per variant should be a variant annotation from which all other annotations are derived. Note that if variants are distributed in intervals, this annotation won't be well-balanced as difficult-to-sequence regions will have far more error. .mendalF (nError per nuclear family) could be sample-keyed by the mother",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/597:107,inherit,inheritance,107,https://hail.is,https://github.com/hail-is/hail/issues/597,1,['inherit'],['inheritance']
Modifiability,"These top-shelf methods make applying the LinearMixedModel class as easy as 1-2-3:; 1: create model from matrix table expressions (and possibly kinship matrix) ; 2: fit model; 3: test each row of matrix table using fit model. E.g., using markers as random effects:; ```; mt0 = dataset.filter_rows(mt.use_as_marker); model, _ = hl.linear_mixed_model(y=mt0.height, x=[1, mt0.sex], z_t=mt0.n_alt_alleles(), p_path='p.bm'); model.fit(); mt = hl.linear_mixed_regression(mt.n_alt_alleles(), model); ```. E.g., using any ndarray kinship matrix:; ```; model, _ = hl.linear_mixed_model(y=mt0.height, x=[1, mt0.sex], k, p_path='p.bm'); model.fit(); mt = hl.linear_mixed_regression(mt.n_alt_alleles(), model); ```. This required smaller changes to LinearMixedRegression class, particularly adding `p_path` as a member variable and to all constructors. I've also modified and run `doctest_write_data.py` which rebuilt the datasets, accounting for the large binary file count (I can factor this out as it's own pre-PR if you prefer). I renamed `from_mixed_effects` to `from_random_effects` since a mixed model has fixed and random effects, and `z` is the random ones.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4191:807,variab,variable,807,https://hail.is,https://github.com/hail-is/hail/pull/4191,1,['variab'],['variable']
Modifiability,They use the deprecated AST interface. Rewrite in terms of Table interface.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3738:39,Rewrite,Rewrite,39,https://hail.is,https://github.com/hail-is/hail/pull/3738,1,['Rewrite'],['Rewrite']
Modifiability,"This PR _explicitly_ changes the defaults. It optionally accepts the old values in an _attempt_ to not break current users. At worst, this would require another `hailctl auth login`. This is the patch I _wanted_ to write. ```patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 't",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:458,config,config,458,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,2,['config'],['config']
Modifiability,"This PR adds `utils/StackSafe.scala`, which contains generic tools for writing stack safe code. To illustrate its use, I've converted `NormalizeNames` and `RewriteBottomUp` to be stack safe. This approach optimizes for the minimal possible change to existing code to make it stack safe. I originally expected this to have mediocre performance, and designed this to have optimization opportunities--requiring more substantial rewrites--where we found it was necessary. In a follow up PR, I converted the IR parser to be stack safe. In benchmarking that, I'm not able to see any performance penalty (if anything, the stack safe version looks slightly faster, which is probably just noise). So it's possible this will perform well enough as is, but we can keep an eye on it as we convert more passes. The basic idea is to rewrite functions that can be called recursively (directly or indirectly through a path of mutually recursive functions) from `f: (...) => A` to `f: (...) => StackFrame[A]`. Where the former evaluates, executing all recursive calls, and then returns the `A` result, the later returns a description of the work to be done before making any recursive calls. The method `StackFrame[A].run(): A` executes that description in a non-recursive loop. `StackFrame` is a monad, implementing `map` and `flatMap`, which allows the `for` syntactic sugar to be used. When a method makes several recursive calls, this can be significantly more readable. The public api is small. There are the free functions; ```scala; def done[A](result: A): StackFrame[A]; def call[A](body: => StackFrame[A]): StackFrame[A]; ```; and the methods; ```scala; abstract class StackFrame[A] {; def flatMap[B](f: A => StackFrame[B]): StackFrame[B]; def map[B](f: A => B): StackFrame[B] = flatMap(a => Done(f(a))); def run(): A; }; ```; `done` is basically the return statement. `call` is very important: it wraps a recursive call in a thunk, so that returning a `StackFrame` doesn't require descending all the way to t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9320:156,Rewrite,RewriteBottomUp,156,https://hail.is,https://github.com/hail-is/hail/pull/9320,3,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite', 'rewrites--where']"
Modifiability,"This PR adds a new column to the `resources` table (resource_id) as well as to the `attempt_resources` table. The `resource_id` represents the mapping from the string resource name to an integer identifier. To make this migration work, we first need to add the new columns to the corresponding tables along with a new trigger. The trigger makes sure any writes that occur after we start populating the older values will have the new resource_ids filled into the `attempt_resources` table. I decided to not convert the `attempt_{batch, job,billing_project}_resources` tables to have the resource_id because we'll drop them anyways in the future and it was adding 75% more time to the migration. My original script had the updates happening to all tables, hence why there's overkill for the way it's structured. I can try and refactor the code back to the way it was before if you'd like. The code to insert into the `attempt_resources` table in `job.py` needs to insert both the resource and the resource_id because we do not want to rely on the new trigger for adding the resource_id to the record so we can remove the resource column later on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12028:824,refactor,refactor,824,https://hail.is,https://github.com/hail-is/hail/pull/12028,1,['refactor'],['refactor']
Modifiability,"This PR adds environment variable options to skip tests requiring plink/Rscript executables. - If `HAIL_TEST_SKIP_PLINK` is set, skip tests requiring the `plink` binary.; - If `HAIL_TEST_SKIP_R` is set, skip tests requiring `RScript`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5123:25,variab,variable,25,https://hail.is,https://github.com/hail-is/hail/pull/5123,1,['variab'],['variable']
Modifiability,"This PR adds the job groups functionality as described in this [RFC](https://github.com/hail-is/hail-rfcs/pull/5) to the Batch backend and `hailtop.batch_client`. This includes supporting nested job groups up to a maximum depth of 5. Note, that none of these changes are user-facing yet (hence no change log here). The PRs that came before this one:; - #13475 ; - #13487 ; - #13810 (note that this database migration required a shutdown). Subsequent PRs will need to implement the following:; - Querying job groups with the flexible query language (v2); - Implementing job groups in the Scala Client for QoB; - Using job groups in QoB with `cancel_after_n_failures=1` for all new stages of worker jobs; - UI functionality to page and sort through job groups; - A new `hailtop.batch` interface for users to define and work with Job Groups. A couple of nuances in the implementation came up that I also tried to articulate in the RFC:; 1. A root job group with ID = 0 does not belong to an update (""update_id"" IS NULL). This means that any checks that look for ""committed"" job groups need to do `(batch_updates.committed OR job_groups.job_group_id = %s)` where ""%s"" is the ROOT_JOB_GROUP_ID.; 2. When job groups are cancelled, only the specific job group that was cancelled is inserted into `job_groups_cancelled`. This table does **NOT** contain all transitive job groups that were also cancelled indirectly. The reason for this is we cannot guarantee that a user wouldn't have millions of job groups and we can't insert millions of records inside a single SQL stored procedure. Now, any query on the driver / front_end must look up the tree and see if any parent has been cancelled. This code looks similar to the code below [1].; 3. There used to be `DELETE FROM` statements in `commit_batch_update` and `commit_batch` that cleaned up old records that were no longer used in `job_group_inst_coll_cancellable_resources` and `job_groups_inst_coll_staging`. This cleanup now occurs in a periodic loop on",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14282:524,flexible,flexible,524,https://hail.is,https://github.com/hail-is/hail/pull/14282,1,['flexible'],['flexible']
Modifiability,"This PR and comments like these:. ```; // head, next, and hasNext are not meant to be used directly, only to enable; // FlipbookIterator to be used where an Iterator is expected.; ```. make me think FlipbookIterator shouldn't extend Iterator, but should have an explicit `toIterator` method.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3111#issuecomment-371883548:226,extend,extend,226,https://hail.is,https://github.com/hail-is/hail/pull/3111#issuecomment-371883548,1,['extend'],['extend']
Modifiability,"This PR attempts to simplify the use of TLS and HTTP(S) in Hail. The big changes; are in `hail/python/hailtop`. In particular I removed several functions with; confusingly overlapping functionality in `tls.py`. Instead, we now have three; functions:. - `internal_server_ssl_context`; - `internal_client_ssl_context`; - `external_client_ssl_context`. The client context is configured to seek certificates from its peers. Both; internal contexts load the Hail certificate chain specified in the Hail SSL; Config. The external client context does not load the hail certificate chain. I intend all Hail's HTTP(S) requests to use `httpx.py` (so named to not conflict; with modules named `http`). Again, I have simplified the landscape. We now have; two functions:. - `httpx.client_session`: The constructor for all asynchronous, HTTPS client; sessions.; - `httpx.blocking_client_session`: The constructor for all synchronous, HTTPS; client sessions. Both sessions have the exact same configuration parameters. The API is exactly; the same except the blocking client session replaces asynchronous methods with; synchronous ones. Both sessions accept the `aiohttp.ClientSession` constructor parameters. They; support one new parameter and modify the behavior of one old parameter.; - `retry_transient`: when set to `True` this parameter will retry all transient; errors in all requests made by this session. This defaults to `True`.; - `raise_for_status`: this parameter now defaults to `True` and includes the; response body text in the error message. Both; Both parameters may be overridden on a per-request basis. - `httpx.ResponseManager` and `httpx.ClientSession` work together to enable; `retry_transient` and `raise_for_status`. Aiohttp has this unusual structure where; all the request methods are synchronous but they return an object that is both; awaitable and an async context manager. I mirror their structure exactly. The; `httpx.ResponseManager` is both awaitable and an async context manager.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9554:372,config,configured,372,https://hail.is,https://github.com/hail-is/hail/pull/9554,3,"['Config', 'config']","['Config', 'configuration', 'configured']"
Modifiability,"This PR changes our vep init scripts to pull from the new `hail-us-vep` google bucket, which is requester pays. . I realized while doing this that my previous PR (#8253) really had nothing to do with supporting this, since that really just sets the configuration for the Google Cloud / Hadoop connector, which is not how we get vep data. I tried to enforce the rules from those command line arguments anyway by rejecting `--vep` flag if they don't specify they're ok with requester pays and manually checking that `hail-us-vep` was in their approved bucket list. But if a user was to specify the init scripts using `gcloud dataproc` and didn't go through `hailctl`, there'd be no catch to check if they were ok with requester pays. Perhaps there is some way we could set environment variables on the dataproc machines based on the `--requester-pays-allow....` flags and use those in the init scripts. I also expanded `make test-dataproc` to test with a GRCh38 cluster as well, as we use separate scripts to make them and I'm uncomfortable with only testing one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8268:249,config,configuration,249,https://hail.is,https://github.com/hail-is/hail/pull/8268,2,"['config', 'variab']","['configuration', 'variables']"
Modifiability,"This PR enables shuffling in the service. It is stacked on several other PRs, so look only at the; most recent commit. Some highlights:; - Open the public network back up. We should probably make query jobs special so that they can; access the internal network. To do that, batch would need to accept a ""acting on behalf of"" user; account: Query submits the job using its account ""acting on behalf of"" the user. Batch allows; query to use the private network, but for all other purposes, the job is owned by the user. - Allow public access to some the `gcr.io/hail-vdc/query` Docker image. - Automatically rewrite uses of `hailgenetics/` Docker images to their `gcr.io` equivalents. - Move `deploy_address` above `deploy_query` so that query can depend on address (necessary for; shufles). - Fix logging configuration. Services team wants all logs all the time to go to stdout. - Implement lowerDistributedSort using the shuffler. - Allow shuffle ids to be encoded so they can be used in `Literal`.; Unified Split",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9848:606,rewrite,rewrite,606,https://hail.is,https://github.com/hail-is/hail/pull/9848,2,"['config', 'rewrite']","['configuration', 'rewrite']"
Modifiability,"This PR extends #1902, merge that one first. Now, 50% of the time a type is a Scalar type rather than a recursive one. Additionally, this PR adds a generator for pairs of types and values of that type. Here's a few samples:; ```scala; t Struct{; W: Call,; GL: Genotype,; nsfJ: Long,; tiX0pLA: Int,; B7vWAW: Call,; a6_mm: Boolean,; AGS1Eh: Interval,; tqbHQbDv: Dict[AltAllele, Long],; HUh: Set[Double],; Xgb26Wlws: Double,; qe_XlLJt7_X: Dict[Double, Int],; U: Array[Call],; j: Double; }; v [ 1; , 5/6:.:.:.:GP=0.0,0.0,6.103515625E-4,0.0,0.0,0.0,0.0,0.00286865234375,0.0,0.058441162109375,0.0,0.0,0.0,0.00567626953125,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.004730224609375,0.153839111328125,0.0,0.0,0.0,0.704742431640625,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.018402099609375,0.050689697265625,0.0,0.0,0.0,0.0,0.0,0.0,376902925948550232; , 2147483647; , 1; , true; , 15:396751431-18:1945111790; , Map(GT/C -> -9223372036854775808, GC/TGTG -> 0, GTCAC/AC -> 1, TCG/G -> -2853243060319614448, TTTG/G -> 0); , Set(0.0, 1.2590508536333097E308, -26.66700965052354, -1.0, 1.7976931348623157E308); , -44.18866673875504; , Map(4.9E-324 -> 1069254047, -29.21881886290265 -> 0, -7.511481628119398E307 -> -51783790, 78.3075905923555 -> -1679218199, -1.0 -> -1476797686, 66.69344244874847 -> -2147483648, -74.87563451361888 -> -50, 8.529797881337316E306 -> -38); , WrappedArray(2, 2, null, 2, 2, 1); , 1.7044473544408425E308; ]; ```; ```scala; t Struct{; jHUkH: Interval,; c: Struct{; EZyb77: Boolean,; qckA6k: Empty; },; K: Interval,; X: Locus,; BuujVaardN: Call,; drTH1J: Set[Locus],; DJL9uj7D: Variant,; vwuq: Set[Int],; cKAObAm1oh7: Boolean,; FqhLLOlV4p: Struct{; Ri631ZK2TiA: Empty; },; vtQ: Set[String],; HzHvw: Locus,; g: Double,; inwJHuBmLUM: Boolean,; Q: Float,; i: Genotype,; q: Float,; p_Wmn2Q: Variant,; Y6foXEa7F: Dict[Set[Float], Double],; SuXouO__uX: Int,; hrfM: Locus,; k: Variant,; V1WzY: Struct{; xWj: Struct{; bg: AltAllele; }; },; L3Ol_: Call,; Bmt: Variant,; EExpF_H: Struct{; DyAZQ1pL: Empty; },; JS",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1903:8,extend,extends,8,https://hail.is,https://github.com/hail-is/hail/pull/1903,1,['extend'],['extends']
Modifiability,This PR extends a previous PR that added g2-standard-4 machines into batch by adding the full machine family. The code adds a new Accelerator Resource that takes the number of gpus.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14519:8,extend,extends,8,https://hail.is,https://github.com/hail-is/hail/pull/14519,1,['extend'],['extends']
Modifiability,This PR extends environment variable options to skip Scala tests requiring plink/Rscript executables.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5126:8,extend,extends,8,https://hail.is,https://github.com/hail-is/hail/pull/5126,2,"['extend', 'variab']","['extends', 'variable']"
Modifiability,"This PR incorporates @cseed 's changes from #3477, brings everything up to date with master, and adds/fixes the following:; - added a test for linreg with no covariates against R, and deleted old `test_linear_regression_with_no_cov` since that still had intercept.; - extended Skat to work without covariates and added test that it runs, but it’s hard to test result against R given that the latter fails with no covariates: `Error in solve.default(t(X1) %*% X1) : 'a' is 0-diml`. The result look ""reasonable"" to me.; - added req of at least one covariate for logreg in doc and code. It's going to be painful to get logistic to take no covariates, we can always come back to it if priority goes up. Related fun breeze behavior: `a(::, *) *:* b` with `a` an `(n, 0)` matrix and `b` an `n`-vector has dimensions `(0, 0)`.; - removed default value of empty list for `covariate`, both to help signal users to consider putting in the intercept (pipelines currently using intercept only with default empty `[]` will break) and because empty is not currently valid for logreg.; - noted in docs that intercept must be included explicitly.; - added comment of R code against which linreg and logreg are testing",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4067:268,extend,extended,268,https://hail.is,https://github.com/hail-is/hail/pull/4067,1,['extend'],['extended']
Modifiability,"This PR introduces a new type `Name` for representing bound variables in the IR, replacing `String`. For now, it is just an `AnyVal` wrapper around `String`, but in the future I would like to take advantage of the new type. For example, I'd like to:; * change equality of `Name` from string comparison to comparing object identity with `eq`. That way `freshName` becomes just `new Name()`, with stronger guarantees that the new name doesn't occur anywhere in the current IR, without needing to maintain global state as we do now.; * get rid of `NormalizeNames`, instead enforcing the global uniqueness of names as a basic invariant of the IR (typecheck could also check this invariant); * keep a string in the `Name`, but no longer require it to be unique. Instead it's just a suggestion for how to show the name in printouts, adding a uniqueifying suffix as needed. With `NormalizeNames` gone, this would let us preserve meaningful variable names further in the lowering pipeline.; * possibly keep other state in the `Name`, for example to allow a more efficient implementation of environments, similar to the `mark` state on `BaseIR`. This is obviously a large change, but there are only a few conceptual pieces (appologies for not managing to separate these out):; * attempt to minimize the number of locations in which the `Name` constructor is called, to make future refactorings easier; * add `freshName()`, which just wraps `genUID()`, returning a `Name`; * convert IR construction to use the convenience methods in `ir.package`, which take scala lambdas to represent blocks with bound variables, instead of manually creating new variable names; * replace uses of the magic constant variable names (`row`, `va`, `sa`, `g`, `global`) with constants (`TableIR.{rowName, globalName}`, `MatrixIR.{rowName, colName, entryName, globalName}`); * the above changes modified the names we use for bound variables in many places. That shouldn't matter, but it cought a couple bugs where it did.; * `Normal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14547:60,variab,variables,60,https://hail.is,https://github.com/hail-is/hail/pull/14547,2,['variab'],"['variable', 'variables']"
Modifiability,"This PR is a step towards a general picture for generating debugging information in bytecode. The general picture is to write a sequence of files, each corresponding to a certain point in the compile pipeline, where each line in each file includes a line number pointing to the line in the previous file from which this line was derived. (We may eventually want richer source information than just a single line number, like a range or list of ranges.) The top of each file has the file name of the previous printout, which is the target of all line numbers in the file. We could in the future also print a list of transformations that were applied to get from the previous printed state to this one. The idea to implement this picture is simple. Each IR node stores a line number in a mutable variable. When we want to generate a printed checkpoint, we walk the IR, printing a representation of each node, including the stored line number, and then overwriting the node's line number with the current line count of the file being written to. Some work will be required to preserve this source information in all IR transformations. This PR implements this idea in lir only. If the `HAIL_WRITE_IR_FILES` environment variable is set, it is hardcoded to print the lir after the first `SimplifyControl` (because before that is very hard to read), and after method splitting right before emitting bytecode. It also prints out the class files themselves. Longer term we'll want to be able to control which points in the compilation get printed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9521:794,variab,variable,794,https://hail.is,https://github.com/hail-is/hail/pull/9521,2,['variab'],['variable']
Modifiability,"This PR is in @catoverdrive path toward backend refactor. I would like to get this in before any further changes occur, 2nd conflict since this was opened & passing all tests. @catoverdrive Can you help me understand why AddHadoopConfiguration went away? Its body appears inlined in GetHadoopConfiguration. As a result the filesystem object (hConf) is no longer passed around, which is moving things in the opposite direction of future PRs related to this one, which will explicitly pass FS objects to all methods that perform file system operations. . edit: The AddHadoopConfiguration default implementation (on the trait didn't go away, but the overriding implementation did, resulting in an apparent noop, and maybe a potential bug, although I don't understand this portion of the codebase as well as I should yet. ```scala; trait FunctionWithHadoopConfiguration {; def addHadoopConfiguration(hConf: SerializableHadoopConfiguration): Unit; }. // No overriding addHadoopConfiguration implementation; def getHadoopConfiguration: Code[SerializableHadoopConfiguration] = {; if (_hconf == null) {; cn.interfaces.asInstanceOf[java.util.List[String]].add(typeInfo[FunctionWithFS].iname); val confField = newField[FS]; val mb = new EmitMethodBuilder(this, ""addHadoopConfiguration"", Array(typeInfo[SerializableHadoopConfiguration]), typeInfo[Unit]); methods.append(mb); mb.emit(confField := mb.getArg[SerializableHadoopConfiguration](1)); _hconf = HailContext.sHadoopConf. def resultWithIndex(print: Option[PrintWriter] = None): Int => F = {; if (localHConf != null); f.asInstanceOf[FunctionWithHadoopConfiguration].addHadoopConfiguration(localHConf); ```. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6263#issuecomment-500879573:48,refactor,refactor,48,https://hail.is,https://github.com/hail-is/hail/pull/6263#issuecomment-500879573,1,['refactor'],['refactor']
Modifiability,"This PR is to enable `hail-az;` file references to contain SAS tokens to enable bearer-auth style file access to Azure storage. Basic summary of the changes:; 	- Update `AzureAsyncFS` url parsing function to look for and separate out a SAS-token-like query string. Note: made fairly specific to SAS tokens - generic detection of query string syntax interferes with glob support and '?' characters in file names; 	- Added `generate_sas_token` convenience function to `AzureAsyncFS`. Adds new `azure-mgmt-storage` package requirement.; 	- Updated `AzureAsyncFS` to use `(account, credential)` tuple as internal `BlobServiceClient` cache key; 	- Updated `AzureAsyncFSURL` and `AzureFileListEntry` to track the token separately from the name, and extend the base classes to allow returning url with or without a token ; 	- Update `RouterFS.ls` function and associated `listfiles` function to allow for trailing query strings during path traversal ; 	- Change to existing behavior: `LocalAsyncFSURL.__str__`no longer returns 'file:' prefix. Done to make `str()` output be appropriate for input to `fs` functions across all subclasses; 	- Updated `inter_cloud/test_fs.py` to generically use query-string-friendly file path building functions; - Updated InputResource to not include the SAS token as part of the destination file name . `test_fs.py` has been updated to respect the new model, where it is no longer safe to extend URLs by just appending new segments with + ""/"" because there may be a query string. But actually running those tests for the SAS case will require some new test variables to allow the test code to generate SAS tokens (`build.yaml/test_hail_python_fs`): ; ```; export HAIL_TEST_AZURE_ACCOUNT=hailtest; export HAIL_TEST_AZURE_CONTAINER=hail-test-4nxei; # Required for SAS testing on Azure; export HAIL_TEST_AZURE_RESGRP=hailms02; export HAIL_TEST_AZURE_SUBID=12ab51c6-da79-4a99-8dec-3d2decc97343; ```; So the SAS case is disabled for now (`test_fs.py`):; ```; @pytest.fixture(param",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12877:743,extend,extend,743,https://hail.is,https://github.com/hail-is/hail/pull/12877,1,['extend'],['extend']
Modifiability,"This PR makes a number of changes to reduce the overhead of the interpreted `TableAggregate` in general, plus a couple of tweaks to `ApproxCDFCombiner` to eliminate sources of boxed primitives. The main changes are:; * Make `RegionMemory` track the number of Java objects being held, and include that in the log.; * Make the combOp in `TableAggregate` interpreter operate directly on `RegionValue`s. It modifies and returns the left state, and frees the right one.; * To generate the combOp function, I had to compile a function with two agg states (instead of concatenating two `TupleAggregatorStates`, which must live in a single region). That meant not using the `CombOp` IR node. I couldn't quite get rid of the `CombOp` node completely, because I don't understand how it's being used in `Aggregators2Suite` well enough to rewrite it, but that is now the only use.; * In `TableAggregate`, work with `RDD[WrappedByteArray]` instead of `RDD[Array[Byte]]`, to allow the incoming `Array[Byte]` to be GCed as soon as we have decoded it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8794:827,rewrite,rewrite,827,https://hail.is,https://github.com/hail-is/hail/pull/8794,1,['rewrite'],['rewrite']
Modifiability,This PR makes a small change to `build.gradle` so that the Eclipse gradle project plugin will import the project correctly.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4334:82,plugin,plugin,82,https://hail.is,https://github.com/hail-is/hail/pull/4334,1,['plugin'],['plugin']
Modifiability,"This PR makes the following changes:; * Add a `separateRegions` flag to `PStream`.; * Add a `separateRegions` flag to the stream producer nodes `MakeStream`, `StreamRange`, and `ToStream`.; * Propogate `separateRegions` through all stream nodes in `InferPType`. The intended semantics is that if a stream's type has the `separateRegions` flag set, its consumer must pass it a region which gets cleared every element. If the flag is not set, there is no requirement; depending on context, the consumer may put every element in the same region, but is also allowed to use separate regions for each element. For example, in a zip, the elements of the zipped stream are put in separate regions iff at least one child stream requires separate regions; in that case, all children will get emitted with separate regions, whether they requested it or not. In this PR, the `separateRegions` flags are left unused. Eventually, stream consumers will inspect the flag on their child streams' types, and use that information to construct the appropriate `StagedRegion` to pass to `emitStream`. In implementing that, I did some refactoring of the `StagedRegion` design, which I separated out into a follow-up PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9388:1114,refactor,refactoring,1114,https://hail.is,https://github.com/hail-is/hail/pull/9388,1,['refactor'],['refactoring']
Modifiability,"This PR pushes aggregators much closer to where we want; them to be with regardes to SCodes/STypes. Here are the; important conceptual changes:; 1. Aggregators are no longer parameterized by the ptypes; of seqop and initop args. Instead, the state signature; contains a sequence of VirtualTypeWithRequiredness, which; is exactly what its name says. Aggregators use ptypes; internally, but this is not in the state signature.; 2. As a consequence of 1), we no longer cast argument types; to seqop/initops. We are still able to, for instance,pass a seqop; argument of a different type than the container type to; the collect aggregator because the collect aggregator accepts; an SCode argument, and uses the appropriate PType constructor to; store that SCode (with a possible conversion if necessary, as here).; 3. We codebuilder-ify most of the aggregator package. There are some; straggling bits, but I will clean that up in a followup since the; time to get a change in will scale super-linearly with its size. Benchmarks forthcoming. Agg SCodes hopefully done",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9868:174,parameteriz,parameterized,174,https://hail.is,https://github.com/hail-is/hail/pull/9868,1,['parameteriz'],['parameterized']
Modifiability,"This PR refactors the `Binds.scala` file, which encodes the binding structure of the IR. The binding structure controls what variable bindings are in scope in each location of the IR. The encoding specifies, for each node `x` and each child `c` of `x`, how does the environment (the set of bindings which are in scope, and their types) of 'x' differ from the environment of `c`. For instance, `c`'s environment might add a new bindings (e.g. in the body of a `StreamMap`), or it might drop all bindings, isolating the child, so that it cannot refer to anything bound outside of `x` (e.g. all relational nodes). The binding structure is complicated by the separation of the environment into ""eval"", ""agg"", ""scan"", and ""relational"" scopes. The child `c`'s environment might add new bindings in any of these scopes, and it might modify the parent's environment in other ways, like replacing the ""eval"" scope with the ""agg"" scope. In `Binds.scala`, the encoding of this relationship between parent's and child's environments was split across many methods, separately encoding bindings added to each of the scopes, and a prior modification to the parent scope. This made it difficult to understand the binding behavior of any particular node. And for nodes with more complicated binding behavior, these separate encodings became confusingly entangled. In this PR, `Binds.scala` is rewritten so that the binding behavior of a node is specified in a single place. Instead of computing a list of new bindings, it simply takes the parent's environment and returns the child's. Some use cases do require knowing what new bindings are added by a node in a child's environment. To support this, I made the `BindingEnvironment` interface a trait, and added another implementor `SegregatedBindingEnv`, which wraps two separate `BindingEnvironment`s, and puts all new bindings into the second. This satisfies the invariant that for any function `f(GenericBindingEnv): GenericBindingEnv` which modifies an environment",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14404:8,refactor,refactors,8,https://hail.is,https://github.com/hail-is/hail/pull/14404,2,"['refactor', 'variab']","['refactors', 'variable']"
Modifiability,This PR refactors the aioclient to merge the functionality of Batch and BatchBuilder into just a single Batch object. The reason for making this change is to make adding job groups simpler. I will follow up with a change to Jobs after this merges. I apologize for the number of line changes. Most are just renaming `bb -> b` in the tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13458:8,refactor,refactors,8,https://hail.is,https://github.com/hail-is/hail/pull/13458,1,['refactor'],['refactors']
Modifiability,"This PR rewrites the hailctl command line argument parsing code. While the interface remains largely the same, a few changes were made to how options are handled. We first introduce a bit of terminology. In a shell command invocation like `$ cmd a -o c`, `a`, `-o` and `c` are called parameters. `a` and `c` which do not start with dashes, are called arguments. `-o`, which starts with a dash, is an option. This PR makes the following changes:. - For dataproc commands taking extra gcloud parameters, all parameters after a double-dash (--) are passed to gcloud.; - The actual rule is slightly more complicated, but I think the above rule is the right take away. In detail, extra parameters are passed to gcloud. Unknown options (starting with a dash) before `--` are reported as an error. So arguments (not options) before `--` and all parameters after are passed to gcloud. ; - Short options don't need a `=` when specifying a value. It is now `-p2`, not `-p=2`.; - While I was making breaking changes, I changed `dataproc submit` `--gcloud_configuration` to `--gcloud-configuration`. I am happy to undo this one.; - Group arguments must go before the next command. Write `hailctl dataproc --beta start ...` not `hailctl dataproc start --beta ...`, which is an error since `start` has no option `--beta`. This PR rewrites argument parsing to use click instead of argparse: https://click.palletsprojects.com/en/7.x/. Things you need to know about click:; - A group is a group of commands or subgroups, like `hailctl dataproc`, `hailctl batch`, etc. Groups are defined like this:; ; ```; @hailctl.group(; help=""Manage the Hail Batch service.""); def batch():; pass; ```; - A command in a group is defined like this:. ```; @batch.command(; help=""Get a particular batch's info.""); @click.argument('batch_id', type=int); @click.option('--output-format', '-o',; type=click.Choice(['yaml', 'json']),; default='yaml', show_default=True,; help=""Specify output format"",); def get(batch_id, output_format):; ..",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842:8,rewrite,rewrites,8,https://hail.is,https://github.com/hail-is/hail/pull/9842,1,['rewrite'],['rewrites']
Modifiability,"This PR supersedes #9146, sets ndarray slicing behavior to follow ndarray/python semantics when out of bounds: upper_bound = len_dim if step > 0 else len_dim - 1, lower_bound = 0 if step > 0 else -1 (since our -1 has the effect of a slice bound that is None). The other PR doesn't clamp dimensions properly (min and max out of bounds for start and stop are identical), and missed a 2nd issue, which is that python changes clamping behavior when step is negative. I bet this polymorphism is the reason we don't allow non-1 steps for ArrayExpressions.; - @johnc1231 could you still follow up on the byte code generation issue?. I've verified this works in the boundary cases.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9152:474,polymorphi,polymorphism,474,https://hail.is,https://github.com/hail-is/hail/pull/9152,1,['polymorphi'],['polymorphism']
Modifiability,"This PR teaches gear/database.py to respect four more MySQL configuration parameters: `ssl-mode`, `ssl-ca`, `ssl-cert`, `ssl-key`. In particular, we can now turn TLS on or off and rotate keys by simply changing secrets and restarting the services. Since all sql-config secrets (except those in my namespace) currently have no certs, no keys, and no ssl parameters, after this PR merges all services will still use plaintext communications to the database. After this PR merges, I will update the root secret as well as all the service secrets (e.g. sql-auth-user-config) to have a shared client cert/key and our sql database's cert. Moreover I will set `ssl-mode` to `VERIFY_CA` which means (in our world, at least) verify the server's certificate but not the hostname (we use IPs to connect to our sql server) and present your own certificate for verification. Then I will restart all the services. Then I will ban plaintext connections to the database. Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. I also:; - updated `create_database.py` so that it will propagate these TLS settings, if present, to created secrets, and; - updated CI to use `gear/database.py` and standard sql-config locations. All these parameters are defined by MySQL. We only support three options for [`ssl-mode`](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode), the remainder are either unnecessary or not supported (e.g. we have no hostnames so `VERIFY_IDENTITY` is irrelevant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8433:60,config,configuration,60,https://hail.is,https://github.com/hail-is/hail/pull/8433,4,['config'],"['config', 'configuration']"
Modifiability,"This PR teaches most of our cluster how to exclusively speak HTTPS instead of; HTTP. The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). ## Changes from Original PR Proposal. ### Root Certificate. I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. The original PR text follows. ---. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol def",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:364,config,config-hail-root,364,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['config'],['config-hail-root']
Modifiability,"This PR uses BLAS `dgemm` function to multiply matrices when we are multiplying two 2 dimensional tensors of floats. I should extend this to work in the arbitrary tensor case with floats, but I have not done so yet. I'll do the extending after I've reworked the NDArray code to use column major, which I think will make it less complicated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7773:126,extend,extend,126,https://hail.is,https://github.com/hail-is/hail/pull/7773,2,['extend'],"['extend', 'extending']"
Modifiability,"This PR:. - Introduces `StructComparisonOp`, a subclass of `ComparisonOp` that is just for comparing structs. They have an array of sort fields for this purpose.; - Extends `StreamDistribute` to take in a `ComparisonOp`, so that it doesn't just assume data sorted in ascending order.; - Updates `LowerDistributedSort.scala` to actually use the `SortFields` that get passed into it, allowing for arbitrary sort orders.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11270:165,Extend,Extends,165,https://hail.is,https://github.com/hail-is/hail/pull/11270,1,['Extend'],['Extends']
Modifiability,"This PR:. 1. Refactors `ResultOp` to just take an index and return whatever agg result is at that index. No more returning a suffix of the total aggregator tuple based on a starting index. This is necessary for me to effectively implement my Fold aggregator, which will be included in a subsequent PR. ; 2. Pushes `EmitType` through aggregators, uses them as the basis for analyzing the requiredness of aggregator results.; 3. Changes `_storeResult` on aggregators to instead just be `_result`, which directly returns an `IEmitCode`. No reason that `ResultOp` had to be so wound up in `PType`s, and for the most part this made the code simpler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10894:13,Refactor,Refactors,13,https://hail.is,https://github.com/hail-is/hail/pull/10894,1,['Refactor'],['Refactors']
Modifiability,"This PR:. 1. Refactors `listify` to return pandas dataframes, and every geom to take in pandas dataframes. This significantly simplifies the code and should also speed things up.; 2. Refactors the `apply_to_fig` method of most of the geoms to rely on a specification dict and then just loop over that. This simplifies adding a new argument / aesthethic. In the future, it may be best to just make all of the geoms take `**kwargs` so that arguments added to that dict will immediately be used for plotting, as right now I have to add something to `geom_bar`, `GeomBar.__init__`, and that dictionary for it to start showing up in plots. ; 3. Adds `identity` as a bar position, which means to plot bars on top of each other. This is useful along with the `alpha` argument added to bars and histograms, which sets transparency of points.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11317:13,Refactor,Refactors,13,https://hail.is,https://github.com/hail-is/hail/pull/11317,2,['Refactor'],['Refactors']
Modifiability,This PR:; - Replaces Fluentd with Filebeat (Filebeat config based on the recommended kubernetes filebeat config from elastic repo); - Increases elasticsearch storage. ; - Modifies Kibana's security context so that it doesn't run as root (Kibana will print an error message if it's running as root).; - Adds the `decode_json_fields` processor to filebeat so that it parse our structured log messages as json.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6659:53,config,config,53,https://hail.is,https://github.com/hail-is/hail/pull/6659,2,['config'],['config']
Modifiability,"This adds `test_storage_uri` and `batch_logs_storage_uri` fields in the global config. In GCP, this meant just copying existing GCS bucket names and prepending `gs://`, which I've done in the terraform and manually in default. For azure, in the terraform I add two storage accounts, `batch` and `ci`, with `logs` and `test` containers, respectively. This felt like an intuitive consolidation of containers under storage accounts that would make permissioning cleaner. E.g. the batch service principal has access to the entire batch storage account, but only to the `test` container in the ci storage account. However, I've not thought about it deeper than that so it might be worth some looking into. Luckily this decision has no impact on application code. There's still more to be done in a follow-up PR to replace instances of `hail_test_gcs_bucket_name` with `test_storage_uri`, but I think this takes care of the batch deployment's dependency on GCS.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11014:79,config,config,79,https://hail.is,https://github.com/hail-is/hail/pull/11014,1,['config'],['config']
Modifiability,This adds a control to CI where an operator can adjust the rate limit on a particular service in a particular namespace. CI stores and propagates that information when it generates the Envoy configs for `gateway` and `internal-gateway`. This enables operators to shield an overwhelmed batch driver by decreasing its rate limit without needing to push a code change. Increasing the rate limit can increase throughput if workers are being rate limited but the batch driver is not fully utilized.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14609:191,config,configs,191,https://hail.is,https://github.com/hail-is/hail/pull/14609,1,['config'],['configs']
Modifiability,This adds simple unit tests to the CI functionality that generates the configs for `gateway` and `internal-gateway`. It also adds a developer-only endpoint to CI for easier inspection of what configs CI is currently generating in production. These configs do not contain any sensitive information.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14524:71,config,configs,71,https://hail.is,https://github.com/hail-is/hail/pull/14524,3,['config'],['configs']
Modifiability,"This adds some useful infrastructure for Scala-generated C++. 1. Initial support for a small number of C++-to-Scala upcalls. 2. C++ info/warn/error implemented as upcall going through is.hail.utils,{info,warn,error}. 3. Scala PrettyCode auto-indenter for Scala-generated C++, so you don't have to try to; get the indentation right while generating the C++ source. 4. src/main/c/Makefile has a variable HAIL_ENABLE_DEBUG if set as "":=1"", then libs will be; built with -O1, and the initialization in src/main/NativePtr.cpp will try to start gdb in an xterm; and attach back to the hail process, allowing gdb debugging of generated C++ called from; Scala. 5. ObjectArray is a NativeObj which can hold any number of Scala Objects, holding them in C++; as JNI global-ref jobjects. This can be used for example to hold InputBuffer or InputStream; objects, and to pass them down to a constructor for a C++ object (e.g. a decoder) which will then ; make upcalls to methods of those objects. . A subsequent commit will have the RowStore/C++ decoder using all this infrastructure (passing an; InputBuffer down to the C++ decoder, which holds it and makes upcalls to pull a block of data).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4302:393,variab,variable,393,https://hail.is,https://github.com/hail-is/hail/pull/4302,1,['variab'],['variable']
Modifiability,"This adds the Google Cloud Monitoring and Prometheus datasources to the grafana configuration. I had done this initially by hand in the UI but this is the first step toward reproducible monitoring, and I'll eventually follow up with dashboards as code. The one ""change"" I made is I exposed the prometheus port sitting behind nginx so that grafana can talk directly to prometheus. Currently, there's an nginx sitting in front of prometheus so that the prometheus UI can be exposed at prometheus.hail.is with https and dev authentication. This hasn't changed. Currently though, grafana is piggybacking on this flow by forwarding the user's session (which I set up in the UI), but I couldn't figure out an easy way to set that in the config and it seemed unnecessarily complicated. I ended up going the simpler route of just letting grafana talk to prometheus directly and not go through nginx. The 9090 endpoint is not reachable outside of the cluster. I considered namespacing the prometheus domain (`{{ default_ns.name }}` instead of `default`), but I pretty much never find it useful to spin up my own prometheus. In the rare case I run my own grafana I just point it to the data from default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10627:80,config,configuration,80,https://hail.is,https://github.com/hail-is/hail/pull/10627,2,['config'],"['config', 'configuration']"
Modifiability,"This adds the following pieces of infrastructure:. - Fully scripted bootstrapping process, from creating a managed identity to run terraform through (instead of the current service principal), to creating a VM to run the bootstrap process off of, through all following steps until running bootstrap.py; - Adds the root CA certificate that azure uses to sign the MySQL server certificates so that we can connect to the database with `VERIFY_CA`. Unlike gcp, however, this still doesn't allow us to use mTLS since it doesn't look like we can request a client cert/key for our database. Still this is not so bad for now.; - Creates a separate k8s module for terraform. This currently just holds the global-config and sql-config resources, but establishes a boundary between the cloud-specific terraform and purely k8s terraform. Later on I'll refactor the GCP terraform to use the k8s module so that different clouds can use the same k8s configuration.; - Adds a pool of spot instances to the AKS cluster and adds the required toleration to all of our preemptible deployments. Part of the node selection process for a pod requires that exist a toleration on the pod for every taint on the node. In other words, it is ok for a pod to have redundant tolerations, so it's fine to have azure-specific tolerations even if we're running in gcp.; - Refactor the az-create-worker-image.sh script to complete the entire batch worker image creation process from start to finish. This involved sending a command over ssh that previously had to be executed by hand. This meant we could combine the two-script process into one shell script. This fully matches the google setup we have currently up until running `bootstrap.py`, which is still google-specific, mainly w.r.t. gcp service accounts. The next step is to adapt this to azure, but I think we need to come to a decision about exactly how we're representing application credentials (just service principals vs managed identities?). Once we have that figured o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10919:703,config,config,703,https://hail.is,https://github.com/hail-is/hail/pull/10919,4,"['config', 'refactor']","['config', 'configuration', 'refactor']"
Modifiability,This adds the following terraform capabilities:; - A reserved public IP for the k8s gateway; - A container registry and pull access for the k8s cluster; - A [private link](https://azure.microsoft.com/en-us/services/private-link/) for the mysql database that makes it accessible on the k8s subnet. I haven't set up the config to use the database yet but ensured that the hostname for the database was resolvable from a pod on the cluster. I think this covers most of the azure specific resources that we need. Most of the rest of our terraform for gcp creating k8s secrets for the database config and various service accounts. I'd like to approach that in a single chunk to find out how best to abstract those into modules.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10838:318,config,config,318,https://hail.is,https://github.com/hail-is/hail/pull/10838,2,['config'],['config']
Modifiability,"This adds the function to TableReader:; ```; rowAndGlobalPTypes(ctx: ExecuteContext, requrestedType: Type): (PStruct, PStruct); ```. (The context is necessary for native readers to be able to get the filesystem in order to read the metadata.). I'm not sure if this is the best way to implement this, but it feels like TableReaders ought to be able to tell you what PType they expect to be decoding into at compile time, since we can use this information to make decisions about requiredness, etc. on IRs that contain TableReads. I rely on this to extend the requiredness analysis to TableIR nodes; I think it's probably reasonable to have one function that goes from requestedType => PType instead of separate functions for requiredness and other analyses we might want to do un the future, since I don't think we have imminent plans to decode directly into different PTypes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8740:547,extend,extend,547,https://hail.is,https://github.com/hail-is/hail/pull/8740,1,['extend'],['extend']
Modifiability,"This adds the k8s config necessary to host the [guide browser](https://hub.docker.com/r/gneak123/guide_browser/tags) in our k8s cluster. You can see it running in dev [here](https://internal.hail.is/dgoldste/guide-analysis/). There's not much special here, a deployment with the browser app and an envoy sidecar to handle TLS. Once this merges and the `ssl-config-guide-analysis` is created in `default` I can `make -C guide deploy NAMESPACE=default` and then recreate the certs to pick up the new subdomain, after which it should be live. Resolves #14067",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14078:18,config,config,18,https://hail.is,https://github.com/hail-is/hail/pull/14078,2,['config'],"['config', 'config-guide-analysis']"
Modifiability,"This adds the minimal resources to k8s to allow us to modify the gateway's configuration to include redirects for https://notebook.hail.is. Currently, that domain will timeout, but there should otherwise be no errors introduces to the k8s system. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4645:75,config,configuration,75,https://hail.is,https://github.com/hail-is/hail/pull/4645,2,['config'],['configuration']
Modifiability,"This allows the user to specify the cloud platform ('gcp' or 'aws') they are using when accessing datasets via the datasets API and annotation DB. A user running hail on AWS would read from the s3 bucket, and a user running on GCP would read from the gs bucket (can also still read locally from gs bucket with cloud storage connector installed). Not intended for cross-platform use like running a dataproc cluster and trying to access the s3 bucket, or trying to access the gs bucket on an EMR cluster. Will assume user on AWS has their configuration set with their credentials. . Did not have permissions to set up a cluster or EC2 instance on AWS to test, but was able to access all the datasets without issue on a dataproc cluster when using s3a:// prefixes and providing my AWS credentials in the spark config. So these changes should (hopefully) work fine on an EMR cluster with the s3:// client. Everything worked as expected on GCP. Overview of changes:; - Added missing type hints to `load_dataset()` function and methods in `db.py`.; - In `datasets.json`:; - Added AWS urls so now for each version of a dataset in dataset[""versions""], the url entry looks like:; ```; ""url"": {; ""aws"": {; ""us"": ""s3://hail-datasets-us-east-1/...""; },; ""gcp"": {; ""eu"": ""gs://hail-datasets-eu/..."",; ""us"": ""gs://hail-datasets-us/...; }; ```; - In `load_dataset()` function:; - Added `cloud` parameter, set default values to `region='us'` and `cloud='gcp`.; - In `DB` class:; - Added `cloud` parameter to constructor, set default values to `region='us'` and `cloud='gcp`.; - All datasets in `datasets.json` currently end up in the `_DB__by_name` dictionary, even if not annotation datasets. Added line 279 in `db.py` to fix this and filter out datasets that are not annotation datasets (datasets missing ""annotation_db"" key).; - In `Dataset` class:; - Added `cloud` and `custom_config` parameters to `Dataset.from_name_and_json()` to pass to `DatasetVersion.from_json()` to grab correct urls for platform.; - Refac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9605:537,config,configuration,537,https://hail.is,https://github.com/hail-is/hail/pull/9605,2,['config'],"['config', 'configuration']"
Modifiability,"This allows us to write functions for the IR that handle missingness specially, in cases where we don't necessarily want the behavior where any missing argument implies that the entire function result is considered missing. I implemented `&&` and `||` using this in order to test. I also refactored the return from Emit.emit to a case class `EmitTriplet` to make it easier to talk about them in other contexts. cc @cseed @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3210:288,refactor,refactored,288,https://hail.is,https://github.com/hail-is/hail/pull/3210,1,['refactor'],['refactored']
Modifiability,"This also fixes the currently broken `delete_*_tables` steps. In the past, all dev and test databases shared the same MySQL Server (with production), so instead of each service getting its own dedicated database, there was one database per dev/test namespace with all the tables for all the services. This made it difficult to reset the database state of a particular service -- you needed to explicitly delete only the tables for that particular service. Nowadays, dev and test databases live on their own MySQL Servers, so each service gets its own database (like in production). This makes it a lot easier to reset a service's database, we just drop the MySQL database for that service. This PR makes that change, deletes all the now unused `delete-*-tables.sql` files, and adds a dev doc explaining how to reset a dev database. The reason these steps were broken is that the sql configs in dev/test namespaces use K8s DNS for the `host`, which does not work out of the box in batch jobs because they are not in the K8s network. There's code in `database.py` that uses the K8s API to resolve the database host to an IP address that the batch jobs can access. This is why I wrote a python script instead of just using `mysql`. I tested these with the following dev deploy, which scrapped everything and I was able to log in after it was done!. ```; hailctl dev deploy -b daniel-goldstein/hail:dev-ns-delete-db -s delete_auth_tables,delete_batch_tables,deploy_batch,add_developers; ```. cc: @sjparsa, @iris-garden given your recent dev namespace woes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13456:883,config,configs,883,https://hail.is,https://github.com/hail-is/hail/pull/13456,1,['config'],['configs']
Modifiability,"This basically just pulls out the logic from the `NativeDecoder` and `NativeEncoder` stuff in RowStore.scala into their own objects, and dynamically generates a c++ class for the row type. (The only things that have changed between `NativeDecoder`/`cxx.PackDecoder` and the encoders are the `apply` functions; I'm generating an Encoder class that inherits NativeObj instead of relying on the wrapper classes in `Encoder.h` and `Decoder.h`. This mostly felt like it made things easier to reason about when I started writing the full-stage code generation stuff, but I've pulled it out here as a separate PR. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4722:347,inherit,inherits,347,https://hail.is,https://github.com/hail-is/hail/pull/4722,1,['inherit'],['inherits']
Modifiability,"This builds on: https://github.com/hail-is/hail/pull/3107. We were getting absolutely murdered by serializing the partitioner on every task. I made OrderedRVDPartitioner not inherit from Spark Partitioner, non-serializable, and modified it to be broadcasted where needed. There is a good chance this will speed up OrderedRVD shuffles as well, but I haven't timed. Timing:. ```; import hail as hl. mt = hl.read_matrix_table('gnomad.exomes.vds', _drop_cols=True); mt = mt._filter_partitions(list(range(1000))); mt._force_count_rows(); ```. master: 1m15s; no_ser_ord_part: 9s. That's about 8x improvement. This includes startup time so it's actually much better.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3110:174,inherit,inherit,174,https://hail.is,https://github.com/hail-is/hail/pull/3110,1,['inherit'],['inherit']
Modifiability,"This came to mind yesterday during our pairing. This PR introduces the following properties that our image building targets do not currently have:; 1. If your intention is only to build images, you shouldn't need `kubectl`. When `DOCKER_PREFIX` is used as a docker build arg it is because we mirror some dockerhub images inside our registry (for reliability/rate limiting reasons). But for local building there's no reason you can't use the dockerhub image. Also, other people should be able to build the hail image if they want to!; 2. One should *only* need to use `kubectl` if they are intending to use an image in a kubernetes deployment. In other words, you should only need the private registry `DOCKER_PREFIX` for pushing images.; 3. One should not need to endure image pushing if the only goal is to build the image locally; 4. No intermediate tags should end up in the private registry. If we push on every image build, the private docker registry will accumulate a lot of `hail-ubuntu:dev-xxxxxx` tags that are never used again because `hail-ubuntu` is just an intermediate used to build other images. This does *not* change the number of layers that end up in the registry, but reduces a bit of the work that the registry cleanup job needs to do to untag and delete images and just seems cleaner.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13890:1149,layers,layers,1149,https://hail.is,https://github.com/hail-is/hail/pull/13890,1,['layers'],['layers']
Modifiability,"This change applies each currently-ignored `ruff` rule progressively; each commit applies one rule. The changes were applied manually to avoid known issues with the automatic fixes; for example, given the code; ```python; return (; is_container(t); or isinstance(t, tstruct); or isinstance(t, tunion); or isinstance(t, ttuple); or isinstance(t, tndarray); ); ```; the automatic fixes produce; ```python; return isinstance(t, (tndarray, tstruct, ttuple, tunion)); ```; instead of ; ```python; return is_container(t) or isinstance(t, (tstruct, tunion, ttuple, tndarray)); ```; where not only has the call to `is_container` been removed, but also the order of the `isinstance` comparisons has been changed, which has the potential to produce side effects (though in this case, I don’t think it does). Similarly, when eliminating assignments to unused variables, I left the right-hand side of the assignment intact in case of side effects, except where I myself wrote the code in question and know there are no side effects produced by it. See also https://github.com/hail-is/hail/pull/14150 and https://github.com/hail-is/hail/pull/14159.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14415:848,variab,variables,848,https://hail.is,https://github.com/hail-is/hail/pull/14415,1,['variab'],['variables']
Modifiability,"This change combines cloud auth logic that was previously duplicated; between the various `FS` implementations and the `BatchClient`. . The main refactoring is to make the interface between the `ServiceBackend` more; high-level and leave json serialisation to the `BatchClient`. To do this, I've; added a bunch of case classes that resemble the python objects the batch service ; expects (or a subset of the data). To simplify the interface, I've split batch; creation from job submission (update). For QoB, the python client creates the ; batch before handing control to the query driver; batch creation is necessary; for testing only. This change has low security impact as there are minor changes to the creation; and scoping of service account credentials. Note that for each `FS`, credentials; are scoped to the default storage oauth2 scopes for each service.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14684:145,refactor,refactoring,145,https://hail.is,https://github.com/hail-is/hail/pull/14684,1,['refactor'],['refactoring']
Modifiability,"This change creates mysql pods for test and dev namespaces, instead of sharing the CloudSQL database server. The areas of change are as follows:. ### Generation of the namespace's database-server-config; The current approach in main does a little trick. Since the current `createDatabase` step uses the `database-server-config` from default to generate admin/user sql configs, the CI pipeline creates a dummy database `test-database-instance` to create a `sql-test-instance-admin-config` that inherits the credentials from the production `database-server-config`, and then copies that within the test namespace to `database-server-config`. In this change, since we are creating the server ourselves, we can just replace these with a step that creates a `database-server-config` from scratch, and then uses that for the DB pod. Overall making these changes really gave me the heebie jeebies that the test and dev namespaces have all these credentials to the CloudSQL server. I'm glad this gets rid of that. ### Accessing the database server; We use the DB pod's service DNS name as the `host` so inside Kubernetes this Just Works. The one caveat is the CI pipeline in which we run migrations in batch jobs. Those jobs need a way to reach the DB pod. I achieve this with a NodePort and then use the job's K8s credentials to resolve the node and port that the DB is on. The code I've added to do this resolution feels a bit janky, wouldn't mind some feedback on that. In terms of security, if a user job was able to somehow resolve the address of a test db, they would still not have the credentials to access it, and this is currently also the case with the production database. Nevertheless, this does raise an action item that we should only allow traffic to the k8s and DB subnets for `network=private` jobs, but I think we should make that a separate PR. ### Database creation; In order to test this properly in a dev deploy, I needed to make some changes to `create_database.py`. In main, dev deplo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13030:196,config,config,196,https://hail.is,https://github.com/hail-is/hail/pull/13030,8,"['config', 'inherit']","['config', 'configs', 'inherits']"
Modifiability,"This change exists as part of larger refactoring work. Herein, I've exchanged; hard-coded contextual strings passed to `ExecutionTimer.time` with implict; contexts, drawing inspiration from scalatest. These contexts are now supplied after entering functions like `Compile` and; `Emit` instead of before (see `ExecuteContext.time`). By sprinking calls to ; `time` throughout the codebase after entering functions, we obtain a nice trace; of the timings with `sourcecode.Enclosing`, minus the previous verbosity. See [1] for more information about what pre-built macros are available. We can; always build our own later. See comments in [pull request id] for example output.; Note that `ExectionTimer.time` still accepts a string to support uses like; `Optimise` and `LoweringPass` where those contexts are provided already.; It is also exception-safe now. This change exposed many similarities between the implementations of query; execution across all three backends. I've stopped short of full unification; which is a greater work, I've instead simplified and moved duplicated result; encoding into the various backend api implementations. More interesting changes are to `ExecuteContext`, which now supports; - `time`, as discussed above; - `local`, a generalisation for temporarily overriding properties of an ; `ExecuteContext` (inspired by [2]). While I've long wanted this for testing,; we were doing some questionable things when reporting timings back to python,; for which locally overriding the `timer` of a `ctx` has been very useful.; We also follow this pattern for local regions. [1] https://github.com/com-lihaoyi/sourcecode; [2] https://hackage.haskell.org/package/mtl-2.3.1/docs/Control-Monad-Reader.html#v:local",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679:37,refactor,refactoring,37,https://hail.is,https://github.com/hail-is/hail/pull/14679,1,['refactor'],['refactoring']
Modifiability,"This change is split out from a larger refactoring effort on the various Backend ; implementations. The goals of this effort are to provide query-level ; configuration to the backend that's currently tied to the lifetime of a backend,; reduce code duplication and reduce state duplication. In this change, I'm restoring references to the execute context [1] and ; decoupling them from the backend. In a future change, they'll be lifted out of ; the backend implementations altogether. This is to reduce the surface area of ; the Backend interface to the details that are actually different. Both the Local and Spark backend have state that's manipulated from python via ; various py methods. These pollute the Backend interface [2] and so have been ; extracted into the trait Py4JBackendExtensions. In future changes, this will ; become a facade that owns state set in python. Notes; [1] ""Restoring"" old behaviour I foolishly removed in fe5ed32; [2] ""Pollute"" in that they obfuscate what's different about backend query plan ; and execution",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14686:39,refactor,refactoring,39,https://hail.is,https://github.com/hail-is/hail/pull/14686,2,"['config', 'refactor']","['configuration', 'refactoring']"
Modifiability,"This change is split out from a larger refactoring effort on the various Backend; implementations. The goals of this effort are to provide query-level; configuration to the backend that's currently tied to the lifetime of a backend,; reduce code duplication and reduce state duplication. In this change, I'm removing blockmatrix persist/unpersist from the `Backend`; interface by adding `BlockMatrixCache: mutable.Map[String, BlockMatrix]` to; `ExecuteContext`. The various reader/writer implementations simply fetch the ; block matrix from this cache. For the spark backend, this is backed by a cache; whose lifetime is tied to the spark backend. Since block matrices are not; supported in the local and service backends, the cache is an empty map. Note that block matrix persist is broken in python (#14689)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14690:39,refactor,refactoring,39,https://hail.is,https://github.com/hail-is/hail/pull/14690,2,"['config', 'refactor']","['configuration', 'refactoring']"
Modifiability,"This change makes the labels for groups of points on scatterplots created through `geom_point` configurable by the user. Consider the following sample code:. ```python; import hail as hl; from hail.ggplot import ggplot, aes, geom_point; ht = hl.utils.range_table(10); ht = ht.annotate(squared=ht.idx ** 2); ht = ht.annotate(even=hl.if_else(ht.idx % 2 == 0, ""yes"", ""no"")); ht = ht.annotate(threeven=hl.if_else(ht.idx % 3 == 0, ""good"", ""bad"")); fig = (; ggplot(ht, aes(x=ht.idx, y=ht.squared, color=ht.even, shape=ht.threeven)); + geom_point(); ); fig.show(); ```. Before this change, this code generates the following legend for the plot:. <img width=""118"" alt=""Screen Shot 2022-09-29 at 12 29 27"" src=""https://user-images.githubusercontent.com/84595986/193087326-a1bed417-b314-442c-a8a0-c2f506ca7d93.png"">. After this change, the legend looks like this (`×` is now the default group name separator):. <img width=""107"" alt=""Screen Shot 2022-09-29 at 12 22 57"" src=""https://user-images.githubusercontent.com/84595986/193085964-e4545e78-473f-46a3-8c8c-7d6189eb7adc.png"">. If `legend_format` is passed to `geom_point`, like so:. ```python; ...; + geom_point(legend_format=""{color} ({shape})""); ...; ```. the following legend will be generated, replacing the names of the aesthetics within brackets in the format string with their values:. <img width=""104"" alt=""Screen Shot 2022-09-29 at 12 25 17"" src=""https://user-images.githubusercontent.com/84595986/193086499-4ffaacac-ada1-43ea-b056-9334e26b713d.png"">. Notably, if the reverse ordering of group names (`""{shape} ({color})""`) is used, the legend will look like this:. <img width=""103"" alt=""Screen Shot 2022-09-29 at 12 26 35"" src=""https://user-images.githubusercontent.com/84595986/193086758-f7accbe4-4427-4b06-8b5c-f200a3ceec3e.png"">. The order in which the groups in the legend appear can be adjusted, if desired, by changing the order of the aesthetics when they are passed into `aes`:. ```python; ...; ggplot(ht, aes(x=ht.idx, y=ht.squared, shape=h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12244:95,config,configurable,95,https://hail.is,https://github.com/hail-is/hail/pull/12244,1,['config'],['configurable']
Modifiability,"This change simplifies aspects of the annotation db's implementation as well as adding new features such as annotating a Table or using a custom JSON configuration file. The Annotation DB will remain experimental until we iron out the JSON configuration file's structure and we're confident in the deploy process. - Allow custom URL or JSON for configuration (enabling testing and local development).; - Support Tables.; - Restructure the annotation db JSON to reduce duplication. It now maps from dataset name to dataset metadata and dataset versions.; - Simplify JS logic based on new JSON structure.; - Check-in and implement versioned deployment of the annotation db configuration JSON.; - Add a JS file to the website that defines `hail_version` and `hail_pip_version`.; - Add `key_properties` which currently supports two properties `gene` and `unique`. Gene keyed datasets require using the `gencode` dataset to crosswalk from locus to gene before joining.; - Rudimentary test of key properties functionality. Foundational Changes Outside Annotation DB:; - Define `__pip_version__` in `hail`.; - Teach `StructExpression` and `TupleExpression` how to slice by integers, facilitating the construction of structs of a prefix of fields.; - Make `ttuple` a mapping from integers to the tuple elements.; - Implement `Table._maybe_flexindex_table_by_expr` which, given a indexer expression, finds a prefix of the expression that can index the indexee, if such an expression exists. Unrelated changes:; - Clarify Makefile error echos with `ERROR:`. ---. ## flexindex. The primary use case for this is a dataset which is `locus, allele` keyed and needs to index into a `locus` keyed or `interval<locus>` keyed dataset. Hail's normal join logic will return a key mismatch error:. ```python; import hail as hl; t = hl.utils.range_table(10); t2 = t.key_by(x=t.idx, y=t.idx); t.index(t2.key); ```; ```; Traceback (most recent call last):; File ""<ipython-input-6-3ddc90774dfe>"", line 1, in <module>; t.index(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7178:150,config,configuration,150,https://hail.is,https://github.com/hail-is/hail/pull/7178,4,['config'],['configuration']
Modifiability,"This changes allow `hl.init()` to run against the query service without starting JVM/Spark on the client:. ```; $ python3; Python 3.7.3 (default, Oct 7 2019, 12:56:13) ; [GCC 8.3.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail as hl; >>> hl.init(_backend=hl.backend.ServiceBackend()); Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-0ef20f14e0c1; LOGGING: writing to /home/cotton/hail/hail-20200402-0120-0.2.34-0ef20f14e0c1.log; ```. Summary of changes:; - Move initialization of Java HailContext from init to SparkBackend ctor. There is no JVM or Java HailContext when using the service backend.; - Env no longer carries the gateway. (Next: jvm); - make Java HailContext.tmpDir construction lazy. It requires a fs, but HailContext will only carry an fs for the SparkBackend. This will have to get rethought.; - Make Java ServiceBackend extend Backend.; - Construct a HailContext in the query service.; - Implement /references/get in query backend which is needed by hl.init to get the builtin reference genomes on startup.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8434:932,extend,extend,932,https://hail.is,https://github.com/hail-is/hail/pull/8434,1,['extend'],['extend']
Modifiability,"This changes the Azure database from Azure database for MySQL Single Server to Azure database for MySQL Flexible Server. The major changes are:. - Fixed several small rots across the bootstrap code; - Altered the database module in terraform to use flexible server. This configuration mostly matches what we had with single server, importantly that it is only accessible on our vnet.; - Makes the client key/certificate in the SQLConfig optional (the current use of SQLConfig is a little repetitious and could probably use a refactor).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11423:104,Flexible,Flexible,104,https://hail.is,https://github.com/hail-is/hail/pull/11423,4,"['Flexible', 'config', 'flexible', 'refactor']","['Flexible', 'configuration', 'flexible', 'refactor']"
Modifiability,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:945,config,config,945,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965,2,['config'],['config']
Modifiability,"This commit changes the tracking of Joins on Python expressions; in two ways:. First, joins are switched from being tracked explicitly; on Expression to being a part of the expr AST. Second, broadcasts are split off as a separate AST node, which; lets us make them significantly simpler. It also made it easy; to collapse all broadcasts for a given MT/Table operation into; one MapGlobals IR, instead of one per broadcast variable. In the future, all the metadata on Expression should be tracked; on AST (type, aggregations, indices). This is a first incremental; step in getting there.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3573:422,variab,variable,422,https://hail.is,https://github.com/hail-is/hail/pull/3573,1,['variab'],['variable']
Modifiability,"This consisted almost entirely of `region` -> `Region`. I had to rewrite some things to replace `appendBinary` and `appendString`, but there shouldn't be any other substantive changes in here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7092:65,rewrite,rewrite,65,https://hail.is,https://github.com/hail-is/hail/pull/7092,1,['rewrite'],['rewrite']
Modifiability,"This doesn't look like it can hurt, but why is this necessary? `SplitCompressionInputStream` extends `CompressionInputStream` which has a `close` method already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9295#issuecomment-675544269:93,extend,extends,93,https://hail.is,https://github.com/hail-is/hail/pull/9295#issuecomment-675544269,1,['extend'],['extends']
Modifiability,"This enables Query-on-Batch pipelines to read from requester pays buckets. @tpoterba curious for your thoughts on the flag situation. I suspect this PR will induce the Australians to start including requester pays config in their pipelines. If you describe an API you like, I can implement it for this PR. Otherwise, I think this is ready. It works, it is tested. The changes to GoogleStorageFS suck, but its due to the reality of the GCS API.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12133:214,config,config,214,https://hail.is,https://github.com/hail-is/hail/pull/12133,1,['config'],['config']
Modifiability,"This fixes site to work inside PRs and dev namespaces. The main fix is to teach; site that, when the namespace is not default, all its resources are located at; `/$NAMESPACE/site`. I also use `subs_filter` to rewrite images, anchors, and; stylesheets that have absolute links (this can be `<a href=""/""` or `<a; href=""/foo/bar/baz.html""`) to include the namespace prefix. I also added a test that the website is up and returning 200 with our infrastructure. I also added `updated_host` which uses the X-Fowarded-For host if it exists (i.e. in a namespace).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8763:209,rewrite,rewrite,209,https://hail.is,https://github.com/hail-is/hail/pull/8763,1,['rewrite'],['rewrite']
Modifiability,"This guy will be totally retooled with the table changes, so I'll wait for that to go in too. The GenomicIndex here actually predated the interval rewrite",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/438#issuecomment-232252468:147,rewrite,rewrite,147,https://hail.is,https://github.com/hail-is/hail/pull/438#issuecomment-232252468,1,['rewrite'],['rewrite']
Modifiability,"This illustrates one of the footguns in using CodeBuilder. When using; functionality like ifx, we must be sure to actually put the resultant; code on the CodeBuilder in the block we pass to ifx. Otherwise we won't; add any code and the behavior may be surprising. This is basically equivalent to storing some Code in a variable and then; never passing that variable into a returned Code, so it is never; executed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8487:319,variab,variable,319,https://hail.is,https://github.com/hail-is/hail/pull/8487,2,['variab'],['variable']
Modifiability,This implements [Apache Arrow](https://arrow.apache.org/docs/format/Columnar.html#variable-size-list-layout) style nested arrays. A more detailed write up will appear here as this gets closer to being ready.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10542:82,variab,variable-size-list-layout,82,https://hail.is,https://github.com/hail-is/hail/pull/10542,1,['variab'],['variable-size-list-layout']
Modifiability,"This introduces a new version of the batch worker instance: one without `docker`. Instead we bring in `podman` to cover the functions of pulling images, extracting expanding filesystems from those images, and running the worker container. `podman` by default uses `crun` as its low-level runtime so we can get rid of the independent `crun` installation in the worker image. `podman` is daemonless and can be run rootless. For the most part you can't tell the difference, except this makes `podman` easy to run under multiple users with different caches per user. So if you ssh into a worker, be sure to `sudo` before any `podman` (or `crun`) command or else you might think nothing is running when in fact the worker is running under root's podman configuration. The podman/crun state directories are now shared between the host and the worker so `sudo crun list` on the worker should reveal the running job containers without having to exec into the worker first. For the most part, `podman` is a drop-in replacement for `docker`, but there are a handful of inconsistencies that comprise most of this PR. One notable change is that we no longer persist any GCR credentials in the worker VM image so we authenticate again on start up. cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10693:748,config,configuration,748,https://hail.is,https://github.com/hail-is/hail/pull/10693,1,['config'],['configuration']
Modifiability,"This introduces the necessary pieces of infrastructure to run CI in GCP and a couple of small changes such that it can run as a secondary CI. This is currently failing because one of the secrets I introduce here in the terraform does not exist in hail-vdc. If you approve of the approach I can add it in manually. . ## Terraform changes; This adds a new CI terraform module that adds a CI bucket, sets some permissions for the CI service account and adds some K8s secrets like github tokens and the zulip config. This allows the terraform deployment to optionally include resources needed for CI. This was the best way I could think to introduce this infra with the least changes, but it's not what I want in the long term. Right now we have one monolithic root module that includes all the resources necessary to run batch, with the option for tacking on CI. I would rather extract most of our root module into a `batch` module (and while we're break down the innards into modules like vdc, db, etc.) and have the root module be something that can be easily pieced together from the library of modules. This would be a decently big refactor and more importantly would require existing deployments to manually overhaul their terraform state, so it's something I want to do carefully but also sooner is better than later. Given how terraform state is indexed, I believe more modularity will be easier to manage in the long term. ## CI changes; This adds the following features to CI; - Watched branches can be marked as `mergeable`. `mergeable=true` should be the default behavior and `false` prevents CI from merging a PR on GitHub. This allows multiple CI's to run tests in different environments without stepping on each others' toes. This *does not*, however, consider statuses from multiple CIs when making the decision to merge a PR. That is currently based on the build status, and later should be changed to consider the collection of statuses on GitHub.; - Custom Deploy Steps: This is a colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053:505,config,config,505,https://hail.is,https://github.com/hail-is/hail/pull/11053,1,['config'],['config']
Modifiability,"This is a bad change to a worse problem. The right solution is to redesign the IR; so that implicit init eval scopes don't exist -- but the right solution is hard to justify; going off to do right now. This change patches the Extract.scala lowering logic to track the variables bound inside a; lowered IR and find the highest node that provides all necessary free variables. This change; still makes assumptions about the structure of the IR -- namely, it is still invalid to write; an IR like:. ```; MakeTuple; Let; initBinding1; <something>; ApplyAggOp with ref to initBinding1; Let; initBinding2; <something2>; ApplyAggOp with ref to initbinding2; ```. However, this fix resolves the case where init args reference a high single binding chain, as; in the test added.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12092:268,variab,variables,268,https://hail.is,https://github.com/hail-is/hail/pull/12092,2,['variab'],['variables']
Modifiability,"This is a bit of a mess so feel free to ask that I break it down or explain more in depth. In short, this:. - adds terraform for forgotten bits and pieces necessary for running PR tests like test buckets and the necessary permissions on those resources; - Adds a couple of flags that allow a CI `WatchedBranch` to be considered mergeable or not. This shouldn't change anything in default CI, but it allows you to specify that a secondary CI should run PRs, post statuses, and deploy new commits, but never actually commit anything to GitHub. Similarly there's a flag for turning off zulip notifications, but annoyingly the zulip config is still a required secret. I plan to make that nicer in the future.; - Fixes a lot of previously-unreached syntax errors in the batch tests. Following PRs will have relatively less functionality but probably a fair bit of cleanup and reorganization, e.g. getting rid of config.mk and generating it from terraform output, making scripts of the bootstrapping process etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10866:629,config,config,629,https://hail.is,https://github.com/hail-is/hail/pull/10866,2,['config'],['config']
Modifiability,"This is a known limitation -- it's hard to fix without significant investment in a backend we don't want to maintain forever. This issue generally pops up when people use tables instead of matrix tables to process wide files. Can you rewrite the GTEx processing script to use import_matrix_table instead of import_table, then process as a matrix?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11972#issuecomment-1171453473:234,rewrite,rewrite,234,https://hail.is,https://github.com/hail-is/hail/issues/11972#issuecomment-1171453473,1,['rewrite'],['rewrite']
Modifiability,"This is a little confusing and not especially well documented on our end. The `--tmpdir` option set within Hail defines the scratch space for certain Hail operations, but not Spark ones. The tempdir that's blowing up on you is the `spark.local.dir` setting (see http://spark.apache.org/docs/latest/configuration.html for more info). This directory is used when Spark needs to do a data shuffle, and currently the `splitmulti` command requires this. I assume you're submitting to a cluster with a `spark-submit` invocation. You can set spark settings by passing the `--conf` argument, as here:. ``` text; /usr/bin/spark-submit \ ; --conf spark.shuffle.compress=true \ ; --conf spark.local.dir=/correct/tmp/dir \; --class org.broadinstitute.hail.driver.Main \ ; JAR \; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/902#issuecomment-251721691:298,config,configuration,298,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251721691,1,['config'],['configuration']
Modifiability,"This is a long overdue description of how our K8s ingress / config is set up. I've tried to keep it as concise as possible, so ask for elaboration if there are unexplained gaps.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14488:60,config,config,60,https://hail.is,https://github.com/hail-is/hail/pull/14488,1,['config'],['config']
Modifiability,"This is a minor architectural change (cc'ing @cseed @tpoterba) that I hope will improve maintainability of `batch`. It foreshadows the DAG functionality. There may be shared data structures between the server and the client. At the very least, the client sends structured data to the server (e.g. a pod spec and metadata about the job). Often, the server parses this data into an object or series of objects which contain methods for performing the server's job (e.g. `batch/server/job.py`). I think this architecture is more or less a different way of defining the API (see `batch/api.py`). I think defining the API via data objects is appealing because; - it centralizes serialization and deserialization for each data structure in one class,; - it enables sharing (via object composition) of that basic data structure between potentially complex client and server objects that implement algorithms on that data structure (I want to do this with the forthcoming DAG stuff), and; - the client has objects representing its ideas (i.e. ""a job"") and those objects can have `__str__`'s and `__repr__`'s facilitating debugging of the client. Moreover, this change pushes the use of k8s' swagger models everywhere possible. This means it's harder for us to make code mistakes because pylint will notice when we, for example, misspell a parameter to a k8s model.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4804:88,maintainab,maintainability,88,https://hail.is,https://github.com/hail-is/hail/pull/4804,1,['maintainab'],['maintainability']
Modifiability,"This is a multi-stage overhaul of our Kubernetes load balancers / service discovery. This involves moving off of NGINX onto Envoy, but more importantly involves better control of what namespaces and services are active in our cluster at a given point in time. TL;DR Switching from NGINX to Envoy with CI acting as the ""control plane"" for our internal networking allows us to more easily dynamically configure our Kubernetes networking and achieve proper connection pooling/load-balancing over TLS, which translates to less resource consumption and lower request latencies. ## Motivation; This is primarily a performance-motivated change, and one largely based on our (ab)use of NGINX in order to work with our dynamically-generated Kubernetes test namespaces. Currently, we configure NGINX by creating server blocks that dynamically resolve and dispatch requests based on matching regular expressions on the host and path headers. This is in large part due that at gateway deploy time we do not statically know all of the namespaces and namespace-service combinations that will exist in the cluster in the future. This is true for `default`, but not test namespaces, and NGINX will refuse to start with statically-configured clusters that it cannot reach. Making the server blocks make the routing decisions dynamically circumvents this limitation. However, this prevents usage of NGINX [upstream](http://nginx.org/en/docs/http/ngx_http_upstream_module.html) blocks that provide connection pooling, at least in the community edition, and as a result the gateways will create and terminate a TCP connection per http request. This likely causes minor delays on the front-end through gateway, but this hampers performance greatly in job scheduling. The batch driver is forced to establish a new TCP connection and do an SSL handshake with the internal-gateway multiple times per job, which is expensive and slow. We currently have to dedicate a 2-core NGINX sidecar for the batch-driver just to terminate",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:399,config,configure,399,https://hail.is,https://github.com/hail-is/hail/pull/12095,2,['config'],['configure']
Modifiability,"This is a name to IP address and port service. GKE exposes pod IPs onto our VDC; network. As such, regular Google Cloud VMs can access pods by IP. GKE cannot; expose our services as IPs on our VDC because the way services load balance; traffic is more complex than DNS can handle. We acknowledge and accept the; limitations of client-side load balancing. In particular, if there are not many; clients and clients re-use address-port-pairs traffic will likely be; unbalanced. This is not a problem for the planned Shuffle service because the; clients are intended to be numerous (consider all the workers in a Query or; Batch pipeline). The big change is that deploy config now has an `addresses` function which will; return a list of address-port pairs. Deploy config also now has `address` which; randomly chooses one of the address-port pairs. I have included a simple test. Please review both code and overall design, considering how it fits in the wider system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9129:666,config,config,666,https://hail.is,https://github.com/hail-is/hail/pull/9129,2,['config'],['config']
Modifiability,"This is a pod running in default that mounts deploy-config and database-server-config. `mysql` will connect to the database as root. Useful for troubleshooting. I often have it running, but figure it should be official. Doesn't run in test since it doesn't have a database-server-config.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7819:52,config,config,52,https://hail.is,https://github.com/hail-is/hail/pull/7819,3,['config'],['config']
Modifiability,"This is a pretty simple change, though the diff is a little bizarre. I don't want running a ukbb server to be a requirement of running a hail instance. Since the ukbb app is a special case in the way we deploy apps in k8s, we would render the gateway configuration server blocks with the logic:. - For each service, if it's not ukbb, render it the usual way; - Render the ukbb block. This is a very simple change to instead make the logic. - For each service, if it's ukbb render it the ukbb way, else render it the usual way. Together with the separation of the ukbb terraform into its own module in #10842, it should be easier to deploy hail without the ukbb app and simplify the bootstrap process. This is currently running in hail-vdc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10845:251,config,configuration,251,https://hail.is,https://github.com/hail-is/hail/pull/10845,1,['config'],['configuration']
Modifiability,"This is a prototype for just build 37. My plan is to add other genome builds, expose as variable in HailContext, and add support for reading JSON from a file not in the Java resources in subsequent PRs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1780:88,variab,variable,88,https://hail.is,https://github.com/hail-is/hail/pull/1780,1,['variab'],['variable']
Modifiability,"This is a rebased update to the closed PR #3715. Updates:. - `locus_table` is now `locus_expr`, an expression on a table or matrix table. This is more flexible, and avoids unnatural requirements on key (rather than order). - uses `global_position` and requires/checks ascending order, rather than reordering within the method. Re-ordering, if non-trivial, would silently invalidate the results with respect to the row-order of the source. This also avoids a potentially unnecessary shuffle, and addresses the issue that contig order may not be alphabetical in the reference. - keeps the size of data collected close to the minimal data necessary (in particular, no collection of contigs). When `coord_expr` is not set, its an array of int. When `coord_expr` is set, its an array of (int, float). A tuple of arrays would be better, but directly collecting two arrays in order would require two actions with the current infrastructure since agg.collect() does not preserve order.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3873:151,flexible,flexible,151,https://hail.is,https://github.com/hail-is/hail/pull/3873,1,['flexible'],['flexible']
Modifiability,"This is a series of small refactorings in AppendOnlyBTree that removes; the folds in code generation, replacing a pattern that I find difficult; to follow with a more imperative CodeBuilder style that is necessary for; future changes to proceed as the folding style does not work well with; functions that need code builders.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9850:26,refactor,refactorings,26,https://hail.is,https://github.com/hail-is/hail/pull/9850,1,['refactor'],['refactorings']
Modifiability,"This is a simple refactor, that moves `Stream[A]` to a top-level class. Some of my wip depends on this, so it will be helpful to get it merged. This should also simplify merging `Stream` with the PType infrastructure, where `Stream` should become a `PCode`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8547:17,refactor,refactor,17,https://hail.is,https://github.com/hail-is/hail/pull/8547,1,['refactor'],['refactor']
Modifiability,"This is a simple refactoring of `lir.Emit` to directly use the core visitor based interface of ASM, rather than the higher level `tree` interface. This should have a small performance benefit, as we aren't building the in-memory tree representation only to immediately walk it with a visitor. But I also find this version of `Emit` slightly cleaner. For reference, you can find the documentation for ASM 5.1 [here](https://javadoc.io/doc/org.ow2.asm/asm/5.1/index.html).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9511:17,refactor,refactoring,17,https://hail.is,https://github.com/hail-is/hail/pull/9511,1,['refactor'],['refactoring']
Modifiability,"This is a simple tidying refactoring to hide the `children` array from consumers of `BaseIR`. My main motivation is to make it easier to explore other ir data structure designs, and to migrate to a new design in the future, e.g. to allow for in-place mutation without requiring large-scale changes to every compiler pass, and to simplify how we encode binding structure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13214:25,refactor,refactoring,25,https://hail.is,https://github.com/hail-is/hail/pull/13214,1,['refactor'],['refactoring']
Modifiability,"This is a step towards requiring names on all generated objects.; - {Emit}{Class, Method, Function}Builder now takes a type parameter that represents (a supertype) of the class being built: e.g. MethodBuilder[C] is a builder for a method on a class of type C. Note, we can't have a type parameter that represents the actual class type because that doesn't exist yet.; - {Emit}FunctionBuilder all but gone: {Emit}FunctionBuilder is now just a {Emit}MethodBuilder is an apply method. Most functionality moved to {Emit}ClassBuilder.; - Added EmitClassBuilder.; - It is convenient to have e.g. MethodBuilder support the ClassBuilder interface: this is what the Wrapped traits are for: MethodBuilder extends WrappedClassBuilder and ClassBuilder extends WrappedModuleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class Funct",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8335:1260,extend,extends,1260,https://hail.is,https://github.com/hail-is/hail/pull/8335,1,['extend'],['extends']
Modifiability,This is a trivial (non-semantic) refactor.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10802:33,refactor,refactor,33,https://hail.is,https://github.com/hail-is/hail/pull/10802,1,['refactor'],['refactor']
Modifiability,"This is an adaptation of my comment on the TLS PR. I moved the old `tls.md` to `tls-cookbook.md`. Git doesn't realize that. Dania & @catoverdrive, y'all are probably the two folks most likely to benefit from tls.md, so I'd appreciate your comments on the readability of this document.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9196:11,adapt,adaptation,11,https://hail.is,https://github.com/hail-is/hail/pull/9196,1,['adapt'],['adaptation']
Modifiability,"This is an attempt to modularize / refactor our terraform code regarding google service accounts and kubernetes secrets. This doesn't add any new functionality. Currently, our use of terraform is one flat file `main.tf` where we declare every `resource` in GCP that should exist. Examples of such resources are `google_service_account`, `google_service_account_key` and `kubernetes_secret`. For each of the accounts we create for various services, we end up creating these three resources (and sometimes IAM roles) in the same way. To abstract this, we can create a custom `module`, which is just a collection of resources, a set of inputs called `variables`, and a set of outputs. A module can then be ""instantiated"" using a `module` block in `main.tf`, providing it the source path of the module and values for its declared variables. Tested by hand in my own GCP project.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10785:35,refactor,refactor,35,https://hail.is,https://github.com/hail-is/hail/pull/10785,3,"['refactor', 'variab']","['refactor', 'variables']"
Modifiability,This is an enhancement of #7498. We adjust the number of cores in a user's request to make sure they have enough memory based on the memory per core rather than giving them the exact resources they asked for.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7583:11,enhance,enhancement,11,https://hail.is,https://github.com/hail-is/hail/pull/7583,1,['enhance'],['enhancement']
Modifiability,This is currently blocked by `hailctl config set batch/bucket` not accepting a full GCS uri and only accepts the bucket,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10960#issuecomment-942622228:38,config,config,38,https://hail.is,https://github.com/hail-is/hail/pull/10960#issuecomment-942622228,1,['config'],['config']
Modifiability,"This is currently just dead code, although it could conceivably be useful in the future; I want to remove this for now as it's pretty simple to add back in at a later date (follows the pattern of Serialize/Deserialize Aggs pretty much exactly) and makes for less code to keep refactoring as we optimize the aggregator stuff.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6813:276,refactor,refactoring,276,https://hail.is,https://github.com/hail-is/hail/pull/6813,1,['refactor'],['refactoring']
Modifiability,"This is good. We should do the same thing with IntIterator, etc. (Maybe call it `SIterator` for specialized iterator?). A few remarks:. - Looking over the bytecode, making the members private[this] makes the bytecode much tighter since it doesn't generate accessor methods for b and size_. I'll make a quick PR for this. - Even tho ArrayBuilder is invariant, the specialized versions extend ArrayBuilder[Object] and implement all the generic, unspecialized methods. That worries me, but I don't know why it would ever get called. Maybe for backward compatibility to code compiled without specialization?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1549#issuecomment-287082175:384,extend,extend,384,https://hail.is,https://github.com/hail-is/hail/pull/1549#issuecomment-287082175,1,['extend'],['extend']
Modifiability,"This is great, thanks for working on this!. > I have not added an example to the documentation that uses a matrix table yet. (This is an https://github.com/hail-is/hail/issues/13481.) I wanted to get some advice about the best way to do this. I think ideally, the example would have a binary phenotype, an allele to test for association, and some stratifying variable. I tried to search through the existing code to find suitable example matrix tables in the docstrings, but I didn't find anything promising. I would appreciate help here. The code that sets up the doctest environment is [here](https://github.com/hail-is/hail/blob/8a0e8e3375f1fc11efb5a443a350a8b4e8a24950/hail/python/hail/conftest.py#L55). In particular, the matrixtable available in doc examples as `ds` lives at `hail/hail/python/hail/docs/data/example.mt`. It has lots of fields you can use:; ```; In [3]: mt = hl.read_matrix_table('python/hail/docs/data/example.mt'). In [4]: mt.describe(); ----------------------------------------; Global fields:; 'global_field_1': int32; 'global_field_2': int32; 'pli': dict<str, float64>; 'populations': array<str>; ----------------------------------------; Column fields:; 's': str; 'sample_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; 'is_case': bool; 'pheno': struct {; is_case: bool,; is_female: bool,; age: float64,; height: float64,; blood_pressure: float64,; cohort_name: str; }; 'cov': struct {; PC1: float64; }; 'cov1': float64; 'cov2': float64; 'cohort': str; 'cohorts':",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1932814827:359,variab,variable,359,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1932814827,1,['variab'],['variable']
Modifiability,"This is mostly straightforward, except in the case of PBinary and PString, where I elected to move static methods to instance methods. This was done because these methods completely depend on the PType, and having them as static methods prevents use of non-canonical versions of these methods (regardless of where they are). This includes functions like allocate, which deal with memory layout, and therefore must be configurable by ptype. Places where these static methods are used often include places where a PString or PBinary are passed around. Will finish this up after I get back most likely, or we can punt on the PStirng/PBinary issue for later (but I think it's worth doing now for the reasons outlined above). Stacked on https://github.com/hail-is/hail/pull/7903; ping @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7904:417,config,configurable,417,https://hail.is,https://github.com/hail-is/hail/pull/7904,1,['config'],['configurable']
Modifiability,This is needed for future genome reference pull requests to be able to access the ordering from the GenomeReference after the variable GR has been substituted for.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2528:126,variab,variable,126,https://hail.is,https://github.com/hail-is/hail/pull/2528,1,['variab'],['variable']
Modifiability,"This is not because we forgot to unfreeze CI, we just have simply never added the dockerhub images to azure automatically. The couple that are there now (only 107 and 112) must have been uploaded manually. Because there are some build.yaml steps that run on deploy that are specific to the broad GCP instance (like maybe making a release), non-hail-vdc instances don't run the whole build.yaml pipeline on deploy, but a subset that are specified through terraform (this is how AUS and MS could decide to only deploy a subset of our services e.g. not monitoring. We somewhat recently added a step (separate from the `deploy` step) called `mirror_hailgenetics_images` that was entirely intended so that other hail deployments (including ourselves on Azure!) could pick up the images that we released to dockerhub. I never added that steps to the Azure CI's config. I have done that now. Somehow I had foreseen this incident happening and when it actually did any prior on it disappeared from my brain entirely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1572657390:855,config,config,855,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1572657390,2,['config'],['config']
Modifiability,"This is often the case when passing scripts to languages such as Perl, where extraneous backslashes inside the script can change its meaning or even be a syntax error. One simple way of handling this is to place the quoted string, or even the entire command, into a make variable then use the variable in the recipe. In this situation the newline quoting rules for makefiles will be used, and the backslash/newline will be removed. If we rewrite our example above using this method:; > ; > ```; > HELLO = 'hello \; > world'; > ; > all : ; @echo $(HELLO); > ```; > ; > we will get output like this:; > ; > ```; > hello world; > ```; >; > If you like, you can also use target-specific variables (see [Target-specific Variable Values](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html)) to obtain a tighter correspondence between the variable and the recipe that uses it. It seems to me like there are not any great choices. Putting the JSON into a Make variable seems too magical and likely to confuse a newbie editing this file. Using escaped double quotes is less legible than literal JSON. Putting the whole JSON array on one line is quite long. I guess we can go with double quotes for now. I tested on Make 3.81 and Make 4.4.1. The first EDIT and the original comment follow for context. ---. EDIT: Nope, I still appear to be wrong. Hold on. ---. I have bash 3.2.57; ```; (base) dking@wm28c-761 /tmp % make print-shell; /bin/sh; (base) dking@wm28c-761 /tmp % /bin/sh --version; GNU bash, version 3.2.57(1)-release (arm64-apple-darwin22); Copyright (C) 2007 Free Software Foundation, Inc.; ```. Looks like this was an intentionally backwards incompatible change [in Make 4.0](https://git.savannah.gnu.org/cgit/make.git/tree/NEWS?h=4.0&id=52191d9d613819a77a321ad6c3ab16e1bc73c381#n18) which removed the POSIX-compatible behavior on which our Makefile relies:; ```; * WARNING: Backward-incompatibility!; If .POSIX is specified, then make adheres to the POSIX backslash/newli",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324:1885,variab,variable,1885,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324,2,['variab'],['variable']
Modifiability,This is one piece of a multi-part saga to use tls for any and all communication between pods. This basically just required adding the `create_certs` step to the ci test `build.yaml` so we can create a ssl config for `hello`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10192:205,config,config,205,https://hail.is,https://github.com/hail-is/hail/pull/10192,1,['config'],['config']
Modifiability,This is pretty bare bones but I thought this might help a lot with the testing of all your upcoming `hailctl config` changes rather than having to manually test various possible combinations. Run by just invoking pytest: `pytest hail/python/test/hailtop/hailctl/config`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13389:109,config,config,109,https://hail.is,https://github.com/hail-is/hail/pull/13389,2,['config'],['config']
Modifiability,"This is primarily a condensation and formatting of what you have above. Is there more to document?. ### `filteralleles`. #### Usage; - `-c | --condition <expr>`—a hail language expression, the following table describes the variables in the scope of `<expr>`. | Name | Description |; | --- | --- |; | `v` | variant |; | `va` | variant annotations |; | `aIndex` | allele index |; - `--keep/--remove`—keep or remove the allele if the expression is true; - `-a | --annotation <expr>`—a hail language expression which may update the variant annotations based on the removed alleles, the following table describes the variables in the scope of `<expr>`. | Name | Description |; | --- | --- |; | `v` | the _new_ variant |; | `va` | the _old_ variant annotations |; | `aIndices` | an array of the old indices (such that `aIndices[newIndex] = oldIndex`) |. _NB:_ the allele indices are zero indexed and the zeroth index contains the reference. The `condition` expression will be executed for all alleles with index greater than zero. _NB2:_ if all alternate alleles are filtered the entire variant is filtered. #### Examples. ```; ... filteralleles; --keep; -c 'va.alleleQuality[aIndex] > 0.8'; -a 'va.info.AC = aIndices.map(oldIndex => va.info.AC[oldIndex]), va.info.AN = ...'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/551#issuecomment-240216457:223,variab,variables,223,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240216457,2,['variab'],['variables']
Modifiability,"This is really great. . I have some thoughts below, mostly brain storming. Don't take any of it too seriously. Some thoughts:. 1. I thought you wanted to support f-strings. By making the batch file quote double curly parens, that means if you use them in an f-string, you need to write `f'{{{{foo}}}}'` which is a bit much. But that means that non-batch uses of `{}` need to be double-quoted, so `awk '{{ ... }}'` and `f""awk '{{{{ ... }}}}'""`. Hmm. Maybe using the same escape syntax as f-strings is not ideal. . I don't have a no-brainer suggestion. Happy to brainstorm ideas offline. I think ultimately this is a minor syntactic choice. 2. Inputs seem ... almost redundant, because they also appear in the command strings. What about:. ```; .command('shapeit --bed-file {{<subset.ofile}} --chr ' + contig + ' --out {{>ofile}}'); ```. Then the question becomes, how do associate `subset` with the corresponding Python variable? You could use the task label, but then the user has to maintain two sets of names, which isn't ideal. Hmm, maybe this doesn't work. 3. I like arrays of resources!. > `.command('cat {{files}} >> {{ofile}}')`. I wonder, will we ever want arrays to be formatted other than joined with spaces? I worry the user will want more flexibility in formatting, and we'll want that in Python. What about if the argument is a function, it takes a dictionary from resource names to their string representation, and you can format however you want? Then you could write the last command as:. ```; .command(lambda rs: f'cat {' '.join(rs['files'])} >> {rs['ofile']}'); ```. 4. I was confused by this:. > `p.write_output(merger.ofile + "".haps"", ...)`. What's the left hand side? Why isn't this just `merger.ofile`?. This suggests another issue: what if you want to use `ofile` in a plink command, but plink outputs some files with various extensions with `ofile` as the base? We might need an `outputs` that lists (docker local) output files based on a base path.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-446733609:919,variab,variable,919,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-446733609,1,['variab'],['variable']
Modifiability,"This is runIfRequested deployment that simply has hail; and ipython installed. It facilitates developmnent of; services that interact with Hail Query (i.e. the Shuffler). I do this silly thing with a tar file because:; - I do not know the hail version (which is included in the wheel filename), so; - I am unable to copy it out with a variable name, and; - python refuses to install a wheel that does not have the version in the filename. I added `make update-hail-repl` to `hail/Makefile` which updates the hail wheel on the hail-repo without changing the pod or rebuilding the image. If the pod is restarted you lose your version, but the risk is worth the immense benefit of 5s deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8194:335,variab,variable,335,https://hail.is,https://github.com/hail-is/hail/pull/8194,1,['variab'],['variable']
Modifiability,"This is running on hail-vdc/vdc. Summary:; - parameterize project; - gateway option to forward to letsencrypt only for bootstrapping; - service accounts for gateway, letsencrypt; - mysql instance running. I did not make default:default cluster admin, so it should have no special privileges. Changes are not yet being applied automatically.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4725:45,parameteriz,parameterize,45,https://hail.is,https://github.com/hail-is/hail/pull/4725,1,['parameteriz'],['parameterize']
Modifiability,"This is small addition on top of #2665, only review last commit if that has yet to go in. Here's the logic/plan: rows refers to actual rows as in RowMatrix or MatrixTable. nRows refers to the number of rows, as already used in GridPartitioner, RowMatrix, etc. Breeze uses mat.rows for nRows, but we'll still use nRows as variable name of number of rows in Hail's linear algebra. In Python, we'll use num_rows and num_cols.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2698:321,variab,variable,321,https://hail.is,https://github.com/hail-is/hail/pull/2698,1,['variab'],['variable']
Modifiability,This is stacked on https://github.com/hail-is/hail/pull/9842 which rewrites hailctl argument handling. Probably not worth reviewing until that is in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9913:67,rewrite,rewrites,67,https://hail.is,https://github.com/hail-is/hail/pull/9913,1,['rewrite'],['rewrites']
Modifiability,"This is super cool! I am a big fan of the idea and the overall approach, particularly when it comes to setting up the tmp bucket and getting the permissions on it correct. Here's my high level thoughts. Sorry for the wall of text but I found these a little hard to articulate. ### Regarding number of prompts. I think this is my primary concern. There's a lot of great automation here, but it's a lot right off the bat. I think what this is aiming to do is make it quick and simple to start running batches and every time someone has to stop and ask someone a question as to how they should respond to some prompt that process gets longer and more complicated. I think it's worth considering what the first batch people should run might be and design for a minimal first experience. IMO, a temp bucket is an absolutely crucial piece of configuration before you can do anything interesting and configuring a temp bucket is something that `hailctl` can easily be very opinionated about. Container registry… I feel like there's harder questions there, and you can run a lot of cool batches before having to worry about provisioning your own. It's also not actually a part of the hailctl config (unless something has changed recently) so it feels a little unusual in this flow. I still think that it is helpful to set people up with an AR and keep them from footguns, but maybe that can go in a separate command that the initial init command points to once you're done? Something along the lines of ""if you get to the point where you need to upload custom container images, you can use hailctl to set up a registry""?. Another thing that gives me a little pause is the wording around google projects. I get that you need one to create a bucket, but I think we should just make sure to steer clear of the implication that you are ""selecting a GCP project to use for Hail Batch"", because that implies some link or ownership that isn't there. But I think there's a quick fix here: for a given resource that we",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012:836,config,configuration,836,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012,4,['config'],"['configuration', 'configuring']"
Modifiability,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:343,config,configuration,343,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881,1,['config'],['configuration']
Modifiability,"This is the common use case for collapsed burden testing, and will eventually be the input to linreg_burden and logreg_burden. The resulting table can by pushed to Python or PySpark for flexible analysis. Unlike in the burden regression methods, this table need not have numeric values, and it doesn't filter out samples with missing phenotype or covariates (since there are none to speak of here).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1765:186,flexible,flexible,186,https://hail.is,https://github.com/hail-is/hail/pull/1765,1,['flexible'],['flexible']
Modifiability,"This is the current state of the C++ support. If you look at the tests in src/test/is/hail/nativecode/NativeCodeSuite.scala that should give a; quick overview of how it works, viz. 1. Generate C++ source code as a Scala String, then create a NativeModule which handles; the grunt work of getting it compiled, linked, and loaded, and allows you to look up functions; by name, and get a callable Scala object corresponding to the C++ function. 2. The NativeModule also allows the binary of the DLL to be passed around and instantiated; on other cluster nodes (but note that those nodes will need to have the correct versions of; the C++ runtime shared libraries in the right directories to allow symbols in the DLL to be; correctly resolved). This is not tested yet. 3. I have been using llvm-6.0.0 on Mac, and llvm-5.0 on linux. It makes a half-hearted attempt; to use whatever other compiler you have, but that may not work. We probably need to figure; out a standardized and automated way to get the right tools installed in the right place (and; get the right shared libraries on worker nodes). 4. Data which needs to be accessible to both Scala and C++ is held in C++ objects inheriting; from NativeObj, with lifetimes managed by std::shared_ptr, i.e. reference-counted. There's; some dirty under-the-hood plumbing to allow a shared_ptr to be smuggled into a Scala; object derived from NativeBase. These Scala-side object references must be managed; carefully using copyAssign/moveAssign/close in order to maintain the off-heap ref-counts. 5. There are some gnarly differences between Linux and MacOSX in the linker and dynamic; loading. I think I'm converging on the right compile/link options for each, but in getting; Linux to work it's possible that Mac is temporarily broken ... Not really expecting that we'll merge this right away, but I wanted to put it out there to get the; review process started before it grows any bigger.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3461:1179,inherit,inheriting,1179,https://hail.is,https://github.com/hail-is/hail/pull/3461,1,['inherit'],['inheriting']
Modifiability,"This is the final stop for now on our nginx tour. This builds on #10207, which altered `internal.hail.is` use k8s dns instead of router/router-resolver to proxy directly to services, and does so in default now as well. Unlike #10207, however, traffic coming in through `hail.is` is not necessarily authenticated, and we do not expose just any k8s service that happens to exist. Instead, I've altered `letsencrypt/domains.txt` to now be `letsencrypt/subdomains.txt` and templated the gateway config to generate explicit server blocks for each subdomain in `subdomains.txt`. This enforces that you cannot expose a service unless it is also listed in the `letsencrypt` directory (the dev must still remember to regenerate the certs). Now, the process for exposing a service is:; - Add a subdomain to `subdomains.txt`; - Make a k8s Service with the name of the subdomain that points to new app; - Regenerate certs and redeploy gateway. Also added a default server block that returns a 444 (no response) for invalid subdomains. Unfortunately this still presents to the user that the cert is invalid, since *.hail.is is registered in dns (I think?) and browsers will verify certificates before anything else, but users won't be able to click through and land at the website like they could before.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10247:491,config,config,491,https://hail.is,https://github.com/hail-is/hail/pull/10247,1,['config'],['config']
Modifiability,"This is the first step to removing batch workers' reliance on the docker daemon and docker in general, in favor of a lower level of abstraction that gives us finer control over resources on the worker like overlays and network namespaces, allowing us to shortcut and pre-configure some of the overhead that goes into running a job. ## What this does differently; Currently, the high-level process for running a job involves communicating with the docker daemon to:; 1. Pull an image for a job; 2. Start a container from that image; 3. Run the container; 4. Delete the container and its associated resources. We offload some of these responsibilities into the worker code and onto [crun](https://github.com/containers/crun), a lightweight low-level runtime with the same API as `runc`, what docker uses to run containers. Once docker has retrieved an image, if we see that the pulled image has a new digest from one we currently have cached on the worker, we extract the image's filesystem into a directory on the worker's disk. We then:. - use `mount` to create an overlay on top of the image that the container will use as its rootfs; - use `xfs_quota` to limit the container's storage in the overlay; - invoke `crun` to run a container with the overlay as its root filesystem and an appropriate network namespace that we set up at worker-start time. Since we control the overlay, we can set the XFS quota before creating the container. So what was separate calls to docker create/start/run/delete is just a single `crun run`. Fewer steps, less back and forth with a single daemon, and pre-configuring the networks gives some sizable performance gains reliable, as well as reliable and consistent performance. ## What this doesn't solve; - Docker is still running the worker container. I don't see any real challenge to this it's just a matter of translating the docker parameters; - Still using docker to pull images and extract filesystems / environment variables from them. I don't have a substitu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10376:271,config,configure,271,https://hail.is,https://github.com/hail-is/hail/pull/10376,1,['config'],['configure']
Modifiability,"This is the precursor to a follow-up PR which removes `IntervalTree`. `IntervalTree` was doing two jobs: handling queries against the range bounds of a partitioner, which have (essentially) no overlap, and in these interval join methods, where they may overlap arbitrarily. It ended up being suboptimal for both use cases. This PR rewrites the interval join methods, which should now be faster, and no longer depends on `IntervalTree`. The existing interval join methods only supported distinct joins (`product = false`). The implementation I wrote won't work for the `product = true` case, but I have a plan for that, which will fit into the structure I started here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4487:331,rewrite,rewrites,331,https://hail.is,https://github.com/hail-is/hail/pull/4487,1,['rewrite'],['rewrites']
Modifiability,This is unused and the same as `INTERNAL_GATEWAY_IP` a few variables down,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12005:59,variab,variables,59,https://hail.is,https://github.com/hail-is/hail/pull/12005,1,['variab'],['variables']
Modifiability,This is what I was trying to tell you last week. Daniel had a PR that changed the storage URI that didn't add them from the global config yet and there were going to be merge conflicts based on who merged first. Sorry :(. #11014,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956168436:131,config,config,131,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956168436,1,['config'],['config']
Modifiability,"This is what `hailctl` looks like:. ```. Usage: hailctl [OPTIONS] COMMAND [ARGS]... Manage and monitor hail deployments. ╭─ Options ────────────────────────────────────────────────────────────────────────────────────────────╮; │ --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified │; │ shell. │; │ [default: None] │; │ --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified │; │ shell, to copy it or customize the │; │ installation. │; │ [default: None] │; │ --help Show this message and exit. │; ╰──────────────────────────────────────────────────────────────────────────────────────────────────────╯; ╭─ Commands ───────────────────────────────────────────────────────────────────────────────────────────╮; │ batch Manage batches running on the batch service managed by the Hail team. │; │ config Manage Hail configuration. │; │ curl Issue authenticated curl requests to Hail infrastructure. │; │ version Print version information and exit. │; ╰──────────────────────────────────────────────────────────────────────────────────────────────────────╯; ```. This is what `hailctl batch submit --help` looks like:. ```. Usage: hailctl batch submit [OPTIONS] SCRIPT [ARGUMENTS]... Submit a batch with a single job that runs SCRIPT with the arguments ARGUMENTS. ╭─ Arguments ──────────────────────────────────────────────────────────────────────────────────────────╮; │ * script PATH Path to the script [default: None] [required] │; │ arguments [ARGUMENTS]... [default: None] │; ╰──────────────────────────────────────────────────────────────────────────────────────────────────────╯; ╭─ Options ────────────────────────────────────────────────────────────────────────────────────────────╮; │ --files PATH Files or directories to add to the working directory of the │; │ job. │; │ [default: None] │; │ --name TEXT The name of the batch. │; │ --image-name TEXT Name of Docker image for the job │; │ [default: (hailgenetics/hail)] │; │ --ou",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13109#issuecomment-1561081921:859,config,config,859,https://hail.is,https://github.com/hail-is/hail/pull/13109#issuecomment-1561081921,2,['config'],"['config', 'configuration']"
Modifiability,"This is why copying is so slow:. ```; ==> NOTE: You are uploading one or more large file(s), which would run; significantly faster if you enable parallel composite uploads. This; feature can be enabled by editing the; ""parallel_composite_upload_threshold"" value in your .boto; configuration file. However, note that if you do this large files will; be uploaded as `composite objects; <https://cloud.google.com/storage/docs/composite-objects>`_,which; means that any user who downloads such objects will need to have a; compiled crcmod installed (see ""gsutil help crcmod""). This is because; without a compiled crcmod, computing checksums on composite objects is; so slow that gsutil disables downloads of composite objects. / [1/1 files][ 4.1 GiB/ 4.1 GiB] 100% Done 45.8 MiB/s ETA 00:00:00; Operation completed over 1 objects/4.1 GiB.; ```. We can also set this with -o GSUtil:parallel_composite_upload_threshold on the command line. https://cloud.google.com/storage/docs/gsutil/commands/cp. We currently use `-m` which is parallel per-file:. If you have a large number of files to transfer you might want to use the; gsutil -m option, to perform a parallel (multi-threaded/multi-processing); copy:. gsutil -m cp -r dir gs://my-bucket",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024:277,config,configuration,277,https://hail.is,https://github.com/hail-is/hail/pull/7024,1,['config'],['configuration']
Modifiability,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:944,config,config,944,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251,1,['config'],['config']
Modifiability,"This isn't as easy as I had hoped. We have to sort out how to either let the container directly create overlay mounts or figure out how to get fuse-overlay working. I think for fuse-overlay, we might need to modify the VM image to include fuse-overlay. ```; + set +x; Using GOOGLE_APPLICATION_CREDENTIALS; + export TMPDIR=/io/; + TMPDIR=/io/; + retry buildah build -t us-docker.pkg.dev/hail-vdc/hail/git-make-bash:test-deploy-j6d7pph9mlzf -f /Dockerfile --cache-from us-docker.pkg.dev/hail-vdc/hail/cache --cache-to us-docker.pkg.dev/hail-vdc/hail/cache --layers /io; + buildah build -t us-docker.pkg.dev/hail-vdc/hail/git-make-bash:test-deploy-j6d7pph9mlzf -f /Dockerfile --cache-from us-docker.pkg.dev/hail-vdc/hail/cache --cache-to us-docker.pkg.dev/hail-vdc/hail/cache --layers /io; STEP 1/2: FROM us-docker.pkg.dev/hail-vdc/hail/ubuntu:20.04; Trying to pull us-docker.pkg.dev/hail-vdc/hail/ubuntu:20.04...; Getting image source signatures; Copying blob sha256:ca1778b6935686ad781c27472c4668fc61ec3aeb85494f72deb1921892b9d39e; Copying config sha256:88bd6891718934e63638d9ca0ecee018e69b638270fe04990a310e5c78ab4a92; Writing manifest to image destination; Storing signatures; time=\""2023-05-26T14:52:12Z\"" level=error msg=\""Unmounting /var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/merged: invalid argument\""; Error: mounting new container: mounting build container \""45e0ed631d22b6e1de7945266efcf0b802aa3b919d6b6ebd529ded6fedc11cf9\"": creating overlay mount to /var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/merged, mount_data=\""lowerdir=/var/lib/containers/storage/overlay/l/ZCKOX3GV2VWHWT4DMPLYJGMJWL,upperdir=/var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/diff,workdir=/var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/work,nodev,fsync=0,volatile\"": using mount program /usr/bin/fus",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13103#issuecomment-1564774692:556,layers,layers,556,https://hail.is,https://github.com/hail-is/hail/pull/13103#issuecomment-1564774692,2,['layers'],['layers']
Modifiability,"This isn't quite right. The `check` method will have no way to inspect the context of a sampled value (particularly, the sampled boolean from a proposition). I think `Gen[T]` should probably be a `Reader[Parameters, (Seq[Any], T)]` where the `Seq[Any]` are the witnesses for each universally quantified variable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/400#issuecomment-238904307:303,variab,variable,303,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238904307,1,['variab'],['variable']
Modifiability,"This isn't what I intended with my original design and I'm trying to figure out what you did here. - I intended for entire file, not individual specs, to have a version number. That's why it is in RelationalSpec at the top level.; - FooSpec (e.g. RelationalSpec, RVDSpec, etc.) is supposed to be an abstract class that implements the interface used by the main code for the entity in question. E.g. CodecSpec knows how to build encoders and decoders. It can have many implementations.; - The file version essentially determines which Spec implementations are allowed in the file. Each spec should have a notion of when it was introduced (or deprecated, if we bump the major version) but each implementation should have its own name/version. I was imagining FooRVD2Spec if we needed a new version of the existing RVD specs.; - The type hints in the JSON determine what Spec implementation to use (e.g. UnpartitionedRVDSpec). With this design, you can't change name of an existing spec.; - The Spec is also the intended place for code which matches the interface (which might evolve) with the legacy data in the JSON file (which cannot change).; - The file format is only used to check compatibility. The Spec class hierarchy should drive the decoding of the Specs, not the file format.; - Specs that are not being generated anymore (but still need to be read) should go into a compatibility package. I think following this design will be cleaner. This is exactly the kind of forward-extensibility I designed it to handle cleanly, although I might very well have messed something up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4734#issuecomment-436661120:1074,evolve,evolve,1074,https://hail.is,https://github.com/hail-is/hail/pull/4734#issuecomment-436661120,1,['evolve'],['evolve']
Modifiability,"This issue could have been an RFC, but that felt too heavy. I can move this to a formal RFC if desired, but otherwise feedback and/or questions welcome in the discussion here. # Idea; For any key type, create an encoding to variable-length byte arrays, which preserves the key ordering. That way, algorithms and data structures which use key comparisons can be written monomorphically, with `memcmp` as the only comparison function needed. Idea inspired by [Fast and Memory Efficient Multi-Column Sorts in Apache Arrow Rust](https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-2/) blog post. But while they've optimized for vectorized encoding (which we currently can't do), I've preferred simplicity and smaller encodings. # Design; Type encoders can emit three kinds of output to a byte array buffer:; - byte - simply add a byte to the result, first padding an incomplete byte if necessary; - bit - add a bit to the result, possibly leaving an incomplete byte. We must know statically how many bits are used in the byte.; - pad - add `0`s to pad the last incomplete byte. This is safe (prefix-free) because the number of used bits is a (statically known) constant. We use this to ensure the number of used bits is known statically.; 	; Types:; - missingness; - treat as a type constructor `optional<T>`, i.e. base types don't encode missingness. Emits a single bit in the encoding. Can invert this bit to control whether missing values come first or last in the ordering. If missing, nothing is emitted after.; - sort-order; - treat reversing the default ordering as a type constructor `reverse<T>`; - simply inverts the encoding bitwise; - primitive types; - same as in datafusion, encoding has same size as original type; - signed integers - flip the sign bit; - floating point numbers - if sign bit is set, invert all bits, otherwise only flip the sign bit; - arrays; - before each element and after last element, emit continuation bit (0 if no more elements); - pad be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14396:224,variab,variable-length,224,https://hail.is,https://github.com/hail-is/hail/issues/14396,1,['variab'],['variable-length']
Modifiability,"This larger benchmark shows clearer separation between the old pc-relate approach and the current one. this branch (which include's master's improvements); ```; 2020-01-27 13:16:20,975: INFO: [1/1] Running pc_relate_big...; 2020-01-27 13:18:12,886: INFO: burn in: 111.90s; 2020-01-27 13:19:56,255: INFO: run 1: 103.35s; 2020-01-27 13:21:46,801: INFO: run 2: 110.54s; 2020-01-27 13:23:39,147: INFO: run 3: 112.37s; {""config"": {""cores"": 1, ""version"": ""0.2.31-68d448411ab5"", ""timestamp"": ""2020-01-27 13:23:39.157122"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""pc_relate_big"", ""failed"": false, ""timed_out"": false, ""times"": [103.35172498200001, 110.53654034999997, 112.369625832]}]}; ```; before improvements `becbbc6d2` (run against this branch's new benchmark); ```; 2020-01-27 13:25:15,789: INFO: [1/1] Running pc_relate_big...; 2020-01-27 13:27:25,725: INFO: burn in: 129.92s; 2020-01-27 13:29:44,260: INFO: run 1: 138.48s; 2020-01-27 13:31:49,675: INFO: run 2: 125.40s; 2020-01-27 13:33:59,580: INFO: run 3: 129.86s; ```. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7975:416,config,config,416,https://hail.is,https://github.com/hail-is/hail/pull/7975,1,['config'],['config']
Modifiability,"This leverages open batches to submit all QoB stages as updates to the same batch the Query Driver is running in. Most of this implementation feels uncontroversial, but there is a backwards-incompatible change to the JVMJob that I don't love. I needed to let the Query Driver know which batch it's running in and did so by adding that as another argument to main. I initially wanted this as an environment variable, but considering that these containers are long-lived I wasn't quite sure how to do that.; I didn't stack this on open batches so this won't work until that goes in but the changes are entirely disjoint",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12222:406,variab,variable,406,https://hail.is,https://github.com/hail-is/hail/pull/12222,1,['variab'],['variable']
Modifiability,"This leverages the Indeed LSM tree. It implements this API:; - `start(...)`; - `put(x1, ...)` (keys are extracted from the records themselves); - `get(l, r)` which takes two key records and retrieves the values in `[l, r)`. There's a server (`ShuffleServer.scala`) and a client (`ShuffleClient.scala`). They communicate over TLS-secured TCP/IP sockets on a configurable port. The server has one thread per client socket. The client is currently single-threaded. I had to add a `log4j.properties` because I don't start a HailContext and log4j gets upset when you don't configure it. Files; - `HailLSM.scala` - This wraps the Indeed LSM tree with some shims so that we encoders and decoders use `InputStream` and `OutputStream` instead of these were `Data...` interfaces.; - `HailSSLContext.scala` - This implements creation of an actually secure `SSLContext` from a key store and a trust store. It requires clients to identify themselves with a trusted certificate.; - `ShuffleClient.scala` - Self-explanatory.; - `ShuffleServer.scala` - Three classes: `Handler` corresponds to a client connection. It has its own thread. `Shuffle` owns the `Region` , the LSM tree, and the encoder/decoders. `ShuffleServer` waits for connections and spawns threads. It owns the executor service.; - `ShuffleUtils.scala` - Odds and ends.; - `Wire.scala` - Serializers and deserializers for various things. Includes renames that help me keep everything sensible (e.g. for every X I use, I have ""writeX"" and ""readX"").; - `ShuffleSuite.scala` - One test: write 1,000,000 randomly ordered numbers into the LSM tree and read them all back in order. Takes about 1 minute. Obviously we need to dramatically improve the performance of that (I think this should take not longer than one second).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8361:357,config,configurable,357,https://hail.is,https://github.com/hail-is/hail/pull/8361,2,['config'],"['configurable', 'configure']"
Modifiability,This logic can happen at config time instead of runtime.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11026:25,config,config,25,https://hail.is,https://github.com/hail-is/hail/pull/11026,1,['config'],['config']
Modifiability,This means people who don't use hailctl get these configurations.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7739:50,config,configurations,50,https://hail.is,https://github.com/hail-is/hail/pull/7739,1,['config'],['configurations']
Modifiability,"This mirrors the functionality available on `TypedCodecSpec`. In some cases (the shuffler),; you might be handed a buffer that is already configured, but you still want to create a; decoder whose PType is known to be a subtype of PStruct.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8766:138,config,configured,138,https://hail.is,https://github.com/hail-is/hail/pull/8766,1,['config'],['configured']
Modifiability,"This moves the nginx proxy configuration out of router and into a sidecar in the notebook/workshop pod. This extends TLS termination from router to the notebook pod and consolidates the notebook routing logic. I didn't run a scale test but this doesn't change any functionality, and I tested in dev that I could log in to a workshop, start and open a notebook.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10204:27,config,configuration,27,https://hail.is,https://github.com/hail-is/hail/pull/10204,2,"['config', 'extend']","['configuration', 'extends']"
Modifiability,This needs to be recreated once #2519 (on which this depends) is adapted to handle GenomeReference.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2516#issuecomment-350801948:65,adapt,adapted,65,https://hail.is,https://github.com/hail-is/hail/pull/2516#issuecomment-350801948,1,['adapt'],['adapted']
Modifiability,"This opens the possibility for compiler differences to fail builds for our users. The CI server should simply set CXX and CC to clang and rebuild hail. Moreover, we need to ensure the hail build system passes these variables all the way down to `libsimdpp`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327:215,variab,variables,215,https://hail.is,https://github.com/hail-is/hail/issues/1327,1,['variab'],['variables']
Modifiability,"This probably needs a little cleanup. What's this for? Well, in another branch I have a bunch of IR rewrite optimizations. Those rewrite rules (1) want to test types (e.g. eliminate a cast of a type to itself), and they (2) also want to create new IRs which therefore need well-formed types. Calculating all the intermediate types explicitly (or calling Infer) everywhere both seem like non-starters. Therefore, I changed the IR nodes to compute their own types. I repurposed Infer, but it is no longer recursive. This meant that In, InAgg and Ref needed to carry their types, becuase, in the old, Infer-based way, they were dependent on the environment to type themselves, but that's no longer possible. I also repurposed Infer as a recursive type checker. This created two subtle problems: (1) the IR code uses rvRowType everywhere in stead of rowType (so it can reuse pointers to the full row) and (2) toIR needs to set the Ref type from the symbol table, but the symbol table strips out all missing bits, so the types on Ref terms disagreed with the actual values flowing around. I resolved this in two ways: (1) va now refers to the full rvRowType in all eval contexts, everywhere. (This is closer to the existing IR behavior.) (2) the symbol table no longer strips missingness, but it is stripped by Ref when the symbol is referenced. Ref also records the unstripped type which is used by toIR. The sooner we can kill AST, the better.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3332:100,rewrite,rewrite,100,https://hail.is,https://github.com/hail-is/hail/pull/3332,2,['rewrite'],['rewrite']
Modifiability,"This proposes a way to test `hailctl dataproc`, starting with `hailctl dataproc start`. 1. Move `subprocess` calls to run gcloud commands and get gcloud configuration to a separate `gcloud` module. This module serves as a convenient place to insert mocks in tests.; 2. Automatically (with pytests's `autouse`) mock calls to the `gcloud` module's methods in tests. This prevents actually running `gcloud` in tests. This also provides a pytest fixture to set the mocked `gcloud` configuration values.; 3. Add some tests for `hailctl dataproc start`. These tests pass arguments to `cli.main` and make assertions about the resulting `gcloud` command(s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9066:153,config,configuration,153,https://hail.is,https://github.com/hail-is/hail/pull/9066,2,['config'],['configuration']
Modifiability,"This pull request lowers the TableAggregate node. It also reimplements; the AggStateValue and CombOpValue nodes. However, this is not the final design -- I think that there should also; be an InitFromSerializedValue node that is used to prevent us from; initializing twice (and inlining aggs.init in the lowering). feel free to push back on merging this before that refactoring, but it works and I have a preference for making that change in a followup. After that, I'll move on to lowering TableMapRows with scans, as well as designing the pass to lift out nodes as relational lets that we discussed. Woohoo!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8991:366,refactor,refactoring,366,https://hail.is,https://github.com/hail-is/hail/pull/8991,1,['refactor'],['refactoring']
Modifiability,"This refactors the parser to use `BindingEnv`, which tracks the separate eval, agg, scan, and relational environments. This was motivated by the new randomness work, which needs to be able to rebind row and col references in both eval and agg scopes, which was breaking the parser. I don't like duplicating the (rather complicated) binding behavior of the nodes, but I couldn't find a way to avoid it without a much larger refactoring of the IR. So I tried to make the binding logic in the parser be as direct a copy of what is encoded in `Binds.scala` as possible. For example, relational nodes always pass `env.onlyRelational` to their children, which isn't necessary if we ensure that only relational bindings exist in environments passed to relational nodes, but it matches the logic in `Binds`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12289:5,refactor,refactors,5,https://hail.is,https://github.com/hail-is/hail/pull/12289,2,['refactor'],"['refactoring', 'refactors']"
Modifiability,This removes unused google-specific environment variables from the auth modules.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10899:48,variab,variables,48,https://hail.is,https://github.com/hail-is/hail/pull/10899,1,['variab'],['variables']
Modifiability,"This seems like the right approach. The other option is to expose the password in the config which we are trying to avoid. So ... even this is not idempotent. The current version of batch doesn't guarantee that children (or multiple retries of the same child) get the same or consistent set of files from their parents. This is because, while an attempt may have succeeded, there may be another attempt that is pending that succeeds and overwrites the outputs of the original successful attempt. I fixed this in my google batch backend by storing attempt outputs in per-attempt directory: /path/to/scratch/files/job_id/attempt_id/file. Then each child got the successful attempt (there could be multiple, but the driver selected one and only one) for each parent and that was used to localize the inputs. So this good enough and we should plan to add attempt consistency to Batch in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8833#issuecomment-631473428:86,config,config,86,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631473428,1,['config'],['config']
Modifiability,"This seems to do it:; ```. In [1]: import hailtop.batch as hb; ...: b = hb.Batch(backend=hb.ServiceBackend()); ...: for _ in range(32):; ...: j = b.new_job(); ...: j.command(f'cat >/dev/null {"" "".join(b.read_input(""gs://danking/foo.vcf"") for _ in range(1000))}'); ...: b.run(); /Users/dking/miniconda3/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:29: UserWarning: You have specified the GCS requester pays configuration in both your spark-defaults.conf (/Users/dking/miniconda3/lib/python3.10/site-packages/pyspark/conf/spark-defaults.conf) and either an explicit argument or through `hailctl config`. For GCS requester pays configuration, Hail first checks explicit arguments, then `hailctl config`, then spark-defaults.conf.; warnings.warn(; /Users/dking/miniconda3/lib/python3.10/site-packages/hailtop/batch/backend.py:786: UserWarning: Using an image ubuntu:22.04 from Docker Hub. Jobs may fail due to Docker Hub rate limits.; warnings.warn(f'Using an image {image} from Docker Hub. '. https://batch.hail.is/batches/8090821 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 33/33 0:00:00 0:01:37; batch 8090821 complete: success; Out[1]: <hailtop.batch_client.client.Batch at 0x1086d1bd0>. In [2]: import hailtop.batch as hb; ...: b = hb.Batch(backend=hb.ServiceBackend()); ...: for _ in range(300):; ...: j = b.new_job(); ...: j.command(f'echo {""a"" * 11 * 1024}'); ...: b.run(); ```. Perhaps related to creating a fresh service backend?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14051#issuecomment-1834376467:432,config,configuration,432,https://hail.is,https://github.com/hail-is/hail/issues/14051#issuecomment-1834376467,4,['config'],"['config', 'configuration']"
Modifiability,"This seems tricky! I expected protected variables to be only accessible from child classes. Is the reason this works that adding [ir] hoists the variable to be top-level, when used from an object of type IR, thereby making it once again accessible to sub-packages?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6594#issuecomment-515071056:40,variab,variables,40,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515071056,2,['variab'],"['variable', 'variables']"
Modifiability,"This sets the configuration permanently -- any following commands will use the overridden codecs. Setting a global option is almost certainly better than getting this kind of leakage, I think",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/826#issuecomment-248645129:14,config,configuration,14,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248645129,1,['config'],['configuration']
Modifiability,"This should be configurable, but that was harder than I expected. This should cut down on the number of class A operations we do a bit. It is quite high right now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11984:15,config,configurable,15,https://hail.is,https://github.com/hail-is/hail/pull/11984,1,['config'],['configurable']
Modifiability,This should be just a simple refactoring in anticipation of having bunches for job groups as well.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13448:29,refactor,refactoring,29,https://hail.is,https://github.com/hail-is/hail/pull/13448,1,['refactor'],['refactoring']
Modifiability,"This should go in before the `jb_lmm_getthisin` branch, which is rebased off this one to continue the unification of lin, log, and mixed regression, with more refactoring. (alternatively, the `jb_lmm_getthisin` branch supercedes this one)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1259#issuecomment-272994037:159,refactor,refactoring,159,https://hail.is,https://github.com/hail-is/hail/pull/1259#issuecomment-272994037,1,['refactor'],['refactoring']
Modifiability,"This simply adds the wrapping structure + header from app.hail.is. Includes no scss from app.hail.is. That will be the next PR unless you want it here. We should also decide whether we want a home page (the smoothly-rising Hail that Arcturus liked), i.e whether this will be extended with a link to batch, or not (we discussed the option at our last meeting). cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5429:275,extend,extended,275,https://hail.is,https://github.com/hail-is/hail/pull/5429,1,['extend'],['extended']
Modifiability,"This specifically pertains to the *cgroup* of the user container. This command will probably *not* work inside the user container's mount namespace as 1. they wouldn't have `gcsfuse` installed and 2. we don't give them the capabilities necessary to set up a FUSE mount. `nsenter` should allow us to invoke `gcsfuse` inside the user's cgroup to attribute any memory usage to the user and not as part of the batch container. The tricky bit is that currently we run `gcsfuse`/`blobfuse` before the container is created. Because `crun` currently creates/destroys the cgroup, it does not yet exist when these mounts are set up. It *might* be possible to use [OCI hooks](https://github.com/opencontainers/runtime-spec/blob/main/config.md#prestart) to run `gcsfuse`/`blobfuse` after the container and corresponding cgroup is created but before user code is run.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13996#issuecomment-1804219578:722,config,config,722,https://hail.is,https://github.com/hail-is/hail/issues/13996#issuecomment-1804219578,1,['config'],['config']
Modifiability,This still needs the following modifications to `install_bootstrap_dependencies.sh`:; - Also install the `gke-gcloud-auth-plugin`; - Install the docker buildx plugin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14625#issuecomment-2239160292:122,plugin,plugin,122,https://hail.is,https://github.com/hail-is/hail/pull/14625#issuecomment-2239160292,2,['plugin'],['plugin']
Modifiability,"This the beginning of some Code infrastructure changes and cleanup. The plan is to clean up the tangle of builder classes. I'm still working on the plan for dependent functions, but the skeleton of the rest looks like:. ```; class ClassBuilder[C]; def newMethod(suffix: String, argsInfo: Array[TypeInfo[_]], returnInfo: TypeInfo[_]): MethodBuilder; def newField[T]: ...; def result(): () => C. class MethodBuilder:; def newLocal[T]: LocalRef[T]; def emit(c: Code): Unit. class FunctionBuilder[C]:; val classBuilder: ClassBuilder[C]; def applyMethod: MethodBuilder; def result(): () => C = classBuilder.result(). class EmitMethodBuilder; val mb: MethodBuilder // contain don't extend; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8179:676,extend,extend,676,https://hail.is,https://github.com/hail-is/hail/pull/8179,1,['extend'],['extend']
Modifiability,"This was an easy one, since we already had `BlockMatrixBroadcast` lowered. All `ValueToBlockMatrix` does is make a tiny 1x1 BlockMatrix that can later be broadcast. I kind of think this is a weird IR design and that we should actually have a `BlockMatrixBroadcast` that takes a value `IR` child instead of a `BlockMatrixIR`, but I'll experiment with that refactoring later. Trying to just get the current thing lowered.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10426:355,refactor,refactoring,355,https://hail.is,https://github.com/hail-is/hail/pull/10426,1,['refactor'],['refactoring']
Modifiability,"This was left over from the old AST parsing stuff and I don't think it's used anymore (except in one test, which I just refactored).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6946:120,refactor,refactored,120,https://hail.is,https://github.com/hail-is/hail/pull/6946,1,['refactor'],['refactored']
Modifiability,This was preventing use of cache when building the notebook leader image. The `$*` variable is for use with [Pattern Rules](https://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html#Automatic-Variables). Still note sure why the build is failing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5226:83,variab,variable,83,https://hail.is,https://github.com/hail-is/hail/pull/5226,3,"['Variab', 'variab']","['Variables', 'variable']"
Modifiability,"This was the patch I had to apply to fix the 500s that we had yesterday in PR pages. The URI to URL rewrite enforced GCP which caused the 500. I removed the assertion but don't know yet the proper way to link to a **private** blob storage containers so I just return the URI. I looked at the [canonical URL for blob storage containers](https://docs.microsoft.com/en-us/rest/api/storageservices/naming-and-referencing-containers--blobs--and-metadata#resource-uri-syntax) but appears to only work for public containers. When I looked up how to get a URL for private containers, I get a lot of articles on SAS URLs that bake a token into the URL -- not what we want. We really just use this to link to the portal/console, but when I went to the portal the URL contains subscription-specific parameters that we don't have based on just the URI. This was the point where I figured our time was best spent elsewhere… Unless I'm missing something obvious, I can add this to the bottom of the TODO list. I also fixed the deploy_steps environment variable to match what was actually declared in the ci/deployment.yaml",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11121:100,rewrite,rewrite,100,https://hail.is,https://github.com/hail-is/hail/pull/11121,2,"['rewrite', 'variab']","['rewrite', 'variable']"
Modifiability,"This will hopefully make it easier to develop QoB. Now, running `make -C hail install-for-qob NAMESPACE=default` will push a jar and configure hailctl to use that jar. So `make -C hail install-for-qob NAMESPACE=default && ipython` will drop you into an ipython session pointed at production where any hail queries reflects your local code. `make -C hail pytest-qob NAMESPACE=default PYTEST_ARGS='-k test_foo'` runs `test_foo` with any of your current changes. Things to note:; - Variables like `QUERY_STORAGE_URI` are lazy so this should hopefully not break any current usages of the file for non-developers; - You are at no risk of overwriting a release jar unless you specify `UPLOAD_RELEASE_JAR=true` in the make invocation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12342:133,config,configure,133,https://hail.is,https://github.com/hail-is/hail/pull/12342,2,"['Variab', 'config']","['Variables', 'configure']"
Modifiability,"This works better with my environment and it means that, e.g., Brandon, only needs Java and python set up correctly to run: `git clone ... && cd hail/hail && make pytest`. No environment variables, no packages to install.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6561:187,variab,variables,187,https://hail.is,https://github.com/hail-is/hail/pull/6561,1,['variab'],['variables']
Modifiability,Those env variables most have been copy-pasted from another step because they're neither correct nor necessary. I tried downloading the batch-gsa-key and running `upload-query-jar` with it as the credentials and it succeeded.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11881:10,variab,variables,10,https://hail.is,https://github.com/hail-is/hail/pull/11881,1,['variab'],['variables']
Modifiability,"Though running: `pprint(dict(os.environ.items()))`, yielded:. ```; {'CLICOLOR': '1',; 'GIT_PAGER': 'cat',; 'HOME': '/root',; 'INVOCATION_ID': '0faec80a970f4cf29ce69112519fe641',; 'JOURNAL_STREAM': '8:38888',; 'JPY_PARENT_PID': '5858',; 'LANG': 'en_US.UTF-8',; 'LOGNAME': 'root',; 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',; 'PAGER': 'cat',; 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',; 'SHELL': '/bin/sh',; 'SPARKMONITOR_KERNEL_PORT': '38853',; 'TERM': 'xterm-color',; 'USER': 'root'}; ```. which does not include the environment variable you added saying to use the new thing, though that's clearly present in `init_notebook.py`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7087#issuecomment-532865823:572,variab,variable,572,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532865823,2,['variab'],['variable']
Modifiability,"Three changes:. 1. I noticed the costs were all 0. I think this is due to `cores_mcpu INTO cores_mcpu` scoping issues in the attempt triggers, where the local variable was always zero. At least changing the local variable name immediately fixed the problem. 2. I wrote a test to verify the costs were non-zero and consistent with the reported timing for the succeeding job. It is hard to do on the cost string, so I included msec_mcpu in the batch/job status response to verify it. In trying to verify it, I noticed that timestamps in the attempts table were slightly truncated compared to start/end times in the status (JSON). This lead to rounding errors and slight disagreement. 3. Rather descend into the floating point rabbit hole of madness, I changed times everywhere to be stored as integers in milliseconds (like unix time, since the epoch). In the database, they are not BIGINT. Millisecond resolution seems fine for everything we're building. 4. (Bonus change!) Don't let timing for jobs be negative. This will require another reset.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7628:159,variab,variable,159,https://hail.is,https://github.com/hail-is/hail/pull/7628,2,['variab'],['variable']
Modifiability,"Tim, I've added one of the suggested interfaces. To use `protected var _pType2` instead I believe we need to have InferPType extend `IR` inside of IR.scala, e.g `object InferPType extend IR`, by requirement of sealed traits. Anything else you want? Happy to add additional tests, or to move on to the next pType sub-project.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6594#issuecomment-515061792:125,extend,extend,125,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515061792,2,['extend'],['extend']
Modifiability,"To do this add an intermidiate abstract PartitionWriter,; SimplePartitionWriter, and extend it with TextTablePartitionWriter. SimplePartitionWriter manages the output stream for its subclasses. The; subclasses need to implement consumeElement, and optionally preConsume; and postConsume. TextTablePartitionWriter handles the delimiter delimited writing per; element item and the line delimited writing per element of the stream.; It will also optionally write the header if ExportType.PARALLEL_HEADER_IN_SHARD; has been requested. TextTableFinalizer is the 'MetadataWriter' (name change pending), for; TextTableWriter. That's where the header is written for; PARALLEL_SEPARATE_HEADER and CONCATENATED, and files are merged for; CONCATENATED. There are currently a few idosyncracies that don't replicate bug-for-bug; compatibility with the non-lowered version. 1. We avoid the use of fs.copyMerge, as such, we don't check for the; existence of the _SUCCESS file (even though we create it).; 2. ExportType.PARALLEL_COMPOSABLE is unsupported.; 3. We rely on the temporary file cleaner to clean up the files; created for CONCATENATED export, rather than deleting them; explicitly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11323:85,extend,extend,85,https://hail.is,https://github.com/hail-is/hail/pull/11323,1,['extend'],['extend']
Modifiability,"To enable easy testing, I also parameterized the methods by the branchingFactor and broke generation of the byte array away from writing the byte array to a file. The key issue is that `k * 1024 % 1024 = 0` for any integer `k`, which we were interpreting as meaning that the last block needed 1024 more elements to be full. There are no errors on write. On read, we try to calculate the number of layers present in the BGEN using `calcDepth` but this fails to correctly guess the layers when the size of the file is not a positive integral power of 1024. The only real changes (the rest are restructuring/whitespace) are using `branchingFactor` in place of `1024` and replacing; ```; - // Pad last layer so last block is 1024 elements (1024*8 bytes); - val paddingRequired = 1024 - (arr.length % 1024); ```; with; ```; + // Pad last layer so last block is branchingFactor elements (branchingFactor*8 bytes); + val danglingElements = (arr.length % branchingFactor); + val paddingRequired =; + if (danglingElements == 0) 0; + else branchingFactor - danglingElements; ```. cc: @jigold one of the PRs you asked me to break out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3750:31,parameteriz,parameterized,31,https://hail.is,https://github.com/hail-is/hail/pull/3750,3,"['layers', 'parameteriz']","['layers', 'parameterized']"
Modifiability,To make uber jar: `mvn assembly:single`. `compile` automatically runs `make` for the NativeLib stuff.; `clean` automatically runs `make clean` for NativeLib; Not sure if I needed to incorporate `nativeLibTest` or `nativeLibPrebuilt`. Added two test configurations. One is for all tests and the other is for the set of tests with HAIL_ENABLE_CPP_CODEGEN=1. I double checked the Python tests pass with the uber jar. The test output doesn't have the nice formatting that we have in Gradle. Would be some work with listeners and reporters to do that: http://maven.apache.org/surefire/maven-surefire-plugin/examples/testng.html#. There also isn't the `check` input and some other bells and whistles we have in Gradle. I had to add `com.google.inject:guice` to get rid of some compile warnings with the test-jar. Let me know if there's other things to add/enable or if this is good enough for ci2. We should probably add some CI tests for this in a makefile somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5906:249,config,configurations,249,https://hail.is,https://github.com/hail-is/hail/pull/5906,2,"['config', 'plugin']","['configurations', 'plugin']"
Modifiability,"To properly implement IR sets, I need to staged UnsafeOrdering's, or, at the very least, I need to be able to call them from `Code`-land. Since objects at IR-compile-time are not available at IR-run-time (without shipping them to the nodes and passing them as arguments, which I'd like to avoid), I must be able to call static methods, or have fully code-ified versions of every UnsafeOrdering used in the IR. Whenever possible, I tried to call static methods. In a few cases, I couldn't figure out how to make that work, so I had to reimplement the operation in `Code`. I also had to introduce `BindingCode[T]` which is a type alias for `(FunctionBuilder, StagedBitSet) => Code[T]`. The function builder is used to allocate new variables and the `StagedBitSet` is used to compactly store boolean values. I am also somewhat confused by the `missingGreatest` parameter which existed on the original `UnsafeOrdering`s (which I refactored while Code-ifying). cc: @cseed, I guess this parameter is only sensible on compound data? It seems like there should be a:. ```; def compare(r1: MemoryBuffer, o1: Long, m1: Boolean, r2: MemoryBuffer, o2: Long, m2: Boolean): Int; ```. which correctly applies the `missingGreatest` parameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2519:729,variab,variables,729,https://hail.is,https://github.com/hail-is/hail/pull/2519,2,"['refactor', 'variab']","['refactored', 'variables']"
Modifiability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; First of all, thank you for making such a highly integrated tool. . I learned that this tool could be run in two modes, on Cloud and locally. Well, I happen to have an HPC server that I can work on, so I'd love to use the tool locally. However, many annotation tools require many annotation data that need to be prepared in advance, and no one has seen the exact format of them. Plus, the annotation data sometimes is stored on a google cloud bucket that is requester paid so I don't have a chance to take a peek at them. Therefore, even I try to fill my configuration file, the annotation data needed cannot be prepared unless I have a template of them. . Pls, consider adding a feature like, if we want to run an annotation job locally, let us download package containing all the necessary annotation data in there. So we can set up the configuration file on our own and run the job on a local HPC server. Much appreciated!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9059:848,config,configuration,848,https://hail.is,https://github.com/hail-is/hail/issues/9059,2,['config'],['configuration']
Modifiability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------; Versions of software products:. Running on Apache Spark version 2.4.5; SparkUI available at http://vhabosgen72:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.61-3c86d3ba497a; LOGGING: writing to /dacs1/team/vhabhsxum/tasks/ancestryPainting/hail-20210125-1235-0.2.61-3c86d3ba497a.log. I am running the GWAS tutorial hosted at https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html#Let's-do-a-GWAS. Everything is OK until the line to do GWAS. The error I have is as follows:. -------------- start of error ------------------; 2021-01-25 12:36:11 Hail: INFO: linear_regression_rows: running on 250 samples for 1 response variable y,; with input variable x, and 1 additional covariate...; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1159, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 985, in send_command; response = connection.send_command(command); File ""/fnas/tools/anaconda3/envs/R_361/lib/python3.6/site-packages/py4j/java_gateway.py"", line 1164, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:44859); Traceback (most recent call last):; File ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939:975,variab,variable,975,https://hail.is,https://github.com/hail-is/hail/issues/9939,2,['variab'],['variable']
Modifiability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:. devel-406fc7f6af42. ### What you did:. Calling `export_elasticsearch` without a `config` argument works fine.; ```python; hl.export_elasticsearch(table, ""host"", 9200, ""someindex"", ""sometype"", 1000, config=None); ```. However, attempting to pass `config`, for example:; ```python; es_config = {""es.write.operation"": ""index""}; hl.export_elasticsearch(table, ""host"", 9200, ""someindex"", ""sometype"", 1000, config=es_config); ```; causes the following error:. ### What went wrong (all error messages here, including the full java stack trace):. ```; Traceback (most recent call last):; File ""/tmp/0ba8fc2c770d4b2ba96dc23c50fd6eab/load_clinvar_to_es.py"", line 105, in <module>; verbose=True,; File ""/tmp/0ba8fc2c770d4b2ba96dc23c50fd6eab/hail_v02_scripts.zip/hail_v02_scripts/utils/elasticsearch/client.py"", line 234, in export_table_to_elasticsearch; File ""/home/hail/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; File ""/home/hail/hail.zip/hail/methods/impex.py"", line 1885, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/home/hail/hail.zip/hail/utils/java.py"", line 188, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 323, in get_return_value; py4j.protocol.Py4JError: An error occurred while calling z:is.hail.io.ElasticsearchConnector.export. Trace:; py4j.Py4JException: Method export([class is.hail.table.Table, class java.lang.String, class java.lang.Integer, class java.lang.String, class java.lang.String, class java.lang.Integer, class java.util.HashMap, class java.lang.Boolean]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getM",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4063:335,config,config,335,https://hail.is,https://github.com/hail-is/hail/issues/4063,4,['config'],['config']
Modifiability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:; Export hail table to elasticsearch; >>> hl.utils.get_movie_lens('data/'); >>> users = hl.read_table('data/users.ht'); >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', XXXX, 'data', 'variant', 200,config=N; one, verbose=True). ### What went wrong (all error messages here, including the full java stack trace):. Error:; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1118>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2104, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/table.py"", line 101, in __getattr__; AttributeError: Table instance has no attribute '_jt'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5583:478,config,config,478,https://hail.is,https://github.com/hail-is/hail/issues/5583,1,['config'],['config']
Modifiability,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:; users = hl.read_table('data/users.ht'); hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ). ### What went wrong (all error messages here, including the full java stack trace):; Gotten this error even though the elasticsearch IP and port number 32565 is correct. The IP mentioned in the error 192.168.185.157:9200 was not found anywhere in our EMR or elasticsearch cluster. ; >>> hl.export_elasticsearch(users, 'XX.XXX.XXX.XXX', 32565, 'users', 'movies', 200,config=None, verbose=True ); Config Map(es.nodes -> XX.XXX.XXX.XXX, es.port -> 32565, es.batch.size.entries -> 200, es.index.auto.create -> true); [Stage 0:> (0 + 32) / 65]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</usr/local/lib/python3.6/site-packages/decorator.py:decorator-gen-1122>"", line 2, in export_elasticsearch; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 561, in wrapper; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/methods/impex.py"", line 2106, in export_elasticsearch; File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/hail-on-EMR/src/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 227, in deco; hail.utils.java.FatalError: EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[192.168.185.157:9200, 192.168.81.209:9200]] . Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 73, ip-172-31-10-234.ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5643:397,config,config,397,https://hail.is,https://github.com/hail-is/hail/issues/5643,3,"['Config', 'config']","['Config', 'config']"
Modifiability,"True`.; - `raise_for_status`: this parameter now defaults to `True` and includes the; response body text in the error message. Both; Both parameters may be overridden on a per-request basis. - `httpx.ResponseManager` and `httpx.ClientSession` work together to enable; `retry_transient` and `raise_for_status`. Aiohttp has this unusual structure where; all the request methods are synchronous but they return an object that is both; awaitable and an async context manager. I mirror their structure exactly. The; `httpx.ResponseManager` is both awaitable and an async context manager. Its; `response_coroutine` field is a coroutine that includes the retry and; raise-for-status logic. - The `HailResolver` overrides domain name resolution to first consult the Hail; `address` service. `address` is effectively a domain name server. It watches; kubernetes services and publishes the pod IPs. It supports two name styles:; `service` and `service.namespace`. The former uses the deploy config to; determine in which namespace to find the given service. Currently, the; client-side library only looks up IPs for `shuffler` and `address`. - `BlockingClientSession` and `BlockingContextManager` wrap the; aforementioned `httpx` classes. `BlockingClientResponse` wraps an; `aiohttp.ClientResponse`. ---. Examples of correct usage:. A blocking HTTPS request:. ```python3; with httpx.blocking_client_session() as session:; with session.post(url, json=config, headers=headers) as resp:; assert resp.status == 200; print(resp.text()); ```. An asynchronous HTTPS request to auth:; ```python3; async with httpx.client_session() as session:; async with session.get(; deploy_config.url('auth', '/api/v1alpha/userinfo'),; headers=headers) as resp:; assert resp.status == 200; print(await resp.json()); ```. A blocking HTTPS session with a large default timeout:. ```python3; httpx.blocking_client_session(; headers=service_auth_headers(deploy_config, 'query'),; timeout=aiohttp.ClientTimeout(total=600)); ```. cc: @cat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9554:2400,config,config,2400,https://hail.is,https://github.com/hail-is/hail/pull/9554,1,['config'],['config']
Modifiability,Trying to build on ubuntu ; Welcome to Ubuntu Xenial Xerus (development branch) (GNU/Linux 4.4.0-16-generic x86_64). This tree builds fine on mac using grade installDist. ; On ubuntu it gives the below failure message. We wondered if it might have something to do with case sensitivity issues in file/path naming (mac maintaining case but being case-insensitive). Let me know if you would like more info. ; :compileJava UP-TO-DATE; :compileScala FAILED. FAILURE: Build failed with an exception.; - What went wrong:; A problem was found with the configuration of task ':compileScala'.; > No value has been specified for property 'zincClasspath'.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/275:545,config,configuration,545,https://hail.is,https://github.com/hail-is/hail/issues/275,1,['config'],['configuration']
Modifiability,"Trying to make it more ergonomic to simply do `python3 -m pytest batch/test/test_batch.py::test_job` (now works without any extra environment variables or configuration). This involved the following changes:; - Deleted of some env vars that are no longer used / can be easily consolidated into existing ones; - Gave defaults to those testing env variables for which there are reasonable defaults. E.g. `DOCKER_ROOT_IMAGE` and `HAIL_GENETICS_HAIL_IMAGE`.; - Pushed other environment variables for which there are not reasonable defaults into the tests that need them. If you run a test that requires `HAIL_CLOUD`, you'll still get an error that that env variable is unset and you should set it. But, if you just want to run a single test that doesn't need `HAIL_CLOUD` it won't get in the way.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12862:142,variab,variables,142,https://hail.is,https://github.com/hail-is/hail/pull/12862,5,"['config', 'variab']","['configuration', 'variable', 'variables']"
Modifiability,Trying to slowly add more reliability to the `hailctl` CLI. This adds some very basic tests for the `hailctl batch billing` subcommand that mocks the `BatchClient` so it's just testing that command line parsing and yaml dumping don't break. Most of the other noise in this PR is a refactor. I moved `hail/python/test/hailtop/hailctl/config/conftest.py` up a level so I could reuse its `CLIRunner` fixture across all `hailctl` test modules. That fixture sets a new config directory per test so if you use it in a test the test won't accidentally use or modify the user's actual hailctl config.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13490:281,refactor,refactor,281,https://hail.is,https://github.com/hail-is/hail/pull/13490,4,"['config', 'refactor']","['config', 'refactor']"
Modifiability,"Two of your comments deal with my elimination of redundant sources of information, so I'll address both here. I'm basically thinking of us when we have to debug this system. It's confusing if there's two sources of truth or if we're manually calling `deploy.sh` to isolate issues with that from issues with gradle, I don't want to have two separate knobs to spin. If they accidentally get out of sync that's gonna be double confusing (imagine a PyPI version that disagrees with hail's internal version). How about we put these two versions into two single-line, plain text files and have _generated... load the files to initialize the variables?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512:635,variab,variables,635,https://hail.is,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512,1,['variab'],['variables']
Modifiability,Two things:; - I broke `hailctl batch init` after my changes to auth in the case where you are starting from no config (`~/.config/hail` does not exist); - The service account needs delete permissions on the temp bucket to run `remove_tmpdir` jobs. Cutting a release here so we can use this in today's workshop.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13681:112,config,config,112,https://hail.is,https://github.com/hail-is/hail/pull/13681,2,['config'],['config']
Modifiability,Umm... it just dropped two of my comments. Give me a minute to rewrite them.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3094#issuecomment-372663141:63,rewrite,rewrite,63,https://hail.is,https://github.com/hail-is/hail/pull/3094#issuecomment-372663141,1,['rewrite'],['rewrite']
Modifiability,"Unchecking box above, as KeyTable is a case class it is serializable by default, regardless of extends Serializable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1386#issuecomment-305807317:95,extend,extends,95,https://hail.is,https://github.com/hail-is/hail/issues/1386#issuecomment-305807317,1,['extend'],['extends']
Modifiability,"Unclear what changed. GKE release history doesn't specify when Docker was upgraded to 19.03.1. We think Notebook worked in the past. Anyway, the fix is to never specify ""m"" (lowercase m) as the size modifier for a Kubernetes memory limit. Docker silently drops the ""m"" which means the limit is set to a few thousand bytes (e.g. 3500m becomes 3.5kB). The resulting error message is this:; ```; Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; ```; Which you can see in `kubectl describe pod`:; ```; Warning FailedCreatePodSandBox 73s (x13 over 85s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""notebook-worker-9l2wq"": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:303: getting the final child's pid from pipe caused \""read init-p: connection reset by peer\"""": unknown; Normal SandboxChanged 73s (x12 over 84s) kubelet, gke-vdc-non-preemptible-pool-5-80798769-kp8n Pod sandbox changed, it will be killed and re-created.; ```. We narrowed down to this error by trial and error of removing and adding lines of the YAML file. https://github.com/kubernetes/kubernetes/issues/79950. The fix is to use `Mi` not `m`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8165:411,sandbox,sandbox,411,https://hail.is,https://github.com/hail-is/hail/issues/8165,6,"['Sandbox', 'sandbox']","['SandboxChanged', 'sandbox']"
Modifiability,"Unclear what's wrong, but this k8s container got stuck in container creating. ```; (base) dking@wmb16-359 # k describe pods -n test job-4-7xqf9; Name: job-4-7xqf9; Namespace: test; Node: gke-vdc-non-preemptible-pool-0106a51b-zsmg/10.128.0.5; Start Time: Thu, 17 Jan 2019 16:31:42 -0500; Labels: app=batch-job; hail.is/batch-instance=21706daa42404f5489a53bb5ad22a068; uuid=b4fbcb0d4e2045e8bc4aea6b012ffad6; Annotations: <none>; Status: Pending; IP: ; Containers:; default:; Container ID: ; Image: alpine; Image ID: ; Port: <none>; Host Port: <none>; Command:; sleep; 1; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Environment:; POD_IP: (v1:status.podIP); POD_NAME: job-4-7xqf9 (v1:metadata.name); Mounts:; /var/run/secrets/kubernetes.io/serviceaccount from default-token-85kwr (ro); Conditions:; Type Status; Initialized True ; Ready False ; PodScheduled True ; Volumes:; default-token-85kwr:; Type: Secret (a volume populated by a Secret); SecretName: default-token-85kwr; Optional: false; QoS Class: BestEffort; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal SandboxChanged 11m (x171 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Pod sandbox changed, it will be killed and re-created.; Warning FailedSync 6m kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg error determining status: rpc error: code = Unknown desc = Error: No such container: 741291eb67b9026c0fe4ac52d1f5a553ea420f07f5a7d7368c9dba93e707a079; Warning FailedCreatePodSandBox 1m (x203 over 1h) kubelet, gke-vdc-non-preemptible-pool-0106a51b-zsmg Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin kubenet failed to set up pod ""job-4-7xqf9_test"" network: Error adding container to network: failed to allocate for range 0: no IP addresses available in range set: 10.32.3.1-10.32.3.254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5168:1258,Sandbox,SandboxChanged,1258,https://hail.is,https://github.com/hail-is/hail/issues/5168,3,"['Sandbox', 'sandbox']","['SandboxChanged', 'sandbox']"
Modifiability,Unfortunately this is hard to test in the current setup because the tests never get the root config file (which has no database set).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7706:93,config,config,93,https://hail.is,https://github.com/hail-is/hail/pull/7706,1,['config'],['config']
Modifiability,"Update:. We were able to resolve this issues using a combination of three strategies. 1) We enabled [autoscaling](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling) in our cluster. 2) We changed the spark partition defaults on our cluster to split data into 8,00 partitions. We had [read](https://cloud.google.com/dataproc/docs/support/spark-job-tuning) that this number could be changed to 3x the number of vCPUs on our cluster. Because we are using autoscaling, the number of vCPUs used is not predetermined. Because of this we started with 1x the maximum number of secondary workers in our cluster. Our maximum is set to 1000 n1-highmem-8 machines. These nodes contain 8 vCPUs each, so 8 x 1,000 = 8,000. After speaking with Google, we verified that we could have used 3x the maximum number of vCPUs to increase parallelism. With a maximum of 10 workers and 1,000 secondary workers, all n1-highmem-8 nodes, we could have increased our partition to 24,240. A sample cluster declaration using autoscaling and default shuffle partitions and parallelism of 8000 is below. 3) The hail team had informed us that ""You might try adding `block_size=2048` to your King invocation. That will reduce the memory requirements on the workers to ~1/4 of the default which should give ample room for the analysis."" Because of this, we changed the block size in king to ```block_size=2048```. After looking through the king [source code](https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html#BlockMatrix.default_block_size), we were able to determine the default block size is 4096. . ```; hailctl dataproc start cluster \; --vep GRCh38 \; --autoscaling-policy=MVP_autoscaling_policy \; --requester-pays-allow-annotation-db \; --packages gnomad \; --requester-pays-allow-buckets gnomad-public-requester-pays \; --secondary-worker-type=non-preemptible \; --master-machine-type=n1-highmem-8 \; --worker-machine-type=n1-highmem-8 \; --worker-boot-disk-size=1000 \; --preemptible-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290#issuecomment-1284270117:162,config,configuring-clusters,162,https://hail.is,https://github.com/hail-is/hail/issues/12290#issuecomment-1284270117,1,['config'],['configuring-clusters']
Modifiability,"Updated db.py to allow user to specify region as shown below. `db = hl.experimental.DB(region='us')`; `mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics')`. Data will be accessed from the requester pays bucket in the region specified by the user, available regions are `'us'` and `'eu'`. Modified the following to include region parameter:; - `DB` class ; - `Dataset.from_name_and_json()`. Added method `DatasetVersion.insert_region()` to replace `'{region}'` in `DatasetVersion.url` instance variable with the specified region.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9410:492,variab,variable,492,https://hail.is,https://github.com/hail-is/hail/pull/9410,1,['variab'],['variable']
Modifiability,"Updated the datasets/annotation_db.json config file with datasets currently available in bucket at gs://hail-datasets-hail-data, also updated docs to reflect the datasets available via the `load_datasets()` function.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9331:40,config,config,40,https://hail.is,https://github.com/hail-is/hail/pull/9331,1,['config'],['config']
Modifiability,Updates the GCP terraform to catch up with additions made for Azure. Soon the clouds will share the same module for the global-config so this won't be something we need to worry about.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11012:127,config,config,127,https://hail.is,https://github.com/hail-is/hail/pull/11012,1,['config'],['config']
Modifiability,"Updates the requirements on [astroid](https://github.com/PyCQA/astroid) to permit the latest version.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/astroid/blob/main/ChangeLog"">astroid's changelog</a>.</em></p>; <blockquote>; <h1>What's New in astroid 2.10.0?</h1>; <p>Release date: 2022-02-27</p>; <ul>; <li>; <p>Fixed inference of <code>self</code> in binary operations in which <code>self</code>; is part of a list or tuple.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/4826"">PyCQA/pylint#4826</a></p>; </li>; <li>; <p>Fixed builtin inference on <code>property</code> calls not calling the <code>postinit</code> of the new node, which; resulted in instance arguments missing on these nodes.</p>; </li>; <li>; <p>Fixed a crash on <code>Super.getattr</code> when the attribute was previously uninferable due to a cache; limit size. This limit can be hit when the inheritance pattern of a class (and therefore of the; <code>__init__</code> attribute) is very large.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5679"">PyCQA/pylint#5679</a></p>; </li>; <li>; <p>Inlcude names of keyword-only arguments in <code>astroid.scoped_nodes.Lambda.argnames</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5771"">PyCQA/pylint#5771</a></p>; </li>; <li>; <p>Fixed a crash inferring on a <code>NewType</code> named with an f-string.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5770"">PyCQA/pylint#5770</a></p>; </li>; <li>; <p>Add support for <a href=""https://github.com/python-attrs/attrs/releases/tag/21.3.0"">attrs v21.3.0</a> which; added a new <code>attrs</code> module alongside the existing <code>attr</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/astroid/issues/1330"">#1330</a></p>; </li>; <li>; <p>Use the <code>end_lineno</code> attribute for the <code>NodeNG.tolineno</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11463:952,inherit,inheritance,952,https://hail.is,https://github.com/hail-is/hail/pull/11463,2,['inherit'],['inheritance']
Modifiability,Use ?= in make variables for spark and scala versions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9556:15,variab,variables,15,https://hail.is,https://github.com/hail-is/hail/pull/9556,1,['variab'],['variables']
Modifiability,Use configured compute zone as default for hailctl dataproc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8790:4,config,configured,4,https://hail.is,https://github.com/hail-is/hail/pull/8790,1,['config'],['configured']
Modifiability,"Users who build from source are often confused by the native; library configuration. This adds a target that handles native; library configuration for the user. I also changed the docs to; encourage the use of this target. This means everyone building hail; from source will need to build the C libraries, but I think this; is for the best, since most people building from source need to; correctly handle native libraries anyway. Resolves https://github.com/hail-is/hail/issues/6132",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6134:70,config,configuration,70,https://hail.is,https://github.com/hail-is/hail/pull/6134,2,['config'],['configuration']
Modifiability,"Uses method in line with PStruct, PTuple rewrite to avoid unimplemented def, at the cost of more lines of code.; * lazy in line with PStruct and PTuple fundamentalTypes, and also it doesn't seem right to have more than one fundamental type for a single instance of the class.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7751:41,rewrite,rewrite,41,https://hail.is,https://github.com/hail-is/hail/pull/7751,1,['rewrite'],['rewrite']
Modifiability,"Using `591f7e6`, getting a NullPointer when trying to `explode` and then write to parquet. ```; Caused by: java.lang.NullPointerException; at is.hail.keytable.KeyTable$$anonfun$59.apply(KeyTable.scala:536); at is.hail.keytable.KeyTable$$anonfun$59.apply(KeyTable.scala:534); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); ```. Example:; ```; from hail import *; hc = HailContext(); output = 'gs://' # set this to some output file; a_anns = [u'va.info.AC', u'va.info.AF', u'va.info.AS_FilterStatus']. def index_into_arrays(a_based_annotations, vep_root=None):; annotations = []; if a_based_annotations:; for ann in a_based_annotations:; annotations.append('%s = %s[va.aIndex - 1]' % (ann, ann)); if vep_root:; sub_fields = ['transcript_consequences', 'intergenic_consequences', 'motif_feature_consequences', 'regulatory_feature_consequences']; annotations.extend(['%s.%s = %s.%s.filter(x => x.allele_num == va.aIndex)' % (vep_root, sub_field, vep_root, sub_field) for sub_field in sub_fields]); return annotations. kt = (hc.read('gs://gnomad-public/release-170228/gnomad.exomes.r2.0.1.sites.autosomes.vds').split_multi(); .annotate_variants_expr('va.info = select(va.info, AC, AF, AS_FilterStatus)'); .annotate_variants_expr(index_into_arrays(a_anns, vep_root='va.vep')); .annotate_variants_expr(['va.filters = va.filters.mkString(""|"")',; 'va.info.AS_FilterStatus = va.info.AS_FilterStatus.mkString(""|"")',; 'va.vep = va.vep.transcript_consequences.map(x => drop(x, domains))']); .variants_keytable()). # kt.flatten().to_dataframe().write.parquet(output) # works; kt.flatten().explode('va.vep').to_dataframe().write.parquet(output) # fails; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1601:942,extend,extend,942,https://hail.is,https://github.com/hail-is/hail/issues/1601,1,['extend'],['extend']
Modifiability,"Using hail 0.2.32, `hailctl dataproc submit` results in the following error:. ```; hailctl dataproc submit hail-test {python file} -- {arguments to script}; Submitting to cluster 'hail-test'...; gcloud command:; gcloud dataproc jobs submit pyspark {python file} \; --cluster=hail-test \; --files= \; --py-files=/var/folders/7y/hvrzyxts3xg74r3m2jbq0kc0zt3g3z/T/pyscripts_srh2ze4a.zip \; --properties= \; -- \; {arguments to script}; ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [region] is not currently set.; It can be set on a per-command basis by re-running your command with the [--region] flag. You may set it for your current workspace by running:. $ gcloud config set dataproc/region VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_DATAPROC_REGION]; Traceback (most recent call last):; File ""/Users/aarong/Documents/gtex-wgs/.devenv/bin/hailctl"", line 8, in <module>; sys.exit(main()); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 94, in main; cli.main(args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 107, in main; jmp[args.module].main(args, pass_through_args); File ""/Users/aarong/Documents/gtex-wgs/.devenv/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 78, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 363, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError; ``` . However, adding `--region us-central1` to any location in the argument string to hailctl dataproc submit results in the argument being picked up as an input to the script, not to the underlying gcloud command",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8078:685,config,config,685,https://hail.is,https://github.com/hail-is/hail/issues/8078,2,"['config', 'variab']","['config', 'variable']"
Modifiability,"Using vep99 and Hail 0.2.57 (and 0.2.70) leads to the absence of vep transcript_consequences annotation (mt.vep.transcript_consequences). We verified that if we run vep annotation outside hail the annotation is present while hl.vep gives absent ones with NA in transcript_consequences field. Here I am attaching the vcf file (I changed extension to '.txt' to allow for the file upload) with two variants, where the chr4:113358472 one gives NA in the transcript_consequences field. The gene is in coding region and have many transcripts. Also vep config json is attached (also needed to change extension to '.txt').; [vep99-loftee-grch38-aws.txt](https://github.com/hail-is/hail/files/6733985/vep99-loftee-grch38-aws.txt). [batch109_subset.txt](https://github.com/hail-is/hail/files/6733958/batch109_subset.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10623:546,config,config,546,https://hail.is,https://github.com/hail-is/hail/issues/10623,1,['config'],['config']
Modifiability,VEP will now automatically look for a config file in environment variable `VEP_CONFIG_URI` if one isn't specified. This environment variable is prepopulated on dataproc clusters started with `hailctl dataproc`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8929:38,config,config,38,https://hail.is,https://github.com/hail-is/hail/pull/8929,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"Was hitting a very annoying bug because `PCanonicalNDArray` didn't explicitly override the default. Creators of new `PType`s should not have to magically know that this method exists and override it. It should not have a default implementation. . I had to specify that `PCanonicalNDArray` does have pointers, and that `PVoid` and `PPrimitive` don't. I said `false` for `PCanonicalStream` too, but that was less clear. `false` is the value it was inheriting previously, but idk if you can even `deepCopy` a stream. If you can, then maybe it should recur to the `elementType`? You'd know best Patrick.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9242:446,inherit,inheriting,446,https://hail.is,https://github.com/hail-is/hail/pull/9242,1,['inherit'],['inheriting']
Modifiability,Was tested with:. ```; #!/bin/bash. set -ex. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/small.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; write -o ~/small.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; annotatevariants vds -r va -i ~/small.vds \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample2.vds. ./build/install/hail/bin/hail read -i ~/sample.vds comparevds -i ~/sample2.vds; ```. where sample.vcf is test/resources/sample.vcf and small.vcf is the half of the sample.vcf variants.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/414#issuecomment-224702021:132,config,config,132,https://hail.is,https://github.com/hail-is/hail/pull/414#issuecomment-224702021,3,['config'],['config']
Modifiability,"We are trying to setup hail `0.2.72` on spark `3.1.2` version. However, we are also facing similar error. * java version: `OpenJDK 64-Bit Server VM, 1.8.0_242`; * scala version: `2.12.10`; * py4j: `0.10.9`; * Python: `3.7.10`. <details>; <summary>Stacktrace</summary>. ```; Py4JJavaError: An error occurred while calling o126.exists.; : java.lang.NoClassDefFoundError: com/amazonaws/AmazonClientException; 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2532); 	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2497); 	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593); 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.exists(FS.scala:183); 	at is.hail.io.fs.FS.exists$(FS.scala:181); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:70); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.Ca",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:523,Config,Configuration,523,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,6,['Config'],['Configuration']
Modifiability,"We currently cannot run untrusted code on our cluster and guarantee that malicious code in one pod does not leak into other pods, or affect the entire cluster. This proposal outlines a solution to this problem. *This is a work in progress*. ### TL;DR; Use Kata + CRI-Containerd runtime to sandbox pods, at a low performance cost. [Jessie Frazelle’s Blog: Hard Multi-Tenancy in Kubernetes](https://blog.jessfraz.com/post/hard-multi-tenancy-in-kubernetes/). ### Roadmap; I would like to implement a test cluster that uses this system, and begin migrating our existing workloads to it asap. . *TODO*. ### Rationale; 1. We want resource preemption across users., running multiple user containers on a single cluster.; 2. This means sandboxing at the cluster level is out.; 3. Therefore we must sandbox at the pod (or container) level. Kata + CRI-Containerd chosen for performance and maturity reasons.; CRI-Containerd is much faster than CRI-O, and Kata is much faster than gVisor. Kata is a relatively mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesn’t work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation · Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O · GitHub](h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5111:289,sandbox,sandbox,289,https://hail.is,https://github.com/hail-is/hail/issues/5111,3,['sandbox'],"['sandbox', 'sandboxing']"
Modifiability,We don't make it easy to copy the hail log off the driver when a spark job fails. We should make that as automatic as possible. One way to do this is to add a call to [`hl.copy_log`](https://hail.is/docs/0.2/utils/index.html#hail.utils.copy_log) in the `except` block of `SparkBackend.execute`. We would need to expose some configuration for where to copy logs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14431:324,config,configuration,324,https://hail.is,https://github.com/hail-is/hail/issues/14431,1,['config'],['configuration']
Modifiability,"We get a lot of spurious Grafana alerts because batch-driver has unclosed `aiohttp.ClientSession` objects. `aiohttp` can [report the creation location](https://github.com/aio-libs/aiohttp/blob/master/aiohttp/client.py#L242-L247), but only when aysncio is in debug mode. I am hesitant to enable debug mode because I suspect it will slow down everything by grabbing stack traces for every coroutine (so that it can report an error later). I adapted the code from the linked asyncio code and tested it as follows:. ```; In [1]: import aiohttp; ...: import traceback; ...: import sys; ...:; ...: oldinit = aiohttp.ClientSession.__init__; ...: def newinit(self, *args, **kwargs):; ...: oldinit(self, *args, **kwargs); ...: self._source_traceback: Optional[; ...: traceback.StackSummary; ...: ] = traceback.extract_stack(sys._getframe(1)); ...: aiohttp.ClientSession.__init__ = newinit. In [2]: aiohttp.ClientSession(); <ipython-input-1-028690903e5f>:7: DeprecationWarning: The object should be created within an async function; oldinit(self, *args, **kwargs); Out[2]: <aiohttp.client.ClientSession at 0x104ab3850>. In [3]: aiohttp.ClientSession(); <ipython-input-1-028690903e5f>:7: DeprecationWarning: The object should be created within an async function; oldinit(self, *args, **kwargs); Out[3]: <aiohttp.client.ClientSession at 0x104dac8b0>. In [4]: aiohttp.ClientSession(); <ipython-input-1-028690903e5f>:7: DeprecationWarning: The object should be created within an async function; oldinit(self, *args, **kwargs); Out[4]: <aiohttp.client.ClientSession at 0x104daeec0>. In [5]:. Do you really want to exit ([y]/n)? y; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x104ab3850>; source_traceback: Object created at (most recent call last):; File ""/Users/dking/miniconda3/bin/ipython"", line 8, in <module>; sys.exit(start_ipython()); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/__init__.py"", line 128, in start_ipython; return launch_new_instance(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13421:439,adapt,adapted,439,https://hail.is,https://github.com/hail-is/hail/pull/13421,1,['adapt'],['adapted']
Modifiability,"We have chosen to prototype a new hail query compiler and runtime using; the LLVM project's MLIR framework for query compilation infrastructure.; This creates a simple project skeleton that will serve as a jumping off; point for the work. The README should be enough to get it built. There; is an implementation of the opt tool for investigating IRs, and; a completely empty dialect. I chose to rewrite this rather than copy over [hail-is/mlir-hail] as; breaking API changes from LLVM/MLIR 14 to 15 made it very difficult to; update that repo. The difficulties in updating LLVM versions may prove; to be a source of pain going forward, as this will be a pretty deep; dependency of the system. [hail-is/mlir-hail]: https://github.com/hail-is/mlir-hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12070:395,rewrite,rewrite,395,https://hail.is,https://github.com/hail-is/hail/pull/12070,1,['rewrite'],['rewrite']
Modifiability,We have configured curl to retry so we should always prefer it to wget. I also; fixed that long-standing mistake I made when I added retry-all-errors before; it was supported in our version of curl.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11649:8,config,configured,8,https://hail.is,https://github.com/hail-is/hail/pull/11649,1,['config'],['configured']
Modifiability,"We need to configure Grafana with an SMTP server so that it can send us alert emails. I'd like to have alerts for things like disks getting dangerously close to filling up, abnormally high latency, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6697:11,config,configure,11,https://hail.is,https://github.com/hail-is/hail/issues/6697,1,['config'],['configure']
Modifiability,"We need to include the :members: directive in newer Sphinx versions. I also elided; hidden and inherited members for now, though we can add those back if we like.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9402:95,inherit,inherited,95,https://hail.is,https://github.com/hail-is/hail/pull/9402,1,['inherit'],['inherited']
Modifiability,"We now have logistic Wald, LRT, and Score via #585 . I'll be adding Firth soon. We also need to extend these to subset to non-missing genotypes rather than mean impute.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/133#issuecomment-249706530:96,extend,extend,96,https://hail.is,https://github.com/hail-is/hail/issues/133#issuecomment-249706530,1,['extend'],['extend']
Modifiability,"We should extend filtervariants to take a file consisting of a list of positions (contig, start) or a list of variants (contig, start, ref, alt). In GoT2D, there are related .pos files of positions that pass various filters, and vcftools allows one to subset.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/114:10,extend,extend,10,https://hail.is,https://github.com/hail-is/hail/issues/114,1,['extend'],['extend']
Modifiability,"We should never have been using `await`. (aiomysql should probably not implement `__await__`). `create_pool` returns `aiomysql.utils._PoolContextManager` which inherits from `aiomysql.utils._ContextManager` which implements `__await__`, `__aenter__`, and `__aexit__` thusly:. ```python3; def __await__(self):; return self._coro.__await__(). async def __aenter__(self):; self._obj = await self._coro; return self._obj. async def __aexit__(self, exc_type, exc, tb):; await self._obj.close(); self._obj = None; ```. `__await__` is a footgun! You should never do that! You should close the return value of the coroutine!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13869:160,inherit,inherits,160,https://hail.is,https://github.com/hail-is/hail/pull/13869,1,['inherit'],['inherits']
Modifiability,"We should use __init__ to initialize variables to avoid duplication. This requires changing the init methods to not do any work and have static methods that do the database calls, construct pods, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5944:37,variab,variables,37,https://hail.is,https://github.com/hail-is/hail/issues/5944,1,['variab'],['variables']
Modifiability,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:517,layers,layers,517,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073,1,['layers'],['layers']
Modifiability,"We switched to Python-based FSes which work fine (see below). The issue in Scala will be fixed by https://github.com/hail-is/hail/pull/13434. ---. Here's a folder with 10k files:; ```; (base) dking@wm28c-761 hail % gsutil ls gs://gcp-public-data--gnomad/release/2.1.1/ht/genomes/gnomad.genomes.r2.1.1.sites.ht/rows/parts/ | wc -l; 10000; ```. And here's Hail working correctly as of 0.2.120.; ```; (base) dking@wm28c-761 hail % HAIL_QUERY_BACKEND=batch ipython; Python 3.10.9 (main, Jan 11 2023, 09:18:18) [Clang 14.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.12.2 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl; ...: print(hl.version()); ...: x = hl.utils.hadoop_ls('gs://gcp-public-data--gnomad/release/2.1.1/ht/genomes/gnomad.genomes.r2.1.1.sites.ht/rows/parts/'); ...: len(x); 0.2.120-f00f916faf78. Out[1]: 10000; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12932#issuecomment-1703524347:607,enhance,enhanced,607,https://hail.is,https://github.com/hail-is/hail/issues/12932#issuecomment-1703524347,1,['enhance'],['enhanced']
Modifiability,"We use feature flags to communicate requester pays information to the service backend.; In this change, I've made the local backend do the same to make a future refactoring simpler.; I intend to follow up this change that'll split config from feature flags.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14407#issuecomment-1998508308:161,refactor,refactoring,161,https://hail.is,https://github.com/hail-is/hail/pull/14407#issuecomment-1998508308,4,"['config', 'refactor']","['config', 'refactoring']"
Modifiability,"We're really being compatible with R here, no? [Pandas defaults to the empty string](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html) (which seems *definitively* worse). It's a compelling argument nonetheless. I would prefer an export mode that's ""R compatible"" which basically means flattening out any structs and writing all missing values as `NA` (or some user configurable string). Then, I'd prefer the default is the JSON-y representation, which is to say, `null` for missing values whether they're alone in a column or inside a struct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4147#issuecomment-415033609:400,config,configurable,400,https://hail.is,https://github.com/hail-is/hail/issues/4147#issuecomment-415033609,1,['config'],['configurable']
Modifiability,"We've had a number of pip issues recently, particularly incompatibilities with the `six` package. The central issue is that pip will install incompatible package versions with only a warning message. This primarily happens when a set of packages is installed (say in a parent image) and then a new set of packages are installed later. Pip attempts to satisfy all requirements of the packages in an installed set, but it does not consider the versions of packages already installed. In this PR, I pervasively change every use of `pip` to be followed by the use of `pip check`. Pip check errors if there are incompatible package versions installed. Rather than change every place that we use `pip install`, I created an ubuntu image with a script, `hail-pip-install`, and pip configuration which makes it easy to do the right thing. Now, we should always install python packages like this:. ```; hail-pip-install package1 package2 package3; ```. The script will ensure that all packages are compatible with existing packages. Moreover, it will retry downloads five times. I did not make every Dockerfile depend on the new hail-ubuntu-image because they are legacy Dockerfile that I didn't want to edit. These include the benchmark Dockerfile and some notebook workers. PySpark presented a challenge because we did not install it via pip. As a result, packages, such as Hail, which depend on PySpark failed the `pip check`. I modified all uses of PySpark to instead use the standard pip-installed version of PySpark. In particular, take a look at how query and shuffler changed. cc: @Dania-Abuhijleh @catoverdrive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9593:774,config,configuration,774,https://hail.is,https://github.com/hail-is/hail/pull/9593,1,['config'],['configuration']
Modifiability,"We've had to do a redeploy of our hail batch instance on Azure. This PR resolves/clarifies two issues we encountered. 1) Storage Account Name Uniqueness. Due to Azure's restrictions on storage account naming (mainly that names must be globally unique) the redeploy did not succeed. This is because the resource group name (we chose to reuse hail) is possible under a new subscription, but the generated storage account names were therefore identical to our previous stack. I've added in an argument called `storage_account_suffix` to account for this issue. It can be set to any arbitrary string that complies with Azure's storage account naming scheme in order to avoid naming conflicts in the future. While the option remains to simply choose a novel resource group name this is not enforced by Azure and anyone deploying a stack similarly named to someone else would not know until the `terraform apply` stage that the name would not work. 2) Mysql Flexible Server Zones. The only other issue is that the zone argument for the mysql flexible server is no longer always valid depending on your compute region. We needed to comment it out for a successful deploy in Australia East. The comment that has been added we hope will be helpful for others in future.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13058:952,Flexible,Flexible,952,https://hail.is,https://github.com/hail-is/hail/pull/13058,2,"['Flexible', 'flexible']","['Flexible', 'flexible']"
Modifiability,"Were you using the old copier or the new (not yet merged) `hailctl fs sync`? I had hoped the latter was finally robust enough for real use. `hailtop.aiotools.copy` is indeed not very reliable. Regardless, using the rewrite action when the source and destination agree is the correct move.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14601#issuecomment-2214474849:215,rewrite,rewrite,215,https://hail.is,https://github.com/hail-is/hail/issues/14601#issuecomment-2214474849,1,['rewrite'],['rewrite']
Modifiability,"What do we want a new Hail user to see / do, and in what order? This is what someone sees when they open the docs:. <img width=""1221"" alt=""Screen Shot 2020-03-02 at 3 10 09 PM"" src=""https://user-images.githubusercontent.com/13773586/75713528-fa74d800-5c97-11ea-9e55-01065f5f8c21.png"">. They get information from two places now. The side bar has some stuff, and this paragraph does as well. The first thing this paragraph recommends is the `Overview`, which is the wrong first step in my opinion, unless we rewrite the overview. The Overview is written in a very ""bottom up"" way, starting with talking about hail types. A geneticist does not want to arrive at `import_vcf`/`variant_qc`/`sample_qc` by first slowly walking through our weird version of lazy typed python. I think putting the tutorials first would be a good first step. Going through all the tutorials in order (even they we really mostly like the first tutorial) is more useful than jumping into hail types. . I also don't like the ordering on the sidebar. ""Installation"" is a fine start, but ""Hail on the Cloud"" is too soon. ""Datasets"" and ""Annotation Database"" are very experimental, those should be towards the very bottom, maybe just above ""Hail for Developers"". ""Cheat Sheets"" should be higher up. ""Cheat Sheets"" should also be advertised somewhere in the tutorials / intro paragraph, as I think getting the matrix table and hail table pictures in front of people is crucial to understanding. Finally, I want to pick a place in the hail new user flow where users find out hail is lazy. I frequently run into users who don't know it works this way, and I want to make that set of people smaller.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8217:506,rewrite,rewrite,506,https://hail.is,https://github.com/hail-is/hail/issues/8217,1,['rewrite'],['rewrite']
Modifiability,"What do you mean by validation? . For the 'annotation line' are you suggesting a general error-catching wrapper? I actually really like that, and I'll give it a go. > CNV work; > What I want to do with CNVs is something like ; > ; > ```; > val files: Array[String]; > sc.paralellize(files); > .map { f => readTable(f, config...) }; > .,map (convert to a hail better cnv representation); > ```; > ; > Can't do that if readTable gives you an RDD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233007740:318,config,config,318,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233007740,1,['config'],['config']
Modifiability,What was the issue with the third variable?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5829#issuecomment-481371995:34,variab,variable,34,https://hail.is,https://github.com/hail-is/hail/pull/5829#issuecomment-481371995,1,['variab'],['variable']
Modifiability,What were the other config changes you had to make to Auth to get it to deploy? I didn't see any here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11005#issuecomment-949802668:20,config,config,20,https://hail.is,https://github.com/hail-is/hail/pull/11005#issuecomment-949802668,1,['config'],['config']
Modifiability,"When Hail is pip-installed, you cannot use it with an already constructed SparkContext, unless said SparkContext's class path includes the hail JAR file. It is somewhat annoying to find the hail JAR location and add that to the class path. The primary reason users want to pass an already constructed SparkContext is to specify some configuration parameters. `hl.init` should take a `conf` as either a `SparkConf` or a `dict`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7080:333,config,configuration,333,https://hail.is,https://github.com/hail-is/hail/issues/7080,1,['config'],['configuration']
Modifiability,"When I attempt to rewrite `transform` to use a function, I'm getting a parser error. ```; is.hail.utils.HailException: no conversion found for __uid_1(struct{locus: locus<GRCh38>, alleles: array<str>, rsid: str, qual: float64, filters: set<str>, info: struct{BaseQRankSum: float64, ClippingRankSum: float64, DP: int32, END: int32, ExcessHet: float64, MQ: float64, MQRankSum: float64, MQ_DP: int32, QUALapprox: int32, RAW_MQ: float64, ReadPosRankSum: float64, VarDP: int32}, __entries: array<struct{AD: array<int32>, DP: int32, GQ: int32, GT: call, MIN_DP: int32, PGT: call, PID: str, PL: array<int32>, SB: array<int32>}>}); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.package$.invoke(package.scala:76); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:751); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:718); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.named_value_ir(Parser.scala:500); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:495); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:495); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:282); 	at is.hail.expr.ir.IRParser$.named_value_irs(Parser.scala:495); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:714); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:943); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:942); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$$anonfun$table_ir_children$1.apply(Parser.scala:846); 	at is.hail.expr.ir.IRParser$$anonfun$table_ir_children$1.apply(Parser.scala:846); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:282); 	at is.ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5435#issuecomment-467547718:18,rewrite,rewrite,18,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467547718,1,['rewrite'],['rewrite']
Modifiability,"When I check my spark configuration, it appears that:; My spark version is version 1.6.0; Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_67). scala> System.getProperty(""java.version""); res0: String = 1.7.0_67. scala> val rdd = sc.parallelize(0 to 1000, 4); scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.7.0_67, 1.7.0_67, 1.7.0_67, 1.7.0_67). It seems I should also update the java relate to the Spark cluster, Thank you !",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-248826154:22,config,configuration,22,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-248826154,1,['config'],['configuration']
Modifiability,"When I try applying this patch:. ```diff; diff --git a/batch/batch/worker/worker.py b/batch/batch/worker/worker.py; index 9d3d89498b..a9106b7a60 100644; --- a/batch/batch/worker/worker.py; +++ b/batch/batch/worker/worker.py; @@ -1228,22 +1228,28 @@ class Container:; return config; ; async def _get_in_container_user(self) -> Tuple[int, int]:; + # https://docs.docker.com/engine/reference/builder/#user; assert self.image.image_config; user = self.image.image_config['Config']['User']; if not user:; return 0, 0; if "":"" in user:; - uid, gid = user.split("":""); - else:; - uid, gid = await self._read_user_from_rootfs(user); - return int(uid), int(gid); + user, group = user.split("":""); + try:; + return int(user), int(group); + except ValueError:; + return await self._read_uid_gid_from_rootfs(user); + try:; + return int(user), 0; + except ValueError:; + return await self._read_uid_gid_from_rootfs(user); ; - async def _read_user_from_rootfs(self, user) -> Tuple[str, str]:; + async def _read_uid_gid_from_rootfs(self, user: str) -> Tuple[int, int]:; with open(f'{self.image.rootfs_path}/etc/passwd', 'r', encoding='utf-8') as passwd:; for record in passwd:; if record.startswith(user):; _, _, uid, gid, _, _, _ = record.split("":""); - return uid, gid; + return int(uid), int(gid); raise ValueError(""Container user not found in image's /etc/passwd""); ; def _mounts(self, uid: int, gid: int) -> List[MountSpecification]:; ```. I can run the container successfully but it fails with the following error:. ```; mkdir: cannot create directory '/io/batch': Permission denied; ```. Which is caused by the following line inserted by the `hailtop.batch` client library:. ```; mkdir -p /io/batch/4c8107/NjztN; ```. so images with non-existent users would fail to run on batch regardless of whether we were using crun / docker / podman, because Batch assumes that the user in the container will have permission to write to `/io`, regardless of whether the job requires input/output files.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13679#issuecomment-1728424448:274,config,config,274,https://hail.is,https://github.com/hail-is/hail/issues/13679#issuecomment-1728424448,2,"['Config', 'config']","['Config', 'config']"
Modifiability,"When python loads a file it runs everything at the top level. That means running statements like `import aiohttp`, `MY_CONSTANT = 5`, and `def foo: …`. So importing files can define functions, so they can be used later, but those functions are not run (how could they be?). By putting the imports inside the functions themselves, we defer the import of aiohttp from when the deploy_config is imported to when the functions that depend on `aiohttp` are actually used. If they are never used, `aiohttp` will never be imported! This is why it made `hailctl dev config list` much faster, since it doesn't need to use those functions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11293#issuecomment-1027295448:558,config,config,558,https://hail.is,https://github.com/hail-is/hail/pull/11293#issuecomment-1027295448,1,['config'],['config']
Modifiability,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:203,config,configurations,203,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530,4,['config'],"['configuration', 'configurations']"
Modifiability,"Will the additional --plugin options change the output schema? We fix the schema in the VEP command which is one reason we've resisted supporting general VEP options. To support a general schema, we probably need to impute the schema from the VEP JSON output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1712#issuecomment-299003723:22,plugin,plugin,22,https://hail.is,https://github.com/hail-is/hail/pull/1712#issuecomment-299003723,1,['plugin'],['plugin']
Modifiability,"With EIGENSTRAT's smartpca, you can specify a grouping variable in the indiv file then specify which group to call PCs on using -w flag & project onto the rest. Would be great to have this feature as this will allow projection with 1000G samples, related samples, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/442:55,variab,variable,55,https://hail.is,https://github.com/hail-is/hail/issues/442,1,['variab'],['variable']
Modifiability,"With new aggregators, users could e.g. access Apache Math3 libraries to do arbitrary analyses based on case/control genotype counts.; Furthermore having ability to add new jars to classpath would allow flexible addition of new functions by users.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/359:202,flexible,flexible,202,https://hail.is,https://github.com/hail-is/hail/issues/359,1,['flexible'],['flexible']
Modifiability,With this new approach I'm trying to use a PHP script to fetch the metadata from SQLite and pass it off as a JSON file to the client to build the documentation web page. It looks like the script isn't running properly on the CI server -- I guess maybe there is some configuration to be done to allow PHP scripts to run on the web server?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1936#issuecomment-311068359:266,config,configuration,266,https://hail.is,https://github.com/hail-is/hail/pull/1936#issuecomment-311068359,1,['config'],['configuration']
Modifiability,"Without this `make test-local` fails because there is no running server. This ensures that `make test-local` first starts a server to test against. Recreated for stacked PRs from #4785. ---. The `until curl ...` nonsense is because the server takes some time to start up, so we poll until we get a successful return value from `curl`. `-f` means return non-zero-exit-code on failure. `-L` means follow redirects (not really necessary here, but I think it's good practice to use `-L`). `BATCH_USE_KUBE_CONFG=1` tells batch to use the latent kubernetes configuration, which means the developer must already have set up `kubectl`. This is a reasonable expectation for a developer of `batch`. The `trap cleanup EXIT` ensures we run cleanup before the shell exits. `trap ""exit 24"" INT TERM` converts interruption (`Ctrl-c`) and termination (`kill -15`) into an `EXIT` signal. We do this to ensure that the exit handler is called once. if we did `trap cleanup EXIT INT TERM` some shells would call `cleanup` twice. Once for the interruption and once for the shell exiting. Inside `cleanup` we `trap """" INT TERM` to make `Ctrl-c` do nothing, because the user COUGH cotton COUGH might smash ctrl-c repeatedly and we might not kill the subprocess before they kills us ;).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4786:551,config,configuration,551,https://hail.is,https://github.com/hail-is/hail/pull/4786,1,['config'],['configuration']
Modifiability,"Without this `make test-local` fails because there is no running server. This ensures that `make test-local` first starts a server to test against. The `until curl ...` nonsense is because the server takes some time to start up, so we poll until we get a successful return value from `curl`. `-f` means return non-zero-exit-code on failure. `-L` means follow redirects (not really necessary here, but I think it's good practice to use `-L`). `BATCH_USE_KUBE_CONFG=1` tells batch to use the latent kubernetes configuration, which means the developer must already have set up `kubectl`. This is a reasonable expectation for a developer of `batch`. The `trap cleanup EXIT` ensures we run cleanup before the shell exits. `trap ""exit 24"" INT TERM` converts interruption (`Ctrl-c`) and termination (`kill -15`) into an `EXIT` signal. We do this to ensure that the exit handler is called once. if we did `trap cleanup EXIT INT TERM` some shells would call `cleanup` twice. Once for the interruption and once for the shell exiting. Inside `cleanup` we `trap """" INT TERM` to make `Ctrl-c` do nothing, because the user COUGH cotton COUGH might smash ctrl-c repeatedly and we might not kill the subprocess before they kills us ;).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4785:508,config,configuration,508,https://hail.is,https://github.com/hail-is/hail/pull/4785,1,['config'],['configuration']
Modifiability,"Worked out the kinks in Python. Here's a working example:; ```; import hail; hc = hail.HailContext(); vds = (hc.import_vcf('src/test/resources/regressionLogistic.vcf'); .annotate_samples_table('src/test/resources/regressionLogistic.cov', 'Sample', root='sa.cov', config=pyhail.TextTableConfig(impute=True)); .annotate_samples_table('src/test/resources/regressionLogisticBoolean.pheno', 'Sample', root='sa.pheno', config=pyhail.TextTableConfig(types='isCase: Boolean', missing='0')); .logreg('wald', 'sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2']); .linreg('sa.pheno.isCase', ['sa.cov.Cov1', 'sa.cov.Cov2'])); ```. And another:; ```; vds = (hc.import_vcf('src/test/resources/sample.vcf'); .split_multi(); .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""20"", 13753124, ""A"", ""C"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.culprit2 = gs.filter(g => v == Variant(""20"", 13767943, ""C"", ""T"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno = pcoin((sa.culprit + sa.culprit2)/ 4.0)'); .annotate_samples_expr('sa.cov1 = rnorm(0,1)'); .annotate_samples_expr('sa.cov2 = rnorm(0,1)'); .linreg('sa.pheno', ['sa.cov1', 'sa.cov2']); .logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.wald'); .logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.lrt'); .logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], 'va.score')); ; vds.export_variants('pvals.tsv', 'Variant = v, lin_pval = va.linreg.pval, wald_pval = va.wald.wald.pval, lrt_pval = va.lrt.lrt.pval, score_pval = va.score.score.pval, va.wald.fit.*'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1259#issuecomment-272988219:263,config,config,263,https://hail.is,https://github.com/hail-is/hail/pull/1259#issuecomment-272988219,2,['config'],['config']
Modifiability,"Would it be possible to add an option to annotate with --tab and --pick_allele, as below:; This would be super helpful!. <hail.vep.perl>; <hail.vep.location>; --format vcf; --tab; --pick_allele; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.cache_dir>/homo_sapiens/81_GRCh37/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa; --minimal; --assembly GRCh37; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/489:417,plugin,plugin,417,https://hail.is,https://github.com/hail-is/hail/issues/489,1,['plugin'],['plugin']
Modifiability,Would previously pull out address and load length into local variables even if we just need to pass along pointer.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8236#issuecomment-594546138:61,variab,variables,61,https://hail.is,https://github.com/hail-is/hail/pull/8236#issuecomment-594546138,1,['variab'],['variables']
Modifiability,"Ya nearly, this is a subset of the changes that I made and deployed into a Terra dev environment. So technically I didn't deploy this branch (because things in the batch code base would have broken) but these are all the config/build system changes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14450#issuecomment-2060017171:221,config,config,221,https://hail.is,https://github.com/hail-is/hail/pull/14450#issuecomment-2060017171,1,['config'],['config']
Modifiability,"Yeah, I'd like to make it required. It's one of the things that NIST 800-53 asks for: automated vulnerability detection. To be honest, I'm not impressed by a tool that finds variables whose names include the character string ""secret"" and warns if they're printed, but, 🤷.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12269#issuecomment-1272063880:174,variab,variables,174,https://hail.is,https://github.com/hail-is/hail/pull/12269#issuecomment-1272063880,1,['variab'],['variables']
Modifiability,"Yeah, it's all in the same JVM process so those variables would have no effect: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala#L97",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583453677:48,variab,variables,48,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583453677,1,['variab'],['variables']
Modifiability,"Yeah, so it looks like nullable=False isn't working in a keyschema. I'm just going to rewrite this by hand like I did for jobs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7915#issuecomment-575941769:86,rewrite,rewrite,86,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575941769,1,['rewrite'],['rewrite']
Modifiability,"Yeah, that's true. I've updated this PR to include the default gce deploy config for the default namespace, which I think is generally what you'd want for both testing and in production; I'm not sure under what circumstances you'd want separate service namespaces?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9437#issuecomment-691245476:74,config,config,74,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691245476,1,['config'],['config']
Modifiability,"Yeah, the laziness worked until just recently. I broke the laziness when I refactored things for Caitlin.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3862#issuecomment-401795671:75,refactor,refactored,75,https://hail.is,https://github.com/hail-is/hail/issues/3862#issuecomment-401795671,1,['refactor'],['refactored']
Modifiability,"Yeah, this PR really shouldn't have introduced the kinds of failures we're seeing. That seems to be the refrain of all of my lowering refactoring PRs though!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7567#issuecomment-557024561:134,refactor,refactoring,134,https://hail.is,https://github.com/hail-is/hail/pull/7567#issuecomment-557024561,1,['refactor'],['refactoring']
Modifiability,"Yeah, this was broken in a recent update to the gcloud libraries. For now, following the command to do:; ```; gcloud config set dataproc/region VALUE; ```; will generate the best user experience. We'll fix this for the next version though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8078#issuecomment-584868077:117,config,config,117,https://hail.is,https://github.com/hail-is/hail/issues/8078#issuecomment-584868077,2,['config'],['config']
Modifiability,"Yes! I want `make deploy` to always mean ""`kubectl apply` this service's kubernetes configuration"" and/or ""push to appropriate public repository"" (c.f. hail's python lib). Cotton can comment more directly on lets encrypt, but there's an issue wrt sharing a volume between two pods that isn't easily resolved. I'm not exactly sure how `make run` is intended to be used.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5413#issuecomment-467103158:84,config,configuration,84,https://hail.is,https://github.com/hail-is/hail/issues/5413#issuecomment-467103158,1,['config'],['configuration']
Modifiability,"Yes, CI generates the config for gateway which needs the root domain name tied to the cluster, e.g. `hail.is` or `azure.hail.is`. But if it uses the environment variable name `HAIL_DOMAIN`, it will clobber the deploy config field `domain` in test namespaces which should be `internal.hail.is` not `hail.is`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14164#issuecomment-1898947090:22,config,config,22,https://hail.is,https://github.com/hail-is/hail/pull/14164#issuecomment-1898947090,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"Yes. from scala.concurrent.ExecutionContext:. ```; object ExecutionContext {; /**; * The explicit global `ExecutionContext`. Invoke `global` when you want to provide the global; * `ExecutionContext` explicitly.; *; * The default `ExecutionContext` implementation is backed by a work-stealing thread pool.; * It can be configured via the following [[scala.sys.SystemProperties]]:; *; * `scala.concurrent.context.minThreads` = defaults to ""1""; * `scala.concurrent.context.numThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxExtraThreads` = defaults to ""256""; *; * The pool size of threads is then `numThreads` bounded by `minThreads` on the lower end and `maxThreads` on the high end.; *; * The `maxExtraThreads` is the maximum number of extra threads to have at any given time to evade deadlock,; * see [[scala.concurrent.BlockContext]].; *; * @return the global `ExecutionContext`; */; def global: ExecutionContextExecutor = Implicits.global.asInstanceOf[ExecutionContextExecutor]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9775#issuecomment-738140989:318,config,configured,318,https://hail.is,https://github.com/hail-is/hail/pull/9775#issuecomment-738140989,1,['config'],['configured']
Modifiability,"Yet Another Terraform Refactoring PR, this creates two simple modules:; - A gcs_bucket module to remove the redundancy of resources that we have across the batch-logs, query and test bucket; - A ukbb module which sets up the ukbb k8s resources. While this technically would allow us to reuse this, say in azure, it's more an attempt to tease it apart from the google-specific infrastructure so that we wouldn't have to. In short, it would be nice to organize things such that deploying hail with or without the ukbb site is as simple as choosing to include or omit a terraform module.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10842:22,Refactor,Refactoring,22,https://hail.is,https://github.com/hail-is/hail/pull/10842,1,['Refactor'],['Refactoring']
Modifiability,You can use another variable name like `regions_from_conf`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1271873027:20,variab,variable,20,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1271873027,1,['variab'],['variable']
Modifiability,"You have an error based on where the maven code is being run (not the directory with the pom file).; ```; [[1;31mERROR[m] Failed to execute goal [32morg.apache.maven.plugins:maven-dependency-plugin:2.8:resolve[m [1m(default-cli)[m: [1;31mGoal requires a project to execute but there is no POM in this directory (/). Please verify you invoked Maven from the correct directory.[m -> [1m[Help 1][m; [[1;31mERROR[m] ; [[1;31mERROR[m] To see the full stack trace of the errors, re-run Maven with the [1m-e[m switch.; [[1;31mERROR[m] Re-run Maven using the [1m-X[m switch to enable full debug logging.; [[1;31mERROR[m] ; [[1;31mERROR[m] For more information about the errors and possible solutions, please read the following articles:; [[1;31mERROR[m] [1m[Help 1][m http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException; The command '/bin/sh -c mvn dependency:resolve && rm pom.xml test-jar-with-dependencies.xml' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270:169,plugin,plugins,169,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"Your previous post includes a warning message that your `SPARK_HOME`, in that shell, is set to:; ```; /opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2; ```; Could you try setting `SPARK_HOME` in the configuration file to `/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2` and then try again to create a `HailContext`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337610577:228,config,configuration,228,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337610577,1,['config'],['configuration']
Modifiability,[Config] Appsec sandbox config and files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14726:1,Config,Config,1,https://hail.is,https://github.com/hail-is/hail/pull/14726,3,"['Config', 'config', 'sandbox']","['Config', 'config', 'sandbox']"
Modifiability,"[First successful master build with this configuration](https://ci.hail.is/viewLog.html?buildId=493&buildTypeId=HailSourceCode_HailCi&tab=buildLog&consoleStyle=false#_state=116,125&focus=123). PR builds will also trigger the three compiles before the `clean test createDocs`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/742#issuecomment-245038405:41,config,configuration,41,https://hail.is,https://github.com/hail-is/hail/issues/742#issuecomment-245038405,1,['config'],['configuration']
Modifiability,"[Here](https://github.com/hail-is/hail/blob/62f606c5dd2f013ba7b43049f415ac0914bd6cf9/batch/batch/worker/worker.py#L1539-L1544) is where we specify the mount options for the `/io` mount, and [here](https://github.com/hail-is/hail/blob/62f606c5dd2f013ba7b43049f415ac0914bd6cf9/batch/batch/worker/worker.py#L1103) is the bulk of the container config and a link to the container runtime specification. For the most part the mount configuration maps to Linux mount options. I suspect we need to provide a mapping so that the `io` directory in the container is owned by the appropriate user and not just root.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13154#issuecomment-1588391511:340,config,config,340,https://hail.is,https://github.com/hail-is/hail/issues/13154#issuecomment-1588391511,2,['config'],"['config', 'configuration']"
Modifiability,[QOB] Parameterize code cache by backend (don't cache on service),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12618:6,Parameteriz,Parameterize,6,https://hail.is,https://github.com/hail-is/hail/pull/12618,1,['Parameteriz'],['Parameterize']
Modifiability,[RFC] Proposal: Move to sandboxed containers,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5111:24,sandbox,sandboxed,24,https://hail.is,https://github.com/hail-is/hail/issues/5111,1,['sandbox'],['sandboxed']
Modifiability,"[SIG Node]</li>; <li>Kubelet: add '--logging-format' flag to support structured logging (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91532"">kubernetes/kubernetes#91532</a>, <a href=""https://github.com/afrouzMashaykhi""><code>@​afrouzMashaykhi</code></a>) [SIG API Machinery, Cluster Lifecycle, Instrumentation and Node]</li>; <li>Kubernetes is now built with golang 1.15.0-rc.1.; <ul>; <li>The deprecated, legacy behavior of treating the CommonName field on X.509 serving certificates as a host name when no Subject Alternative Names are present is now disabled by default. It can be temporarily re-enabled by adding the value x509ignoreCN=0 to the GODEBUG environment variable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/93264"">kubernetes/kubernetes#93264</a>, <a href=""https://github.com/justaugustus""><code>@​justaugustus</code></a>) [SIG API Machinery, Auth, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Network, Node, Release, Scalability, Storage and Testing]</li>; </ul>; </li>; <li>Promote Immutable Secrets/ConfigMaps feature to Beta and enable the feature by default.; This allows to set <code>Immutable</code> field in Secrets or ConfigMap object to mark their contents as immutable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89594"">kubernetes/kubernetes#89594</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>) [SIG Apps and Testing]</li>; <li>Remove <code>BindTimeoutSeconds</code> from schedule configuration <code>KubeSchedulerConfiguration</code> (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91580"">kubernetes/kubernetes#91580</a>, <a href=""https://github.com/cofyc""><code>@​cofyc</code></a>) [SIG Scheduling and Testing]</li>; <li>Remove kubescheduler.config.k8s.io/v1alpha1 (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89298"">kubernetes/kubernetes#89298</a>, <a href=""https://github.com/gavi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:11158,Config,ConfigMaps,11158,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['Config'],['ConfigMaps']
Modifiability,"[VCF version 4.5](https://samtools.github.io/hts-specs/VCFv4.5.pdf) contains the changes we developed as part of our work developing the [Scalable Variant Call Representation](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1). As the developers and drivers of these changes, we should fully support v4.5 via import to VDS and export from VDS. Current checklist. May be extended over time:. - [x] Prefer `LEN` over `END` for reference blocks. (Begins in #14675); - [x] Update the combiner to convert to `LEN` from INFO `END` (Part of #14675).; - [x] Update `to_dense_mt` to use `LEN` (we think it may be more efficient).; - [ ] Add VDS to VCF export. (#14743); - [ ] Add Sparse VCF to VDS import. (#14743); - [x] ~'Official' non-ref genotype `<*>` support?~ (not part of this issue); - [ ] Make sure that we output well formed VCF 4.5, this includes things like VCF 4.4's phased haploid calls (this will also require updates to our parser)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14655:380,extend,extended,380,https://hail.is,https://github.com/hail-is/hail/issues/14655,1,['extend'],['extended']
Modifiability,"[`BlockMatrixIsDistributedMatrix`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/distributedmatrix/BlockMatrixIsDistributedMatrix.scala) implements the [`DistributedMatrix`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/distributedmatrix/DistributedMatrix.scala) API for Spark's `BlockMatrix` type. We should rewrite `BlockMatrix` from scratch to use Breeze matrices because the Spark `DenseMatrix` type doesn't provide a rich interface, in particular there are no exposed mutation primitives. I hope that an implementation on top of Breeze can more efficiently implement `vectorAddToEveryColumn` and `vectorPointwiseMultiplyEveryColumn` and `vectorPointwiseMultiplyEveryRow`. Also, we can move into `is.hail.distributedmatrix` `BetterBlockMatrix` which we, rather illicitly, shove into the apache package during jar creation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1979:351,rewrite,rewrite,351,https://hail.is,https://github.com/hail-is/hail/issues/1979,1,['rewrite'],['rewrite']
Modifiability,[aioclient] Refactor BatchBuilder submit for ease of job groups changes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13448:12,Refactor,Refactor,12,https://hail.is,https://github.com/hail-is/hail/pull/13448,1,['Refactor'],['Refactor']
Modifiability,"[annotationdb] [datasets] specify region in annotation db instance, use reformatted/unified json config file, updated datasets",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9496:97,config,config,97,https://hail.is,https://github.com/hail-is/hail/pull/9496,1,['config'],['config']
Modifiability,[apiserver]: fix variable expansion,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5829:17,variab,variable,17,https://hail.is,https://github.com/hail-is/hail/pull/5829,1,['variab'],['variable']
Modifiability,[auth] Mount the global config in the auth driver pod,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10962:24,config,config,24,https://hail.is,https://github.com/hail-is/hail/pull/10962,1,['config'],['config']
Modifiability,[auth] Remove dead google config variables,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10899:26,config,config,26,https://hail.is,https://github.com/hail-is/hail/pull/10899,2,"['config', 'variab']","['config', 'variables']"
Modifiability,[auth] get GSuite organization from global-config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9793:43,config,config,43,https://hail.is,https://github.com/hail-is/hail/pull/9793,1,['config'],['config']
Modifiability,[batch2] a config error won't have a timing field,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7542:11,config,config,11,https://hail.is,https://github.com/hail-is/hail/pull/7542,1,['config'],['config']
Modifiability,[batch2] add memory request to docker container config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7498:48,config,config,48,https://hail.is,https://github.com/hail-is/hail/pull/7498,2,['config'],['config']
Modifiability,[batch2] expose configuration in batch2 driver web UI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7606:16,config,configuration,16,https://hail.is,https://github.com/hail-is/hail/pull/7606,1,['config'],['configuration']
Modifiability,[batch2] fix status if error occurred when getting the job config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7522:59,config,config,59,https://hail.is,https://github.com/hail-is/hail/pull/7522,1,['config'],['config']
Modifiability,"[batch2] refactor Batch, Job",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7211:9,refactor,refactor,9,https://hail.is,https://github.com/hail-is/hail/pull/7211,1,['refactor'],['refactor']
Modifiability,[batch] Add batch-config to expose current batch id to user jobs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12246:18,config,config,18,https://hail.is,https://github.com/hail-is/hail/pull/12246,1,['config'],['config']
Modifiability,[batch] Add configurable spot price percentage increases,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13252:12,config,configurable,12,https://hail.is,https://github.com/hail-is/hail/pull/13252,1,['config'],['configurable']
Modifiability,[batch] Add environment variable for batch id in worker,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12662:24,variab,variable,24,https://hail.is,https://github.com/hail-is/hail/pull/12662,1,['variab'],['variable']
Modifiability,[batch] Add hailctl config batch/backend,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12522:20,config,config,20,https://hail.is,https://github.com/hail-is/hail/pull/12522,1,['config'],['config']
Modifiability,[batch] Add json parsing and severity to GCP Ops Agent config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14187:55,config,config,55,https://hail.is,https://github.com/hail-is/hail/pull/14187,1,['config'],['config']
Modifiability,[batch] Clean up environment variables for easier local execution of batch tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12862:29,variab,variables,29,https://hail.is,https://github.com/hail-is/hail/pull/12862,1,['variab'],['variables']
Modifiability,[batch] Compare and reject stale driver config changes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11798:40,config,config,40,https://hail.is,https://github.com/hail-is/hail/pull/11798,1,['config'],['config']
Modifiability,[batch] Compute optimal worker type based on cpu / memory configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10097:58,config,configuration,58,https://hail.is,https://github.com/hail-is/hail/pull/10097,1,['config'],['configuration']
Modifiability,[batch] Configurable worker disk type,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9288:8,Config,Configurable,8,https://hail.is,https://github.com/hail-is/hail/pull/9288,1,['Config'],['Configurable']
Modifiability,[batch] Configure XFS quotas without configuration files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10467:8,Config,Configure,8,https://hail.is,https://github.com/hail-is/hail/pull/10467,2,"['Config', 'config']","['Configure', 'configuration']"
Modifiability,[batch] Delete unused batch user code ssl config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13068:42,config,config,42,https://hail.is,https://github.com/hail-is/hail/pull/13068,1,['config'],['config']
Modifiability,[batch] Expose region variable to users,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12465:22,variab,variable,22,https://hail.is,https://github.com/hail-is/hail/pull/12465,1,['variab'],['variable']
Modifiability,[batch] Fix autoscaling policy in GCP Terraform config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12025:48,config,config,48,https://hail.is,https://github.com/hail-is/hail/pull/12025,1,['config'],['config']
Modifiability,[batch] Fix defaulting to config.ini for domain,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11131:26,config,config,26,https://hail.is,https://github.com/hail-is/hail/pull/11131,1,['config'],['config']
Modifiability,[batch] Fix jq wiping deploy config in test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10850:29,config,config,29,https://hail.is,https://github.com/hail-is/hail/pull/10850,1,['config'],['config']
Modifiability,[batch] Flexible storage configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9598:8,Flexible,Flexible,8,https://hail.is,https://github.com/hail-is/hail/pull/9598,4,"['Flexible', 'config']","['Flexible', 'configuration']"
Modifiability,[batch] Let worker control job deploy-config instead of using k8s secret,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13056:38,config,config,38,https://hail.is,https://github.com/hail-is/hail/pull/13056,1,['config'],['config']
Modifiability,[batch] Make ServiceBackend respect environment variable configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12799:48,variab,variable,48,https://hail.is,https://github.com/hail-is/hail/pull/12799,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,[batch] Make driver parameters configurable in the UI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12575:31,config,configurable,31,https://hail.is,https://github.com/hail-is/hail/pull/12575,1,['config'],['configurable']
Modifiability,[batch] Mitigate test failures by extending batch client timeout,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12709:34,extend,extending,34,https://hail.is,https://github.com/hail-is/hail/pull/12709,1,['extend'],['extending']
Modifiability,[batch] Mount external deploy config for public jobs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10774:30,config,config,30,https://hail.is,https://github.com/hail-is/hail/pull/10774,1,['config'],['config']
Modifiability,[batch] Mount worker deploy config instead of using k8s secret,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13203:28,config,config,28,https://hail.is,https://github.com/hail-is/hail/pull/13203,1,['config'],['config']
Modifiability,[batch] Move config.json into the container bundle,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13438:13,config,config,13,https://hail.is,https://github.com/hail-is/hail/pull/13438,1,['config'],['config']
Modifiability,[batch] Refactor resource billing checks with additional debugging info,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12713:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/12713,1,['Refactor'],['Refactor']
Modifiability,[batch] Refactor scheduler code for multiple instance pools,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9774:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/9774,1,['Refactor'],['Refactor']
Modifiability,[batch] Refactor the driver to be cloud-agnostic,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10860:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/10860,1,['Refactor'],['Refactor']
Modifiability,[batch] Refactor worker Container class into Image and Container,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11396:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/11396,1,['Refactor'],['Refactor']
Modifiability,[batch] Refactor zone quotas code into a zone monitor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9769:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/9769,1,['Refactor'],['Refactor']
Modifiability,[batch] Remove redundant env variable for internal gateway ip,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12005:29,variab,variable,29,https://hail.is,https://github.com/hail-is/hail/pull/12005,1,['variab'],['variable']
Modifiability,[batch] Specify web server port through env variable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12613:44,variab,variable,44,https://hail.is,https://github.com/hail-is/hail/pull/12613,1,['variab'],['variable']
Modifiability,[batch] Use weights for cpu and blkio in container config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8973:51,config,config,51,https://hail.is,https://github.com/hail-is/hail/pull/8973,1,['config'],['config']
Modifiability,[batch] add batch/bucket configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8852:25,config,configuration,25,https://hail.is,https://github.com/hail-is/hail/pull/8852,1,['config'],['configuration']
Modifiability,[batch] adds requester pays config to `hailtop.fs.open`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13795:28,config,config,28,https://hail.is,https://github.com/hail-is/hail/pull/13795,1,['config'],['config']
Modifiability,[batch] configure logging,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6873:8,config,configure,8,https://hail.is,https://github.com/hail-is/hail/pull/6873,1,['config'],['configure']
Modifiability,[batch] fix HAIL_GENETICS_IMAGES variable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11117:33,variab,variable,33,https://hail.is,https://github.com/hail-is/hail/pull/11117,1,['variab'],['variable']
Modifiability,[batch] fix deserialize for compound variable names,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5738:37,variab,variable,37,https://hail.is,https://github.com/hail-is/hail/pull/5738,1,['variab'],['variable']
Modifiability,[batch] fix getting status when job config fails,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8760:36,config,config,36,https://hail.is,https://github.com/hail-is/hail/pull/8760,1,['config'],['config']
Modifiability,[batch] fix warning to use forward slash in config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11116:44,config,config,44,https://hail.is,https://github.com/hail-is/hail/pull/11116,1,['config'],['config']
Modifiability,[batch] make master url configurable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6805:24,config,configurable,24,https://hail.is,https://github.com/hail-is/hail/pull/6805,1,['config'],['configurable']
Modifiability,[batch] make standing worker configurable in the UI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9844:29,config,configurable,29,https://hail.is,https://github.com/hail-is/hail/pull/9844,1,['config'],['configurable']
Modifiability,[batch] reads `HAIL_BATCH_REGIONS` environment variable correctly,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13239:47,variab,variable,47,https://hail.is,https://github.com/hail-is/hail/pull/13239,1,['variab'],['variable']
Modifiability,[batch] refactor how jobs are submitted in batches,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6479:8,refactor,refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/6479,1,['refactor'],['refactor']
Modifiability,[batch] refactor job schema to understand different job types,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9857:8,refactor,refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/9857,1,['refactor'],['refactor']
Modifiability,[batch] refactor test_batch.py to use pytest instead of unittest,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9584:8,refactor,refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/9584,1,['refactor'],['refactor']
Modifiability,[batch] remove unused variable in test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5542:22,variab,variable,22,https://hail.is,https://github.com/hail-is/hail/pull/5542,1,['variab'],['variable']
Modifiability,[batch] rewrite schema validation so that schema description is working validation function,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9861:8,rewrite,rewrite,8,https://hail.is,https://github.com/hail-is/hail/pull/9861,1,['rewrite'],['rewrite']
Modifiability,"[benchmark] Add more configuration, add version info to json output",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6796:21,config,configuration,21,https://hail.is,https://github.com/hail-is/hail/pull/6796,1,['config'],['configuration']
Modifiability,[benchmark] Allow configuration of `BENCHMARK_DOCKER_TAG`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12846:18,config,configuration,18,https://hail.is,https://github.com/hail-is/hail/pull/12846,1,['config'],['configuration']
Modifiability,[benchmark] Parameterize batch service,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7461:12,Parameteriz,Parameterize,12,https://hail.is,https://github.com/hail-is/hail/pull/7461,1,['Parameteriz'],['Parameterize']
Modifiability,"[benchmark] Parameterize output formats, files, verbosity.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6558:12,Parameteriz,Parameterize,12,https://hail.is,https://github.com/hail-is/hail/pull/6558,1,['Parameteriz'],['Parameterize']
Modifiability,[benchmark] add `visualize` + refactor hail-bench,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12856:30,refactor,refactor,30,https://hail.is,https://github.com/hail-is/hail/pull/12856,1,['refactor'],['refactor']
Modifiability,[benchmark] add debugging / refactor test,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10144:28,refactor,refactor,28,https://hail.is,https://github.com/hail-is/hail/pull/10144,1,['refactor'],['refactor']
Modifiability,"[benchmark] parameterize metric, skip short benchmarks",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7105:12,parameteriz,parameterize,12,https://hail.is,https://github.com/hail-is/hail/pull/7105,1,['parameteriz'],['parameterize']
Modifiability,[benchmark] use HAIL_PYTHON3 variable instead of `python`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7017:29,variab,variable,29,https://hail.is,https://github.com/hail-is/hail/pull/7017,1,['variab'],['variable']
Modifiability,"[bugfix] rewrite StringLength and StringSlice as IR functions, fix behavior of len(:poop:)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5619:9,rewrite,rewrite,9,https://hail.is,https://github.com/hail-is/hail/pull/5619,1,['rewrite'],['rewrite']
Modifiability,[build.yaml] standardize on one location for sql-config.cnf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8416:49,config,config,49,https://hail.is,https://github.com/hail-is/hail/pull/8416,1,['config'],['config']
Modifiability,[build.yaml] steps using database-server-config must depend on its creation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11441:41,config,config,41,https://hail.is,https://github.com/hail-is/hail/pull/11441,1,['config'],['config']
Modifiability,[build] Check cloud at config time in deploy config step,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11026:23,config,config,23,https://hail.is,https://github.com/hail-is/hail/pull/11026,2,['config'],['config']
Modifiability,[build] rewrite kubectl secret copying to avoid deprecated --export flag,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9561:8,rewrite,rewrite,8,https://hail.is,https://github.com/hail-is/hail/pull/9561,1,['rewrite'],['rewrite']
Modifiability,[build] stop evaluating upload-qob-jar variables on makefile parsing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12616:39,variab,variables,39,https://hail.is,https://github.com/hail-is/hail/pull/12616,1,['variab'],['variables']
Modifiability,[c++] Pass Hadoop Config to C++ and write NDArray,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5837:18,Config,Config,18,https://hail.is,https://github.com/hail-is/hail/pull/5837,1,['Config'],['Config']
Modifiability,[ci] Add kubernetes server url to global variables,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7459:41,variab,variables,41,https://hail.is,https://github.com/hail-is/hail/pull/7459,1,['variab'],['variables']
Modifiability,[ci] Add tests for generating envoy xds configs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14524:40,config,configs,40,https://hail.is,https://github.com/hail-is/hail/pull/14524,1,['config'],['configs']
Modifiability,[ci] Alert zulip when deploy config fails to build,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10828:29,config,config,29,https://hail.is,https://github.com/hail-is/hail/pull/10828,1,['config'],['config']
Modifiability,[ci] Catch cloud-incompatible steps at build config time,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11159:45,config,config,45,https://hail.is,https://github.com/hail-is/hail/pull/11159,1,['config'],['config']
Modifiability,[ci] Clean up temporary config for OAuth changes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13570:24,config,config,24,https://hail.is,https://github.com/hail-is/hail/pull/13570,1,['config'],['config']
Modifiability,[ci] Fix errant line in environment variable spec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13628:36,variab,variable,36,https://hail.is,https://github.com/hail-is/hail/pull/13628,1,['variab'],['variable']
Modifiability,[ci] Fix global-config creation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9795:16,config,config,16,https://hail.is,https://github.com/hail-is/hail/pull/9795,1,['config'],['config']
Modifiability,[ci] Make zulip-config optional,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11299:16,config,config,16,https://hail.is,https://github.com/hail-is/hail/pull/11299,1,['config'],['config']
Modifiability,[ci] Mostly pass through global-config in CI instead of constructing from env variables,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10911:32,config,config,32,https://hail.is,https://github.com/hail-is/hail/pull/10911,2,"['config', 'variab']","['config', 'variables']"
Modifiability,[ci] Pretty print create_database.py json config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9533:42,config,config,42,https://hail.is,https://github.com/hail-is/hail/pull/9533,1,['config'],['config']
Modifiability,[ci] Remove dead code from CI configs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13967:30,config,configs,30,https://hail.is,https://github.com/hail-is/hail/pull/13967,1,['config'],['configs']
Modifiability,[ci] Remove unnecessary worker-deploy-config secret,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13343:38,config,config,38,https://hail.is,https://github.com/hail-is/hail/pull/13343,1,['config'],['config']
Modifiability,[ci] actually fix nullness in sql config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8500:34,config,config,34,https://hail.is,https://github.com/hail-is/hail/pull/8500,1,['config'],['config']
Modifiability,[ci] add docker root image and prefix to CI global config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10340:51,config,config,51,https://hail.is,https://github.com/hail-is/hail/pull/10340,1,['config'],['config']
Modifiability,[ci] change CreateDatabase step to use config file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7302:39,config,config,39,https://hail.is,https://github.com/hail-is/hail/pull/7302,1,['config'],['config']
Modifiability,"[ci] create global-config in ""default"" namespaces created by CI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9777:19,config,config,19,https://hail.is,https://github.com/hail-is/hail/pull/9777,1,['config'],['config']
Modifiability,[ci] parameterize CreateDatabase step,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7283:5,parameteriz,parameterize,5,https://hail.is,https://github.com/hail-is/hail/pull/7283,1,['parameteriz'],['parameterize']
Modifiability,[ci] use dev database server config in dev deploy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7683:29,config,config,29,https://hail.is,https://github.com/hail-is/hail/pull/7683,1,['config'],['config']
Modifiability,[ci] use global config in ci test deployment,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9790:16,config,config,16,https://hail.is,https://github.com/hail-is/hail/pull/9790,1,['config'],['config']
Modifiability,[combiner] use a single variable rather than a map for merge function,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5585:24,variab,variable,24,https://hail.is,https://github.com/hail-is/hail/pull/5585,1,['variab'],['variable']
Modifiability,[compiler] Add `scalafix` and `scalafmt` configs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14126:41,config,configs,41,https://hail.is,https://github.com/hail-is/hail/pull/14126,1,['config'],['configs']
Modifiability,[compiler] Add rewrite rule for nested CastRename nodes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10577:15,rewrite,rewrite,15,https://hail.is,https://github.com/hail-is/hail/pull/10577,1,['rewrite'],['rewrite']
Modifiability,"[compiler] BIG PR: refactor type propagation to EmitCodes, remove InferPType",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10477:19,refactor,refactor,19,https://hail.is,https://github.com/hail-is/hail/pull/10477,1,['refactor'],['refactor']
Modifiability,[compiler] Refactor EncodedLiteral / TableLiteral to use multiple byte arrays,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11062:11,Refactor,Refactor,11,https://hail.is,https://github.com/hail-is/hail/pull/11062,1,['Refactor'],['Refactor']
Modifiability,[compiler] Refactor ExecuteContext to live in `is.hail.backend`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10967:11,Refactor,Refactor,11,https://hail.is,https://github.com/hail-is/hail/pull/10967,1,['Refactor'],['Refactor']
Modifiability,[compiler] Refactor UID knowledge into TableReader subtrait,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12579:11,Refactor,Refactor,11,https://hail.is,https://github.com/hail-is/hail/pull/12579,1,['Refactor'],['Refactor']
Modifiability,[compiler] Refactor compiled functions to take a HailTaskContext inst…,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12597:11,Refactor,Refactor,11,https://hail.is,https://github.com/hail-is/hail/pull/12597,1,['Refactor'],['Refactor']
Modifiability,[compiler] Rewrite StreamTake and StreamDrop to TakeWhile and DropWhile,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10773:11,Rewrite,Rewrite,11,https://hail.is,https://github.com/hail-is/hail/pull/10773,1,['Rewrite'],['Rewrite']
Modifiability,[compiler] Rewrite lowered partition alignment with takeWhile/dropWhile,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10788:11,Rewrite,Rewrite,11,https://hail.is,https://github.com/hail-is/hail/pull/10788,1,['Rewrite'],['Rewrite']
Modifiability,[compiler] extend + fix simplifier for integral types,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754:11,extend,extend,11,https://hail.is,https://github.com/hail-is/hail/pull/12754,2,['extend'],['extend']
Modifiability,[compiler] refactor BlockMatrix lowering to generate small IR,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12624:11,refactor,refactor,11,https://hail.is,https://github.com/hail-is/hail/pull/12624,1,['refactor'],['refactor']
Modifiability,"[compiler] refactor PType.store, SType.coerceOrCopy to take/return values",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10848:11,refactor,refactor,11,https://hail.is,https://github.com/hail-is/hail/pull/10848,1,['refactor'],['refactor']
Modifiability,[compiler] refactor types in bindings,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14517:11,refactor,refactor,11,https://hail.is,https://github.com/hail-is/hail/pull/14517,1,['refactor'],['refactor']
Modifiability,[compiler] rewrite ExtractIntervalFilters to be more robust,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13355:11,rewrite,rewrite,11,https://hail.is,https://github.com/hail-is/hail/pull/13355,1,['rewrite'],['rewrite']
Modifiability,[compiler] rewrite FreeVariables to not duplicate binding structure,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14451:11,rewrite,rewrite,11,https://hail.is,https://github.com/hail-is/hail/pull/14451,1,['rewrite'],['rewrite']
Modifiability,[config] Add max_polling_delay environment variable for wait ops,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13281:1,config,config,1,https://hail.is,https://github.com/hail-is/hail/pull/13281,2,"['config', 'variab']","['config', 'variable']"
Modifiability,[config] Fixed handling of non-existent domain,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11334:1,config,config,1,https://hail.is,https://github.com/hail-is/hail/pull/11334,1,['config'],['config']
Modifiability,[datasets] Update datasets.py to use checked-in config file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9411:48,config,config,48,https://hail.is,https://github.com/hail-is/hail/pull/9411,1,['config'],['config']
Modifiability,[deploy-config] add service deploy-config location to defaults,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8580:8,config,config,8,https://hail.is,https://github.com/hail-is/hail/pull/8580,2,['config'],['config']
Modifiability,[deploy_config] Add domain support to hailctl config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11113:46,config,config,46,https://hail.is,https://github.com/hail-is/hail/pull/11113,1,['config'],['config']
Modifiability,"[dev] Parameterize shadowJar/releaseJar, better names for install targets",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6450:6,Parameteriz,Parameterize,6,https://hail.is,https://github.com/hail-is/hail/pull/6450,1,['Parameteriz'],['Parameterize']
Modifiability,[devdocs] Add dev doc describing gateways configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14488:42,config,configuration,42,https://hail.is,https://github.com/hail-is/hail/pull/14488,1,['config'],['configuration']
Modifiability,[docker] Add config support for docker prefix and root image,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10347:13,config,config,13,https://hail.is,https://github.com/hail-is/hail/pull/10347,1,['config'],['config']
Modifiability,[docker] Reorder hailgenetics hail docker image layers,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12301:48,layers,layers,48,https://hail.is,https://github.com/hail-is/hail/pull/12301,1,['layers'],['layers']
Modifiability,[docker] fix keyfile name in Spark config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7834:35,config,config,35,https://hail.is,https://github.com/hail-is/hail/pull/7834,1,['config'],['config']
Modifiability,[docs] Add VEP config file locations on Google Storage to docs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6143:15,config,config,15,https://hail.is,https://github.com/hail-is/hail/pull/6143,1,['config'],['config']
Modifiability,[feature] Extend import_table missing parameter to support multiple v…,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5614:10,Extend,Extend,10,https://hail.is,https://github.com/hail-is/hail/pull/5614,1,['Extend'],['Extend']
Modifiability,[fs] Teach GoogleStorageAsyncFS to use the rewrite API,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14603:43,rewrite,rewrite,43,https://hail.is,https://github.com/hail-is/hail/pull/14603,1,['rewrite'],['rewrite']
Modifiability,[fs] hailtop.fs makes it impossible to explicitly configure the GCS requester pays project in code,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13567:50,config,configure,50,https://hail.is,https://github.com/hail-is/hail/issues/13567,1,['config'],['configure']
Modifiability,[gear] Add GCPConfig that can be deduced from global config at app runtime,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10957:53,config,config,53,https://hail.is,https://github.com/hail-is/hail/pull/10957,1,['config'],['config']
Modifiability,[global] add batch regions to global-config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9786:37,config,config,37,https://hail.is,https://github.com/hail-is/hail/pull/9786,1,['config'],['config']
Modifiability,[gradle] update shadow plugin,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10263:23,plugin,plugin,23,https://hail.is,https://github.com/hail-is/hail/pull/10263,1,['plugin'],['plugin']
Modifiability,[grafana] Configure datasources in ConfigMap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10627:10,Config,Configure,10,https://hail.is,https://github.com/hail-is/hail/pull/10627,2,['Config'],"['ConfigMap', 'Configure']"
Modifiability,[hail/ptypes] improve representation type check for ComplexPType inheritors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7746:65,inherit,inheritors,65,https://hail.is,https://github.com/hail-is/hail/issues/7746,1,['inherit'],['inheritors']
Modifiability,"[hail/ptypes] lift rvd.rowPType/bind to variable, to avoid rvd serialization",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8134:40,variab,variable,40,https://hail.is,https://github.com/hail-is/hail/pull/8134,1,['variab'],['variable']
Modifiability,[hail/ptypes] refactor InferPType.getNestedElementPTypesOfSameType to method on ptype,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7921:14,refactor,refactor,14,https://hail.is,https://github.com/hail-is/hail/issues/7921,1,['refactor'],['refactor']
Modifiability,[hail/ptypes] refactor getNestedElementPTypes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7927:14,refactor,refactor,14,https://hail.is,https://github.com/hail-is/hail/pull/7927,1,['refactor'],['refactor']
Modifiability,[hail/ptypes] refactor pType.isOfType,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7926:14,refactor,refactor,14,https://hail.is,https://github.com/hail-is/hail/issues/7926,1,['refactor'],['refactor']
Modifiability,[hail] (PTypes) Extend function registry to support returned PTypes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6650:16,Extend,Extend,16,https://hail.is,https://github.com/hail-is/hail/pull/6650,1,['Extend'],['Extend']
Modifiability,"[hail] Extend AggArrayPerElement to support a ""knownLength"" argument.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6125:7,Extend,Extend,7,https://hail.is,https://github.com/hail-is/hail/pull/6125,1,['Extend'],['Extend']
Modifiability,[hail] Extend ExtractIntervalFilters to operate on key structs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6302:7,Extend,Extend,7,https://hail.is,https://github.com/hail-is/hail/pull/6302,1,['Extend'],['Extend']
Modifiability,[hail] Extend PruneDeadFields to prune tuples,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6699:7,Extend,Extend,7,https://hail.is,https://github.com/hail-is/hail/pull/6699,1,['Extend'],['Extend']
Modifiability,[hail] Extend TableKeyByAndAggregate => TableAggregateByKey rewrite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7073:7,Extend,Extend,7,https://hail.is,https://github.com/hail-is/hail/pull/7073,2,"['Extend', 'rewrite']","['Extend', 'rewrite']"
Modifiability,[hail] Extend naming functionality on FunctionBuilder,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7053:7,Extend,Extend,7,https://hail.is,https://github.com/hail-is/hail/pull/7053,1,['Extend'],['Extend']
Modifiability,[hail] Fix memory leak in BM and make cache size configurable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9501:49,config,configurable,49,https://hail.is,https://github.com/hail-is/hail/pull/9501,1,['config'],['configurable']
Modifiability,[hail] Fix simplify rewrite of ArrayLen(TableCollect),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7539:20,rewrite,rewrite,20,https://hail.is,https://github.com/hail-is/hail/pull/7539,2,['rewrite'],['rewrite']
Modifiability,"[hail] Introduce ptypes to BGEN, refactor pruning logic to be simpler",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6576:33,refactor,refactor,33,https://hail.is,https://github.com/hail-is/hail/pull/6576,2,['refactor'],['refactor']
Modifiability,[hail] Parameterize check on union_rows. Don't check in `split_multi`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6669:7,Parameteriz,Parameterize,7,https://hail.is,https://github.com/hail-is/hail/pull/6669,1,['Parameteriz'],['Parameterize']
Modifiability,[hail] Refactor ExtractIntervalFilters,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6298:7,Refactor,Refactor,7,https://hail.is,https://github.com/hail-is/hail/pull/6298,1,['Refactor'],['Refactor']
Modifiability,[hail] Refactor LowerTableIR to use new LoweringPipeline infrastructure.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7535:7,Refactor,Refactor,7,https://hail.is,https://github.com/hail-is/hail/pull/7535,1,['Refactor'],['Refactor']
Modifiability,"[hail] Refactor getOrDefineMethod to be on FB, not EFB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8108:7,Refactor,Refactor,7,https://hail.is,https://github.com/hail-is/hail/pull/8108,1,['Refactor'],['Refactor']
Modifiability,"[hail] Rewrite PType.subsetTo to avoid using `canonical`, which is wrong",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7870:7,Rewrite,Rewrite,7,https://hail.is,https://github.com/hail-is/hail/pull/7870,1,['Rewrite'],['Rewrite']
Modifiability,[hail] Rewrite VCF INFO Parser to not use htsjdk,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5828:7,Rewrite,Rewrite,7,https://hail.is,https://github.com/hail-is/hail/pull/5828,1,['Rewrite'],['Rewrite']
Modifiability,[hail] Rewrite the boundary of encoders and physical types,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6748:7,Rewrite,Rewrite,7,https://hail.is,https://github.com/hail-is/hail/pull/6748,1,['Rewrite'],['Rewrite']
Modifiability,[hail] Unify sql-config secret creation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9113:17,config,config,17,https://hail.is,https://github.com/hail-is/hail/pull/9113,1,['config'],['config']
Modifiability,[hail] makefiles traditionally have spaces around variable defs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6294:50,variab,variable,50,https://hail.is,https://github.com/hail-is/hail/pull/6294,1,['variab'],['variable']
Modifiability,"[hail] parameterize the `In` node by ptype, not virtual type.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7451:7,parameteriz,parameterize,7,https://hail.is,https://github.com/hail-is/hail/pull/7451,1,['parameteriz'],['parameterize']
Modifiability,[hail] refactor RPrimitive.typeSupported,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8765:7,refactor,refactor,7,https://hail.is,https://github.com/hail-is/hail/pull/8765,1,['refactor'],['refactor']
Modifiability,[hail] refactor context and broadcast value handling in TableStage,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7898:7,refactor,refactor,7,https://hail.is,https://github.com/hail-is/hail/pull/7898,1,['refactor'],['refactor']
Modifiability,[hail] remove setRegion(region) parameterization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7908:32,parameteriz,parameterization,32,https://hail.is,https://github.com/hail-is/hail/issues/7908,1,['parameteriz'],['parameterization']
Modifiability,[hail] rewrite uniroot in python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7748:7,rewrite,rewrite,7,https://hail.is,https://github.com/hail-is/hail/pull/7748,1,['rewrite'],['rewrite']
Modifiability,[hail][annotationdb] refactor annotation db,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7178:21,refactor,refactor,21,https://hail.is,https://github.com/hail-is/hail/pull/7178,1,['refactor'],['refactor']
Modifiability,[hail][bugfix] Fix TableFilterIntervals/TableMapRows rewrite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6419:53,rewrite,rewrite,53,https://hail.is,https://github.com/hail-is/hail/pull/6419,1,['rewrite'],['rewrite']
Modifiability,[hailctl config] Add checking to 'batch/bucket' and 'email',MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9115:9,config,config,9,https://hail.is,https://github.com/hail-is/hail/pull/9115,1,['config'],['config']
Modifiability,"[hailctl dataproc] Use an env variable for log dir, set on dataproc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10574:30,variab,variable,30,https://hail.is,https://github.com/hail-is/hail/pull/10574,1,['variab'],['variable']
Modifiability,[hailctl] Add basic tests for hailctl config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13389:38,config,config,38,https://hail.is,https://github.com/hail-is/hail/pull/13389,1,['config'],['config']
Modifiability,[hailctl] Add list command to dump parts of user config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9529:49,config,config,49,https://hail.is,https://github.com/hail-is/hail/pull/9529,1,['config'],['config']
Modifiability,"[hailctl] Autocomplete for hailctl config {get,set,unset}",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13224:35,config,config,35,https://hail.is,https://github.com/hail-is/hail/pull/13224,1,['config'],['config']
Modifiability,[hailctl] Configure Hail/Spark to use proper memory allocations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11016:10,Config,Configure,10,https://hail.is,https://github.com/hail-is/hail/pull/11016,1,['Config'],['Configure']
Modifiability,[hailctl] Dev config set should only change one property,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14169:14,config,config,14,https://hail.is,https://github.com/hail-is/hail/pull/14169,1,['config'],['config']
Modifiability,[hailctl] Don't parameterize safe_call on python version,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10324:16,parameteriz,parameterize,16,https://hail.is,https://github.com/hail-is/hail/pull/10324,1,['parameteriz'],['parameterize']
Modifiability,[hailctl] Dont copy the user's config file into hailctl batch submit jobs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13506:31,config,config,31,https://hail.is,https://github.com/hail-is/hail/pull/13506,1,['config'],['config']
Modifiability,[hailctl] Dont show local variables in stacktraces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13212:26,variab,variables,26,https://hail.is,https://github.com/hail-is/hail/pull/13212,1,['variab'],['variables']
Modifiability,[hailctl] Fix config file name,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9493:14,config,config,14,https://hail.is,https://github.com/hail-is/hail/pull/9493,1,['config'],['config']
Modifiability,[hailctl] Fix environments and configurations so things actually work.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6297:31,config,configurations,31,https://hail.is,https://github.com/hail-is/hail/pull/6297,1,['config'],['configurations']
Modifiability,[hailctl] Move default location for hail config directory,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125:41,config,config,41,https://hail.is,https://github.com/hail-is/hail/pull/7125,1,['config'],['config']
Modifiability,[hailctl] include deploy configuration in module,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6196:25,config,configuration,25,https://hail.is,https://github.com/hail-is/hail/pull/6196,1,['config'],['configuration']
Modifiability,[hailctl] rewrite argument parsing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842:10,rewrite,rewrite,10,https://hail.is,https://github.com/hail-is/hail/pull/9842,1,['rewrite'],['rewrite']
Modifiability,[hailctl] support missing domain in deploy config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11333:43,config,config,43,https://hail.is,https://github.com/hail-is/hail/pull/11333,1,['config'],['config']
Modifiability,[hailctl] use consistent names for variables,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10240:35,variab,variables,35,https://hail.is,https://github.com/hail-is/hail/pull/10240,1,['variab'],['variables']
Modifiability,[hailctl][batch] add hailctl config & make tests work locally,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8559:29,config,config,29,https://hail.is,https://github.com/hail-is/hail/pull/8559,1,['config'],['config']
Modifiability,[hailtop] Add subpath to deploy config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14056:32,config,config,32,https://hail.is,https://github.com/hail-is/hail/pull/14056,1,['config'],['config']
Modifiability,[hailtop] Fix defaulting to hail.is when the user has no deploy-config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13585:64,config,config,64,https://hail.is,https://github.com/hail-is/hail/pull/13585,1,['config'],['config']
Modifiability,[hailtop] allow configuration of default HTTP timeout,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14206:16,config,configuration,16,https://hail.is,https://github.com/hail-is/hail/pull/14206,1,['config'],['configuration']
Modifiability,[influxdb] Influxdb Config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10725:20,Config,Config,20,https://hail.is,https://github.com/hail-is/hail/pull/10725,1,['Config'],['Config']
Modifiability,[infra] Add resource group to azure terraform state remote config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13125:59,config,config,59,https://hail.is,https://github.com/hail-is/hail/pull/13125,1,['config'],['config']
Modifiability,[infra] Add scripts for azure setup and sql/global configs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10919:51,config,configs,51,https://hail.is,https://github.com/hail-is/hail/pull/10919,1,['config'],['configs']
Modifiability,[infra] Configure GCP peering routes in tf intsead of manually,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11031:8,Config,Configure,8,https://hail.is,https://github.com/hail-is/hail/pull/11031,1,['Config'],['Configure']
Modifiability,[infra] Move azure from mysql single server to mysql flexible server,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11423:53,flexible,flexible,53,https://hail.is,https://github.com/hail-is/hail/pull/11423,1,['flexible'],['flexible']
Modifiability,[infra] Provide namespace variable to bootstrap uses of make deploy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12044:26,variab,variable,26,https://hail.is,https://github.com/hail-is/hail/pull/12044,1,['variab'],['variable']
Modifiability,[infra] Remove hard-coded values from config.mk,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11414:38,config,config,38,https://hail.is,https://github.com/hail-is/hail/pull/11414,1,['config'],['config']
Modifiability,[infra] Revert GKE metadata workload identity config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13208:46,config,config,46,https://hail.is,https://github.com/hail-is/hail/pull/13208,1,['config'],['config']
Modifiability,[infra] Update gcp global-config with cloud field,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11012:26,config,config,26,https://hail.is,https://github.com/hail-is/hail/pull/11012,1,['config'],['config']
Modifiability,[infra] Use https instead of hail-az in service configs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14290:48,config,configs,48,https://hail.is,https://github.com/hail-is/hail/pull/14290,1,['config'],['configs']
Modifiability,[infra] allow configuration of oauth2 callback list,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11322:14,config,configuration,14,https://hail.is,https://github.com/hail-is/hail/pull/11322,1,['config'],['configuration']
Modifiability,[k8s] Use global config values from k8s secret when possible,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10869:17,config,config,17,https://hail.is,https://github.com/hail-is/hail/pull/10869,1,['config'],['config']
Modifiability,[local] Parameterize heap size of local backend,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13122:8,Parameteriz,Parameterize,8,https://hail.is,https://github.com/hail-is/hail/pull/13122,1,['Parameteriz'],['Parameterize']
Modifiability,[lowering] Rewrite maximal independent set to be its own value IR,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12295:11,Rewrite,Rewrite,11,https://hail.is,https://github.com/hail-is/hail/pull/12295,1,['Rewrite'],['Rewrite']
Modifiability,[lowering] TableStage Refactor / TableParallelize Lowering Improvement,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8627:22,Refactor,Refactor,22,https://hail.is,https://github.com/hail-is/hail/pull/8627,1,['Refactor'],['Refactor']
Modifiability,[make] Make pytest-qob not change the user's hail config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12368:50,config,config,50,https://hail.is,https://github.com/hail-is/hail/pull/12368,1,['config'],['config']
Modifiability,[many] put shared make config in config.mk file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9339:23,config,config,23,https://hail.is,https://github.com/hail-is/hail/pull/9339,2,['config'],['config']
Modifiability,[memory] Use GCP project from GCPConfig not env variable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10961:48,variab,variable,48,https://hail.is,https://github.com/hail-is/hail/pull/10961,1,['variab'],['variable']
Modifiability,[nginx] ensure nginx configs dont overwrite each other in build.yaml,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10181:21,config,configs,21,https://hail.is,https://github.com/hail-is/hail/pull/10181,1,['config'],['configs']
Modifiability,[notebook] cleanup notebook_app too and refactor app config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9983:40,refactor,refactor,40,https://hail.is,https://github.com/hail-is/hail/pull/9983,2,"['config', 'refactor']","['config', 'refactor']"
Modifiability,[notebook] notebook also need global config now,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12429:37,config,config,37,https://hail.is,https://github.com/hail-is/hail/pull/12429,1,['config'],['config']
Modifiability,[notebook][hailtop][gear] move logging configuration out of gear,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9122:39,config,configuration,39,https://hail.is,https://github.com/hail-is/hail/pull/9122,1,['config'],['configuration']
Modifiability,[pipeline] Refactor backend code,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5962:11,Refactor,Refactor,11,https://hail.is,https://github.com/hail-is/hail/pull/5962,1,['Refactor'],['Refactor']
Modifiability,[qob] Update scala deploy config to use new base_path field,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14195:26,config,config,26,https://hail.is,https://github.com/hail-is/hail/pull/14195,1,['config'],['config']
Modifiability,"[query, wip] Some aggregator refactoring",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14289:29,refactor,refactoring,29,https://hail.is,https://github.com/hail-is/hail/pull/14289,1,['refactor'],['refactoring']
Modifiability,[query/ggplot] Adds configurable legend format to point geom,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12244:20,config,configurable,20,https://hail.is,https://github.com/hail-is/hail/pull/12244,1,['config'],['configurable']
Modifiability,[query/service] streamline & standardize configuration for query,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11745:41,config,configuration,41,https://hail.is,https://github.com/hail-is/hail/pull/11745,1,['config'],['configuration']
Modifiability,[query] Add JAVA_HOME configuration to Makefile,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9221:22,config,configuration,22,https://hail.is,https://github.com/hail-is/hail/pull/9221,1,['config'],['configuration']
Modifiability,[query] Add adaptive branching to LowerDistributedSort,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11811:12,adapt,adaptive,12,https://hail.is,https://github.com/hail-is/hail/pull/11811,1,['adapt'],['adaptive']
Modifiability,[query] Drastically simplify binding-based computation/rewrite code,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9247:55,rewrite,rewrite,55,https://hail.is,https://github.com/hail-is/hail/pull/9247,2,['rewrite'],['rewrite']
Modifiability,[query] Extend BindingEnv to include relational bindings,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9041:8,Extend,Extend,8,https://hail.is,https://github.com/hail-is/hail/pull/9041,1,['Extend'],['Extend']
Modifiability,[query] Extend `func_spec` to support functions with default arguments,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12814:8,Extend,Extend,8,https://hail.is,https://github.com/hail-is/hail/pull/12814,1,['Extend'],['Extend']
Modifiability,[query] Extend `hl.export_vcf` to work on Table,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8914:8,Extend,Extend,8,https://hail.is,https://github.com/hail-is/hail/pull/8914,1,['Extend'],['Extend']
Modifiability,[query] Fix TableAggregateByKey with extended physical key,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8729:37,extend,extended,37,https://hail.is,https://github.com/hail-is/hail/pull/8729,1,['extend'],['extended']
Modifiability,[query] Fix `intervals` option on read with extended key,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8708:44,extend,extended,44,https://hail.is,https://github.com/hail-is/hail/pull/8708,1,['extend'],['extended']
Modifiability,[query] Fix lowering of aggs to preserve free variables in init args,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12092:46,variab,variables,46,https://hail.is,https://github.com/hail-is/hail/pull/12092,1,['variab'],['variables']
Modifiability,[query] Make VEP on Dataproc not require a config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8929:43,config,config,43,https://hail.is,https://github.com/hail-is/hail/pull/8929,1,['config'],['config']
Modifiability,[query] Parameterize Hail Query Class Loading by ClassLoader,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11212:8,Parameteriz,Parameterize,8,https://hail.is,https://github.com/hail-is/hail/pull/11212,1,['Parameteriz'],['Parameterize']
Modifiability,[query] Refactor LoweringPipeline so that optimization is a pass,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9030:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/9030,1,['Refactor'],['Refactor']
Modifiability,[query] Refactor NDArray constructor interfaces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9960:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/9960,1,['Refactor'],['Refactor']
Modifiability,[query] Refactor PNDArray.construct to return a PCode,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9719:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/9719,1,['Refactor'],['Refactor']
Modifiability,[query] Refactor PStruct to avoid abstract methods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8528:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/8528,1,['Refactor'],['Refactor']
Modifiability,[query] Refactor `coerce` functions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12333:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/12333,1,['Refactor'],['Refactor']
Modifiability,[query] Refactor agg lowering to use new InitFromSerializedState node,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9024:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/9024,1,['Refactor'],['Refactor']
Modifiability,[query] Refactor and fix Backend's persist and unpersist,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12864:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/12864,1,['Refactor'],['Refactor']
Modifiability,[query] Refactor define_function to go through backend.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9287:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/9287,1,['Refactor'],['Refactor']
Modifiability,[query] Refactor function registry to not use overloading,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8570:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/8570,1,['Refactor'],['Refactor']
Modifiability,"[query] Refactor ggplot to use pandas, support alpha on histograms and bar charts",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11317:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/11317,1,['Refactor'],['Refactor']
Modifiability,[query] Refactor incomplete struct ordering to be distinct functionality,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8674:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/8674,1,['Refactor'],['Refactor']
Modifiability,[query] Refactor lowering to expose ExecuteContext,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8614:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/8614,1,['Refactor'],['Refactor']
Modifiability,[query] Refactor registerIEmitCode to use EmitCode instead of thunks,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9977:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/9977,1,['Refactor'],['Refactor']
Modifiability,[query] Refactor stream lengths to use code builders,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10446:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/10446,1,['Refactor'],['Refactor']
Modifiability,"[query] Refactor struct code constructor to take codes, not values",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9951:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/9951,1,['Refactor'],['Refactor']
Modifiability,[query] Refactor vds.sample_qc to use independent aggregators,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14297:8,Refactor,Refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/14297,1,['Refactor'],['Refactor']
Modifiability,[query] Rewrite ExportEntriesByCol to stage locally,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8613:8,Rewrite,Rewrite,8,https://hail.is,https://github.com/hail-is/hail/pull/8613,1,['Rewrite'],['Rewrite']
Modifiability,[query] Rewrite import_table type imputation in Python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9086:8,Rewrite,Rewrite,8,https://hail.is,https://github.com/hail-is/hail/pull/9086,1,['Rewrite'],['Rewrite']
Modifiability,[query] Rewrite ld prune to use java objects for filtering,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12076:8,Rewrite,Rewrite,8,https://hail.is,https://github.com/hail-is/hail/pull/12076,1,['Rewrite'],['Rewrite']
Modifiability,[query] Some refactoring of TableStage,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8917:13,refactor,refactoring,13,https://hail.is,https://github.com/hail-is/hail/pull/8917,1,['refactor'],['refactoring']
Modifiability,[query] correctly handle variables used in aggregator init ops,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14305:25,variab,variables,25,https://hail.is,https://github.com/hail-is/hail/issues/14305,1,['variab'],['variables']
Modifiability,[query] do not store function return values in variables,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13777:47,variab,variables,47,https://hail.is,https://github.com/hail-is/hail/pull/13777,1,['variab'],['variables']
Modifiability,[query] more refactoring towards removing SCode,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10797:13,refactor,refactoring,13,https://hail.is,https://github.com/hail-is/hail/pull/10797,1,['refactor'],['refactoring']
Modifiability,[query] parameterize field name of TableGroupWithinPartitions and add parser tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8607:8,parameteriz,parameterize,8,https://hail.is,https://github.com/hail-is/hail/pull/8607,1,['parameteriz'],['parameterize']
Modifiability,[query] parameterize test expr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11934:8,parameteriz,parameterize,8,https://hail.is,https://github.com/hail-is/hail/pull/11934,1,['parameteriz'],['parameterize']
Modifiability,[query] refactor BaseIR so children only exposes iterable interface,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13214:8,refactor,refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/13214,1,['refactor'],['refactor']
Modifiability,[query] refactor Code[_],MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8211:8,refactor,refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/8211,1,['refactor'],['refactor']
Modifiability,[query] refactor MakeRVDSpec to have intermediate with no partFiles,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8924:8,refactor,refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/8924,1,['refactor'],['refactor']
Modifiability,[query] refactor PCNDArray constructors,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10769:8,refactor,refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/10769,1,['refactor'],['refactor']
Modifiability,[query] refactor Stream to top-level class,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8547:8,refactor,refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/8547,1,['refactor'],['refactor']
Modifiability,[query] refactor TableStage.context for ease of use and better scoping,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8658:8,refactor,refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/8658,1,['refactor'],['refactor']
Modifiability,[query] refactor approx_cdf to support manual combining,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13935:8,refactor,refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/13935,1,['refactor'],['refactor']
Modifiability,[query] refactor block matrix persistence to take backend context,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9421:8,refactor,refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/9421,1,['refactor'],['refactor']
Modifiability,[query] refactor math in linear_regression_rows,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11070:8,refactor,refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/11070,1,['refactor'],['refactor']
Modifiability,"[query] refactor pca, add spectral moments estimators",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11045:8,refactor,refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/11045,1,['refactor'],['refactor']
Modifiability,[query] refactor primitive SValue -> Value accessor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11242:8,refactor,refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/11242,1,['refactor'],['refactor']
Modifiability,[query] refactor stream length tracking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8519:8,refactor,refactor,8,https://hail.is,https://github.com/hail-is/hail/pull/8519,1,['refactor'],['refactor']
Modifiability,[query] store job configuration in user bucket,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8937:18,config,configuration,18,https://hail.is,https://github.com/hail-is/hail/pull/8937,1,['config'],['configuration']
Modifiability,[query] use parameterization for test_blanczos_flags,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13694:12,parameteriz,parameterization,12,https://hail.is,https://github.com/hail-is/hail/pull/13694,1,['parameteriz'],['parameterization']
Modifiability,[query][aggs][ptypes] refactor extracted agg signatures,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8800:22,refactor,refactor,22,https://hail.is,https://github.com/hail-is/hail/pull/8800,1,['refactor'],['refactor']
Modifiability,[query][qggs] refactor PhysicalAggSig,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8996:14,refactor,refactor,14,https://hail.is,https://github.com/hail-is/hail/pull/8996,1,['refactor'],['refactor']
Modifiability,[query][smm 4] rewrite StreamFold and StreamFold2 to prepare for memory management,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9177:15,rewrite,rewrite,15,https://hail.is,https://github.com/hail-is/hail/pull/9177,1,['rewrite'],['rewrite']
Modifiability,[release] Fix EV syntax in release job for WHEEL variable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14436:49,variab,variable,49,https://hail.is,https://github.com/hail-is/hail/pull/14436,1,['variab'],['variable']
Modifiability,"[ruff](https://beta.ruff.rs/docs/) is a *very* fast linter that we can use as a drop-in replacement for flake8 and isort. Fast enough to run in a pre-commit hook. While there are many pylint rules implemented in ruff, it is not at parity with pylint yet. This PR replaces flake8 and isort in favor of ruff but does not remove pylint yet. Nevertheless, from what I have seen so far ruff + mypy does catch a vast swath of everyday errors, and I have found that the 30+ seconds it can take to run pylint on any of our python modules is a deterrent to me linting often and catching lint errors early. So I added Makefile targets such as `check-batch-fast` that run all the linters except for pylint. The `check-batch` rule now does `check-batch-fast` and `pylint-batch`. So linter coverage should have strictly increased in CI, but there is a <5s linting target now available for devs in addition to the >30s it can take to also run pylint. You can also now just run `ruff .` in the root of the repo and it completes for me in 0.293 seconds. For the most part in this PR, I added ruff, with the config enabling flake8 + isort + pylint rules, then disabled rules until there were no errors, save for a few rules that I thought to just fix immediately. These mostly line up with the flake8 rules we had already disabled. I also then added ruff's own ruleset ([RUF](https://beta.ruff.rs/docs/rules/#ruff-specific-rules-ruf)) particularly because I appreciated the `asyncio-dangling-task` and `unused-noqa` rules.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12967:1091,config,config,1091,https://hail.is,https://github.com/hail-is/hail/pull/12967,1,['config'],['config']
Modifiability,[services] Fix domain configuration in dev namespaces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14164:22,config,configuration,22,https://hail.is,https://github.com/hail-is/hail/pull/14164,1,['config'],['configuration']
Modifiability,[shuffle] Track bytes from initial write; adaptive branching all the …,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11850:42,adapt,adaptive,42,https://hail.is,https://github.com/hail-is/hail/pull/11850,1,['adapt'],['adaptive']
Modifiability,[shuffler] Refactor/simplify codegen interfaces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10421:11,Refactor,Refactor,11,https://hail.is,https://github.com/hail-is/hail/pull/10421,1,['Refactor'],['Refactor']
Modifiability,[terraform] Add test bucket to the terraform global config,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10807:52,config,config,52,https://hail.is,https://github.com/hail-is/hail/pull/10807,1,['config'],['config']
Modifiability,[test] FSSuite extends TestNGSuite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12243:15,extend,extends,15,https://hail.is,https://github.com/hail-is/hail/pull/12243,1,['extend'],['extends']
Modifiability,[tests] use generated config and real sha for tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5813:22,config,config,22,https://hail.is,https://github.com/hail-is/hail/pull/5813,1,['config'],['config']
Modifiability,[tls] Make ssl configs use a relative path,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10275:15,config,configs,15,https://hail.is,https://github.com/hail-is/hail/pull/10275,1,['config'],['configs']
Modifiability,[tls] make internal cert config optional,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13187:25,config,config,25,https://hail.is,https://github.com/hail-is/hail/pull/13187,1,['config'],['config']
Modifiability,"[uv](https://github.com/astral-sh/uv) is a new package resolver by the same folks who make `ruff`. It's boasted for being really fast, which honestly it is, but mostly it's appealing to me because they support generating lockfiles for alternative platforms and python versions than the system you run it on, which allows us to delete all this dockerizing `pip-compile` in order to generate lockfiles for linux. It's a really green project, so I'm open to pushback on incorporating it, but it seemed like a worthwhile simplification. I also quite like that it allows for additional strategies in generating lockfiles. By default, it behaves as would be expected, where it locks packages to the highest version within the acceptable bounds. But you can also configure it to generate the *lowest* acceptable pins, so we could actually verify whether the lower bounds that we have in our requirements files are actually acceptable or not.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14503:756,config,configure,756,https://hail.is,https://github.com/hail-is/hail/pull/14503,1,['config'],['configure']
Modifiability,[vcf-combiner] Refactor VCF combiner to support other GVCF schemas.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8942:15,Refactor,Refactor,15,https://hail.is,https://github.com/hail-is/hail/pull/8942,1,['Refactor'],['Refactor']
Modifiability,[vds/combiner] ensure variable is defined on all code paths,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10934:22,variab,variable,22,https://hail.is,https://github.com/hail-is/hail/pull/10934,1,['variab'],['variable']
Modifiability,_.py'; adding 'hailtop/config/deploy_config.py'; adding 'hailtop/config/user_config.py'; adding 'hailtop/fs/__init__.py'; adding 'hailtop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init__.py'; adding 'hailtop/hailctl/config/cli.py'; adding 'hailtop/hailctl/dataproc/__init__.py'; adding 'hailtop/hailctl/dataproc/cli.py'; adding 'hailtop/hailctl/dataproc/cluster_config.py'; adding 'hailtop/hailctl/dataproc/connect.py'; adding 'hailtop/hailctl/dataproc/deploy_metadata.py'; adding 'hailtop/hailctl/dataproc/diagnose.py'; adding 'hailtop/hailctl/dataproc/gcloud.py'; adding 'hailtop/hailctl/dataproc/modify.py'; adding 'hailtop/hailctl/dataproc/start.py'; adding 'hailtop/hailctl/dataproc/submit.py'; adding 'hailtop/hailctl/dataproc/utils.py'; adding 'hailtop/hailctl/dev/__init__.py'; adding 'hailtop/hailctl/dev/ci_client.py'; adding 'hailtop/hailctl/dev/cli.py'; adding 'hailtop/hailctl/dev/config.py'; adding 'hailtop/hailctl/hdinsight/__init__.py'; adding 'hailtop/hailctl/hdinsight/cli.py'; adding 'hailtop/hailctl/hdinsight/start.py'; adding 'hailtop/hailctl/hdinsight/submit.py'; adding 'hailtop/utils/__init__.py'; adding 'hailtop/utils/process.py'; adding 'hailtop/utils/rate_limiter.py'; adding ',MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445:11831,config,config,11831,https://hail.is,https://github.com/hail-is/hail/issues/13445,1,['config'],['config']
Modifiability,"_</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/8685c60dc0e82ee246fbe3d1aa272d1dfe57c24c""><code>8685c60</code></a> Bump mypy from 0.910 to 0.920 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/f0a0914345b224448220aeb00d71e6a04a5d24bd""><code>f0a0914</code></a> Bump twine from 3.7.0 to 3.7.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/e5c813173b8811b30f4a30eeba56fa8808ab15bb""><code>e5c8131</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/271"">#271</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c7e10db3d0965422122bef263fb36f6fd7572330""><code>c7e10db</code></a> Fix linter configs</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/6af8bf421a799208b27d57c6bf69de0d998fedec""><code>6af8bf4</code></a> Bump pre-commit from 2.15 to 2.16.0 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/269"">#269</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c71bbb5b5d7330f6dabfde7a1adec30a4611c0be""><code>c71bbb5</code></a> Bump docutils from 0.18 to 0.18.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/266"">#266</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/aio-libs/async-timeout/compare/v3.0.1...v4.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=async-timeout&package-manager=pip&previous-version=3.0.1&new-version=4.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:5648,config,configs,5648,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['config'],['configs']
Modifiability,"_From @cseed on August 26, 2015 14:0_. This might involve patching the Gradle Jacoco plugin. _Copied from original issue: cseed/hail#4_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5:85,plugin,plugin,85,https://hail.is,https://github.com/hail-is/hail/issues/5,1,['plugin'],['plugin']
Modifiability,"_From @jbloom22 on September 29, 2015 17:21_. Once we handle multi-allelic sites, we will need to adapt mendel errors so that, for example, it does not double count errors in multi-allelic trios. _Copied from original issue: cseed/hail#65_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/45:98,adapt,adapt,98,https://hail.is,https://github.com/hail-is/hail/issues/45,1,['adapt'],['adapt']
Modifiability,"___/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 3.5.2 (default, Jul 12 2017 14:00:23); SparkSession available as 'spark'. In [1]: from hail import *; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-1-3181c5d8fca5> in <module>(); ----> 1 from hail import *. /opt/Software/hail/python/hail/__init__.py in <module>(); ----> 1 import hail.expr; 2 from hail.representation import *; 3 from hail.context import HailContext; 4 from hail.dataset import VariantDataset; 5 from hail.expr import *. /opt/Software/hail/python/hail/expr.py in <module>(); 1 import abc; 2 from hail.java import scala_object, Env, jset; ----> 3 from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call; 4 ; 5 . /opt/Software/hail/python/hail/representation/__init__.py in <module>(); ----> 1 from hail.representation.variant import Variant, Locus, AltAllele; 2 from hail.representation.interval import Interval; 3 from hail.representation.genotype import Genotype, Call; 4 from hail.representation.annotations import Struct; 5 from hail.representation.pedigree import Trio, Pedigree. /opt/Software/hail/python/hail/representation/variant.py in <module>(); 1 from hail.java import scala_object, Env, handle_py4j; ----> 2 from hail.typecheck import *; 3 ; 4 class Variant(object):; 5 """""". /opt/Software/hail/python/hail/typecheck/__init__.py in <module>(); ----> 1 from check import *; 2 ; 3 __all__ = ['typecheck',; 4 'typecheck_method',; 5 'none',. ImportError: No module named 'check'. In [2]: hc = HailContext(sc); ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-2-2e980fcce31d> in <module>(); ----> 1 hc = HailContext(sc). NameError: name 'HailContext' is not defined. In [3]: ; ```; There are still some errors, is there something wrong with my configurations?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609:3980,config,configurations,3980,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609,1,['config'],['configurations']
Modifiability,"__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2106,config,config,2106,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['config'],['config']
Modifiability,__m92846DECODE_o_dict_of_o_struct_of_o_binaryANDo_int32END_TO_SIndexablePointer(Unknown Source); E 	at __C92844etypeDecode.apply(Unknown Source); E 	at is.hail.io.CompiledDecoder.readRegionValue(Decoder.scala:31); E 	at is.hail.io.AbstractTypedCodecSpec.decodeArrays(CodecSpec.scala:57); E 	at is.hail.io.AbstractTypedCodecSpec.decodeArrays$(CodecSpec.scala:54); E 	at is.hail.io.TypedCodecSpec.decodeArrays(TypedCodecSpec.scala:19); E 	at is.hail.expr.ir.Interpret$.$anonfun$run$1(Interpret.scala:80); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:79); E 	at is.hail.expr.ir.Interpret$.interpret$1(Interpret.scala:67); E 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:110); E 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:58); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$foldConstants$1(FoldConstants.scala:47); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.FoldConstants$.foldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:6204,Rewrite,RewriteBottomUp,6204,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Rewrite'],['RewriteBottomUp']
Modifiability,_main__.py'; adding 'hailtop/config/__init__.py'; adding 'hailtop/config/deploy_config.py'; adding 'hailtop/config/user_config.py'; adding 'hailtop/fs/__init__.py'; adding 'hailtop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init__.py'; adding 'hailtop/hailctl/config/cli.py'; adding 'hailtop/hailctl/dataproc/__init__.py'; adding 'hailtop/hailctl/dataproc/cli.py'; adding 'hailtop/hailctl/dataproc/cluster_config.py'; adding 'hailtop/hailctl/dataproc/connect.py'; adding 'hailtop/hailctl/dataproc/deploy_metadata.py'; adding 'hailtop/hailctl/dataproc/diagnose.py'; adding 'hailtop/hailctl/dataproc/gcloud.py'; adding 'hailtop/hailctl/dataproc/modify.py'; adding 'hailtop/hailctl/dataproc/start.py'; adding 'hailtop/hailctl/dataproc/submit.py'; adding 'hailtop/hailctl/dataproc/utils.py'; adding 'hailtop/hailctl/dev/__init__.py'; adding 'hailtop/hailctl/dev/ci_client.py'; adding 'hailtop/hailctl/dev/cli.py'; adding 'hailtop/hailctl/dev/config.py'; adding 'hailtop/hailctl/hdinsight/__init__.py'; adding 'hailtop/hailctl/hdinsight/cli.py'; adding 'hailtop/hailctl/hdinsight/start.py'; adding 'hailtop/hailctl/hdinsight/submit.py'; adding 'hailtop/utils/__init__.py'; adding 'hailtop/utils/process.py'; adding,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445:11786,config,config,11786,https://hail.is,https://github.com/hail-is/hail/issues/13445,1,['config'],['config']
Modifiability,"_required=False); ```. ---. ### What happened?. https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/checkpoint.20with.20missing.20fields. ```; is.hail.utils.HailException: gs://jn-vcf-cleanup-central1/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626.vcf.gz:offset 1344376382: error while parsing line; chr1	10403	.	ACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	A,ACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	.	LowQual	AC=1,1;AF=0.250,0.250;AN=4;AS_QUALapprox=0|23|45;AS_VQSLOD=.,.;AS_YNG=.,.;QUALapprox=45	GT:AD:GQ:RGQ	./.	0/1:23,7,0:20:23	./.	./.	./.	0/2:6,0,4:35:45	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:21); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:21); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1934); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1922); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C2005collect_distributed_array_matrix_native_writer.apply_region1_27(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:52); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:51); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:751); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13346:1134,adapt,adapted,1134,https://hail.is,https://github.com/hail-is/hail/issues/13346,1,['adapt'],['adapted']
Modifiability,"`. activate NAME` might silently fail if `NAME` does not exist or `conda` is not configured. `. ./loadconda` tries to find conda in a variety of places and configure it (meaning source `conda.sh`). After this, `conda activate NAME` will work correctly. ---. This is already in my batch dag PR, but that's getting bogged down in test issues, and this is blocking @akotlar 's https://github.com/hail-is/hail/pull/5065 PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5066:81,config,configured,81,https://hail.is,https://github.com/hail-is/hail/pull/5066,2,['config'],"['configure', 'configured']"
Modifiability,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8359:53,inherit,inheritance,53,https://hail.is,https://github.com/hail-is/hail/issues/8359,1,['inherit'],['inheritance']
Modifiability,"`OnDiskBTreeIndexToValue` can index the elements at the base of a BTree. The BGen BTree is a BTree on the byte-offsets of variants in the BGen file. In a following PR, I will use this class to filter the BGen file to a subset of variants, specified by their index. This kind of filtering happens at the level of bytes, it permits me to avoid decoding/decompressing any variants I don't need. Ideally, the index would also include the variant keys themselves. That would be a really nice extension of this work and would enable a more natural file-level filtering user experience. Aside: `IndexBTree` could use some TLC. I'm sort of making the minimal changes to get Caitlin cooking. Last night, I slipped into a rewrite and it's just too big a task to get right before Friday because I don't have any documentation of precisely what our btree file format is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3794:712,rewrite,rewrite,712,https://hail.is,https://github.com/hail-is/hail/pull/3794,1,['rewrite'],['rewrite']
Modifiability,"`Table._select` got way too complicated (mostly my fault) when key changing moved from `TableMapRows` to `TableKeyBy`. Making `_select` a simple wrapper around `TableMapRows`, and moving the key logic to `key_by`, made both way simpler. I then realized the `key_by` code could be even simpler by adding some rules to the optimizer to clean up the case where all new keys are existing fields. I actually think some things had gotten broken in the old `_select` (performance wise). In particular, in `split_multi`, in the `split_rows` function with `rekey=false`, I think it's supposed to extend the key from `['locus']` to `['locus', 'alleles']`, but that wasn't happening. I changed `key_by` to no longer accept `key_by(None)` or `key_by([])`, both of which should now be `key_by()`, which is more consistent with the rest of our interface, but is a breaking change. Is it worth the disruption? Should I add a warning? Or just continue to accept both?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4455:587,extend,extend,587,https://hail.is,https://github.com/hail-is/hail/pull/4455,1,['extend'],['extend']
Modifiability,"``; the dataproc cluster command would be provided the following environment variable through the `--metadata` flag: `VEP_REPLICATE=aus-sydney`. This variable is used within the script `gs://hail-common/hailctl/dataproc/0.2.115/vep-GRCh38.sh` to determine which bucket to pull the VEP cache data from. In more recent versions (tested with 0.2.130), this `VEP_REPLICATE` variable has been changed to `VEP_REPLICATE=australia-southeast1`, however the Australian bucket containing the VEP cache data is still `aus-sydney`, meaning that the VEP data is not copied into the dataproc cluster, and when trying to run VEP I get the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:1233,plugin,plugin,1233,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,"['Plugin', 'plugin']","['Plugins', 'plugin']"
Modifiability,"```; (py311) jigold@wm349-8c4 hail % gcloud artifacts repositories set-cleanup-policies hail \; --project=hail-vdc \; --location=us \; --policy=/Users/jigold/projects/hail/infra/gcp-broad/gcp-ar-cleanup-policy.txt \; --no-dry-run; WARNING: Python 3.5-3.7 will be deprecated on August 8th, 2023. Please use Python version 3.8 and up. If you have a compatible Python interpreter installed, you can use it by setting; the CLOUDSDK_PYTHON environment variable to point to it. Updated repository [hail].; Dry run is disabled.; ```. I checked the UI and it seems correct now.; <img width=""420"" alt=""Screenshot 2023-11-15 at 7 50 06 AM"" src=""https://github.com/hail-is/hail/assets/1693348/630f0481-c24a-4e41-9350-ef091cc62b1b"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14001#issuecomment-1812482207:447,variab,variable,447,https://hail.is,https://github.com/hail-is/hail/pull/14001#issuecomment-1812482207,1,['variab'],['variable']
Modifiability,"```; * installing *source* package ‘ncdf4’ ...; ** package ‘ncdf4’ successfully unpacked and MD5 sums checked; configure.ac: starting; checking for nc-config... no; -----------------------------------------------------------------------------------; Error, nc-config not found or not executable. This is a script that comes with the; netcdf library, version 4.1-beta2 or later, and must be present for configuration; to succeed. If you installed the netcdf library (and nc-config) in a standard location, nc-config; should be found automatically. Otherwise, you can specify the full path and name of; the nc-config script by passing the --with-nc-config=/full/path/nc-config argument; flag to the configure script. For example:. ./configure --with-nc-config=/sw/dist/netcdf4/bin/nc-config. Special note for R users:; -------------------------; To pass the configure flag to R, use something like this:. R CMD INSTALL --configure-args=""--with-nc-config=/home/joe/bin/nc-config"" ncdf4. where you should replace /home/joe/bin etc. with the location where you have; installed the nc-config script that came with the netcdf 4 distribution.; -----------------------------------------------------------------------------------; ERROR: configuration failed for package ‘ncdf4’; * removing ‘/usr/local/lib/R/3.3/site-library/ncdf4’; ERROR: dependency ‘ncdf4’ is not available for package ‘GWASTools’; * removing ‘/usr/local/lib/R/3.3/site-library/GWASTools’; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3273#issuecomment-377701057:111,config,configure,111,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377701057,19,['config'],"['config', 'configuration', 'configure', 'configure-args']"
Modifiability,"```; + CHANGED=yes; + [[ -e notebook/get-deployed-sha.sh ]]; + [[ yes != no ]]; + cd notebook; + /bin/bash hail-ci-deploy.sh; cat: notebook-image: No such file or directory; sed -e ""s,@sha@,17a365c57d0f,"" \; -e ""s,@image@,,"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""notebook"", Namespace: ""batch-pods""; Object: &{map[""metadata"":map[""labels"":map[""app"":""notebook""] ""name"":""notebook"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""port"":'P' ""protocol"":""TCP"" ""targetPort"":'\u1388']] ""selector"":map[""app"":""notebook""]] ""apiVersion"":""v1"" ""kind"":""Service""]}; from server for: ""deployment.yaml"": services ""notebook"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:4: recipe for target 'deploy' failed; make: *** [deploy] Error 1; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4656:363,config,configuration,363,https://hail.is,https://github.com/hail-is/hail/issues/4656,2,['config'],['configuration']
Modifiability,```; + cd /io; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; ++ mktemp -d; + dir=/tmp/tmp.Txkg8yv5oW; + git clone https://github.com/hail-is/hail.git /tmp/tmp.Txkg8yv5oW; Cloning into '/tmp/tmp.Txkg8yv5oW'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: protocol error: bad pack header; ++ ls -A /tmp/tmp.Txkg8yv5oW. real	1m0.562s; user	0m0.135s; sys	0m0.086s; + git config user.email ci@hail.is; fatal: not in a git directory; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8170:496,config,config,496,https://hail.is,https://github.com/hail-is/hail/issues/8170,1,['config'],['config']
Modifiability,```; + date; Mon Mar 30 22:11:05 UTC 2020; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; + set -e; ++ mktemp -d; + dir=/tmp/tmp.3H7wTmq0R2; + git clone https://github.com/danking/hail.git /tmp/tmp.3H7wTmq0R2; Cloning into '/tmp/tmp.3H7wTmq0R2'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: early EOF; fatal: index-pack failed; ++ ls -A /tmp/tmp.3H7wTmq0R2. real	0m3.373s; user	0m0.006s; sys	0m0.025s; + git config user.email ci@hail.is; fatal: not in a git directory; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8397:538,config,config,538,https://hail.is,https://github.com/hail-is/hail/issues/8397,1,['config'],['config']
Modifiability,"```; + make -k check-services; PYTHONPATH=""hail/python:auth:batch:ci:memory:notebook:monitoring:website:gear:web_common"" python3 -m flake8 --config setup.cfg auth; auth/auth/auth.py:515:86: W291 trailing whitespace; make: *** [Makefile:42: check-auth] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12889#issuecomment-1515270732:141,config,config,141,https://hail.is,https://github.com/hail-is/hail/pull/12889#issuecomment-1515270732,1,['config'],['config']
Modifiability,```; TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E reactor.core.Exceptions$ReactiveException: java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.Exceptions.propagate(Exceptions.java:392); E 	at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:97); E 	at reactor.core.publisher.Flux.blockLast(Flux.java:2519); E 	at com.azure.core.util.paging.ContinuablePagedByIteratorBase.requestPage(ContinuablePagedByIteratorBase.java:94); E 	at com.azure.core.util.paging.ContinuablePagedByItemIterable$ContinuablePagedByItemIterator.<init>(ContinuablePagedByItemIterable.java:50); E 	at com.azure.core.util.paging.ContinuablePagedByItemIterable.iterator(ContinuablePagedByItemIterable.java:37); E 	at com.azure.core.util.paging.ContinuablePagedIterable.iterator(ContinuablePagedIterable.java:106); E 	at java.lang.Iterable.forEach(Iterable.java:74); E 	at is.hail.io.fs.AzureStorageFS.delete(AzureStorageFS.scala:203); E 	at is.hail.backend.OwningTempFileManager.$anonfun$cleanup$1(ExecuteContext.scala:27); E 	at is.hail.backend.OwningTempFileManager.$anonfun$cleanup$1$adapted(ExecuteContext.scala:26); E 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); E 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); E 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); E 	at is.hail.backend.OwningTempFileManager.cleanup(ExecuteContext.scala:26); E 	at is.hail.backend.ExecuteContext.close(ExecuteContext.scala:148); E 	at is.hail.utils.package$.using(package.scala:660); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.Ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:120,config,configured,120,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,2,['config'],['configured']
Modifiability,"```; Traceback (most recent call last):; File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/sandbox.py"", line 17, in <module>; pprint(hc.read('%s/variantqc/exacv2_rf.vds' % root, sites_only=True).filter_variants_intervals('gs://exac2/temp').head()); File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/utils.py"", line 201, in head; return json.loads(self.variants_keytable().to_dataframe().toJSON().first()); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1328, in first; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1310, in take; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py"", line 933, in runJob; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 20022, exac-sw-3pdd.c.broad-mpg-gnomad.internal): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.sql.Row; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1311:89,sandbox,sandbox,89,https://hail.is,https://github.com/hail-is/hail/issues/1311,1,['sandbox'],['sandbox']
Modifiability,"```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8201:435,config,config,435,https://hail.is,https://github.com/hail-is/hail/issues/8201,1,['config'],['config']
Modifiability,"```; io/test/test_batch.py::Test::test_batch_create_validation FAILED; ______________________ Test.test_batch_create_validation _______________________. self = <test.test_batch.Test testMethod=test_batch_create_validation>. def test_batch_create_validation(self):; bad_configs = [; # unexpected field fleep; {'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz', 'fleep': 'quam'},; # billing project None/missing; {'billing_project': None, 'n_jobs': 5, 'token': 'baz'},; {'n_jobs': 5, 'token': 'baz'},; # n_jobs None/missing; {'billing_project': 'foo', 'n_jobs': None, 'token': 'baz'},; {'billing_project': 'foo', 'token': 'baz'},; # n_jobs wrong type; {'billing_project': 'foo', 'n_jobs': '5', 'token': 'baz'},; # token None/missing; {'billing_project': 'foo', 'n_jobs': 5, 'token': None},; {'billing_project': 'foo', 'n_jobs': 5},; # attribute key/value None; {'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; {'attributes': {'k': None}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; ]; url = deploy_config.url('batch', '/api/v1alpha/batches/create'); headers = service_auth_headers(deploy_config, 'batch'); for config in bad_configs:; r = requests.post(url, json=config, allow_redirects=True, headers=headers); > assert r.status_code == 400, (config, r); E AssertionError: ({'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'}, <Response [403]>); E assert 403 == 400; E -403; E +400. io/test/test_batch.py:487: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099:1161,config,config,1161,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099,3,['config'],['config']
Modifiability,"```; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=namespaces"", GroupVersionKind: ""/v1, Kind=Namespace""; Name: ""batch-pods"", Namespace: """"; Object: &{map[""apiVersion"":""v1"" ""kind"":""Namespace"" ""metadata"":map[""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""name"":""batch-pods"" ""namespace"":""""]]}; from server for: ""deployment.yaml"": namespaces ""batch-pods"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get namespaces in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=serviceaccounts"", GroupVersionKind: ""/v1, Kind=ServiceAccount""; Name: ""batch-svc"", Namespace: ""batch-pods""; Object: &{map[""kind"":""ServiceAccount"" ""metadata"":map[""name"":""batch-svc"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""apiVersion"":""v1""]}; from server for: ""deployment.yaml"": serviceaccounts ""batch-svc"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get serviceaccounts in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=roles"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=Role""; Name: ""batch-pods-admin"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""Role"" ""metadata"":map[""name"":""batch-pods-admin"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""rules"":[map[""apiGroups"":[""""] ""resources"":[""pods""] ""verbs"":[""get"" ""list"" ""watch"" ""create"" ""update"" ""patch"" ""delete""]] map[""apiGroups"":[""""] ""resources"":[""pods/log""] ""verbs"":[""get""]]]]}; from server for: ""deployment.yaml"": roles.rbac.author",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:100,config,configuration,100,https://hail.is,https://github.com/hail-is/hail/issues/4609,3,['config'],['configuration']
Modifiability,"```python; >>> hl.eval(hl.min_rep(hl.locus('1', 10000), ['G', hl.null(hl.tstr)])); ```; I don't have any problem with this erroring, but it's mode should be more user friendly. Either that or we allow `NA` in `min_rep` (just return the NA) in the `alleles` array and don't use it for the purposes of actually `min_rep`ping.; ```; java.lang.NullPointerException: null ; at is.hail.codegen.generated.C172.method_2(Unknown Source) ; at is.hail.codegen.generated.C172.method_1(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:711) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:690) ; at is.hail.utils.package$.using(package.scala:596); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:690); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:91); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:61); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:30); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:8); at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:7); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:8); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:7); at is.hail.utils.package$.using(package.scala:596); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6889:1253,Rewrite,RewriteBottomUp,1253,https://hail.is,https://github.com/hail-is/hail/issues/6889,6,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,"`aiohttp.ClientOSError` inherits from `OSError`, so we can just use `errno` or `strerror` directly. We should not directly use the `args` because one of the subclasses of `ClientOSError` sets them to *its* arguments after initializing its super classes with the expected arguments:. ```python3; class ClientConnectorError(ClientOSError):; """"""Client connector error. Raised in :class:`aiohttp.connector.TCPConnector` if; a connection can not be established.; """""". def __init__(self, connection_key: ConnectionKey, os_error: OSError) -> None:; self._conn_key = connection_key; self._os_error = os_error; super().__init__(os_error.errno, os_error.strerror); self.args = (connection_key, os_error); ```. I also tried to remove `e.args` from the `ClientPayloadError` case (the one right above this, and the only one still using `e.args`), but neither that class nor any super class sets a field with the error message (in fact, no fields are ever set so we can only use `e.args`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13921:24,inherit,inherits,24,https://hail.is,https://github.com/hail-is/hail/pull/13921,1,['inherit'],['inherits']
Modifiability,"`asyncinit` is unused AFAICT and the `frozenlist` requirement is already inherited from hailtop (though it is not used in `hailtop` only in query code, so am also happy to move it out of hailtop fully and into query).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13988:73,inherit,inherited,73,https://hail.is,https://github.com/hail-is/hail/pull/13988,1,['inherit'],['inherited']
Modifiability,"`cond` to `cond2`, such that `cond & (intervals.contains(key))` is equivalent to `cond2 & intervals.contains(key)` (in other words `cond` implies `cond2`, and `cond2 & intervals.contains(key)` implies `cond`). This means it is safe to replace the `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond2)`. A common example is when `cond` can be completely captured by the interval filter, i.e. `cond` is equivant to `intervals.contains(key)`, in which case we can take `cond2 = True`, and the `TableFilter` can be optimized away. This all happens in the function; ```scala; def extractPartitionFilters(ctx: ExecuteContext, cond: IR, ref: Ref, key: IndexedSeq[String]): Option[(IR, IndexedSeq[Interval])] = {; if (key.isEmpty) None; else {; val extract = new ExtractIntervalFilters(ctx, ref.typ.asInstanceOf[TStruct].typeAfterSelectNames(key)); val trueSet = extract.analyze(cond, ref.name); if (trueSet == extract.KeySetLattice.top); None; else {; val rw = extract.Rewrites(mutable.Set.empty, mutable.Set.empty); extract.analyze(cond, ref.name, Some(rw), trueSet); Some((extract.rewrite(cond, rw), trueSet)); }; }; }; ```; `trueSet` is the set of intervals which contains all rows where `cond` is true. This set is passed back into `analyze` in a second pass, which asks it to rewrite `cond` to something equivalent, under the assumption that all keys are contained in `trueSet`. The abstraction of runtime values tracks two types of information:; * Is this value a reference to / copy of one of the key fields of this row? We need to know this to be able to recognize comparisons with key values, which we want to extract to interval filters.; * For boolean values (including, ultimately, the filter predicate itself), we track three sets of intervals of the key type: overapproximations of when the bool is true, false, and missing. Overapproximation here means, for example, if the boolean evaluates to true in some row with key `k`, then `k` must be contained in the ""tr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13355:2221,Rewrite,Rewrites,2221,https://hail.is,https://github.com/hail-is/hail/pull/13355,1,['Rewrite'],['Rewrites']
Modifiability,`config['domain']` is actually never None in `from_config` so I moved the fallback to the config.ini value of domain to the `from_config_file` function that currently hard-codes `hail.is` when there's no deploy config.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11131:1,config,config,1,https://hail.is,https://github.com/hail-is/hail/pull/11131,3,['config'],['config']
Modifiability,`f` is a thunk so it is currently being evaluated thrice before inserted into the code cache. The `compiledFunction` variable was unused so I think this is what was originally intended.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13796:117,variab,variable,117,https://hail.is,https://github.com/hail-is/hail/pull/13796,1,['variab'],['variable']
Modifiability,`hailctl dataproc connect` and `hailctl dataproc modify` hard-code a default compute zone of us-central1-b. This changes those two commands to use the `compute/zone` value from the user's gcloud configuration if a zone argument is not provided. @johnc1231 [mentioned this in Zulip](https://hail.zulipchat.com/#narrow/stream/128581-Cloud-support/topic/Unable.20to.20launch.20notebook) the other day.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8790:195,config,configuration,195,https://hail.is,https://github.com/hail-is/hail/pull/8790,1,['config'],['configuration']
Modifiability,"`hailctl dataproc` can pass through gcloud configurations specified with `--configuration` to the `gcloud dataproc` commands that it runs. However, that argument is often not respected for other `gcloud` commands that `hailctl dataproc` may run. For example, `hailctl dataproc start --configuration some_config` gets the project ID for requester pays configuration from the default gcloud configuration, not the specified configuration. https://github.com/hail-is/hail/blob/9706dd493515bce5aa88c623a83f899e8f8b801b/hail/python/hailtop/hailctl/dataproc/start.py#L262-L265. Hail version: 0.2.57-3f3afaa1d7bd. Zulip chat for context; https://hail.zulipchat.com/#narrow/stream/128581-Cloud-support/topic/403.20on.20public.20file",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9587:43,config,configurations,43,https://hail.is,https://github.com/hail-is/hail/issues/9587,6,['config'],"['configuration', 'configurations']"
Modifiability,`hailctl dev config show` is currently inconsistent with the verbiage used; to change those variables. This change ensures `hailctl dev config set k v`; is displayed as `k: v`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10240:13,config,config,13,https://hail.is,https://github.com/hail-is/hail/pull/10240,3,"['config', 'variab']","['config', 'variables']"
Modifiability,"`hailtop.batch.ServiceBackend` uses `get_user_config().get` to read the `HAIL_BATCH_REGIONS` environment variable, when it should use `configuration_of`. This change fixes that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13239:105,variab,variable,105,https://hail.is,https://github.com/hail-is/hail/pull/13239,1,['variab'],['variable']
Modifiability,"`hb.Batch` now supports `default_regions` which completes the natural hierarchy of: config, envvar, backend, batch, job. I went a little hog wild with examples. I think we should have more examples everywhere!. The ServiceBackend doc page also had several basic formatting issues which I addressed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14224:84,config,config,84,https://hail.is,https://github.com/hail-is/hail/pull/14224,1,['config'],['config']
Modifiability,`hl.allele_type` could also return missing for invalid input. But I don't want to refactor all the scala if we're just going to remove it,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3491#issuecomment-386427843:82,refactor,refactor,82,https://hail.is,https://github.com/hail-is/hail/pull/3491#issuecomment-386427843,1,['refactor'],['refactor']
Modifiability,"`hl.balding_nichols_model` generates a MatrixTable representing a genetic dataset randomly drawn according to the Balding Nichols model.; ```; In [5]: hl.balding_nichols_model(2,3,3).show() ; 2019-08-15 10:38:05 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 3 samples, and 3 variants...; +---------------+------------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT |; +---------------+------------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call |; +---------------+------------+------+------+------+; | 1:1 | [""A"",""C""] | 0/0 | 0/0 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 1/1 | 1/1 |; | 1:3 | [""A"",""C""] | 1/1 | 0/1 | 0/0 |; +---------------+------------+------+------+------+. ```; These MatrixTables are useful both as examples and test datasets for genetics-related Hail code. Unfortunately, the loci are chosen sequentially starting with chromosome 1, position 1. This region of chromosome 1 is in the telomere. Many genetic annotations contain no information in this region. As a result, `hl.balding_nichols_model` is not useful when demonstrating the annotation database or third-party genetic annotations. We want to enhance `hl.balding_nichols_model` to select variants (loci-allele-array pairs) that are likely to appear in real genetic datasets. One very simple model would be to draw variants according to their alternate/minor allele frequency in the gnomAD or 1000 Genomes datasets. An additional improvement would be to generate chromosomes roughly proportionally to their true sizes. These changes should not significantly slow down the method. We may want to include a small dataset of allele frequencies with Hail for use when the requested number of variants is small, only loading the full gnomAD or 1000 Genomes allele frequencies when the requested number of variants is in the millions or tens of millions. This functionality should be enabled and disabled by a parameter to `hl.balding_nichols_model`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6880:1187,enhance,enhance,1187,https://hail.is,https://github.com/hail-is/hail/issues/6880,1,['enhance'],['enhance']
Modifiability,"`hl.foo.bar.baz` looks at the __init__files, which work differently from imports. I think you could do `from hail.experimental.import_gtf import _load_gencode_gtf`. One of the tasks for 0.3 is a refactor that prevents everything from being imported and exposed as `hl.blah.blah.blah`, which leads to super long import times.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8264#issuecomment-597047221:195,refactor,refactor,195,https://hail.is,https://github.com/hail-is/hail/pull/8264#issuecomment-597047221,1,['refactor'],['refactor']
Modifiability,"`network=private` is an escape hatch for CI so certain jobs can talk to internal endpoints on our network that we do not permit user jobs to reach. In `main`, all CI jobs are hard-coded to use `private` in `build.py`, but few jobs in the CI pipeline actually require these heightened privileges. The steps in the CI pipeline are of the following types:. - `BuildImageStep`: These do not need to use the private network and have now all been made to use the public network namespaces; - `CreateDatabaseStep`: These *do* need to use `network='private'` because all our DBs only have private IPs on our internal network; - `RunImageStep`: Those steps that contact the DB need the private network. This PR makes the network configurable for these steps but default to public, so steps that need DB access explicitly do `network: private`; - `DeployStep`: These do not need to use the private network because they use the public K8s API server endpoint. Whether they should is perhaps a different question. I'm open to keeping these on the private networks and creating an issue to use the internal API server endpoint instead. We definitely have a static internal IP in GKE but I don't believe we have one for AKS and that would involve some research.; - `CreateNamespaceStep`: I don't believe that this needs the private network because it is functionally the same as a `DeployStep` in that it just talks to K8s, but I am unable to test this step in `test_ci` so I am reluctant to make a change that could brick CI. I instead made it configurable and default to its current value ('private'). We could then make a follow-up PR that tries turning it public.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14294:720,config,configurable,720,https://hail.is,https://github.com/hail-is/hail/pull/14294,2,['config'],['configurable']
Modifiability,`orjson` 3.9.15 fixed the rare segfault that we saw in `3.9.11`. Besides just updating to latest patch and minor versions:. - Removed a redundant requirement of `orjson` in `gear/requirements.txt` -- it inherits `orjson` from hailtop; - Bokeh `3.4` made a breaking change w.r.t. the `circle` method on figures. I have restricted the bounds for `bokeh` to avoid this breaking change but will follow up with a PR that changes our usage of bokeh to follow the deprecation/upgrade advice and undo the bounds restriction,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14471:203,inherit,inherits,203,https://hail.is,https://github.com/hail-is/hail/pull/14471,1,['inherit'],['inherits']
Modifiability,"`pyright` is an alternative to `mypy` for type checking python. It's also a pretty solid language server. While they both adhere to the PEPs for type checking, they [have a few differences](https://github.com/microsoft/pyright/blob/main/docs/mypy-comparison.md#differences-between-pyright-and-mypy). That doc is worth a read, but the parts that I found most compelling are pyright's return-type inference that mypy doesn't do and that pyright differentiates between an `Unknown` type and an `Any` type. `mypy`, for the most part, treats variables as `Any` if it can't figure out what type they are, and as a result lets a lot of behaviors on `Any` variables slide. `pyright` tends to be much stricter (even when not on strict mode!) and while some of the assertions that I had to make here to appease `pyright` are a bit noisy, it's also pointed out a lot of areas where we can be stricter with our typing and then clean up code because some assertions are caught instead by the type system. It also did a good job informing of bad import practices like how `hl.utils` was not actually exported by the `hail` module but incidentally imported by some file in the repo.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13437:537,variab,variables,537,https://hail.is,https://github.com/hail-is/hail/pull/13437,2,['variab'],['variables']
Modifiability,"`rich` stacktraces will by default show the local variables at each level of scope, which is nice for debugging but can leak things like tokens. Best not to have that show up in logs or accidentally pasted into zulip",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13212:50,variab,variables,50,https://hail.is,https://github.com/hail-is/hail/pull/13212,1,['variab'],['variables']
Modifiability,`server.py` contains global variables that we really ought not to evaluate when running tests. This moves the minimal set of things out of `server.py` so that the tests do not evaluate `server.py`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5795:28,variab,variables,28,https://hail.is,https://github.com/hail-is/hail/pull/5795,1,['variab'],['variables']
Modifiability,"`tls.py` has many different functions with long names. This change reduces it to three functions:. - internal_server_ssl_context; - internal_client_ssl_context; - external_client_ssl_context. I also added `httpx.py` which contains the HTTPS-related functions that `tls.py` previously; contained. I also simplified the HTTPS-related functions to just:. - client_session; - blocking_client_session. I determine internal vs. external using the deploy config. ---. An [`ssl.SSLContext`](https://docs.python.org/3/library/ssl.html#ssl.SSLContext) defines how a; network library (such as `aiohttp`) should perform SSL/TLS. Let's look at an example:. ```python3; server_ssl_context = ssl.create_default_context(; purpose=Purpose.CLIENT_AUTH,; cafile='/incoming.cacerts'); server_ssl_context.load_cert_chain(ssl_config['cert'],; keyfile=ssl_config['key'],; password=None); server_ssl_context.verify_mode = ssl.CERT_OPTIONAL; server_ssl_context.check_hostname = False; ```. The first function call states that we are a *server* performing *client; authentication* (`Purpose.CLIENT_AUTH`). We also state that anyone who sends requests to us will be; identified by a certificate that is trusted by our certificate database: `/incoming.cacerts` (which; is a file). `load_cert_chain` states where to find the certificates and secret key that prove who we are. The; certificate and secret key together are like a property title that proves someone owns a house. The; `password=None` means that our secret key has no password. Some keys are themselves locked by a; password. `verify_mode` means what do we expect our clients to have. `CERT_OPTIONAL` means anonymous clients; are OK. This is how servers normally operate (https://google.com does not care who you are). `check_hostname` means should we verify that the client certificate matches the client's; hostname. Since we allow anonymous clients, this must be `False`. ---. `test-address.py` is a gross hack. It will disappear in subsequent PRs. For now, I push",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9862:448,config,config,448,https://hail.is,https://github.com/hail-is/hail/pull/9862,1,['config'],['config']
Modifiability,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6248:174,plugin,plugins,174,https://hail.is,https://github.com/hail-is/hail/pull/6248,1,['plugin'],['plugins']
Modifiability,"a host name when no Subject Alternative Names are present is now disabled by default. It can be temporarily re-enabled by adding the value x509ignoreCN=0 to the GODEBUG environment variable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/93264"">kubernetes/kubernetes#93264</a>, <a href=""https://github.com/justaugustus""><code>@​justaugustus</code></a>) [SIG API Machinery, Auth, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Network, Node, Release, Scalability, Storage and Testing]</li>; </ul>; </li>; <li>Promote Immutable Secrets/ConfigMaps feature to Beta and enable the feature by default.; This allows to set <code>Immutable</code> field in Secrets or ConfigMap object to mark their contents as immutable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89594"">kubernetes/kubernetes#89594</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>) [SIG Apps and Testing]</li>; <li>Remove <code>BindTimeoutSeconds</code> from schedule configuration <code>KubeSchedulerConfiguration</code> (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91580"">kubernetes/kubernetes#91580</a>, <a href=""https://github.com/cofyc""><code>@​cofyc</code></a>) [SIG Scheduling and Testing]</li>; <li>Remove kubescheduler.config.k8s.io/v1alpha1 (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89298"">kubernetes/kubernetes#89298</a>, <a href=""https://github.com/gavinfish""><code>@​gavinfish</code></a>) [SIG Scheduling]</li>; <li>Reserve plugins that fail to reserve will trigger the unreserve extension point (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92391"">kubernetes/kubernetes#92391</a>, <a href=""https://github.com/adtac""><code>@​adtac</code></a>) [SIG Scheduling and Testing]</li>; <li>Resolve regression in <code>metadata.managedFields</code> handling in update/patch requests submitted by older API clients (<a href=""https://github-redirect.d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:11606,config,configuration,11606,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['config'],['configuration']
Modifiability,"a substep of a pod/job, and as docker container. My last renaming proposal got shot down, but we clearly need to improve this in a later PR.; - Heavily reworked worker.py. I believe this fixes https://github.com/hail-is/hail/issues/7350. The main design idea is to having all state creation and cleanup in Pod.run and Container.run.; - worker: Just support pods/status and pods/log, not container level status or logs.; - Pod now writes final status, not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""job_id"": 1,; ""user"": ""test"",; ""state"": ""succeeded"",; ""container_statuses"": {; ""setup"": {; ""name"": ""setup"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.038861751556396484,; ""creating"": 0.7245609760284424,; ""starting"": 4.770207166671753,; ""running"": 1.1384251117706299,; ""runtime"": 5.909235715866089,; ""uploading_log"": 0.3659687042236328,; ""deleting"": 0.013197660446166992",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7354:1360,config,config,1360,https://hail.is,https://github.com/hail-is/hail/pull/7354,1,['config'],['config']
Modifiability,"a/pull/983"">python-jsonschema/jsonschema#983</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.3...v4.11.0"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.3...v4.11.0</a></p>; <h2>v4.10.3</h2>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.2...v4.10.3"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.2...v4.10.3</a></p>; <h2>v4.10.2</h2>; <ul>; <li>Fix a second place where subclasses may have added attrs attributes (<a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/issues/982"">#982</a>).</li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.1...v4.10.2"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.1...v4.10.2</a></p>; <h2>v4.10.1</h2>; <ul>; <li>Fix Validator.evolve (and APIs like <code>iter_errors</code> which call it) for cases; where the validator class has been subclassed. Doing so wasn't intended to be; public API, but given it didn't warn or raise an error it's of course; understandable. The next release however will make it warn (and a future one; will make it error). If you need help migrating usage of inheriting from a; validator class feel free to open a discussion and I'll try to give some; guidance (<a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/issues/982"">#982</a>).</li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.0...v4.10.1"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.0...v4.10.1</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/python-jsonschema/jsonschema/blob/main/CHANGELOG.rst"">jsonschema's changelog</a>.</em></p>; <blockquote>; <h1>v4.15.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12163:3098,evolve,evolve,3098,https://hail.is,https://github.com/hail-is/hail/pull/12163,1,['evolve'],['evolve']
Modifiability,"a2 (see below).; From the error it seems like this is due to Hail's dependency of bokeh using the latest version of jinja2. Downgrading jinja2 to 3.0.0 solves the problem, and it seems like other people have seen this too with the latest release of jinja2:. https://github.com/holoviz/panel/issues/3260. This may be transient and may be solved by bokeh / jinja2 folks but thought I'd let you know in case you hit this issue. ```; ../conda/envs/glow/lib/python3.7/site-packages/bokeh/core/templates.py:43: in <module>; from jinja2 import Environment, Markup, FileSystemLoader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:114",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:1105,adapt,adapted,1105,https://hail.is,https://github.com/hail-is/hail/issues/11705,1,['adapt'],['adapted']
Modifiability,"a8966fa96859c6672aba986b""><code>25ded57</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ipython/ipython/issues/13804"">#13804</a> from Carreau/sd-x</li>; <li><a href=""https://github.com/ipython/ipython/commit/98e3599e130e253f292679f74982a3a2cd3a7a7a""><code>98e3599</code></a> exclude 3.8</li>; <li><a href=""https://github.com/ipython/ipython/commit/fcdcddd5e528844672688e07bfa5188e48e37521""><code>fcdcddd</code></a> iterate</li>; <li><a href=""https://github.com/ipython/ipython/commit/8ca7b420a29ad781cc6c701dd4a6af0dd21b35c4""><code>8ca7b42</code></a> fix stack-data 0.6 failing tests</li>; <li><a href=""https://github.com/ipython/ipython/commit/93992a7ecd086bb24840ba03cd69960daf41575d""><code>93992a7</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ipython/ipython/issues/13768"">#13768</a> from osherdp/feature/raise-when-opening-standard-st...</li>; <li><a href=""https://github.com/ipython/ipython/commit/f44e27095fd647cc22bf37874f183ec4db85949f""><code>f44e270</code></a> Refactor a bit of uniformity.</li>; <li><a href=""https://github.com/ipython/ipython/commit/1b5674ca8bbac62daa42eb460848173c0542cf2e""><code>1b5674c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ipython/ipython/issues/13778"">#13778</a> from zhizheng1/fix-mpl-webagg</li>; <li>Additional commits viewable in <a href=""https://github.com/ipython/ipython/compare/7.34.0...8.6.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=ipython&package-manager=pip&previous-version=7.34.0&new-version=8.6.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-autome",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12442:1771,Refactor,Refactor,1771,https://hail.is,https://github.com/hail-is/hail/pull/12442,1,['Refactor'],['Refactor']
Modifiability,a:1431); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:641); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:68); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:78); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:50); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12531:8149,rewrite,rewrite,8149,https://hail.is,https://github.com/hail-is/hail/issues/12531,1,['rewrite'],['rewrite']
Modifiability,"a> from fabianegli/main</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/7f924b13a50a05b8dc894418fa7faf779201e129""><code>7f924b1</code></a> Fix typo in deprecation documentation</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/4a8f8ada431974f2837260af3ed36299fd382814""><code>4a8f8ad</code></a> build(deps): Bump django from 4.0.2 to 4.0.3 in /testing/plugins_integration ...</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/c0fd2d883940f1292d5e8234803beaacd08315e6""><code>c0fd2d8</code></a> build(deps): Bump pytest-asyncio from 0.18.1 to 0.18.2 in /testing/plugins_in...</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/843e01824c257c3190792a9df430289c3abe349d""><code>843e018</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9732"">#9732</a> from nicoddemus/9730-toml-failure</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/bc43d66b47b917d43a22e0c703ecfe4eea342263""><code>bc43d66</code></a> [automated] Update plugin list (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9733"">#9733</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/e38d1cac489e42f4bdbecbb50f9f25dc9c36c19f""><code>e38d1ca</code></a> Improve error message for malformed pyproject.toml files</li>; <li>Additional commits viewable in <a href=""https://github.com/pytest-dev/pytest/compare/6.2.5...7.1.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pytest&package-manager=pip&previous-version=6.2.5&new-version=7.1.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11571:6301,plugin,plugin,6301,https://hail.is,https://github.com/hail-is/hail/pull/11571,2,['plugin'],['plugin']
Modifiability,"ability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, sldsc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, rhemc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64}, rhemc_8bin: struct{h2_liability: float64, h2_liability_se: float64, h2_observed: float64, h2_observed_se: float64, h2_z: float64}, rhemc_25bin_50rv: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}, final: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}}, qcflags: struct{GWAS_run: bool, defined_h2: bool, significant_z: bool, in_bounds_h2: bool, normal_lambda: bool, normal_ratio: bool, EUR_plus_1: bool, pass_all: bool}, N_ancestry_QC_pass: int32}, saige_version: str, inv_normalized: bool, pop: str, lambda_gc: float64, n_variants: int64, n_sig_variants: int64, saige_heritability: float64}))}; at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:15); at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:15); at is.hail.utils.package$.fatal(package.scala:78); at is.hail.expr.ir.PruneDeadFields$.isSupertype(PruneDeadFields.scala:75); at is.hail.rvd.RVDCoercer.coerce(RVD.scala:31); at is.hail.rvd.RVD$.coerce(RVD.scala:1262); at is.hail.rvd.RVD.changeKey(RVD.scala:143); at is.hail.rvd.RVD.changeKey(RVD.scala:136); [...]; java.util.NoSuchElementException: key not found: 0; at scala.collection.immutable.Map$Map1.apply(Map.scala:114); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2(PruneDeadFields.scala:62); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2$adapted(PruneDeadFields.scala:61); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10858:4164,adapt,adapted,4164,https://hail.is,https://github.com/hail-is/hail/issues/10858,1,['adapt'],['adapted']
Modifiability,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trave,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:2282,Rewrite,RewriteBottomUp,2282,https://hail.is,https://github.com/hail-is/hail/issues/9128,6,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:3762,Rewrite,RewriteBottomUp,3762,https://hail.is,https://github.com/hail-is/hail/issues/9128,3,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5153,Rewrite,RewriteBottomUp,5153,https://hail.is,https://github.com/hail-is/hail/issues/6458,3,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:4502,Rewrite,RewriteBottomUp,4502,https://hail.is,https://github.com/hail-is/hail/issues/9128,3,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,able.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:2467,Rewrite,RewriteBottomUp,2467,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Rewrite'],['RewriteBottomUp']
Modifiability,able.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:3947,Rewrite,RewriteBottomUp,3947,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"abot.com/psf/black/issues/2879"">#2879</a>).</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/psf/black/blob/main/CHANGES.md"">black's changelog</a>.</em></p>; <blockquote>; <h2>22.3.0</h2>; <h3>Preview style</h3>; <ul>; <li>Code cell separators <code>#%%</code> are now standardised to <code># %%</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2919"">#2919</a>)</li>; <li>Remove unnecessary parentheses from <code>except</code> statements (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2939"">#2939</a>)</li>; <li>Remove unnecessary parentheses from tuple unpacking in <code>for</code> loops (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2945"">#2945</a>)</li>; <li>Avoid magic-trailing-comma in single-element subscripts (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2942"">#2942</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not format <code>__pypackages__</code> directories by default (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2836"">#2836</a>)</li>; <li>Add support for specifying stable version with <code>--required-version</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2832"">#2832</a>).</li>; <li>Avoid crashing when the user has no homedir (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2814"">#2814</a>)</li>; <li>Avoid crashing when md5 is not available (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2905"">#2905</a>)</li>; <li>Fix handling of directory junctions on Windows (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2904"">#2904</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Update pylint config documentation (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2931"">#2931</a>)</li>; </ul>; <h3>Integrations</h3>; <ul>; <li>Move test to disable plugin in Vim/Neovim, which speeds up loadi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11696:4087,Config,Configuration,4087,https://hail.is,https://github.com/hail-is/hail/pull/11696,1,['Config'],['Configuration']
Modifiability,"abot.com/sphinx-doc/sphinx/issues/10648"">#10648</a>: LaTeX: CSS-named-alike additional :ref:<code>'sphinxsetup' &lt;latexsphinxsetup&gt;</code>; keys allow to configure four separate border-widths, four paddings, four; corner radii, a shadow (possibly inset), colours for border, background, shadow; for each of the code-block, topic, attention, caution, danger, error and warning; directives.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10655"">#10655</a>: LaTeX: Explain non-standard encoding in LatinRules.xdy</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10599"">#10599</a>: HTML Theme: Wrap consecutive footnotes in an <code>&lt;aside&gt;</code> element when; using Docutils 0.18 or later, to allow for easier styling. This matches the; behaviour introduced in Docutils 0.19. Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10518"">#10518</a>: config: Add <code>include_patterns</code> as the opposite of <code>exclude_patterns</code>.; Patch by Adam Turner.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/e712eae382d213ce3f4866ad6f5b3c84ce4f4409""><code>e712eae</code></a> Bump to 5.1.1 final</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/0555345ad715b1e5ec83bce2e4a993441ffb8f29""><code>0555345</code></a> Fix ValueError popping out in <code>sphinx.ext.napoleon</code> (<a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10709"">#10709</a>)</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/9845500ffa6b75266b1e34701c15eb8e586aa17e""><code>9845500</code></a> Improve support for deprecated builders without env arg (<a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10702"">#10702</a>)</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12165:4924,config,config,4924,https://hail.is,https://github.com/hail-is/hail/pull/12165,1,['config'],['config']
Modifiability,"abot.com/tqdm/tqdm/issues/1218"">#1218</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1082"">#1082</a>, <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1217"">#1217</a>)</li>; <li>warn on positional CLI arguments</li>; <li>misc build/test framework updates; <ul>; <li>enable <code>py3.10</code> tests</li>; <li>add <code>conda</code> dependencies</li>; <li>update pre-commit hooks</li>; <li>fix <code>pytest</code> config (<code>nbval</code>, <code>asyncio</code>)</li>; <li>fix dependencies &amp; tests</li>; <li>fix site deployment</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.3 stable</h2>; <ul>; <li>fix minor typo (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>minor example fix (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>misc tidying &amp; refactoring</li>; <li>misc build/dev framework updates; <ul>; <li>update dependencies</li>; <li>update linters</li>; <li>update docs deployment branches</li>; </ul>; </li>; <li>misc test/ci updates; <ul>; <li>test forks</li>; <li>tidy OS &amp; Python version tests</li>; <li>bump primary python version 3.7 =&gt; 3.8</li>; <li>beta py3.10 testing</li>; <li>fix py2.7 tests</li>; <li>better timeout handling</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.2 stable</h2>; <ul>; <li>fix notebook memory leak (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1216"">#1216</a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/tqdm/tqdm/commit/6791e8c5b3d6c30bdd2060c346996bfb5a6f10d1""><code>6791e8c</code></a> bump version, merge pull request <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1366"">#1366</a> from tqdm/devel</li>; <li><a href=""https://github.com/tqdm/tqdm/commit/754186291e6b4e28ea8b56c9493adc03bf14c404""><code>7541862</code></a> tests: hotfix skip windows errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12260:3277,refactor,refactoring,3277,https://hail.is,https://github.com/hail-is/hail/pull/12260,1,['refactor'],['refactoring']
Modifiability,"abot.com/tqdm/tqdm/issues/1218"">#1218</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1082"">#1082</a>, <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1217"">#1217</a>)</li>; <li>warn on positional CLI arguments</li>; <li>misc build/test framework updates; <ul>; <li>enable <code>py3.10</code> tests</li>; <li>add <code>conda</code> dependencies</li>; <li>update pre-commit hooks</li>; <li>fix <code>pytest</code> config (<code>nbval</code>, <code>asyncio</code>)</li>; <li>fix dependencies &amp; tests</li>; <li>fix site deployment</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.3 stable</h2>; <ul>; <li>fix minor typo (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>minor example fix (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>misc tidying &amp; refactoring</li>; <li>misc build/dev framework updates; <ul>; <li>update dependencies</li>; <li>update linters</li>; <li>update docs deployment branches</li>; </ul>; </li>; <li>misc test/ci updates; <ul>; <li>test forks</li>; <li>tidy OS &amp; Python version tests</li>; <li>bump primary python version 3.7 =&gt; 3.8</li>; <li>beta py3.10 testing</li>; <li>fix py2.7 tests</li>; <li>better timeout handling</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.2 stable</h2>; <ul>; <li>fix notebook memory leak (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1216"">#1216</a>)</li>; <li>fix <code>contrib.concurrent</code> with generators (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1233"">#1233</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1231"">#1231</a>)</li>; </ul>; <h2>tqdm v4.62.1 stable</h2>; <ul>; <li><code>contrib.logging</code>: inherit existing handler output stream (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1191"">#1191</a>)</li>; <li>fix <code>PermissionError</code> by using <code>weakref</code> in <code>DisableOnWriteErro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11587:1487,refactor,refactoring,1487,https://hail.is,https://github.com/hail-is/hail/pull/11587,1,['refactor'],['refactoring']
Modifiability,"ache` tag, and ensures the cache is updated on each build.; 5. Remove the `gcloud` binary from Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sure how much mileage we can get out of layer squashing. You can take a look at a service-base build [here](https://gist.github.com/danking/830af0688970c176ff25dbfeb4b222e7). Note the stdout comes first and then I `cat` the ""trace"" file which is a very weirdly formatted series of JS",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10502:1273,layers,layers,1273,https://hail.is,https://github.com/hail-is/hail/pull/10502,1,['layers'],['layers']
Modifiability,ackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:14); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:167); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:161); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); E 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); E 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.execute$1(EvalRelationalLets.scala:10); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:18); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:32); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:157); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:11101,adapt,adapted,11101,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['adapt'],['adapted']
Modifiability,ackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:151); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); E 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); E 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.execute$1(EvalRelationalLets.scala:10); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:18); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:37); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:147); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:2806,adapt,adapted,2806,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['adapt'],['adapted']
Modifiability,"acked on #10791~. This PR attempts to allow linear algebra codegen methods, like the LAPACK wrappers and the local whitening methods I'm working on, to defensively assert shape compatibility preconditions, without generating redundant runtime checks. (I always hate when we're pushed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerceToShape(cb, IndexedSeq(k",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10783:1057,variab,variable,1057,https://hail.is,https://github.com/hail-is/hail/pull/10783,1,['variab'],['variable']
Modifiability,"action for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:1818,Config,Configuring,1818,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020,1,['Config'],['Configuring']
Modifiability,"actually, hold off on this for a couple days -- I want to rewrite the logic in the whole file to support a feature I need to lower MatrixMapCols",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6197#issuecomment-496521985:58,rewrite,rewrite,58,https://hail.is,https://github.com/hail-is/hail/issues/6197#issuecomment-496521985,1,['rewrite'],['rewrite']
Modifiability,"add array_windows / locus_windows, refactor ld_prune, remove filteredEntriesTable and UpperIndexBounds",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3715:35,refactor,refactor,35,https://hail.is,https://github.com/hail-is/hail/pull/3715,1,['refactor'],['refactor']
Modifiability,"add array_windows / locus_windows, refactor ld_prune, remove filteredEntriesTable and UpperIndexBounds (v2)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3873:35,refactor,refactor,35,https://hail.is,https://github.com/hail-is/hail/pull/3873,1,['refactor'],['refactor']
Modifiability,add ci2 build configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5842:14,config,configuration,14,https://hail.is,https://github.com/hail-is/hail/pull/5842,1,['config'],['configuration']
Modifiability,add grouped type variable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11558:17,variab,variable,17,https://hail.is,https://github.com/hail-is/hail/pull/11558,1,['variab'],['variable']
Modifiability,add hello rule to router; parameterize wait-for by location; use in test-ci,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7495:26,parameteriz,parameterize,26,https://hail.is,https://github.com/hail-is/hail/pull/7495,1,['parameteriz'],['parameterize']
Modifiability,added RowMatrix and adapted ExportableMatrix functionality,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2661:20,adapt,adapted,20,https://hail.is,https://github.com/hail-is/hail/pull/2661,1,['adapt'],['adapted']
Modifiability,"after rtfm (https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/README.md#getting-the-connector), seems credentials need to be configured in `core-site.xml`. I don't know where we do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1986218258:148,config,configured,148,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1986218258,1,['config'],['configured']
Modifiability,age 10.0 (TID 461381) (cluster-w-144.c.gbsc-project.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2244); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2269); 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:2893,adapt,adapted,2893,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['adapt'],['adapted']
Modifiability,"age is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The firs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:2516,enhance,enhanced,2516,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['enhance'],['enhanced']
Modifiability,"agma: no cover&quot; <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7668"">#7668</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Trim glyph size in ImageFont.getmask() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7669"">#7669</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fix loading IPTC images and update test <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7667"">#7667</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>; <li>Allow uncompressed TIFF images to be saved in chunks <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7650"">#7650</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Concatenate multiple JPEG EXIF markers <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7496"">#7496</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Changed IPTC tile tuple to match other plugins <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7661"">#7661</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Do not assign new fp attribute when exiting context manager <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7566"">#7566</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Support arbitrary masks for uncompressed RGB DDS images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7589"">#7589</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Support setting ROWSPERSTRIP tag <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7654"">#7654</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Apply ImageFont.MAX_STRING_LENGTH to ImageFont.getmask() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7662"">#7662</a> [<a href=""https://github.com/ra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:1961,plugin,plugins,1961,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['plugin'],['plugins']
Modifiability,"ah wait - in the example we discussed, there needs to be randomness *after* the order by to fully test this rewrite rule",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4104#issuecomment-411814585:108,rewrite,rewrite,108,https://hail.is,https://github.com/hail-is/hail/pull/4104#issuecomment-411814585,1,['rewrite'],['rewrite']
Modifiability,"ah yes - the identity used to create the SAS token needs to have a control plane role on the Storage Account - Owner, Contributor, or (most specific) Storage Account Key Operator Service Role... Is that a manageable role to configure for testing or should I try to explore alternatives in the generation?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13140#issuecomment-1579149359:224,config,configure,224,https://hail.is,https://github.com/hail-is/hail/pull/13140#issuecomment-1579149359,1,['config'],['configure']
Modifiability,ah! I added a rewrite rule here which uses the old path.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4070#issuecomment-410453178:14,rewrite,rewrite,14,https://hail.is,https://github.com/hail-is/hail/issues/4070#issuecomment-410453178,1,['rewrite'],['rewrite']
Modifiability,ahhhhh yes it was. aggregateEntries was refactored to aggregateEntriesAST but the new aggregateEntries (IR) didn't get the new node.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4177#issuecomment-414490470:40,refactor,refactored,40,https://hail.is,https://github.com/hail-is/hail/pull/4177#issuecomment-414490470,1,['refactor'],['refactored']
Modifiability,"ail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in <module>; 143 ; 144 ; --> 145 _cell_set_template_code = _make_cell_set_template_code(); 146 ; 147 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in _make_cell_set_template_code(); 124 ); 125 else:; --> 126 return types.CodeType(; 127 co.co_argcount,; 128 co.co_kwonlyargcount,. TypeError: an integer is required (got type bytes). In [2]: . ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:10990,sandbox,sandbox,10990,https://hail.is,https://github.com/hail-is/hail/issues/10197,4,['sandbox'],['sandbox']
Modifiability,"ail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . /home/hail/hail.zip/hail/table.py in _process_joins(self, *exprs); 1459 def broadcast_f(left, data, jt):; 1460 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1461 return process_joins(self, exprs, broadcast_f); 1462 ; 1463 def cache(self):. /home/hail/hail.zip/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 354 for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; 355 if j not in used_joins:; --> 356 left = j.join_func(left); 357 all_uids.extend(j.temp_vars); 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, Table); -> 1450 return Table(Env.jutils().joinGlobals(obj._jt, self._jt, uid)); 1451 ; 1452 ast = Join(Select(TopLevelReference('global', Indices()), uid),. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.Asserti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3728:3810,extend,extend,3810,https://hail.is,https://github.com/hail-is/hail/issues/3728,1,['extend'],['extend']
Modifiability,ailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/utils.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init__.py'; adding 'hailtop/hailctl/config/cli.py'; adding 'hailtop/hailctl/config/config_variables.py'; adding 'hailtop/hailctl/dataproc/__init__.py'; adding 'hailtop/hailctl/dataproc/cli.py'; adding 'hailtop/hailctl/dataproc/cluster_config.py'; adding 'hailtop/hailctl/dataproc/connect.py'; adding 'hailtop/hailctl/dataproc/deploy_metadata.py'; adding 'hailtop/hailctl/dataproc/diagnose.py'; adding 'hailtop/hailctl/dataproc/gcloud.py'; adding 'hailtop/hailctl/dataproc/modify.py'; adding 'hailtop/hailctl/dataproc/start.py'; adding 'hailtop/hailctl/dataproc/submit.py'; adding 'hailtop/hailctl/dataproc/utils.py'; adding 'hailtop/hailctl/dev/__init__.py'; adding 'hailtop/hailctl/dev/ci_client.py'; adding 'hailtop/hailctl/dev/cli.py'; adding 'hailtop/hailctl/dev/config.py'; adding 'hailtop/hailctl/hdinsight/__init__.py'; adding 'hailtop/hailctl/hdinsight/cli.py'; adding 'hailtop/hailctl/hdinsight/start.py'; adding 'hailtop/hailctl/hdinsight/submit.py'; adding 'hailtop/utils/__init__.py'; adding 'hailtop/utils/filesize.py'; adding 'hailtop/utils/process.py'; adding 'hailtop/utils/rate_limiter.py'; adding 'hailtop/utils/rates.py'; adding 'hailtop/utils/rich_progress_bar.py'; adding 'hailtop/utils/serialization.py'; adding 'hailtop/utils/time.py'; adding 'hailtop/utils/utils.py'; adding 'hailtop/utils/validate/__init__.py'; adding 'hailtop/utils/validate/validate.py'; adding 'hail-0.2.124.dist-info/METADATA'; adding 'hail-0.2.124.dist-info/WHEEL'; adding 'hail-0.2.124.dist-info/entry_points.txt'; adding 'hail-0.2.124.dist-info/top_level.txt'; adding 'hail-0.2.124.dist-info/RECORD'; emoving build/bdist.linux-x86_64/wheel; python3 -m pip install 'pip-tools==6.13.0' && bash ../check_pip_requirements.sh python; Defaulting to user install,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:28232,config,config,28232,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['config'],['config']
Modifiability,al(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6155,adapt,adapted,6155,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['adapt'],['adapted']
Modifiability,al.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:144); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:144); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$2(LocalBackend.scala:223); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBackend.scala:309); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1$adapted(LocalBackend.scala:308); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); 	at is.hail.backend.local.LocalBackend.$anonfun$withExecuteContext$2(LocalBackend.scala:144); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); 	at is.hail.backend.local.LocalBackend.withExecuteContext(LocalBackend.scala:130); 	at is.hail.backend.local.LocalBackend.execute(LocalBackend.scala:308); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpser,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:2764,adapt,adapted,2764,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,4,['adapt'],['adapted']
Modifiability,"al_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/test.py"", line 6, in <module>; ht.write('gs://ehigham-hail-tmp/test_hail_in_notebook.ht'); hail.utils.java.FatalError: SocketTimeoutException: connect timed out. Java stack trace:; java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:254); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:12366,config,configure,12366,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,1,['config'],['configure']
Modifiability,ala:341); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:715); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:985); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:68); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:77); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:50); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:21334,rewrite,rewrite,21334,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['rewrite'],['rewrite']
Modifiability,all builds are failing because the repository the hosts the shadow jar plugin is down,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10985#issuecomment-1011304877:71,plugin,plugin,71,https://hail.is,https://github.com/hail-is/hail/pull/10985#issuecomment-1011304877,1,['plugin'],['plugin']
Modifiability,"all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be rebuilt) target is dynamically generated. That target will force a execution of any dependent targets, in the example above, it will force `build` to be executed. If the variable's current value and it's previous value do not differ, no target is generated and thus nothing is executed. #### MAKEFLAGS += --no-builtin-rules. This disables all the automatic rules, making Makefile parsing/interpretation faster. #### .SUFFIXES:. This sets the suffixes to nothing, disabling a bunch of implicit rules. For example, the rule for compiling C files. #### Breeze versions. I found the breeze versions by manually looking at the Spark pom file at the tags for each release, starting with [2.2.0](https://github.com/apache/spark/blob/v2.2.0/pom.xml#L654). ---. [1] A python module is any folder containing a file called `__init__.py`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130:3083,variab,variable,3083,https://hail.is,https://github.com/hail-is/hail/pull/5130,2,['variab'],['variable']
Modifiability,"an</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2816"">cbeust/testng#2816</a></li>; <li>Make PackageUtils compliant with JPMS by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2817"">cbeust/testng#2817</a></li>; <li>Ability to retry a data provider during failures by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2820"">cbeust/testng#2820</a></li>; <li>Refactoring by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2821"">cbeust/testng#2821</a></li>; <li>Fixing bug with DataProvider retry by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2822"">cbeust/testng#2822</a></li>; <li>Add config key for callback discrepancy behavior by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2823"">cbeust/testng#2823</a></li>; <li>Upgrading versions by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2824"">cbeust/testng#2824</a></li>; <li>Fix <a href=""https://github-redirect.dependabot.com/cbeust/testng/issues/2770"">#2770</a>: FileAlreadyExistsException on copy by <a href=""https://github.com/melloware""><code>@​melloware</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2827"">cbeust/testng#2827</a></li>; <li>JarFileUtils.delete(File f) throw actual exception (instead of FileNotFound) when file cannot be deleted <a href=""https://github-redirect.dependabot.com/cbeust/testng/issues/2825"">#2825</a> by <a href=""https://github.com/speedythesnail""><code>@​speedythesnail</code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:3942,config,config,3942,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['config'],['config']
Modifiability,"anges actually improves everyone's understanding of this. ---. The goal of this PR is to make this work:. ```; HAIL_QUERY_BACKEND=service \; python3 -c 'import hail as hl; hl.utils.range_table(10).write(""gs://foo/bar.t"")`; ```. In particular, a normal user should not need to know the location of a Hail Query JAR. Currently, you must specify two environment variables: `HAIL_SHA` and `HAIL_JAR_URL`. This PR takes advantage of the well known location of a Hail Query JAR [1]. We use the newly introduced `hl.revision()` to determine the SHA-1 of the currently installed Hail. This PR includes the revision in the driver job spec. The front end has been modified to convert the revision into a cloud storage URL. This PR also provides three escape hatches to the aforementioned default behavior. These escape hatches should more or less only be used by developers. They're specified from highest priority to lowest.; 1. Specify the `jar_url` parameter to `ServiceBackend`.; 2. Specify the `HAIL_JAR_URL` environment variable.; 3. Specify a JAR url in the user config: `hailctl config set query/jar_url gs://...`. While writing this PR, I decided to clean up five bits of cruft I left when I first built the service backend. First, I took the JAR URL out of the ""command"" of the job spec. This ""command"" is just an array of strings. The fact that certain parts of that array *must* be the JAR URL and the SHA-1 is confusing. Instead, there are now two keys in a JVM process specification:; 1. `jar_spec`, which may be either `{""type"": ""jar_url"", ""value"": ""gs://..../abc123....jar""}` or `{""type"":""git_revision"", ""value"": ""abc123...""}`.; 2. `argv`, an opaque list of strings which are passed, by the JVMEntryway, along with a few more args, to `is.hail.backend.service.Main`. The `Main` class dispatches to either `ServiceBackendSocketAPI2` or the `Worker` based on the first element of `argv`. Each class expects different contents in `argv` that suits its needs. Second, I completely eliminated the HAI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11645:1085,variab,variable,1085,https://hail.is,https://github.com/hail-is/hail/pull/11645,1,['variab'],['variable']
Modifiability,ants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:1727,Rewrite,RewriteBottomUp,1727,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-values, but order). Could generate multiple-hypothesis-test corrected aggregate. These users get publication credit as consortia; 3) People donate minor intellectual capital: Re-run analysis with full available code. Report on success. This will catch bugs, and non-deterministic results (for instance, if reported accuracy depends on local minima..similar or better minima may only occur once in a great while). Similar to 2. ## Timetables; 1-3a: 12/10/18; 3b: by 12/15/18; 4a-4b: by 12/12/18; 4c-d: by 12/15/18. This probably shouldn't be merged for a while. Still working on authentication handling for third party APIs. All first party APIs (our stuff) is well controlled, can be extended from existing codebase.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:9778,extend,extended,9778,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['extend'],['extended']
Modifiability,"apiGroups"":[""""] ""resources"":[""pods""] ""verbs"":[""get"" ""list"" ""watch"" ""create"" ""update"" ""patch"" ""delete""]] map[""apiGroups"":[""""] ""resources"":[""pods/log""] ""verbs"":[""get""]]]]}; from server for: ""deployment.yaml"": roles.rbac.authorization.k8s.io ""batch-pods-admin"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get roles.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=rolebindings"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=RoleBinding""; Name: ""batch-pods-admin-binding"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""RoleBinding"" ""metadata"":map[""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""name"":""batch-pods-admin-binding"" ""namespace"":""batch-pods""] ""roleRef"":map[""apiGroup"":"""" ""kind"":""Role"" ""name"":""batch-pods-admin""] ""subjects"":[map[""kind"":""ServiceAccount"" ""name"":""batch-svc"" ""namespace"":""default""]]]}; from server for: ""deployment.yaml"": rolebindings.rbac.authorization.k8s.io ""batch-pods-admin-binding"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get rolebindings.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""hail.is/sha"":""1c6dbf20333a"" ""app"":""batch""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:2654,config,configuration,2654,https://hail.is,https://github.com/hail-is/hail/issues/4609,1,['config'],['configuration']
Modifiability,apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:4565,Rewrite,RewriteBottomUp,4565,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"argh, apologies for the branch on origin - just configured this hail setup and haven't set the forks properly",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5636#issuecomment-474554637:48,config,configured,48,https://hail.is,https://github.com/hail-is/hail/pull/5636#issuecomment-474554637,1,['config'],['configured']
Modifiability,ariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:11600,Config,Configuration,11600,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,"as I understand, a java object is shared to Python via some reference, and it doesn't actually serialize the class variables. My guess is that the slow part in import_vcfs is actually constructing the MatrixTable objects -- this is quite slow due to all of the type nonsense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5613#issuecomment-473453502:115,variab,variables,115,https://hail.is,https://github.com/hail-is/hail/pull/5613#issuecomment-473453502,1,['variab'],['variables']
Modifiability,"ase, we aggregate bandwidth substantially decreases at 40k files of size ~100kB. Theoretical egress from the GCP VM I used is 2 GB/s. Ingress is ~3 GB/s. S3 claims as much as ~12 GB/s of aggregate bandwidth. We seem to have room for improvement, but this seems good enough for now. # On AWS, GCS -> S3. | Files | Bytes | Time | Rate |; | ----- | ----- | ---- | ---- |; | 1 | 5.4 GB | 34 seconds | 154.5 MB/s |; | 1 | 42.9 GB | 4 minutes | 161.6 MB/s |; | 200 | 5.4 GB | 35 seconds | 151.1 MB/s |; | 40000 | 5.4 GB | 4 minutes | 22.0 MB/s |. # On GCP, S3 -> GCS. | Files | Bytes | Time | Rate |; | ----- | ----- | ---- | ---- |; | 1 | 5.4 GB | 17 seconds | 304.2 MB/s |; | 1 | 42.9 GB | 3 minutes | 235.5 MB/s |; | 200 | 5.4 GB | 20 seconds | 267.8 MB/s |; | 40000 | 5.4 GB | 6 minutes | 13.3 MB/s |. # machine parsable form; ```; [{'config': 'one',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/one',; 'times': [34.76],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/one'},; {'config': 'some',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/some',; 'times': [35.527],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/some'},; {'config': 'many',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/many',; 'times': [244.154],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/many'},; {'config': 'huge',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/huge',; 'times': [265.719],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/huge'},; {'config': 'one',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/one',; 'times': [17.65],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/one'},; {'config': 'some',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/some',; 'times': [20.048],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/some'},; {'config': 'many',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/many',; 'times': [402.267],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/many'},; {'config':",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10752#issuecomment-897651697:1085,config,config,1085,https://hail.is,https://github.com/hail-is/hail/pull/10752#issuecomment-897651697,1,['config'],['config']
Modifiability,at __C8160Compiled.apply(Emit.scala); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$4(CompileAndEvaluate.scala:61); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$2(CompileAndEvaluate.scala:61); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$2$adapted(CompileAndEvaluate.scala:59); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:140); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:140); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:59); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:33); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:58); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:63); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13486:3672,rewrite,rewrite,3672,https://hail.is,https://github.com/hail-is/hail/issues/13486,1,['rewrite'],['rewrite']
Modifiability,"at immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc start` argument, but `--num-secondary-worker-local-ssds` is not. This could cause some confusion/annoyance. This could potentially be reduced by minimizing the number of `gcloud dataproc` arguments that are defined as `hailctl dataproc` arguments. For example, `--num-worker-local-ssds` is just passed through to `gcloud`, so there's no real need for it to be a `hailctl dataproc start` argument. On the other hand, it is nice to have some of those `gcloud` arguments show up in `hailctl dataproc start --help`.; - While making breaking changes, it would be nice if the `--configuration`/`--gcloud-configuration` argument was consistent across `hailctl dataproc` commands. For example, currently `hailctl dataproc start` takes `--configuration` but `hailctl dataproc submit` takes `--gcloud-configuration`. It would also be nice to standardize on kebab case for all arguments. There are some other arguments that use underscores (ex. `--init_timeout` to `hailctl dataproc start`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:2799,config,configuration,2799,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034,4,['config'],['configuration']
Modifiability,at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:1615,Rewrite,RewriteBottomUp,1615,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.service.ServiceBackendSocketAPI2.withExecuteContext$1(ServiceBackend.scala:633); 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:695); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:461); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6$adapted(ServiceBackend.scala:460); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:460); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:460); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:12919,adapt,adapted,12919,https://hail.is,https://github.com/hail-is/hail/issues/12982,6,['adapt'],['adapted']
Modifiability,"at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: is.hail.backend.service.EndOfInputException; 	at is.hail.backend.service.ServiceBackendSocketAPI2.read(ServiceBackend.scala:497); 	at is.hail.backend.service.ServiceBackendSocketAPI2.readInt(ServiceBackend.scala:510); 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:561); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:462); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6$adapted(ServiceBackend.scala:461); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:461); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:141); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:460); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:459); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:141); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:15); 	at is.hail.backend.service.Main.main(Main.scala); 	... 11 more; ```. Which suggests that the service backend experienced an EOF somewhere in the first four bytes of the input file. Unfortunately, we automatically cleanup the input and output files, so I can't investigate further. This PR reads the input and output files and stores them in the error message so that next time this happens we get more information.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:6094,adapt,adapted,6094,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['adapt'],['adapted']
Modifiability,at org.apache.spark.rdd.RDD.withScope(RDD.scala:414); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029); 	at is.hail.backend.spark.SparkBackend.parallelizeAndComputeWithIndex(SparkBackend.scala:355); 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:43); 	at __C16570Compiled.__m16792split_CollectDistributedArray(Emit.scala); 	at __C16570Compiled.__m16791begin_group_0(Emit.scala); 	at __C16570Compiled.apply(Emit.scala); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$3(CompileAndEvaluate.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:57); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(I,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:7942,rewrite,rewrite,7942,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['rewrite'],['rewrite']
Modifiability,at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.Mat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5603,adapt,adapted,5603,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['adapt'],['adapted']
Modifiability,"at(it: Iterable[Doc])`; * `Group(body: Doc)`. Ignoring `Group` for the moment, the first four constructors define formatted documents with only one possible layout, regardless of `width` or `ribbonLength`. `Text` simply prints the string `t`; `Line` prints a newline, followed by the current level of indentation (`ifFlat` is explained when we discuss `Group`); `Indent` increases the indentation of all `Line`s contained in `body` by `i`; and `Concat` simply prints all documents in `it` sequentially. `Group` is the sole source of alternatives which `render` must choose between. `Group(body)` can be rendered in one of two ways:; * replace all `Line`s contained in `body` (including in nested `Group`s) by their `ifFlat` alternative (almost always either "" "" or """"), or, if that would cause the line to exceed `width`; * print `body` normally, as described in the previous paragraph, allowing nested `Group`s to print either flat or normally. This pretty-printer DSL has become fairly standard, with some common enhancements that I don't think we need. It was first described in [A prettier printer](https://homepages.inf.ed.ac.uk/wadler/papers/prettier/prettier.pdf) by Wadler (though my implementation is completely different). This achieves stack safety by `Concat` taking an `Iterable`, so each contained `Doc` can be produced on demand. `render` pulls from these iterators, keeping in memory only things that might print to the current line, but where the format hasn't been decided yet. As soon as the formatting of a group is decided, as much of its body as possible is written to the `java.io.Writer`. A very quick and dirty performance comparison had the new pretty printer about 20% slower. That's paying for both the stack safety and the added smarts. And I think there's still room for optimization if it becomes necessary. Here is a snippet of the IR generated by `test_ld_score_regression`, first on master, then this PR:; ```; (InsertFields; (SelectFields (SNP A1 A2 N Z); (Ref row)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9652:2935,enhance,enhancements,2935,https://hail.is,https://github.com/hail-is/hail/pull/9652,1,['enhance'],['enhancements']
Modifiability,"atch-jigold-yrxul be created in? This project will incur costs for storing your Hail generated data. (Example: hail-jigold): hail-jigold; Which region does your data reside in? (Example: us-central1): us-central1; Do you want to set a lifecycle policy (automatically delete files after a time period) on the bucket hail-batch-jigold-yrxul? [y/n]: y; After how many days should files be automatically deleted from bucket hail-batch-jigold-yrxul? (30): 15; Created bucket hail-batch-jigold-yrxul in project hail-jigold.; Updated bucket hail-batch-jigold-yrxul in project hail-jigold with lifecycle rule set to 15 days and labels {'bucket': 'hail-batch-jigold-yrxul', 'owner': 'jigold', 'data_type': 'temporary'}.; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-yrxul in project hail-jigold.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-yrxul/batch/tmp; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. Use an existing bucket and give permissions:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init ; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:1415,CONFIG,CONFIGURATION,1415,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['CONFIG'],['CONFIGURATION']
Modifiability,"atch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (90, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (94, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,598	job.py	schedule_job:443	schedule job (97, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,656	job.py	schedule_job:443	schedule job (99, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,662	job.py	schedule_job:443	schedule job (100, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,663	job.py	schedule_job:443	schedule job (98, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (101, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (102, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:33,456	hail_logging.py	log:40	https POST /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/activate done in 3.2369999999998527s: 200; ERROR	2022-03-02 19:06:33,492	job.py	schedule_job:473	error while scheduling job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvlo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:2415,config,config,2415,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['config'],['config']
Modifiability,"atch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (94, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,598	job.py	schedule_job:443	schedule job (97, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,656	job.py	schedule_job:443	schedule job (99, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,662	job.py	schedule_job:443	schedule job (100, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,663	job.py	schedule_job:443	schedule job (98, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (101, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (102, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:33,456	hail_logging.py	log:40	https POST /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/activate done in 3.2369999999998527s: 200; ERROR	2022-03-02 19:06:33,492	job.py	schedule_job:473	error while scheduling job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:2578,config,config,2578,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['config'],['config']
Modifiability,"ates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_client_ssl_context` which should only be used in; publicly consumable tools (*never* in a service). This function allows the same; tool to be used inside and outside the cluster. It will load the correct certs; for your environment (it will load public certs if you're outside the cluster,; it will load in-cluster-only certs if you're in the cluster). I also added types to `tls.py` and fixed some type errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9120:1817,config,configuration,1817,https://hail.is,https://github.com/hail-is/hail/pull/9120,1,['config'],['configuration']
Modifiability,atic/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/pkgs/nbclassic-0.5.6-pyhb4ecaf3_1/site-packages/nbclassic/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/pkgs/notebook-6.2.0-py38h578d9bd_0/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/pkgs/notebook-6.2.0-py38h578d9bd_0/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/lib/python3.8/site-packages/nbclassic/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/lib/python3.8/site-packages/nbclassic/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/google-fluentd/embedded/lib/ruby/gems/2.7.0/gems/fluent-plugin-kubernetes_metadata_filter-2.5.2/test/cassettes/kubernetes_docker_metadata_dotted_labels.yml; /opt/google-fluentd/embedded/lib/ruby/gems/2.7.0/gems/fluent-plugin-kubernetes_metadata_filter-2.5.2/test/cassettes/kubernetes_docker_metadata_annotations.yml; /usr/share/man/man1/gcloud_artifacts_docker_images_scan.1.gz; /usr/share/man/man1/gcloud_artifacts_docker_images_list-vulnerabilities.1.gz; /usr/share/man/man1/gcloud_beta_artifacts_docker_images_describe.1.gz; /usr/share/man/man1/gcloud_beta_artifacts_docker_images_scan.1.gz; /usr/share/man/man1/gcloud_alpha_auth_configure-docker.1.gz; /usr/share/man/man1/gcloud_beta_artifacts_docker_images.1.gz; /usr/share/man/man1/gcloud_beta_artifacts_docker_images_list.1.gz; /usr/share/man/man1/gcloud_artifacts_docker_images_delete.1.gz; /usr/share/man/man1/gcloud_beta_artifacts_docker_images_delete.1.gz; /usr/share/man/man1/gcloud_alpha_artifacts_docker_images.1.gz; /usr/share/man/man1/gcloud_beta_auth_configure-docker.1.gz; /usr/share/man/man1/gcloud,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:2071,plugin,plugin-,2071,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['plugin'],['plugin-']
Modifiability,"ation.k8s.io/v1alpha1</code> API version is removed; use the <code>rbac.authorization.k8s.io/v1</code> API, available since v1.8. The <code>scheduling.k8s.io/v1alpha1</code> API version is removed; use the <code>scheduling.k8s.io/v1</code> API, available since v1.14. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104248"">kubernetes/kubernetes#104248</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>Kube-scheduler: support for configuration file version <code>v1beta1</code> is removed. Update configuration files to v1beta2(xref: <a href=""https://github-redirect.dependabot.com/kubernetes/enhancements/issues/2901"">kubernetes/enhancements#2901</a>) or v1beta3 before upgrading to 1.23. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104782"">kubernetes/kubernetes#104782</a>, <a href=""https://github.com/kerthcet""><code>@​kerthcet</code></a>)</li>; <li>KubeSchedulerConfiguration provides a new field <code>MultiPoint</code> which will register a plugin for all valid extension points (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105611"">kubernetes/kubernetes#105611</a>, <a href=""https://github.com/damemi""><code>@​damemi</code></a>) [SIG Scheduling and Testing]</li>; <li>Kubelet should reject pods whose OS doesn't match the node's OS label. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105292"">kubernetes/kubernetes#105292</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>) [SIG Apps and Node]</li>; <li>Kubelet: turn the KubeletConfiguration v1beta1 <code>ResolverConfig</code> field from a <code>string</code> to <code>*string</code>. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104624"">kubernetes/kubernetes#104624</a>, <a href=""https://github.com/Haleygo""><code>@​Haleygo</code></a>)</li>; <li>Kubernetes is now built using go 1.17. (<a href=""https://github-redir",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:8932,plugin,plugin,8932,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['plugin'],['plugin']
Modifiability,"aultExecutor.execute(ServerImpl.java:201); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:15939,Plugin,Plugins,15939,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,aversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:2355,Rewrite,RewriteBottomUp,2355,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Rewrite'],['RewriteBottomUp']
Modifiability,aversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:3835,Rewrite,RewriteBottomUp,3835,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"aximum to begin with as the attempt start time was always the same. This is probably a place to double check before merging. **Job State Changes:**; - We now support a new state ""Creating"" which represents an instance has been spun up for a job, but it has not been activated yet; - The SQL user resources table and cancellable resources table needed to be changed to add the n_creating_jobs, n_cancellable_creating_jobs, and the n_cancelled_creating_jobs; - Added a new SQL function `mark_job_creating` that is only called by the job private instance creator.; - A lot of the existing SQL functions had to change to account for the fact that an instance in the pending state can have job-specific operations done such as mark_job_complete if the job is cancelled. In addition, the ""Creating"" state is like ""Running"" for some operations in that an attempt has been created and actions are happening on behalf of the user. **Driver Changes:**; - New cancel_creating_jobs event; - Two separate methods to get the pools or job private UI pages and two separate configuration methods. One each for pool and job-private. **JobPrivateInstanceCollection:**; - Has two new loops: an instance creation loop and a scheduling loop; - The instance creation loop does a fair share calculation that is almost identical to the pool one except the resource being allocated is n_ready_jobs compared to total_jobs rather than ready_cores_mcpu. ; - The instance creation loop needs to extract the machine_type, storage_gib, and preemptible from the spec without hitting GCS. Therefore, it is stored in the ""spec"" field in the database which required changing the batch format version a bit.; - We avoid double scheduling by requiring that there are no live instances assigned to attempts for that job before creating an instance.; - We mark a job as creating after creating the instance for the new attempt; - The number of instances that can be created is similar to the pool control loop. The total number of instances",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9972:2180,config,configuration,2180,https://hail.is,https://github.com/hail-is/hail/pull/9972,1,['config'],['configuration']
Modifiability,"b7dc1c</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8481"">#8481</a></li>; <li>fix(dev): avoid FOUC when swapping out link tag (fix <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7973"">#7973</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8495"">#8495</a>) (<a href=""https://github.com/vitejs/vite/commit/01fa807"">01fa807</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/7973"">#7973</a> <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8495"">#8495</a></li>; </ul>; <h2><!-- raw HTML omitted -->2.9.10 (2022-06-06)<!-- raw HTML omitted --></h2>; <ul>; <li>feat: treat Astro file scripts as TS (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8151"">#8151</a>) (<a href=""https://github.com/vitejs/vite/commit/9fdd0a3"">9fdd0a3</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8151"">#8151</a></li>; <li>feat: new hook <code>configurePreviewServer</code> (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7658"">#7658</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8437"">#8437</a>) (<a href=""https://github.com/vitejs/vite/commit/7b972bc"">7b972bc</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/7658"">#7658</a> <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8437"">#8437</a></li>; <li>fix: remove empty chunk css imports when using esnext (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8345"">#8345</a>) (<a href=""https://github.com/vitejs/vite/commit/9fbc1a9"">9fbc1a9</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8345"">#8345</a></li>; <li>fix: EPERM error on Windows when processing dependencies (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8235"">#8235</a>) (<a href=""https://github.com/vitejs/vite/commit/dfe4307"">dfe4307</a>), ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12142:3955,config,configurePreviewServer,3955,https://hail.is,https://github.com/hail-is/hail/pull/12142,2,['config'],['configurePreviewServer']
Modifiability,"base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:20530,Plugin,Plugins,20530,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:606,config,configuration,606,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914,1,['config'],['configuration']
Modifiability,"batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,584	job.py	schedule_job:443	schedule job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,585	job.py	schedule_job:443	schedule job (93, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (90, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (94, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,598	job.py	schedule_job:443	schedule job (97, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,656	job.py	schedule_job:443	schedule job (99, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,662	job.py	schedule_job:443	schedule job (100, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,663	job.py	schedule_job:443	schedule job (98, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (101, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (102, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:33,456	hail_logging.py	log:40	https POST /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/activate done in 3.2369999999998527s: 200; ERROR	2022-03-02 19:06:33,492	job.py	schedule_job:473	error while scheduling job (95, 1) on instance batch-worker-pr-11438-defaul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:2090,config,config,2090,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['config'],['config']
Modifiability,"batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,585	job.py	schedule_job:443	schedule job (93, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (90, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (94, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,598	job.py	schedule_job:443	schedule job (97, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,656	job.py	schedule_job:443	schedule job (99, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,662	job.py	schedule_job:443	schedule job (100, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,663	job.py	schedule_job:443	schedule job (98, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (101, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (102, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:33,456	hail_logging.py	log:40	https POST /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/activate done in 3.2369999999998527s: 200; ERROR	2022-03-02 19:06:33,492	job.py	schedule_job:473	error while scheduling job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:2253,config,config,2253,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['config'],['config']
