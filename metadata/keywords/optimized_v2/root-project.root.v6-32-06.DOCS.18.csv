quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,filename,wiki,url,total_similar,target_keywords,target_matched_words
Performance,"---------. If :program:`FileCheck` verifies that the file matches the expected contents,; it exits with 0. Otherwise, if not, or if an error occurs, it will exit with a; non-zero value. TUTORIAL; --------. FileCheck is typically used from LLVM regression tests, being invoked on the RUN; line of the test. A simple example of using FileCheck from a RUN line looks; like this:. .. code-block:: llvm. ; RUN: llvm-as < %s | llc -march=x86-64 | FileCheck %s. This syntax says to pipe the current file (""``%s``"") into ``llvm-as``, pipe; that into ``llc``, then pipe the output of ``llc`` into ``FileCheck``. This; means that FileCheck will be verifying its standard input (the llc output); against the filename argument specified (the original ``.ll`` file specified by; ""``%s``""). To see how this works, let's look at the rest of the ``.ll`` file; (after the RUN line):. .. code-block:: llvm. define void @sub1(i32* %p, i32 %v) {; entry:; ; CHECK: sub1:; ; CHECK: subl; %0 = tail call i32 @llvm.atomic.load.sub.i32.p0i32(i32* %p, i32 %v); ret void; }. define void @inc4(i64* %p) {; entry:; ; CHECK: inc4:; ; CHECK: incq; %0 = tail call i64 @llvm.atomic.load.add.i64.p0i64(i64* %p, i64 1); ret void; }. Here you can see some ""``CHECK:``"" lines specified in comments. Now you can; see how the file is piped into ``llvm-as``, then ``llc``, and the machine code; output is what we are verifying. FileCheck checks the machine code output to; verify that it matches what the ""``CHECK:``"" lines specify. The syntax of the ""``CHECK:``"" lines is very simple: they are fixed strings that; must occur in order. FileCheck defaults to ignoring horizontal whitespace; differences (e.g. a space is allowed to match a tab) but otherwise, the contents; of the ""``CHECK:``"" line is required to match some thing in the test file exactly. One nice thing about FileCheck (compared to grep) is that it allows merging; test cases together into logical groups. For example, because the test above; is checking for the ""``sub1:``""",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst:8597,load,load,8597,interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,1,['load'],['load']
Performance,"---------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===--------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3246,optimiz,optimization,3246,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,3,"['Optimiz', 'optimiz']","['OptimizeReturned', 'optimization']"
Performance,"--------. Clang supports ARC-style weak and unsafe references in Objective-C even; outside of ARC mode. Weak references must be explicitly enabled with; the ``-fobjc-weak`` option; use ``__has_feature((objc_arc_weak))``; to test whether they are enabled. Unsafe references are enabled; unconditionally. ARC-style weak and unsafe references cannot be used; when Objective-C garbage collection is enabled. Except as noted below, the language rules for the ``__weak`` and; ``__unsafe_unretained`` qualifiers (and the ``weak`` and; ``unsafe_unretained`` property attributes) are just as laid out; in the :doc:`ARC specification <AutomaticReferenceCounting>`.; In particular, note that some classes do not support forming weak; references to their instances, and note that special care must be; taken when storing weak references in memory where initialization; and deinitialization are outside the responsibility of the compiler; (such as in ``malloc``-ed memory). Loading from a ``__weak`` variable always implicitly retains the; loaded value. In non-ARC modes, this retain is normally balanced; by an implicit autorelease. This autorelease can be suppressed; by performing the load in the receiver position of a ``-retain``; message send (e.g. ``[weakReference retain]``); note that this performs; only a single retain (the retain done when primitively loading from; the weak reference). For the most part, ``__unsafe_unretained`` in non-ARC modes is just the; default behavior of variables and therefore is not needed. However,; it does have an effect on the semantics of block captures: normally,; copying a block which captures an Objective-C object or block pointer; causes the captured pointer to be retained or copied, respectively,; but that behavior is suppressed when the captured variable is qualified; with ``__unsafe_unretained``. Note that the ``__weak`` qualifier formerly meant the GC qualifier in; all non-ARC modes and was silently ignored outside of GC modes. It now; means the ARC-st",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:72561,load,loaded,72561,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['load'],['loaded']
Performance,"--------. Currently, the ``LoopInterchange`` pass does not use any metadata. Ambiguous Transformation Order; ==============================. If there multiple transformations defined, the order in which they are; executed depends on the order in LLVM's pass pipeline, which is subject; to change. The default optimization pipeline (anything higher than; ``-O0``) has the following order. When using the legacy pass manager:. - LoopInterchange (if enabled); - SimpleLoopUnroll/LoopFullUnroll (only performs full unrolling); - VersioningLICM (if enabled); - LoopDistribute; - LoopVectorizer; - LoopUnrollAndJam (if enabled); - LoopUnroll (partial and runtime unrolling). When using the legacy pass manager with LTO:. - LoopInterchange (if enabled); - SimpleLoopUnroll/LoopFullUnroll (only performs full unrolling); - LoopVectorizer; - LoopUnroll (partial and runtime unrolling). When using the new pass manager:. - SimpleLoopUnroll/LoopFullUnroll (only performs full unrolling); - LoopDistribute; - LoopVectorizer; - LoopUnrollAndJam (if enabled); - LoopUnroll (partial and runtime unrolling). Leftover Transformations; ========================. Forced transformations that have not been applied after the last; transformation pass should be reported to the user. The transformation; passes themselves cannot be responsible for this reporting because they; might not be in the pipeline, there might be multiple passes able to; apply a transformation (e.g. ``LoopInterchange`` and Polly) or a; transformation attribute may be 'hidden' inside another passes' followup; attribute. The pass ``-transform-warning`` (``WarnMissedTransformationsPass``); emits such warnings. It should be placed after the last transformation; pass. The current pass pipeline has a fixed order in which transformations; passes are executed. A transformation can be in the followup of a pass; that is executed later and thus leftover. For instance, a loop nest; cannot be distributed and then interchanged with the current pass; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst:14326,perform,performs,14326,interpreter/llvm-project/llvm/docs/TransformMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst,1,['perform'],['performs']
Performance,"--------. First, configure and build LLVM. Next, you need to create a new directory; somewhere in the LLVM source base. For this example, we'll assume that you; made ``lib/Transforms/Hello``. Finally, you must set up a build script; that will compile the source code for the new pass. To do this,; copy the following into ``CMakeLists.txt``:. .. code-block:: cmake. add_llvm_library( LLVMHello MODULE; Hello.cpp. PLUGIN_TOOL; opt; ). and the following line into ``lib/Transforms/CMakeLists.txt``:. .. code-block:: cmake. add_subdirectory(Hello). (Note that there is already a directory named ``Hello`` with a sample ""Hello""; pass; you may play with it -- in which case you don't need to modify any; ``CMakeLists.txt`` files -- or, if you want to create everything from scratch,; use another name.). This build script specifies that ``Hello.cpp`` file in the current directory; is to be compiled and linked into a shared object ``$(LEVEL)/lib/LLVMHello.so`` that; can be dynamically loaded by the :program:`opt` tool via its :option:`-load`; option. If your operating system uses a suffix other than ``.so`` (such as; Windows or macOS), the appropriate extension will be used. Now that we have the build scripts set up, we just need to write the code for; the pass itself. .. _writing-an-llvm-pass-basiccode:. Basic code required; -------------------. Now that we have a way to compile our new pass, we just have to write it.; Start out with:. .. code-block:: c++. #include ""llvm/Pass.h""; #include ""llvm/IR/Function.h""; #include ""llvm/Support/raw_ostream.h"". Which are needed because we are writing a `Pass; <https://llvm.org/doxygen/classllvm_1_1Pass.html>`_, we are operating on; `Function <https://llvm.org/doxygen/classllvm_1_1Function.html>`_\ s, and we will; be doing some printing. Next we have:. .. code-block:: c++. using namespace llvm;. ... which is required because the functions from the include files live in the; llvm namespace. Next we have:. .. code-block:: c++. namespace {. ... whic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:3447,load,loaded,3447,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,2,['load'],"['load', 'loaded']"
Performance,"--------. The Apple stage1 cache configures a two stage build similar to how Apple builds; the clang shipped with Xcode. The build files generated from this invocation has; a target named ""stage2"" which performs an LTO build of clang. The Apple-stage2 cache can be used directly to match the build settings Apple; uses in shipping builds without doing a full bootstrap build. PGO; ---. The PGO CMake cache can be used to generate a multi-stage instrumented compiler.; You can configure your build directory with the following invocation of CMake:. cmake -G <generator> -C <path_to_clang>/cmake/caches/PGO.cmake <source dir>. After configuration the following additional targets will be generated:. stage2-instrumented:; Builds a stage1 x86 compiler, runtime, and required tools (llvm-config,; llvm-profdata) then uses that compiler to build an instrumented stage2 compiler. stage2-instrumented-generate-profdata:; Depends on ""stage2-instrumented"" and will use the instrumented compiler to; generate profdata based on the training files in <clang>/utils/perf-training. stage2:; Depends on ""stage2-instrumented-generate-profdata"" and will use the stage1; compiler with the stage2 profdata to build a PGO-optimized compiler. stage2-check-llvm:; Depends on stage2 and runs check-llvm using the stage3 compiler. stage2-check-clang:; Depends on stage2 and runs check-clang using the stage3 compiler. stage2-check-all:; Depends on stage2 and runs check-all using the stage3 compiler. stage2-test-suite:; Depends on stage2 and runs the test-suite using the stage3 compiler (requires; in-tree test-suite). 3-stage; -------. This cache file can be used to generate a 3-stage clang build. You can configure; using the following CMake command:. cmake -C <path to clang>/cmake/caches/3-stage.cmake -G Ninja <path to llvm>. You can then run ""ninja stage3-clang"" to build stage1, stage2 and stage3 clangs. This is useful for finding non-determinism the compiler by verifying that stage2; and stage3 are identical.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/cmake/caches/README.txt:1695,optimiz,optimized,1695,interpreter/llvm-project/clang/cmake/caches/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/cmake/caches/README.txt,3,"['cache', 'optimiz']","['cache', 'caches', 'optimized']"
Performance,"--------; class Thing {; }; ------------- Defs -----------------. ```tablegen; // The cache is reset here so this is an error.; def AThing: Thing {}; ```. <stdin>:2:13: error: Couldn't find class 'Thing'; def AThing: Thing {}; ^. The default value is `off`, meaning cells are connected. If you want to override the default for one cell only, use the `%reset` or `%noreset` magic. These always override the default. ```tablegen; class Thing {}; ```. ------------- Classes -----------------; class Thing {; }; ------------- Defs -----------------. ```tablegen; %noreset; // This works because of the noreset above.; def AThing: Thing {}; ```. ------------- Classes -----------------; class Thing {; }; ------------- Defs -----------------; def AThing {	// Thing; }. ```tablegen; // This does not because we're not changing the default.; def AnotherThing: Thing {}; ```. <stdin>:2:19: error: Couldn't find class 'Thing'; def AnotherThing: Thing {}; ^. ```tablegen; %config cellreset off; %reset; // Here we have an empty cache and default reset behaviour.; ```. ------------- Classes -----------------; ------------- Defs -----------------. It is not valid to have `%reset` and `%noreset` in the same cell. ```tablegen; %reset; %noreset; ```. %reset and %noreset in the same cell is not allowed. Use only one, or neither. Consider setting `cellreset` to the majority usecase for your notebook. For example a tutorial building a large example across many cells will likely want it `off`. One with many standalone examples, `on`. There is a ""magic"" directive `%args` that you can use to send command line arguments to `llvm-tblgen`. For example, here we have some code that shows a warning. ```tablegen; %reset; class Thing <int A, int B> {; int num = A;; }; ```. <stdin>:1:25: warning: unused template argument: Thing:B; class Thing <int A, int B> {; ^. We can pass an argument to ignore that warning. ```tablegen; %args --no-warn-on-unused-template-args; ```. ------------- Classes -----------------; cl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/jupyter/LLVM_TableGen.md:2370,cache,cache,2370,interpreter/llvm-project/llvm/utils/TableGen/jupyter/LLVM_TableGen.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/jupyter/LLVM_TableGen.md,1,['cache'],['cache']
Performance,"-------. .. image:: ARM-BE-bitcastfail.png; :align: right. The main problem with the ``LD1`` solution is dealing with bitconverts (or bitcasts, or reinterpret casts). These are pseudo instructions that only change the compiler's interpretation of data, not the underlying data itself. A requirement is that if data is loaded and then saved again (called a ""round trip""), the memory contents should be the same after the store as before the load. If a vector is loaded and is then bitconverted to a different vector type before storing, the round trip will currently be broken. Take for example this code sequence::. %0 = load <4 x i32> %x; %1 = bitcast <4 x i32> %0 to <2 x i64>; store <2 x i64> %1, <2 x i64>* %y. This would produce a code sequence such as that in the figure on the right. The mismatched ``LD1`` and ``ST1`` cause the stored data to differ from the loaded data. .. container:: clearer. When we see a bitcast from type ``X`` to type ``Y``, what we need to do is to change the in-register representation of the data to be *as if* it had just been loaded by a ``LD1`` of type ``Y``. .. image:: ARM-BE-bitcastsuccess.png; :align: right. Conceptually this is simple - we can insert a ``REV`` undoing the ``LD1`` of type ``X`` (converting the in-register representation to the same as if it had been loaded by ``LDR``) and then insert another ``REV`` to change the representation to be as if it had been loaded by an ``LD1`` of type ``Y``. For the previous example, this would be::. LD1 v0.4s, [x]. REV64 v0.4s, v0.4s // There is no REV128 instruction, so it must be synthesizedcd; EXT v0.16b, v0.16b, v0.16b, #8 // with a REV64 then an EXT to swap the two 64-bit elements. REV64 v0.2d, v0.2d; EXT v0.16b, v0.16b, v0.16b, #8. ST1 v0.2d, [y]. It turns out that these ``REV`` pairs can, in almost all cases, be squashed together into a single ``REV``. For the example above, a ``REV128 4s`` + ``REV128 2d`` is actually a ``REV64 4s``, as shown in the figure on the right. .. [1] One lane ve",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:10835,load,loaded,10835,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,1,['load'],['loaded']
Performance,"-------. :doc:`WritingAnLLVMPass`; Information on how to write LLVM transformations and analyses. :doc:`WritingAnLLVMNewPMPass`; Information on how to write LLVM transformations under the new pass; manager. :doc:`Passes`; A list of optimizations and analyses implemented in LLVM. :doc:`StackSafetyAnalysis`; This document describes the design of the stack safety analysis of local; variables. :doc:`MergeFunctions`; Describes functions merging optimization. :doc:`AliasAnalysis`; Information on how to write a new alias analysis implementation or how to; use existing analyses. :doc:`MemorySSA`; Information about the MemorySSA utility in LLVM, as well as how to use it. :doc:`LoopTerminology`; A document describing Loops and associated terms as used in LLVM. :doc:`CycleTerminology`; A document describing cycles as a generalization of loops. :doc:`Vectorizers`; This document describes the current status of vectorization in LLVM. :doc:`LinkTimeOptimization`; This document describes the interface between LLVM intermodular optimizer; and the linker and its design. :doc:`GoldPlugin`; How to build your programs with link-time optimization on Linux. :doc:`Remarks`; A reference on the implementation of remarks in LLVM. :doc:`Source Level Debugging with LLVM <SourceLevelDebugging>`; This document describes the design and philosophy behind the LLVM; source-level debugger. :doc:`How to Update Debug Info <HowToUpdateDebugInfo>`; This document specifies how to correctly update debug info in various kinds; of code transformations. :doc:`InstrRefDebugInfo`; This document explains how LLVM uses value tracking, or instruction; referencing, to determine variable locations for debug info in the final; stages of compilation. :doc:`RemoveDIsDebugInfo`; This is a migration guide describing how to move from debug info using; intrinsics such as dbg.value to using the non-instruction DPValue object. :doc:`InstrProfileFormat`; This document explains two binary formats of instrumentation-based profile",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/UserGuides.rst:3648,optimiz,optimizer,3648,interpreter/llvm-project/llvm/docs/UserGuides.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/UserGuides.rst,1,['optimiz'],['optimizer']
Performance,"-----; ``undef`` is the LLVM way of representing a value that is not defined. You; can get these if you do not initialize a variable before you use it. For; example, the C function:. .. code-block:: c. int X() { int i; return i; }. Is compiled to ""``ret i32 undef``"" because ""``i``"" never has a value specified; for it. Why does instcombine + simplifycfg turn a call to a function with a mismatched calling convention into ""unreachable""? Why not make the verifier reject it?; ----------------------------------------------------------------------------------------------------------------------------------------------------------; This is a common problem run into by authors of front-ends that are using; custom calling conventions: you need to make sure to set the right calling; convention on both the function and on each call to the function. For; example, this code:. .. code-block:: llvm. define fastcc void @foo() {; ret void; }; define void @bar() {; call void @foo(); ret void; }. Is optimized to:. .. code-block:: llvm. define fastcc void @foo() {; ret void; }; define void @bar() {; unreachable; }. ... with ""``opt -instcombine -simplifycfg``"". This often bites people because; ""all their code disappears"". Setting the calling convention on the caller and; callee is required for indirect calls to work, so people often ask why not; make the verifier reject this sort of thing. The answer is that this code has undefined behavior, but it is not illegal.; If we made it illegal, then every transformation that could potentially create; this would have to ensure that it doesn't, and there is valid code that can; create this sort of construct (in dead code). The sorts of things that can; cause this to happen are fairly contrived, but we still need to accept them.; Here's an example:. .. code-block:: llvm. define fastcc void @foo() {; ret void; }; define internal void @bar(void()* %FP, i1 %cond) {; br i1 %cond, label %T, label %F; T:; call void %FP(); ret void; F:; call fastcc void ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FAQ.rst:9698,optimiz,optimized,9698,interpreter/llvm-project/llvm/docs/FAQ.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FAQ.rst,1,['optimiz'],['optimized']
Performance,"----. * Upgrade cppyy-cling to 6.18.2; * Various patches to upstream's pre-compiled header generation and use; * Instantiate templates with larger integer types if argument values require; * Improve cppyy.interactive and partially enable it on PyPy, IPython, etc.; * Let ``__overload__`` be more flexible in signature matching; * Make list filtering of dir(cppyy.gbl) on Windows same as Linux/Mac; * Extended documentation. 2019-08-18: 1.5.0; -----------------. * Upgrade cppyy-cling to 6.18.0; * Allow python-derived classes to be used in templates; * Stricter template resolution and better caching/performance; * Detailed memory management for make_shared and shared_ptr; * Two-way memory management for cross-inherited objects; * Reduced memory footprint of proxy objects in most common cases; * Allow implicit conversion from a tuple of arguments; * Data set on namespaces reflected on C++ even if data not yet bound; * Generalized resolution of binary operators in wrapper generation; * Proper naming of arguments in namespaces for ``std::function<>``; * Cover more cases of STL-liker iterators; * Allow ``std::vector`` initialization with a list of constructor arguments; * Consistent naming of ``__cppname__`` to ``__cpp_name__``; * Added ``__set_lifeline__`` attribute to overloads; * Fixes to the cmake fragments for Ubuntu; * Fixes linker errors on Windows in some configurations; * Support C++ naming of typedef of bool types; * Basic views of 2D arrays of builtin types; * Extended documentation. 2019-07-01 : 1.4.12; -------------------. * Automatic conversion of python functions to ``std::function`` arguments; * Fix for templated operators that can map to different python names; * Fix on p3 crash when setting a detailed exception during exception handling; * Fix lookup of ``std::nullopt``; * Fix bug that prevented certain templated constructors from being considered; * Support for enum values as data members on ""enum class"" enums; * Support for implicit conversion when passing ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/changelog.rst:17740,perform,performance,17740,bindings/pyroot/cppyy/cppyy/doc/source/changelog.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/changelog.rst,1,['perform'],['performance']
Performance,"----. LLVM includes several alias-analysis driven transformations which can be used; with any of the implementations above. The ``-adce`` pass; ^^^^^^^^^^^^^^^^^^. The ``-adce`` pass, which implements Aggressive Dead Code Elimination uses the; ``AliasAnalysis`` interface to delete calls to functions that do not have; side-effects and are not used. The ``-licm`` pass; ^^^^^^^^^^^^^^^^^^. The ``-licm`` pass implements various Loop Invariant Code Motion related; transformations. It uses the ``AliasAnalysis`` interface for several different; transformations:. * It uses mod/ref information to hoist or sink load instructions out of loops if; there are no instructions in the loop that modifies the memory loaded. * It uses mod/ref information to hoist function calls out of loops that do not; write to memory and are loop-invariant. * It uses alias information to promote memory objects that are loaded and stored; to in loops to live in a register instead. It can do this if there are no may; aliases to the loaded/stored memory location. The ``-argpromotion`` pass; ^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``-argpromotion`` pass promotes by-reference arguments to be passed in; by-value instead. In particular, if pointer arguments are only loaded from it; passes in the value loaded instead of the address to the function. This pass; uses alias information to make sure that the value loaded from the argument; pointer is not modified between the entry of the function and any load of the; pointer. The ``-gvn``, ``-memcpyopt``, and ``-dse`` passes; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. These passes use AliasAnalysis information to reason about loads and stores. .. _the clients:. Clients for debugging and evaluation of implementations; -------------------------------------------------------. These passes are useful for evaluating the various alias analysis; implementations. You can use them with commands like:. .. code-block:: bash. % opt -ds-aa -aa-eval foo.bc -disable-output -sta",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst:28174,load,loaded,28174,interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,1,['load'],['loaded']
Performance,"----middle end----------------┼----backend----┤; │ │ │ │; └---parsing----sema----codegen--┴----- transformations ---- codegen ----┴---- codegen --┘. ┌---------------------------------------------------------------------------------------┐; | │; | source file │; | │; └---------------------------------------------------------------------------------------┘. ┌--------┐; │ │; │imported│; │ │; │ code │; │ │; └--------┘. Here we can see that the source file (could be a non-module unit or a module unit) would get processed by the; whole pipeline.; But the imported code would only get involved in semantic analysis, which is mainly about name lookup,; overload resolution and template instantiation.; All of these processes are fast relative to the whole compilation process.; More importantly, the imported code only needs to be processed once in frontend code generation,; as well as the whole middle end and backend.; So we could get a big win for the compilation time in O0. But with optimizations, things are different:. (we omit ``code generation`` part for each end due to the limited space). .. code-block:: none. ├-------- frontend ---------┼--------------- middle end --------------------┼------ backend ----┤; │ │ │ │; └--- parsing ---- sema -----┴--- optimizations --- IPO ---- optimizations---┴--- optimizations -┘. ┌-----------------------------------------------------------------------------------------------┐; │ │; │ source file │; │ │; └-----------------------------------------------------------------------------------------------┘; ┌---------------------------------------┐; │ │; │ │; │ imported code │; │ │; │ │; └---------------------------------------┘. It would be very unfortunate if we end up with worse performance after using modules.; The main concern is that when we compile a source file, the compiler needs to see the function body; of imported module units so that it can perform IPO (InterProcedural Optimization, primarily inlining; in practice) to optimize functio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst:41396,optimiz,optimizations,41396,interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/StandardCPlusPlusModules.rst,1,['optimiz'],['optimizations']
Performance,"--.; | 0x00001000 | BUCKETS[0]; | 0x00002000 | BUCKETS[1]; | 0x00002200 | BUCKETS[2]; | 0x000034f0 | BUCKETS[3]; | | ...; | 0xXXXXXXXX | BUCKETS[n_buckets]; '------------'. So for ``bucket[3]`` in the example above, we have an offset into the table; 0x000034f0 which points to a chain of entries for the bucket. Each bucket must; contain a next pointer, full 32 bit hash value, the string itself, and the data; for the current string value. .. code-block:: none. .------------.; 0x000034f0: | 0x00003500 | next pointer; | 0x12345678 | 32 bit hash; | ""erase"" | string value; | data[n] | HashData for this bucket; |------------|; 0x00003500: | 0x00003550 | next pointer; | 0x29273623 | 32 bit hash; | ""dump"" | string value; | data[n] | HashData for this bucket; |------------|; 0x00003550: | 0x00000000 | next pointer; | 0x82638293 | 32 bit hash; | ""main"" | string value; | data[n] | HashData for this bucket; `------------'. The problem with this layout for debuggers is that we need to optimize for the; negative lookup case where the symbol we're searching for is not present. So; if we were to lookup ""``printf``"" in the table above, we would make a 32-bit; hash for ""``printf``"", it might match ``bucket[3]``. We would need to go to; the offset 0x000034f0 and start looking to see if our 32 bit hash matches. To; do so, we need to read the next pointer, then read the hash, compare it, and; skip to the next bucket. Each time we are skipping many bytes in memory and; touching new pages just to do the compare on the full 32 bit hash. All of; these accesses then tell us that we didn't have a match. Name Hash Tables; """""""""""""""""""""""""""""""". To solve the issues mentioned above we have structured the hash tables a bit; differently: a header, buckets, an array of all unique 32 bit hash values,; followed by an array of hash value data offsets, one for each hash value, then; the data for all hash values:. .. code-block:: none. .-------------.; | HEADER |; |-------------|; | BUCKETS |; |-------------|",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:62164,optimiz,optimize,62164,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimize']
Performance,"--===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits shrinks the ""and"" constant to 2 but instcombine misses the; icmp transform. //===---------------------------------------------------------------------===//. This code:. typedef struct {; int f1:1;; int f2:1;; int f3:1;; int f4:29;; } t1;. typedef struct {; int f1:1;; int f2:1;; int f3:30;; } t2;. t1 s1;; t2 s2;. void func1(void); {; s1.f1 = s2.f1;; s1.f2 = s2.f2;; }. Compiles into this IR (on x86-64 at least):. %struct.t1 = type { i8, [3 x i8] }; @s2 = global %struct.t1 zeroinitializer, align 4; @s1 = global %struct.t1 zeroinitializer, align 4; define void @func1() nounwind ssp noredzone {; entry:; %0 = load i32* bitcast (%struct.t1* @s2 to i32*), align 4; %bf.val.sext5 = and i32 %0, 1; %1 = load i32* bitcast (%struct.t1* @s1 to i32*), align 4; %2 = and i32 %1, -4; %3 = or i32 %2, %bf.val.sext5; %bf.val.sext26 = and i32 %0, 2; %4 = or i32 %3, %bf.val.sext26; store i32 %4, i32* bitcast (%struct.t1* @s1 to i32*), align 4; ret void; }. The two or/and's should be merged into one each. //===---------------------------------------------------------------------===//. Machine level code hoisting can be useful in some cases. For example, PR9408; is about:. typedef union {; void (*f1)(int);; void (*f2)(long);; } funcs;. void foo(funcs f, int which) {; int a = 5;; if (which) {; f.f1(a);; } else {; f.f2(a);; }; }. which we compile to:. foo: # @foo; # %bb.0: # %entry; pushq %rbp; movq %rsp, %rbp; testl %esi, %esi; movq %rdi, %rax; je .LBB0_2; # %bb.1: # %if.then; movl $5, %edi; callq *%rax; popq %rbp; ret; .LBB0_2: # %if.else; movl $5, %edi; callq *%rax; popq %rbp; ret. Note that bb1 and bb2 are the same. This doesn't happen at the IR level; because one call is passing an i32 and the o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:66279,load,load,66279,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['load'],['load']
Performance,"--enable-gold=default`` to the above configure invocation; to automatically install the newly built gold as the default linker with; ``make install``. * Build the LLVMgold plugin. Run CMake with; ``-DLLVM_BINUTILS_INCDIR=/path/to/binutils/include``. The correct include; path will contain the file ``plugin-api.h``. Usage; =====. You should produce bitcode files from ``clang`` with the option; ``-flto``. This flag will also cause ``clang`` to look for the gold plugin in; the ``lib`` directory under its prefix and pass the ``-plugin`` option to; ``ld``. It will not look for an alternate linker without ``-fuse-ld=gold``,; which is why you otherwise need gold to be the installed system linker in; your path. ``ar`` and ``nm`` also accept the ``-plugin`` option and it's possible to; to install ``LLVMgold.so`` to ``/usr/lib/bfd-plugins`` for a seamless setup.; If you built your own gold, be sure to install the ``ar`` and ``nm-new`` you; built to ``/usr/bin``. Example of link time optimization; ---------------------------------. The following example shows a worked example of the gold plugin mixing LLVM; bitcode and native code. .. code-block:: c. --- a.c ---; #include <stdio.h>. extern void foo1(void);; extern void foo4(void);. void foo2(void) {; printf(""Foo2\n"");; }. void foo3(void) {; foo4();; }. int main(void) {; foo1();; }. --- b.c ---; #include <stdio.h>. extern void foo2(void);. void foo1(void) {; foo2();; }. void foo4(void) {; printf(""Foo4"");; }. .. code-block:: bash. --- command lines ---; $ clang -flto a.c -c -o a.o # <-- a.o is LLVM bitcode file; $ ar q a.a a.o # <-- a.a is an archive with LLVM bitcode; $ clang b.c -c -o b.o # <-- b.o is native object file; $ clang -flto a.a b.o -o main # <-- link with LLVMgold plugin. Gold informs the plugin that foo3 is never referenced outside the IR,; leading LLVM to delete that function. However, unlike in the :ref:`libLTO; example <libLTO-example>` gold does not currently eliminate foo4. Quickstart for using LTO with autotoo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GoldPlugin.rst:3730,optimiz,optimization,3730,interpreter/llvm-project/llvm/docs/GoldPlugin.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GoldPlugin.rst,1,['optimiz'],['optimization']
Performance,"-. The advertised premise of Numba is that it ""makes Python code fast.""; However, there is a much more compelling reason: Numba allows developers to; stay in their chosen ecosystem, be it Python or C++, in mixed environments,; without paying for their choice in lost performance.; For example, a Python developer using Numba does not need to rewrite a kernel; into C++ just to run performantly in a C++ framework.; Similarly, a C++ developer can use Numba to compile and create function; pointers to Python code for easy, performant, access.; This becomes even more compelling if the deployment target is a GPU, which; would otherwise certainly require a rewrite of the Python code.; Add that Numba, as a JIT-compiler, is fully run-time just like ``cppyy``,; and the use case for integration is clear.; (Numba does not currently provide support for C++.). Usage; -------. ``cppyy`` does not use Numba extension hooks to minimize accidental; dependencies.; Instead, it requires that the extensions are loaded explicitly by any code; that uses it::. import cppyy.numba_ext. After that, Numba is able to trace ``cppyy`` bound code when applying the; usual ``numba.njit`` decorator. Numba type declarations are done lazily, with the ``numba_ext`` module only; initially registering hooks on proxy base classes, to keep overheads in; Numba's type-resolution to a minimum.; On use in a JITed trace, each C++ type or function call is refined to the; actual, concrete types and type-specific overloads, with templates; instantiated as-needed.; Where possible, lowering is kept generic to reduce the number of callbacks; in Numba's compilation chain. Examples; --------. The following, non-exhaustive, set of examples gives an idea of the; current level of support.; More examples can be found in the `test suite`_. C++ free (global) functions can be called and overloads will be selected, or; a template will be instantiated, based on the provided types.; Exact type matches are fully supported, there is some",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/numba.rst:2379,load,loaded,2379,bindings/pyroot/cppyy/cppyy/doc/source/numba.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/numba.rst,1,['load'],['loaded']
Performance,"-===//. clang -O3 -fno-exceptions currently compiles this code:. struct S {; unsigned short m1, m2;; unsigned char m3, m4;; };. void f(int N) {; std::vector<S> v(N);; extern void sink(void*); sink(&v);; }. into poor code for zero-initializing 'v' when N is >0. The problem is that; S is only 6 bytes, but each element is 8 byte-aligned. We generate a loop and; 4 stores on each iteration. If the struct were 8 bytes, this gets turned into; a memset. In order to handle this we have to:; A) Teach clang to generate metadata for memsets of structs that have holes in; them.; B) Teach clang to use such a memset for zero init of this struct (since it has; a hole), instead of doing elementwise zeroing. //===---------------------------------------------------------------------===//. clang -O3 currently compiles this code:. extern const int magic;; double f() { return 0.0 * magic; }. into. @magic = external constant i32. define double @_Z1fv() nounwind readnone {; entry:; %tmp = load i32* @magic, align 4, !tbaa !0; %conv = sitofp i32 %tmp to double; %mul = fmul double %conv, 0.000000e+00; ret double %mul; }. We should be able to fold away this fmul to 0.0. More generally, fmul(x,0.0); can be folded to 0.0 if we can prove that the LHS is not -0.0, not a NaN, and; not an INF. The CannotBeNegativeZero predicate in value tracking should be; extended to support general ""fpclassify"" operations that can return ; yes/no/unknown for each of these predicates. In this predicate, we know that uitofp is trivially never NaN or -0.0, and; we know that it isn't +/-Inf if the floating point type has enough exponent bits; to represent the largest integer value as < inf. //===---------------------------------------------------------------------===//. When optimizing a transformation that can change the sign of 0.0 (such as the; 0.0*val -> 0.0 transformation above), it might be provable that the sign of the; expression doesn't matter. For example, by the above rules, we can't transform; fmul(sitofp(",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:60598,load,load,60598,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['load'],['load']
Performance,-Optimization-Cpu COMMAND testOptimizationCpu). # DNN - MethodDL SGD Optimization CPU; ROOT_EXECUTABLE(testMethodDLSGDOptimizationCpu TestMethodDLSGDOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-SGD-Optimization-Cpu COMMAND testMethodDLSGDOptimizationCpu). # DNN - MethodDL Adam Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdamOptimizationCpu TestMethodDLAdamOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adam-Optimization-Cpu COMMAND testMethodDLAdamOptimizationCpu TIMEOUT 1800). # DNN - MethodDL Adagrad Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdagradOptimizationCpu TestMethodDLAdagradOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adagrad-Optimization-Cpu COMMAND testMethodDLAdagradOptimizationCpu). # DNN - MethodDL RMSProp Optimization CPU; ROOT_EXECUTABLE(testMethodDLRMSPropOptimizationCpu TestMethodDLRMSPropOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-RMSProp-Optimization-Cpu COMMAND testMethodDLRMSPropOptimizationCpu). # DNN - MethodDL Adadelta Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdadeltaOptimizationCpu TestMethodDLAdadeltaOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adadelta-Optimization-Cpu COMMAND testMethodDLAdadeltaOptimizationCpu). # DNN - Regression CPU; ROOT_EXECUTABLE(testRegressionCpu TestRegressionMethodDL.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Regression-Cpu COMMAND testRegressionCpu). #( old-dnn-test ); # DNN - DataLoader CPU; ROOT_EXECUTABLE(testDataLoaderCpu TestDataLoaderCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Data-Loader-Cpu COMMAND testDataLoaderCpu). # DNN - Minimization CPU; ROOT_EXECUTABLE(testMinimizationCpu TestMinimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Minimization-Cpu COMMAND testMinimizationCpu). # tests using TReference architecture; if ( reference-tests). # DNN - Activation Functions; ROOT_EXECUTABLE(testActivatio,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt:6504,Optimiz,Optimization-Cpu,6504,tmva/tmva/test/DNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt,1,['Optimiz'],['Optimization-Cpu']
Performance,"-bisect process. The OptBisect object that manages the process is; entirely passive and has no knowledge of how any pass is implemented. When a; pass is run if the pass may be skipped, it should call the OptBisect object to; see if it should be skipped. The OptBisect object is intended to be accessed through LLVMContext and each; Pass base class contains a helper function that abstracts the details in order; to make this check uniform across all passes. These helper functions are:. .. code-block:: c++. bool ModulePass::skipModule(Module &M);; bool CallGraphSCCPass::skipSCC(CallGraphSCC &SCC);; bool FunctionPass::skipFunction(const Function &F);; bool LoopPass::skipLoop(const Loop *L);. A MachineFunctionPass should use FunctionPass::skipFunction() as such:. .. code-block:: c++. bool MyMachineFunctionPass::runOnMachineFunction(Function &MF) {; if (skipFunction(*MF.getFunction()); return false;; // Otherwise, run the pass normally.; }. In addition to checking with the OptBisect class to see if the pass should be; skipped, the skipFunction(), skipLoop() and skipBasicBlock() helper functions; also look for the presence of the ""optnone"" function attribute. The calling; pass will be unable to determine whether it is being skipped because the; ""optnone"" attribute is present or because the opt-bisect-limit has been; reached. This is desirable because the behavior should be the same in either; case. The majority of LLVM passes which can be skipped have already been instrumented; in the manner described above. If you are adding a new pass or believe you; have found a pass which is not being included in the opt-bisect process but; should be, you can add it as described above. Adding Finer Granularity; ========================. Once the pass in which an incorrect transformation is performed has been; determined, it may be useful to perform further analysis in order to determine; which specific transformation is causing the problem. Debug counters; can be used for this purpose.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:8752,perform,performed,8752,interpreter/llvm-project/llvm/docs/OptBisect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst,2,['perform'],"['perform', 'performed']"
Performance,"-block:: c. #include <SomeLib.h>. The implementation is handled separately by linking against the appropriate library. For example, by passing ``-lSomeLib`` to the linker. Modules provide an alternative, simpler way to use software libraries that provides better compile-time scalability and eliminates many of the problems inherent to using the C preprocessor to access the API of a library. Problems with the current model; -------------------------------; The ``#include`` mechanism provided by the C preprocessor is a very poor way to access the API of a library, for a number of reasons:. * **Compile-time scalability**: Each time a header is included, the; compiler must preprocess and parse the text in that header and every; header it includes, transitively. This process must be repeated for; every translation unit in the application, which involves a huge; amount of redundant work. In a project with *N* translation units; and *M* headers included in each translation unit, the compiler is; performing *M x N* work even though most of the *M* headers are; shared among multiple translation units. C++ is particularly bad,; because the compilation model for templates forces a huge amount of; code into headers. * **Fragility**: ``#include`` directives are treated as textual; inclusion by the preprocessor, and are therefore subject to any; active macro definitions at the time of inclusion. If any of the; active macro definitions happens to collide with a name in the; library, it can break the library API or cause compilation failures; in the library header itself. For an extreme example,; ``#define std ""The C++ Standard""`` and then include a standard; library header: the result is a horrific cascade of failures in the; C++ Standard Library's implementation. More subtle real-world; problems occur when the headers for two different libraries interact; due to macro collisions, and users are forced to reorder; ``#include`` directives or introduce ``#undef`` directives to break; t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:1504,perform,performing,1504,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['perform'],['performing']
Performance,"-block:: c. lto_module_dispose(lto_module_t). The linker can introspect the non-native object file by getting the number of; symbols and getting the name and attributes of each symbol via:. .. code-block:: c. lto_module_get_num_symbols(lto_module_t); lto_module_get_symbol_name(lto_module_t, unsigned int); lto_module_get_symbol_attribute(lto_module_t, unsigned int). The attributes of a symbol include the alignment, visibility, and kind. Tools working with object files on Darwin (e.g. lipo) may need to know properties like the CPU type:. .. code-block:: c. lto_module_get_macho_cputype(lto_module_t mod, unsigned int *out_cputype, unsigned int *out_cpusubtype). ``lto_code_gen_t``; ------------------. Once the linker has loaded each non-native object files into an; ``lto_module_t``, it can request ``libLTO`` to process them all and generate a; native object file. This is done in a couple of steps. First, a code generator; is created with:. .. code-block:: c. lto_codegen_create(). Then, each non-native object file is added to the code generator with:. .. code-block:: c. lto_codegen_add_module(lto_code_gen_t, lto_module_t). The linker then has the option of setting some codegen options. Whether or not; to generate DWARF debug info is set with:. .. code-block:: c. lto_codegen_set_debug_model(lto_code_gen_t). which kind of position independence is set with:. .. code-block:: c. lto_codegen_set_pic_model(lto_code_gen_t). And each symbol that is referenced by a native object file or otherwise must not; be optimized away is set with:. .. code-block:: c. lto_codegen_add_must_preserve_symbol(lto_code_gen_t, const char*). After all these settings are done, the linker requests that a native object file; be created from the modules with the settings using:. .. code-block:: c. lto_codegen_compile(lto_code_gen_t, size*). which returns a pointer to a buffer containing the generated native object file.; The linker then parses that and links it with the rest of the native object; files.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:10822,optimiz,optimized,10822,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,1,['optimiz'],['optimized']
Performance,"-block:: text. {<2 x i64>, <2 x i64>} llvm.experimental.vector.deinterleave2.v4i64(<4 x i64> <i64 0, i64 1, i64 2, i64 3>); ==> {<2 x i64> <i64 0, i64 2>, <2 x i64> <i64 1, i64 3>}. Arguments:; """""""""""""""""""". The argument is a vector whose type corresponds to the logical concatenation of; the two result types. '``llvm.experimental.vector.interleave2``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <4 x double> @llvm.experimental.vector.interleave2.v4f64(<2 x double> %vec1, <2 x double> %vec2); declare <vscale x 8 x i32> @llvm.experimental.vector.interleave2.nxv8i32(<vscale x 4 x i32> %vec1, <vscale x 4 x i32> %vec2). Overview:; """""""""""""""""". The '``llvm.experimental.vector.interleave2``' intrinsic constructs a vector; by interleaving two input vectors. This intrinsic works for both fixed and scalable vectors. While this intrinsic; supports all vector types the recommended way to express this operation for; fixed-width vectors is still to use a shufflevector, as that may allow for more; optimization opportunities. For example:. .. code-block:: text. <4 x i64> llvm.experimental.vector.interleave2.v4i64(<2 x i64> <i64 0, i64 2>, <2 x i64> <i64 1, i64 3>); ==> <4 x i64> <i64 0, i64 1, i64 2, i64 3>. Arguments:; """"""""""""""""""""; Both arguments must be vectors of the same type whereby their logical; concatenation matches the result type. '``llvm.experimental.cttz.elts``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ```llvm.experimental.cttz.elts```; on any vector of integer elements, both fixed width and scalable. ::. declare i8 @llvm.experimental.cttz.elts.i8.v8i1(<8 x i1> <src>, i1 <is_zero_poison>). Overview:; """""""""""""""""". The '``llvm.experimental.cttz.elts``' intrinsic counts the number of trailing; zero elements of a vector. Arguments:; """""""""""""""""""". The first argument is the vector to be counted. This argument must be a vector;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:668736,optimiz,optimization,668736,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===----------------------------------------------------------------------===//. If we want to tie instruction selection into the scheduler, we can do some; constant formation with different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-pass scheduler. //===----------------------------------------------------------------------===//. For this",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:2083,load,load,2083,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,2,['load'],['load']
Performance,"-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33405,LOAD,LOAD,33405,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,1,['LOAD'],['LOAD']
Performance,"-exporting symbols from several other JITDylibs. .. _Laziness:. Laziness; ========. Laziness in ORC is provided by a utility called ""lazy reexports"". A lazy; reexport is similar to a regular reexport or alias: It provides a new name for; an existing symbol. Unlike regular reexports however, lookups of lazy reexports; do not trigger immediate materialization of the reexported symbol. Instead, they; only trigger materialization of a function stub. This function stub is; initialized to point at a *lazy call-through*, which provides reentry into the; JIT. If the stub is called at runtime then the lazy call-through will look up; the reexported symbol (triggering materialization for it if necessary), update; the stub (to call directly to the reexported symbol on subsequent calls), and; then return via the reexported symbol. By re-using the existing symbol lookup; mechanism, lazy reexports inherit the same concurrency guarantees: calls to lazy; reexports can be made from multiple threads concurrently, and the reexported; symbol can be any state of compilation (uncompiled, already in the process of; being compiled, or already compiled) and the call will succeed. This allows; laziness to be safely mixed with features like remote compilation, concurrent; compilation, concurrent JIT'd code, and speculative compilation. There is one other key difference between regular reexports and lazy reexports; that some clients must be aware of: The address of a lazy reexport will be; *different* from the address of the reexported symbol (whereas a regular; reexport is guaranteed to have the same address as the reexported symbol).; Clients who care about pointer equality will generally want to use the address; of the reexport as the canonical address of the reexported symbol. This will; allow the address to be taken without forcing materialization of the reexport. Usage example:. If JITDylib ``JD`` contains definitions for symbols ``foo_body`` and; ``bar_body``, we can create lazy entry poi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:17414,concurren,concurrency,17414,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,2,['concurren'],"['concurrency', 'concurrently']"
Performance,"-fimplicit-module-maps -fmodules-cache-path=prebuilt; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt -DENABLE_A; find prebuilt -name ""*.pcm""; # prebuilt/1AYBIGPM8R2GA/A-3L1K4LUA6O31.pcm; # prebuilt/1AYBIGPM8R2GA/B-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/A-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/B-3L1K4LUA6O31.pcm; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -DENABLE_A. Finally we want to allow implicit modules for configurations that were not prebuilt. When using the clang driver a module cache path is implicitly selected. Using ``-cc1``, we simply add use the ``-fmodules-cache-path`` option. .. code-block:: sh. clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache -DENABLE_A; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache -DENABLE_A -DOTHER_OPTIONS. This way, a single directory containing multiple variants of modules can be prepared and reused. The options configuring the module cache are independent of other options. Module Semantics; ================. Modules are modeled as if each submodule were a separate translation unit, and a module import makes names from the other translation unit visible. Each submodule starts with a new preprocessor state and an empty translation unit. .. note::. This behavior is currently only approximated when building a module with submodules. Entities within a submodule that has already been built are visibl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:23539,cache,cache-path,23539,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,2,['cache'],"['cache', 'cache-path']"
Performance,"-gfx11-table`. .. table:: AMDHSA Memory Model Code Sequences GFX10-GFX11; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx10-gfx11-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX10-GFX11; Space; ============ ============ ============== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; slc=1 dlc=1. - If GFX10, omit dlc=1. - volatile. 1. buffer/global/flat_load; glc=1 dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1 dlc=1. - If GFX10, omit dlc=1. - volatile. 1. buffer/global/flat_store; dlc=1. - If GFX10, omit dlc=1. 2. s_waitcnt vscnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:344173,load,load,344173,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations.; Completion of load/store/sample operations are reported to a wavefront in; execution order of other load/store/sample operations performed by that; wavefront.; * The vector memory operations access a vector L0 cache. There is a single L0; cache per CU. Each SIMD of a CU accesses the same L0 cache. Therefore, no; special action is required for coherence between the lanes of a single; wavefront. However, a ``buffer_gl0_inv`` is required for coherence between; wavefronts executing in the same work-group as they may be executing on SIMDs; of different CUs that access different L0s. A ``buffer_gl0_inv`` is also; required for coherence between wavefronts executing in different work-groups; as they may be executing on different WGPs.; * The scalar memory operations access a scalar L0 cache shared by all wavefronts; on a WGP. The scalar and vector L0 caches are not coherent. However, scalar; operations are used in a restricted way so do not impact the memory model. See; :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory L0 caches use an L1 cache shared by all WGPs on; the same SA. Therefore, no special action is required for coherence between; the wavefronts of a single work-group. However, a ``buffer_gl1_inv`` is; required for coherence between wavefronts executing in different work-groups; as they may be executing on different SAs that access different L1s.; * The L1 caches have independent quadrants to service disjoint ranges of virtual; addresses.; * Each L0 cache has a separate request queue per L1 quadrant. Therefore, the; vector and scalar memory operations performed by different wavefronts, whether; executing in the same or different work-groups (which may be executing on; different CUs accessing different L0s), can be reordered relative to each; other. A ``s_waitcnt vmcnt(0) & vscnt(0)`` is required to en",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:337440,cache,caches,337440,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"-id-x"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.workgroup.id.x intrinsic. ""amdgpu-no-workgroup-id-y"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.workgroup.id.y intrinsic. ""amdgpu-no-workgroup-id-z"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.workgroup.id.z intrinsic. ""amdgpu-no-dispatch-ptr"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.dispatch.ptr intrinsic. ""amdgpu-no-implicitarg-ptr"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.implicitarg.ptr intrinsic. ""amdgpu-no-dispatch-id"" The same as amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.dispatch.id intrinsic. ""amdgpu-no-queue-ptr"" Similar to amdgpu-no-workitem-id-x, except for the; llvm.amdgcn.queue.ptr intrinsic. Note that unlike the other ABI hint; attributes, the queue pointer may be required in situations where the; intrinsic call does not directly appear in the program. Some subtargets; require the queue pointer for to handle some addrspacecasts, as well; as the llvm.amdgcn.is.shared, llvm.amdgcn.is.private, llvm.trap, and; llvm.debug intrinsics. ""amdgpu-no-hostcall-ptr"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the pointer to the hostcall buffer. If this; attribute is absent, then the amdgpu-no-implicitarg-ptr is also removed. ""amdgpu-no-heap-ptr"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the pointer to an initialized memory buffer; that conforms to the requirements of the malloc/free device library V1; version implementation. If this attribute is absent, then the; amdgpu-no-implicitarg-ptr is also removed. ""amdgpu-no-multigrid-sync-arg"" Similar to amdgpu-no-implicitarg-ptr, except specific to the implicit; kernel argument that holds the multigrid synchronization pointer. If this; attribute is absent, then the amdgpu-no-implicitarg-ptr is also removed. ""amdgpu-no-default-queue"" Similar to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:49887,queue,queue,49887,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"-memory-model-single-thread-optimization-constraints-table. ============ ==============================================================; LLVM Memory Optimization Constraints; Ordering; ============ ==============================================================; unordered *none*; monotonic *none*; acquire - If a load atomic/atomicrmw then no following load/load; atomic/store/store atomic/atomicrmw/fence instruction can be; moved before the acquire.; - If a fence then same as load atomic, plus no preceding; associated fence-paired-atomic can be moved after the fence.; release - If a store atomic/atomicrmw then no preceding load/load; atomic/store/store atomic/atomicrmw/fence instruction can be; moved after the release.; - If a fence then same as store atomic, plus no following; associated fence-paired-atomic can be moved before the; fence.; acq_rel Same constraints as both acquire and release.; seq_cst - If a load atomic then same constraints as acquire, plus no; preceding sequentially consistent load atomic/store; atomic/atomicrmw/fence instruction can be moved after the; seq_cst.; - If a store atomic then the same constraints as release, plus; no following sequentially consistent load atomic/store; atomic/atomicrmw/fence instruction can be moved before the; seq_cst.; - If an atomicrmw/fence then same constraints as acq_rel.; ============ ==============================================================. The code sequences used to implement the memory model are defined in the; following sections:. * :ref:`amdgpu-amdhsa-memory-model-gfx6-gfx9`; * :ref:`amdgpu-amdhsa-memory-model-gfx90a`; * :ref:`amdgpu-amdhsa-memory-model-gfx942`; * :ref:`amdgpu-amdhsa-memory-model-gfx10-gfx11`. .. _amdgpu-amdhsa-memory-model-gfx6-gfx9:. Memory Model GFX6-GFX9; ++++++++++++++++++++++. For GFX6-GFX9:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:205362,load,load,205362,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"-null pointers tagged with; ``dereferenceable_or_null(<n>)`` are ``dereferenceable(<n>)``.; For address space 0 ``dereferenceable_or_null(<n>)`` implies that; a pointer is exactly one of ``dereferenceable(<n>)`` or ``null``,; and in other address spaces ``dereferenceable_or_null(<n>)``; implies that a pointer is at least one of ``dereferenceable(<n>)``; or ``null`` (i.e. it may be both ``null`` and; ``dereferenceable(<n>)``). This attribute may only be applied to; pointer typed parameters. ``swiftself``; This indicates that the parameter is the self/context parameter. This is not; a valid attribute for return values and can only be applied to one; parameter. .. _swiftasync:. ``swiftasync``; This indicates that the parameter is the asynchronous context parameter and; triggers the creation of a target-specific extended frame record to store; this pointer. This is not a valid attribute for return values and can only; be applied to one parameter. ``swifterror``; This attribute is motivated to model and optimize Swift error handling. It; can be applied to a parameter with pointer to pointer type or a; pointer-sized alloca. At the call site, the actual argument that corresponds; to a ``swifterror`` parameter has to come from a ``swifterror`` alloca or; the ``swifterror`` parameter of the caller. A ``swifterror`` value (either; the parameter or the alloca) can only be loaded and stored from, or used as; a ``swifterror`` argument. This is not a valid attribute for return values; and can only be applied to one parameter. These constraints allow the calling convention to optimize access to; ``swifterror`` variables by associating them with a specific register at; call boundaries rather than placing them in memory. Since this does change; the calling convention, a function which uses the ``swifterror`` attribute; on a parameter is not ABI-compatible with one which does not. These constraints also allow LLVM to assume that a ``swifterror`` argument; does not alias any other mem",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:63197,optimiz,optimize,63197,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimize']
Performance,"-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not it should be skipped; each time it is invoked and each invocation will be assigned a unique value.; For example, if a FunctionPass is used with a module containing three functions; a different index value will be assigned to the pass for each of the functions; as the pass is run. The pass may be run on two functions but skipped for the; third. If the pass internally performs operations on a smaller IR unit the pass must be; specifically instrumented to enable bisection at this finer level of granularity; (see below for details). Example Usage; =============. .. code-block:: console. $ opt -O2 -o test-opt.bc -opt-bisect-limit=16 test.ll. BISECT: running pass (1) Simplify the CFG on function (g); BISECT: running pass (2) SROA on function (g); BISECT: running pass (3) Early CSE",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:4291,optimiz,optimizations,4291,interpreter/llvm-project/llvm/docs/OptBisect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst,2,['optimiz'],['optimizations']
Performance,"-point; operations, such as fused multiply-add (FMA). Fused operations are; permitted to produce more precise results than performing the same; operations separately. The C standard permits intermediate floating-point results within an; expression to be computed with more precision than their type would; normally allow. This permits operation fusing, and Clang takes advantage; of this by default. This behavior can be controlled with the ``FP_CONTRACT``; and ``clang fp contract`` pragmas. Please refer to the pragma documentation; for a description of how the pragmas interact with this option. Valid values are:. * ``fast`` (fuse across statements disregarding pragmas, default for CUDA); * ``on`` (fuse in the same statement unless dictated by pragmas, default for languages other than CUDA/HIP); * ``off`` (never fuse); * ``fast-honor-pragmas`` (fuse across statements unless dictated by pragmas, default for HIP). .. option:: -f[no-]honor-infinities. Allow floating-point optimizations that assume arguments and results are; not +-Inf.; Defaults to ``-fhonor-infinities``. If both ``-fno-honor-infinities`` and ``-fno-honor-nans`` are used,; has the same effect as specifying ``-ffinite-math-only``. .. option:: -f[no-]honor-nans. Allow floating-point optimizations that assume arguments and results are; not NaNs.; Defaults to ``-fhonor-nans``. If both ``-fno-honor-infinities`` and ``-fno-honor-nans`` are used,; has the same effect as specifying ``-ffinite-math-only``. .. option:: -f[no-]approx-func. Allow certain math function calls (such as ``log``, ``sqrt``, ``pow``, etc); to be replaced with an approximately equivalent set of instructions; or alternative math function calls. For example, a ``pow(x, 0.25)``; may be replaced with ``sqrt(sqrt(x))``, despite being an inexact result; in cases where ``x`` is ``-0.0`` or ``-inf``.; Defaults to ``-fno-approx-func``. .. option:: -f[no-]signed-zeros. Allow optimizations that ignore the sign of floating point zeros.; Defaults to ``-fsig",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:57770,optimiz,optimizations,57770,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"-project/root/issues/15156)] - Segfault in TMVA::Experimental::SOFIE::RModel::Streamer; * [[#15139](https://github.com/root-project/root/issues/15139)] - cmake option -Dall=YES breaks the cmake; * [[#15118](https://github.com/root-project/root/issues/15118)] - jsoninterface does not build if provided with RapidYAML; * [[#15108](https://github.com/root-project/root/issues/15108)] - Turn off clang-format for Linkdef files; * [[#15090](https://github.com/root-project/root/issues/15090)] - TClass::GetClassInfo() is not thread safe; * [[#15077](https://github.com/root-project/root/issues/15077)] - Passing different floating point types to `RVec` utility functions; * [[#15048](https://github.com/root-project/root/issues/15048)] - [ntuple] Handling of virtual inheritance broken; * [[#15040](https://github.com/root-project/root/issues/15040)] - [RDataFrame] Inaccurate example of progress bar from documentation; * [[#15028](https://github.com/root-project/root/issues/15028)] - [RDataFrame] Unable to cacheread remote file; * [[#15027](https://github.com/root-project/root/issues/15027)] - spurrious cmake message about AfterImage with -Dminimal=ON; * [[#14981](https://github.com/root-project/root/issues/14981)] - RVecs leak memory with np.asarray in pyROOT; * [[#14964](https://github.com/root-project/root/issues/14964)] - ROOT-HEAD fails with ""cling interactive line includer >>>: fatal error: module file '[snip]/Vc.pcm' not found: module file not found""; * [[#14958](https://github.com/root-project/root/issues/14958)] - ROOT_HEAD failed with error message: Fail to detect cryptographic random generator; * [[#14921](https://github.com/root-project/root/issues/14921)] - ROOT Fails to build macOS 14.4 arm64 Xcode 15.3; * [[#14914](https://github.com/root-project/root/issues/14914)] - VecOps::Take with default argument doesn't check correctly the out of boundary condition; * [[#14910](https://github.com/root-project/root/issues/14910)] - hadd issue when using parallelization together",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md:27272,cache,cacheread,27272,README/ReleaseNotes/v632/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md,1,['cache'],['cacheread']
Performance,"-state`). For; GFX9-GFX11 the aperture base addresses are directly available as inline; constant registers ``SRC_SHARED_BASE/LIMIT`` and ``SRC_PRIVATE_BASE/LIMIT``.; In 64-bit address mode the aperture sizes are 2^32 bytes and the base is; aligned to 2^32 which makes it easier to convert from flat to segment or; segment to flat. A global address space address has the same value when used as a flat address; so no conversion is needed. **Global and Constant**; The global and constant address spaces both use global virtual addresses,; which are the same virtual address space used by the CPU. However, some; virtual addresses may only be accessible to the CPU, some only accessible; by the GPU, and some by both. Using the constant address space indicates that the data will not change; during the execution of the kernel. This allows scalar read instructions to; be used. As the constant address space could only be modified on the host; side, a generic pointer loaded from the constant address space is safe to be; assumed as a global pointer since only the device global memory is visible; and managed on the host side. The vector and scalar L1 caches are invalidated; of volatile data before each kernel dispatch execution to allow constant; memory to change values between kernel dispatches. **Region**; The region address space uses the hardware Global Data Store (GDS). All; wavefronts executing on the same device will access the same memory for any; given region address. However, the same region address accessed by wavefronts; executing on different devices will access different memory. It is higher; performance than global memory. It is allocated by the runtime. The data; store (DS) instructions can be used to access it. **Local**; The local address space uses the hardware Local Data Store (LDS) which is; automatically allocated when the hardware creates the wavefronts of a; work-group, and freed when all the wavefronts of a work-group have; terminated. All wavefronts belongin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:25154,load,loaded,25154,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loaded']
Performance,-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.h; clang-tools-extra/clang-tidy/performance/UnnecessaryCopyInitialization.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.h; clang-tools-extra/clang-tidy/plugin/ClangTidyPlugin.cpp; clang-tools-extra/clang-tidy/portability/PortabilityTidyModule.cpp; clang-tools-extra/clang-tidy/portability/RestrictSystemIncludesCheck.cpp; clang-tools-extra/clang-tidy/portability/SIMDIntrinsicsCheck.cpp; clang-tools-extra/clang-tidy/readability/AvoidConstParamsInDecls.h; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.cpp; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.h; clang-tools-extra/clang-tidy/readability/ConstReturnTypeCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerContainsCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerContainsCheck.h; clang-tools-extra/clang-tidy/readability/ContainerDataPointerCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerDataPointerCheck.h; clang-tools-extra/clang-tidy/readability/ContainerSizeEmptyCheck,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:66241,perform,performance,66241,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,". ## Glossary. The following glossary is adapted from the description of the Rogue Wave; `Threads.h`++ package. A **`process`** is a program that is loaded into memory and prepared for; execution. Each process has a private address space. Processes begin; with a single thread. A **`thread`** is a sequence of instructions being executed in a; program. A thread has a program counter and a private stack to keep; track of local variables and return addresses. A multithreaded process; is associated with one or more threads. Threads execute independently.; All threads in a given process share the private address space of that; process. **`Concurrency`** exists when at least two threads are in progress at; the same time. A system with only a single processor can support; concurrency by switching execution contexts among multiple threads. **`Parallelism`** arises when at least two threads are executing; simultaneously. This requires a system with multiple processors.; Parallelism implies concurrency, but not vice-versa. A function is **`reentrant`** if it will behave correctly even if a; thread of execution enters the function while one or more threads are; already executing within the function. These could be the same thread,; in the case of recursion, or different threads, in the case of; concurrency. **`Thread-specific data`** (**`TSD`**) is also known as thread-local; storage (TLS). Normally, any data that has lifetime beyond the local; variables on the thread's private stack are shared among all threads; within the process. Thread-specific data is a form of static or global; data that is maintained on a per-thread basis. That is, each thread gets; its own private copy of the data. Left to their own devices, threads execute independently.; **`Synchronization`** is the work that must be done when there are, in; fact, interdependencies that require some form of communication among; threads. Synchronization tools include mutexes, semaphores, condition; variables, and other",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md:20524,concurren,concurrency,20524,documentation/users-guide/Threads.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md,1,['concurren'],['concurrency']
Performance,". **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 2 of the ""Building an ORC-based JIT in LLVM"" tutorial. In; `Chapter 1 <BuildingAJIT1.html>`_ of this series we examined a basic JIT; class, KaleidoscopeJIT, that could take LLVM IR modules as input and produce; executable code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimizati",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:1522,optimiz,optimizing,1522,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimizing']
Performance,". .. _arc.misc.interior:. Interior pointers; -----------------. An Objective-C method returning a non-retainable pointer may be annotated with; the ``objc_returns_inner_pointer`` attribute to indicate that it returns a; handle to the internal data of an object, and that this reference will be; invalidated if the object is destroyed. When such a message is sent to an; object, the object's lifetime will be extended until at least the earliest of:. * the last use of the returned pointer, or any pointer derived from it, in the; calling function or; * the autorelease pool is restored to a previous state. .. admonition:: Rationale. Rationale: not all memory and resources are managed with reference counts; it; is common for objects to manage private resources in their own, private way.; Typically these resources are completely encapsulated within the object, but; some classes offer their users direct access for efficiency. If ARC is not; aware of methods that return such ""interior"" pointers, its optimizations can; cause the owning object to be reclaimed too soon. This attribute informs ARC; that it must tread lightly. The extension rules are somewhat intentionally vague. The autorelease pool; limit is there to permit a simple implementation to simply retain and; autorelease the receiver. The other limit permits some amount of; optimization. The phrase ""derived from"" is intended to encompass the results; both of pointer transformations, such as casts and arithmetic, and of loading; from such derived pointers; furthermore, it applies whether or not such; derivations are applied directly in the calling code or by other utility code; (for example, the C library routine ``strchr``). However, the implementation; never need account for uses after a return from the code which calls the; method returning an interior pointer. As an exception, no extension is required if the receiver is loaded directly; from a ``__strong`` object with :ref:`precise lifetime semantics; <arc.optimizatio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:100537,optimiz,optimizations,100537,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimizations']
Performance,". .. code-block:: c++. struct HostS { ... };; struct DeviceS { ... };. #ifdef __NVCC__; #ifndef __CUDA_ARCH__; __host__ HostS MakeStruct() { return HostS(); }; #else; __device__ DeviceS MakeStruct() { return DeviceS(); }; #endif; #else; __host__ HostS MakeStruct() { return HostS(); }; __device__ DeviceS MakeStruct() { return DeviceS(); }; #endif. // Now host and device code can call MakeStruct(). Hopefully you don't have to do this sort of thing often. Optimizations; =============. Modern CPUs and GPUs are architecturally quite different, so code that's fast; on a CPU isn't necessarily fast on a GPU. We've made a number of changes to; LLVM to make it generate good GPU code. Among these changes are:. * `Straight-line scalar optimizations <https://goo.gl/4Rb9As>`_ -- These; reduce redundancy within straight-line code. * `Aggressive speculative execution; <https://llvm.org/docs/doxygen/html/SpeculativeExecution_8cpp_source.html>`_; -- This is mainly for promoting straight-line scalar optimizations, which are; most effective on code along dominator paths. * `Memory space inference; <https://llvm.org/doxygen/NVPTXInferAddressSpaces_8cpp_source.html>`_ --; In PTX, we can operate on pointers that are in a particular ""address space""; (global, shared, constant, or local), or we can operate on pointers in the; ""generic"" address space, which can point to anything. Operations in a; non-generic address space are faster, but pointers in CUDA are not explicitly; annotated with their address space, so it's up to LLVM to infer it where; possible. * `Bypassing 64-bit divides; <https://llvm.org/docs/doxygen/html/BypassSlowDivision_8cpp_source.html>`_ --; This was an existing optimization that we enabled for the PTX backend. 64-bit integer divides are much slower than 32-bit ones on NVIDIA GPUs.; Many of the 64-bit divides in our benchmarks have a divisor and dividend; which fit in 32-bits at runtime. This optimization provides a fast path for; this common case. * Aggressive loop unroll",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CompileCudaWithLLVM.rst:18700,optimiz,optimizations,18700,interpreter/llvm-project/llvm/docs/CompileCudaWithLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CompileCudaWithLLVM.rst,1,['optimiz'],['optimizations']
Performance,". .. code-block:: text. #ifdef GET_CTable_DECL; const CEntry *lookupCEntryByEncoding(uint16_t Encoding);; #endif. #ifdef GET_CTable_IMPL; constexpr CEntry CTable[] = {; { ""Apple"", CFoo, 0xA }, // 0; { ""Apple"", CBar, 0xD }, // 1; { ""Pear"", CBaz, 0xF }, // 2; };. const CEntry *lookupCEntryByEncoding(uint16_t Encoding) {; struct KeyType {; uint16_t Encoding;; };; KeyType Key = { Encoding };; auto Table = ArrayRef(CTable);; auto Idx = std::lower_bound(Table.begin(), Table.end(), Key,; [](const CEntry &LHS, const KeyType &RHS) {; if (LHS.Encoding < RHS.Encoding); return true;; if (LHS.Encoding > RHS.Encoding); return false;; return false;; });. if (Idx == Table.end() ||; Key.Encoding != Idx->Encoding); return nullptr;; return &*Idx;; }. The ``PrimaryKeyEarlyOut`` field, when set to 1, modifies the lookup; function so that it tests the first field of the primary key to determine; whether it is within the range of the collected records' primary keys. If; not, the function returns the null pointer without performing the binary; search. This is useful for tables that provide data for only some of the; elements of a larger enum-based space. The first field of the primary key; must be an integral type; it cannot be a string. Adding ``let PrimaryKeyEarlyOut = 1`` to the ``ATable`` above:. .. code-block:: text. def ATable : GenericTable {; let FilterClass = ""AEntry"";; let Fields = [""Str"", ""Val1"", ""Val2""];; let PrimaryKey = [""Val1"", ""Val2""];; let PrimaryKeyName = ""lookupATableByValues"";; let PrimaryKeyEarlyOut = 1;; }. causes the lookup function to change as follows:. .. code-block:: text. const AEntry *lookupATableByValues(uint8_t Val1, uint16_t Val2) {; if ((Val1 < 0x2) ||; (Val1 > 0x5)); return nullptr;. struct KeyType {; ... We can construct two GenericTables with the same ``FilterClass``, so that they; select from the same overall set of records, but assign them with different; ``FilterClassField`` values so that they include different subsets of the; records of that class. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:29397,perform,performing,29397,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,1,['perform'],['performing']
Performance,". ; Networking. NET; ; TWebFile. Several pptimizations in TWebFile improving performance especially for TTree::Map() by about 35%. This has been achieved with a better caching strategy for request strings (especially avoiding to recalculate the auth base64 encoding), and with a drastic optimization in reading the response headers.; Fixes in the counting of the bytes read. TWebSystem. New implementation of TSystem allowing to use TSystem::AccessPathName() and GetPathInfo() to check if a web file exists and to get its size. Directory browsing is not available yet. NETX; ; TXNetFile. Several fixes and optimisations, mainly in the use of the cache; Fix an offset issue affecting the use of the cache with files in archives. TXNetSystem. A few optimizations in the use of retry mechanism, path locality checks, file online checks. XROOTD. Import a new version of XROOTD (20091202-0509); ; Fixes in bulk prepare and sync readv operations; Add support for 'make install' / 'make uninstall' and; other improvements in configure.classic; Several improvements / fixes:; ; reduced memory and CPU consumption;; extreme cp optimizations;; windows porting; new cache policies on the client side; new listing features implemented recently in the 'cns' module.; optimizations in cmsd and cnsd (performance improvements); support for openssl 1.0.0 (required by Fedora 12). Support for if/else if/else/fi constructs; Several portability fixes; ; Support 32-bit builds with icc on 64-bit platforms; Improved detection of libreadline and lib(n)curses. Increase the flexibility for configuring with an external xrootd; ; Add standard switches to disentangle lib and inc dirs;       --with-xrootd-incdir=<path_to dir_containing_XrdVersion.hh>;       --with-xrootd-libdir=<path_to_dir_containing_xrootd_plugins_and_libs>; ; When; passing a global xrootd dir with --with-xrootd, check both; src/XrdVersion.hh and include/xrootd/XrdVersion.hh so that both build; and install distributions are supported. Fix a problem ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/doc/v526/index.html:77,perform,performance,77,net/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/doc/v526/index.html,10,"['cache', 'optimiz', 'perform']","['cache', 'optimization', 'optimizations', 'performance']"
Performance,". ; PROOF. Warning. The; classes; TProofDataSetManager; and TProofDataSetManagerFile have been renamed; TDataSetManager; and TDataSetManagerFile. New; functionality. Add support for session; queuing in the scheduler. This; allows to control the number of sessions allowed to process queries; concurrently. The feature is enabled by a new parameter 'queue:fifo' in; the 'xpd.schedparam'; directive. In case of static worker assignment; (default, random,; round-robin) the max number of running sessions can be limited by; another new parameter 'mxrun';; for; example;            ;     xpd.schedparam default; mxrun:3 queue:fifo; will run concurrently only 3 sessions. Additional requests are queued; and run as soon as one of the running; sessions goes idle. The current policy is FIFO, so that there is a; rotation among queued; sessions. In the case of load-based worker assignment, the max number; of running; queries is determined dynamically.; Add support for repeat functionality in the xrd.worker; directive. To avoid repeating the same line N times; one can just add; 'repeat=N'; in the line; for; example;            ;     xpd.worker worker; proofwrks:2093 repeat=4; will define 4 workers on port 2093 of machine 'proofwrks'.; Add support for port specification via the directive; 'xpd.port'; Enable variable; substitution in 'xpd.' directives using the standard; Scalla mechanism described in; http://xrootd.slac.stanford.edu/doc/dev/Syntax_config.htm .; Build also a binary named 'xproofd' which runs; a xrootd; daemon with only the XrdProofdProtocol (i.e. no data serving).; This simplifies setups when data serving is not needed and also allows; to better disantagle problems related to one specific protocol. The new; binary accepts the same arguments as 'xrootd' and parses the same; directives form the same configuration file, with the exception of; 'xpd.protocol xproofd libXrdProofd.so' which should now be dropped. AN; alternative port can be specified via the new 'xpd.port' direct",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v524/index.html:292,concurren,concurrently,292,proof/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v524/index.html,14,"['concurren', 'load', 'queue']","['concurrently', 'load-based', 'queue', 'queued']"
Performance,". ; Proof. New functionality. TProofMgr. Add support for the following functionality:. sandbox file listing and browsing; sandbox file removal; file upload, download. TProofDraw. Allow to set a color, size, size, width for lines,; area, markers; the attributes are transmitted via the input list and; automatically derived from the ones of the chain. Support automatic creation of a dataset out of files; created on the worker nodes by worker processes. The implementation is; an extension of the functionality of the class TProofOutputFile used; for merging via file.; Add the possibility to enable/disable the tree cache and; to change its size on per-query base; two new parameters are available:. PROOF_UseTreeCache   ; Int_t       ; Enable (0) or Disable (1) the tree cache (default 1); PROOF_CacheSize      ; Long64_t     Cache size in bytes; (default 10000000). Examples:;        ; a) to disable the cache for the next run enter:;                                 ; proof->SetParameter(""PROOF_UseTreeCache"", 0);        ; b) to set the cache size to 20M;                                 ; proof->SetParameter(""PROOF_CacheSize"", 20000000);  Add the parameter; PROOF_UseParallelUnzip to toggle the use of the parallel unzip; (default off for now); to enable it add the following call;            ;            ;        ;  proof->SetParameter(""PROOF_UseParallelUnzip"", 1).  Add the possibility to give indications about; the number of workers at startup.;  E.g.;        1. To; start max 5 workers;             ; TProof::Open(""<master>"",""workers=5"");        2. To; start max 2 workers per physical machine;             ; TProof::Open(""<master>"",""workers=2x"");      This is useful in general when; running tests (equivalent but quicker then full startup;      followed by; TProof::SetParallel(n) or TProof::DeactivateWorker(...)).; Add support for the worker SysInfo_t in TSlaveInfo; (obtained via TProof::GetListOfSlaveInfos()); Add new submerger functionality to speed up the merging; phase. At the e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html:617,cache,cache,617,proof/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html,5,"['Cache', 'cache']","['Cache', 'cache']"
Performance,". ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-by",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2465,optimiz,optimizations,2465,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,2,['optimiz'],['optimizations']
Performance,". An alloca instruction can be used to represent a function scoped stack slot,; but can also represent dynamic frame expansion. When representing function; scoped variables or locations, placing alloca instructions at the beginning of; the entry block should be preferred. In particular, place them before any; call instructions. Call instructions might get inlined and replaced with; multiple basic blocks. The end result is that a following alloca instruction; would no longer be in the entry basic block afterward. The SROA (Scalar Replacement Of Aggregates) and Mem2Reg passes only attempt; to eliminate alloca instructions that are in the entry basic block. Given; SSA is the canonical form expected by much of the optimizer; if allocas can; not be eliminated by Mem2Reg or SROA, the optimizer is likely to be less; effective than it could be. Avoid loads and stores of large aggregate type; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. LLVM currently does not optimize well loads and stores of large :ref:`aggregate; types <t_aggregate>` (i.e. structs and arrays). As an alternative, consider; loading individual fields from memory. Aggregates that are smaller than the largest (performant) load or store; instruction supported by the targeted hardware are well supported. These can; be an effective way to represent collections of small packed fields. Prefer zext over sext when legal; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. On some architectures (X86_64 is one), sign extension can involve an extra; instruction whereas zero extension can be folded into a load. LLVM will try to; replace a sext with a zext when it can be proven safe, but if you have; information in your source language about the range of an integer value, it can; be profitable to use a zext rather than a sext. Alternatively, you can :ref:`specify the range of the value using metadata; <range-metadata>` and LLVM can do the sext to zext conversion for you. Zext GEP indices to machine register width; ^^^^^^^^^^^^^^^^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:3114,optimiz,optimize,3114,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,2,"['load', 'optimiz']","['loads', 'optimize']"
Performance,". Another place this surprises people is if you; have two types in a high-level language that have the same structure; (e.g. two different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code generator"" and implement the high-level optimizations; you desire in your front-end, on the language-specific AST. Tips and Tricks; ===============. There is a variety of useful tips and tricks that you come to know after; working on/with LLVM that aren't obvious at first glance. Instead of; letting everyone rediscover them, this section talks about some of these; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:10477,optimiz,optimization,10477,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,1,['optimiz'],['optimization']
Performance,". Available Checkers. Available Checkers; The analyzer performs checks that are categorized into families or ""checkers"". The; default set of checkers covers a variety of checks targeted at finding security; and API usage bugs, dead code, and other logic errors. See the; Default Checkers list below. In addition to; these, the analyzer contains a number of ; Experimental (Alpha) Checkers. Writeups with examples of some of the bugs that the analyzer finds. Bug Finding With Clang: 5 Resources To Get You Started; Finding Memory Leaks With The LLVM/Clang Static Analyzer; Under the Microscope - The Clang Static Analyzer; Mike Ash - Using the Clang Static Analyzer. Default Checkers. Core Checkers model core language features and perform general-purpose checks such as division by zero, null pointer dereference, usage of uninitialized values, etc.; C++ Checkers perform C++-specific checks; Dead Code Checkers check for unused code; Nullability Checkers ; Optin Checkers ; OS X Checkers perform Objective-C-specific checks and check the use of Apple's SDKs (OS X and iOS); Security Checkers check for insecure API usage and perform checks based on the CERT Secure Coding Standards; Unix Checkers check the use of Unix and POSIX APIs. Core Checkers. Name, DescriptionExample. core.CallAndMessage; (C, C++, ObjC); Check for logical errors for function calls and Objective-C message expressions; (e.g., uninitialized arguments, null function pointers). // C; struct S {; int x;; };. void f(struct S s);. void test() {; struct S s;; f(s); // warn: passed-by-value arg contain uninitialized data; }. // C; void test() {; void (*foo)(void);; foo(); // warn: function pointer is uninitialized; }. // C; void test() {; void (*foo)(void);; foo = 0;; foo(); // warn: function pointer is null; }. // C++; class C {; public:; void f();; };. void test() {; C *pc;; pc->f(); // warn: object pointer is uninitialized; }. // C++; class C {; public:; void f();; };. void test() {; C *pc = 0;; pc->f(); // warn: objec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/available_checks.html:55,perform,performs,55,interpreter/llvm-project/clang/www/analyzer/available_checks.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/available_checks.html,4,['perform'],"['perform', 'performs']"
Performance,". Checker Developer Manual. This Page Is Under Construction; Checker Developer Manual; The static analyzer engine performs path-sensitive exploration of the program and; relies on a set of checkers to implement the logic for detecting and; constructing specific bug reports. Anyone who is interested in implementing their own; checker, should check out the Building a Checker in 24 Hours talk; (slides; video); and refer to this page for additional information on writing a checker. The static analyzer is a; part of the Clang project, so consult Hacking on Clang; and LLVM Programmer's Manual; for developer guidelines and post your questions and proposals to the; Static Analyzer subcategory at; the official LLVM Discourse server. Getting Started; Static Analyzer Overview. Interaction with Checkers; Representing Values. Idea for a Checker; Checker Registration; Events, Callbacks, and Checker Class Structure; Custom Program States; Bug Reports; AST Visitors; Testing; Useful Commands/Debugging Hints. Attaching the Debugger; Narrowing Down the Problem; Visualizing the Analysis; Debug Prints and Tricks. Additional Sources of Information; Useful Links. Getting Started. To check out the source code and build the project, follow steps 1-4 of; the Clang Getting Started; page.; The analyzer source code is located under the Clang source tree:; ; $ cd llvm/tools/clang. See: include/clang/StaticAnalyzer, lib/StaticAnalyzer,; test/Analysis.; The analyzer regression tests can be executed from the Clang's build; directory:; ; $ cd ../../../; cd build/tools/clang; TESTDIRS=Analysis make test. Analyze a file with the specified checker:; ; $ clang -cc1 -analyze -analyzer-checker=core.DivideZero test.c. List the available checkers:; ; $ clang -cc1 -analyzer-checker-help. See the analyzer help for different output formats, fine tuning, and; debug options:; ; $ clang -cc1 -help | grep ""analyzer"". Static Analyzer Overview; The analyzer core performs symbolic execution of the given program. All t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/checker_dev_manual.html:114,perform,performs,114,interpreter/llvm-project/clang/www/analyzer/checker_dev_manual.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/checker_dev_manual.html,2,['perform'],['performs']
Performance,". Clang - C++ Defect Report Status. C++ Defect Report Support in Clang. C++ defect report implementation status; This page tracks which C++ defect reports are implemented within Clang. Number; Status; Issue title; Available in Clang?. 1; TC1; What if two using-declarations refer to the same function but the declarations introduce different default-arguments?; No. 2; drafting; How can dependent names be used in member declarations that appear outside of the class template definition?; Not resolved. 3; NAD; The template compilation model rules render some explicit specialization declarations not visible during instantiation; Yes. 4; CD1; Does extern ""C"" affect the linkage of function names with internal linkage?; Clang 2.8. 5; CD1; CV-qualifiers and type conversions; Clang 3.1. 6; NAD; Should the optimization that allows a class object to alias another object also allow the case of a parameter in an inline function to alias its argument?; Unknown. 7; NAD; Can a class with a private virtual base class be derived from?; Clang 3.4. 8; CD1; Access to template arguments used in a function return type and in the nested name specifier; Duplicate of 45. 9; CD1; Clarification of access to base class members; Clang 2.8. 10; CD1; Can a nested class access its own class name as a qualified name if it is a private member of the enclosing class?; Duplicate of 45. 11; CD1; How do the keywords typename/template interact with using-declarations?; Yes. 12; dup; Default arguments on different declarations for the same function and the Koenig lookup; Superseded by 239. 13; NAD; extern ""C"" for Parameters of Function Templates; No. 14; NAD; extern ""C"" functions and declarations in different namespaces; Clang 3.4. 15; dup; Default arguments for parameters of function templates; Yes. 16; CD1; Access to members of indirect private base classes; Clang 2.8. 17; NAD; Footnote 99 should discuss the naming class when describing members that can be accessed from friends; Yes. 18; NAD; f(TYPE) where ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html:806,optimiz,optimization,806,interpreter/llvm-project/clang/www/cxx_dr_status.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html,2,['optimiz'],['optimization']
Performance,". Clang - Features and Goals. Clang - Features and Goals. This page describes the features and goals of; Clang in more detail and gives a more broad explanation about what we mean.; These features are:. End-User Features:. Fast compiles and low memory use; Expressive diagnostics; GCC compatibility. Utility and Applications:. Library based architecture; Support diverse clients; Integration with IDEs; Use the LLVM 'BSD' License. Internal Design and Implementation:. A real-world, production quality compiler; A simple and hackable code base; A single unified parser for C, Objective C, C++,; and Objective C++; Conformance with C/C++/ObjC and their; variants. End-User Features. Fast compiles and Low Memory Use. A major focus of our work on clang is to make it fast, light and scalable.; The library-based architecture of clang makes it straight-forward to time and; profile the cost of each layer of the stack, and the driver has a number of; options for performance analysis. Many detailed benchmarks can be found online.; Compile time performance is important, but when using clang as an API, often; memory use is even more so: the less memory the code takes the more code you can; fit into memory at a time (useful for whole program analysis tools, for; example).; In addition to being efficient when pitted head-to-head against GCC in batch; mode, clang is built with a library based; architecture that makes it relatively easy to adapt it and build new tools; with it. This means that it is often possible to apply out-of-the-box thinking; and novel techniques to improve compilation in various ways. Expressive Diagnostics. In addition to being fast and functional, we aim to make Clang extremely user; friendly. As far as a command-line compiler goes, this basically boils down to; making the diagnostics (error and warning messages) generated by the compiler; be as useful as possible. There are several ways that we do this, but the; most important are pinpointing exactly what is wrong i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/features.html:780,scalab,scalable,780,interpreter/llvm-project/clang/www/features.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/features.html,4,"['perform', 'scalab']","['performance', 'scalable']"
Performance,". Cling Website. Cling Website; Interactive Compiler Interface. Home; Download; News; Docs. Documentation. Developer. Doxygen; Extensions; Error Recovery; Late Binding. End User. User Manual; Status of ObjectiveC[++]. Get Involved. Latest News; Cling goes public; July 25th, 2011; Cling was officially announced to the Clang community Read more. New website launched; July 1st, 2011; Welcome to the new website of the project. Read more; Useful Links. CERN. Cling. Welcome to our interactive C++ interpreter, built on the top of LLVM and Clang libraries. Its advantages over the standard interpreters are that it has command line prompt and uses just-in-time (JIT) compiler for compilation. An interactive prompt is usually referred to as a read eval print loop or repl. Many of the developers (e.g. Mono in their project called CSharpRepl) of such kind of software applications name them interactive compilers.; . One of Cling's main goals is to provide contemporary, high-performance alternative of the current C++ interpreter in the ROOT project - CINT. The backward-compatibility with CINT is major priority during the development.; ; How to use it; You can start typing not only C++ top level declaratons but statements, too. ; **** Welcome to the cling prototype! ****; * Type C code and press enter to run it *; * Type .q, exit or ctrl+D to quit *; *****************************************; [cling]$. Statements and expressions could take more than one input line. The interactive prompt changes from ""[cling]$"" to ""[cling]$ ?"".; ; [cling]$ #include ""math.h""; [cling]$ #include ""stdio.h""; [cling]$ for (unsigned i = 0; i < 5; ++i) {; [cling]$ ? printf(""%f\n"", sin(i));; [cling]$ ? }; 0.000000; 0.841471; 0.909297; 0.141120; -0.756802; Grammar; Cling is able to parse everything that clang can. Current clang status can be found here. At the moment, there are use cases only for C++ that's why cling is best in working with C++. Clang has support of C, objC, objC++ and we are looking forward t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/www/old/index.html:974,perform,performance,974,interpreter/cling/www/old/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/www/old/index.html,2,['perform'],['performance']
Performance,". Cling. Cling interprets C++. ****************** CLING ******************; * Type C++ code and press enter to run it *; * Type .q to exit *; *******************************************; [cling]$ #include <string>; [cling]$ std::string s(""abc"");; [cling]$ s.find('b'); (std::basic_string<char, std::char_traits<char>, std::allocator<char> >::size_type) 1; [cling]$. Cling is built on the top of LLVM and Clang libraries. In addition to standard interpreters it has a command line prompt and uses just-in-time (JIT) compiler. This kind of software application is commonly known as an interactive compiler.; ; Cling started off as a contemporary, high-performance alternative of the current C++ interpreter in the ROOT project - CINT.; ; Why interpreting C++ with Cling?. Learning C++ . ; One use case of cling is to aid the C++ learning process. Offering imediate feedback the user can easily get familiar with the structures and spelling of the language. ; . Creating scripts; ; The power of an interpreter lays as well in the compactness and ease of repeatedly running a small snippet of code - aka a script. This can be done in cling by inserting the bash-like style line: ; . #!/usr/bin/cling; . Rapid Application Development (RAD) . ; Cling can be used successfully for Rapid Application Development allowing for prototyping and proofs of concept taking advantage of dynamicity and feedback during the implementation process.; . Runtime-Generated Code ; ; Sometime it's convenient to create code as a reaction to input; (user/network/configuration).; Runtime-generated code can interface with C++ libraries.; . Embedding Cling . The functionality of an application can be enriched by embedding Cling. To embed Cling, the main program has to be provided. One of the things this main program has to do is initialize the Cling interpreter. There are optional calls to pass command line arguments to Cling. Afterwards, you can call the interpreter from any anywhere within the application. For compila",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/www/index.html:650,perform,performance,650,interpreter/cling/www/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/www/index.html,2,['perform'],['performance']
Performance,". Cling. Grammar; Cling is able to parse everything that clang can. Current clang status can be found here. At the moment, there are use cases only for C++ that's why cling is best in working with C++. Clang has support of C, objC, objC++ and we are looking forward to having more use-cases and extend our tool in that direction.; Cling has internal commands, which can change its behavior at runtime. Those commands usually start with dot (.):; .I <path> - Adds an include path;; .x <filename> - #include-s the filename; and calls function called filename(); ; .L <libname> - Loads libname or #include-s the libname if libname is file;; .@ - Cancels the multiline input;; .printAST - (DEBUG ONLY) Turns on the printing of the compiler's abstract syntax tree (AST);; .dynamicExtensions - Turns on cling's dynamic extensions. This in turn enables the dynamic lookup and the late resolving of the identifier. With that option cling tries to heal the compile-time failed lookups at runtime;. Details; Command line. The interactive prompt supports an emacs-like command line editor, just like bash terminal, which makes it easy to integrate and use. Cling uses TextInput and doesn't depend on ncurses.; . Autocompletion should be coming soon!; . ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/www/use.html:577,Load,Loads,577,interpreter/cling/www/use.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/www/use.html,1,['Load'],['Loads']
Performance,". Core Libraries; ROOT Error Handlers; There is a new rootrc variable which allows to control the; installation of the ROOT error handlers. By default the handlers; are activated:. Root.ErrorHandlers: 1. but setting the value to 0 result in no error handlers being installed; and the originals remaining in place. This can be useful if ROOT is used in; conjunction with other frameworks that already installed their own handlers. TString; TString::Hash() and thus also TMath::Hash() now use MurmurHash3_x64_128; from http://code.google.com/p/smhasher/ which is public domain.; To accelerate the hash in the case of pointers even further, pointers (and same-sized texts) are hashed using a simple bitwise xor.; This dramatically increases the hash performance for long texts, and still by a factor 5 for pointers.; The pointer case is most visible for certain I/O operations (TExMap).; TColor; Add the method SetAlpha() to set the alpha value (transparency; level) for an existing color. TStyle. The default font set by gStyle->SetLegendFont() was ignored. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v534/index.html:747,perform,performance,747,core/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v534/index.html,2,['perform'],['performance']
Performance,". Core. New class TBase64 providing Base64 encoding and decoding. Base64 encoded; messages are typically used in authentication protocols and to pack binary; data in HTTP or mail messages. New method in TSystem:. TString TSystem::GetFromPipe(const char *command). which executes ""command"" in the shell and returns the output in the TString.; Multi-line output is separated by \n's. Add proper support for Microsoft Visual C++ 9.0; Add support for 'unix' sockets on Windows.; New method TString::Clear() to reset the string but not to resize it to the default; (small) size. Useful when the string was pre-allocated to a large size and; has to be re-used.; Insure that ROOT's autoloader is always enabled whenever loading rootmap files.; Add function void TAttAxis::SetNdivisions(Int_t n1, Int_t n2, Int_t n3, Bool_t optim); ; Enable autoloading of typedef.; The statically linked roota executable and libRoot.a are currently; only supported on Linux platforms. We hope to extend this to MacOS X; soon. Meta. Add new macro ClassDefNV (ClassDef Non Virtual) which does not define any virtual function. ClassDef does define IsA, Streamer and ShowMember as virtual. This should be used only in classes that are never inherited from!; Improve performance of TClass::GetMethod (and friends). ACLiC. Implement TClassEdit::InsertStd() which puts ""std::"" in front of all STL classes.; The generated library now always checks with which version of ROOT the library was build and rebuilt the library if the running version of ROOT is different.; Add support for '+' character embedded in the script's name or directory name.; The dependency tracking file (script_C.d) is now always created when the library is built.; The dependency tracking file now records with which version of ROOT the library was built and the library is now rebuilt if it is loaded in a different version of ROOT. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v524/index.html:713,load,loading,713,core/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v524/index.html,6,"['load', 'perform']","['loaded', 'loading', 'performance']"
Performance,". Core; Build system and Platform support. The build system has been extended to support out-of-source builds.; This means you can build different sets of binaries from one source tree, like:. mkdir root-debug; cd root-debug; ../root/configure --build=debug. The build system now supports cross compilation, where typically you need tools; like rootcint, rootmap, etc, to be compiled on the build platform to be able; to generate dictionaries for the target platform. ROOT has been ported to iOS. This port required the above mentioned; changes in the build system. To build ROOT for the iPhone/iPad sumilator do:. ./configure iossim; make. To build a native iOS armv7 version do:. ./configure ios; make. Both builds create a libRoot.a that can be used to create ROOT based iOS apps; (iOS does not allow apps to load non-system dynamic libraries at run time).; Some sample Xcode projects using ROOT will soon be made available. Base. Change TTime data member from Long_t to Long64_t. On 32-bit systems the; Long_t is 32-bits and too small to keep the time in milliseconds since the ROOT EPOCH (1-1-1995). Added new operators:. operator long long(); operator unsigned long long(). The existing operators long and unsigned long on 32-bit machines return; an error in case the stored time is larger then 32-bit and truncation; occurs (like was always the case till now, but silently). New method ExitOnException() which allows to set the behaviour of; TApplication in case of an exception (sigsegv, sigbus, sigill, sigfpe).; The default is to trap the signal and continue with the event loop,; using this method one can specify to exit with the signal number to the; shell, or to abort() which in addition generates a core dump. New command line argument -x which forces ROOT to exit on an exception.; Add TSystem::AddDynamicPath. Build. New option '-t' for rmkdepend to allow the caller to fully specify the name to be used as a target; This supersedes the name calculated from the input file name and t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v528/index.html:812,load,load,812,core/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v528/index.html,2,['load'],['load']
Performance,". Core; TString. Reimplementation of the internals of TString to not use reference counting and; copy on write, but to use a more modern and thread safe Short String; Optimization (SSO) technique. Using SSO short strings (<15 on 64-bit and; <11 on 32-bit) are contained in the TString internal data structure; without the need for mallocing the required space. TObject. In TObject::ls, add support for the option 'noaddr' which ; prevents the printing of the address of the object. This; is useful in particular in roottest. Use this in hadd; and TFileMerger. TROOT. New routine CloseFiles used automatically shortly before termination to ; insure closing of files and sockets before the unload of any library.; New collection 'ClosedObjects' holding pointers to TFile or TSocket that; have been closed but not deleted it. In the case of TSocket, they are added only; if they are closed by the CloseFiles.; Add a Close member function to TProofMgr since it is added to the list of socket.; Migrate the closing of files from various to a single place (T*System::Exit).; Fill in the implementation of TROOT::FindObjectAnyFile.; Mark TROOT as TObject::kInvalidObject as soon as its destructor starts,; in order to be able to veto some action later on (like autoloading). TSystem. Better handle the cases where the information in the rootmap file is (almost) empty. ; Avoid infinite loop if one of the dependent library is missing. Meta. Add new fast accessors to Merge routines (See the I/O package for more details.; Improve error message in case a schema evolution rule can not be loaded when the library is loaded; (from the generic 'it conflicts with one of the other rules' to 'the target member ... is unknown'.; Add the ability to explicitly forbid (or allow) the splitting of a class; (TClass::SetSplit ) so that user can inforce the use of a custom streamer in all possible split cases.; Improve the performance of TProcessUUID::AddUUID by reintroducing the THashList.; This significanly improve",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v530/index.html:167,Optimiz,Optimization,167,core/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v530/index.html,1,['Optimiz'],['Optimization']
Performance,". Currently, the only; way to do this is to manually check each place in your front-end where; GetElementPtr operators are created. It's not possible to write a checker which could find all rule violations; statically. It would be possible to write a checker which works by instrumenting; the code with dynamic checks though. Alternatively, it would be possible to; write a static checker which catches a subset of possible problems. However, no; such checker exists today. Rationale; =========. Why is GEP designed this way?; -----------------------------. The design of GEP has the following goals, in rough unofficial order of; priority:. * Support C, C-like languages, and languages which can be conceptually lowered; into C (this covers a lot). * Support optimizations such as those that are common in C compilers. In; particular, GEP is a cornerstone of LLVM's `pointer aliasing; model <LangRef.html#pointeraliasing>`_. * Provide a consistent method for computing addresses so that address; computations don't need to be a part of load and store instructions in the IR. * Support non-C-like languages, to the extent that it doesn't interfere with; other goals. * Minimize target-specific information in the IR. Why do struct member indices always use ``i32``?; ------------------------------------------------. The specific type i32 is probably just a historical artifact, however it's wide; enough for all practical purposes, so there's been no need to change it. It; doesn't necessarily imply i32 address arithmetic; it's just an identifier which; identifies a field in a struct. Requiring that all struct indices be the same; reduces the range of possibilities for cases where two GEPs are effectively the; same but have distinct operand types. What's an uglygep?; ------------------. Some LLVM optimizers operate on GEPs by internally lowering them into more; primitive integer expressions, which allows them to be combined with other; integer expressions and/or split into multiple separat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst:19710,load,load,19710,interpreter/llvm-project/llvm/docs/GetElementPtr.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst,1,['load'],['load']
Performance,". Due to the self-containment of assemblies, they are very; practical to use when a container is hard to define due to possible; overlaps during positioning. For instance, it is very easy creating; honeycomb structures. A very useful example for creating and using; assemblies can be found at:; <http://root.cern.ch/root/html/examples/assembly.C.html>`.`. Creation of an assembly is very easy: one has just to create a; **`TGeoVolumeAssembly`** object and position the components inside as; for any volume:. ``` {.cpp}; TGeoVolume *vol = new TGeoVolumeAssembly(name);; vol->AddNode(vdaughter1, cpy1, matrix1);; vol->AddNode(vdaughter2, cpy2, matrix2);; ```. Note that components cannot be declared as ""overlapping"" and that a; component can be an assembly volume. For existing flat volume; structures, one can define assemblies to force a hierarchical structure; therefore optimizing the performance. Usage of assemblies does NOT imply; penalties in performance, but in some cases, it can be observed that it; is not as performing as bounding the structure in a container volume; with a simple shape. Choosing a normal container is therefore; recommended whenever possible. ![Assemblies of volumes](pictures/080001CF.png). ### Geometrical Transformations. All geometrical transformations handled by the modeller are provided as; a built-in package. This was designed to minimize memory requirements; and optimize performance of point/vector master-to-local and; local-to-master computation. We need to have in mind that a; transformation in **`TGeo`** has two major use-cases. The first one is; for defining the placement of a volume with respect to its container; reference frame. This frame will be called 'master' and the frame of the; positioned volume - 'local'. If `T` is a transformation used for; positioning volume daughters, then: `MASTER = T * LOCAL`. Therefore `T `is used to perform a local to master conversion, while; `T-1` for a master to local conversion. The second use case is the;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:90706,perform,performance,90706,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,2,['perform'],"['performance', 'performing']"
Performance,". For example,. .. code-block:: c++. PassBuilder PB;; PB.registerPipelineStartEPCallback([&](ModulePassManager &MPM,; PassBuilder::OptimizationLevel Level) {; MPM.addPass(FooPass());; };. will add ``FooPass`` near the very beginning of the pipeline for pass; managers created by that ``PassBuilder``. See the documentation for; ``PassBuilder`` for the various places that passes can be added. If a ``PassBuilder`` has a corresponding ``TargetMachine`` for a backend, it; will call ``TargetMachine::registerPassBuilderCallbacks()`` to allow the; backend to inject passes into the pipeline. Clang's ``BackendUtil.cpp`` shows examples of a frontend adding (mostly; sanitizer) passes to various parts of the pipeline.; ``AMDGPUTargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:6618,load,load-pass-plugin,6618,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,1,['load'],['load-pass-plugin']
Performance,". GUI; TRootCanvas. In SetWindowSize the event queue is flushed to make sure; the window size change is really done. TRootContextMenu. When creating the dialog from the context menu, skip arguments that are pointers (but not char *) and have a default value. This should avoid confusing input fields in dialog.; Implemented online help in root dialogs (the dialog boxes used with contextual menus) via a new ""Online Help"" button. This opens a Root HTML browser at the right class/method location in the Root reference guide on the web.; The base url can be changed with the Browser.StartUrl option in system.rootrc (by default: http://root.cern.ch/root/html/ClassIndex.html); Added a small '?' on the right of the context menu entries, giving access to online help. TGMenu. Add possibility to add a right aligned shortcut by using a tab character ('\t') before the shortcut string, as shown below:; fMenuFile->AddEntry(""&Open...\tCtrl+O"", kOpenFile);; Use new way of adding right aligned shortcuts in the menu entries in most of the GUI classes using shortcuts in their menu. TGSlider. Added HandleConfigureNotify() to handle resizing events. New Browser. Automatically browse ROOT files if there is any open when starting the browser.; Correct system files manipulations (copy, rename, delete) and automatic update of the list tree. GUIHTML; TGHtmlBrowser. Added ability to display single picture from the web and to open pdf files with external viewer (Windows only); Implemented anchor navigation (e.g. http://root.cern.ch/root/html/TH1.html#TH1:Multiply). ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/doc/v524/index.html:47,queue,queue,47,gui/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/doc/v524/index.html,2,['queue'],['queue']
Performance,". Help for ROOT's Reference Guide. Help for ROOT's Reference Guide. Contents. Parts of the Reference Guide. Type Index; Class Index; Inheritance. Modules. What they are; List of Modules; Modules' Library Dependencies. Class Reference. Sections; Link Box; Info Box; List of Data and Function Members. Display Options; Access (public / protected / private); Inheritance. Class Charts. Inheritance; Inherited Members; Includes; Libraries. Member Function Documentation. Parts of the Reference Guide; Type Index; All basic types and typedefs are documented in the Type Index. Class Index; All classes and namespaces are listed alphabetically in the Class Index,; along with a short info of what they are used for.; You can jump to a class's entry using the shortcuts ontop of the page.; They are optimized for quick access: they are distributed evenly, and short.; Classes are grouped in modules; these, too, are listed ontop of the list of classes. Inheritance; All classes along with their derived and base classes are shown in the; Class Hierarchy.; The documentation for each class also shows the inheritance diagram.; The class hierarchy is meant to give an overview of available classes; and their relations. Modules; What they are; Classes are grouped into modules. For ROOT, this is done on a per-directory basis:; each module corresponds to a sub-directory. In other cases one module might represent; one library. Either way, modules are meant to combine similar or related classes,; allowing users to find classes close by context. If you need some functionality that; you cannot find in a class you know, you might want to check for classes in the same; module - maybe one of them does what you need. List of Modules; Modules are listed ontop of the Class Index and as part; of the Library Dependencies Chart. Modules' Library Dependencies; Each module is assumed to be part of a library. The dependencies of libraries are; not only relevant for linking, but often reflect also the contextual d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/etc/html/HELP.html:792,optimiz,optimized,792,etc/html/HELP.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/etc/html/HELP.html,2,['optimiz'],['optimized']
Performance,". Histogram Libraries; TH1. Improve performances of TH1::Merge in case of histograms with same bin limits. Now an copy of the; initial histogram is not done. These improvements have been applied also in the TH2, TH3 and TProfile's classes.; . TH2. Add a new option ""S"" in FitSlice which performs a sliding merge: merge n consecutive bins along Y accordingly to what value in option Gn is given.; . TProfile2D and TProfile3D. Implement SetBins for variable bin sizes; ; Add support for variable bins in TProjectionXY. TH2Poly. The values set by SetMaximum() and SetMinimum() were not; taken into account by GetMaximum() and GetMinimum().; The Palette and the statistics box were not pickable when TH2Poly was drawn; with option COLZ.; TH2Poly was wrongly picked in the canvas area after a zoom along axis. TEfficiency. list holding the associated functions is created only on demand; default constructor creates two dummy histograms; can now be filled with weights (only Bayesian methods and the normal; approximation are supported) ; update TEfficiency::SavePrimitive to store also the set bits. TGraphAsymmErrors. add option to TGraphAsymmErrors::Divide for interpreting the given; histograms as ratio of Poisson means. TMultiGraph. The following macro did not show the x-axis in TimeDisplay mode. The; mg->GetYaxis()->UnZoom(); command erased the TimeDisplay attribute of; the axis. (fix from beischer@physik.rwth-aachen.de). {; TMultiGraph* mg = new TMultiGraph;; TGraph* g = new TGraph;; for (int i = 0; i < 100; i++) g->SetPoint(i, 1289420808+i, i+2);; mg->Add(g, ""P"");; mg->Draw(""AP"");; mg->GetXaxis()->SetTimeDisplay(1);; mg->GetYaxis()->UnZoom();; gPad->Modified();; gPad->Update();; }. TPaletteAxis. In TPaletteAxis::Paint() now makes sure the min and max of the; palette are not 0 when the histogram content is 0. on Ubuntu the following macro crashed. A char variable was too small. {; TCanvas *tmp = new TCanvas();; TH2F *h1 = new TH2F(""h1"",""h1"",40,0.,10.,40,1.e-2,1.e2);; h1->Fill(5,10);;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v532/index.html:36,perform,performances,36,hist/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v532/index.html,4,['perform'],"['performances', 'performs']"
Performance,". Histogram package. Modify implementation of the Fit function in the histograms and data classes to use a new common implementation, defined in the file HFitImpl.cxx. This file provides the implementation of the function used for bin fitting of the data objects of the histogram package, TH1's, TGraph, TMultiGraph, TGraph2D, and un-binned maximum likelihood fit for TTree data (used by the TTree::UnbinnedFit method). HFitInterface.h; Header file defining functions in the namespace ROOT::Fit providing functions required for fitting the data objects of the histogram package. These functions are used for example from other libraries like the FitPanel or the TTreePlayer for performing the fits.; . ROOT::Fit::FitObject: function for fitting the various data objects. The user must pass in addition to a pointer to the fit object, the fit options (via the FOption class and not a string), the minimizer options and the fit data range.; ; ROOT::Fit::FillData: function for filling the fit data from the histogram data objects. Used for fitting an histogram using the ROOT::Fit::Fitter class.; ; ROOT::Fit::UnBinFit:: function for fitting an unbinned data sets, for example obtained from TTree data.; . TBackCompFitter; New class providing a backward compatible implementation of the; TVirtualFitter using the new fitting class. It is wrapping the functionality of the ROOT::Math::Fitter and it; can be used to retrieve the fit information (result and; configuration) via the; TVirtualFitter API from FitConfig and FitResult. A static instance of this class; is created after calling the histograms and graph Fit methods in order to retrieve the full fit information after having fit. This instace will be deleted only when the next fit is executed.; This gives full backward compatibility functionality in fitting.; This class in addition to the TVirtualFitter provides the following functionality:; ; access direct to references to ROOT::Fit::FitResult and ROOT::FitConfig objects via the member fu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v522/index.html:678,perform,performing,678,hist/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v522/index.html,2,['perform'],['performing']
Performance,". Histogram package; All Histogram classes (including the TProfile's). Add support for filling histograms with negative weights. Now the correct global histogram statistics (Mean, Standard; deviation, etc..) is computed, while before the abs(w) was used when computing the histogram statistics.; ; Updated in all the projection function the check for the compatibility with previously existing histograms, now; the limits are checked using a tolerance/; Fix a bug in computing the point errors when fitting a 2d (or 3D) histogram with a 1d (or 2D) function. TH1. Add support for weighted likelihood fit of histogram by using a new option, WL and suppress the old option; LL.; The histogram must have the sum of the weight squared stored bin by bin to use this fit option; (i.e. TH1::Sumw2() has been called before filling).; Now one can perform likelihoof fit to weighted or scaled histograms and get the correct errors in the fit parameters.; (see bug report 79754).; ; Fix for the bug 82562.; Fix a bug in TH1::Merge for histogram with labels (bug 75902).; Fix few bugs related with the Buffer. . TProfile. Fix a bug in TProfile::Merge when the kCanRebin bit is set; (bug 79675).; Fix a bug in LabelsDeflate (bug 77149). TH1. Add new method TH3::Rebin3D and alsoRebinX, RebinY and RebinZ thanks to Zhiyi Liu. THistPainter. TPad::SetTheta() and TPad::SetPhi() did not cause the; canvas redrawing.; Protection added in case two histograms were plotted in the same pad; using the option BOX (the 2nd one with option SAME).; The clipping was not correct when an interactive zoom was performed. The 2D functions, for instance a fit function, associated to a 2D; histogram were always drawn as scatter plots. This was very confusing.; Now they are drawn as a surface if the histogram plotting option is a; lego or a surface or as a contour plot for any other plotting options. When drawing scatter plot for TH2 or TH2Poly do not use gRandom, but an independent random generator instance,; to avoid interfe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v530/index.html:837,perform,perform,837,hist/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v530/index.html,2,['perform'],['perform']
Performance,". However, those dependencies can be; removed at register renaming stage (at the cost of allocating register aliases,; and therefore consuming physical registers). Table *Average Wait times* helps diagnose performance issues that are caused by; the presence of long latency instructions and potentially long data dependencies; which may limit the ILP. Last row, ``<total>``, shows a global average over all; instructions measured. Note that :program:`llvm-mca`, by default, assumes at; least 1cy between the dispatch event and the issue event. When the performance is limited by data dependencies and/or long latency; instructions, the number of cycles spent while in the *ready* state is expected; to be very small when compared with the total number of cycles spent in the; scheduler's queue. The difference between the two counters is a good indicator; of how large of an impact data dependencies had on the execution of the; instructions. When performance is mostly limited by the lack of hardware; resources, the delta between the two counters is small. However, the number of; cycles spent in the queue tends to be larger (i.e., more than 1-3cy),; especially when compared to other low latency instructions. Bottleneck Analysis; ^^^^^^^^^^^^^^^^^^^; The ``-bottleneck-analysis`` command line option enables the analysis of; performance bottlenecks. This analysis is potentially expensive. It attempts to correlate increases in; backend pressure (caused by pipeline resource pressure and data dependencies) to; dynamic dispatch stalls. Below is an example of ``-bottleneck-analysis`` output generated by; :program:`llvm-mca` for 500 iterations of the dot-product example on btver2. .. code-block:: none. Cycles with backend pressure increase [ 48.07% ]; Throughput Bottlenecks:; Resource Pressure [ 47.77% ]; - JFPA [ 47.77% ]; - JFPU0 [ 47.77% ]; Data Dependencies: [ 0.30% ]; - Register Dependencies [ 0.30% ]; - Memory Dependencies [ 0.00% ]. Critical sequence based on the simulation:. Instr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:26224,perform,performance,26224,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['perform'],['performance']
Performance,". I/O Libraries; TFileCacheRead. Support for multiple TFileCacheRead per TFile.; Multiple TFileCacheRead per TFile are supported by augmenting the existing TFile::SetCacheRead() function with an optional TObject* argument specifying the owner (i.e. tree) of the cache. This function will assign a TFileCacheRead to a TFile for the given TTree. A cache can be removed by setting the pointer TFileCacheRead to 0.; Similarly, in TFile::GetCacheRead() an optional TObject* argument was added to obtain the TFileCacheRead from a TFile.; In addition to the unassigned TFileCacheRead pointer, TFile will maintain a map of tree specific cache pointers.; Backward compatibility in both functions is handled by making the TObject* argument optional. If it is not specified in the TFile::SetCacheRead() call, only the unassigned TFileCacheRead pointer is updated, otherwise the map and the unassigned cache are updated. In TFile::GetCacheRead(), if an owner is not specified or doesn't exist in the file's cache map, the unassigned cache is returned, unless it is 0 and there is exactly one entry in the cache map.; Distinguish counter for bytes read and read calls for learning phase. TFileMerger. Improve efficiency of TFileMerger when merging a single file by doing a TFile::Cp rather than a load/write of the objects.; In TFileMerger and hadd when objects can not be merged do not overwrite the last object in the set with the first!; Renable warning about not being able to merge objects in TFileMerger and hadd.; Fix hadd problem where the incremental merging fails if the TTree are stored in sub-directories.; Improve the code used for forward compatibility (record the type as TDirectory even-though the class is now TDirectoryFile) by delaying the switching of the class name until it is written (to the buffer). This avoids problem where a TKey is created (by TFile::mkdir) and then immediately used for reading (this happens in the incremental file merger). ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v534/index.html:262,cache,cache,262,io/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v534/index.html,16,"['cache', 'load']","['cache', 'load']"
Performance,". I/O. Add support for the Chirp filesystem. To configure and build, chirp 3.2.2 must be installed.; When a TFile object is deleted, make sure that CINT also 'removes' any global variables that might point to it.; Fix support for the automatic addition to the current directory (for TTree and TH1 for example) in TKey::Read(TObject*).; In TKey, properly handle error in the I/O routines.; Explicitly check the validity of the zipped buffer before calling R__unzip, this allow for better error recovery.; When double checking whether a checksum difference is sustantial, ignore the std namespace. Use CompareContent also in the case of where; the class is versioned but the 'current' streamerInfo has not yet been built.; Prevent the I/O engine from mistakenly applying schema evolution to the TObject::fBits.; Make sure that when a streamer info of a base class is used to stream memberwise that is always not-optimized. If the StreamerInfo on file; has the same version as the StreamerInfo in memory but the one on file need to be 'not optimized' while the one in memory is not yet built, make; sure it will not be optimized.; Fix the reading of empty collection of object when reading without the library.; If the sequence of actions for streaming member-wise is not created correctly (i.e. where fReadMemberWise was null previously),; we now explicitly issue a Fatal error:. Fatal in <ReadSequence>: The sequence of actions to read AliESDVertex:7 member-wise was not initialized.; aborting. Add new optional parameter maxbuf to TXMLEngine::ParseFile() allowing the specification of the XML file size to be parsed. This fixes issue #78864.; Add function TBuffer::AutoExpand to centralize the automatic buffer extension policy. This enable the ability to tweak it later (for example instead of always doubling the size, increasing by only at most 2Mb or take hints from the number of entries already; in a TBasket).; Migrate the class TFileMerger from the proofplayer library to ROOT I/O library and ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v530/index.html:910,optimiz,optimized,910,io/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v530/index.html,2,['optimiz'],['optimized']
Performance,. I/O; File Format; The default for streaming the content of STL containers was changed from object-wise; to member-wise. We evaluated the impact of moving to MemberWise streaming using 5 different CMS data files:. cms1.root an older split using level 99 Reco file; cms2.root a more recent non split raw data file; cms3.root a more recent non split Reco file; cms4.root is an example of the lepton plus jet analysis format known as a user PAT-tuple (split); cms5.root is an example AOD (analysis object dataset) format file. It is not split because the objects here are a strict subset of the RECO objects. We rewrote all the files using the v5.26/00 new basket clustering algorithm using both memberwise streaming and objectwise streaming. In the table below the read time is the CPU time to completion including loading the libraries.; When reading the file where always in the os cache (since we are focusing evaluating cpu time). The number of event actually read varies from file set to file set but was calibrated to result; in about 10s of cpu time. The files are read using a library generated with TFile::MakeProject. The object-wise files are between 2% and 10% larger compared to their member-wise counterpart. The CPU time for reading object-wise files is 12% higher for split files; and 30% higher for non-split files. So the improvement is significant enough to warrant switch the default from objectwise to memberwise. Split files. FilenameMemberwiseSizeCpu Time To read. cms1.root N 17.5 Gb 10.55s +/- 0.15 (2200 entries) ; cms1.root Y 16.8 Gb 9.12s +/- 0.08 (2200 entries) ; cms4.root N 1.47 Gb 10.18s +/- 0.19 (2500 entries) ; cms4.root Y 1.43 Gb 9.24s +/- 0.06 (2500 entries) . Non Split files. FilenameMemberwiseSizeCpu Time To read. cms2.root N 1.65 Gb 10.95s +/- 0.05 (1000 entries) ; cms2.root Y 1.53 Gb 8.20s +/- 0.05 (1000 entries) ; cms3.root N 0.780 Gb 10.59s +/- 0.05 (700 entries) ; cms3.root Y 0.717 Gb 8.29s +/- 0.08 (700 entries) ; cms5.root N 1.55 Gb 10.20s +/- 0.17 (,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v528/index.html:814,load,loading,814,io/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v528/index.html,4,"['cache', 'load']","['cache', 'loading']"
Performance,". I/O; Schema Evolution. Change TExMap hash, key and values from (U)Long_t to (U)Long64_t. This makes TExMap streamable in a portable way. On 64-bit platforms there is; no difference, but on 32-bit platforms all values will now be 64-bit. This fixes a big portability issue with THnSparse which uses TExMap internally; where the versions created on a 32-bit platform could not be read on a 64-bit platform and vice versa.; Avoid reporting I/O error for members of a class that is used only for a transient member; Concrete implementation of TClassGenerator needs to be updated to also avoid the warnings.; Fix the rule lookup based on checksum; Extend support of the schema evolution rules to fixed length array.; Prevent a process abort (due to a call to Fatal) when we are missing the dictionary for (one of) the; content of an STL collection when this collection is 'only' use has a transient member.; Fix the case where the source and target of a rule have the same name.; Avoid using the 'current' StreamerInfo to read an older streamerInfo that is missing (in case of corrupted files). Misc. New TFile plugin for the Hadoop Distributed File System (protocol hdfs:); Unregister stack objects from their TDirectory when the TList tries to delete them.; When streaming a base class without StreamerNVirtual() use an external streamer if it was set.; Many improvement to the I/O run-time performance.; DCache:; Increase readahead size from 8k to 128k and make it settable via DCACHE_RA_BUFFER env var.; dCap client does not ignore ?filetpye=raw and other options, so remove it. The function TFile::GetRelOffset is now public instead of protected.; Corrected the reading of the TFile record of large files.; MakeProject: several updates to improve support for CMS and Atlas data files (add support for auto_ptr, bitset, class name longer than 255 characters, etc.). ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v526/index.html:1390,perform,performance,1390,io/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v526/index.html,2,['perform'],['performance']
Performance,". If any instruction should have either of; these flags set, it should be done within the target's InstrPostProcess class.; For an example, look at the `X86InstrPostProcess::postProcessInstruction` method; within `llvm/lib/Target/X86/MCA/X86CustomBehaviour.cpp`. A load/store barrier consumes one entry of the load/store queue. A load/store; barrier enforces ordering of loads/stores. A younger load cannot pass a load; barrier. Also, a younger store cannot pass a store barrier. A younger load; has to wait for the memory/load barrier to execute. A load/store barrier is; ""executed"" when it becomes the oldest entry in the load/store queue(s). That; also means, by construction, all of the older loads/stores have been executed. In conclusion, the full set of load/store consistency rules are:. #. A store may not pass a previous store.; #. A store may not pass a previous load (regardless of ``-noalias``).; #. A store has to wait until an older store barrier is fully executed.; #. A load may pass a previous load.; #. A load may not pass a previous store unless ``-noalias`` is set.; #. A load has to wait until an older load barrier is fully executed. In-order Issue and Execute; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; In-order processors are modelled as a single ``InOrderIssueStage`` stage. It; bypasses Dispatch, Scheduler and Load/Store unit. Instructions are issued as; soon as their operand registers are available and resource requirements are; met. Multiple instructions can be issued in one cycle according to the value of; the ``IssueWidth`` parameter in LLVM's scheduling model. Once issued, an instruction is moved to ``IssuedInst`` set until it is ready to; retire. :program:`llvm-mca` ensures that writes are committed in-order. However,; an instruction is allowed to commit writes and retire out-of-order if; ``RetireOOO`` property is true for at least one of its writes. Custom Behaviour; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Due to certain instructions not being expressed perfec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:42954,load,load,42954,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,2,['load'],['load']
Performance,". In that case, no; *other* input may share the same register as the input tied to the early-clobber; (even when the other input has the same value). You may only tie an input to an output which has a register constraint, not a; memory constraint. Only a single input may be tied to an output. There is also an ""interesting"" feature which deserves a bit of explanation: if a; register class constraint allocates a register which is too small for the value; type operand provided as input, the input value will be split into multiple; registers, and all of them passed to the inline asm. However, this feature is often not as useful as you might think. Firstly, the registers are *not* guaranteed to be consecutive. So, on those; architectures that have instructions which operate on multiple consecutive; instructions, this is not an appropriate way to support them. (e.g. the 32-bit; SparcV8 has a 64-bit load, which instruction takes a single 32-bit register. The; hardware then loads into both the named register, and the next register. This; feature of inline asm would not be useful to support that.). A few of the targets provide a template string modifier allowing explicit access; to the second register of a two-register operand (e.g. MIPS ``L``, ``M``, and; ``D``). On such an architecture, you can actually access the second allocated; register (yet, still, not any subsequent ones). But, in that case, you're still; probably better off simply splitting the value into two separate operands, for; clarity. (e.g. see the description of the ``A`` constraint on X86, which,; despite existing only for use with this feature, is not really a good idea to; use). Indirect inputs and outputs; """""""""""""""""""""""""""""""""""""""""""""""""""""". Indirect output or input constraints can be specified by the ""``*``"" modifier; (which goes after the ""``=``"" in case of an output). This indicates that the asm; will write to or read from the contents of an *address* provided as an input; argument. (Note that in this way, i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:214850,load,loads,214850,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,". Internals - do not select pad (aka gPad) for objects drawing, always use assigned pad painter; 32. Fix - properly save zoomed ranges in drawingJSON(); 33. Fix - properly redraw TMultiGraph; 34. Fix - show empty bin in TProfile2D if it has entries #316; 35. Fix - unzooming on log scale was extending range forevever; 36. Fix - do not force style 8 for hist markers; 37. Fix - ensure minimal hist title height; 38. Fix - disable Bloom effects on Android TGeo displays; 39. Fix - handle reordering of fragments in multipart reply #319; 40. Fix - properly show non-zero entries #320; 41. Fix - display empty hist bin if fSumw2 not zero. ## Changes in 7.7.4; 1. Fix - TGraph Y range selection, do not cross 0; 2. Fix - correctly handle `#font[id]` in latex; 3. Fix - store canvas with embed geometry drawing; 4. Fix - upgrade rollup and import.meta polyfill. ## Changes in 7.7.3; 1. Fix - correctly handle in I/O empty std::map; 2. Fix - reading of small (<1KB) ROOT files; 3. Fix - race condition in zstd initialization #318; 4. Fix - deployment with zstd #317. ## Changes in 7.7.2; 1. Fix - hide empty title on the canvas; 2. Fix - properly handle zooming in THStack histogram; 3. Fix - always use 0 as minimum in THStack drawings; 4. Fix - always show all ticks for labeled axis; 5. Fix - draw TProfile2D bins content as text, not entries; 6. Fix - interactive zooming on log color palette; 7. Fix - keyboard handling while input dialog active; 8. Fix - legend entry with not configured fill attributes; 9. Fix - prevent that color palette exceed graphical range; 10. Fix - exponential log axis labels with kMoreLogLabels bit set. ## Changes in 7.7.1; 1. Fix - properly select TF1 range after zooming; 2. Fix - TH1 y-range selection; 3. Fix - add 'gl' and svg2pdf-related packages to dependencies in package.json. ## Changes in 7.7.0; 1. Let plot current time, file creation or modification time with `&optdate=[1,2,3]` URL parameters; 2. Let plot file name, full file name or item name with `&optfil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:3181,race condition,race condition,3181,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['race condition'],['race condition']
Performance,". It is capable of forming; memory addresses of the following expression directly in integer instructions; (which use ModR/M addressing):. ::. SegmentReg: Base + [1,2,4,8] * IndexReg + Disp32. In order to represent this, LLVM tracks no less than 5 operands for each memory; operand of this form. This means that the ""load"" form of '``mov``' has the; following ``MachineOperand``\s in this order:. ::. Index: 0 | 1 2 3 4 5; Meaning: DestReg, | BaseReg, Scale, IndexReg, Displacement Segment; OperandTy: VirtReg, | VirtReg, UnsImm, VirtReg, SignExtImm PhysReg. Stores, and all other instructions, treat the four memory operands in the same; way and in the same order. If the segment register is unspecified (regno = 0),; then no segment override is generated. ""Lea"" operations do not have a segment; register specified, so they only have 4 operands for their memory reference. X86 address spaces supported; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. x86 has a feature which provides the ability to perform loads and stores to; different address spaces via the x86 segment registers. A segment override; prefix byte on an instruction causes the instruction's memory access to go to; the specified segment. LLVM address space 0 is the default address space, which; includes the stack, and any unqualified memory accesses in a program. Address; spaces 1-255 are currently reserved for user-defined code. The GS-segment is; represented by address space 256, the FS-segment is represented by address space; 257, and the SS-segment is represented by address space 258. Other x86 segments; have yet to be allocated address space numbers. While these address spaces may seem similar to TLS via the ``thread_local``; keyword, and often use the same underlying hardware, there are some fundamental; differences. The ``thread_local`` keyword applies to global variables and specifies that they; are to be allocated in thread-local memory. There are no type qualifiers; involved, and these variables can be pointed to with norma",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:91777,perform,perform,91777,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,2,"['load', 'perform']","['loads', 'perform']"
Performance,". It may seem that ``LD1`` should suffice to perform vector loads on a big endian machine. However there are pros and cons to the two approaches that make it less than simple which register format to pick. There are two options:. 1. The content of a vector register is the same *as if* it had been loaded with an ``LDR`` instruction.; 2. The content of a vector register is the same *as if* it had been loaded with an ``LD1`` instruction. Because ``LD1 == LDR + REV`` and similarly ``LDR == LD1 + REV`` (on a big endian system), we can simulate either type of load with the other type of load plus a ``REV`` instruction. So we're not deciding which instructions to use, but which format to use (which will then influence which instruction is best to use). .. The 'clearer' container is required to make the following section header come after the floated; images above.; .. container:: clearer. Note that throughout this section we only mention loads. Stores have exactly the same problems as their associated loads, so have been skipped for brevity. Considerations; ==============. LLVM IR Lane ordering; ---------------------. LLVM IR has first class vector types. In LLVM IR, the zero'th element of a vector resides at the lowest memory address. The optimizer relies on this property in certain areas, for example when concatenating vectors together. The intention is for arrays and vectors to have identical memory layouts - ``[4 x i8]`` and ``<4 x i8>`` should be represented the same in memory. Without this property there would be many special cases that the optimizer would have to cleverly handle. Use of ``LDR`` would break this lane ordering property. This doesn't preclude the use of ``LDR``, but we would have to do one of two things:. 1. Insert a ``REV`` instruction to reverse the lane order after every ``LDR``.; 2. Disable all optimizations that rely on lane layout, and for every access to an individual lane (``insertelement``/``extractelement``/``shufflevector``) reverse the lane ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:4526,load,loads,4526,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,1,['load'],['loads']
Performance,". It's inadvisable to store raw profiles for; long periods of time. * Tools must retain **backwards** compatibility with indexed profile formats.; These formats are not forwards-compatible: i.e, a tool which uses format; version X will not be able to understand format version (X+k). * Tools must also retain **backwards** compatibility with the format of the; coverage mappings emitted into instrumented binaries. These formats are not; forwards-compatible. * The JSON coverage export format has a (major, minor, patch) version triple.; Only a major version increment indicates a backwards-incompatible change. A; minor version increment is for added functionality, and patch version; increments are for bugfixes. Impact of llvm optimizations on coverage reports; ================================================. llvm optimizations (such as inlining or CFG simplification) should have no; impact on coverage report quality. This is due to the fact that the mapping; from source regions to profile counters is immutable, and is generated before; the llvm optimizer kicks in. The optimizer can't prove that profile counter; instrumentation is safe to delete (because it's not: it affects the profile the; program emits), and so leaves it alone. Note that this coverage feature does not rely on information that can degrade; during the course of optimization, such as debug info line tables. Using the profiling runtime without static initializers; =======================================================. By default the compiler runtime uses a static initializer to determine the; profile output path and to register a writer function. To collect profiles; without using static initializers, do this manually:. * Export a ``int __llvm_profile_runtime`` symbol from each instrumented shared; library and executable. When the linker finds a definition of this symbol, it; knows to skip loading the object which contains the profiling runtime's; static initializer. * Forward-declare ``void __llvm_profi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst:14953,optimiz,optimizer,14953,interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SourceBasedCodeCoverage.rst,1,['optimiz'],['optimizer']
Performance,". Lowers to a call to `objc_destroyWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#void-objc-destroyweak-id-object>`_. '``llvm.objc.initWeak``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.initWeak(ptr, ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_initWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#arc-runtime-objc-initweak>`_. '``llvm.objc.loadWeak``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.loadWeak(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_loadWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#arc-runtime-objc-loadweak>`_. '``llvm.objc.loadWeakRetained``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.loadWeakRetained(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_loadWeakRetained <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#arc-runtime-objc-loadweakretained>`_. '``llvm.objc.moveWeak``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare void @llvm.objc.moveWeak(ptr, ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_moveWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#void-objc-moveweak-id-dest-id-src>`_. '``llvm.objc.release``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare void @llvm.objc.release(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_release <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#void-objc-release-id-value>`_. '``llvm.objc.retain``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.retain(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_retain <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#arc-runtime-objc-retain>`_. '``llvm.objc.retainAutorelease``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.r",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:969181,load,loadweakretained,969181,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loadweakretained']
Performance,". MATLAB; A MATLAB/ROOT interface has been developped by Johannes Kissel.; It is a data interface for reading and writing ROOT files from within MATLAB; just as you do with MATLAB files. For more information, see this announcement at the ROOT Forum. PyROOT. Support for python 2.6 has been added. Older versions will compile fine, but will; have problems when using the buffer interface for C arrays. For user convenience, code was added to load a custom rootlogon.py/.C, if available.; This code is loaded on first use of the ROOT module, and the python rootlogon.py is; loaded as a module. The language was improved by added a _creates property to all MethodProxy methods.; By setting this value to True, objects returned by such methods will be owned (and; reference counted) by the python interpreter.; By default, the Clone() and DrawClone() methods will have _create equal to True. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/doc/v522/index.html:441,load,load,441,bindings/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/doc/v522/index.html,6,['load'],"['load', 'loaded']"
Performance,". Math Libraries. MathCore. Various fixes have been applied in the fitting classes:. Fix issue #46006 for normalization of error resulting from fitting a TGraph; Fix a problem in Chi2 calculation in case of overflow; Fix issue #46601 for avoiding crashes when a linear fit fails.; Fix in the FitData classes the bug #45909 occurring when setting a function range outside histogram range; Fix default integration method to be Gauss algorithm of MathCore instead of the GSL method, when libMathmore is not built or when the plug-in manager fails to load it.; Add a protection against negative log when fitting using the Poisson log likelihood function; Improve calculation of derivative in x for fitted function. This fixes some problem observed when fitting using the error on the coordinates.; Fitter class: add new methods for calculating the error matrix after minimization, Fitter::CalculateHessErrors() and for calculating the Minos errors Fitter::CalculateMinosErrors; FitConfig: add in the configuration the possibility to select a sub-set of the parameters for calculating the Minos errors by using the method FitConfig::SetMinosErrors( listOfParameters ). If no list is passed, by default the Minos error will be computed on all parameters.; UnBinData class: add new constructor for creating a unbin data set passing a range to select the data and copy in the internal array; FitResult: the class now stores a map of the Minos error using as key the parameter index. If the Minos error has not been calculated for the parameter, FitResult::LowerError(i) and FitResult::UpperError(i) returns the parabolic error; ; Add a new class, MinimTransformFunction to perform a transformation of the function object to deal with limited and fixed variables.; This class uses the same transformation which are also used inside Minuit, a sin transformation for double bounded variables and a sqrt transformation for single bound variable defined in the class MinimizerVariableTransformation.; These classes",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v524/index.html:547,load,load,547,math/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v524/index.html,2,['load'],['load']
Performance,". Math Libraries. MathCore; MathCore includes now classes which were previously contained in libCore, like TMath, TComplex and the TRandom classes. Furthermore, some of the algorithms implemented in the TF1 class have been moved to MathCore. This implies that all other ROOT library using one of these classes, such as libHist, have a direct dependency on the Mathcore library.; Linking with libMathCore is therefore required for running any major ROOT application. It has been added to the list of libraries obtained when doing root-config --libs. N.B.: users building ROOT applications and not using root-config MUST add libMathCore to their list of linking libraries. Together with the libraries merge, many changes have been applied to both TMath and the other mathcore classes.; TMath; A major clean-up and re-structuring has been done for the functions present in TMath. Some functions have been implemented using the STL algorithms, which have better performances in term of CPU time and a template interface has been also added.; Some of the basic special mathematical functions of TMath, like the error function or the gamma and beta functions use now the Cephes implementation from Stephen L. Moshier, which is used as well by the ROOT::Math functions. This implementation has been found to be more accurate and in some cases more efficient in term of CPU time. More detailed information on the new mathematical functions can be found in this presentation from M. Slawinska at a ROOT team meeting. define the functions as template functions instead of having the same re-definition for all the various basic types. This is done for TMath::Mean,TMath::GeomMean, TMath::Median, TMath::KOrdStat; Use STL to implement the following algorithms:; ; TMath::Sort is re-implemented using std::sort.; TMath::BinarySearch is re-implemented using the STL algorithm std::lower_bound. The STL algorithms have been found for these cases to be perform better in term of CPU time. For some other algorithms l",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v520/index.html:958,perform,performances,958,math/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v520/index.html,2,['perform'],['performances']
Performance,". Math Libraries; MathCore; TMath. Add a new function TMath::Power(double, int) impelmented using std::pow(double,int) which is 100% faster than the; double version. KDTree. New KDTree class from C. Gumpert. which has different splitting rules using the data population. The splitting can; is based on the basket population, but any function of the data can be used. For example, in case of weighted data one can split according to; the basket total weight or the effective entries in the basket. In this way the class can support weighted data; sets. The splitting of the TKDTree class is instead fixed and based on the basket number of entries.; The tree can also be frozen to fix the splitting and behaving like a multi-dim histograms with bins with variables; hyper-volumes.; Auxiliary classes are provided like the node classes or the data point class, which can exists also with a compile timed fixed dimension; for better performances. . Fitter classes. Provide support for weighted likelihood unbinned fits; Provide support for extended likelihood unbinned fits; Provide support for not-extended binned likelihood fits (i.e. multinomial distribution instead of Poisson for bin; contents); In case of binned likelihood fit build a Poisson or (Multinomial) likelihood ratio with the saturated model. So a; constant term is subtracted to the likelihood. The formulae described in the Baker and Cousins; paper (N.I.M. 221 (1984) 437) are now used. The obtained negative likelihood ratio value from the fit and multiplied; by a factor 2 is now asymptotically distributed as a chi square. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v532/index.html:929,perform,performances,929,math/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v532/index.html,2,['perform'],['performances']
Performance,". MonteCarlo Libraries; VMC. Update of the VMC interfaces for multi-threading:; TVirtualMC and TVirtualMCApplication instances are now declared thread local.; Added new functions in TVirtualMC and TVirtualMCApplication for multi-threading; applications with default implementation. ; In TVirtualMC:. virtual Bool_t IsMT() const { return kFALSE; }. In TVirtualMCApplication:. virtual TVirtualMCApplication* CloneForWorker() const { return 0;}; virtual void InitForWorker() const {}; virtual void BeginWorkerRun() const {}; virtual void FinishWorkerRun() const {}; virtual void Merge(TVirtualMCApplication* /*localMCApplication*/) {}. Removed default implementation of newly added functions in TVirtualMC:. virtual Bool_t IsRootGeometrySupported() const = 0;; virtual Bool_t GetMaterial(Int_t imat, TString& name,...) = 0;; virtual Bool_t CurrentBoundaryNormal(..) = 0;. Removed deprecated functions from TVirtualMC:. // Return parameters for material specified by material number imat; // Deprecated - replaced with GetMaterial(); virtual void Gfmate(Int_t imat, char *name, Float_t &a, Float_t &z,; Float_t &dens, Float_t &radl, Float_t &absl,; Float_t *ubuf, Int_t &nbuf) = 0;. // Return parameters for material specified by material number imat; // (in double precision); // Deprecated - replaced with GetMaterial(); virtual void Gfmate(Int_t imat, char *name, Double_t &a, Double_t &z,; Double_t &dens, Double_t &radl, Double_t &absl,; Double_t *ubuf, Int_t &nbuf) = 0;. // Check the parameters of a tracking medium; // Deprecated; virtual void Gckmat(Int_t imed, char *name) = 0;. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/montecarlo/doc/v534/index.html:62,multi-thread,multi-threading,62,montecarlo/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/montecarlo/doc/v534/index.html,4,['multi-thread'],['multi-threading']
Performance,. NET release notes. XROOTD. Fixes:; . Fix race condition that would disable connections; . ,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/doc/v522/index.html:43,race condition,race condition,43,net/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/doc/v522/index.html,2,['race condition'],['race condition']
Performance,". Networking. Class TWebFile now supports proxies. Set the proxy URL either via; static method:; TWebFile::SetProxy(const char *url);; or via the shell variable http_proxy, like is used for e.g. wget. Class TWebFile now supports the basic authentication (AuthType Basic) scheme.; The user name and password can be specified in the URL like this:; http://username:mypasswd@pcsalo.cern.ch/files/aap.root. XROOTDNew version 20090610-0430ImprovementsAdd the possibility of using the xrd command line from; batch scriptsAdd support for Adler32 checksum calculation of; a local unix file (including stdin) and file on a remote xrootd data; server.Add support for the so-called Xtreme copy, allowing xrdcp; to read multiple chunks from several servers, in parallel.Add possibility to use a different version of a given C++; compiler or linker (--with-cxx=..., etc)Increase flexibility in configuring openssl and openafs; supportIn GSI authentication, automatize the loading of CRL; the; information; about the URI is looked for either in the dedicated extension on the CA; certificate or from the file ""<CA hash>.crl_url"" and the; file; automatically downloaded and transformed in PEM formatFixesServer sideFix wrong reporting of the refresh option for LocateFix incorrect propagation of selected nodesPrevent potential long duration loop (15 mins) after client disconnectionsAvoid potential deadlocks when trying to remove a node from a clusterCorrect matching of incoming connection with previously dropped connectionCorrect export of cluster identificationCorrectly propagate information about files that could not be stagedPrevent endsess deadlock when parallel streams stall due to large WAN RTTFix infinite wait for primary login that will never; happen if you are a manager without a meta-managerPrevent annoying (but not deadly) infinite loop should a; server go offline that is subject to a locate request display.Client sideBetter handling of errno, especially for parallel streamsAllow the client ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/doc/v524/index.html:959,load,loading,959,net/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/doc/v524/index.html,2,['load'],['loading']
Performance,". Networking. TMessage now has schema evolution and TRef (i.e. ProcessID) support. To; enable; schema evolution for all messages call; TMessage::EnableSchemaEvolutionForAll(kTRUE).; To enable it only for a specific message call; mess->EnableSchemaEvolution(kTRUE).; The default for schema evolution is off. The streamer and process id; information are send only once per socket (and is supported for all; types of; sockets, TSocket, TPSocket and TXSocket). If you communicate between; two; ROOT based applications, check the version numbers on both sides. If; they; are not the same enable the schema evolution support (in case ROOT; objects; are transferred). . XROOTD. New version 20080621-0000 containing several improvements and fixes; Server:. New daemon 'cmsd' supposed to replace 'olbd' with improved performances; Improved polling strategy; Fix problem with handling writev creating unjustified disconnections ; Fix problem with setrlimit on MacOsX Leopard. Client:; ; Fix a nasty memory leak in XrdClientCacheRead affecting; processing via TChain; Optimized file closing recipe; Fix; potential cache thrashing problem with big blocks requests. Fixes / improvements in the GSI plug-in:; ; support for large (> 32 bits) certificate serial; numbers in CRL handling; support for an external function for DN-to-username; mapping function; provide example for an LDAP based search; fixed a few problem with return code checking. netx. TXNetFile:; . Enable dynamic cache size synchronization; ; Enable per-instance control of the cache parameters; also for RAW files; by; default cache is OFF for these files, but there maybe cases in which the cache can; improve performances.; Remove call to XrdClient::Sync in SysStat. Correctly honor the create/recreate options coming from TFile::Open(); Allow the size of the (written) file to be retrieved after the Close (solves several reported file size mismatches).; . TXNetSystem:; ; Fix problem with GetDirEntry: the entry object was; going out-of-scope",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/doc/v520/index.html:808,perform,performances,808,net/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/doc/v520/index.html,2,['perform'],['performances']
Performance,". Note that a store-store fence is not sufficient to; implement Release semantics; store-store fences are generally not exposed to; IR because they are extremely difficult to use correctly. AcquireRelease; --------------. AcquireRelease (``acq_rel`` in IR) provides both an Acquire and a Release; barrier (for fences and operations which both read and write memory). Relevant standard; This corresponds to the C++/C ``memory_order_acq_rel``. Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Acquire only provides a semantic guarantee when paired with a Release; operation, and vice versa. Notes for optimizers; In general, optimizers should treat this like a nothrow call; the possible; optimizations are usually not interesting. Notes for code generation; This operation has Acquire and Release semantics; see the sections on Acquire; and Release. SequentiallyConsistent; ----------------------. SequentiallyConsistent (``seq_cst`` in IR) provides Acquire semantics for loads; and Release semantics for stores. Additionally, it guarantees that a total; ordering exists between all SequentiallyConsistent operations. Relevant standard; This corresponds to the C++/C ``memory_order_seq_cst``, Java volatile, and; the gcc-compatible ``__sync_*`` builtins which do not specify otherwise. Notes for frontends; If a frontend is exposing atomic operations, these are much easier to reason; about for the programmer than other kinds of operations, and using them is; generally a practical performance tradeoff. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. For; SequentiallyConsistent loads and stores, the same reorderings are allowed as; for Acquire loads and Release stores, except that SequentiallyConsistent; operations may not be reordered. Notes for code generation; SequentiallyConsistent loads minimally require the same barriers as Acquire; operations and SequentiallyConsistent stores require Release; barr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:14496,load,loads,14496,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['loads']
Performance,". Note that we; describe this in the `instruction selection section`_ because it operates on; a `SelectionDAG`_. 3. `SSA-based Machine Code Optimizations`_ --- This optional stage consists of a; series of machine-code optimizations that operate on the SSA-form produced by; the instruction selector. Optimizations like modulo-scheduling or peephole; optimization work here. 4. `Register Allocation`_ --- The target code is transformed from an infinite; virtual register file in SSA form to the concrete register file used by the; target. This phase introduces spill code and eliminates all virtual register; references from the program. 5. `Prolog/Epilog Code Insertion`_ --- Once the machine code has been generated; for the function and the amount of stack space required is known (used for; LLVM alloca's and spill slots), the prolog and epilog code for the function; can be inserted and ""abstract stack location references"" can be eliminated.; This stage is responsible for implementing optimizations like frame-pointer; elimination and stack packing. 6. `Late Machine Code Optimizations`_ --- Optimizations that operate on ""final""; machine code can go here, such as spill code scheduling and peephole; optimizations. 7. `Code Emission`_ --- The final stage actually puts out the code for the; current function, either in the target assembler format or in machine; code. The code generator is based on the assumption that the instruction selector will; use an optimal pattern matching selector to create high-quality sequences of; native instructions. Alternative code generator designs based on pattern; expansion and aggressive iterative peephole optimization are much slower. This; design permits efficient compilation (important for JIT environments) and; aggressive optimization (used when generating code offline) by allowing; components of varying levels of sophistication to be used for any step of; compilation. In addition to these stages, target implementations can insert arbitrary; ta",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:6894,optimiz,optimizations,6894,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimizations']
Performance,". Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Acquire only provides a semantic guarantee when paired with a Release; operation, and vice versa. Notes for optimizers; In general, optimizers should treat this like a nothrow call; the possible; optimizations are usually not interesting. Notes for code generation; This operation has Acquire and Release semantics; see the sections on Acquire; and Release. SequentiallyConsistent; ----------------------. SequentiallyConsistent (``seq_cst`` in IR) provides Acquire semantics for loads; and Release semantics for stores. Additionally, it guarantees that a total; ordering exists between all SequentiallyConsistent operations. Relevant standard; This corresponds to the C++/C ``memory_order_seq_cst``, Java volatile, and; the gcc-compatible ``__sync_*`` builtins which do not specify otherwise. Notes for frontends; If a frontend is exposing atomic operations, these are much easier to reason; about for the programmer than other kinds of operations, and using them is; generally a practical performance tradeoff. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. For; SequentiallyConsistent loads and stores, the same reorderings are allowed as; for Acquire loads and Release stores, except that SequentiallyConsistent; operations may not be reordered. Notes for code generation; SequentiallyConsistent loads minimally require the same barriers as Acquire; operations and SequentiallyConsistent stores require Release; barriers. Additionally, the code generator must enforce ordering between; SequentiallyConsistent stores followed by SequentiallyConsistent loads. This; is usually done by emitting either a full fence before the loads or a full; fence after the stores; which is preferred varies by architecture. Atomics and IR optimization; ===========================. Predicates for optimizer writers to query:. * ``isSimple()``: A load or store which is not ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:15007,perform,performance,15007,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['perform'],['performance']
Performance,". Overview:; """""""""""""""""". The '``atomicrmw``' instruction is used to atomically modify memory. Arguments:; """""""""""""""""""". There are three arguments to the '``atomicrmw``' instruction: an; operation to apply, an address whose value to modify, an argument to the; operation. The operation must be one of the following keywords:. - xchg; - add; - sub; - and; - nand; - or; - xor; - max; - min; - umax; - umin; - fadd; - fsub; - fmax; - fmin; - uinc_wrap; - udec_wrap. For most of these operations, the type of '<value>' must be an integer; type whose bit width is a power of two greater than or equal to eight; and less than or equal to a target-specific size limit. For xchg, this; may also be a floating point or a pointer type with the same size constraints; as integers. For fadd/fsub/fmax/fmin, this must be a floating point type. The; type of the '``<pointer>``' operand must be a pointer to that type. If; the ``atomicrmw`` is marked as ``volatile``, then the optimizer is not; allowed to modify the number or order of execution of this; ``atomicrmw`` with other :ref:`volatile operations <volatile>`. Note: if the alignment is not greater or equal to the size of the `<value>`; type, the atomic operation is likely to require a lock and have poor; performance. The alignment is only optional when parsing textual IR; for in-memory IR, it is; always present. If unspecified, the alignment is assumed to be equal to the; size of the '<value>' type. Note that this default alignment assumption is; different from the alignment used for the load/store instructions when align; isn't specified. A ``atomicrmw`` instruction can also take an optional; "":ref:`syncscope <syncscope>`"" argument. Semantics:; """""""""""""""""""". The contents of memory at the location specified by the '``<pointer>``'; operand are atomically read, modified, and written back. The original; value at the location is returned. The modification is specified by the; operation argument:. - xchg: ``*ptr = val``; - add: ``*ptr = *ptr + val``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:430196,optimiz,optimizer,430196,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizer']
Performance,". Overview:; """""""""""""""""". This intrinsic loads a 32-bit value from the address ``%ptr + %offset``,; adds ``%ptr`` to that value and returns it. The constant folder specifically; recognizes the form of this intrinsic and the constant initializers it may; load from; if a loaded constant initializer is known to have the form; ``i32 trunc(x - %ptr)``, the intrinsic call is folded to ``x``. LLVM provides that the calculation of such a constant initializer will; not overflow at link time under the medium code model if ``x`` is an; ``unnamed_addr`` function. However, it does not provide this guarantee for; a constant initializer folded into a function body. This intrinsic can be; used to avoid the possibility of overflows when loading from such a constant. .. _llvm_sideeffect:. '``llvm.sideeffect``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.sideeffect() inaccessiblememonly nounwind willreturn. Overview:; """""""""""""""""". The ``llvm.sideeffect`` intrinsic doesn't perform any operation. Optimizers; treat it as having side effects, so it can be inserted into a loop to; indicate that the loop shouldn't be assumed to terminate (which could; potentially lead to the loop being optimized away entirely), even if it's; an infinite loop with no other side effects. Arguments:; """""""""""""""""""". None. Semantics:; """""""""""""""""""". This intrinsic actually does nothing, but optimizers must assume that it; has externally observable side effects. '``llvm.is.constant.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use llvm.is.constant with any argument type. ::. declare i1 @llvm.is.constant.i32(i32 %operand) nounwind memory(none); declare i1 @llvm.is.constant.f32(float %operand) nounwind memory(none); declare i1 @llvm.is.constant.TYPENAME(TYPE %operand) nounwind memory(none). Overview:; """""""""""""""""". The '``llvm.is.constant``' intrinsic will return true if the argument; is known to be a manifest compile-time c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:951900,perform,perform,951900,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,". PROOF System. Added functionality; ; Added interface to simplify the creation of the performance; tree: two new methods TProof::SetPerfTree(""<file>"") and; TProof::SavePerfTree(""<file>"", ""<queryref>"") allow set; and/or save the information to a given file path. The perfomance tree; settim=ngs are diabled after each query, so they need to be enabled; each time.; Add support for a command line test run of 'proofserv'; this is; useful to test that the environment is setup correctly.; In TProofBench::DrawCPU, add possibility to extract of a couple; of numbers supposed to give an idea of the computing specs of the; cluster being benchmarked. These are the maximum rate for the standard; CPU intensive task and the normalized, per worker, rate. Both are; expressed in RNGPS (RaNdom Generation Per Second).; Add class TProofPerfAnalysis collecting a set of tools to; analyse the performance tree.; Add support for selector-by-object processing in PROOF. The; selector object, created and configured locally by the user, is added; to the input list and recuperated from there on the worker machines for; processing. Any input list setting in the selector itself is not; streamed but temporarly moved to then standard input list, so that user; can use the selector input list as container of processing information; if they find convenient to do so. Process(...) methods with the file; name argument replaced by 'TSelector *' have  introduced where; relevant (TProof, TProofPlayer and their derivatives, TDSet).  ; Add the possibility to force submerging at node level, i.e. one; submerger per physical machine. In this way the network traffic can be; minimized, for example when merging large output files. The new feature; is enabled by setting the Int_t parameter 'PROOF_MergersByHost' (or the; directive 'Proof.MergersByHost') to a non-null value.; Simplify enabling of basic feedback. In TProof::Process, add; support for switches ""fb=name1,name2,name3,... "" or; ""feedback=name1,name2,name3,... """,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v534/index.html:87,perform,performance,87,proof/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v534/index.html,4,['perform'],['performance']
Performance,". PROOF. New functionality. ; Dataset management. ; New class TProofDataSetManager definining the interface; of PROOF to dataset metainfo database; New class TProofDataSetManagerFile implementating; TProofDataSetManager using the file system as back-end;  the; separation is needed to load dataset menagers using different backends;; for example ATLAS foresees to have a MySQL-based implementation.; The instance of the appropriate TProofDataSetManager is; instantiated via the plugin manager; by default an instance; of TProofDataSetManagerFile; managing the <sandbox>/datasets; area is created. The directive 'Proof.DataSetManager' can be used to; modify the settings for TProofDataSetManagerFile or to load a; different dataset manager; for example, to '/pool/datasets' as area for; the dataset information, the following directive can be added to the; xrootd config file; xpd.putrc Proof.DataSetManager file dir:/pool/datasets. Interface to TProofMgr::GetSessionLogs() in the dialog; box. The graphics layout of the logbox has been re-designed, with new; buttons to grep the logs and to save them to a file. It is also; possible to choose the range of lines to be displayed and the subset of; nodes.; ; Support for connection control base on the UNIX group; (new directive 'xpd.allowedgroups; <grp1>,<grp2>, ...'). Improvements:. ; In the case of mismatch between the expected and actual; number of processed events, send back to the client the list of failed; packets.; Implement the classic strategy of the TPacketizer in; TPacketizerAdaptive; the strategy can be changed from adaptive; (default) to TPacketizer with: ""PROOF_PacketizerStrategy"" parameter to; PROOF; The max workers per node can now be also set in the; xrootd config file with.        xpd.putrc ; Packetizer.MaxWorkersPerNode: <desired number>. Make fCacheDir and fPackageDir controllable via directive; . Fixes. ; Two memory leaks in TProofServ affecting repeated runs; withing the same session. Fix a problem cleaning-up the in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v520/index.html:285,load,load,285,proof/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v520/index.html,4,['load'],['load']
Performance,". Proof. New functionality; ; In TProof::Load, add the possibility to define a list of files to be; sent because needed by the main macro file. The list is comma-separated; and the first file is considered as the main one. For example.        ; proof->Load(""<macropath>/mymacro.C+,<thispath>/thisheader.h,<thatpath>/thatheader.h""). will make sure that the files 'thisheader.h' and 'thatheader.h', needed; by 'mymacro.C' are available in the sandbox on the worker machines.; Note that 'thisheader.h' and 'thatheader.h' will be available remotely; in the sandbox, as 'mymacro.C'; so they should be included directly by; 'mymacro.C', e.g. '#include ""thisheader.h""' .; Import the dataset stager daemon 'afdsmgrd' into ROOT; this is used; to manage data staging based on the dataset information (see; http://code.google.com/p/afdsmgrd/ for more info). The daemon is; located under $ROOTSYS/proof/afdsmgrd .; New PROOF bench suite, a framework to run CPU and IO benchmarks with; default selectors/data or with user-provided ones. The code is located; under proof/proofbench.; Add the possibility to access the files on the workers via the same; port used by PROOF. This is useful for cases when it is not possible to; start a file server daemon on a different port (because, for eample, of; a firewall or just inconvenience) and workers do not share a file; system. Internally this works by forking a 'rootd' after identifying a; file request and trasferring the connection to it. The client side is a; TNetFile and it is triggered by the protocol ""rootd://"" (the just; implemented etc/plugins/TFile/P120_TNetFile.C includes this; protocol).; Add support for log file truncation. Truncation is disabled by; default. Enabling is controlled by the rootrc variable.           ; ProofServ.LogFileMaxSize  ; {<bytes>|<kilobytes>K|<megabytes>M|<gigabytes>G}.  indicating the max number of bytes. The number can be followed by; a {K,M,G} to indicate KBytes, MBytes or GBytes, respectively.; Add new derivation of T",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v530/index.html:41,Load,Load,41,proof/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v530/index.html,2,['Load'],['Load']
Performance,". PyROOT. A couple of minor bug fixes related to TPython::LoadMacro, TClass::DynamicCast and the setting of pointers through operator[] on STL containers. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/doc/v530/index.html:58,Load,LoadMacro,58,bindings/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/doc/v530/index.html,1,['Load'],['LoadMacro']
Performance,". PyROOT. Null-pointers now carry type, rather than being the None object, to make sure; that correct overloads are selected.; Memory policy is settable on individual functions, rather than only globally,; through the _mempolicy data member that functions carry. In order to support PyPy analysis of PyROOT code, getter/setter methods have; been added to the proxies.; The pydoc tool already benefits from this, since PyROOT objects are now a bit; easier to inspect by such standard tools. By short-circuiting some paths during class proxy creation, loading of the; libPyROOT module is now faster. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/doc/v524/index.html:550,load,loading,550,bindings/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/doc/v524/index.html,2,['load'],['loading']
Performance,". PyROOT. This release contains two big new features: the ability to use PROOF with; python, and the ability to pickle (python serialize) ROOT objects.; Pickling of ROOT objects is straightforward: just hand them to pickle (or; cPickle) like any other python object.; To use PROOF with python, derive your custom class from TPySelector, override; the methods that you want to specialize, and put it in a file that is shipped; to the worker nodes, e.g.:. from ROOT import TPySelector. class MyPySelector( TPySelector ):; def Begin( self ):; print 'py: beginning'. def SlaveBegin( self, tree ):; print 'py: slave beginning'. def Process( self, entry ):; self.fChain.GetEntry( entry ); print 'py: processing', self.fChain.ipi; return 1. def SlaveTerminate( self ):; print 'py: slave terminating'. def Terminate( self ):; print 'py: terminating'. The file containing the class (e.g. mymodule.py) will be treated as a; python module and should be loadable through PYTHONPATH (typically '.') at; the worker node.; Setup PROOF as normal, and call:. dataset.Process( 'TPySelector', 'mymodule' ). PROOF will instantiate a TPySelector instance, which will in turn pick up; the python class from module 'mymodule' and forward all calls. There are several improvements in language mappings, as well as cleanup of; the code for python2.2 (Py_ssize_t handling) and MacOS 10.3. Additionally,; there are code cleanups (removing use of CINT internals) that should be; fully transparent to the end-user. The language mapping improvements are:. Abstract classes can no longer be instantiated (__init__ will raise an exception); Looping over empty STL(-like) containers will yield an immediate StopIteration; Unknown& is favored over Unknown* in function overloading; Implemented unary-, unary+ (__neg__ and __pos__); Mapped operator bool() to __nonzero__; Support for templated member functions; Implemented __setitem__ for unsigned int& and unsigned long& returns. The python presentation of ROOT objects (ObjectProxy) ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/doc/v520/index.html:942,load,loadable,942,bindings/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/doc/v520/index.html,2,['load'],['loadable']
Performance,. RWebWindow latency test. Halt; Draw; Main: Msg. ,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tutorials/webgui/ping/ping.html:13,latency,latency,13,tutorials/webgui/ping/ping.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/webgui/ping/ping.html,2,['latency'],['latency']
Performance,". Reads a vector from memory according to the provided mask. The mask holds a bit for each vector lane, and is used to prevent memory accesses to the masked-off lanes. The masked-off lanes in the result vector are taken from the corresponding lanes of the '``passthru``' operand. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is the alignment of the source location. It must be a power of two constant integer value. The third operand, mask, is a vector of boolean values with the same number of elements as the return type. The fourth is a pass-through value that is used to fill the masked-off lanes of the result. The return type, underlying type of the base pointer and the type of the '``passthru``' operand are the same vector types. Semantics:; """""""""""""""""""". The '``llvm.masked.load``' intrinsic is designed for conditional reading of selected vector elements in a single IR operation. It is useful for targets that support vector masked loads and allows vectorizing predicated basic blocks on these targets. Other targets may support this intrinsic differently, for example by lowering it into a sequence of branches that guard scalar load operations.; The result of this operation is equivalent to a regular vector load instruction followed by a 'select' between the loaded and the passthru values, predicated on the same mask. However, using this intrinsic prevents exceptions on memory access to masked-off lanes. ::. %res = call <16 x float> @llvm.masked.load.v16f32.p0(ptr %ptr, i32 4, <16 x i1>%mask, <16 x float> %passthru). ;; The result of the two following instructions is identical aside from potential memory access exception; %loadlal = load <16 x float>, ptr %ptr, align 4; %res = select <16 x i1> %mask, <16 x float> %loadlal, <16 x float> %passthru. .. _int_mstore:. '``llvm.masked.store.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. The data stored in memory is a vecto",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:844741,load,loads,844741,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,". RooFit Package. RooFit 3.50 has undergone a substantial amount of core engineering to improve computational efficiency; and improve algorithmic likelihood optimizations. The expected increases in execution speed range from; roughly 20% (for problems that were already implemented in a close-to optimal form) to more than 2000%; for certain type of problems. Below is a summary of the changes made. All of these changes are; transparent to end-use cases ; ; ; New implementation of RooFit data types. The implementation of data stored in RooDataSet and RooDataHist; was historically handled by ROOT TTrees (though class RooTreeDataStore). The default storage type; has now been changed to class RooVectorDataStore which stores the information in STL arrays. Existing; datasets based on trees can be read in transparently, and are converted to vector form in the ; persistent-to-transient conversion (the datafile is not modified in this operation); ; The vector store has two important advantages: 1) faster data access (raw data access times are 70 times ; faster than for TTrees), 2) ability to rewrite columns on the fly. The first advantage is important; for the existing constant-term precalculation optimization in roofit likelihoods as these are now; also stored in vectors rather than trees. The faster access speed of vectors make that the constant; term optimization inside likelihoods results in a larger speed increase. This is particulatly noticeable in pdfs with; many constant expressions from pdfs that were moderately fast to begin with (e.g. RooHistPdf).; The second advantages allows new types of algorithmic likelihood optimization in RooFit detailed below. New algorithmic optimization in the caching of pdfs. So far - in the likelihood - two classes of; objects are identified: those that change with every event (i.e. the pdf) and those that change; only with the parameters (typically pdf normalization integrals). Pdfs are always recalculated; for every event, whereas integr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html:157,optimiz,optimizations,157,roofit/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v532/index.html,2,['optimiz'],['optimizations']
Performance,". See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operations use an L2 cache shared by all CUs on; the same agent. * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel. Therefore, the vector and; scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different CUs), or the same; work-group if executing in tgsplit mode, of an agent can be reordered; relative to each other. A ``s_waitcnt vmcnt(0)`` is required to ensure; synchronization between vector memory operations of different CUs. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire and release.; * The L2 cache of one agent can be kept coherent with other agents by:; using the MTYPE RW (read-write) or MTYPE CC (cache-coherent) with the PTE; C-bit for memory local to the L2; and using the MTYPE NC (non-coherent) with; the PTE C-bit set or MTYPE UC (uncached) for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by coherent requests. Coherent requests are caused; by GPU accesses to pages with the PTE C-bit set, by CPU accesses over; XGMI, and by PCIe requests that are configured to be coherent requests.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequent access from the GPU will automatically invalidate or writeback; the CPU cache due to the L2 probe filter and and the PTE C-bit being set.; * Since all work-groups on the same agent share the same L2, no L2; invalidation or writeback is required for coherence.; * To ensure coherence of local and remote memory writes of work-groups in; different agents a ``buffer_wbl2`` is required. It will writeb",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:237609,cache,cache,237609,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['cache'],"['cache', 'cache-coherent']"
Performance,". Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <2 x double> @llvm.experimental.vector.splice.v2f64(<2 x double> %vec1, <2 x double> %vec2, i32 %imm); declare <vscale x 4 x i32> @llvm.experimental.vector.splice.nxv4i32(<vscale x 4 x i32> %vec1, <vscale x 4 x i32> %vec2, i32 %imm). Overview:; """""""""""""""""". The '``llvm.experimental.vector.splice.*``' intrinsics construct a vector by; concatenating elements from the first input vector with elements of the second; input vector, returning a vector of the same type as the input vectors. The; signed immediate, modulo the number of elements in the vector, is the index; into the first vector from which to extract the result value. This means; conceptually that for a positive immediate, a vector is extracted from; ``concat(%vec1, %vec2)`` starting at index ``imm``, whereas for a negative; immediate, it extracts ``-imm`` trailing elements from the first vector, and; the remaining elements from ``%vec2``. These intrinsics work for both fixed and scalable vectors. While this intrinsic; is marked as experimental, the recommended way to express this operation for; fixed-width vectors is still to use a shufflevector, as that may allow for more; optimization opportunities. For example:. .. code-block:: text. llvm.experimental.vector.splice(<A,B,C,D>, <E,F,G,H>, 1); ==> <B, C, D, E> index; llvm.experimental.vector.splice(<A,B,C,D>, <E,F,G,H>, -3); ==> <B, C, D, E> trailing elements. Arguments:; """""""""""""""""""". The first two operands are vectors with the same type. The start index is imm; modulo the runtime number of elements in the source vector. For a fixed-width; vector <N x eltty>, imm is a signed integer constant in the range; -N <= imm < N. For a scalable vector <vscale x N x eltty>, imm is a signed; integer constant in the range -X <= imm < X where X=vscale_range_min * N. '``llvm.experimental.stepvector``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This is an overloaded intrinsic. You can use ``llvm.exper",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:671497,scalab,scalable,671497,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,". TH1. Add support for weighted likelihood fit of histogram by using a new option, WL and suppress the old option; LL.; The histogram must have the sum of the weight squared stored bin by bin to use this fit option; (i.e. TH1::Sumw2() has been called before filling).; Now one can perform likelihoof fit to weighted or scaled histograms and get the correct errors in the fit parameters.; (see bug report 79754).; ; Fix for the bug 82562.; Fix a bug in TH1::Merge for histogram with labels (bug 75902).; Fix few bugs related with the Buffer. . TProfile. Fix a bug in TProfile::Merge when the kCanRebin bit is set; (bug 79675).; Fix a bug in LabelsDeflate (bug 77149). TH1. Add new method TH3::Rebin3D and alsoRebinX, RebinY and RebinZ thanks to Zhiyi Liu. THistPainter. TPad::SetTheta() and TPad::SetPhi() did not cause the; canvas redrawing.; Protection added in case two histograms were plotted in the same pad; using the option BOX (the 2nd one with option SAME).; The clipping was not correct when an interactive zoom was performed. The 2D functions, for instance a fit function, associated to a 2D; histogram were always drawn as scatter plots. This was very confusing.; Now they are drawn as a surface if the histogram plotting option is a; lego or a surface or as a contour plot for any other plotting options. When drawing scatter plot for TH2 or TH2Poly do not use gRandom, but an independent random generator instance,; to avoid interfering with gRandom (bug 83021).; Now the same random sequence is always used for drawing the same histograms, giving therefore exactly the same scatter plot for the same; histogram, while before a slightly different plot was obtained every time. TH2Poly. Add(const TH1 *h1, Double_t c1) has been implemented.; Reset() has been implemented.; The destructor has been completed. THStack. When the 1D histograms in a stack are painted with patterns or hatches; the histograms are first painted with the TFrame background color to avoid; the hatches overlaps. I",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v530/index.html:1581,perform,performed,1581,hist/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v530/index.html,2,['perform'],['performed']
Performance,". TMVA. TMVA version 4.0.4 is included in this root release. Methods. A new Category method allowing the user to; separate the training data (and accordingly the application; data) into disjoint sub-populations exhibiting significantly; different properties. The separation into phase space regions is; done by applying requirements on the input and/or spectator; variables. In each of these disjoint regions (each event must; belong to one and only one region), an independent training is; performed using the most appropriate MVA method, training; options and set of training variables in that zone. The division; into categories in presence of distinct sub-populations reduces; the correlations between the training variables, improves the; modelling, and hence increases the classification and regression; performance. Presently, the Category method works for; classification only, but regression will follow soon. Please; contact us if urgently needed. An example scripts and data files illustrating how the new; Category method is configured and used. Please check the macros; test/TMVAClassificationCategory.C and; test/TMVAClassificationCategoryApplication.C or the; corresponding executables.; Regression functionality for gradient boosted trees using a Huber loss function. Comments. On Input Data: . New TMVA event vector building. The code for splitting the input; data into training and test samples for all classes and the; mixing of those samples to one training and one test sample has; been rewritten completely. The new code is more performant and; has a clearer structure. This fixes several bugs which have been; reported by the TMVA users. On Minimization: . Variables, targets and spectators are now checked if they are; constant. (The execution of TMVA is stopped for variables and; targets, a warning is given for spectators.). On Regression:; ; The analysis type is no longer defined by calling a dedicated; TestAllMethods-member-function of the Factory, but with the; option ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v526/index.html:491,perform,performed,491,tmva/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v526/index.html,4,['perform'],"['performance', 'performed']"
Performance,". The '``llvm.experimental.stepvector``' intrinsics are used to create vectors; of integers whose elements contain a linear sequence of values starting from 0; with a step of 1. This experimental intrinsic can only be used for vectors; with integer elements that are at least 8 bits in size. If the sequence value; exceeds the allowed limit for the element type then the result for that lane is; undefined. These intrinsics work for both fixed and scalable vectors. While this intrinsic; is marked as experimental, the recommended way to express this operation for; fixed-width vectors is still to generate a constant vector instead. Arguments:; """""""""""""""""""". None. '``llvm.experimental.get.vector.length``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.experimental.get.vector.length.i32(i32 %cnt, i32 immarg %vf, i1 immarg %scalable); declare i32 @llvm.experimental.get.vector.length.i64(i64 %cnt, i32 immarg %vf, i1 immarg %scalable). Overview:; """""""""""""""""". The '``llvm.experimental.get.vector.length.*``' intrinsics take a number of; elements to process and returns how many of the elements can be processed; with the requested vectorization factor. Arguments:; """""""""""""""""""". The first argument is an unsigned value of any scalar integer type and specifies; the total number of elements to be processed. The second argument is an i32; immediate for the vectorization factor. The third argument indicates if the; vectorization factor should be multiplied by vscale. Semantics:; """""""""""""""""""". Returns a positive i32 value (explicit vector length) that is unknown at compile; time and depends on the hardware specification.; If the result value does not fit in the result type, then the result is; a :ref:`poison value <poisonvalues>`. This intrinsic is intended to be used by loop vectorization with VP intrinsics; in order to get the number of elements to process on each loop iteration. The; result should be used",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:673793,scalab,scalable,673793,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,". The call to `pn1->Align()` will invalidate the pointer to the node `B_1`; in `pn2` object.. The way out is to either call `pn1->Align()` before; the creation of `pn2`, either to use a global method that will correct; all existing physical nodes:. ``` {.cpp}; void RefreshPhysicalNodes(Bool_t lock = kTRUE); ```. The method above will optionally lock the possibility of doing any; further misalignment. ## Geometry I/O. Once geometry is successfully built, it can be saved in a root file, as; C++ macro or as GDML file by calling:. ``` {.cpp}; TGeoManager::Export(const char *filename,const char*keyname="""",; Option_t *opt=""vg""); ```. - `Filename`is the name of the file to be written (mandatory).; Depending on the extension of the file, the geometry is exported; either as ,root file or .C(.cxx) macro or GDML file in case; extension is .gdml.; - `keyname`is the name of the key in the file (default """"); - `opt` = `""v""` is an export voxelization (default), otherwise; voxelization is recomputed after loading the geometry, `""g""` this; option (default) is taken into account only for exporting to gdml; file and it ensures compatibility with Geant4 (e.g. it adds extra; plane to incorrectly set polycone, it checks whether offset of Phi; division is in (-360;0\> range, ...), for this gdml export there are; two more option, that are not set by default: `""f""` and `""n""`. If; none of this two options are set, then names of solids and volumes; in resulting gdml file will have incremental suffix (e.g.; TGeoBBox\_0x1, TGeoBBox\_0x2, ...). If `""f""` option is set then then; suffix will contain pointer of object (e.g. TGeoBBox\_0xAAAAA01,; ...). Finally if option `""n""` is set then no suffix will be added,; though in this case uniqueness of the names is not ensured and it can; cause that file will be invalid. Loading geometry from a root file can be done in the same way as for any; other ROOT object, but a static method is also provided:. ``` {.cpp}; TGeoManager::Import(const char *filename,con",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:150653,load,loading,150653,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['load'],['loading']
Performance,". The exception is when in; tgsplit execution mode as wavefronts of the same work-group can be in; different CUs and so a ``buffer_wbinvl1_vol`` is required as described in; the following item. * A ``buffer_wbinvl1_vol`` is required for coherence between wavefronts; executing in different work-groups as they may be executing on different; CUs. * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a group of CUs. The scalar and vector L1 caches are not coherent. However,; scalar operations are used in a restricted way so do not impact the memory; model. See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operations use an L2 cache shared by all CUs on; the same agent. * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel. Therefore, the vector and; scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different CUs), or the same; work-group if executing in tgsplit mode, of an agent can be reordered; relative to each other. A ``s_waitcnt vmcnt(0)`` is required to ensure; synchronization between vector memory operations of different CUs. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire and release.; * The L2 cache of one agent can be kept coherent with other agents by:; using the MTYPE RW (read-write) or MTYPE CC (cache-coherent) with the PTE; C-bit for memory local to the L2; and using the MTYPE NC (non-coherent) with; the PTE C-bit set or MTYPE UC (uncached) for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by coherent requests. Coherent requests are caused; by GPU accesses to pages with the PTE",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:237081,perform,performed,237081,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,". The first argument is an operand which is used as the returned value. Overview:; """""""""""""""""""". The ``llvm.ssa.copy`` intrinsic can be used to attach information to; operations by copying them and giving them new names. For example,; the PredicateInfo utility uses it to build Extended SSA form, and; attach various forms of information to operands that dominate specific; uses. It is not meant for general use, only for building temporary; renaming forms that require value splits at certain points. .. _type.test:. '``llvm.type.test``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i1 @llvm.type.test(ptr %ptr, metadata %type) nounwind memory(none). Arguments:; """""""""""""""""""". The first argument is a pointer to be tested. The second argument is a; metadata object representing a :doc:`type identifier <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.test`` intrinsic tests whether the given pointer is associated; with the given type identifier. .. _type.checked.load:. '``llvm.type.checked.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load(ptr %ptr, i32 %offset, metadata %type) nounwind memory(argmem: read). Arguments:; """""""""""""""""""". The first argument is a pointer from which to load a function pointer. The; second argument is the byte offset from which to load the function pointer. The; third argument is a metadata object representing a :doc:`type identifier; <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.checked.load`` intrinsic safely loads a function pointer from a; virtual table pointer using type metadata. This intrinsic is used to implement; control flow integrity in conjunction with virtual call optimization. The; virtual call optimization pass will optimize away ``llvm.type.checked.load``; intrinsics associated with devirtualized calls, thereby removing the type; check in cases where it is not needed to enforce the control flow integrity; constraint. If the giv",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:937679,load,load,937679,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,". The index; of the current suspension point of the coroutine is emitted as `__coro_index`.; In the above example, the `__coro_index` value of `1` means the coroutine; stopped at the second suspend point (Note that `__coro_index` is zero indexed); which is the first `co_await await_counter{};` in `coro_task`. Note that the; first initial suspend point is the compiler generated; `co_await promise_type::initial_suspend()`. However, when optimizations are enabled, the printed result changes drastically:. .. parsed-literal::. {__resume_fn = 0x401280 <coro_task(int)>, __destroy_fn = 0x401390 <coro_task(int)>, __promise = {count = 1}, __int_32_0 = 43, __coro_index = 1 '\001'}. Unused values are optimized out, as well as the name of the local variable `a`.; The only information remained is the value of a 32 bit integer. In this simple; case, it seems to be pretty clear that `__int_32_0` represents `a`. However, it; is not true. An important note with optimization is that the value of a variable may not; properly express the intended value in the source code. For example:. .. code-block:: c++. static task coro_task(int v) {; int a = v;; co_await await_counter{};; a++; // __int_32_0 is 43 here; std::cout << a << ""\n"";; a++; // __int_32_0 is still 43 here; std::cout << a << ""\n"";; a++; // __int_32_0 is still 43 here!; std::cout << a << ""\n"";; co_await await_counter{};; a++; // __int_32_0 is still 43 here!!; std::cout << a << ""\n"";; a++; // Why is __int_32_0 still 43 here?; std::cout << a << ""\n"";; }. When debugging step-by-step, the value of `__int_32_0` seemingly does not; change, despite being frequently incremented, and instead is always `43`.; While this might be surprising, this is a result of the optimizer recognizing; that it can eliminate most of the load/store operations. The above code gets; optimized to the equivalent of:. .. code-block:: c++. static task coro_task(int v) {; store v to __int_32_0 in the frame; co_await await_counter{};; a = load __int_32_0; std::cou",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst:8835,optimiz,optimization,8835,interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,1,['optimiz'],['optimization']
Performance,". The most appealing way to harden loads is to mask out all of the bits loaded.; The key requirement is that for each bit loaded, along the misspeculated path; that bit is always fixed at either 0 or 1 regardless of the value of the bit; loaded. The most obvious implementation uses either an `and` instruction with; an all-zero mask along misspeculated paths and an all-one mask along correct; paths, or an `or` instruction with an all-one mask along misspeculated paths; and an all-zero mask along correct paths. Other options become less appealing; such as multiplying by zero, or multiple shift instructions. For reasons we; elaborate on below, we end up suggesting you use `or` with an all-ones mask,; making the x86 instruction sequence look like the following:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; movl (%rsi), %edi # Load potentially secret data from %rsi.; orl %eax, %edi; ```. Other useful patterns may be to fold the load into the `or` instruction itself; at the cost of a register-to-register copy. There are some challenges with deploying this approach:; 1. Many loads on x86 are folded into other instructions. Separating them would; add very significant and costly register pressure with prohibitive; performance cost.; 1. Loads may not target a general purpose register requiring extra instructions; to map the state value into the correct register class, and potentially more; expensive instructions to mask the value in some way.; 1. The flags registers on x86 are very likely to be live, and challenging to; preserve cheaply.; 1. There are many more values loaded than pointers & indices used for loads. As; a consequence, hardening the result of a load requires substantially more; instructions than hardening the address of the load (see below). Despite these challenges, hardening the result of the load critically allows; the load to proceed and thus has dramatically less impact on the total; speculative / out-of-order pote",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:23719,load,load,23719,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load']
Performance,". The rules mentioned in; this section only pertain to TBAA nodes living under the same root. .. _tbaa_node_semantics:. Semantics; """""""""""""""""". The TBAA metadata system, referred to as ""struct path TBAA"" (not to be; confused with ``tbaa.struct``), consists of the following high level; concepts: *Type Descriptors*, further subdivided into scalar type; descriptors and struct type descriptors; and *Access Tags*. **Type descriptors** describe the type system of the higher level language; being compiled. **Scalar type descriptors** describe types that do not; contain other types. Each scalar type has a parent type, which must also; be a scalar type or the TBAA root. Via this parent relation, scalar types; within a TBAA root form a tree. **Struct type descriptors** denote types; that contain a sequence of other type descriptors, at known offsets. These; contained type descriptors can either be struct type descriptors themselves; or scalar type descriptors. **Access tags** are metadata nodes attached to load and store instructions.; Access tags use type descriptors to describe the *location* being accessed; in terms of the type system of the higher level language. Access tags are; tuples consisting of a base type, an access type and an offset. The base; type is a scalar type descriptor or a struct type descriptor, the access; type is a scalar type descriptor, and the offset is a constant integer. The access tag ``(BaseTy, AccessTy, Offset)`` can describe one of two; things:. * If ``BaseTy`` is a struct type, the tag describes a memory access (load; or store) of a value of type ``AccessTy`` contained in the struct type; ``BaseTy`` at offset ``Offset``. * If ``BaseTy`` is a scalar type, ``Offset`` must be 0 and ``BaseTy`` and; ``AccessTy`` must be the same; and the access tag describes a scalar; access with scalar type ``AccessTy``. We first define an ``ImmediateParent`` relation on ``(BaseTy, Offset)``; tuples this way:. * If ``BaseTy`` is a scalar type then ``ImmediateParent",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:274554,load,load,274554,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,". They may touch on one boundaries or shape vertex. The daughter nodes of a volume can be also removed or replaced with; other nodes:. ~~~{.cpp}; void RemoveNode(TGeoNode* node); TGeoNode*ReplaceNode(TGeoNode* nodeorig, TGeoShape* newshape = 0,; TGeoMatrix* newpos = 0, TGeoMedium* newmed = 0); ~~~. The last method allows replacing an existing daughter of a volume with; another one. Providing only the node to be replaced will just create a; new volume for the node but having exactly the same parameters as the; old one. This helps in case of divisions for decoupling a node from the; logical hierarchy so getting new content/properties. For non-divided; volumes, one can change the shape and/or the position of the daughter. \anchor GP01bd; #### Virtual Containers and Assemblies of Volumes. Virtual containers are volumes that do not represent real objects, but; they are needed for grouping and positioning together other volumes.; Such grouping helps not only geometry creation, but also optimizes; tracking performance; therefore, it is highly recommended. Virtual; volumes need to inherit material/medium properties from the volume they; are placed into in order to be ""invisible"" at tracking time. Let us suppose that we need to group together two volumes `A` and `B`; into a structure and position this into several other volumes `D,E,` and; `F`. What we need to do is to create a virtual container volume `C`; holding `A` and `B`, then position `C` in the other volumes. Note that `C` is a volume having a determined medium. Since it is not a; real volume, we need to manually set its medium the same as that of; `D,E` or `F` in order to make it ""invisible"" (same physics properties).; In other words, the limitation in proceeding this way is that `D,E,` and; `F` must point to the same medium. If this was not the case, we would; have to define different virtual volumes for each placement: `C`, `C`'; and `C`\"", having the same shape but different media matching the; corresponding cont",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:30286,optimiz,optimizes,30286,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,2,"['optimiz', 'perform']","['optimizes', 'performance']"
Performance,". This is basically the same as ``switch`` statement in ``__builtin_expect``.; The probability that ``exp`` is equal to the expect value is given in; the third argument ``probability``, while the probability of other value is; the average of remaining probability(``1.0 - probability``). For example:. .. code-block:: c++. switch (__builtin_expect_with_probability(x, 5, 0.7)) {; default: break; // Take this case with probability 10%; case 0: break; // Take this case with probability 10%; case 3: break; // Take this case with probability 10%; case 5: break; // This case is likely to be taken with probability 70%; }. CFG Modifications; =================. Branch Weight Metatada is not proof against CFG changes. If terminator operands'; are changed some action should be taken. In other case some misoptimizations may; occur due to incorrect branch prediction information. Function Entry Counts; =====================. To allow comparing different functions during inter-procedural analysis and; optimization, ``MD_prof`` nodes can also be assigned to a function definition.; The first operand is a string indicating the name of the associated counter. Currently, one counter is supported: ""function_entry_count"". The second operand; is a 64-bit counter that indicates the number of times that this function was; invoked (in the case of instrumentation-based profiles). In the case of; sampling-based profiles, this operand is an approximation of how many times; the function was invoked. For example, in the code below, the instrumentation for function foo(); indicates that it was called 2,590 times at runtime. .. code-block:: llvm. define i32 @foo() !prof !1 {; ret i32 0; }; !1 = !{!""function_entry_count"", i64 2590}. If ""function_entry_count"" has more than 2 operands, the later operands are; the GUID of the functions that needs to be imported by ThinLTO. This is only; set by sampling based profile. It is needed because the sampling based profile; was collected on a binary that had alre",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BranchWeightMetadata.rst:5731,optimiz,optimization,5731,interpreter/llvm-project/llvm/docs/BranchWeightMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BranchWeightMetadata.rst,1,['optimiz'],['optimization']
Performance,". Tree Libraries; Performance. Automatic support for multiple TTreeCache per TFile.; Multiple TTreeCache per TFile for reading are supported by using the existing TTree::SetCacheSize(Long64_t) interface.; In addition, a TTreeCache for a TTree can be added using TFile::SetCacheRead(TFileCacheRead*, TObject*), where the second (optional) argument is a pointer to the TTree. The cache can be removed by setting the pointer to 0. In that case the user will have to take ownership for the cache.; Similarily, a pointer to the TTreeCache for a TTree can be obtained using TFile::GetCacheRead(TObject*). In TBuffer::Expand, when shrinking the buffer do not shrink below the size of the; data already accumulated in the buffer (i. no less than the value of TBuffer::Length). In TBranch::SetBasketSize, instead of using the hard minimum of 100, use; 100 + the length of the branch name (as 100 is too small to hold the; basket's key information for any branch name larger than 30 characters). Reading form text file. Reworked TTree::ReadStream and TTree::ReadFile mainly to fix delimited reading of string columns:. TLeaf::ReadValue now takes an optional delimiter argument that is ignored for all but TLeafC. Here, input stops when reading this character, instead of at the first whitespace.; Use that in TTree::ReadStream() to delimit reading of TLeafC.; TTree::ReadStream now tokenizes the row itself, and passes a stringstream containing nothing but the current column to TLeaf::ReadValue.; Separate concepts of number of input line (for communication with user) and number of good lines (as returned).; Fix windows files leaving '\n' in branch names when reading them from the file.; Add error message for TLeaf::ReadValue(), i.e. if ReadValue() is called on a derived class that doesn't implement it.; Updated and clarified the documentation. TEntryList. Add new methods to find the base location of files and to modify it.; This allows to relocate the entry-lists to be able to use them of a; system w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v534/index.html:378,cache,cache,378,tree/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v534/index.html,5,"['Perform', 'cache']","['Performance', 'cache']"
Performance,". Tree Libraries; TEntryListArray: a list of entries and subentries in a TTree or TChain. TEntryListArray is an extension of TEntryList, used to hold selected entries and subentries (sublists) for when the user has a TTree with containers (vectors, arrays, ...). Usage with TTree::Draw to select entries and subentries . To fill a list elist :; ; tree->Draw("">> elist"", ""x > 0"", ""entrylistarray"");; . To use a list to select entries and subentries:. tree->SetEntryList(elist);; tree->Draw(""y"");; tree->Draw(""z"");; . Its main purpose is to improve the performance of a code that needs to apply complex cuts on TTree::Draw multiple times. After the first call above to TTree::Draw, a TEntryListArray is created and filled with the entries and the indices of the arrays that satisfied the selection cut (x > 0). In the subsequent calls to TTree::Draw, only these entries / subentries are used to fill histograms. About the class . The class derives from TEntryList and can be used basically in the same way. This same class is used to keep entries and subentries, so there are two types of TEntryListArray's:. The ones that only hold subentries; fEntry is set to the entry# for which the subentries correspond; fSubLists must be 0; The ones that hold entries and eventually lists with subentries in fSubLists.; fEntry = -1 for those; If there are no sublists for a given entry, all the subentries will be used in the selection. Additions with respect to TEntryList ; Data members:; fSubLists: a container to hold the sublists; fEntry: the entry number if the list is used to hold subentries; fLastSubListQueried and fSubListIter: a pointer to the last sublist queried and an iterator to resume the loop from the last sublist queried (to speed up selection and insertion in TTree::Draw); Public methods:; Contains, Enter and Remove with subentry as argument; GetSubListForEntry: to return the sublist corresponding to the given entry; Protected methods:; AddEntriesAndSubLists: called by Add when adding t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v532/index.html:551,perform,performance,551,tree/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v532/index.html,2,['perform'],['performance']
Performance,". Tree. Restore support for IsAutoDelete in a TBranchElement (IsAutoDelete is an explicit request by the user to have the object deleted/newed each time GetEntry is called).; Allow .root in the name of directory in TChain::Add and TChain::AddFile (however in this case the root file must be ending .root.); Improve support for circular TTree friendship in LoadTree.; Insure that the in-memory tree (not attached to a file) are saved in their new style (i.e. each basket saved separately) and prevent the printing of the misleading error message:; Error in : Cannot create key without file ; Repaired TTreeSQL:; The existing code was not compatible with the change made in TTree to reduce the number of baskets in memory.; If the TreeFriend is entered via a TTree*, properly detect that it is in the same file and do not record the filename (since we will alway know where to find it.); Add "","" in the list of special characters replaced by ""_"" in the TTree::MakeClass; and TTree::MakeCode functions.; The fast cloning now explicitly rejects trying to merge TTrees with different split level; The fast cloning now supports the case where one of the branch in the output tree in; not present and also supports the case where branch are not the same order.; New bit flag kMapObject [mybranch->ResetBit(kMapObject)] to explicitly disable the; object registration during streaming within a branch (Use only if you are sure that there; is not a pointer pointing back to the nesting object within this branch). Fix tree->Draw(""s1.value"");; when the top level branch does not have a trailing dot; (and hence the real branch name is only 'value'). Fixed support for vector<bool> and vector<string> ; Added support for top level object that do not inherit from TObject _AND_ have a custom streamer (like std::string and TString);; Tree Viewer. In TParallelCoordVar the ""average marker"" for candle plots was not painted at; the right place in case of horizontal view.; Protection added in:; TParallelCoord::TPara",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v522/index.html:356,Load,LoadTree,356,tree/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v522/index.html,1,['Load'],['LoadTree']
Performance,". Tree. Significantly improve performance of TTree Proxy.; Improve read performance of sub-branch containing vector of single types.; Fix TTree::LoadBasket to properly handle the (new) case where no basket is stored with the TTree object.; Fix the axis used for an histogram created by TTree::Draw for a branch of TString or std::string objects.; MakeProxy now correctly support branches that created with a leaflist with more than one leaf; (usually used for C-struct).; TTree::CloneTree and TChain::Merge in fast mode now can recover from some mismatch errors between; the input and output TTrees by falling back to using the 'slow' mode. In particular this allow; a 'fast cloning' to handle files that requires schema evolution (albeit it is of course much slower).; Make sure that the TTreeCache is not attempting to cache (wrongly) the content of branches that are in an auxiliary files.; Make sure that FillBuffer does it work when the learning phase is over even if the entry number is 'low' for the 'current' file of a chain.; If TTree::SetEventList is called, TTree::GetEntryList no longer relinquish ownership of the automatically created TEntryList; Add the ability to see the TTree UserInfo list from the TBrowser; Fix the case of reading a TTree containing an 'old' class layout that contained a std::vector that is no longer part of the current class layout; Implement direct interfaces from TTree to the result of TSelector::Draw; TTree:GetVal(int) and TTree::GetVar(int); In TTree::ReadFile add the possibility to read multiple input files and add support for large/wide Trees definition.; Added support for ""5-D"" plotting.; Added support for std::bitset; Reduce the memory used by the mechanism keeping track of the entry of variables sizes within a basket (fEntryOffset).; The memory used now automatically decrease if the number of entries in the basket is less than 1/4 oflength of fEntryOffset.; Also the default length fEntryOffset can be set via TTree::SetDefaultEntryOffsetLen ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v524/index.html:30,perform,performance,30,tree/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v524/index.html,7,"['Load', 'cache', 'perform']","['LoadBasket', 'cache', 'performance']"
Performance,". Tree; Performance. Introduce support for TTree with variable cluster size (i.e. value of fAutoFlush).; Iterating through the cluster should be done via the new class TTree::TClusterIterator (i.e. this replaces += fAutoFlush):. TTree::TClusterIterator clusterIter = tree->GetClusterIterator(which_entry_to_start_from);; Long64_t clusterStart;; while( (clusterStart = clusterIter()) < tree-<GetEntries()) {; printf(""The cluster starts at %lld and ends at %lld\n"",clusterStart,clusterIter.GetNextEntry()-1);; }; See TTreeCache::FillBuffer for a concrete usage example. Significant improvement of the performance of SetBranchAddress/SetAddress (by a factor 3 to 10 depending on the length/complexity of the classname).; Prevent the unlimited growth of the TBasket's buffer even if the basket is reused.; When the basket is Reset (this happens when it is written and will be reused),; if the TBuffer size is greater than. - twice the data in the current basket; and - twice the average data in each basket (of this branch); and - twice the requeste basket size (TBranch::GetBasketSize).; the size of the buffer is reduced to the max of; 'the data in the current basket' and 'the average' and the requested; buffer size and aligned to next highest multiple of 512.; In TBranchRef distinguish between the entry we need (now called RequestedEntry) and the; entry we have read (fReadEntry) so that we can avoid re-reading the same entry too many; times when executing TRef::GetObject.; Reduce by 40% the time taken GetEntry for a branch created using a leaflist (exclusive of the decompression time).; Introduce TVirtualPerfStats::FileUnzipEvent to be able to keep track of the cost of unzipping and use this in TTreePerfStats and TBasket ... This give a good picture of where the time in unzip or in unstreaming; Add more clusters to the TTreeCache buffer until fBufferMinSize is hit to avoid severely underfilled buffer when; a low number of branches is selected/used.; When reading backwards, make sure to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v530/index.html:599,perform,performance,599,tree/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v530/index.html,3,"['Perform', 'perform']","['Performance', 'performance']"
Performance,". Use TAxis attributes in lego plots - ticks/labels/title colors, sizes, offsets; 2. Correctly resize stats box when number of lines changes; 3. Support JSROOT usage with yarn and webpack; 4. Provide `FileProxy` class to let read ROOT files from arbitrary place; 5. Let 'hook' save file functionality to use alternative method to store image files; 6. Implement 'tabs' layout for objects display (#238); 7. Upgrade d3.js to version 7.6.1; 8. Fix - adjust pad margins when moving palette and frame. ## Changes in 7.1.1; 1. Fix - let modify node visibility bits via context menu; 2. Fix - menu position adjusting; 3. Fix - tree_draw.js example, export treeDraw function from main.mjs; 4. Fix - TH3 scatter plot with large number of bins converted to box2; 5. Fix - create geo css entries also when expand object in hierarchy (#240). ## Changes in 7.1.0; 1. Let change `settings` and `gStyle` parameters via ""Settings"" menu of the top hierarchy item; 2. Settings and gStyle can be stored as cookies, automatically read when next time loading webpage; 3. `settings.OnlyLastCycle` defines if only last object version shown in TFile (also as `&lastcycle` URL parameter); 4. `settings.DarkMode` configures dark mode for GUI and drawings (also as `&dark` URL parameter); 5. Support new `TGraph2DAsymmErrors` class; 6. Support `gStyle.fOptDate` and `gStyle.fOptFile` (also as `&optdate` and `&optfile` URL parameter); 7. Support `gStyle.fDateX` and `gStyle.fDateY` used for positioning date and file on canvas; 8. Support `gStyle.fHistTopMargin` (also as `&histmargin=value` URL parameter); 9. Let save frame, title and stats properties to `gStyle` via correspondent context menus; 10. Support majority of special symbols in TMathText; 11. Fix several issues with TPaveText. ## Changes in 7.0.2; 1. Fix - TH2 arrow drawing; 2. Fix - interactive change of fonts attributes; 3. Fix - proper draw results of TTree::Draw; 4. Fix - draw new histogram on same canvas. ## Changes in 7.0.1; 1. Fix problem with irregu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:17577,load,loading,17577,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loading']
Performance,". Use ``__has_feature(memory_sanitizer)`` to check if the code is being built; with :doc:`MemorySanitizer`. Use ``__has_feature(dataflow_sanitizer)`` to check if the code is being built; with :doc:`DataFlowSanitizer`. Use ``__has_feature(safe_stack)`` to check if the code is being built; with :doc:`SafeStack`. Extensions for selectively disabling optimization; =================================================. Clang provides a mechanism for selectively disabling optimizations in functions; and methods. To disable optimizations in a single function definition, the GNU-style or C++11; non-standard attribute ``optnone`` can be used. .. code-block:: c++. // The following functions will not be optimized.; // GNU-style attribute; __attribute__((optnone)) int foo() {; // ... code; }; // C++11 attribute; [[clang::optnone]] int bar() {; // ... code; }. To facilitate disabling optimization for a range of function definitions, a; range-based pragma is provided. Its syntax is ``#pragma clang optimize``; followed by ``off`` or ``on``. All function definitions in the region between an ``off`` and the following; ``on`` will be decorated with the ``optnone`` attribute unless doing so would; conflict with explicit attributes already present on the function (e.g. the; ones that control inlining). .. code-block:: c++. #pragma clang optimize off; // This function will be decorated with optnone.; int foo() {; // ... code; }. // optnone conflicts with always_inline, so bar() will not be decorated.; __attribute__((always_inline)) int bar() {; // ... code; }; #pragma clang optimize on. If no ``on`` is found to close an ``off`` region, the end of the region is the; end of the compilation unit. Note that a stray ``#pragma clang optimize on`` does not selectively enable; additional optimizations when compiling at low optimization levels. This feature; can only be used to selectively disable optimizations. The pragma has an effect on functions only at the point of their definition; for; functio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:159137,optimiz,optimize,159137,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimize']
Performance,". Whenever positioned inside; a mother volume, this will create a normal **`TGeoVolume`** object; having as shape a box with `dz` fitting the corresponding `dz `of the; mother shape. Generally, this type of parameterization is used when; positioning volumes in containers having a matching shape, but it works; also for most reasonable combinations. ## Geometry Creation. A given geometry can be built in various ways, but one has to follow; some mandatory steps. Even if we might use some terms that will be; explained later, here are few general rules:. - Volumes need media and shapes in order to be created.; - Both containers and contained volumes must be created before linking; them together, and the relative transformation matrix must be; provided.; - Any volume have to be positioned somewhere otherwise it will not be; considered as part of the geometry.; - Visibility or tracking properties of volumes can be provided both at; build time or after geometry is closed, but global visualization; settings (see section: ""The Drawing Package"") should not be provided; at build time, otherwise the drawing package will be loaded. There is also a list of specific rules:. - Positioned volumes should not extrude their container or intersect; with others within this unless it is specified (see section:; Overlapping Volumes).; - The top volume (containing all geometry trees) must be specified; before closing the geometry and must not be positioned - it; represents the global reference frame.; - After building the full geometry tree, the geometry must be closed; (see the method **`TGeoManager`**`::CloseGeometry()`). Voxelization; can be redone per volume after this process. The list is much bigger and we will describe in more detail the geometry; creation procedure in the following sections. Provided that geometry was; successfully built and closed, the **`TGeoManager`** class will register; itself to ROOT and the logical/physical structures will become; immediately browsable. ### Th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:57490,load,loaded,57490,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['load'],['loaded']
Performance,". You can use ``llvm.uadd.with.overflow``; on any integer bit width or vectors of integers. ::. declare {i16, i1} @llvm.uadd.with.overflow.i16(i16 %a, i16 %b); declare {i32, i1} @llvm.uadd.with.overflow.i32(i32 %a, i32 %b); declare {i64, i1} @llvm.uadd.with.overflow.i64(i64 %a, i64 %b); declare {<4 x i32>, <4 x i1>} @llvm.uadd.with.overflow.v4i32(<4 x i32> %a, <4 x i32> %b). Overview:; """""""""""""""""". The '``llvm.uadd.with.overflow``' family of intrinsic functions perform; an unsigned addition of the two arguments, and indicate whether a carry; occurred during the unsigned summation. Arguments:; """""""""""""""""""". The arguments (%a and %b) and the first element of the result structure; may be of integer types of any bit width, but they must have the same; bit width. The second element of the result structure must be of type; ``i1``. ``%a`` and ``%b`` are the two values that will undergo unsigned; addition. Semantics:; """""""""""""""""""". The '``llvm.uadd.with.overflow``' family of intrinsic functions perform; an unsigned addition of the two arguments. They return a structure --- the; first element of which is the sum, and the second element of which is a; bit specifying if the unsigned summation resulted in a carry. Examples:; """""""""""""""""". .. code-block:: llvm. %res = call {i32, i1} @llvm.uadd.with.overflow.i32(i32 %a, i32 %b); %sum = extractvalue {i32, i1} %res, 0; %obit = extractvalue {i32, i1} %res, 1; br i1 %obit, label %carry, label %normal. '``llvm.ssub.with.overflow.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.ssub.with.overflow``; on any integer bit width or vectors of integers. ::. declare {i16, i1} @llvm.ssub.with.overflow.i16(i16 %a, i16 %b); declare {i32, i1} @llvm.ssub.with.overflow.i32(i32 %a, i32 %b); declare {i64, i1} @llvm.ssub.with.overflow.i64(i64 %a, i64 %b); declare {<4 x i32>, <4 x i1>} @llvm.ssub.with.overflow.v4i32(<4 x i32> %a, <4 x i32> %b). Overview:; """""""""""""""""". The '``llvm.ssu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:603626,perform,perform,603626,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,". _writing-an-llvm-pass-releaseMemory:. The ``releaseMemory`` method; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: c++. virtual void releaseMemory();. The ``PassManager`` automatically determines when to compute analysis results,; and how long to keep them around for. Because the lifetime of the pass object; itself is effectively the entire duration of the compilation process, we need; some way to free analysis results when they are no longer useful. The; ``releaseMemory`` virtual method is the way to do this. If you are writing an analysis or any other pass that retains a significant; amount of state (for use by another pass which ""requires"" your pass and uses; the :ref:`getAnalysis <writing-an-llvm-pass-getAnalysis>` method) you should; implement ``releaseMemory`` to, well, release the memory allocated to maintain; this internal state. This method is called after the ``run*`` method for the; class, before the next call of ``run*`` in your pass. Registering dynamically loaded passes; =====================================. *Size matters* when constructing production quality tools using LLVM, both for; the purposes of distribution, and for regulating the resident code size when; running on the target system. Therefore, it becomes desirable to selectively; use some passes, while omitting others and maintain the flexibility to change; configurations later on. You want to be able to do all this, and, provide; feedback to the user. This is where pass registration comes into play. The fundamental mechanisms for pass registration are the; ``MachinePassRegistry`` class and subclasses of ``MachinePassRegistryNode``. An instance of ``MachinePassRegistry`` is used to maintain a list of; ``MachinePassRegistryNode`` objects. This instance maintains the list and; communicates additions and deletions to the command line interface. An instance of ``MachinePassRegistryNode`` subclass is used to maintain; information provided about a particular pass. This information includes the; c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:47823,load,loaded,47823,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['load'],['loaded']
Performance,". buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. fence acq_rel - system *none* 1. buffer_wbl2. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:278564,load,load,278564,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,". clang -O2 -mllvm -opt-bisect-limit=256 my_file.c. The -opt-bisect-limit option may also be applied to link-time optimizations by; using a prefix to indicate that this is a plug-in option for the linker. The; following syntax will set a bisect limit for LTO transformations:. ::. # When using lld, or ld64 (macOS); clang -flto -Wl,-mllvm,-opt-bisect-limit=256 my_file.o my_other_file.o; # When using Gold; clang -flto -Wl,-plugin-opt,-opt-bisect-limit=256 my_file.o my_other_file.o. LTO passes are run by a library instance invoked by the linker. Therefore any; passes run in the primary driver compilation phase are not affected by options; passed via '-Wl,-plugin-opt' and LTO passes are not affected by options; passed to the driver-invoked LLVM invocation via '-mllvm'. Passing ``-opt-bisect-print-ir-path=path/foo.ll`` will dump the IR to; ``path/foo.ll`` when -opt-bisect-limit starts skipping passes. Bisection Index Values; ======================. The granularity of the optimizations associated with a single index value is; variable. Depending on how the optimization pass has been instrumented the; value may be associated with as much as all transformations that would have; been performed by an optimization pass on an IR unit for which it is invoked; (for instance, during a single call of runOnFunction for a FunctionPass) or as; little as a single transformation. The index values may also be nested so that; if an invocation of the pass is not skipped individual transformations within; that invocation may still be skipped. The order of the values assigned is guaranteed to remain stable and consistent; from one run to the next up to and including the value specified as the limit.; Above the limit value skipping of optimizations can cause a change in the; numbering, but because all optimizations above the limit are skipped this; is not a problem. When an opt-bisect index value refers to an entire invocation of the run; function for a pass, the pass will query whether or not",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:3534,optimiz,optimizations,3534,interpreter/llvm-project/llvm/docs/OptBisect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst,1,['optimiz'],['optimizations']
Performance,". clang -O3 currently compiles this code:. extern const int magic;; double f() { return 0.0 * magic; }. into. @magic = external constant i32. define double @_Z1fv() nounwind readnone {; entry:; %tmp = load i32* @magic, align 4, !tbaa !0; %conv = sitofp i32 %tmp to double; %mul = fmul double %conv, 0.000000e+00; ret double %mul; }. We should be able to fold away this fmul to 0.0. More generally, fmul(x,0.0); can be folded to 0.0 if we can prove that the LHS is not -0.0, not a NaN, and; not an INF. The CannotBeNegativeZero predicate in value tracking should be; extended to support general ""fpclassify"" operations that can return ; yes/no/unknown for each of these predicates. In this predicate, we know that uitofp is trivially never NaN or -0.0, and; we know that it isn't +/-Inf if the floating point type has enough exponent bits; to represent the largest integer value as < inf. //===---------------------------------------------------------------------===//. When optimizing a transformation that can change the sign of 0.0 (such as the; 0.0*val -> 0.0 transformation above), it might be provable that the sign of the; expression doesn't matter. For example, by the above rules, we can't transform; fmul(sitofp(x), 0.0) into 0.0, because x might be -1 and the result of the; expression is defined to be -0.0. If we look at the uses of the fmul for example, we might be able to prove that; all uses don't care about the sign of zero. For example, if we have:. fadd(fmul(sitofp(x), 0.0), 2.0). Since we know that x+2.0 doesn't care about the sign of any zeros in X, we can; transform the fmul to 0.0, and then the fadd to 2.0. //===---------------------------------------------------------------------===//. We should enhance memcpy/memcpy/memset to allow a metadata node on them; indicating that some bytes of the transfer are undefined. This is useful for; frontends like clang when lowering struct copies, when some elements of the; struct are undefined. Consider something like this:. str",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:61371,optimiz,optimizing,61371,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimizing']
Performance,". eBPF maps are provided for sharing data between kernel and user-space.; Currently implemented types are hash and array, with potential extension to; support bloom filters, radix trees, etc. A map is defined by its type,; maximum number of elements, key size and value size in bytes. eBPF syscall; supports create, update, find and delete functions on maps. Function calls; ^^^^^^^^^^^^^^. Function call arguments are passed using up to five registers (R1 - R5).; The return value is passed in a dedicated register (R0). Four additional; registers (R6 - R9) are callee-saved, and the values in these registers; are preserved within kernel functions. R0 - R5 are scratch registers within; kernel functions, and eBPF programs must therefor store/restore values in; these registers if needed across function calls. The stack can be accessed; using the read-only frame pointer R10. eBPF registers map 1:1 to hardware; registers on x86_64 and other 64-bit architectures. For example, x86_64; in-kernel JIT maps them as. ::. R0 - rax; R1 - rdi; R2 - rsi; R3 - rdx; R4 - rcx; R5 - r8; R6 - rbx; R7 - r13; R8 - r14; R9 - r15; R10 - rbp. since x86_64 ABI mandates rdi, rsi, rdx, rcx, r8, r9 for argument passing; and rbx, r12 - r15 are callee saved. Program start; ^^^^^^^^^^^^^. An eBPF program receives a single argument and contains; a single eBPF main routine; the program does not contain eBPF functions.; Function calls are limited to a predefined set of kernel functions. The size; of a program is limited to 4K instructions: this ensures fast termination and; a limited number of kernel function calls. Prior to running an eBPF program,; a verifier performs static analysis to prevent loops in the code and; to ensure valid register usage and operand types. The AMDGPU backend; ------------------. The AMDGPU code generator lives in the ``lib/Target/AMDGPU``; directory. This code generator is capable of targeting a variety of; AMD GPU processors. Refer to :doc:`AMDGPUUsage` for more information.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:108144,perform,performs,108144,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['perform'],['performs']
Performance,". scan-build: running the analyzer from the command line. scan-build: running the analyzer from the command line. What is it?; scan-build is a command line utility that enables a user to run the; static analyzer over their codebase as part of performing a regular build (from; the command line).; How does it work?; During a project build, as source files are compiled they are also analyzed; in tandem by the static analyzer.; Upon completion of the build, results are then presented to the user within a; web browser.; Will it work with any build system?; scan-build has little or no knowledge about how you build your code.; It works by overriding the CC and CXX environment variables to; (hopefully) change your build to use a ""fake"" compiler instead of the; one that would normally build your project. This fake compiler executes either; clang or gcc (depending on the platform) to compile your; code and then executes the static analyzer to analyze your code.; This ""poor man's interposition"" works amazingly well in many cases; and falls down in others. Please consult the information on this page on making; the best use of scan-build, which includes getting it to work when the; aforementioned hack fails to work. Viewing static analyzer results in a web browser. Contents. Getting Started. Basic Usage; For Windows Users; Other Options; Output of scan-build. Recommended Usage Guidelines. Always Analyze a Project in its ""Debug"" Configuration; Use Verbose Output when Debugging scan-build; Run './configure' through scan-build. Analyzing iPhone Projects. Getting Started; The scan-build command can be used to analyze an entire project by; essentially interposing on a project's build process. This means that to run the; analyzer using scan-build, you will use scan-build to analyze; the source files compiled by gcc/clang during a project build.; This means that any files that are not compiled will also not be analyzed.; Basic Usage; Basic usage of scan-build is designed to be simple: j",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/scan-build.html:243,perform,performing,243,interpreter/llvm-project/clang/www/analyzer/scan-build.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/scan-build.html,2,['perform'],['performing']
Performance,".); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. *Same as corresponding; load atomic acquire,; except must generate; all instructions even; for OpenCL.*. load atomic seq_cst - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt lgkmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; hav",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:281765,load,load,281765,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,".. -*- mode: rst -*-. CPyCppyy: Python-C++ bindings interface based on Cling/LLVM; ===========================================================. CPyCppyy is the CPython equivalent of _cppyy in PyPy.; It provides dynamic Python-C++ bindings by leveraging the Cling C++; interpreter and LLVM.; Details and performance are described in; `this paper <http://conferences.computer.org/pyhpc/2016/papers/5220a027.pdf>`_. CPyCppyy is a CPython extension module built on top of the same backend API; as PyPy/_cppyy.; It thus requires the installation of the; `cppyy backend <https://pypi.python.org/pypi/cppyy-backend/>`_; for use, which will pull in Cling.; CPython/cppyy and PyPy/cppyy are designed to be compatible, although there; are differences due to the former being reference counted and the latter; being garbage collected, as well as temporary differences due to different; release cycles of the respective projects. ----. Find the cppyy documentation here:; http://cppyy.readthedocs.io. Change log:; https://cppyy.readthedocs.io/en/latest/changelog.html. Bug reports/feedback:; https://github.com/wlav/cppyy/issues; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/CPyCppyy/README.rst:303,perform,performance,303,bindings/pyroot/cppyy/CPyCppyy/README.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/CPyCppyy/README.rst,1,['perform'],['performance']
Performance,".. -*- mode: rst -*-. cppyy: Python-C++ bindings interface based on Cling/LLVM; ========================================================. cppyy provides fully automatic, dynamic Python-C++ bindings by leveraging; the Cling C++ interpreter and LLVM.; It supports both PyPy (natively), CPython, and C++ language standards; through C++17 (and parts of C++20). Details and performance are described in; `this paper <http://cern.ch/wlav/Cppyy_LavrijsenDutta_PyHPC16.pdf>`_,; originally presented at PyHPC'16, but since updated with improved performance; numbers. Full documentation: `cppyy.readthedocs.io <http://cppyy.readthedocs.io/>`_. Notebook-based tutorial: `Cppyy Tutorial <https://github.com/wlav/cppyy/blob/master/doc/tutorial/CppyyTutorial.ipynb>`_. For Anaconda/miniconda, install cppyy from `conda-forge <https://anaconda.org/conda-forge/cppyy>`_. ----. Change log:; https://cppyy.readthedocs.io/en/latest/changelog.html. Bug reports/feedback:; https://github.com/wlav/cppyy/issues; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/README.rst:369,perform,performance,369,bindings/pyroot/cppyy/cppyy/README.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/README.rst,2,['perform'],['performance']
Performance,".. _basic_types:. Basic types; ===========. C++ has a far richer set of builtin types than Python.; Most Python code can remain relatively agnostic to that, and ``cppyy``; provides automatic conversions as appropriate.; On the other hand, Python builtin types such as lists and maps are far; richer than any builtin types in C++.; These are mapped to their Standard Template Library equivalents instead. The C++ code used for the examples below can be found; :doc:`here <cppyy_features_header>`, and it is assumed that that code is; loaded before running any of the example code snippets.; Download it, save it under the name ``features.h``, and simply include it:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('features.h'); >>>. `Builtins`; """""""""""""""""""". The selection of builtin data types varies greatly between Python and C++.; Where possible, builtin data types map onto the expected equivalent Python; types, with the caveats that there may be size differences, different; precision or rounding, etc.; For example, a C++ ``float`` is returned as a Python ``float``, which is in; fact a C++ ``double``.; If sizes allow, conversions are automatic.; For example, a C++ ``unsigned int`` becomes a Python2 ``long`` or Python3; ``int``, but unsigned-ness is still honored:. .. code-block:: python. >>> cppyy.gbl.gUint; 0L; >>> type(cppyy.gbl.gUint); <type 'long'>; >>> cppyy.gbl.gUint = -1; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; ValueError: cannot convert negative integer to unsigned; >>>. On some platforms, 8-bit integer types such as ``int8_t`` and ``uint8_t`` are; represented as `char` types.; For consistency, these are mapped onto Python `int`. Some types are builtin in Python, but (STL) classes in C++.; Examples are ``str`` vs. ``std::string`` (see also the; :doc:`Strings <strings>` section) and ``complex`` vs. ``std::complex``.; These classes have been pythonized to behave the same wherever possible.; For example, string comparison work",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/basic_types.rst:533,load,loaded,533,bindings/pyroot/cppyy/cppyy/doc/source/basic_types.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/basic_types.rst,1,['load'],['loaded']
Performance,".. _classes:. Classes; =======. Both Python and C++ support object-oriented code through classes and thus; it is logical to expose C++ classes as Python ones, including the full; inheritance hierarchy. The C++ code used for the examples below can be found; :doc:`here <cppyy_features_header>`, and it is assumed that that code is; loaded at the start of any session.; Download it, save it under the name ``features.h``, and load it:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('features.h'); >>>. `Basics`; --------. All bound C++ code starts off from the global C++ namespace, represented in; Python by ``gbl``.; This namespace, as any other namespace, is treated as a module after it has; been loaded.; Thus, we can import C++ classes that live underneath it:. .. code-block:: python. >>> from cppyy.gbl import Concrete; >>> Concrete; <class cppyy.gbl.Concrete at 0x2058e30>; >>>. Placing classes in the same structure as imposed by C++ guarantees identity,; even if multiple Python modules bind the same class.; There is, however, no necessity to expose that structure to end-users: when; developing a Python package that exposes C++ classes through ``cppyy``,; consider ``cppyy.gbl`` an ""internal"" module, and expose the classes in any; structure you see fit.; The C++ names will continue to follow the C++ structure, however, as is needed; for e.g. pickling:. .. code-block:: python. >>> from cppyy.gbl import Namespace; >>> Concrete == Namespace.Concrete; False; >>> n = Namespace.Concrete.NestedClass(); >>> type(n); <class cppyy.gbl.Namespace.Concrete.NestedClass at 0x22114c0>; >>> type(n).__name__; NestedClass; >>> type(n).__module__; cppyy.gbl.Namespace.Concrete; >>> type(n).__cpp_name__; Namespace::Concrete::NestedClass; >>>. `Constructors`; --------------. Python and C++ both make a distinction between allocation (``__new__`` in; Python, ``operator new`` in C++) and initialization (``__init__`` in Python,; the constructor call in C++).; When binding, however, ther",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/classes.rst:331,load,loaded,331,bindings/pyroot/cppyy/cppyy/doc/source/classes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/classes.rst,3,['load'],"['load', 'loaded']"
Performance,".. _cmake_interface:. CMake interface; ===============. CMake fragments are provided for an Automated generation of an end-user; bindings package from a CMake-based project build.; The bindings generated by rootcling, are 'raw' in the sense that:. * The .cpp file be compiled. The required compilation steps are; platform-dependent.; * The bindings are not packaged for distribution. Typically, users expect; to have a pip-compatible package.; * The binding are in the 'cppyy.gbl' namespace. This is an inconvenience at; best for users who might expect C++ code from KF5::Config to appear in; Python via ""import KF5.Config"".; * The bindings are loaded lazily, which limits the discoverability of the; content of the bindings.; * ``cppyy`` supports customization of the bindings via 'Pythonization' but; there is no automated way to load them. These issues are addressed by the CMake support. This is a blend of Python; packaging and CMake where CMake provides:. * Platform-independent scripting of the creation of a Python 'wheel' package; for the bindings.; * An facility for CMake-based projects to automate the entire bindings; generation process, including basic automated tests. .. note::. The JIT needs to resolve linker symbols in order to call them through; generated wrappers.; Thus, any classes, functions, and data that will be used in Python need; to be exported.; This is the default behavior on Mac and Linux, but not on Windows.; On that platform, use ``__declspec(dllexport)`` to explicitly export the; classes and function you expect to call.; CMake has simple `support for exporting all`_ C++ symbols. Python packaging; ----------------. Modern Python packaging usage is based on the 'wheel'. This is places the onus; on the creation of binary artifacts in the package on the distributor. In this; case, this includes the platform-dependent steps necessary to compile the .cpp; file. The generated package also takes advantage of the __init__.py load-time; mechanism to enhance the b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/cmake_interface.rst:645,load,loaded,645,bindings/pyroot/cppyy/cppyy/doc/source/cmake_interface.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/cmake_interface.rst,2,['load'],"['load', 'loaded']"
Performance,".. _debugging:; ; Debugging; =========. By default, the ``clang`` JIT as used by cppyy does not generate debugging; information.; This is first of all because it has proven to be not reliable in all cases,; but also because in a production setting this information, being internal to; the wrapper generation, goes unused.; However, that does mean that a debugger that starts from python will not be; able to step through JITed code into the C++ function that needs debugging,; even when such information is available for that C++ function. To enable debugging information in JITed code, set the ``EXTRA_CLING_ARGS``; envar to ``-g`` (and any further compiler options you need, e.g. add ``-O2``; to debug optimized code). On a crash in C++, the backend will attempt to provide a stack trace.; This works quite well on Linux (through ``gdb``) and decently on MacOS; (through ``unwind``), but is currently unreliable on MS Windows.; To prevent printing of this trace, which can be slow to produce, set the; envar ``CPPYY_CRASH_QUIET`` to '1'. It is even more useful to obtain a traceback through the Python code that led; up to the problem in C++.; Many modern debuggers allow mixed-mode C++/Python debugging (for example; `gdb`_ and `MSVC`_), but cppyy can also turn abortive C++ signals (such as a; segmentation violation) into Python exceptions, yielding a normal traceback.; This is particularly useful when working with cross-inheritance and other; cross-language callbacks. To enable the signals to exceptions conversion, import the lowlevel module; ``cppyy.ll`` and use:. .. code-block:: python. import cppyy.ll; cppyy.ll.set_signals_as_exception(True). Call ``set_signals_as_exception(False)`` to disable the conversion again.; It is recommended to only have the conversion enabled around the problematic; code, as it comes with a performance penalty.; If the problem can be localized to a specific function, you can use its; ``__sig2exc__`` flag to only have the conversion active in that functi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/debugging.rst:704,optimiz,optimized,704,bindings/pyroot/cppyy/cppyy/doc/source/debugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/debugging.rst,1,['optimiz'],['optimized']
Performance,".. _features:. Miscellaneous; =============. .. toctree::; :hidden:. cppyy_features_header. This is a collection of a few more features listed that do not have a proper; place yet in the rest of the documentation. The C++ code used for the examples below can be found; :doc:`here <cppyy_features_header>`, and it is assumed that that code is; loaded at the start of any session.; Download it, save it under the name ``features.h``, and load it:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('features.h'); >>>. `Special variables`; -------------------. There are several conventional ""special variables"" that control behavior of; functions or provide (internal) information.; Often, these can be set/used in pythonizations to handle memory management or; Global Interpreter Lock (GIL) release. * ``__python_owns__``: a flag that every bound instance carries and determines; whether Python or C++ owns the C++ instance (and associated memory).; If Python owns the instance, it will be destructed when the last Python; reference to the proxy disappears.; You can check/change the ownership with the __python_owns__ flag that every; bound instance carries.; Example:. .. code-block:: python. >>> from cppyy.gbl import Concrete; >>> c = Concrete(); >>> c.__python_owns__ # True: object created in Python; True; >>>. * ``__creates__``: a flag that every C++ overload carries and determines; whether the return value is owned by C++ or Python: if ``True``, Python owns; the return value, otherwise C++. * ``__set_lifeline__``: a flag that every C++ overload carries and determines; whether the return value should place a back-reference on ``self``, to; prevent the latter from going out of scope before the return value does.; The default is ``False``, but will be automatically set at run-time if a; return value's address is a C++ object pointing into the memory of ``this``,; or if ``self`` is a by-value return. * ``__release_gil__``: a flag that every C++ overload carries and determine",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/misc.rst:343,load,loaded,343,bindings/pyroot/cppyy/cppyy/doc/source/misc.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/misc.rst,2,['load'],"['load', 'loaded']"
Performance,".. _functions:. Functions; =========. C++ functions are first-class objects in Python and can be used wherever; Python functions can be used, including for dynamically constructing; classes. The C++ code used for the examples below can be found; :doc:`here <cppyy_features_header>`, and it is assumed that that code is; loaded at the start of any session.; Download it, save it under the name ``features.h``, and load it:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('features.h'); >>>. Function argument type conversions follow the expected rules, with implicit; conversions allowed, including between Python builtin types and STL types,; but it is rather more efficient to make conversions explicit. `Free functions`; ----------------. All bound C++ code starts off from the global C++ namespace, represented in; Python by ``gbl``.; This namespace, as any other namespace, is treated as a module after it has; been loaded.; Thus, we can directly import C++ functions from it and other namespaces that; themselves may contain more functions.; All lookups on namespaces are done lazily, thus if loading more headers bring; in more functions (incl. new overloads), these become available dynamically. .. code-block:: python. >>> from cppyy.gbl import global_function, Namespace; >>> global_function == Namespace.global_function; False; >>> from cppyy.gbl.Namespace import global_function; >>> global_function == Namespace.global_function; True; >>> from cppyy.gbl import global_function; >>>. Free functions can be bound to a class, following the same rules as apply to; Python functions: unless marked as static, they will turn into member; functions when bound to an instance, but act as static functions when called; through the class.; Consider this example:. .. code-block:: python. >>> from cppyy.gbl import Concrete, call_abstract_method; >>> c = Concrete(); >>> Concrete.callit = call_abstract_method; >>> Concrete.callit(c); called Concrete::abstract_method; >>> c.callit(); ca",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/functions.rst:320,load,loaded,320,bindings/pyroot/cppyy/cppyy/doc/source/functions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/functions.rst,3,['load'],"['load', 'loaded']"
Performance,".. _history:. History; =======. .. toctree::; :hidden:. What is now called `cppyy` started life as `RootPython` from `CERN`_, but; cppyy is not associated with CERN (it is still used there, however,; underpinning `PyROOT`_). Back in late 2002, Pere Mato of CERN, had the idea of using the `CINT`_ C++; interpreter, which formed the interactive interface to `ROOT`_, to call from; Python into C++: this became RootPython.; This binder interfaced with Python through `boost.python`_ (v1), transpiling; Python code into C++ and interpreting the result with CINT.; In early 2003, I ported this code to boost.python v2, then recently released.; In practice, however, re-interpreting the transpiled code was unusably slow,; thus I modified the code to make direct use of CINT's internal reflection; system, gaining about 25x in performance.; I presented this work as `PyROOT` at the ROOT Users' Workshop in early 2004,; and, after removing the boost.python dependency by using the C-API directly; (gaining another factor 7 in speedup!), it was included in ROOT.; PyROOT was presented at the SciPy'06 conference, but was otherwise not; advocated outside of High Energy Physics (HEP). In 2010, the PyPy core developers and I held a `sprint at CERN`_ to use; `Reflex`, a standalone alternative to CINT's reflection of C++, to add; automatic C++ bindings, PyROOT-style, to `PyPy`_.; This is where the name ""cppyy"" originated.; Coined by Carl Friedrich Bolz, if you want to understand the meaning, just; pronounce it slowly: cpp-y-y. After the ROOT team replaced CINT with `Cling`_, PyROOT soon followed.; As part of Google's Summer of Code '16, Aditi Dutta moved PyPy/cppyy to Cling; as well, and packaged the code for use through `PyPI`_.; I continued this integration with the Python eco-system by forking PyROOT,; reducing its dependencies, and repackaging it as CPython/cppyy.; The combined result is the current cppyy project.; Mid 2018, version 1.0 was released. .. _`CERN`: https://cern.ch/; .. _`PyROOT`",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/history.rst:822,perform,performance,822,bindings/pyroot/cppyy/cppyy/doc/source/history.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/history.rst,1,['perform'],['performance']
Performance,".. _loop-terminology:. ===========================================; LLVM Loop Terminology (and Canonical Forms); ===========================================. .. contents::; :local:. Loop Definition; ===============. Loops are an important concept for a code optimizer. In LLVM, detection; of loops in a control-flow graph is done by :ref:`loopinfo`. It is based; on the following definition. A loop is a subset of nodes from the control-flow graph (CFG; where; nodes represent basic blocks) with the following properties:. 1. The induced subgraph (which is the subgraph that contains all the; edges from the CFG within the loop) is strongly connected; (every node is reachable from all others). 2. All edges from outside the subset into the subset point to the same; node, called the **header**. As a consequence, the header dominates; all nodes in the loop (i.e. every execution path to any of the loop's; node will have to pass through the header). 3. The loop is the maximum subset with these properties. That is, no; additional nodes from the CFG can be added such that the induced; subgraph would still be strongly connected and the header would; remain the same. In computer science literature, this is often called a *natural loop*.; In LLVM, a more generalized definition is called a; :ref:`cycle <cycle-terminology>`. Terminology; -----------. The definition of a loop comes with some additional terminology:. * An **entering block** (or **loop predecessor**) is a non-loop node; that has an edge into the loop (necessarily the header). If there is; only one entering block, and its only edge is to the; header, it is also called the loop's **preheader**. The preheader; dominates the loop without itself being part of the loop. * A **latch** is a loop node that has an edge to the header. * A **backedge** is an edge from a latch to the header. * An **exiting edge** is an edge from inside the loop to a node outside; of the loop. The source of such an edge is called an **exiting block**, i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst:258,optimiz,optimizer,258,interpreter/llvm-project/llvm/docs/LoopTerminology.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LoopTerminology.rst,1,['optimiz'],['optimizer']
Performance,".. _milegalizer:. Legalizer; ---------. This pass transforms the generic machine instructions such that they are legal. A legal instruction is defined as:. * **selectable** --- the target will later be able to select it to a; target-specific (non-generic) instruction. This doesn't necessarily mean that; :doc:`InstructionSelect` has to handle it though. It just means that; **something** must handle it. * operating on **vregs that can be loaded and stored** -- if necessary, the; target can select a ``G_LOAD``/``G_STORE`` of each gvreg operand. As opposed to SelectionDAG, there are no legalization phases. In particular,; 'type' and 'operation' legalization are not separate. Legalization is iterative, and all state is contained in GMIR. To maintain the; validity of the intermediate code, instructions are introduced:. * ``G_MERGE_VALUES`` --- concatenate multiple registers of the same; size into a single wider register. * ``G_UNMERGE_VALUES`` --- extract multiple registers of the same size; from a single wider register. * ``G_EXTRACT`` --- extract a simple register (as contiguous sequences of bits); from a single wider register. As they are expected to be temporary byproducts of the legalization process,; they are combined at the end of the :ref:`milegalizer` pass.; If any remain, they are expected to always be selectable, using loads and stores; if necessary. The legality of an instruction may only depend on the instruction itself and; must not depend on any context in which the instruction is used. However, after; deciding that an instruction is not legal, using the context of the instruction; to decide how to legalize the instruction is permitted. As an example, if we; have a ``G_FOO`` instruction of the form::. %1:_(s32) = G_CONSTANT i32 1; %2:_(s32) = G_FOO %0:_(s32), %1:_(s32). it's impossible to say that G_FOO is legal iff %1 is a ``G_CONSTANT`` with; value ``1``. However, the following::. %2:_(s32) = G_FOO %0:_(s32), i32 1. can say that it's legal iff operand 2 is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/Legalizer.rst:440,load,loaded,440,interpreter/llvm-project/llvm/docs/GlobalISel/Legalizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/Legalizer.rst,1,['load'],['loaded']
Performance,".. _philosophy:. Philosophy; ==========. .. toctree::; :hidden:. As a Python-C++ language binder, cppyy has several unique features: it fills; gaps and covers use cases not available through other binders.; This document explains some of the design choices made and the thinking; behind the implementations of those features.; It's categorized as ""philosophy"" because a lot of it is open to; interpretation.; Its main purpose is simply to help you decide whether cppyy covers your use; cases and binding requirements, before committing any time to; :ref:`trying it out <starting>`. Run-time v.s. compile-time; --------------------------. What performs better, run-time or compile-time?; The obvious answer is compile-time: see the performance differences between; C++ and Python, for example.; Obvious, but completely wrong, however.; In fact, when it comes to Python, it is even the `wrong question.`. Everything in Python is run-time: modules, classes, functions, etc. are all; run-time constructs.; A Python module that defines a class is a set of instructions to the Python; interpreter that lead to the construction of the desired class object.; A C/C++ extension module that defines a class does the same thing by calling; a succession of Python interpreter Application Programming Interfaces (APIs;; the exact same that Python uses itself internally).; If you use a compile-time binder such as `SWIG`_ or `pybind11`_ to bind a C++; class, then what gets compiled is the series of API calls necessary to; construct a Python-side equivalent at `run-time` (when the module gets; loaded), not the Python class object.; In short, whether a binding is created at ""compile-time"" or at run-time has; no measurable bearing on performance. What does affect performance is the overhead to cross the language barrier.; This consists of unboxing Python objects to extract or convert the underlying; objects or data to something that matches what C++ expects; overload; resolution based on the unboxed argume",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst:643,perform,performs,643,bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,2,['perform'],"['performance', 'performs']"
Performance,".. _python:. Python; ======. The C++ code used for the examples below can be found; :doc:`here <cppyy_features_header>`, and it is assumed that that code is; loaded at the start of any session.; Download it, save it under the name ``features.h``, and load it:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('features.h'); >>>. `PyObject`; ----------. Arguments and return types of ``PyObject*`` can be used, and passed on to; CPython API calls (or through ``cpyext`` in PyPy). `Doc strings`; -------------. The documentation string of a method or function contains the C++; arguments and return types of all overloads of that name, as applicable.; Example:. .. code-block:: python. >>> from cppyy.gbl import Concrete; >>> print Concrete.array_method.__doc__; void Concrete::array_method(int* ad, int size); void Concrete::array_method(double* ad, int size); >>>. `Help`; ------. Bound C++ class is first-class Python and can thus be inspected like any; Python objects can.; For example, we can ask for ``help()``:. .. code-block:: python. >>> help(Concrete); Help on class Concrete in module gbl:. class Concrete(Abstract); | Method resolution order:; | Concrete; | Abstract; | CPPInstance; | __builtin__.object; |; | Methods defined here:; |; | __assign__(self, const Concrete&); | Concrete& Concrete::operator=(const Concrete&); |; | __init__(self, *args); | Concrete::Concrete(int n = 42); | Concrete::Concrete(const Concrete&); |; etc. .... ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/python.rst:158,load,loaded,158,bindings/pyroot/cppyy/cppyy/doc/source/python.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/python.rst,2,['load'],"['load', 'loaded']"
Performance,".. _starting:. Trying it out; =============. This is a basic guide to try cppyy and see whether it works for you.; Large code bases will benefit from more advanced features such as; :doc:`pythonizations <pythonizations>` for a cleaner interface to clients;; precompiled modules for faster parsing and reduced memory usage;; "":ref:`dictionaries <dictionaries>`"" to package locations and manage; dependencies; and mapping files for automatic, lazy, loading.; You can, however, get very far with just the basics and it may even be; completely sufficient for small packages with fewer classes. cppyy works by parsing C++ definitions through ``cling``, generating tiny; wrapper codes to honor compile-time features and create standardized; interfaces, then compiling/linking those wrappers with the ``clang`` JIT.; It thus requires only those two ingredients: *C++ definitions* and; *linker symbols*.; All cppyy uses, the basic and the more advanced, are variations on the; theme of bringing these two together at the point of use. Definitions typically live in header files and symbols in libraries.; Headers can be loaded with ``cppyy.include`` and libraries with the; ``cppyy.load_library`` call.; Loading the header is sufficient to start exploring, with ``cppyy.gbl`` the; starting point of all things C++, while the linker symbols are only needed at ; the point of first use. Here is an example using the `zlib`_ library, which is likely available on; your system:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('zlib.h') # bring in C++ definitions; >>> cppyy.load_library('libz') # load linker symbols; >>> cppyy.gbl.zlibVersion() # use a zlib API; '1.2.11'; >>>. Since header files can include other header files, it is easy to aggregate; all relevant ones into a single header to include.; If there are project-specific include paths, you can add those paths through; ``cppyy.add_include_path``.; If a header is C-only and not set for use with C++, use ``cppyy.c_include``,; which ad",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/starting.rst:447,load,loading,447,bindings/pyroot/cppyy/cppyy/doc/source/starting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/starting.rst,1,['load'],['loading']
Performance,".. _stl:. STL; ===. Parts of the Standard Template Library (STL), in particular its container; types, are the de facto equivalent of Python's builtin types.; STL is written in C++ and Python bindings of it are fully functional as-is,; but are much more useful when pluggable into idiomatic expressions where; Python builtin containers are expected (e.g. in list contractions). There are two extremes to achieve such drop-in behavior: copy into Python; builtins, so that the Python-side always deals with true Python objects; or; adjust the C++ interfaces to be the same as their Python equivalents.; Neither is very satisfactory: the former is not because of the existence of; global/static variables and return-by-reference.; If only a copy is available, then expected modifications do not propagate.; Copying is also either slow (when copying every time) or memory intensive (if; the results are cached).; Filling out the interfaces may look more appealing, but all operations then; involve C++ function calls, which can be slower than the Python equivalents,; and C++-style error handling. Given that neither choice will satisfy all cases, ``cppyy`` aims to maximize; functionality and minimum surprises based on common use.; Thus, for example, ``std::vector`` grows a pythonistic ``__len__`` method,; but does not lose its C++ ``size`` method.; Passing a Python container through a const reference to a ``std::vector``; will trigger automatic conversion, but such an attempt through a non-const; reference will fail since a non-temporary C++ object is required [#f1]_ to; return any updates/changes. ``std::string`` is almost always converted to Python's ``str`` on function; returns (the exception is return-by-reference when assigning), but not when; its direct use is more likely such as in the case of (global) variables or; when iterating over a ``std::vector<std::string>``. The rest of this section shows examples of how STL containers can be used in; a natural, pythonistic, way. `std::vec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/stl.rst:898,cache,cached,898,bindings/pyroot/cppyy/cppyy/doc/source/stl.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/stl.rst,1,['cache'],['cached']
Performance,".. _toplevel:. Top Level; =========. cppyy provides a couple of helper functions at the module level that provide; (direct) access to the Cling interpreter (any C++ code is always accessed; through the global namespace ``cppyy.gbl``).; The documentation makes use of these helpers throughout, so they are listed; here first, but their documentation is more conveniently accessible through; the Python interpreter itself, using the ``help()`` function::. $ python; >>> import cppyy; >>> help(cppyy). `Loading C++`; -------------. C++ code can be loaded as text to be JITed, or be compiled ahead of time and; supplied in the form of a shared library.; In the latter case, C++ headers need to be loaded as well to declare; classes, functions, and variables to Cling.; Instead of headers, pre-compiled code can be used; in particular all of the; standard C++ headers and several system headers are pre-compiled at startup.; cppyy provides the following helpers to load C++ code:. * ``cppdef``: direct access to the interpreter.; This function accepts C++ declarations as a string and JITs them (bindings; are not created until actual use).; The code is loaded into the global scope, thus any previously loaded code; is available from one ``cppdef`` call to the next, as are all standard; C++ headers that have been loaded through pre-compiled headers.; Example::. >>> cppyy.cppdef(r""""""\; ... void hello() {; ... std::cout << ""Hello, World!"" << std::endl;; ... }""""""); True; >>> cppyy.gbl.hello(); Hello, World!; >>> . * ``cppexec``: direct access to the interpreter.; This function accepts C++ statements as a string, JITs and executes them.; Just like ``cppdef``, execution is in the global scope and all previously; loaded code is available.; If the statements are declarations, the effect is the same as ``cppdef``,; but ``cppexec`` also accepts executable lines.; Example::. >>> cppyy.cppexec(r""""""std::string hello = ""Hello, World!"";""""""); True; >>> cppyy.cppexec(""std::cout << hello << std::endl;""); He",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst:545,load,loaded,545,bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,3,['load'],"['load', 'loaded']"
Performance,".. _transformation-metadata:. ============================; Code Transformation Metadata; ============================. .. contents::; :local:. Overview; ========. LLVM transformation passes can be controlled by attaching metadata to; the code to transform. By default, transformation passes use heuristics; to determine whether or not to perform transformations, and when doing; so, other details of how the transformations are applied (e.g., which; vectorization factor to select).; Unless the optimizer is otherwise directed, transformations are applied; conservatively. This conservatism generally allows the optimizer to; avoid unprofitable transformations, but in practice, this results in the; optimizer not applying transformations that would be highly profitable. Frontends can give additional hints to LLVM passes on which; transformations they should apply. This can be additional knowledge that; cannot be derived from the emitted IR, or directives passed from the; user/programmer. OpenMP pragmas are an example of the latter. If any such metadata is dropped from the program, the code's semantics; must not change. Metadata on Loops; =================. Attributes can be attached to loops as described in :ref:`llvm.loop`.; Attributes can describe properties of the loop, disable transformations,; force specific transformations and set transformation options. Because metadata nodes are immutable (with the exception of; ``MDNode::replaceOperandWith`` which is dangerous to use on uniqued; metadata), in order to add or remove a loop attributes, a new ``MDNode``; must be created and assigned as the new ``llvm.loop`` metadata. Any; connection between the old ``MDNode`` and the loop is lost. The; ``llvm.loop`` node is also used as LoopID (``Loop::getLoopID()``), i.e.; the loop effectively gets a new identifier. For instance,; ``llvm.mem.parallel_loop_access`` references the LoopID. Therefore, if; the parallel access property is to be preserved after adding/removing; loop attribut",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst:339,perform,perform,339,interpreter/llvm-project/llvm/docs/TransformMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst,4,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,".. _type_conversions:. Type conversions; ================. Most type conversions are done automatically, e.g. between Python ``str``; and C++ ``std::string`` and ``const char*``, but low-level APIs exist to; perform explicit conversions. The C++ code used for the examples below can be found; :doc:`here <cppyy_features_header>`, and it is assumed that that code is; loaded at the start of any session.; Download it, save it under the name ``features.h``, and load it:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('features.h'); >>>. .. _sec-auto-casting-label:. `Auto-casting`; --------------. Object pointer returns from functions provide the most derived class known; (i.e. exposed in header files) in the hierarchy of the object being returned.; This is important to preserve object identity as well as to make casting,; a pure C++ feature after all, superfluous.; Example:. .. code-block:: python. >>> from cppyy.gbl import Abstract, Concrete; >>> c = Concrete(); >>> Concrete.show_autocast.__doc__; 'Abstract* Concrete::show_autocast()'; >>> d = c.show_autocast(); >>> type(d); <class '__main__.Concrete'>; >>>. As a consequence, if your C++ classes should only be used through their; interfaces, then no bindings should be provided to the concrete classes; (e.g. by excluding them using a :ref:`selection file <selection-files>`).; Otherwise, more functionality will be available in Python than in C++. Sometimes, however, full control over a cast is needed.; For example, if the instance is bound by another tool or even a 3rd party,; hand-written, extension library.; Assuming the object supports the ``PyCapsule`` or ``CObject`` abstraction,; then a C++-style reinterpret_cast (i.e. without implicitly taking offsets; into account), can be done by taking and rebinding the address of an; object:. .. code-block:: python. >>> from cppyy import addressof, bind_object; >>> e = bind_object(addressof(d), Abstract); >>> type(e); <class '__main__.Abstract'>; >>>. `Operators`; ---",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/type_conversions.rst:208,perform,perform,208,bindings/pyroot/cppyy/cppyy/doc/source/type_conversions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/type_conversions.rst,3,"['load', 'perform']","['load', 'loaded', 'perform']"
Performance,".. _utilities:. Utilities; =========. The ``cppyy-backend`` package brings in the following utilities to help; with repackaging and redistribution:. * cling-config: for compile time flags; * rootcling and genreflex: for dictionary generation; * cppyy-generator: part of the :doc:`CMake interface <cmake_interface>`. Compiler/linker flags; ---------------------. ``cling-config`` is a small utility to provide access to the as-installed; configuration, such as compiler/linker flags and installation directories, of; other components.; Usage examples::. $ cling-config --help; Usage: cling-config [--cflags] [--cppflags] [--cmake]; $ cling-config --cmake; /usr/local/lib/python2.7/dist-packages/cppyy_backend/cmake. .. _dictionaries:. Dictionaries; ------------. Loading header files or code directly into ``cling`` is fine for interactive; work and smaller packages, but large scale applications benefit from; pre-compiling code, using the automatic class loader, and packaging; dependencies in so-called ""dictionaries."". A `dictionary` is a generated C++ source file containing references to the; header locations used when building (and any additional locations provided),; a set of forward declarations to reduce the need of loading header files, and; a few I/O helper functions.; The name ""dictionary"" is historic: before ``cling`` was used, it contained; the complete generated C++ reflection information, whereas now that is; derived at run-time from the header files.; It is still possible to fully embed header files rather than only storing; their names and search locations, to make the dictionary more self-contained. After generating the dictionary, it should be compiled into a shared library.; This provides additional dependency control: by linking it directly with any; further libraries needed, you can use standard mechanisms such as ``rpath``; to locate those library dependencies.; Alternatively, you can add the additional libraries to load to the mapping; files of the class load",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst:956,load,loader,956,bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,1,['load'],['loader']
Performance,".. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass());; ... It is interesting to see what the code looks like before and after the; mem2reg optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. Here there is only one variable (x, the input argument) but you can; still see the extremely simple-minded code generation strategy we are; using. In the entry block, an alloca is created, and the initial input; value is stored into it. Each reference to the variable does a reload; from the stack. Also, note that we didn't modify the if/then/else; expression, so it still inserts a PHI node. While we could make an; alloca for it, it is actually easier to create a PHI node for it, so we; still just make the PHI. Here is the code after the mem2reg pass runs:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:17554,load,load,17554,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['load'],['load']
Performance,".. cppyy documentation master file, created by; sphinx-quickstart on Wed Jul 12 14:35:45 2017.; You can adapt this file completely to your liking, but it should at least; contain the root `toctree` directive. .. meta::; :description: cppyy: Automatic Python-C++ bindings; :keywords: Python, C++, llvm, cling, binding, bindings, automatic bindings, bindings generator, cross-language inheritance, calling C++ from Python, calling Python from C++, high performance, data science. cppyy: Automatic Python-C++ bindings; ====================================. cppyy is an automatic, run-time, Python-C++ bindings generator, for calling; C++ from Python and Python from C++.; Run-time generation enables detailed specialization for higher performance,; lazy loading for reduced memory use in large scale projects, Python-side; cross-inheritance and callbacks for working with C++ frameworks, run-time; template instantiation, automatic object downcasting, exception mapping, and; interactive exploration of C++ libraries.; cppyy delivers this without any language extensions, intermediate languages,; or the need for boiler-plate hand-written code.; For design and performance, see this `PyHPC'16 paper`_, albeit that the; CPython/cppyy performance has been vastly improved since, as well as this; `CAAS presentation`_.; For a quick teaser, see `Jason Turner's`_ introduction video. cppyy is based on `Cling`_, the C++ interpreter, to match Python's dynamism,; interactivity, and run-time behavior.; Consider this session, showing dynamic, interactive, mixing of C++ and Python; features (there are more examples throughout the documentation and in the; `tutorial`_):. .. code-block:: python. >>> import cppyy; >>> cppyy.cppdef(""""""; ... class MyClass {; ... public:; ... MyClass(int i) : m_data(i) {}; ... virtual ~MyClass() {}; ... virtual int add_int(int i) { return m_data + i; }; ... int m_data;; ... };""""""); True; >>> from cppyy.gbl import MyClass; >>> m = MyClass(42); >>> cppyy.cppdef(""""""; ... void sa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/index.rst:451,perform,performance,451,bindings/pyroot/cppyy/cppyy/doc/source/index.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/index.rst,3,"['load', 'perform']","['loading', 'performance']"
Performance,".. role:: raw-html(raw); :format: html. ========================; LLVM Bitcode File Format; ========================. .. contents::; :local:. Abstract; ========. This document describes the LLVM bitstream file format and the encoding of the; LLVM IR into it. Overview; ========. What is commonly known as the LLVM bitcode file format (also, sometimes; anachronistically known as bytecode) is actually two things: a `bitstream; container format`_ and an `encoding of LLVM IR`_ into the container format. The bitstream format is an abstract encoding of structured data, very similar to; XML in some ways. Like XML, bitstream files contain tags, and nested; structures, and you can parse the file without having to understand the tags.; Unlike XML, the bitstream format is a binary encoding, and unlike XML it; provides a mechanism for the file to self-describe ""abbreviations"", which are; effectively size optimizations for the content. LLVM IR files may be optionally embedded into a `wrapper`_ structure, or in a; `native object file`_. Both of these mechanisms make it easy to embed extra; data along with LLVM IR files. This document first describes the LLVM bitstream format, describes the wrapper; format, then describes the record structure used by LLVM IR files. .. _bitstream container format:. Bitstream Format; ================. The bitstream format is literally a stream of bits, with a very simple; structure. This structure consists of the following concepts:. * A ""`magic number`_"" that identifies the contents of the stream. * Encoding `primitives`_ like variable bit-rate integers. * `Blocks`_, which define nested content. * `Data Records`_, which describe entities within the file. * Abbreviations, which specify compression optimizations for the file. Note that the :doc:`llvm-bcanalyzer <CommandGuide/llvm-bcanalyzer>` tool can be; used to dump and inspect arbitrary bitstreams, which is very useful for; understanding the encoding. .. _magic number:. Magic Numbers; -------------. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BitCodeFormat.rst:904,optimiz,optimizations,904,interpreter/llvm-project/llvm/docs/BitCodeFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BitCodeFormat.rst,1,['optimiz'],['optimizations']
Performance,".. role:: raw-html(raw); :format: html. =================================; LLVM Code Coverage Mapping Format; =================================. .. contents::; :local:. Introduction; ============. LLVM's code coverage mapping format is used to provide code coverage; analysis using LLVM's and Clang's instrumentation based profiling; (Clang's ``-fprofile-instr-generate`` option). This document is aimed at those who would like to know how LLVM's code coverage; mapping works under the hood. A prior knowledge of how Clang's profile guided; optimization works is useful, but not required. For those interested in using; LLVM to provide code coverage analysis for their own programs, see the `Clang; documentation <https://clang.llvm.org/docs/SourceBasedCodeCoverage.html>`. We start by briefly describing LLVM's code coverage mapping format and the; way that Clang and LLVM's code coverage tool work with this format. After; the basics are down, more advanced features of the coverage mapping format; are discussed - such as the data structures, LLVM IR representation and; the binary encoding. High Level Overview; ===================. LLVM's code coverage mapping format is designed to be a self contained; data format that can be embedded into the LLVM IR and into object files.; It's described in this document as a **mapping** format because its goal is; to store the data that is required for a code coverage tool to map between; the specific source ranges in a file and the execution counts obtained; after running the instrumented version of the program. The mapping data is used in two places in the code coverage process:. 1. When clang compiles a source file with ``-fcoverage-mapping``, it; generates the mapping information that describes the mapping between the; source ranges and the profiling instrumentation counters.; This information gets embedded into the LLVM IR and conveniently; ends up in the final executable file when the program is linked. 2. It is also used by *llvm-cov* -",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CoverageMappingFormat.rst:541,optimiz,optimization,541,interpreter/llvm-project/llvm/docs/CoverageMappingFormat.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CoverageMappingFormat.rst,1,['optimiz'],['optimization']
Performance,".. role:: raw-html(raw); :format: html. Libclang tutorial; =================; The C Interface to Clang provides a relatively small API that exposes facilities for parsing source code into an abstract syntax tree (AST), loading already-parsed ASTs, traversing the AST, associating physical source locations with elements within the AST, and other facilities that support Clang-based development tools.; This C interface to Clang will never provide all of the information representation stored in Clang's C++ AST, nor should it: the intent is to maintain an API that is relatively stable from one release to the next, providing only the basic functionality needed to support development tools.; The entire C interface of libclang is available in the file `Index.h`_. Essential types overview; -------------------------. All types of libclang are prefixed with ``CX``. CXIndex; ~~~~~~~; An Index that consists of a set of translation units that would typically be linked together into an executable or library. CXTranslationUnit; ~~~~~~~~~~~~~~~~~; A single translation unit, which resides in an index. CXCursor; ~~~~~~~~; A cursor representing a pointer to some element in the abstract syntax tree of a translation unit. Code example; """""""""""""""""""""""". .. code-block:: cpp. // file.cpp; struct foo{; int bar;; int* bar_pointer;; };. .. code-block:: cpp. #include <clang-c/Index.h>; #include <iostream>. int main(){; CXIndex index = clang_createIndex(0, 0); //Create index; CXTranslationUnit unit = clang_parseTranslationUnit(; index,; ""file.cpp"", nullptr, 0,; nullptr, 0,; CXTranslationUnit_None); //Parse ""file.cpp"". if (unit == nullptr){; std::cerr << ""Unable to parse translation unit. Quitting.\n"";; return 0;; }; CXCursor cursor = clang_getTranslationUnitCursor(unit); //Obtain a cursor at the root of the translation unit; }. Visiting elements of an AST; ~~~~~~~~~~~~~~~~~~~~~~~~~~~; The elements of an AST can be recursively visited with pre-order traversal with ``clang_visitChildren``. .. code-bloc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibClang.rst:219,load,loading,219,interpreter/llvm-project/clang/docs/LibClang.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibClang.rst,1,['load'],['loading']
Performance,"... something else that would/could be important is to; have exceptions as first class types so that they would be handled in; a uniform way for the entire VM... so that C functions can call Java; functions for example... > c. How do we get more high-level information into the VM while keeping; > to a low-level VM design?; > o Explicit array references as operands? An alternative is; > to have just an array type, and let the index computations be; > separate 3-operand instructions. C. In the model I was thinking of (subject to change of course), we; would just have an array type (distinct from the pointer; types). This would allow us to have arbitrarily complex index; expressions, while still distinguishing ""load"" from ""Array load"",; for example. Perhaps also, switch jump tables would be first class; types as well? This would allow better reasoning about the program. 5. Support dynamic loading of code from various sources. Already; mentioned above was the example of loading java bytecodes, but we want; to support dynamic loading of VM code as well. This makes the job of; the runtime compiler much more interesting: it can do interprocedural; optimizations that the static compiler can't do, because it doesn't; have all of the required information (for example, inlining from; shared libraries, etc...). 6. Define a set of generally useful annotations to add to the VM; representation. For example, a function can be analysed to see if it; has any sideeffects when run... also, the MOD/REF sets could be; calculated, etc... we would have to determine what is reasonable. This; would generally be used to make IP optimizations cheaper for the; runtime compiler... > o Explicit instructions to handle aliasing, e.g.s:; > -- an instruction to say ""I speculate that these two values are not; > aliased, but check at runtime"", like speculative execution in; > EPIC?; > -- or an instruction to check whether two values are aliased and; > execute different code depending on the answer, som",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:5664,load,loading,5664,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,2,['load'],['loading']
Performance,"...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The ocaml frametable structure supports liveness information. It would be good; to support it. //===-----------------------------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2986,load,load,2986,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt,2,['load'],['load']
Performance,"...; offsetN[.discriminator]: number_of_samples [fn5:num fn6:num ... ]; offsetA[.discriminator]: fnA:num_of_total_samples; offsetA1[.discriminator]: number_of_samples [fn7:num fn8:num ... ]; offsetA1[.discriminator]: number_of_samples [fn9:num fn10:num ... ]; offsetB[.discriminator]: fnB:num_of_total_samples; offsetB1[.discriminator]: number_of_samples [fn11:num fn12:num ... ]. This is a nested tree in which the indentation represents the nesting level; of the inline stack. There are no blank lines in the file. And the spacing; within a single line is fixed. Additional spaces will result in an error; while reading the file. Any line starting with the '#' character is completely ignored. Inlined calls are represented with indentation. The Inline stack is a; stack of source locations in which the top of the stack represents the; leaf function, and the bottom of the stack represents the actual; symbol to which the instruction belongs. Function names must be mangled in order for the profile loader to; match them in the current translation unit. The two numbers in the; function header specify how many total samples were accumulated in the; function (first number), and the total number of samples accumulated; in the prologue of the function (second number). This head sample; count provides an indicator of how frequently the function is invoked. There are two types of lines in the function body. - Sampled line represents the profile information of a source location.; ``offsetN[.discriminator]: number_of_samples [fn5:num fn6:num ... ]``. - Callsite line represents the profile information of an inlined callsite.; ``offsetA[.discriminator]: fnA:num_of_total_samples``. Each sampled line may contain several items. Some are optional (marked; below):. a. Source line offset. This number represents the line number; in the function where the sample was collected. The line number is; always relative to the line where symbol of the function is; defined. So, if the function has its head",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:98404,load,loader,98404,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['load'],['loader']
Performance,"...; };. template<class T, class U = AStruct<T>>; inline void freeFunction() { /* ... */ }; inline void do(unsigned N = 1) { /* ... */ }. ``` ; The associated with libA header files form libA's full descriptor. A.h,; potentially only part of the descriptor of libA, expands to more than 26000; lines of code. ```cpp; // Main.cpp; #include ""A.h""; int main() {; do();; return 0;; }. ```; Main.cpp, reuses code from libA by including libA's descriptor and links against; libA. The full descriptor can contain thousands of files expanding to millions; of lines of code -- a common case for framework libraries, for instance. ROOT goes further and enhances C++ by allowing the following code to work without; explicitly requiring to `#include <A.h>`. Currently, ROOT's lack of support of; line `#5` is a long-standing, known limitation that is lifted with modules. ```cpp; // ROOT prompt; root [] AStruct<float> S0; // #1: implicit loading of libA. Full descriptor required.; root [] AStruct<float>* S1; // #2: implicit loading of libA. No full descriptor required.; root [] if (gFile) S1->doIt(); // #3: implicit loading of libA. Full descriptor required.; root [] gSystem->Load(""libA""); // #4: explicit loading of libA. No full descriptor required.; root [] do(); // #5: error: implicit loading of libA is currently unsupported. ```. This pattern is not only used in the ROOT prompt but in I/O hotspots such as; `ShowMembers` and `TClass::IsA`. A naive implementation of this feature would require inclusion of all reachable; library descriptors (aka header files) at ROOT startup time. Of course this is; not feasible and ROOT inserts a set of optimizations to fence itself from the; costly full header inclusion. Unfortunately, several of them are home-grown and; in a few cases inaccurate (eg line #5) causing a noticeable technical debt. Here we will briefly describe the three common layers of optimizations: ROOT PCH,; ROOTMAP and RDICT. The ROOT precompiled header (PCH) reduces the CPU and memor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:4942,load,loading,4942,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,1,['load'],['loading']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx1013_vdst_9041ac:. vdst; ====. Image data to be loaded by an image instruction. *Size:* 4 dwords. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1013_vdst_9041ac.rst:234,load,loaded,234,interpreter/llvm-project/llvm/docs/AMDGPU/gfx1013_vdst_9041ac.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1013_vdst_9041ac.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx1013_vdst_eae4c8:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>`, :ref:`tfe<amdgpu_synid_tfe>` and :ref:`d16<amdgpu_synid_d16>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`.; * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1013_vdst_eae4c8.rst:234,load,loaded,234,interpreter/llvm-project/llvm/docs/AMDGPU/gfx1013_vdst_eae4c8.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1013_vdst_eae4c8.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx1030_vdst_4d2300:. vdst; ====. Data loaded from memory. This is an optional operand. It must be used if and only if :ref:`lds<amdgpu_synid_lds>` is omitted. *Size:* 1 dword. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_4d2300.rst:222,load,loaded,222,interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_4d2300.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_4d2300.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx1030_vdst_5ec176:. vdst; ====. Image data to be loaded by an *image_gather4* instruction. *Size:* 4 data elements by default. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`. :ref:`d16<amdgpu_synid_d16>` affects operand size as follows:. * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_5ec176.rst:234,load,loaded,234,interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_5ec176.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_5ec176.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx1030_vdst_9041ac:. vdst; ====. Image data to be loaded by an image instruction. *Size:* 4 dwords. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_9041ac.rst:234,load,loaded,234,interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_9041ac.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_9041ac.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx1030_vdst_dfa6da:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>` and :ref:`tfe<amdgpu_synid_tfe>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies 1 dword.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_dfa6da.rst:234,load,loaded,234,interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_dfa6da.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_dfa6da.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx1030_vdst_eae4c8:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>`, :ref:`tfe<amdgpu_synid_tfe>` and :ref:`d16<amdgpu_synid_d16>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`.; * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_eae4c8.rst:234,load,loaded,234,interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_eae4c8.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx1030_vdst_eae4c8.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx10_vdst_4d2300:. vdst; ====. Data loaded from memory. This is an optional operand. It must be used if and only if :ref:`lds<amdgpu_synid_lds>` is omitted. *Size:* 1 dword. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_4d2300.rst:220,load,loaded,220,interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_4d2300.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_4d2300.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx10_vdst_5ec176:. vdst; ====. Image data to be loaded by an *image_gather4* instruction. *Size:* 4 data elements by default. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`. :ref:`d16<amdgpu_synid_d16>` affects operand size as follows:. * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_5ec176.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_5ec176.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_5ec176.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx10_vdst_dfa6da:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>` and :ref:`tfe<amdgpu_synid_tfe>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies 1 dword.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_dfa6da.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_dfa6da.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_dfa6da.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx10_vdst_eae4c8:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>`, :ref:`tfe<amdgpu_synid_tfe>` and :ref:`d16<amdgpu_synid_d16>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`.; * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_eae4c8.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_eae4c8.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx10_vdst_eae4c8.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx11_vdst_5ec176:. vdst; ====. Image data to be loaded by an *image_gather4* instruction. *Size:* 4 data elements by default. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`. :ref:`d16<amdgpu_synid_d16>` affects operand size as follows:. * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_5ec176.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_5ec176.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_5ec176.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx11_vdst_9041ac:. vdst; ====. Image data to be loaded by an image instruction. *Size:* 4 dwords. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_9041ac.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_9041ac.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_9041ac.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx11_vdst_dfa6da:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>` and :ref:`tfe<amdgpu_synid_tfe>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies 1 dword.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_dfa6da.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_dfa6da.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_dfa6da.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx11_vdst_e2d005:. vdst; ====. Image data to be loaded by an image instruction. *Size:* 4 data elements by default. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`. :ref:`d16<amdgpu_synid_d16>` and :ref:`tfe<amdgpu_synid_tfe>` affect operand size as follows:. * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_e2d005.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_e2d005.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_e2d005.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx11_vdst_eae4c8:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>`, :ref:`tfe<amdgpu_synid_tfe>` and :ref:`d16<amdgpu_synid_d16>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`.; * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_eae4c8.rst:232,load,loaded,232,interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_eae4c8.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_vdst_eae4c8.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx7_vdst_1f3009:. vdst; ====. Image data to be loaded by an *image_gather4* instruction. *Size:* 4 data elements by default. Each data element occupies 1 dword. :ref:`tfe<amdgpu_synid_tfe>` adds one more dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx7_vdst_1f3009.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx7_vdst_1f3009.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx7_vdst_1f3009.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx7_vdst_dfa6da:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>` and :ref:`tfe<amdgpu_synid_tfe>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies 1 dword.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx7_vdst_dfa6da.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx7_vdst_dfa6da.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx7_vdst_dfa6da.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx8_vdst_4730df:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>`, :ref:`tfe<amdgpu_synid_tfe>` and :ref:`d16<amdgpu_synid_d16>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`.; * :ref:`d16<amdgpu_synid_d16>` has different meanings for GFX8.0 and GFX8.1:. * For GFX8.0, this modifier does not affect the size of data elements in registers. Values in registers are stored in low 16 bits, high 16 bits are unused. There is no packing.; * Starting from GFX8.1, this modifier specifies that values in registers are packed; each value occupies 16 bits. * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_4730df.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_4730df.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_4730df.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx8_vdst_829fc5:. vdst; ====. Image data to be loaded by an *image_gather4* instruction. *Size:* 4 data elements by default. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`. :ref:`d16<amdgpu_synid_d16>` and :ref:`tfe<amdgpu_synid_tfe>` affect operand size as follows:. * :ref:`d16<amdgpu_synid_d16>` has different meanings for GFX8.0 and GFX8.1:. * For GFX8.0, this modifier does not affect the size of data elements in registers. Values in registers are stored in low 16 bits, high 16 bits are unused. There is no packing.; * Starting from GFX8.1, this modifier specifies that values in registers are packed; each value occupies 16 bits. * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_829fc5.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_829fc5.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_829fc5.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx8_vdst_dfa6da:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>` and :ref:`tfe<amdgpu_synid_tfe>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies 1 dword.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_dfa6da.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_dfa6da.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx8_vdst_dfa6da.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx90a_vdst_92bb33:. vdst; ====. Data loaded from memory. This is an optional operand. It must be used if and only if :ref:`lds<amdgpu_synid_lds>` is omitted. *Size:* 1 dword. *Operands:* :ref:`v<amdgpu_synid_v>`, :ref:`a<amdgpu_synid_a>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_92bb33.rst:221,load,loaded,221,interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_92bb33.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_92bb33.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx90a_vdst_a9ee3f:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>` and :ref:`d16<amdgpu_synid_d16>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`.; * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits. *Operands:* :ref:`v<amdgpu_synid_v>`, :ref:`a<amdgpu_synid_a>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_a9ee3f.rst:233,load,loaded,233,interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_a9ee3f.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_a9ee3f.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx90a_vdst_f5eb9d:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies 1 dword. *Operands:* :ref:`v<amdgpu_synid_v>`, :ref:`a<amdgpu_synid_a>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_f5eb9d.rst:233,load,loaded,233,interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_f5eb9d.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx90a_vdst_f5eb9d.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx9_vdst_4d2300:. vdst; ====. Data loaded from memory. This is an optional operand. It must be used if and only if :ref:`lds<amdgpu_synid_lds>` is omitted. *Size:* 1 dword. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_4d2300.rst:219,load,loaded,219,interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_4d2300.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_4d2300.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx9_vdst_5ec176:. vdst; ====. Image data to be loaded by an *image_gather4* instruction. *Size:* 4 data elements by default. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`. :ref:`d16<amdgpu_synid_d16>` affects operand size as follows:. * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_5ec176.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_5ec176.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_5ec176.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx9_vdst_dfa6da:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>` and :ref:`tfe<amdgpu_synid_tfe>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies 1 dword.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_dfa6da.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_dfa6da.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_dfa6da.rst,1,['load'],['loaded']
Performance,"..; **************************************************; * *; * Automatically generated file, do not edit! *; * *; **************************************************. .. _amdgpu_synid_gfx9_vdst_eae4c8:. vdst; ====. Image data to be loaded by an image instruction. *Size:* depends on :ref:`dmask<amdgpu_synid_dmask>`, :ref:`tfe<amdgpu_synid_tfe>` and :ref:`d16<amdgpu_synid_d16>`:. * :ref:`dmask<amdgpu_synid_dmask>` may specify from 1 to 4 data elements. Each data element occupies either 32 bits or 16 bits, depending on :ref:`d16<amdgpu_synid_d16>`.; * :ref:`d16<amdgpu_synid_d16>` specifies that data elements in registers are packed; each value occupies 16 bits.; * :ref:`tfe<amdgpu_synid_tfe>` adds 1 dword if specified. *Operands:* :ref:`v<amdgpu_synid_v>`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_eae4c8.rst:231,load,loaded,231,interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_eae4c8.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx9_vdst_eae4c8.rst,1,['load'],['loaded']
Performance,".; - Ensures any; following global; data read is no; older than the load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vm/vscnt(0). - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 4. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0), and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; st",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:367638,perform,performing,367638,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,".; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. fence acq_rel - system *none* 1. buffer_wbl2. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC globa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:278687,load,load,278687,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,".; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release.; - Must happen before; the following; buffer_inv.; - Ensures that the; acquire-fence-paired; atomic h",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:325569,load,load,325569,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,".; - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global memory; operations. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; local/generic store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; req",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:274734,load,load,274734,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,".; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_store; atomicrmw release - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic; atomicrmw release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 2. buffer/global/flat_atomic; atomicrmw rele",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:260441,perform,performing,260441,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,".; Complex types and void views require additional memory copies to populate an object in memory from the column data. A view can iterate over the entry range, over the field range, and over the range of a collection within an entry.; For instance, for a field `std::vector<float> pt`, a view can iterate over all `pt` values of all entries, or over the `pt` values of a particular entry. A view can safely outlive its originating reader.; Once the reader is deconstructed, any attempt to read data will throw an exception, but the view is still properly destructed. Views that originate from the same reader _cannot_ be used concurrently by different threads. Internal Classes; ----------------. ### RNTupleDS; The `RNTupleDS` class is an internal class that provides an RNTuple data source for RDataFrame.; It is part of the `ROOTDataFrame` library.; The RNTuple data source supports chains with a constructor that takes a list of input files.; The RNTuple data source also supports multi-threaded dataframes, parallelized on the file and cluster level. The data source exposes inner fields of complex collections.; For instance, if the data model contains a vector of `Event` classes, where each `Event` has `pt` and `eta` floats,; the dataframe can use the event vector itself (`Event` column) as well as the `float` columns `Event.pt` and `Event.eta`. ### RClusterPool; The RClusterPool is an internal class owned be a page source.; The cluster pool maintains an I/O thread that asynchronously prefetches the next few clusters.; Through `RPageSource::SetEntryRange()`, the cluster pool is instructed to not read beyond the given limit.; This is used in the RNTuple data source when multiple threads work on different clusters of the same file. ### RMiniFile; The RMiniFile is an internal class used to read and write RNTuple data in a ROOT file.; It provides a minimal subset of the `TFile` functionality.; Its purpose is to reduce the coupling between RNTuple and the ROOT I/O library. For writ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md:14705,multi-thread,multi-threaded,14705,tree/ntuple/v7/doc/Architecture.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md,1,['multi-thread'],['multi-threaded']
Performance,".; Implement a new function in the MnUserTransformation class, FindIndex(name), which returns -1 when the parameter name does not exist.; Implement new methods in Minuit2Minimizer as requested by the Minimizer interface:; SetPrecision(double eps) to change the precision value used internally in Minuit2 (in MnPrecision), VariableName(index) to return the name of a variable (parameter) given an index, and VariableIndex(name) to return the index of a variable given a name.; Set a status code in Minuit2Minimizer according to the following convention:; status = minimizeStatus + 10 * minosStatus + 100 * hesseStatus.; See the Minuit2Minimizer reference documentation for the possible values of minimizeStatus , minosStatus and hesseStatus.; In MnHesse. when the inversion of the hessian matrix failed, return MnInvertFailed instead of MnHesseFailed. Mathcore Fitting classes. Fix the fitting with the integral option in multi-dimensions.; Force the gradient calculation when requested in the minimizer; classes and avoid to perform the check when using TMinuit. This was; already the case in Minuit2.; Add new class ROOT::Fit::SparseData for dealing with binned sparse data. This class automatically merges the empty region, so they can be considered, whenever possible as a larger single bin. This improves the performances when doing likelihood fits on the sparse data.; Fix the likelihood fits for variable bin histograms. Now a correct normalization is applied according to the bin volume.; Add new methods in Minimizer class :. Minimizer::SetPrecision(double eps) to change in the minimizer the precision on which the objective functions are evaluated. By default the numerical double precision is used inside the minimizers. This method should be used only if the precision in the function evaluation is worse than the double precision.; std::string Minimizer::VariableName (unsigned int index) to return a name of the minimizer variable (i.e. a fitting parameter) given the integer index. Ret",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v526/index.html:2823,perform,perform,2823,math/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v526/index.html,2,['perform'],['perform']
Performance,".; In the following we will focus on the non-declared overlaps of type A); and B) since this is the main source of errors during tracking. These; are generally non-intended overlaps due to coding mistakes or bad; geometry design. The checking package is loaded together with the; painter classes and contains an automated overlap checker.**. ![Overlap checking](pictures/030001DF.png). This can be activated both at volume level (checking for illegal; overlaps only one level inside a given volume) and from the geometry; manager level (checking full geometry):. ``` {.cpp}; myVolume->CheckOverlaps(precision, option);; gGeoManager->CheckOverlaps(precision);; myNode->CheckOverlaps(precision);; ```. Here precision represents the desired maximum accepted overlap value in; centimeters (default value is 0.1). This tool checks all possible; significant pairs of candidates inside a given volume (not declared as; overlapping or division volumes). The check is performed by verifying; the mesh representation of one candidate against the shape of the other.; This sort of check cannot identify all possible overlapping topologies,; but it works for more than 95% and is much faster than the usual; shape-to-shape comparison. For a 100% reliability, one can perform the; check at the level of a single volume by using `option`=""`d`"" or; `option`=""`d<number>`"" to perform overlap checking by sampling the; volume with \<`number`\> random points (default 1 million). This; produces also a picture showing in red the overlapping region and; estimates the volume of the overlaps. An extrusion A) is declared in any of the following cases:. - At least one of the vertices of the daughter mesh representation is; outside the mother volume (in fact its shape) and having a safety; distance to the mother greater than the desired value;; - At least one of the mother vertices is contained also by one of its; daughters, in the same conditions. An overlap B) is declared if:. - At least one vertex of a positione",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:132982,perform,performed,132982,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performed']
Performance,".; When JSROOT is used with THttpServer, the address looks like:. ```javascript; <script type='module'>; import { httpRequest, draw } from 'http://your_root_server:8080/jsrootsys/modules/main.mjs';; let obj = await httpRequest('http://your_root_server:8080/Objects/hist/root.json','object');; await draw('drawing', obj, 'hist');; </script>; ```. Loading main module is enough to get public JSROOT functionality - reading files and drawing objects.; One also can load some special components directly like:. ```javascript; <script type='module'>; import { HierarchyPainter } from 'https://root.cern/js/latest/modules/gui.mjs';. let h = new HierarchyPainter(""example"", ""myTreeDiv"");. // configure 'simple' in provided <div> element; // one also can specify ""grid2x2"" or ""flex"" or ""tabs""; h.setDisplay(""simple"", ""myMainDiv"");. // open file and display element; await h.openRootFile('../../files/hsimple.root');; await h.display('hpxpy;1"",""colz');; </script>; ```. After script loading one can configure different parameters in `gStyle` object.; It is instance of the `TStyle` object and behaves like `gStyle` variable in ROOT. For instance,; to change stat format using to display value in stats box:. ```javascript; import { gStyle } from 'https://root.cern/js/latest/modules/main.mjs';; gStyle.fStatFormat = '7.5g';; ```. There is also `settings` object which contains all other JSROOT settings. For instance,; one can configure custom format for different axes:. ```javascript; import { settings } from 'https://root.cern/js/latest/modules/main.mjs';; settings.XValuesFormat = '4.2g';; settings.YValuesFormat = '6.1f';; ```. One also can use `build/jsroot.js` bundle to load all functionality at one and access it via `JSROOT` global handle:. ```javascript; <script src=""https://root.cern/js/latest/build/jsroot.js""></script>; <script>; // getting json string from somewhere; let obj = JSROOT.parse(root_json);; JSROOT.draw('plain', obj, 'colz');; </script>; ```. ### Use of JSON. It is strongly reco",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md:35586,load,loading,35586,documentation/JSROOT/JSROOT.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md,1,['load'],['loading']
Performance,".bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; both sinkable to *after* the icmp (e.g. in a subsequent block) which doesn't; increase register pressure. There are a ton of icmp's we aren't simplifying because of the reg pressure; concern. Care is warranted here though because many of these are induction; variables and other cases that matter a lot to performance, like the above.; Here's a blob of code that you can drop into the bottom of visitICmp to see some; missed cases:. { Value *A, *B, *C, *D;; if (match(Op0, m_Add(m_Value(A), m_Value(B))) && ; match(Op1, m_Add(m_Value(C), m_Value(D))) &&; (A == C || A == D || B == C || B == D)) {; errs() << ""OP0 = "" << *Op0 << "" U="" << Op0->getNumUses() << ""\n"";; errs() << ""OP1 = "" << *Op1 << "" U="" << Op1->getNumUses() << ""\n"";; errs() << ""CMP = "" << I << ""\n\n"";; }; }. //===---------------------------------------------------------------------===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits shrinks the ""and"" constant to 2 but instcombine misses the; icmp transform. //===------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:64738,perform,performance,64738,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['perform'],['performance']
Performance,.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.h; clang-tools-extra/clang-tidy/performance/UnnecessaryCopyInitialization.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.h; clang-tools-extra/clang-tidy/plugin/ClangTidyPlugin.cpp; clang-tools-extra/clang-tidy/portability/PortabilityTidyModule.cpp; clang-tools-extra/clang-tidy/portability/RestrictSystemIncludesCheck.cpp; clang-tools-extra/clang-tidy/portability/SIMDIntrinsicsCheck.cpp; clang-tools-extra/clang-tidy/readability/AvoidConstParamsInDecls.h; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.cpp; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.h; clang-tools-extra/clang-tidy/readability/ConstReturnTypeCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerContainsCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerContainsCheck.h; clang-tools-extra/clang-tidy/readability/ContainerDataPointerCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerDataPointe,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:66165,perform,performance,66165,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,".csail.mit.edu/vlk/spectre11.pdf. We will analyze these two variants independently. First, variant #1.1 works by; speculatively storing over the return address after a bounds check bypass. This; speculative store then ends up being used by the CPU during speculative; execution of the return, potentially directing speculative execution to; arbitrary gadgets in the binary. Let's look at an example.; ```; unsigned char local_buffer[4];; unsigned char *untrusted_data_from_caller = ...;; unsigned long untrusted_size_from_caller = ...;; if (untrusted_size_from_caller < sizeof(local_buffer)) {; // Speculative execution enters here with a too-large size.; memcpy(local_buffer, untrusted_data_from_caller,; untrusted_size_from_caller);; // The stack has now been smashed, writing an attacker-controlled; // address over the return address.; minor_processing(local_buffer);; return;; // Control will speculate to the attacker-written address.; }; ```. However, this can be mitigated by hardening the load of the return address just; like any other load. This is sometimes complicated because x86 for example; *implicitly* loads the return address off the stack. However, the; implementation technique below is specifically designed to mitigate this; implicit load by using the stack pointer to communicate misspeculation between; functions. This additionally causes a misspeculation to have an invalid stack; pointer and never be able to read the speculatively stored return address. See; the detailed discussion below. For variant #1.2, the attacker speculatively stores into the vtable or jump; table used to implement an indirect call or indirect jump. Because this is; speculative, this will often be possible even when these are stored in; read-only pages. For example:; ```; class FancyObject : public BaseObject {; public:; void DoSomething() override;; };; void f(unsigned long attacker_offset, unsigned long attacker_data) {; FancyObject object = getMyObject();; unsigned long *arr[4] = getFou",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:12206,load,load,12206,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,2,['load'],['load']
Performance,".d.f.s through composition, e.g. substituting a p.d.f parameter with a function that depends on other observables; rf302_utilfuncs.C - Utility functions classes available for use in tailoring; rf303_conditional.C - Use of tailored p.d.f as conditional p.d.fs.s; rf304_uncorrprod.C - Simple uncorrelated multi-dimensional p.d.f.s; rf305_condcorrprod.C - Multi-dimensional p.d.f.s with conditional p.d.fs in product; rf306_condpereventerrors.C - Complete example with use of conditional p.d.f. with per-event errors; rf307_fullpereventerrors.C -Complete example with use of full p.d.f. with per-event errors; rf308_normintegration2d.C - Examples on normalization of p.d.f.s in more than one dimension; rf309_ndimplot.C - Making 2 and 3 dimensional plots of p.d.f.s and datasets; rf310_sliceplot.C -Projecting p.d.f and data slices in discrete observables; rf311_rangeplot.C -Projecting p.d.f and data ranges in continuous observables; rf312_multirangefit.C - Performing fits in multiple (disjoint) ranges in one or more dimensions; rf313_paramranges.C - Working with parameterized ranges to define non-rectangular regions; rf314_paramfitrange.C - Working with parameterized ranges in a fit.; rf315_projectpdf.C - Marginizalization of multi-dimensional p.d.f.s through integration; rf316_llratioplot.C - Using the likelihood ratio technique to construct a signal enhanced 1-D projection of a multi-dimensional p.d.f.; ; DATA AND CATEGORIES. rf401_importttreethx.C -Overview of advanced option for importing data from ROOT TTree and THx histograms; rf402_datahandling.C - Tools for manipulation of (un)binned datasets; rf403_weightedevts.C - Using weights in unbinned datasets; rf404_categories.C - Working with RooCategory objects to describe discrete variables; rf405_realtocatfuncs.C - Demonstration of real-->discrete mapping functions; rf406_cattocatfuncs.C - Demonstration of discrete-->discrete (invertable) functions; rf407_latextables.C - Latex printing of lists and sets of RooArgSets; ; ORGANIZ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v522/index.html:3084,Perform,Performing,3084,roofit/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v522/index.html,1,['Perform'],['Performing']
Performance,".f, do. RooAbsReal* dgdx = gauss.derivative(x,1) ;. A more complete example is available in the new tutorial macro rf111_derivatives.C. Improved handling of chi-squared fits; Chi-squared fits can now be performed through the same style of interface as likelihood fits,; through the newly added method RooAbsReal::chi2FitTo(const RooDataHist&,...). Functions that can be fitted with chi-squared minimization are any RooAbsReal based function; as well as RooAbsPdf based p.d.f.s. In case of non-extended p.d.f.s the probability density; calculated by the p.d.f. is multiplied with the number of events in the histogram to adjust; the scale of the function. In case of extended p.d.f.s, the adjustment is made with the expected; number of events, rather than the observed number of events. Tutorial macro rf602_chi2fit.C has been updated to use this new interface. Chi-squared fits to X-Y datasets now possible; In addition to the ability to perform chi-squared fits to histograms it is now also possible; to perform chi-squared fits to unbinned datasets containing a series of X and Y values; with associated errors on Y and optionally on X. These 'X-Y' chi-squared fits are interfaced through newly added method; RooAbsReal::chi2FitTo(const RooDataSet&,...). By default the event weight is; interpreted as the 'Y' value, but an YVar() argument can designate any other; dataset column as Y value. If X errors are defined, one can choose to integrate the fitted; function over the range of the X errors, rather than taking the central value by adding; an Integrate(true) argument to chi2FitTo(); Two new arguments, StoreError(const RooArgSet&) and StoreAsymError(const RooArgSet&); have been added to the RooDataSet constructor to simplify the process of storing the errors; of X and Y variables along with their values in a dataset. The newly added tutorial macro rf609_xychi2fit.C illustrates the use of all this; new functionality. Uniform interface for creation of (profile likelihoods) and chi-squa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html:8230,perform,perform,8230,roofit/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html,4,['perform'],['perform']
Performance,".fr = freeze <2 x i32> %v ; element-wise freeze; %d = extractelement <2 x i32> %v.fr, i32 0 ; not undef; %add.f = add i32 %d, %d ; even number. ; branching on frozen value; %poison = add nsw i1 %k, undef ; poison; %c = freeze i1 %poison; br i1 %c, label %foo, label %bar ; non-deterministic branch to %foo or %bar. .. _i_call:. '``call``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = [tail | musttail | notail ] call [fast-math flags] [cconv] [ret attrs] [addrspace(<num>)]; <ty>|<fnty> <fnptrval>(<function args>) [fn attrs] [ operand bundles ]. Overview:; """""""""""""""""". The '``call``' instruction represents a simple function call. Arguments:; """""""""""""""""""". This instruction requires several arguments:. #. The optional ``tail`` and ``musttail`` markers indicate that the optimizers; should perform tail call optimization. The ``tail`` marker is a hint that; `can be ignored <CodeGenerator.html#tail-call-optimization>`_. The; ``musttail`` marker means that the call must be tail call optimized in order; for the program to be correct. This is true even in the presence of; attributes like ""disable-tail-calls"". The ``musttail`` marker provides these; guarantees:. #. The call will not cause unbounded stack growth if it is part of a; recursive cycle in the call graph.; #. Arguments with the :ref:`inalloca <attr_inalloca>` or; :ref:`preallocated <attr_preallocated>` attribute are forwarded in place.; #. If the musttail call appears in a function with the ``""thunk""`` attribute; and the caller and callee both have varargs, then any unprototyped; arguments in register or memory are forwarded to the callee. Similarly,; the return value of the callee is returned to the caller's caller, even; if a void return type is in use. Both markers imply that the callee does not access allocas from the caller.; The ``tail`` marker additionally implies that the callee does not access; varargs from the caller. Calls marked ``musttail`` must obey the following; additional rules:. - T",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:472831,optimiz,optimized,472831,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimized']
Performance,".h""; #include ""TLine.h""; #include ""TEventList.h"". const Double_t dxbin = (0.17-0.13)/40; // Bin-width; const Double_t sigma = 0.0012;; TEventList *elist = 0;; Bool_t useList, fillList;; TH1F *hdmd;; TH2F *h2;. //_________________________________________________________; Double_t fdm5(Double_t *xx, Double_t *par); {; Double_t x = xx[0];; if (x <= 0.13957) return 0;; Double_t xp3 = (x-par[3])*(x-par[3]);; Double_t res = dxbin*(par[0]*TMath::Power(x-0.13957,par[1]); + par[2]/2.5066/par[4]*TMath::Exp(-xp3/2/par[4]/par[4]));; return res;; }. //_________________________________________________________; Double_t fdm2(Double_t *xx, Double_t *par); {; Double_t x = xx[0];; if (x <= 0.13957) return 0;; Double_t xp3 = (x-0.1454)*(x-0.1454);; Double_t res = dxbin*(par[0]*TMath::Power(x-0.13957,0.25); + par[1]/2.5066/sigma*TMath::Exp(-xp3/2/sigma/sigma));; return res;; }. //_________________________________________________________; void h1analysis::Begin(TTree *tree); {; // function called before starting the event loop; // -it performs some cleanup; // -it creates histograms; // -it sets some initialization for the event list. //initialize the Tree branch addresses; Init(tree);. //print the option specified in the Process function; TString option = GetOption();; printf(""Starting h1analysis with process option: %sn"",option.Data());. //Some cleanup in case this function had already been executed; //Delete any previously generated histograms or functions; gDirectory->Delete(""hdmd"");; gDirectory->Delete(""h2*"");; delete gROOT->GetFunction(""f5"");; delete gROOT->GetFunction(""f2"");. //create histograms; hdmd = new TH1F(""hdmd"",""dm_d"",40,0.13,0.17);; h2 = new TH2F(""h2"",""ptD0 vs dm_d"",30,0.135,0.165,30,-3,6);. //process cases with event list; fillList = kFALSE;; useList = kFALSE;; fChain->SetEventList(0);; delete gDirectory->GetList()->FindObject(""elist"");. // case when one creates/fills the event list; if (option.Contains(""fillList"")) {; fillList = kTRUE;; elist = new TEventList(""elist"",""s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/ExampleAnalysis.md:6364,perform,performs,6364,documentation/users-guide/ExampleAnalysis.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/ExampleAnalysis.md,1,['perform'],['performs']
Performance,".html); has shown that this approach is incomplete and only catches a small and limited; subset of attackable patterns which happen to resemble very closely the initial; proofs of concept. As such, while its performance is acceptable, it does not; appear to be an adequate systematic mitigation. ## Performance Overhead. The performance overhead of this style of comprehensive mitigation is very; high. However, it compares very favorably with previously recommended; approaches such as the `lfence` instruction. Just as users can restrict the; scope of `lfence` to control its performance impact, this mitigation technique; could be restricted in scope as well. However, it is important to understand what it would cost to get a fully; mitigated baseline. Here we assume targeting a Haswell (or newer) processor and; using all of the tricks to improve performance (so leaves the low 2gb; unprotected and +/- 2gb surrounding any PC in the program). We ran both; Google's microbenchmark suite and a large highly-tuned server built using; ThinLTO and PGO. All were built with `-march=haswell` to give access to BMI2; instructions, and benchmarks were run on large Haswell servers. We collected; data both with an `lfence`-based mitigation and load hardening as presented; here. The summary is that mitigating with load hardening is 1.77x faster than; mitigating with `lfence`, and the overhead of load hardening compared to a; normal program is likely between a 10% overhead and a 50% overhead with most; large applications seeing a 30% overhead or less. | Benchmark | `lfence` | Load Hardening | Mitigated Speedup |; | -------------------------------------- | -------: | -------------: | ----------------: |; | Google microbenchmark suite | -74.8% | -36.4% | **2.5x** |; | Large server QPS (using ThinLTO & PGO) | -62% | -29% | **1.8x** |. Below is a visualization of the microbenchmark suite results which helps show; the distribution of results that is somewhat lost in the summary. The y-axis is; a ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:47167,tune,tuned,47167,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['tune'],['tuned']
Performance,".inc`` specifies enum values to identify the features,; arrays of constants to represent the CPU features and CPU subtypes, and the; ``ParseSubtargetFeatures`` method that parses the features string that sets; specified subtarget options. The generated ``SparcGenSubtarget.inc`` file; should be included in the ``SparcSubtarget.cpp``. The target-specific; implementation of the ``XXXSubtarget`` method should follow this pseudocode:. .. code-block:: c++. XXXSubtarget::XXXSubtarget(const Module &M, const std::string &FS) {; // Set the default features; // Determine default and user specified characteristics of the CPU; // Call ParseSubtargetFeatures(FS, CPU) to parse the features string; // Perform any additional operations; }. JIT Support; ===========. The implementation of a target machine optionally includes a Just-In-Time (JIT); code generator that emits machine code and auxiliary structures as binary; output that can be written directly to memory. To do this, implement JIT code; generation by performing the following steps:. * Write an ``XXXCodeEmitter.cpp`` file that contains a machine function pass; that transforms target-machine instructions into relocatable machine; code. * Write an ``XXXJITInfo.cpp`` file that implements the JIT interfaces for; target-specific code-generation activities, such as emitting machine code and; stubs. * Modify ``XXXTargetMachine`` so that it provides a ``TargetJITInfo`` object; through its ``getJITInfo`` method. There are several different approaches to writing the JIT support code. For; instance, TableGen and target descriptor files may be used for creating a JIT; code generator, but are not mandatory. For the Alpha and PowerPC target; machines, TableGen is used to generate ``XXXGenCodeEmitter.inc``, which; contains the binary coding of machine instructions and the; ``getBinaryCodeForInstr`` method to access those codes. Other JIT; implementations do not. Both ``XXXJITInfo.cpp`` and ``XXXCodeEmitter.cpp`` must include the; ``llvm/Cod",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:75710,perform,performing,75710,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['perform'],['performing']
Performance,".org/>`_. These attributes are documented; in the analyzer's `list of source-level annotations; <https://clang-analyzer.llvm.org/annotations.html>`_. Extensions for Dynamic Analysis; ===============================. Use ``__has_feature(address_sanitizer)`` to check if the code is being built; with :doc:`AddressSanitizer`. Use ``__has_feature(thread_sanitizer)`` to check if the code is being built; with :doc:`ThreadSanitizer`. Use ``__has_feature(memory_sanitizer)`` to check if the code is being built; with :doc:`MemorySanitizer`. Use ``__has_feature(dataflow_sanitizer)`` to check if the code is being built; with :doc:`DataFlowSanitizer`. Use ``__has_feature(safe_stack)`` to check if the code is being built; with :doc:`SafeStack`. Extensions for selectively disabling optimization; =================================================. Clang provides a mechanism for selectively disabling optimizations in functions; and methods. To disable optimizations in a single function definition, the GNU-style or C++11; non-standard attribute ``optnone`` can be used. .. code-block:: c++. // The following functions will not be optimized.; // GNU-style attribute; __attribute__((optnone)) int foo() {; // ... code; }; // C++11 attribute; [[clang::optnone]] int bar() {; // ... code; }. To facilitate disabling optimization for a range of function definitions, a; range-based pragma is provided. Its syntax is ``#pragma clang optimize``; followed by ``off`` or ``on``. All function definitions in the region between an ``off`` and the following; ``on`` will be decorated with the ``optnone`` attribute unless doing so would; conflict with explicit attributes already present on the function (e.g. the; ones that control inlining). .. code-block:: c++. #pragma clang optimize off; // This function will be decorated with optnone.; int foo() {; // ... code; }. // optnone conflicts with always_inline, so bar() will not be decorated.; __attribute__((always_inline)) int bar() {; // ... code; }; #pragma cl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:158661,optimiz,optimizations,158661,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimizations']
Performance,".txt"")` or; `gStyle->SetPalette(""filename.txt"")`. The input file contains one color; per line in ""r g b"" float format. This function is useful to profit from; a full set of colour-vision deficiency friendly and perceptually uniform; colour maps that are available to; [download](https://doi.org/10.5281/zenodo.4491293). ## The Graphics Editor. A new graphics editor took place in ROOT v4.0. The editor can be; activated by selecting the Editor menu entry in the canvas View menu or; one of the context menu entries for setting line, fill, marker or text; attributes. The following object editors are available for the current; ROOT version. ### TAxisEditor. ![](pictures/030000D5.png). This user interface gives the possibility for changing the following; axis attributes:. - color of the selected axis, the axis' title and labels;. - the length of thick parameters and the possibility to set them on; both axis sides (if `+-` is selected);. - to set logarithmic or linear scale along the selected axis with a; choice for optimized or more logarithmic labels;. - primary, secondary and tertiary axis divisions can be set via the; three number fields;. - the axis title can be added or edited and the title's color,; position, offset, size and font can be set interactively;. - the color, size, and offset of axis labels can be set similarly. In; addition, there is a check box for no exponent choice, and another; one for setting the same decimal part for all labels. ### TPadEditor. ![](pictures/030000D6.png). - It provides the following user interface:. - Fixed aspect ratio - can be set for pad resizing. - Edit - sets pad or canvas as editable. - Cross-hair - sets a cross hair on the pad. - TickX - set ticks along the X axis. - TickY - set ticks along the Y axis. - GridX - set a grid along the X axis. - GridY - set a grid along the Y axis. - The pad or canvas border size can be set if a sunken or a raised; border mode is. - selected; no border mode can be set too. ## Copy and Paste. You ca",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:84737,optimiz,optimized,84737,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['optimiz'],['optimized']
Performance,".txt` files which may be required. - `TEST_SUITE_COLLECT_STATS`. Collect internal LLVM statistics. Appends `-save-stats=obj` when invoking the; compiler and makes the lit runner collect and merge the statistic files. - `TEST_SUITE_RUN_BENCHMARKS`. If this is set to `OFF` then lit will not actually run the tests but just; collect build statistics like compile time and code size. - `TEST_SUITE_USE_PERF`. Use the `perf` tool for time measurement instead of the `timeit` tool that; comes with the test-suite. The `perf` is usually available on linux systems. - `TEST_SUITE_SPEC2000_ROOT`, `TEST_SUITE_SPEC2006_ROOT`, `TEST_SUITE_SPEC2017_ROOT`, ... Specify installation directories of external benchmark suites. You can find; more information about expected versions or usage in the README files in the; `External` directory (such as `External/SPEC/README`). ### Common CMake Flags. - `-GNinja`. Generate build files for the ninja build tool. - `-Ctest-suite/cmake/caches/<cachefile.cmake>`. Use a CMake cache. The test-suite comes with several CMake caches which; predefine common or tricky build configurations. Displaying and Analyzing Results; --------------------------------. The `compare.py` script displays and compares result files. A result file is; produced when invoking lit with the `-o filename.json` flag. Example usage:. - Basic Usage:. ```text; % test-suite/utils/compare.py baseline.json; Warning: 'test-suite :: External/SPEC/CINT2006/403.gcc/403.gcc.test' has No metrics!; Tests: 508; Metric: exec_time. Program baseline. INT2006/456.hmmer/456.hmmer 1222.90; INT2006/464.h264ref/464.h264ref 928.70; ...; baseline; count 506.000000; mean 20.563098; std 111.423325; min 0.003400; 25% 0.011200; 50% 0.339450; 75% 4.067200; max 1222.896800; ```. - Show compile_time or text segment size metrics:. ```bash; % test-suite/utils/compare.py -m compile_time baseline.json; % test-suite/utils/compare.py -m size.__text baseline.json; ```. - Compare two result files and filter short running t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md:7080,cache,cache,7080,interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,1,['cache'],['cache']
Performance,"// top doc Doxygen page for minuit2. /**. \page Minuit2Page Minuit2. The Minuit2 library is a new object-oriented implementation, written in C++, of the popular MINUIT minimization package.; These new version provides basically all the functionality present in the old Fortran version,; with almost equivalent numerical accuracy and computational performances.; Furthermore, it contains new functionality, like the possibility to set single side parameter limits or; the FUMILI algorithm, which is an optimized method for least square and log likelihood minimizations.; The package has been originally developed by M. Winkler and F. James.; More information on the new C++ version can be found on the; MINUIT Web Site. Minuit2, originally developed in the SEAL project, is now distributed within %ROOT.; The API has been then changed in this new version to follow the %ROOT coding convention (function names starting with capital letters) and the classes have been moved inside the namespace ROOT::Minuit2.; In addition, the %ROOT distribution contains classes needed to integrate Minuit2 in the %ROOT framework. A new class has been introduced, ROOT::Minuit2::Minuit2Minimizer, which implements the interface; ROOT::Math::Minimizer. Within %ROOT, it can be instantiates also using the %ROOT plug-in manager. This class provides a convenient entry point for using Minuit2. An example of using this interface is; the %ROOT tutorial tutorials/fit/NumericalMinimization.C or; the Minuit2 test program testMinimize.cxx. A standalone version of Minuit2 (independent of %ROOT) can be downloaded from here. It does not contain the %ROOT interface and it is therefore totally independent of external packages and can be simply build using the configure script and then make. Example tests are provided in the directory test/MnSim and test/MnTutorial and they can be built with the make check command. The Minuit2 User Guide provides all the information needed for using directly (without add-on packages like ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/minuit2/doc/Minuit2.html:347,perform,performances,347,math/minuit2/doc/Minuit2.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/minuit2/doc/Minuit2.html,4,"['optimiz', 'perform']","['optimized', 'performances']"
Performance,"//===- README.txt - Notes for improving PowerPC-specific code gen ---------===//. TODO:; * lmw/stmw pass a la arm load store optimizer for prolog/epilog. ===-------------------------------------------------------------------------===. This code:. unsigned add32carry(unsigned sum, unsigned x) {; unsigned z = sum + x;; if (sum + x < x); z++;; return z;; }. Should compile to something like:. 	addc r3,r3,r4; 	addze r3,r3. instead we get:. 	add r3, r4, r3; 	cmplw cr7, r3, r4; 	mfcr r4 ; 1; 	rlwinm r4, r4, 29, 31, 31; 	add r3, r3, r4. Ick. ===-------------------------------------------------------------------------===. We compile the hottest inner loop of viterbi to:. li r6, 0; b LBB1_84 ;bb432.i; LBB1_83: ;bb420.i; lbzx r8, r5, r7; addi r6, r7, 1; stbx r8, r4, r7; LBB1_84: ;bb432.i; mr r7, r6; cmplwi cr0, r7, 143; bne cr0, LBB1_83 ;bb420.i. The CBE manages to produce:. 	li r0, 143; 	mtctr r0; loop:; 	lbzx r2, r2, r11; 	stbx r0, r2, r9; 	addi r2, r2, 1; 	bdz later; 	b loop. This could be much better (bdnz instead of bdz) but it still beats us. If we; produced this with bdnz, the loop would be a single dispatch group. ===-------------------------------------------------------------------------===. Lump the constant pool for each function into ONE pic object, and reference; pieces of it as offsets from the start. For functions like this (contrived; to have lots of constants obviously):. double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }. We generate:. _X:; lis r2, ha16(.CPI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:114,load,load,114,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,4,"['load', 'optimiz']","['load', 'optimizer']"
Performance,"//===- README_X86_64.txt - Notes for X86-64 code gen ----------------------===//. AMD64 Optimization Manual 8.2 has some nice information about optimizing integer; multiplication by a constant. How much of it applies to Intel's X86-64; implementation? There are definite trade-offs to consider: latency vs. register; pressure vs. code size. //===---------------------------------------------------------------------===//. Are we better off using branches instead of cmove to implement FP to; unsigned i64?. _conv:; 	ucomiss	LC0(%rip), %xmm0; 	cvttss2siq	%xmm0, %rdx; 	jb	L3; 	subss	LC0(%rip), %xmm0; 	movabsq	$-9223372036854775808, %rax; 	cvttss2siq	%xmm0, %rdx; 	xorq	%rax, %rdx; L3:; 	movq	%rdx, %rax; 	ret. instead of. _conv:; 	movss LCPI1_0(%rip), %xmm1; 	cvttss2siq %xmm0, %rcx; 	movaps %xmm0, %xmm2; 	subss %xmm1, %xmm2; 	cvttss2siq %xmm2, %rax; 	movabsq $-9223372036854775808, %rdx; 	xorq %rdx, %rax; 	ucomiss %xmm1, %xmm0; 	cmovb %rcx, %rax; 	ret. Seems like the jb branch has high likelihood of being taken. It would have; saved a few instructions. //===---------------------------------------------------------------------===//. It's not possible to reference AH, BH, CH, and DH registers in an instruction; requiring REX prefix. However, divb and mulb both produce results in AH. If isel; emits a CopyFromReg which gets turned into a movb and that can be allocated a; r8b - r15b. To get around this, isel emits a CopyFromReg from AX and then right shift it; down by 8 and truncate it. It's not pretty but it works. We need some register; allocation magic to make the hack go away (e.g. putting additional constraints; on the result of the movb). //===---------------------------------------------------------------------===//. The x86-64 ABI for hidden-argument struct returns requires that the; incoming value of %rdi be copied into %rax by the callee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Ca",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:144,optimiz,optimizing,144,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,5,"['Optimiz', 'latency', 'optimiz']","['Optimization', 'latency', 'optimizing']"
Performance,"//===----------------------------------------------------------------------===//; // Representing sign/zero extension of function results; //===----------------------------------------------------------------------===//. Mar 25, 2009 - Initial Revision. Most ABIs specify that functions which return small integers do so in a; specific integer GPR. This is an efficient way to go, but raises the question:; if the returned value is smaller than the register, what do the high bits hold?. There are three (interesting) possible answers: undefined, zero extended, or; sign extended. The number of bits in question depends on the data-type that; the front-end is referencing (typically i1/i8/i16/i32). Knowing the answer to this is important for two reasons: 1) we want to be able; to implement the ABI correctly. If we need to sign extend the result according; to the ABI, we really really do need to do this to preserve correctness. 2); this information is often useful for optimization purposes, and we want the; mid-level optimizers to be able to process this (e.g. eliminate redundant; extensions). For example, lets pretend that X86 requires the caller to properly extend the; result of a return (I'm not sure this is the case, but the argument doesn't; depend on this). Given this, we should compile this:. int a();; short b() { return a(); }. into:. _b:; 	subl	$12, %esp; 	call	L_a$stub; 	addl	$12, %esp; 	cwtl; 	ret. An optimization example is that we should be able to eliminate the explicit; sign extension in this example:. short y();; int z() {; return ((int)y() << 16) >> 16;; }. _z:; 	subl	$12, %esp; 	call	_y; 	;; movswl %ax, %eax -> not needed because eax is already sext'd; 	addl	$12, %esp; 	ret. //===----------------------------------------------------------------------===//; // What we have right now.; //===----------------------------------------------------------------------===//. Currently, these sorts of things are modelled by compiling a function to return; the small type a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt:973,optimiz,optimization,973,interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,2,['optimiz'],"['optimization', 'optimizers']"
Performance,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the first blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-07-22-using-mcjit-with-kaleidoscope-tutorial/. The source code in this directory demonstrates the initial working version of; the program before subsequent performance improvements are applied. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required, as mentioned in the blog posts.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/initial/README.txt:634,perform,performance,634,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/initial/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/initial/README.txt,1,['perform'],['performance']
Performance,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the first in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The source code in this directory combines all previous versions, including the; old JIT-based implementation, into a single file for easy comparison with; command line options to select between the various possibilities. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons. Another Python script will split generated input files into; definitions and function calls for the purpose of testing the IR input and; caching facilities.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt:969,perform,performance,969,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt,1,['perform'],['performance']
Performance,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the second blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-07-29-kaleidoscope-performance-with-mcjit/. The source code in this directory demonstrates the second version of the; program, now modified to implement a sort of 'lazy' compilation. The toy-jit.cpp file contains a version of the original JIT-based source code; that has been modified to disable most stderr output for timing purposes. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt:498,perform,performance-with-mcjit,498,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt,2,['perform'],"['performance', 'performance-with-mcjit']"
Performance,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the third blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-08-02-object-caching-with-kaleidoscope/. The source code in this directory demonstrates the third version of the; program, now modified to accept an input IR file on the command line and,; optionally, to use a basic caching mechanism to store generated object images. The toy-jit.cpp file contains a version of the original JIT-based source code; that has been modified to support the input IR file command line option. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons. Another Python script will split generated input files into; definitions and function calls for the purpose of testing the IR input and; caching facilities.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt:1229,perform,performance,1229,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt,1,['perform'],['performance']
Performance,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend (Thumb specific).; //===---------------------------------------------------------------------===//. * Add support for compiling functions in both ARM and Thumb mode, then taking; the smallest. * Add support for compiling individual basic blocks in thumb mode, when in a ; larger ARM function. This can be used for presumed cold code, like paths; to abort (failure path of asserts), EH handling code, etc. * Thumb doesn't have normal pre/post increment addressing modes, but you can; load/store 32-bit integers with pre/postinc by using load/store multiple; instrs with a single register. * Make better use of high registers r8, r10, r11, r12 (ip). Some variants of add; and cmp instructions can use high registers. Also, we can use them as; temporaries to spill values into. * In thumb mode, short, byte, and bool preferred alignments are currently set; to 4 to accommodate ISA restriction (i.e. add sp, #imm, imm must be multiple; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also doe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:600,load,load,600,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,4,['load'],['load']
Performance,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend.; //===---------------------------------------------------------------------===//. Reimplement 'select' in terms of 'SEL'. * We would really like to support UXTAB16, but we need to prove that the; add doesn't need to overflow between the two 16-bit chunks. * Implement pre/post increment support. (e.g. PR935); * Implement smarter constant generation for binops with large immediates. A few ARMv6T2 ops should be pattern matched: BFI, SBFX, and UBFX. Interesting optimization for PIC codegen on arm-linux:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43129. //===---------------------------------------------------------------------===//. Crazy idea: Consider code that uses lots of 8-bit or 16-bit values. By the; time regalloc happens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===----------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:580,optimiz,optimization,580,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,2,['optimiz'],['optimization']
Performance,"//===---------------------------------------------------------------------===//; // Random ideas for the X86 backend: SSE-specific stuff.; //===---------------------------------------------------------------------===//. //===---------------------------------------------------------------------===//. SSE Variable shift can be custom lowered to something like this, which uses a; small table + unaligned load + shuffle instead of going through memory. __m128i_shift_right:; 	.byte	 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15; 	.byte	 -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1. ...; __m128i shift_right(__m128i value, unsigned long offset) {; return _mm_shuffle_epi8(value,; _mm_loadu_si128((__m128 *) (___m128i_shift_right + offset)));; }. //===---------------------------------------------------------------------===//. SSE has instructions for doing operations on complex numbers, we should pattern; match them. For example, this should turn into a horizontal add:. typedef float __attribute__((vector_size(16))) v4f32;; float f32(v4f32 A) {; return A[0]+A[1]+A[2]+A[3];; }. Instead we get this:. _f32: ## @f32; 	pshufd	$1, %xmm0, %xmm1 ## xmm1 = xmm0[1,0,0,0]; 	addss	%xmm0, %xmm1; 	pshufd	$3, %xmm0, %xmm2 ## xmm2 = xmm0[3,0,0,0]; 	movhlps	%xmm0, %xmm0 ## xmm0 = xmm0[1,1]; 	movaps	%xmm0, %xmm3; 	addss	%xmm1, %xmm3; 	movdqa	%xmm2, %xmm0; 	addss	%xmm3, %xmm0; 	ret. Also, there are cases where some simple local SLP would improve codegen a bit.; compiling this:. _Complex float f32(_Complex float A, _Complex float B) {; return A+B;; }. into:. _f32: ## @f32; 	movdqa	%xmm0, %xmm2; 	addss	%xmm1, %xmm2; 	pshufd	$1, %xmm1, %xmm1 ## xmm1 = xmm1[1,0,0,0]; 	pshufd	$1, %xmm0, %xmm3 ## xmm3 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm3; 	movaps	%xmm2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:404,load,load,404,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,2,['load'],['load']
Performance,"//===---------------------------------------------------------------------===//; // Random notes about and ideas for the SystemZ backend.; //===---------------------------------------------------------------------===//. The initial backend is deliberately restricted to z10. We should add support; for later architectures at some point. --. If an inline asm ties an i32 ""r"" result to an i64 input, the input; will be treated as an i32, leaving the upper bits uninitialised.; For example:. define void @f4(i32 *%dst) {; %val = call i32 asm ""blah $0"", ""=r,0"" (i64 103); store i32 %val, i32 *%dst; ret void; }. from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.; to load 103. This seems to be a general target-independent problem. --. The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:674,load,load,674,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,5,"['LOAD', 'load', 'perform']","['LOAD', 'load', 'performance']"
Performance,"//root.cern/doc/v624/classRooCrystalBall.html), which implements some common generalizations of the Crystal Ball shape:. - symmetric or asymmetric power-law tails on both sides; - different width parameters for the left and right sides of the Gaussian core. The new `RooCrystalBall` class can substitute the `RooDSCBShape` and `RooSDSCBShape`, which were passed around in the community. ## 2D Graphics Libraries. - Add the method `AddPoint`to `TGraph(x,y)` and `TGraph2D(x,y,z)`, equivalent to `SetPoint(g->GetN(),x,y)`and `SetPoint(g->GetN(),x,y,z)`; - Option `E0` draws error bars and markers are drawn for bins with 0 contents. Now, combined; with options E1 and E2, it avoids error bars clipping. ## 3D Graphics Libraries. ## Geometry Libraries. ## Database Libraries. ## Networking Libraries. ### Multithreaded support for FastCGI. Now when THttpServer creates FastCGI engine, 10 worker threads used to process requests; received via FastCGI channel. This significantly increase a performance, especially when; several clients are connected. ### Better security for THttpServer with webgui. If THttpServer created for use with webgui widgets (RBrowser, RCanvas, REve), it only will; provide access to the widgets via websocket connection - any other kind of requests like root.json; or exe.json will be refused completely. Combined with connection tokens and https protocol,; this makes usage of webgui components in public networks more secure. ### Enabled WLCG Bearer Tokens support in RDavix. Bearer tokens are part of WLCG capability-based infrastructure with capability-based scheme which uses an infrastructure that describes what the bearer is allowed to do as opposed to who that bearer is. Token discovery procedure are developed according WLCG Bearer Token Discovery specification document (https://github.com/WLCG-AuthZ-WG/bearer-token-discovery/blob/master/specification.md). Short overview:. 1. If the `BEARER_TOKEN` environment variable is set, then the value is taken to be the tok",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:25204,perform,performance,25204,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,1,['perform'],['performance']
Performance,"//root.cern/files/brun_lcgapp09.pdf .; The baskets are flushed and the Tree header saved at regular intervals (See AutoFlush and OptimizeBaskets); When the amount of data written so far (fTotBytes) is greater than fAutoFlush (see SetAutoFlush) all the baskets are flushed to disk.; This makes future reading faster as it guarantees that baskets belonging to nearby entries will be on the same disk region.; When the first call to flush the baskets happens, we also take this opportunity to optimize the baskets buffers.; We also check if the number of bytes written is greater than fAutoSave (see SetAutoSave).; In this case we also write the Tree header. This makes the Tree recoverable up to this point in case the program writing the Tree crashes.; Note that the user can also decide to call FlushBaskets and AutoSave in her event loop on the base of the number of events written instead of the number of bytes written.; New function TTree::OptimizeBaskets. void TTree::OptimizeBaskets(Int_t maxMemory, Float_t minComp, Option_t *option). This function may be called after having filled some entries in a Tree; using the information in the existing branch buffers, it will reassign; new branch buffer sizes to optimize time and memory.; The function computes the best values for branch buffer sizes such that; the total buffer sizes is less than maxMemory and nearby entries written; at the same time.; In case the branch compression factor for the data written so far is less; than compMin, the compression is disabled. if option =""d"" an analysis report is printed.; This function may also be called on an existing Tree to figure out the best values; given the information in the Tree header. TFile f(""myfile.root"");; TTree *T = (TTree*)f.Get(""mytreename"");; T->Print(); //show the branch buffer sizes before optimization; T->OptimizeBaskets(10000000,1,""d"");; T->Print(); //show the branch buffer sizes after optimization. New interface functions to customize the TreeCache; virtual void AddBranch",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html:4348,Optimiz,OptimizeBaskets,4348,tree/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html,1,['Optimiz'],['OptimizeBaskets']
Performance,"/C++ lookup then we create; redundant AST nodes during the merge! Also, having two instances of the same; node could result in false :ref:`structural in-equivalencies <structural-eq>`; of other nodes which depend on the duplicated node. Because of these reasons,; we created a lookup class which has the sole purpose to register all; declarations, so later they can be looked up by subsequent import requests.; This is the ``ASTImporterLookupTable`` class. This lookup table should be; shared amongst the different ``ASTImporter`` instances if they happen to import; to the very same ""to"" context. This is why we can use the importer specific; lookup only via the ``ASTImporterSharedState`` class. ExternalASTSource; ~~~~~~~~~~~~~~~~~. The ``ExternalASTSource`` is an abstract interface associated with the; ``ASTContext`` class. It provides the ability to read the declarations stored; within a declaration context either for iteration or for name lookup. A; declaration context with an external AST source may load its declarations; on-demand. This means that the list of declarations (represented as a linked; list, the head is ``DeclContext::FirstDecl``) could be empty. However, member; functions like ``DeclContext::lookup()`` may initiate a load. Usually, external sources are associated with precompiled headers. For example,; when we load a class from a PCH then the members are loaded only if we do want; to look up something in the class' context. In case of LLDB, an implementation of the ``ExternalASTSource`` interface is; attached to the AST context which is related to the parsed expression. This; implementation of the ``ExternalASTSource`` interface is realized with the help; of the ``ASTImporter`` class. This way, LLDB can reuse Clang's parsing; machinery while synthesizing the underlying AST from the debug data (e.g. from; DWARF). From the view of the ``ASTImporter`` this means both the ""to"" and the; ""from"" context may have declaration contexts with external lexical storage.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst:106689,load,load,106689,interpreter/llvm-project/clang/docs/InternalsManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/InternalsManual.rst,1,['load'],['load']
Performance,/ExceptionEscapeCheck.h; clang-tools-extra/clang-tidy/openmp/OpenMPTidyModule.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.h; clang-tools-extra/clang-tidy/performance/FasterStringFindCheck.cpp; clang-tools-extra/clang-tidy/performance/ForRangeCopyCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.h; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.h; clang-tools-extra/clang-tidy/performance/UnnecessaryCopyInitialization.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.h; clang-tools-extra/clang-tidy/plugin/ClangTidyPlugin.cpp; clang-tools-extra/clang-tidy/portability/PortabilityTidyModul,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:65474,perform,performance,65474,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,"/F"").; - Allow reading of older version of TTreePerfStats (ROOT-8520); - In `TTree::OptimizeBaskets()` do not call GetBasket(0) to avoid disc reads; - It is now possible to define the precision of the default histogram created; by `TTree::Draw`. Three new parameters are available in `$ROOTSYS/etcsystem.rootrc`; ```{.cpp}; Hist.Precision.1D: float; Hist.Precision.2D: float; Hist.Precision.3D: float; ```; the default values are `float`. They can be set to `double`.; - Fix ROOT-8742: TTree::SetBranchAddress could not be invoked safely even when dealing with the same tree obtained from the same file opened in different threads.; - TTree::Branch() now complains if a ""name[size]/F"" branch specification is passed wrongly (e.g. as ""name/F[size]""). ### TDataFrame; - Creation of the TDataFrame class. The TDataFrame allows to interact with data; stored in columnar format in a functional and intuitive way in order to perform; data analysis. Parallelism is accessible simply by activating implicit; multi-threading with the ROOT::EnableImplicitMT() function.; In a nutshell, the functionality provided is:; - Create and fill histograms with one single method invocation; - Express filtering of entries with strings, lambdas or functions; - Easy creation of efficiencies of cut-flows; - Possibility to run on ranges of entries; - Creating columns not present in the original dataset; - Chain multiple actions to be executed on the same event loop; - Creation of events on-the-fly (e.g. via Pythia or user-define generator functors), with no need for an input TTree; - Snapshot on a rootfile the dataset after cuts and after augmentation with columns created by the user; - Run analyses expressed as chains of actions in parallel in a transparent way for the user; See [the online documentation](https://root.cern.ch/doc/master/classROOT_1_1Experimental_1_1TDF_1_1TDataFrame.html) for more details. ## 2D Graphics Libraries; - If one used ""col2"" or ""colz2"", the value of `TH1::fMaximum` got modified.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v610/index.md:6341,multi-thread,multi-threading,6341,README/ReleaseNotes/v610/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v610/index.md,1,['multi-thread'],['multi-threading']
Performance,/MissingHashCheck.cpp; clang-tools-extra/clang-tidy/objc/MissingHashCheck.h; clang-tools-extra/clang-tidy/objc/NSInvocationArgumentLifetimeCheck.cpp; clang-tools-extra/clang-tidy/objc/NSInvocationArgumentLifetimeCheck.h; clang-tools-extra/clang-tidy/objc/PropertyDeclarationCheck.h; clang-tools-extra/clang-tidy/objc/SuperSelfCheck.cpp; clang-tools-extra/clang-tidy/objc/SuperSelfCheck.h; clang-tools-extra/clang-tidy/openmp/ExceptionEscapeCheck.cpp; clang-tools-extra/clang-tidy/openmp/ExceptionEscapeCheck.h; clang-tools-extra/clang-tidy/openmp/OpenMPTidyModule.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.h; clang-tools-extra/clang-tidy/performance/FasterStringFindCheck.cpp; clang-tools-extra/clang-tidy/performance/ForRangeCopyCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.h; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleChec,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:64979,perform,performance,64979,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,"/WLCG-AuthZ-WG/bearer-token-discovery/blob/master/specification.md). Short overview:. 1. If the `BEARER_TOKEN` environment variable is set, then the value is taken to be the token contents.; 2. If the `BEARER_TOKEN_FILE` environment variable is set, then its value is interpreted as a filename. The contents of the specified file are taken to be the token contents.; 3. If the `XDG_RUNTIME_DIR` environment variable is set, then take the token from the contents of `$XDG_RUNTIME_DIR/bt_u$ID`(this additional location is intended to provide improved security for shared login environments as `$XDG_RUNTIME_DIR` is defined to be user-specific as opposed to a system-wide directory.).; 4. Otherwise, take the token from `/tmp/bt_u$ID`. ## GUI Libraries. ### RBrowser improvements. - central factory methods to handle browsing, editing and drawing of different classes; - simple possibility to extend RBrowser on user-defined classes; - support of web-based geometry viewer; - better support of TTree drawing; - server-side handling of code editor and image viewer widgets; - rbrowser content is fully recovered when web-browser is reloaded; - load of widgets code only when really required (shorter startup time for RBrowser). ## Montecarlo Libraries. ## PROOF Libraries. ## Language Bindings. ## JavaScript ROOT. ### Major JSROOT update to version 6. - update all used libraries `d3.js`, `three.js`, `MathJax.js`, openui5; - change to Promise based interface for all async methods, remove call-back arguments; - change scripts names, core scripts name now `JSRoot.core.js`; - unify function/methods naming conventions, many changes in method names; - provide central code loader via `JSROOT.require`, supporting 4 different loading engines; - many nice features and many bug fixes; see JSROOT v6 release notes. ## Tutorials. ## Class Reference Guide. ## Build, Configuration and Testing Infrastructure. - a new cmake variable, `CMAKE_INSTALL_PYTHONDIR`, has been added: it allows customization of the i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:27182,load,load,27182,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,1,['load'],['load']
Performance,"/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - s_waitcnt lgkmcnt(0) must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:331427,load,load,331427,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If CU wavefront execution; mode, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If CU wavefront execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:347143,load,load,347143,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - s_waitcnt lgkmcnt(0) must; happen after; preceding; local/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/store; atomic/atomicrmw; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before execut",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:330726,load,load,330726,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 4. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - I",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:365956,load,load,365956,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"/interleaved loop; Stmt(i:i+3);; }; for (; i < n; i+=1) // epilogue loop; Stmt(i);. where ``rtc`` is a generated runtime check. ``llvm.loop.vectorize.followup_vectorized`` will set the attributes for; the vectorized loop. If not specified, ``llvm.loop.isvectorized`` is; combined with the original loop's attributes to avoid it being; vectorized multiple times. ``llvm.loop.vectorize.followup_epilogue`` will set the attributes for; the remainder loop. If not specified, it will have the original loop's; attributes combined with ``llvm.loop.isvectorized`` and; ``llvm.loop.unroll.runtime.disable`` (unless the original loop already; has unroll metadata). The attributes specified by ``llvm.loop.vectorize.followup_all`` are; added to both loops. When using a follow-up attribute, it replaces any automatically deduced; attributes for the generated loop in question. Therefore it is; recommended to add ``llvm.loop.isvectorized`` to; ``llvm.loop.vectorize.followup_all`` which avoids that the loop; vectorizer tries to optimize the loops again. Loop Unrolling; --------------. Unrolling is interpreted as forced any ``!{!""llvm.loop.unroll.enable""}``; metadata or option (``llvm.loop.unroll.count``, ``llvm.loop.unroll.full``); is present. Unrolling can be full unrolling, partial unrolling of a loop; with constant trip count or runtime unrolling of a loop with a trip; count unknown at compile-time. If the loop has been unrolled fully, there is no followup-loop. For; partial/runtime unrolling, the original loop of. .. code-block:: c. for (int i = 0; i < n; i+=1) // original loop; Stmt(i);. is transformed into (using an unroll factor of 4):. .. code-block:: c. int i = 0;; for (; i + 3 < n; i+=4) { // unrolled loop; Stmt(i);; Stmt(i+1);; Stmt(i+2);; Stmt(i+3);; }; for (; i < n; i+=1) // remainder loop; Stmt(i);. ``llvm.loop.unroll.followup_unrolled`` will set the loop attributes of; the unrolled loop. If not specified, the attributes of the original loop; without the ``llvm.loop.unroll.*``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst:7079,optimiz,optimize,7079,interpreter/llvm-project/llvm/docs/TransformMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst,1,['optimiz'],['optimize']
Performance,"/pymva/src/RModelParser_PyTorch.cxx) parser for translating models in respective formats into SOFIE's internal representation. From ROOT command line, or in a ROOT macro, we can proceed with an ONNX model:. ```c++; using namespace TMVA::Experimental;; SOFIE::RModelParser_ONNX parser;; SOFIE::RModel model = parser.Parse(“./example_model.onnx”);; model.Generate();; model.OutputGenerated(“./example_output.hxx”);; ```. And an C++ header file and a `.dat` file containing the model weights will be generated. You can also use. ```c++; model.PrintRequiredInputTensors();; ```. to check the required size and type of input tensor for that particular model, and use. ```c++; model.PrintInitializedTensors();; ```. to check the tensors (weights) already included in the model. To use the generated inference code:. ```c++; #include ""example_output.hxx""; float input[INPUT_SIZE];; std::vector<float> out = TMVA_SOFIE_example_model::infer(input);. // Generated header file shall contain a Session class which requires initialization to load the corresponding weights.; TMVA_SOFIE_example_model::Session s(""example_model.dat""). // Once instantiated the session object's infer method can be used; std::vector<float> out = s.infer(input);; ```. With the default settings, the weights are contained in a separate binary file, but if the user instead wants them to be in the generated header file itself, they can use approproiate generation options. ```c++; model.Generate(Options::kNoWeightFile);; ```. Other such options includes `Options::kNoSession` (for not generating the Session class, and instead keeping the infer function independent).; SOFIE also supports generating inference code with RDataFrame as inputs, refer to the tutorials below for examples. ## Supported ONNX operators. Here is the updated list of supported ONNX operators. - [x] Add; - [x] AveragePool; - [x] BatchNormalization; - [x] Cast; - [x] Concat; - [x] Constant; - [x] ConstantOfShape; - [x] Conv; - [x] ConvTranspose; - [x] Elu; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/sofie/README.md:2124,load,load,2124,tmva/sofie/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/sofie/README.md,1,['load'],['load']
Performance,"/res; ${CMAKE_SOURCE_DIR}/core/meta/inc; ${CMAKE_SOURCE_DIR}/core/metacling/res; ${CMAKE_SOURCE_DIR}/core/thread/inc; ${CMAKE_SOURCE_DIR}/core/zip/inc; ${CMAKE_SOURCE_DIR}/io/io/inc; ${CMAKE_BINARY_DIR}/ginclude; ). set_target_properties(MetaCling PROPERTIES; COMPILE_FLAGS ""${CMAKE_CXX_FLAGS} ${CLING_CXXFLAGS}""; VISIBILITY_INLINES_HIDDEN ""ON""; ). if(MSVC); target_include_directories(MetaCling PRIVATE; ${CMAKE_SOURCE_DIR}/core/winnt/inc; ); set_source_files_properties(TCling.cxx COMPILE_FLAGS /bigobj); endif(). add_dependencies(MetaCling CLING). ##### libCling #############################################################. if(NOT builtin_clang); set(prefixed_link_libraries); foreach(dep ${CLING_DEPEND_LIBS}); if(""${dep}"" MATCHES ""^clang""); set(dep ""${LLVM_LIBRARY_DIR}/lib${dep}.a""); endif(); list(APPEND prefixed_link_libraries ""${dep}""); endforeach(); set(LINK_LIBS ""${prefixed_link_libraries}""); link_directories(""${LLVM_LIBRARY_DIR}""); endif(). # We need to paste the content of the cling plugins disabling link symbol optimizations.; set(CLING_PLUGIN_LINK_LIBS); if (clad); if (APPLE); set(CLING_PLUGIN_LINK_LIBS -Wl,-force_load cladPlugin -Wl,-force_load cladDifferentiator); elseif(MSVC); set(CLING_PLUGIN_LINK_LIBS cladPlugin cladDifferentiator); set(CLAD_LIBS ""-WHOLEARCHIVE:cladPlugin.lib -WHOLEARCHIVE:cladDifferentiator.lib""); else(); set(CLING_PLUGIN_LINK_LIBS -Wl,--whole-archive cladPlugin cladDifferentiator -Wl,--no-whole-archive); endif(); if(TARGET clang); # Link our clad libraries to clang. If users use the clang from ROOT they will; # also be able to use clad out of the box.; add_dependencies(clang clad); target_link_libraries(clang PUBLIC ${CLING_PLUGIN_LINK_LIBS}); endif(); endif(). ROOT_LINKER_LIBRARY(Cling; $<TARGET_OBJECTS:ClingUtils>; $<TARGET_OBJECTS:Dictgen>; $<TARGET_OBJECTS:MetaCling>; LIBRARIES ${CLING_LIBRARIES} ${LINK_LIBS} ${CLING_PLUGIN_LINK_LIBS}). # When these two link at the same time, they can exhaust the RAM on many machines, since they both ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/CMakeLists.txt:3019,optimiz,optimizations,3019,core/metacling/src/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/CMakeLists.txt,1,['optimiz'],['optimizations']
Performance,"/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkmcnt(0). - If OpenCL and; address space is; not generic, omit.; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Must happen after; any preceding; local/generic; load/load; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:226652,load,load,226652,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"/test: TMVAMulticlass.C; and TMVAMulticlassApplication.C. New TMVA event vector building. The code; for splitting the input data into training and test samples for; all classes and the mixing of those samples to one training and; one test sample has been rewritten completely. The new code is; more performant and has a clearer structure. This fixes several; bugs which have been reported by some users of TMVA.; Code and performance test framework: A unit; test framework for daily software and method performance; validation has been implemented.; . Methods. BDT Automatic parameter optimisation for building the; tree architecture: The optimisation procedure uses the; performance of the trained classifier on the ""test sample"" for; finding the set of optimal parameters. Two different methods to; traverse the parameter space are available (scanning, genetic; algorithm). Currently parameter optimization is implemented only; for these three parameters that influence the tree architectur:; the maximum depth of a tree, MaxDepth, the minimum; number of events in each node, NodeMinEvents, and; the number of tress, NTrees. Optimization can; is invoked by calling; factory->OptimizeAllMethods(); prior to the call; factory->TrainAllMethods();. Automated and configurable parameter optimization is soon to; be enabled for all methods (for those parameters where; optimization is applicable).; . BDT node splitting: While Decision Trees; typically have only univariate splits, in TMVA one can now; also opt for multivariate splits that use a ""Fisher; Discriminant"" (option: UseFisherCuts), built from all; observables that show correlations larger than some threshold; (MinLinCorrForFisher). The training will then test at each; split a cut on this fisher discriminant in addition to all; univariate cuts on the variables (or only on those variables; that have not been used in the Fisher discriminant, option; UseExcusiveVars). No obvious improvement betwen very simple; decision trees after boostin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v528/index.html:1892,optimiz,optimization,1892,tmva/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v528/index.html,2,['optimiz'],['optimization']
Performance,"/tmp/cc-Sn4RKF.s""], output: ""/tmp/cc-gvSnbS.o""; # ""i386-apple-darwin9"" - ""darwin::Link"", inputs: [""/tmp/cc-gvSnbS.o""], output: ""/tmp/cc-jgHQxi.out""; # ""ppc-apple-darwin9"" - ""gcc::Compile"", inputs: [""t0.c""], output: ""/tmp/cc-Q0bTox.s""; # ""ppc-apple-darwin9"" - ""gcc::Assemble"", inputs: [""/tmp/cc-Q0bTox.s""], output: ""/tmp/cc-WCdicw.o""; # ""ppc-apple-darwin9"" - ""gcc::Link"", inputs: [""/tmp/cc-WCdicw.o""], output: ""/tmp/cc-HHBEBh.out""; # ""i386-apple-darwin9"" - ""darwin::Lipo"", inputs: [""/tmp/cc-jgHQxi.out"", ""/tmp/cc-HHBEBh.out""], output: ""a.out"". This shows the tool chain, tool, inputs and outputs which have been; bound for this compilation sequence. Here clang is being used to; compile t0.c on the i386 architecture and darwin specific versions of; the tools are being used to assemble and link the result, but generic; gcc versions of the tools are being used on PowerPC. #. **Translate: Tool Specific Argument Translation**. Once a Tool has been selected to perform a particular Action, the; Tool must construct concrete Commands which will be executed during; compilation. The main work is in translating from the gcc style; command line options to whatever options the subprocess expects. Some tools, such as the assembler, only interact with a handful of; arguments and just determine the path of the executable to call and; pass on their input and output arguments. Others, like the compiler; or the linker, may translate a large number of arguments in addition. The ArgList class provides a number of simple helper methods to; assist with translating arguments; for example, to pass on only the; last of arguments corresponding to some option, or all arguments for; an option. The result of this stage is a list of Commands (executable paths and; argument strings) to execute. #. **Execute**. Finally, the compilation pipeline is executed. This is mostly; straightforward, although there is some interaction with options like; ``-pipe``, ``-pass-exit-codes`` and ``-time``. Additional Notes; --",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DriverInternals.rst:11017,perform,perform,11017,interpreter/llvm-project/clang/docs/DriverInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DriverInternals.rst,1,['perform'],['perform']
Performance,"/wiki/Curiously_recurring_template_pattern. E.g. .. code-block:: c++. class Shape : public RTTIExtends<Shape, RTTIRoot> {; public:; static char ID;; virtual double computeArea() = 0;; };. class Square : public RTTIExtends<Square, Shape> {; double SideLength;; public:; static char ID;. Square(double S) : SideLength(S) {}; double computeArea() override;; };. class Circle : public RTTIExtends<Circle, Shape> {; double Radius;; public:; static char ID;. Circle(double R) : Radius(R) {}; double computeArea() override;; };. char Shape::ID = 0;; char Square::ID = 0;; char Circle::ID = 0;. Advanced Use Cases; ==================. The underlying implementation of isa/cast/dyn_cast is all controlled through a; struct called ``CastInfo``. ``CastInfo`` provides 4 methods, ``isPossible``,; ``doCast``, ``castFailed``, and ``doCastIfPossible``. These are for ``isa``,; ``cast``, and ``dyn_cast``, in order. You can control the way your cast is; performed by creating a specialization of the ``CastInfo`` struct (to your; desired types) that provides the same static methods as the base ``CastInfo``; struct. This can be a lot of boilerplate, so we also have what we call Cast Traits.; These are structs that provide one or more of the above methods so you can; factor out common casting patterns in your project. We provide a few in the; header file ready to be used, and we'll show a few examples motivating their; usage. These examples are not exhaustive, and adding new cast traits is easy; so users should feel free to add them to their project, or contribute them if; they're particularly useful!. Value to value casting; ----------------------; In this case, we have a struct that is what we call 'nullable' - i.e. it is; constructible from ``nullptr`` and that results in a value you can tell is; invalid. .. code-block:: c++. class SomeValue {; public:; SomeValue(void *ptr) : ptr(ptr) {}; void *getPointer() const { return ptr; }; bool isValid() const { return ptr != nullptr; }; private:; void *pt",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst:14885,perform,performed,14885,interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToSetUpLLVMStyleRTTI.rst,1,['perform'],['performed']
Performance,"0 then the collection; will be written in split mode. Ie. if it contains objects of any; types deriving from TTrack this function will sort the objects; basing on their type and store them in separate branches in split; mode.; The ROOT test example in ROOTSYS/test/bench.cxx shows many examples of collections; and storage in a TTree when using split mode or not. This program illustrates the important; gain in space and time when using this new facility. Parallel unzipping. Introducing a parallel unzipping algorithm for pre-fetched buffers. Since we already know what buffers are going to be read, we can decompress a few of them in advance in an additional thread and give the impression that the data decompression comes for free (we gain up to 30% in reading intensive jobs). The size of this unzipping cache is 20% the size of the TTreeCache and can be modified with TTreeCache::SetUnzipBufferSize(Long64_t bufferSize). Theoretically, we only need one buffer in advance but in practice we might fall short if the unzipping cache is too small (synchronization costs). This experimental feature is disabled by default, to activate it use the static function TTreeCache::SetParallelUnzip(TTreeCacheUnzip::EParUnzipMode option = TTreeCacheUnzip::kEnable). The possible values to pass are: TTreeCacheUnzip::kEnable to enable itTTreeCacheUnzip::kDisable to disable itTTreeCacheUnzip::kForce to force it.The TTreeCacheUnzip is actived; only if you have more than one core. To activate it with only one core useTTreeCacheUnzip::kForce option (for example to measure the overhead). Disk and Memory Space Gain. In ROOT older than v5.20/00, the branches' last basket, also known as the write basket, was always saved in the same ""key"" as the TTree object and was always present in memory when reading or writing.; When reading this write basket was always present in memory even if the branch was never accessed. Starting in v5.20/00, TTree::Write closes out, compresses (when requested) and writes to di",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v520/index.html:5229,cache,cache,5229,tree/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v520/index.html,2,['cache'],['cache']
Performance,"0' bits, these operations are identical to :ref:`llvm.masked.store <int_mstore>` and :ref:`llvm.masked.load <int_mload>`. .. _int_expandload:. '``llvm.masked.expandload.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. Several values of integer, floating point or pointer data type are loaded from consecutive memory addresses and stored into the elements of a vector according to the mask. ::. declare <16 x float> @llvm.masked.expandload.v16f32 (ptr <ptr>, <16 x i1> <mask>, <16 x float> <passthru>); declare <2 x i64> @llvm.masked.expandload.v2i64 (ptr <ptr>, <2 x i1> <mask>, <2 x i64> <passthru>). Overview:; """""""""""""""""". Reads a number of scalar values sequentially from memory location provided in '``ptr``' and spreads them in a vector. The '``mask``' holds a bit for each vector lane. The number of elements read from memory is equal to the number of '1' bits in the mask. The loaded elements are positioned in the destination vector according to the sequence of '1' and '0' bits in the mask. E.g., if the mask vector is '10010001', ""expandload"" reads 3 values from memory addresses ptr, ptr+1, ptr+2 and places them in lanes 0, 3 and 7 accordingly. The masked-off lanes are filled by elements from the corresponding lanes of the '``passthru``' operand. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. It has the same underlying type as the element of the returned vector. The second operand, mask, is a vector of boolean values with the same number of elements as the return type. The third is a pass-through value that is used to fill the masked-off lanes of the result. The return type and the type of the '``passthru``' operand have the same vector type. Semantics:; """""""""""""""""""". The '``llvm.masked.expandload``' intrinsic is designed for reading multiple scalar values from adjacent memory addresses into possibly non-adjacent vector lanes. It is useful for targets that support vector expanding load",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:855911,load,loaded,855911,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,"0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_wbinvl1_vol.; - Ensures the atomicrmw; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following glob",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:249266,cache,cache,249266,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If not TgSplit execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. buffer_wbl2 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; 4. s_waitcnt vmcnt(0). - Must happen before;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:319498,load,loads,319498,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 3. flat_atomic; 4. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 5. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - generic 1. buffer_wbl2 sc0=1 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. flat_atomic sc1=1; 4. s_waitcnt vmcnt",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:323407,load,loads,323407,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"0, 0);; works/worked fine). Add type information to the result of TTree::Print in the case of; TBranchElement:; *Br 17 :fH : TH1F* *; *Entries : 20 : Total Size= 19334 bytes File Size = 1671 *; *Baskets : 2 : Basket Size= 16000 bytes Compression= 11.29 *; *............................................................................*; *Br 18 :fTriggerBits : TBits *; *Entries : 20 : Total Size= 1398 bytes File Size = 400 *; *Baskets : 1 : Basket Size= 16000 bytes Compression= 2.23 *; *............................................................................*; *Br 19 :fIsValid : Bool_t *; *Entries : 20 : Total Size= 582 bytes File Size = 92 *; *Baskets : 1 : Basket Size= 16000 bytes Compression= 1.00 *. Add a new function TBranch::SetStatus It is much faster to call this function in case of a Tree with many branches; instead of calling TTree::SetBranchStatus.; Implement TTreeCache::Print that shows information like:; // ******TreeCache statistics for file: cms2.root ******; // Number of branches in the cache ...: 1093; // Cache Efficiency ..................: 0.997372; // Cache Efficiency Rel...............: 1.000000; // Learn entries......................: 100; // Reading............................: 72761843 bytes in 7 transactions; // Readahead..........................: 256000 bytes with overhead = 0 bytes; // Average transaction................: 10394.549000 Kbytes; // Number of blocks in current cache..: 210, total size: 6280352; This function can be called directly from TTree: T->PrintCacheStats();. Add support for variable size array of object in a TTree (when the owner of the array is split.); And many other bug fixes, security fixes, thread safety and performance improvements ; see the svn log for details. TTree Scan and Draw. Insured that the generated histogram as an integral bin width when plotting a string or integer.; Improved the output of TTree::Scan by inserting a blank space whenever a value is not available because there is no proper row in a frien",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v528/index.html:5787,cache,cache,5787,tree/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v528/index.html,2,['cache'],['cache']
Performance,"0, i32 2; store i32 %inc, ptr %inc.spill.addr; call void @print(i32 %n). ret ptr %frame; }. Outlined resume part of the coroutine will reside in function `f.resume`:. .. code-block:: llvm. define internal fastcc void @f.resume(ptr %frame.ptr.resume) {; entry:; %inc.spill.addr = getelementptr %f.frame, ptr %frame.ptr.resume, i64 0, i32 2; %inc.spill = load i32, ptr %inc.spill.addr, align 4; %inc = add i32 %inc.spill, 1; store i32 %inc, ptr %inc.spill.addr, align 4; tail call void @print(i32 %inc); ret void; }. Whereas function `f.destroy` will contain the cleanup code for the coroutine:. .. code-block:: llvm. define internal fastcc void @f.destroy(ptr %frame.ptr.destroy) {; entry:; tail call void @free(ptr %frame.ptr.destroy); ret void; }. Avoiding Heap Allocations; -------------------------. A particular coroutine usage pattern, which is illustrated by the `main`; function in the overview section, where a coroutine is created, manipulated and; destroyed by the same calling function, is common for coroutines implementing; RAII idiom and is suitable for allocation elision optimization which avoid; dynamic allocation by storing the coroutine frame as a static `alloca` in its; caller. In the entry block, we will call `coro.alloc`_ intrinsic that will return `true`; when dynamic allocation is required, and `false` if dynamic allocation is; elided. .. code-block:: llvm. entry:; %id = call token @llvm.coro.id(i32 0, ptr null, ptr null, ptr null); %need.dyn.alloc = call i1 @llvm.coro.alloc(token %id); br i1 %need.dyn.alloc, label %dyn.alloc, label %coro.begin; dyn.alloc:; %size = call i32 @llvm.coro.size.i32(); %alloc = call ptr @CustomAlloc(i32 %size); br label %coro.begin; coro.begin:; %phi = phi ptr [ null, %entry ], [ %alloc, %dyn.alloc ]; %hdl = call noalias ptr @llvm.coro.begin(token %id, ptr %phi). In the cleanup block, we will make freeing the coroutine frame conditional on; `coro.free`_ intrinsic. If allocation is elided, `coro.free`_ returns `null`; thus skipping t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst:16479,optimiz,optimization,16479,interpreter/llvm-project/llvm/docs/Coroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst,1,['optimiz'],['optimization']
Performance,"0.; GFX11; Number of instruction bytes to prefetch, starting at the kernel's entry; point instruction, before wavefront starts execution. The value is 0..63; with a granularity of 128 bytes.; 10 1 bit TRAP_ON_START GFX10; Reserved, must be 0.; GFX11; Must be 0. If 1, wavefront starts execution by trapping into the trap handler. CP is responsible for filling in the trap on start bit in; ``COMPUTE_PGM_RSRC3.TRAP_ON_START`` according to what the runtime; requests.; 11 1 bit TRAP_ON_END GFX10; Reserved, must be 0.; GFX11; Must be 0. If 1, wavefront execution terminates by trapping into the trap handler. CP is responsible for filling in the trap on end bit in; ``COMPUTE_PGM_RSRC3.TRAP_ON_END`` according to what the runtime requests.; 30:12 19 bits Reserved, must be 0.; 31 1 bit IMAGE_OP GFX10; Reserved, must be 0.; GFX11; If 1, the kernel execution contains image instructions. If executed as; part of a graphics pipeline, image read instructions will stall waiting; for any necessary ``WAIT_SYNC`` fence to be performed in order to; indicate that earlier pipeline stages have completed writing to the; image. Not used for compute kernels that are not part of a graphics pipeline and; must be 0.; 32 **Total size 4 bytes.**; ======= ===================================================================================================================. .. .. table:: compute_pgm_rsrc3 for GFX12; :name: amdgpu-amdhsa-compute_pgm_rsrc3-gfx12-table. ======= ======= =============================== ===========================================================================; Bits Size Field Name Description; ======= ======= =============================== ===========================================================================; 3:0 4 bits RESERVED Reserved, must be 0.; 11:4 8 bits INST_PREF_SIZE Number of instruction bytes to prefetch, starting at the kernel's entry; point instruction, before wavefront starts execution. The value is 0..255; with a granularity of 128 bytes.; 12 1 bit RESE",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:177992,perform,performed,177992,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"000000 (-1.7976922776554302e308). Examples of disabled conversions:. .. parsed-literal::. // GFX9. v_add_u16 v0, 0x1ff00, v0 // truncated bits are not all 0 or 1; v_add_u16 v0, 0xffffffffffff00ff, v0 // truncated bits do not match MSB of the result. .. _amdgpu_synid_fp_conv:. Conversion of Floating-Point Values; -----------------------------------. Instruction operands may be specified as 64-bit; :ref:`floating-point numbers<amdgpu_synid_floating-point_number>`.; These values are converted to the; :ref:`expected operand type<amdgpu_syn_instruction_type>`; using the following steps:. 1. *Validation*. Assembler checks if the input f64 number can be converted; to the *required floating-point type* (see the table below) without overflow; or underflow. Precision lost is allowed. If this conversion is not possible,; the assembler triggers an error. 2. *Conversion*. The input value is converted to the expected type; as described in the table below. Depending on operand kind, this is; performed by either assembler or AMDGPU H/W (or both). ============== ================ ================= =================================================================; Expected type Required FP Type Conversion Description; ============== ================ ================= =================================================================; i16, u16, b16 f16 f16(num) Convert to f16 and use bits of the result as an integer value.; The value has to be encoded as a literal, or an error occurs.; Note that the value cannot be encoded as an inline constant.; i32, u32, b32 f32 f32(num) Convert to f32 and use bits of the result as an integer value.; i64, u64, b64 \- \- Conversion disabled.; f16 f16 f16(num) Convert to f16.; f32 f32 f32(num) Convert to f32.; f64 f64 {num.u32.hi,0} Use high 32 bits of the number as high 32 bits of the result;; zero-fill low 32 bits of the result. Note that the result may differ from the original number.; ============== ================ ================= ================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUOperandSyntax.rst:37901,perform,performed,37901,interpreter/llvm-project/llvm/docs/AMDGPUOperandSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUOperandSyntax.rst,1,['perform'],['performed']
Performance,"00; ev++) {; Float_t sigmat, sigmas;; gRandom->Rannor(sigmat,sigmas);; Int_t ntrack = Int_t(600 + 600 *sigmat/120.);; Float_t random = gRandom->Rndm(1);; sprintf(etype,""type%d"",ev%5);; event->SetType(etype);; event->SetHeader(ev, 200, 960312, random);; event->SetNseg(Int_t(10*ntrack+20*sigmas));; event->SetNvertex(Int_t(1+20*gRandom->Rndm()));; event->SetFlag(UInt_t(random+0.5));; event->SetTemperature(random+20.);; for(UChar_t m = 0; m < 10; m++) {; event->SetMeasure(m, Int_t(gRandom->Gaus(m,m+1)));; }; // continued...; // fill the matrix; for(UChar_t i0 = 0; i0 < 4; i0++) {; for(UChar_t i1 = 0; i1 < 4; i1++) {; event->SetMatrix(i0,i1,gRandom->Gaus(i0*i1,1));; }; }; // create and fill the Track objects; for (Int_t t = 0; t < ntrack; t++) event->AddTrack(random);; t4.Fill(); // Fill the tree; event->Clear(); // Clear before reloading event; }; f.Write(); // Write the file header; t4.Print(); // Print the tree contents; }; ```. ### Reading the Tree. First, we check if the shared library with the class definitions is; loaded. If not we load it. Then we read two branches, one for the number; of tracks and one for the entire event. We check the number of tracks; first, and if it meets our condition, we read the entire event. We show; the fist entry that meets the condition. ``` {.cpp}; void tree4r() {; // check if the event class is in the dictionary; // if it is not load the definition in libEvent.so; if (!TClassTable::GetDict(""Event"")) {; gSystem->Load(""$ROOTSYS/test/libEvent.so"");; }; // read the tree generated with tree4w. // note that we use ""new"" to create the TFile and TTree objects, because we; // want to keep these objects alive when we leave this function.; TFile *f = new TFile(""tree4.root"");; TTree *t4 = (TTree*)f->Get(""t4"");. // create a pointer to an event object for reading the branch values.; Event *event = new Event();; // get two branches and set the branch address; TBranch *bntrack = t4->GetBranch(""fNtrack"");; TBranch *branch = t4->GetBranch(""event_spli",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:63936,load,loaded,63936,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['loaded']
Performance,"0324235020.49706-1-michael@platin.gs/>`_; whether similar functionality could be added to ``git blame`` itself. Minimising cost of downstream merges; ************************************. There are many forks of LLVM with downstream changes. Merging a large-scale; renaming change could be difficult for the fork maintainers. **Mitigation**: A large-scale renaming would be automated. A fork maintainer can; merge from the commit immediately before the renaming, then apply the renaming; script to their own branch. They can then merge again from the renaming commit,; resolving all conflicts by choosing their own version. This could be tested on; the [SVE]_ fork. Provisional Plan; ================. This is a provisional plan for the `Big bang`_ approach. It has not been agreed. #. Investigate improving ``git blame``. The extent to which it can be made to; ""look through"" commits may impact how big a change can be made. #. Write a script to expand acronyms. #. Experiment and perform dry runs of the various refactoring options.; Results can be published in forks of the LLVM Git repository. #. Consider the evidence and agree on the new policy. #. Agree & announce a date for the renaming of the starter project (LLD). #. Update the `policy page <../CodingStandards.html>`_. This will explain the; old and new rules and which projects each applies to. #. Refactor the starter project in two commits:. 1. Add or change the project's .clang-tidy to reflect the agreed rules.; (This is in a separate commit to enable the merging process described in; `Minimising cost of downstream merges`_).; Also update the project list on the policy page.; 2. Apply ``clang-tidy`` to the project's files, with only the; ``readability-identifier-naming`` rules enabled. ``clang-tidy`` will also; reformat the affected lines according to the rules in ``.clang-format``.; It is anticipated that this will be a good dog-fooding opportunity for; clang-tidy, and bugs should be fixed in the process, likely includin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:10728,perform,perform,10728,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,1,['perform'],['perform']
Performance,"08:xnack+``; Enable the ``xnack`` feature.; ``-mcpu=gfx908:xnack-``; Disable the ``xnack`` feature.; ``-mcumode``; Enable the ``cumode`` feature.; ``-mno-cumode``; Disable the ``cumode`` feature. .. table:: AMDGPU Target Features; :name: amdgpu-target-features-table. =============== ============================ ==================================================; Target Feature Clang Option to Control Description; Name; =============== ============================ ==================================================; cumode - ``-m[no-]cumode`` Control the wavefront execution mode used; when generating code for kernels. When disabled; native WGP wavefront execution mode is used,; when enabled CU wavefront execution mode is used; (see :ref:`amdgpu-amdhsa-memory-model`). sramecc - ``-mcpu`` If specified, generate code that can only be; - ``--offload-arch`` loaded and executed in a process that has a; matching setting for SRAMECC. If not specified for code object V2 to V3, generate; code that can be loaded and executed in a process; with SRAMECC enabled. If not specified for code object V4 or above, generate; code that can be loaded and executed in a process; with either setting of SRAMECC. tgsplit ``-m[no-]tgsplit`` Enable/disable generating code that assumes; work-groups are launched in threadgroup split mode.; When enabled the waves of a work-group may be; launched in different CUs. wavefrontsize64 - ``-m[no-]wavefrontsize64`` Control the wavefront size used when; generating code for kernels. When disabled; native wavefront size 32 is used, when enabled; wavefront size 64 is used. xnack - ``-mcpu`` If specified, generate code that can only be; - ``--offload-arch`` loaded and executed in a process that has a; matching setting for XNACK replay. If not specified for code object V2 to V3, generate; code that can be loaded and executed in a process; with XNACK replay enabled. If not specified for code object V4 or above, generate; code that can be loaded and executed in a pr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:17405,load,loaded,17405,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loaded']
Performance,"0],s[3])"") ;; w.factory(""Chebychev::g(x[-10,10],{0,1,2})"") ; // Now OK, x has identical spec, existing x will be used. Improvements to functions and pdfs. Addition to, reorganization of morphing operator classes. The existing class RooLinearMorph which; implements 'Alex Read' morphing has been renamed RooIntegralMorph. A new class RooMomentMorph; has been added (contribution from Max Baak and Stefan Gadatsch) that implements a different morphing algorithm ; based on shifting the mean and variance of the input pdfs. The new moment morphing class can also interpolate ; between multiple input templates and works with multi-dimensional input pdfs. One of the appealing features; is that no expensive calculations are required to calculate in the interpolated pdfs shapes after the pdf; initialization. An extension that allows morphing in two parameters is foreseen for the next root release.; Progress indication in plot projections; The RooAbsReal::plotOn() now accepts a new argument ShowProgress() that will print a dot for every; function evaluation performed in the process of creating the plot. This can be useful when plotting very expensive; functions such as profile likelihoods; Automatic handling of constraint terms; It is no longer necessary to add a Constrain() argument to fitTo() calls to have internal constraints; applied. Any pdf term appearing in a product that does not contain an observable and shares one or more parameters; with another pdf term in the same product that does contain an observable is automatically picked up as a constraint term.; For example given a dataset D(x) which defines variable x as observable, the default logic works out as follows. F(x,a,b)*G(a,a0,a1) --> G is constraint term (a also appears in F(x)); F(x,a,b)*G(y,c,d) --> G is dropped (factorizing term). A Constrain(y) term in the above example will still force term G(y,c,d) to be interpreted as constraint term; Automatic caching of numeric integral calculations; Integrals that require ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v526/index.html:6808,perform,performed,6808,roofit/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v526/index.html,2,['perform'],['performed']
Performance,"0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1 dlc=1; - system; - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_gl*_invl.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vm/vscnt(0). - If CU wavefront execution; mode, omit.; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - workgroup - local 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:348448,cache,caches,348448,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If CU wavefront execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1 dlc=1; - system; - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_gl*_invl.; - Ensures the flat_load; has completed; before invalidat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:347459,load,load,347459,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"1, 0, 0, 0, 0). // Reverse 4-element vector V1.; __builtin_shufflevector(V1, V1, 3, 2, 1, 0). // Concatenate every other element of 4-element vectors V1 and V2.; __builtin_shufflevector(V1, V2, 0, 2, 4, 6). // Concatenate every other element of 8-element vectors V1 and V2.; __builtin_shufflevector(V1, V2, 0, 2, 4, 6, 8, 10, 12, 14). // Shuffle v1 with some elements being undefined; __builtin_shufflevector(v1, v1, 3, -1, 1, -1). **Description**:. The first two arguments to ``__builtin_shufflevector`` are vectors that have; the same element type. The remaining arguments are a list of integers that; specify the elements indices of the first two vectors that should be extracted; and returned in a new vector. These element indices are numbered sequentially; starting with the first vector, continuing into the second vector. Thus, if; ``vec1`` is a 4-element vector, index 5 would refer to the second element of; ``vec2``. An index of -1 can be used to indicate that the corresponding element; in the returned vector is a don't care and can be optimized by the backend. The result of ``__builtin_shufflevector`` is a vector with the same element; type as ``vec1``/``vec2`` but that has an element count equal to the number of; indices specified. Query for this feature with ``__has_builtin(__builtin_shufflevector)``. .. _langext-__builtin_convertvector:. ``__builtin_convertvector``; ---------------------------. ``__builtin_convertvector`` is used to express generic vector; type-conversion operations. The input vector and the output vector; type must have the same number of elements. **Syntax**:. .. code-block:: c++. __builtin_convertvector(src_vec, dst_vec_type). **Examples**:. .. code-block:: c++. typedef double vector4double __attribute__((__vector_size__(32)));; typedef float vector4float __attribute__((__vector_size__(16)));; typedef short vector4short __attribute__((__vector_size__(8)));; vector4float vf; vector4short vs;. // convert from a vector of 4 floats to a vector of 4 d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:107868,optimiz,optimized,107868,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimized']
Performance,"1, <2 x double> %vec2, i32 %imm); declare <vscale x 4 x i32> @llvm.experimental.vector.splice.nxv4i32(<vscale x 4 x i32> %vec1, <vscale x 4 x i32> %vec2, i32 %imm). Overview:; """""""""""""""""". The '``llvm.experimental.vector.splice.*``' intrinsics construct a vector by; concatenating elements from the first input vector with elements of the second; input vector, returning a vector of the same type as the input vectors. The; signed immediate, modulo the number of elements in the vector, is the index; into the first vector from which to extract the result value. This means; conceptually that for a positive immediate, a vector is extracted from; ``concat(%vec1, %vec2)`` starting at index ``imm``, whereas for a negative; immediate, it extracts ``-imm`` trailing elements from the first vector, and; the remaining elements from ``%vec2``. These intrinsics work for both fixed and scalable vectors. While this intrinsic; is marked as experimental, the recommended way to express this operation for; fixed-width vectors is still to use a shufflevector, as that may allow for more; optimization opportunities. For example:. .. code-block:: text. llvm.experimental.vector.splice(<A,B,C,D>, <E,F,G,H>, 1); ==> <B, C, D, E> index; llvm.experimental.vector.splice(<A,B,C,D>, <E,F,G,H>, -3); ==> <B, C, D, E> trailing elements. Arguments:; """""""""""""""""""". The first two operands are vectors with the same type. The start index is imm; modulo the runtime number of elements in the source vector. For a fixed-width; vector <N x eltty>, imm is a signed integer constant in the range; -N <= imm < N. For a scalable vector <vscale x N x eltty>, imm is a signed; integer constant in the range -X <= imm < X where X=vscale_range_min * N. '``llvm.experimental.stepvector``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This is an overloaded intrinsic. You can use ``llvm.experimental.stepvector``; to generate a vector whose lane values comprise the linear sequence; <0, 1, 2, ...>. It is primarily intended fo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:671696,optimiz,optimization,671696,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"1, any corpus inputs from the 2nd, 3rd etc. corpus directories; that trigger new code coverage will be merged into the first corpus; directory. Defaults to 0. This flag can be used to minimize a corpus.; ``-merge_control_file``; Specify a control file used for the merge process.; If a merge process gets killed it tries to leave this file in a state; suitable for resuming the merge. By default a temporary file will be used.; ``-minimize_crash``; If 1, minimizes the provided crash input.; Use with -runs=N or -max_total_time=N to limit the number of attempts.; ``-reload``; If set to 1 (the default), the corpus directory is re-read periodically to; check for new inputs; this allows detection of new inputs that were discovered; by other fuzzing processes.; ``-jobs``; Number of fuzzing jobs to run to completion. Default value is 0, which runs a; single fuzzing process until completion. If the value is >= 1, then this; number of jobs performing fuzzing are run, in a collection of parallel; separate worker processes; each such worker process has its; ``stdout``/``stderr`` redirected to ``fuzz-<JOB>.log``.; ``-workers``; Number of simultaneous worker processes to run the fuzzing jobs to completion; in. If 0 (the default), ``min(jobs, NumberOfCpuCores()/2)`` is used.; ``-dict``; Provide a dictionary of input keywords; see Dictionaries_.; ``-use_counters``; Use `coverage counters`_ to generate approximate counts of how often code; blocks are hit; defaults to 1.; ``-reduce_inputs``; Try to reduce the size of inputs while preserving their full feature sets;; defaults to 1.; ``-use_value_profile``; Use `value profile`_ to guide corpus expansion; defaults to 0.; ``-only_ascii``; If 1, generate only ASCII (``isprint``+``isspace``) inputs. Defaults to 0.; ``-artifact_prefix``; Provide a prefix to use when saving fuzzing artifacts (crash, timeout, or; slow inputs) as ``$(artifact_prefix)file``. Defaults to empty.; ``-exact_artifact_path``; Ignored if empty (the default). If non-empty",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LibFuzzer.rst:12641,perform,performing,12641,interpreter/llvm-project/llvm/docs/LibFuzzer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LibFuzzer.rst,1,['perform'],['performing']
Performance,"1-bit significand; (counting the implicit leading 1). ``__bf16`` uses the `bfloat16; <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format>`_ format,; which provides an 8-bit exponent and an 8-bit significand; this is the same; exponent range as `float`, just with greatly reduced precision. ``_Float16`` and ``__bf16`` follow the usual rules for arithmetic; floating-point types. Most importantly, this means that arithmetic operations; on operands of these types are formally performed in the type and produce; values of the type. ``__fp16`` does not follow those rules: most operations; immediately promote operands of type ``__fp16`` to ``float``, and so; arithmetic operations are defined to be performed in ``float`` and so result in; a value of type ``float`` (unless further promoted because of other operands).; See below for more information on the exact specifications of these types. When compiling arithmetic on ``_Float16`` and ``__bf16`` for a target without; native support, Clang will perform the arithmetic in ``float``, inserting; extensions and truncations as necessary. This can be done in a way that; exactly matches the operation-by-operation behavior of native support,; but that can require many extra truncations and extensions. By default,; when emulating ``_Float16`` and ``__bf16`` arithmetic using ``float``, Clang; does not truncate intermediate operands back to their true type unless the; operand is the result of an explicit cast or assignment. This is generally; much faster but can generate different results from strict operation-by-operation; emulation. Usually the results are more precise. This is permitted by the; C and C++ standards under the rules for excess precision in intermediate operands;; see the discussion of evaluation formats in the C standard and [expr.pre] in; the C++ standard. The use of excess precision can be independently controlled for these two; types with the ``-ffloat16-excess-precision=`` and; ``-fbfloat16-excess-precision=",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:32317,perform,perform,32317,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['perform'],['perform']
Performance,"1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; sc1=1; 4. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acq_rel - agent - generic 1. buffer_wbl2 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/sto",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:321859,cache,caches,321859,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"1. Cross-process and cross-architecture linking of single relocatable objects; into a target *executor* process. 2. Support for all object format features. 3. Open linker data structures (``LinkGraph``) and pass system. JITLink and ObjectLinkingLayer; ==============================. ``ObjectLinkingLayer`` is ORCs wrapper for JITLink. It is an ORC layer that; allows objects to be added to a ``JITDylib``, or emitted from some higher level; program representation. When an object is emitted, ``ObjectLinkingLayer`` uses; JITLink to construct a ``LinkGraph`` (see :ref:`constructing_linkgraphs`) and; calls JITLink's ``link`` function to link the graph into the executor process. The ``ObjectLinkingLayer`` class provides a plugin API,; ``ObjectLinkingLayer::Plugin``, which users can subclass in order to inspect and; modify ``LinkGraph`` instances at link time, and react to important JIT events; (such as an object being emitted into target memory). This enables many features; and optimizations that were not possible under MCJIT or RuntimeDyld. ObjectLinkingLayer Plugins; --------------------------. The ``ObjectLinkingLayer::Plugin`` class provides the following methods:. * ``modifyPassConfig`` is called each time a LinkGraph is about to be linked. It; can be overridden to install JITLink *Passes* to run during the link process. .. code-block:: c++. void modifyPassConfig(MaterializationResponsibility &MR,; const Triple &TT,; jitlink::PassConfiguration &Config). * ``notifyLoaded`` is called before the link begins, and can be overridden to; set up any initial state for the given ``MaterializationResponsibility`` if; needed. .. code-block:: c++. void notifyLoaded(MaterializationResponsibility &MR). * ``notifyEmitted`` is called after the link is complete and code has been; emitted to the executor process. It can be overridden to finalize state; for the ``MaterializationResponsibility`` if needed. .. code-block:: c++. Error notifyEmitted(MaterializationResponsibility &MR). * ``noti",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst:2564,optimiz,optimizations,2564,interpreter/llvm-project/llvm/docs/JITLink.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst,1,['optimiz'],['optimizations']
Performance,"1. flat_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If CU wavefront execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1 dlc=1; - system; - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_gl*_invl.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vm/vscnt(0). - If CU wavefront execution; mode, omit.; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; followi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:348166,load,load,348166,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"1.0). If set to; non zero auto cache creation is enabled and both auto created and; default sized caches will use the configured factor: 0.0 no automatic cache; and greater than 0.0 to enable cache. This value can be overridden by the; environment variable ROOT_TTREECACHE_SIZE. The resource variable TTreeCache.Prefill sets the default TTreeCache prefilling; type. The prefill type may be: 0 for no prefilling and 1 to prefill all; the branches. It can be overridden by the environment variable ROOT_TTREECACHE_PREFILL. In particular the default can be set back to the same as in version 5 by; setting TTreeCache.Size (or ROOT_TTREECACHE_SIZE) and TTreeCache.Prefill; (or ROOT_TTREECACHE_PREFILL) both to zero. TTree methods which are expected to modify a cache, like AddBranchToCache, will; attempt to setup a cache of default size if one does not exist, irrespective of; whether the auto cache creation is enabled. Additionally several methods giving; control of the cache have changed return type from void to Int_t, to be able to; return a code to indicate if there was an error. Usually TTree::SetCacheSize will no longer reset the list of branches to be; cached (either set or previously learnt) nor restart the learning phase.; The learning phase is restarted when a new cache is created, e.g. after having; removed a cache with SetCacheSize(0). ### TSelectorDraw. The axis titles in case of a `x:y:z` plot with the option `COLZ` were not correct. ### TParallelCoordVar. Change the format used to print the variables limit for ||-Coord to `%g`. It was; `%6.4f` before. ## Histogram Libraries. ### TFormula. - New version of the TFormula class based on Cling. Formula expressions are now used to create functions which are passed to Cling to be Just In Time compiled.; The expression is therefore compiled using Clang/LLVVM which will give execution time as compiled code and in addition correctness of the result obtained.; - This class is not 100% backward compatible with the old TFormula cl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:14547,cache,cache,14547,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,1,['cache'],['cache']
Performance,"1150; http://gcc.gnu.org/bugzilla/attachment.cgi?id=8701. There is also one case we do worse on PPC. //===---------------------------------------------------------------------===//. For this:. int test(int a); {; return a * 3;; }. We currently emits; 	imull $3, 4(%esp), %eax. Perhaps this is what we really should generate is? Is imull three or four; cycles? Note: ICC generates this:; 	movl	4(%esp), %eax; 	leal	(%eax,%eax,2), %eax. The current instruction priority is based on pattern complexity. The former is; more ""complex"" because it folds a load so the latter will not be emitted. Perhaps we should use AddedComplexity to give LEA32r a higher priority? We; should always try to match LEA first since the LEA matching code does some; estimate to determine whether the match is profitable. However, if we care more about code size, then imull is better. It's two bytes; shorter than movl + leal. On a Pentium M, both variants have the same characteristics with regard; to throughput; however, the multiplication has a latency of four cycles, as; opposed to two cycles for the movl+lea variant. //===---------------------------------------------------------------------===//. It appears gcc place string data with linkonce linkage in; .section __TEXT,__const_coal,coalesced instead of; .section __DATA,__const_coal,coalesced.; Take a look at darwin.h, there are other Darwin assembler directives that we; do not make use of. //===---------------------------------------------------------------------===//. define i32 @foo(i32* %a, i32 %t) {; entry:; 	br label %cond_true. cond_true:		; preds = %cond_true, %entry; 	%x.0.0 = phi i32 [ 0, %entry ], [ %tmp9, %cond_true ]		; <i32> [#uses=3]; 	%t_addr.0.0 = phi i32 [ %t, %entry ], [ %tmp7, %cond_true ]		; <i32> [#uses=1]; 	%tmp2 = getelementptr i32* %a, i32 %x.0.0		; <i32*> [#uses=1]; 	%tmp3 = load i32* %tmp2		; <i32> [#uses=1]; 	%tmp5 = add i32 %t_addr.0.0, %x.0.0		; <i32> [#uses=1]; 	%tmp7 = add i32 %tmp5, %tmp3		; <i32> [#uses=2]; 	%tmp9 = ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:7542,throughput,throughput,7542,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,4,"['latency', 'throughput']","['latency', 'throughput']"
Performance,"16 overflow generates; +/-INF values.; - If 1, fp16 overflow that is the; result of an +/-INF input value; or divide by 0 produces a +/-INF,; otherwise clamps computed; overflow to +/-MAX_FP16 as; appropriate. Used by CP to set up; ``COMPUTE_PGM_RSRC1.FP16_OVFL``.; 28:27 2 bits Reserved, must be 0.; 29 1 bit WGP_MODE GFX6-GFX9; Reserved, must be 0.; GFX10-GFX11; - If 0 execute work-groups in; CU wavefront execution mode.; - If 1 execute work-groups on; in WGP wavefront execution mode. See :ref:`amdgpu-amdhsa-memory-model`. Used by CP to set up; ``COMPUTE_PGM_RSRC1.WGP_MODE``.; 30 1 bit MEM_ORDERED GFX6-GFX9; Reserved, must be 0.; GFX10-GFX11; Controls the behavior of the; s_waitcnt's vmcnt and vscnt; counters. - If 0 vmcnt reports completion; of load and atomic with return; out of order with sample; instructions, and the vscnt; reports the completion of; store and atomic without; return in order.; - If 1 vmcnt reports completion; of load, atomic with return; and sample instructions in; order, and the vscnt reports; the completion of store and; atomic without return in order. Used by CP to set up; ``COMPUTE_PGM_RSRC1.MEM_ORDERED``.; 31 1 bit FWD_PROGRESS GFX6-GFX9; Reserved, must be 0.; GFX10-GFX11; - If 0 execute SIMD wavefronts; using oldest first policy.; - If 1 execute SIMD wavefronts to; ensure wavefronts will make some; forward progress. Used by CP to set up; ``COMPUTE_PGM_RSRC1.FWD_PROGRESS``.; 32 **Total size 4 bytes**; ======= ===================================================================================================================. .. .. table:: compute_pgm_rsrc2 for GFX6-GFX12; :name: amdgpu-amdhsa-compute_pgm_rsrc2-gfx6-gfx12-table. ======= ======= =============================== ===========================================================================; Bits Size Field Name Description; ======= ======= =============================== ===========================================================================; 0 1 bit ENABLE_PRIVATE_SEGMENT * En",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:169789,load,load,169789,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"16 x i32> <op>, <16 x i1> <mask>, i32 <vector_length>, i1 <is_int_min_poison>); declare <vscale x 4 x i32> @llvm.vp.abs.nxv4i32 (<vscale x 4 x i32> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>, i1 <is_int_min_poison>); declare <256 x i64> @llvm.vp.abs.v256i64 (<256 x i64> <op>, <256 x i1> <mask>, i32 <vector_length>, i1 <is_int_min_poison>). Overview:; """""""""""""""""". Predicated abs of a vector of integers. Arguments:; """""""""""""""""""". The first operand and the result have the same vector of integer type. The; second operand is the vector mask and has the same number of elements as the; result vector type. The third operand is the explicit vector length of the; operation. The fourth argument must be a constant and is a flag to indicate; whether the result value of the '``llvm.vp.abs``' intrinsic is a; :ref:`poison value <poisonvalues>` if the argument is statically or dynamically; an ``INT_MIN`` value. Semantics:; """""""""""""""""""". The '``llvm.vp.abs``' intrinsic performs abs (:ref:`abs <int_abs>`) of the first operand on each; enabled lane. The result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.abs.v4i32(<4 x i32> %a, <4 x i1> %mask, i32 %evl, i1 false); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x i32> @llvm.abs.v4i32(<4 x i32> %a, i1 false); %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_smax:. '``llvm.vp.smax.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.smax.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.smax.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.smax.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_leng",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:717080,perform,performs,717080,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"160 %srcval1, i160* %1, align 4; ret void; }. This happens because SRoA sees that the temp alloca has is being memcpy'd into; and out of and it has holes and it has to be conservative. If we knew about the; holes, then this could be much much better. Having information about these holes would also improve memcpy (etc) lowering at; llc time when it gets inlined, because we can use smaller transfers. This also; avoids partial register stalls in some important cases. //===---------------------------------------------------------------------===//. We don't fold (icmp (add) (add)) unless the two adds only have a single use.; There are a lot of cases that we're refusing to fold in (e.g.) 256.bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; both sinkable to *after* the icmp (e.g. in a subsequent block) which doesn't; increase register pressure. There are a ton of icmp's we aren't simplifying because of the reg pressure; concern. Care is warranted here though because many of these are induction; variables and other cases that matter a lot to performance, like the above.; Here's a blob of code that you can drop into the bottom of visitICmp to see some; missed cases:. { Value *A, *B, *C, *D;; if (match(Op0, m_Add(m_Value(A), m_Value(B))) && ; match(Op1, m_Add(m_Value(C), m_Value(D))) &&; (A == C || A ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:64015,perform,performance,64015,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['perform'],['performance']
Performance,"17. One could use demo.htm directly with THttpServer providing address like:; <http://localhost:8080/jsrootsys/demo/demo.htm?addr=../../Files/job1.root/hpx/root.json.gz&layout=3x3>; 18. Also for online server process url options like 'item', 'items', 'layout'; 19. Possibility to generate URL, which reproduces opened page with layout and drawn items. ### August 2014; 1. All communication between server and browser done with JSON format.; 2. Fix small error in dtree.js - one should always set; last sibling (_ls) property while tree can be dynamically changed.; 3. In JSRootCore.js provide central function, which handles different kinds; of XMLHttpRequest. Use only async requests, also when getting file header.; 4. Fully reorganize data management in file/tree/directory/collection hierarchical; display. Now complete description collected in HPainter class and decoupled from; visualization, performed with dTree.js.; 5. Remove all global variables from the code.; 6. Automatic scripts/style loading handled via JSROOT.loadScript() function.; One can specify arbitrary scripts list, which asynchronously loaded by browser.; 7. Method to build simple GUI changed and more simplified :). The example in index.htm.; While loadScript and AssertPrerequisites functions moved to JSROOT, one; can easily build many different kinds of GUIs, reusing provided JSRootCore.js functions.; 8. In example.htm also use AssertPrerequisites to load necessary scripts.; This helps to keep code up-to-date even by big changes in JavaScript code.; 9. Provide monitoring of online THttpServer with similar interface as for ROOT files.; 10. Fix several errors in TKey Streamer, use member names as in ROOT itself.; 11. Keep the only version identifier JSROOT.version for JS code; 12. One can specify in JSROOT.AssertPrerequisites functionality which is required.; One could specify '2d', 'io' (default) or '3d'.; 13. Use new AssertPrerequisites functionality to load only required functionality.; 14. When displaying",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:74398,load,loading,74398,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loading']
Performance,"1; %Y = bitcast i32* %x to i16* ; yields i16*:%x; %Z = bitcast <2 x i32> %V to i64; ; yields i64: %V (depends on endianness); %Z = bitcast <2 x i32*> %V to <2 x i64*> ; yields <2 x i64*>. .. _i_addrspacecast:. '``addrspacecast .. to``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = addrspacecast <pty> <ptrval> to <pty2> ; yields pty2. Overview:; """""""""""""""""". The '``addrspacecast``' instruction converts ``ptrval`` from ``pty`` in; address space ``n`` to type ``pty2`` in address space ``m``. Arguments:; """""""""""""""""""". The '``addrspacecast``' instruction takes a pointer or vector of pointer value; to cast and a pointer type to cast it to, which must have a different; address space. Semantics:; """""""""""""""""""". The '``addrspacecast``' instruction converts the pointer value; ``ptrval`` to type ``pty2``. It can be a *no-op cast* or a complex; value modification, depending on the target and the address space; pair. Pointer conversions within the same address space must be; performed with the ``bitcast`` instruction. Note that if the address; space conversion produces a dereferenceable result then both result; and operand refer to the same memory location. The conversion must; have no side effects, and must not capture the value of the pointer. If the source is :ref:`poison <poisonvalues>`, the result is; :ref:`poison <poisonvalues>`. If the source is not :ref:`poison <poisonvalues>`, and both source and; destination are :ref:`integral pointers <nointptrtype>`, and the; result pointer is dereferenceable, the cast is assumed to be; reversible (i.e. casting the result back to the original address space; should yield the original bit pattern). Example:; """""""""""""""". .. code-block:: llvm. %X = addrspacecast ptr %x to ptr addrspace(1); %Y = addrspacecast ptr addrspace(1) %y to ptr addrspace(2); %Z = addrspacecast <4 x ptr> %z to <4 x ptr addrspace(3)>. .. _otherops:. Other Operations; ----------------. The instructions in this category are the ""miscellane",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:458412,perform,performed,458412,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"1> @llvm.vp.icmp.nxv2i32(<vscale x 2 x i32> <left_op>, <vscale x 2 x i32> <right_op>, metadata <condition code>, <vscale x 2 x i1> <mask>, i32 <vector_length>); declare <128 x i1> @llvm.vp.icmp.v128i8(<128 x i8> <left_op>, <128 x i8> <right_op>, metadata <condition code>, <128 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". The '``llvm.vp.icmp``' intrinsic returns a vector of boolean values based on; the comparison of its operands. The operation has a mask and an explicit vector; length parameter. Arguments:; """""""""""""""""""". The '``llvm.vp.icmp``' intrinsic takes the two values to compare as its first; and second operands. These two values must be vectors of :ref:`integer; <t_integer>` types.; The return type is the result of the comparison. The return type must be a; vector of :ref:`i1 <t_integer>` type. The fourth operand is the vector mask.; The return type, the values to compare, and the vector mask have the same; number of elements. The third operand is the condition code indicating the kind; of comparison to perform. It must be a metadata string with :ref:`one of the; supported integer condition code values <icmp_md_cc>`. The fifth operand is the; explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.icmp``' compares its first two operands according to the; condition code given as the third operand. The operands are compared element by; element on each enabled lane, where the semantics of the comparison are; defined :ref:`according to the condition code <icmp_md_cc_sem>`. Masked-off; lanes are ``poison``. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i1> @llvm.vp.icmp.v4i32(<4 x i32> %a, <4 x i32> %b, metadata !""ne"", <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = icmp ne <4 x i32> %a, %b; %also.r = select <4 x i1> %mask, <4 x i1> %t, <4 x i1> poison. .. _int_vp_ceil:. '``llvm.vp.ceil.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:820436,perform,perform,820436,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"1F * h2 = new TH1F(""h2"", ""h2"", 100, -3., 3.);; h1->FillRandom(""gaus"", 5000);; h2->FillRandom(""gaus"", 4000);; h1->SetMaximum(100);; h1->Draw();; h2->Draw(""same"");; }; . In some case, when a graph had some vertical parts, the exclusion; zone was not drawn correctly. The following small example shows the; problem:; ; {; TCanvas *c1 = new TCanvas();; gPad->DrawFrame(-1,-1,3,3);. TGraph * graph=new TGraph(3);; graph->SetFillColor(3);; graph->SetFillStyle(3001);; graph->SetLineWidth(2000);. graph->SetPoint(0,1.,1.);; graph->SetPoint(1,1.,0);; graph->SetPoint(2,0.,0.);; graph->Draw(""*L"");; }; . TUnfold. Add a new version. A new class TUnfoldSys provides support for the propagation of systematic errors.; Some bugs were also fixed due to multiplication of addition of sparse matrices. Fitting Methods. Introduce a better treatment of the step size used when fitting an object with a TF1. Use now by default is not zero the error provided by TF1. In case of limits use an appropriate step size to avoid Minuit to go over the limits.; Fix bug https://savannah.cern.ch/bugs/?45909 when fitting with bad range values (outside the histogram range).; detect the case when the data set is empty and don't perform any minimizationin this case but exits from fitting and produce a warning message; Fix a bug when fitting histograms with option W and the bin errors are = 0.; Fix a bug in the InitGaus function when having only one data point (see https://savannah.cern.ch/bugs/?48936); Fix a bug in calculating the error on the integral after having fitted when fix parameters were present; Fix a bug in calculating the confidence intervas when the number of bins for the given object is different from the number of bins of the fitted object.; ; FitPanel. Add support for drawing the fit function confidence levels.; Make gaus the default function when fitting 1D objects.; Add GSL minimizer and use now a new widget for showing and selecting the list of available algorithms according to the minimizer.; ; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v524/index.html:8418,perform,perform,8418,hist/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v524/index.html,2,['perform'],['perform']
Performance,1`; - `3`; - :part:`25%`; * - clang-tools-extra/clang-move/tool; - `1`; - `1`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-query; - `5`; - `4`; - `1`; - :part:`80%`; * - clang-tools-extra/clang-query/tool; - `1`; - `0`; - `1`; - :none:`0%`; * - clang-tools-extra/clang-reorder-fields; - `2`; - `1`; - `1`; - :part:`50%`; * - clang-tools-extra/clang-reorder-fields/tool; - `1`; - `0`; - `1`; - :none:`0%`; * - clang-tools-extra/clang-tidy; - `20`; - `14`; - `6`; - :part:`70%`; * - clang-tools-extra/clang-tidy/abseil; - `42`; - `31`; - `11`; - :part:`73%`; * - clang-tools-extra/clang-tidy/altera; - `11`; - `9`; - `2`; - :part:`81%`; * - clang-tools-extra/clang-tidy/android; - `33`; - `23`; - `10`; - :part:`69%`; * - clang-tools-extra/clang-tidy/boost; - `3`; - `3`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-tidy/bugprone; - `125`; - `106`; - `19`; - :part:`84%`; * - clang-tools-extra/clang-tidy/cert; - `29`; - `28`; - `1`; - :part:`96%`; * - clang-tools-extra/clang-tidy/concurrency; - `5`; - `4`; - `1`; - :part:`80%`; * - clang-tools-extra/clang-tidy/cppcoreguidelines; - `45`; - `42`; - `3`; - :part:`93%`; * - clang-tools-extra/clang-tidy/darwin; - `5`; - `2`; - `3`; - :part:`40%`; * - clang-tools-extra/clang-tidy/fuchsia; - `15`; - `10`; - `5`; - :part:`66%`; * - clang-tools-extra/clang-tidy/google; - `33`; - `22`; - `11`; - :part:`66%`; * - clang-tools-extra/clang-tidy/hicpp; - `9`; - `7`; - `2`; - :part:`77%`; * - clang-tools-extra/clang-tidy/linuxkernel; - `3`; - `2`; - `1`; - :part:`66%`; * - clang-tools-extra/clang-tidy/llvm; - `11`; - `10`; - `1`; - :part:`90%`; * - clang-tools-extra/clang-tidy/llvmlibc; - `7`; - `7`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-tidy/misc; - `33`; - `30`; - `3`; - :part:`90%`; * - clang-tools-extra/clang-tidy/modernize; - `67`; - `48`; - `19`; - :part:`71%`; * - clang-tools-extra/clang-tidy/mpi; - `5`; - `5`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-tidy/objc; - `17`; - `12`; - `5`; - :part:`70%`; * -,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangFormattedStatus.rst:17040,concurren,concurrency,17040,interpreter/llvm-project/clang/docs/ClangFormattedStatus.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangFormattedStatus.rst,1,['concurren'],['concurrency']
Performance,"1minimum.html. //===---------------------------------------------------------------------===//. Should we promote i16 to i32 to avoid partial register update stalls?. //===---------------------------------------------------------------------===//. Leave any_extend as pseudo instruction and hint to register; allocator. Delay codegen until post register allocation.; Note. any_extend is now turned into an INSERT_SUBREG. We still need to teach; the coalescer how to deal with it though. //===---------------------------------------------------------------------===//. It appears icc use push for parameter passing. Need to investigate. //===---------------------------------------------------------------------===//. The instruction selector sometimes misses folding a load into a compare. The; pattern is written as (cmp reg, (load p)). Because the compare isn't; commutative, it is not matched with the load on both sides. The dag combiner; should be made smart enough to canonicalize the load into the RHS of a compare; when it can invert the result of the compare for free. //===---------------------------------------------------------------------===//. In many cases, LLVM generates code like this:. _test:; movl 8(%esp), %eax; cmpl %eax, 4(%esp); setl %al; movzbl %al, %eax; ret. on some processors (which ones?), it is more efficient to do this:. _test:; movl 8(%esp), %ebx; xor %eax, %eax; cmpl %ebx, 4(%esp); setl %al; ret. Doing this correctly is tricky though, as the xor clobbers the flags. //===---------------------------------------------------------------------===//. We should generate bts/btr/etc instructions on targets where they are cheap or; when codesize is important. e.g., for:. void setbit(int *target, int bit) {; *target |= (1 << bit);; }; void clearbit(int *target, int bit) {; *target &= ~(1 << bit);; }. //===---------------------------------------------------------------------===//. Instead of the following for memset char*, 1, 10:. 	movl $16843009, 4(%edx); 	movl $",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:2902,load,load,2902,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,2,['load'],['load']
Performance,"2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing this with integers allows the use of read/modify/write; instructions. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; bool Alt; };; extern THotKey m_HotKey;; THotKey GetHotKey () { return m_HotKey; }. into (-m64 -O3 -fno-exceptions -static -fomit-frame-pointer):. __Z9GetHotKeyv: ## @_Z9GetHotKeyv; 	movq	_m_HotKey@GOTPCREL(%rip)",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:13212,load,load,13212,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,4,['load'],['load']
Performance,"2 = INC32r %0, implicit-def dead $eflags, debug-location !5; DBG_VALUE %1, $noreg, !3, !DIExpression(), debug-location !5; %6:gr32 = ADD32rm %4, %2, 4, killed %5, 0, $noreg, implicit-def dead $eflags :: (load 4 from %ir.addr2); %7:gr32 = SUB32rr %6, %0, implicit-def $eflags, debug-location !5; JB_1 %bb.1, implicit $eflags, debug-location !5; JMP_1 %bb.2, debug-location !5. bb.2.bb2:; %8:gr32 = MOV32r0 implicit-def dead $eflags; $eax = COPY %8, debug-location !5; RET 0, $eax, debug-location !5. Observe first that there is a DBG_VALUE instruction for every ``llvm.dbg.value``; intrinsic in the source IR, ensuring no source level assignments go missing.; Then consider the different ways in which variable locations have been recorded:. * For the first dbg.value an immediate operand is used to record a zero value.; * The dbg.value of the PHI instruction leads to a DBG_VALUE of virtual register; ``%0``.; * The first GEP has its effect folded into the first load instruction; (as a 4-byte offset), but the variable location is salvaged by folding; the GEPs effect into the DIExpression.; * The second GEP is also folded into the corresponding load. However, it is; insufficiently simple to be salvaged, and is emitted as a ``$noreg``; DBG_VALUE, indicating that the variable takes on an undefined location.; * The final dbg.value has its Value placed in virtual register ``%1``. Instruction Scheduling; ----------------------. A number of passes can reschedule instructions, notably instruction selection; and the pre-and-post RA machine schedulers. Instruction scheduling can; significantly change the nature of the program -- in the (very unlikely) worst; case the instruction sequence could be completely reversed. In such; circumstances LLVM follows the principle applied to optimizations, that it is; better for the debugger not to display any state than a misleading state.; Thus, whenever instructions are advanced in order of execution, any; corresponding DBG_VALUE is kept in its origin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:32223,load,load,32223,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['load'],['load']
Performance,"2 and; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_wbinvl1_vol.; - Ensures the atomicrmw; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - E",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:249390,load,load,249390,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"2();; virtual void f3();; };. struct B : A {; virtual void f1();; virtual void f2();; virtual void f3();; };. struct C : A {; virtual void f1();; virtual void f2();; virtual void f3();; };. The scheme will cause the virtual tables for A, B and C to be laid out; consecutively:. .. csv-table:: Virtual Table Layout for A, B, C; :header: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14. A::offset-to-top, &A::rtti, &A::f1, &A::f2, &A::f3, B::offset-to-top, &B::rtti, &B::f1, &B::f2, &B::f3, C::offset-to-top, &C::rtti, &C::f1, &C::f2, &C::f3. The bit vector for static types A, B and C will look like this:. .. csv-table:: Bit Vectors for A, B, C; :header: Class, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14. A, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0; B, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0; C, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0. Bit vectors are represented in the object file as byte arrays. By loading; from indexed offsets into the byte array and applying a mask, a program can; test bits from the bit set with a relatively short instruction sequence. Bit; vectors may overlap so long as they use different bits. For the full details,; see the `ByteArrayBuilder`_ class. In this case, assuming A is laid out at offset 0 in bit 0, B at offset 0 in; bit 1 and C at offset 0 in bit 2, the byte array would look like this:. .. code-block:: c++. char bits[] = { 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 5, 0, 0 };. To emit a virtual call, the compiler will assemble code that checks that; the object's virtual table pointer is in-bounds and aligned and that the; relevant bit is set in the bit vector. For example on x86 a typical virtual call may look like this:. .. code-block:: none. ca7fbb: 48 8b 0f mov (%rdi),%rcx; ca7fbe: 48 8d 15 c3 42 fb 07 lea 0x7fb42c3(%rip),%rdx; ca7fc5: 48 89 c8 mov %rcx,%rax; ca7fc8: 48 29 d0 sub %rdx,%rax; ca7fcb: 48 c1 c0 3d rol $0x3d,%rax; ca7fcf: 48 3d 7f 01 00 00 cmp $0x17f,%rax; ca7fd5: 0f 87 36 05 00 00 ja ca8511; ca7fdb: 48 8d 15 c0 0b f7 0",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:1736,load,loading,1736,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,1,['load'],['loading']
Performance,"2); %g2 = call i32 @gazonk(); br label %exit; exit:; %merge = phi [ %tval, %truebr ], [ %fval, %falsebr ]; %g = phi [ %g1, %truebr ], [ %g2, %falsebr ]; call @llvm.dbg.value(metadata i32 %merge, metadata !1, metadata !2); call @llvm.dbg.value(metadata i32 %g, metadata !3, metadata !2); %plusten = add i32 %merge, 10; %toret = add i32 %plusten, %g; call @llvm.dbg.value(metadata i32 %toret, metadata !1, metadata !2); ret i32 %toret; }. Containing two source-level variables in ``!1`` and ``!3``. The function could,; perhaps, be optimized into the following code:. .. code-block:: llvm. define i32 @foo(i32 %bar, i1 %cond) {; entry:; %g = call i32 @gazonk(); %addoper = select i1 %cond, i32 11, i32 12; %plusten = add i32 %bar, %addoper; %toret = add i32 %plusten, %g; ret i32 %toret; }. What ``llvm.dbg.value`` intrinsics should be placed to represent the original variable; locations in this code? Unfortunately the second, third and fourth; dbg.values for ``!1`` in the source function have had their operands; (%tval, %fval, %merge) optimized out. Assuming we cannot recover them, we; might consider this placement of dbg.values:. .. code-block:: llvm. define i32 @foo(i32 %bar, i1 %cond) {; entry:; call @llvm.dbg.value(metadata i32 0, metadata !1, metadata !2); %g = call i32 @gazonk(); call @llvm.dbg.value(metadata i32 %g, metadata !3, metadata !2); %addoper = select i1 %cond, i32 11, i32 12; %plusten = add i32 %bar, %addoper; %toret = add i32 %plusten, %g; call @llvm.dbg.value(metadata i32 %toret, metadata !1, metadata !2); ret i32 %toret; }. However, this will cause ``!3`` to have the return value of ``@gazonk()`` at; the same time as ``!1`` has the constant value zero -- a pair of assignments; that never occurred in the unoptimized program. To avoid this, we must terminate; the range that ``!1`` has the constant value assignment by inserting a poison; dbg.value before the dbg.value for ``!3``:. .. code-block:: llvm. define i32 @foo(i32 %bar, i1 %cond) {; entry:; call @llvm.dbg",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:21378,optimiz,optimized,21378,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimized']
Performance,"2**; Depends on stage2-instrumented-generate-profdata and will use the stage1; compiler with the stage2 profdata to build a PGO-optimized compiler. **stage2-check-llvm**; Depends on stage2 and runs check-llvm using the stage2 compiler. **stage2-check-clang**; Depends on stage2 and runs check-clang using the stage2 compiler. **stage2-check-all**; Depends on stage2 and runs check-all using the stage2 compiler. **stage2-test-suite**; Depends on stage2 and runs the test-suite using the stage2 compiler (requires; in-tree test-suite). BOLT; ====. `BOLT <https://github.com/llvm/llvm-project/blob/main/bolt/README.md>`_; (Binary Optimization and Layout Tool) is a tool that optimizes binaries; post-link by profiling them at runtime and then using that information to; optimize the layout of the final binary among other optimizations performed; at the binary level. There are also CMake caches available to build; LLVM/Clang with BOLT. To configure a single-stage build that builds LLVM/Clang and then optimizes; it with BOLT, use the following CMake configuration:. .. code-block:: console. $ cmake <path to source>/llvm -C <path to source>/clang/cmake/caches/BOLT.cmake. Then, build the BOLT-optimized binary by running the following ninja command:. .. code-block:: console. $ ninja clang-bolt. If you're seeing errors in the build process, try building with a recent; version of Clang/LLVM by setting the CMAKE_C_COMPILER and; CMAKE_CXX_COMPILER flags to the appropriate values. It is also possible to use BOLT on top of PGO and (Thin)LTO for an even more; significant runtime speedup. To configure a three stage PGO build with ThinLTO; that optimizes the resulting binary with BOLT, use the following CMake; configuration command:. .. code-block:: console. $ cmake -G Ninja <path to source>/llvm \; -C <path to source>/clang/cmake/caches/BOLT-PGO.cmake \; -DBOOTSTRAP_LLVM_ENABLE_LLD=ON \; -DBOOTSTRAP_BOOTSTRAP_LLVM_ENABLE_LLD=ON \; -DPGO_INSTRUMENT_LTO=Thin. Then, to build the final optimized b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:10278,optimiz,optimizes,10278,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['optimiz'],['optimizes']
Performance,"2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2787,load,load,2787,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,6,['load'],['load']
Performance,"2,0.2,0.8,0.3); root[] b->SetFillColor(5); root[] b->Draw(); ```. ![A rectangle with a border](pictures/020000AF.jpg). A **`TWbox`** is a rectangle (**`TBox`**) with a border size and a; border mode. The attributes of the outline line and of the fill area are; described in ""Graphical Objects Attributes"". ### Markers. A marker is a point with a fancy shape! The possible markers are shown; in the next figure. ![Markers](pictures/030000B0.png). The marker constructor is:. ``` {.cpp}; TMarker(Double_t x,Double_t y,Int_t marker); ```. The parameters `x` and `y` are the marker coordinates and `marker` is; the marker type, shown in the previous figure. Suppose the pointer `ma`; is a valid marker. The marker size is set via `ma->SetMarkerSize(size)`,; where `size` is the desired size. Note, that the marker types 1, 6 and 7; (the dots) cannot be scaled. They are always drawn with the same number; of pixels. `SetMarkerSize` does not apply on them. To have a ""scalable; dot"" a circle shape should be used instead, for example, the marker type; 20. The default marker type is 1, if `SetMarkerStyle` is not specified.; It is the most common one to draw scatter plots. ![Different marker sizes](pictures/030000B1.png). ![Different marker sizes](pictures/030000B2.png). The user interface for changing the marker color, style and size looks; like shown in this picture. It takes place in the editor frame anytime; the selected object inherits the class **`TAttMarker`**. Non-symmetric symbols should be used carefully in plotting. The next two; graphs show how the misleading a careless use of symbols can be. The two; plots represent the same data sets but because of a bad symbol choice,; the two on the top appear further apart from the next example. ![The use of non-symmetric markers](pictures/030000B3.png). A **`TPolyMaker`** is defined by an array on N points in a 2D space. At; each point `x[i]`, `y[i]` a marker is drawn. The list of marker types is; shown in the previous paragraph. The mark",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:36707,scalab,scalable,36707,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['scalab'],['scalable']
Performance,"2-bit; integer). ``` {.cpp}; ""ntrack/I2""; ```. With this Branch method, you can also add a leaf that holds an entire; array of variables. To add an array of floats use the `f[n]` notation; when describing the leaf. ``` {.cpp}; Float_t f[10];; tree->Branch(""fBranch"",f,""f[10]/F"");; ```. You can also add an array of variable length:. ``` {.cpp}; {; TFile *f = new TFile(""peter.root"",""recreate"");; Int_t nPhot;; Float_t E[500];; TTree* nEmcPhotons = new TTree(""nEmcPhotons"",""EMC Photons"");; nEmcPhotons->Branch(""nPhot"",&nPhot,""nPhot/I"");; nEmcPhotons->Branch(""E"",E,""E[nPhot]/F"");; }; ```. See ""Example 2: A Tree with a C Structure"" below; (`$ROOTSYS/tutorials/tree/tree2.C`) and `staff.C` at the beginning of; this chapter. ## Adding a TBranch to Hold an Object. To write a branch to hold an event object, we need to load the; definition of the `Event` class, which is in `$ROOTSYS/test/libEvent.so`; (if it doesn't exist type make in `$ROOTSYS/test`). An object can be; saved in a tree if a ROOT dictionary for its class has been generated; and loaded. ``` {.cpp}; root[] .L libEvent.so; ```. First, we need to open a file and create a tree. ``` {.cpp}; root[] TFile *f = new TFile(""AFile.root"",""RECREATE""); root[] TTree *tree = new TTree(""T"",""A Root Tree""); ```. We need to create a pointer to an `Event` object that will be used as a; reference in the `TTree::Branch` method. Then we create a branch; with the `TTree::Branch` method. ``` {.cpp}; root[] Event *event = new Event(); root[] tree->Branch(""EventBranch"",""Event"",&event,32000,99); ```. To add a branch to hold an object we use the signature above. The first; parameter is the name of the branch. The second parameter is the name of; the class of the object to be stored. The third parameter is the address; of a pointer to the object to be stored. Note that it is an address of a pointer to the object, not just a; pointer to the object. The fourth parameter is the buffer size and is by default 32000 bytes.; It is the number of bytes of d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:23406,load,loaded,23406,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['loaded']
Performance,"2.nxv8i32(<vscale x 4 x i32> %vec1, <vscale x 4 x i32> %vec2). Overview:; """""""""""""""""". The '``llvm.experimental.vector.interleave2``' intrinsic constructs a vector; by interleaving two input vectors. This intrinsic works for both fixed and scalable vectors. While this intrinsic; supports all vector types the recommended way to express this operation for; fixed-width vectors is still to use a shufflevector, as that may allow for more; optimization opportunities. For example:. .. code-block:: text. <4 x i64> llvm.experimental.vector.interleave2.v4i64(<2 x i64> <i64 0, i64 2>, <2 x i64> <i64 1, i64 3>); ==> <4 x i64> <i64 0, i64 1, i64 2, i64 3>. Arguments:; """"""""""""""""""""; Both arguments must be vectors of the same type whereby their logical; concatenation matches the result type. '``llvm.experimental.cttz.elts``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ```llvm.experimental.cttz.elts```; on any vector of integer elements, both fixed width and scalable. ::. declare i8 @llvm.experimental.cttz.elts.i8.v8i1(<8 x i1> <src>, i1 <is_zero_poison>). Overview:; """""""""""""""""". The '``llvm.experimental.cttz.elts``' intrinsic counts the number of trailing; zero elements of a vector. Arguments:; """""""""""""""""""". The first argument is the vector to be counted. This argument must be a vector; with integer element type. The return type must also be an integer type which is; wide enough to hold the maximum number of elements of the source vector. The; behaviour of this intrinsic is undefined if the return type is not wide enough; for the number of elements in the input vector. The second argument is a constant flag that indicates whether the intrinsic; returns a valid result if the first argument is all zero. If the first argument; is all zero and the second argument is true, the result is poison. Semantics:; """""""""""""""""""". The '``llvm.experimental.cttz.elts``' intrinsic counts the trailing (least; significant) zero elements ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:669328,scalab,scalable,669328,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"20 : Total Size= 582 bytes File Size = 92 *; *Baskets : 1 : Basket Size= 16000 bytes Compression= 1.00 *. Add a new function TBranch::SetStatus It is much faster to call this function in case of a Tree with many branches; instead of calling TTree::SetBranchStatus.; Implement TTreeCache::Print that shows information like:; // ******TreeCache statistics for file: cms2.root ******; // Number of branches in the cache ...: 1093; // Cache Efficiency ..................: 0.997372; // Cache Efficiency Rel...............: 1.000000; // Learn entries......................: 100; // Reading............................: 72761843 bytes in 7 transactions; // Readahead..........................: 256000 bytes with overhead = 0 bytes; // Average transaction................: 10394.549000 Kbytes; // Number of blocks in current cache..: 210, total size: 6280352; This function can be called directly from TTree: T->PrintCacheStats();. Add support for variable size array of object in a TTree (when the owner of the array is split.); And many other bug fixes, security fixes, thread safety and performance improvements ; see the svn log for details. TTree Scan and Draw. Insured that the generated histogram as an integral bin width when plotting a string or integer.; Improved the output of TTree::Scan by inserting a blank space whenever a value is not available because there is no proper row in a friend.; (Previously it was re-printing the previous value). This required changes in ; When the draw option to TTree::Draw contains ""norm"" the output histogram is normalized to 1.; Improve the selection of the leaf used for size of an array in a leaflist by giving preference; for the leaf inside the same branch and by adding support for explicit full path name. For example the following now works properly:; tree->Branch(""JET1"", &JET1, ""njets/I:et[njets]/F:pt[njets]/F"");; tree->BranchBranch(""JET2"", &JET2, ""njets/I:et[njets]/F:pt[njets]/F"");; ...; tree->Scan(""njets/I:et[JETS1.njets]/F:pt[JETS1.njets]"");. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v528/index.html:6458,perform,performance,6458,tree/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v528/index.html,2,['perform'],['performance']
Performance,"2> @llvm.vp.sext.v16i32.v16i16 (<16 x i16> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.sext.nxv4i32.nxv4i16 (<vscale x 4 x i16> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". The '``llvm.vp.sext``' intrinsic sign extends its first operand to the return; type. The operation has a mask and an explicit vector length parameter. Arguments:; """""""""""""""""""". The '``llvm.vp.sext``' intrinsic takes a value to cast as its first operand.; The return type is the type to cast the value to. Both types must be vectors of; :ref:`integer <t_integer>` type. The bit size of the value must be smaller than; the bit size of the return type. The second operand is the vector mask. The; return type, the value to cast, and the vector mask have the same number of; elements. The third operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.sext``' intrinsic performs a sign extension by copying the sign; bit (highest order bit) of the value until it reaches the size of the return; type. When sign extending from i1, the result will always be either -1 or 0.; The conversion is performed on lane positions below the explicit vector length; and where the vector mask is true. Masked-off lanes are ``poison``. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.sext.v4i32.v4i16(<4 x i16> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = sext <4 x i16> %a to <4 x i32>; %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_fptrunc:. '``llvm.vp.fptrunc.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fptrunc.v16f32.v16f64 (<16 x double> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.trunc.nxv4f32.nxv4f64 (<vscale x 4 x double> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>). Overv",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:800378,perform,performs,800378,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"2^32 bytes and the base is; aligned to 2^32 which makes it easier to convert from flat to segment or; segment to flat. A global address space address has the same value when used as a flat address; so no conversion is needed. **Global and Constant**; The global and constant address spaces both use global virtual addresses,; which are the same virtual address space used by the CPU. However, some; virtual addresses may only be accessible to the CPU, some only accessible; by the GPU, and some by both. Using the constant address space indicates that the data will not change; during the execution of the kernel. This allows scalar read instructions to; be used. As the constant address space could only be modified on the host; side, a generic pointer loaded from the constant address space is safe to be; assumed as a global pointer since only the device global memory is visible; and managed on the host side. The vector and scalar L1 caches are invalidated; of volatile data before each kernel dispatch execution to allow constant; memory to change values between kernel dispatches. **Region**; The region address space uses the hardware Global Data Store (GDS). All; wavefronts executing on the same device will access the same memory for any; given region address. However, the same region address accessed by wavefronts; executing on different devices will access different memory. It is higher; performance than global memory. It is allocated by the runtime. The data; store (DS) instructions can be used to access it. **Local**; The local address space uses the hardware Local Data Store (LDS) which is; automatically allocated when the hardware creates the wavefronts of a; work-group, and freed when all the wavefronts of a work-group have; terminated. All wavefronts belonging to the same work-group will access the; same memory for any given local address. However, the same local address; accessed by wavefronts belonging to different work-groups will access; different memory. It is h",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:25339,cache,caches,25339,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"3), !dbg !19; ; [debug line = 5:9] [debug variable = Z]. The third intrinsic ``%llvm.dbg.declare`` encodes debugging information for; variable ``Z``. The metadata ``!dbg !19`` attached to the intrinsic provides; scope information for the variable ``Z``. .. code-block:: text. !18 = distinct !DILexicalBlock(scope: !4, file: !1, line: 4, column: 5); !19 = !DILocation(line: 5, column: 11, scope: !18). Here ``!19`` indicates that ``Z`` is declared at line number 5 and column; number 11 inside of lexical scope ``!18``. The lexical scope itself resides; inside of subprogram ``!4`` described above. The scope information attached with each instruction provides a straightforward; way to find instructions covered by a scope. Object lifetime in optimized code; =================================. In the example above, every variable assignment uniquely corresponds to a; memory store to the variable's position on the stack. However in heavily; optimized code LLVM promotes most variables into SSA values, which can; eventually be placed in physical registers or memory locations. To track SSA; values through compilation, when objects are promoted to SSA values an; ``llvm.dbg.value`` intrinsic is created for each assignment, recording the; variable's new location. Compared with the ``llvm.dbg.declare`` intrinsic:. * A dbg.value terminates the effect of any preceding dbg.values for (any; overlapping fragments of) the specified variable.; * The dbg.value's position in the IR defines where in the instruction stream; the variable's value changes.; * Operands can be constants, indicating the variable is assigned a; constant value. Care must be taken to update ``llvm.dbg.value`` intrinsics when optimization; passes alter or move instructions and blocks -- the developer could observe such; changes reflected in the value of variables when debugging the program. For any; execution of the optimized program, the set of variable values presented to the; developer by the debugger should not show a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:18012,optimiz,optimized,18012,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimized']
Performance,"30. Several improvements for touch devices or devices with small displays; 31. Remove settings.FrameNDC, use Style.fPadLeft/Right/Top/BottomMargin values instead; 32. Fix - rescan sumw2 when update TH1; 33. Fix - correct placing for TLegend header; 34. Fix - correctly align sub/super scripts in complex TLatex; 35. Fix - correctly set visibility level for geo drawing (#258); 36. Fix - use more factor for number of nodes in geo drawing (#258). ## Changes in 7.3.4; 1. Fix - failure in normal_cdf calculation; 2. Fix - check in TTree::Draw for null buffer; 3. Fix - do not rise exception in treeProcess; 4. Fix - RH1 zero line drawing only when required; 5. Fix - do not allow move float browser too far left/top. ## Changes in 7.3.2; 1. Fix - undefined graph in TGraphPainter; 2. Fix - error in showing info in the geo painter; 3. Fix - stack limitation with Math.min.apply in tree draw. ## Changes in 7.3.1; 1. Fix - TGeo update in the TWebCanvas; 2. Fix - several tutorials with three.js modules loading; 3. Fix - redraw pad when change text align attributes; 4. Fix - pad ranges for TWebCanvas, handle log2 scales; 5. Fix - support candle and violin options when creating string draw option; 6. Fix - labels and tooltips on reversed axes; 7. Fix - zooming on TRatioPlot; 8. Fix - pad ranges calculations for TWebCanvas; 9. Fix - set proper background for geo drawing. ## Changes in 7.3.0; 1. Mark methods returning `Promise` as **async**; 2. Upgrade three.js to r146; 3. Fix several bugs in `csg.mjs`, improve geometry clipping; 4. Provide `settings.PreferSavedPoints` to exclude function evaluation when there are saved points; 5. Add more interactive features with `TWebCanvas`; 6. 3-dimensional `TTree::Draw()` now produces `TPolyMarker3D` by default; 7. Force MathJax rendering when `\` symbol is found (#243); 8. Support `TButton` class; 9. Remove `localfile` url option, only interactively one can open file selection dialog; 10. Fix - show correct bin index in `TH2` tooltips; 11. Fix - i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:15041,load,loading,15041,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loading']
Performance,"386.c) has a ton of interesting; simplifications for integer ""x cmp y ? a : b"". //===---------------------------------------------------------------------===//. Consider the expansion of:. define i32 @test3(i32 %X) {; %tmp1 = urem i32 %X, 255; ret i32 %tmp1; }. Currently it compiles to:. ...; movl $2155905153, %ecx; movl 8(%esp), %esi; movl %esi, %eax; mull %ecx; ... This could be ""reassociated"" into:. movl $2155905153, %eax; movl 8(%esp), %ecx; mull %ecx. to avoid the copy. In fact, the existing two-address stuff would do this; except that mul isn't a commutative 2-addr instruction. I guess this has; to be done at isel time based on the #uses to mul?. //===---------------------------------------------------------------------===//. Make sure the instruction which starts a loop does not cross a cacheline; boundary. This requires knowning the exact length of each machine instruction.; That is somewhat complicated, but doable. Example 256.bzip2:. In the new trace, the hot loop has an instruction which crosses a cacheline; boundary. In addition to potential cache misses, this can't help decoding as I; imagine there has to be some kind of complicated decoder reset and realignment; to grab the bytes from the next cacheline. 532 532 0x3cfc movb (1809(%esp, %esi), %bl <<<--- spans 2 64 byte lines; 942 942 0x3d03 movl %dh, (1809(%esp, %esi); 937 937 0x3d0a incl %esi; 3 3 0x3d0b cmpb %bl, %dl; 27 27 0x3d0d jnz 0x000062db <main+11707>. //===---------------------------------------------------------------------===//. In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===----------------------------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:10741,cache,cacheline,10741,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,2,['cache'],['cacheline']
Performance,3`; - :part:`93%`; * - clang-tools-extra/clang-tidy/darwin; - `5`; - `2`; - `3`; - :part:`40%`; * - clang-tools-extra/clang-tidy/fuchsia; - `15`; - `10`; - `5`; - :part:`66%`; * - clang-tools-extra/clang-tidy/google; - `33`; - `22`; - `11`; - :part:`66%`; * - clang-tools-extra/clang-tidy/hicpp; - `9`; - `7`; - `2`; - :part:`77%`; * - clang-tools-extra/clang-tidy/linuxkernel; - `3`; - `2`; - `1`; - :part:`66%`; * - clang-tools-extra/clang-tidy/llvm; - `11`; - `10`; - `1`; - :part:`90%`; * - clang-tools-extra/clang-tidy/llvmlibc; - `7`; - `7`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-tidy/misc; - `33`; - `30`; - `3`; - :part:`90%`; * - clang-tools-extra/clang-tidy/modernize; - `67`; - `48`; - `19`; - :part:`71%`; * - clang-tools-extra/clang-tidy/mpi; - `5`; - `5`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-tidy/objc; - `17`; - `12`; - `5`; - :part:`70%`; * - clang-tools-extra/clang-tidy/openmp; - `5`; - `5`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-tidy/performance; - `31`; - `24`; - `7`; - :part:`77%`; * - clang-tools-extra/clang-tidy/plugin; - `1`; - `1`; - `0`; - :good:`100%`; * - clang-tools-extra/clang-tidy/portability; - `5`; - `3`; - `2`; - :part:`60%`; * - clang-tools-extra/clang-tidy/readability; - `88`; - `76`; - `12`; - :part:`86%`; * - clang-tools-extra/clang-tidy/tool; - `3`; - `2`; - `1`; - :part:`66%`; * - clang-tools-extra/clang-tidy/utils; - `35`; - `31`; - `4`; - :part:`88%`; * - clang-tools-extra/clang-tidy/zircon; - `3`; - `3`; - `0`; - :good:`100%`; * - clang-tools-extra/clangd; - `97`; - `81`; - `16`; - :part:`83%`; * - clang-tools-extra/clangd/benchmarks; - `1`; - `1`; - `0`; - :good:`100%`; * - clang-tools-extra/clangd/benchmarks/CompletionModel; - `1`; - `0`; - `1`; - :none:`0%`; * - clang-tools-extra/clangd/fuzzer; - `2`; - `2`; - `0`; - :good:`100%`; * - clang-tools-extra/clangd/index; - `39`; - `36`; - `3`; - :part:`92%`; * - clang-tools-extra/clangd/index/dex; - `9`; - `7`; - `2`; - :part:`77%`; * - clang-tools-,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangFormattedStatus.rst:18154,perform,performance,18154,interpreter/llvm-project/clang/docs/ClangFormattedStatus.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangFormattedStatus.rst,1,['perform'],['performance']
Performance,"3e3d81e041a804481df228fe0081c) to the `RDataFrame` interface, which allows to get useful information, e.g. the columns and their types.; - Add [`DescribeDataset`](https://root.cern/doc/master/classROOT_1_1RDF_1_1RInterface.html#a1bc5b86a2a834bb06711fb535451146d) to the `RDataFrame` interface, which allows to get information about the dataset (subset of the output of Describe()).; - Add [`DefinePerSample`](https://root.cern/doc/master/classROOT_1_1RDF_1_1RInterface.html#a29d77593e95c0f84e359a802e6836a0e), a method which makes it possible to define columns based on the sample and entry range being processed. It is also a useful way to register callbacks that should only be called when the input dataset/TTree changes.; - Add [`HistoND`](https://root.cern/doc/master/classROOT_1_1RDF_1_1RInterface.html#a0c9956a0f48c26f8e4294e17376c7fea) action that fills a N-dimensional histogram.; - `Book` now supports just-in-time compilation, i.e. it can be called without passing the column types as template parameters (with some performance penalty, as usual).; - As an aid to `RDataSource` implementations with which collection sizes can be retrieved more efficiently than the full collection, `#var` can now be used as a short-hand notation for column name `R_rdf_sizeof_var`.; - Helpers have been added to export data from `RDataFrame` to RooFit datasets. See the ""RooFit Libraries"" section below for more details, or see [the tutorial](https://root.cern/doc/master/rf408__RDataFrameToRooFit_8C.html). ### Notable changes in behavior. - Using `Alias`, it is now possible to register homonymous aliases (alternative column names) in different branches of the computation graph, in line with the behavior of `Define` (until now, aliases were required to be unique in the whole computaton graph).; - The `Histo*D` methods now support the combination of scalar values and vector-like weight values. For each entry, the histogram is filled once for each weight, always with the same scalar value.; - The ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v626/index.md:8497,perform,performance,8497,README/ReleaseNotes/v626/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v626/index.md,1,['perform'],['performance']
Performance,"4 (or 5) (1.5 x 1.5 = 2.25). ; Saturation; %res = call i4 @llvm.umul.fix.sat.i4(i4 8, i4 2, i32 0) ; %res = 15 (8 x 2 -> clamped to 15); %res = call i4 @llvm.umul.fix.sat.i4(i4 8, i4 8, i32 2) ; %res = 15 (2 x 2 -> clamped to 3.75). ; Scale can affect the saturation result; %res = call i4 @llvm.umul.fix.sat.i4(i4 2, i4 4, i32 0) ; %res = 7 (2 x 4 -> clamped to 7); %res = call i4 @llvm.umul.fix.sat.i4(i4 2, i4 4, i32 1) ; %res = 4 (1 x 2 = 2). '``llvm.sdiv.fix.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.sdiv.fix``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.sdiv.fix.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.sdiv.fix.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.sdiv.fix.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.sdiv.fix.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.sdiv.fix``' family of intrinsic functions perform signed; fixed point division on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. The arguments may also work with; int vectors of the same length and int size. ``%a`` and ``%b`` are the two; values that will undergo signed fixed point division. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs fixed point division on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. It is undefined behavior if the result value does not fit within the range of; the fixed point type, or if the second argument is zero. Examples; """""""""""""""""". .. co",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:630442,perform,perform,630442,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"4 x float> %t, <4 x float> poison. .. _int_vp_roundtozero:. '``llvm.vp.roundtozero.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.roundtozero.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.roundtozero.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.roundtozero.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point round-to-zero of a vector of floating-point values. Arguments:; """""""""""""""""""". The first operand and the result have the same vector of floating-point type.; The second operand is the vector mask and has the same number of elements as the; result vector type. The third operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.roundtozero``' intrinsic performs floating-point roundeven; (:ref:`llvm.trunc <int_llvm_trunc>`) of the first vector operand on each enabled lane. The; result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.roundtozero.v4f32(<4 x float> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.trunc.v4f32(<4 x float> %a); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_bitreverse:. '``llvm.vp.bitreverse.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.bitreverse.v16i32 (<16 x i32> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.bitreverse.nxv4i32 (<vscale x 4 x i32> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.bitreverse.v256i64 (<256 x i64> <op>, <256 x i1> <mask>, i32 <v",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:830785,perform,performs,830785,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"4* only up to 4 32-bit floating-point parameters,; 4 64-bit floating-point parameters, and 10 bit type parameters; are supported.; - *RISCV64* only supports up to 11 bit type parameters, 4; 32-bit floating-point parameters, and 4 64-bit floating-point; parameters. This calling convention supports `tail call; optimization <CodeGenerator.html#tail-call-optimization>`_ but requires; both the caller and callee are using it.; ""``cc 11``"" - The HiPE calling convention; This calling convention has been implemented specifically for use by; the `High-Performance Erlang; (HiPE) <http://www.it.uu.se/research/group/hipe/>`_ compiler, *the*; native code compiler of the `Ericsson's Open Source Erlang/OTP; system <http://www.erlang.org/download.shtml>`_. It uses more; registers for argument passing than the ordinary C calling; convention and defines no callee-saved registers. The calling; convention properly supports `tail call; optimization <CodeGenerator.html#tail-call-optimization>`_ but requires; that both the caller and the callee use it. It uses a *register pinning*; mechanism, similar to GHC's convention, for keeping frequently; accessed runtime components pinned to specific hardware registers.; At the moment only X86 supports this convention (both 32 and 64; bit).; ""``anyregcc``"" - Dynamic calling convention for code patching; This is a special convention that supports patching an arbitrary code; sequence in place of a call site. This convention forces the call; arguments into registers but allows them to be dynamically; allocated. This can currently only be used with calls to; llvm.experimental.patchpoint because only this intrinsic records; the location of its arguments in a side table. See :doc:`StackMaps`.; ""``preserve_mostcc``"" - The `PreserveMost` calling convention; This calling convention attempts to make the code in the caller as; unintrusive as possible. This convention behaves identically to the `C`; calling convention on how arguments and return values are pass",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:15401,optimiz,optimization,15401,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"4.txt; CODE test.elf 0x4004a0; DATA inlined.elf 0x601028. $ llvm-symbolizer < addr4.txt; main; /tmp/test.cpp:15:0. bar; 6295592 4. Example 6 - path-style options:. This example uses the same source file as above, but the source file's; full path is /tmp/foo/test.cpp and is compiled as follows. The first case; shows the default absolute path, the second --basenames, and the third; shows --relativenames. .. code-block:: console. $ pwd; /tmp; $ clang -g foo/test.cpp -o test.elf; $ llvm-symbolizer --obj=test.elf 0x4004a0; main; /tmp/foo/test.cpp:15:0; $ llvm-symbolizer --obj=test.elf 0x4004a0 --basenames; main; test.cpp:15:0; $ llvm-symbolizer --obj=test.elf 0x4004a0 --relativenames; main; foo/test.cpp:15:0. Example 7 - Addresses as symbol names:. .. code-block:: console. $ llvm-symbolizer --obj=test.elf main; main; /tmp/test.cpp:14:0; $ llvm-symbolizer --obj=test.elf ""CODE foz""; foz; /tmp/test.h:1:0. OPTIONS; -------. .. option:: --adjust-vma <offset>. Add the specified offset to object file addresses when performing lookups.; This can be used to perform lookups as if the object were relocated by the; offset. .. option:: --basenames, -s. Print just the file's name without any directories, instead of the; absolute path. .. option:: --build-id. Look up the object using the given build ID, specified as a hexadecimal; string. Mutually exclusive with :option:`--obj`. .. option:: --color [=<always|auto|never>]. Specify whether to use color in :option:`--filter-markup` mode. Defaults to; ``auto``, which detects whether standard output supports color. Specifying; ``--color`` alone is equivalent to ``--color=always``. .. option:: --debug-file-directory <path>. Provide a path to a directory with a `.build-id` subdirectory to search for; debug information for stripped binaries. Multiple instances of this argument; are searched in the order given. .. option:: --debuginfod, --no-debuginfod. Whether or not to try debuginfod lookups for debug binaries. Unless specified,; debuginfod is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-symbolizer.rst:4874,perform,performing,4874,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-symbolizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-symbolizer.rst,1,['perform'],['performing']
Performance,"483648 by -1. If the ``exact`` keyword is present, the result value of the ``sdiv`` is; a :ref:`poison value <poisonvalues>` if the result would be rounded. Example:; """""""""""""""". .. code-block:: text. <result> = sdiv i32 4, %var ; yields i32:result = 4 / %var. .. _i_fdiv:. '``fdiv``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = fdiv [fast-math flags]* <ty> <op1>, <op2> ; yields ty:result. Overview:; """""""""""""""""". The '``fdiv``' instruction returns the quotient of its two operands. Arguments:; """""""""""""""""""". The two arguments to the '``fdiv``' instruction must be; :ref:`floating-point <t_floating>` or :ref:`vector <t_vector>` of; floating-point values. Both arguments must have identical types. Semantics:; """""""""""""""""""". The value produced is the floating-point quotient of the two operands.; This instruction is assumed to execute in the default :ref:`floating-point; environment <floatenv>`.; This instruction can also take any number of :ref:`fast-math; flags <fastmath>`, which are optimization hints to enable otherwise; unsafe floating-point optimizations:. Example:; """""""""""""""". .. code-block:: text. <result> = fdiv float 4.0, %var ; yields float:result = 4.0 / %var. .. _i_urem:. '``urem``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = urem <ty> <op1>, <op2> ; yields ty:result. Overview:; """""""""""""""""". The '``urem``' instruction returns the remainder from the unsigned; division of its two arguments. Arguments:; """""""""""""""""""". The two arguments to the '``urem``' instruction must be; :ref:`integer <t_integer>` or :ref:`vector <t_vector>` of integer values. Both; arguments must have identical types. Semantics:; """""""""""""""""""". This instruction returns the unsigned integer *remainder* of a division.; This instruction always performs an unsigned division to get the; remainder. Note that unsigned integer remainder and signed integer remainder are; distinct operations; for signed integer remainder, use '``srem``'. Taking the remainder of a division ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:387572,optimiz,optimization,387572,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"4:$XT, (fminnum f64:$XA, f64:$XB)). . xsmaxjdp xsminjdp; (set f64:$XT, (int_ppc_vsx_xsmaxjdp f64:$XA, f64:$XB)); (set f64:$XT, (int_ppc_vsx_xsminjdp f64:$XA, f64:$XB)). - Vector Byte-Reverse H/W/D/Q Word: xxbrh xxbrw xxbrd xxbrq; . Use intrinsic; (set v8i16:$XT, (int_ppc_vsx_xxbrh v8i16:$XB)); (set v4i32:$XT, (int_ppc_vsx_xxbrw v4i32:$XB)); (set v2i64:$XT, (int_ppc_vsx_xxbrd v2i64:$XB)); (set v1i128:$XT, (int_ppc_vsx_xxbrq v1i128:$XB)). - Vector Permute: xxperm xxpermr; . I have checked ""PPCxxswapd"" in PPCInstrVSX.td, but they are different; . Use intrinsic; (set v16i8:$XT, (int_ppc_vsx_xxperm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17365,load,load,17365,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,2,['load'],['load']
Performance,"4; Total number of mappings created: 0; Max number of mappings used: 0. If we look at the *Dynamic Dispatch Stall Cycles* table, we see the counter for; SCHEDQ reports 272 cycles. This counter is incremented every time the dispatch; logic is unable to dispatch a full group because the scheduler's queue is full. Looking at the *Dispatch Logic* table, we see that the pipeline was only able to; dispatch two micro opcodes 51.5% of the time. The dispatch group was limited to; one micro opcode 44.6% of the cycles, which corresponds to 272 cycles. The; dispatch statistics are displayed by either using the command option; ``-all-stats`` or ``-dispatch-stats``. The next table, *Schedulers*, presents a histogram displaying a count,; representing the number of micro opcodes issued on some number of cycles. In; this case, of the 610 simulated cycles, single opcodes were issued 306 times; (50.2%) and there were 7 cycles where no opcodes were issued. The *Scheduler's queue usage* table shows that the average and maximum number of; buffer entries (i.e., scheduler queue entries) used at runtime. Resource JFPU01; reached its maximum (18 of 18 queue entries). Note that AMD Jaguar implements; three schedulers:. * JALU01 - A scheduler for ALU instructions.; * JFPU01 - A scheduler floating point operations.; * JLSAGU - A scheduler for address generation. The dot-product is a kernel of three floating point instructions (a vector; multiply followed by two horizontal adds). That explains why only the floating; point scheduler appears to be used. A full scheduler queue is either caused by data dependency chains or by a; sub-optimal usage of hardware resources. Sometimes, resource pressure can be; mitigated by rewriting the kernel using different instructions that consume; different scheduler resources. Schedulers with a small queue are less resilient; to bottlenecks caused by the presence of long data dependencies. The scheduler; statistics are displayed by using the command option ``-all-st",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:31313,queue,queue,31313,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['queue'],['queue']
Performance,"5]; | 0x........ | HashData[6]; | 0x........ | HashData[7]; | 0x........ | HashData[8]; | 0x00000000 | String offset into .debug_str (terminate data for hash); `------------'. So we still have all of the same data, we just organize it more efficiently for; debugger lookup. If we repeat the same ""``printf``"" lookup from above, we; would hash ""``printf``"" and find it matches ``BUCKETS[3]`` by taking the 32 bit; hash value and modulo it by ``n_buckets``. ``BUCKETS[3]`` contains ""6"" which; is the index into the ``HASHES`` table. We would then compare any consecutive; 32 bit hashes values in the ``HASHES`` array as long as the hashes would be in; ``BUCKETS[3]``. We do this by verifying that each subsequent hash value modulo; ``n_buckets`` is still 3. In the case of a failed lookup we would access the; memory for ``BUCKETS[3]``, and then compare a few consecutive 32 bit hashes; before we know that we have no match. We don't end up marching through; multiple words of memory and we really keep the number of processor data cache; lines being accessed as small as possible. The string hash that is used for these lookup tables is the Daniel J.; Bernstein hash which is also used in the ELF ``GNU_HASH`` sections. It is a; very good hash for all kinds of names in programs with very few hash; collisions. Empty buckets are designated by using an invalid hash index of ``UINT32_MAX``. Details; ^^^^^^^. These name hash tables are designed to be generic where specializations of the; table get to define additional data that goes into the header (""``HeaderData``""),; how the string value is stored (""``KeyType``"") and the content of the data for each; hash value. Header Layout; """""""""""""""""""""""""". The header has a fixed part, and the specialized part. The exact format of the; header is:. .. code-block:: c. struct Header; {; uint32_t magic; // 'HASH' magic value to allow endian detection; uint16_t version; // Version number; uint16_t hash_function; // The hash function enumeration that was used; u",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:67896,cache,cache,67896,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['cache'],['cache']
Performance,"5][003] {Code} 'movl	-0x4(%rbp), %eax'; [0x0000000038][003] {Code} 'popq	%rbp'; [0x0000000039][003] {Code} 'retq'; [0x000000003a][003] 9 {Line} {NewStatement} {EndSequence} '/test.cpp'. -----------------------------; Element Total Printed; -----------------------------; Scopes 3 3; Symbols 4 4; Types 5 5; Lines 25 25; -----------------------------; Total 37 37. Scope Sizes:; 189 (100.00%) : [0x000000000b][001] {CompileUnit} 'test.cpp'; 110 ( 58.20%) : [0x000000002a][002] 2 {Function} extern not_inlined 'foo' -> [0x0000000099]'int'; 27 ( 14.29%) : [0x0000000071][003] {Block}. Totals by lexical level:; [001]: 189 (100.00%); [002]: 110 ( 58.20%); [003]: 27 ( 14.29%). The **Scope Sizes** table shows the contribution in bytes to the debug; information by each scope, which can be used to determine unexpected; size changes in the DWARF sections between different versions of the; same toolchain. .. code-block:: none. [0x000000002a][002] 2 {Function} extern not_inlined 'foo' -> [0x0000000099]'int'; [0x000000002a][003] {Range} Lines 2:9 [0x0000000000:0x000000003a]; [0x000000002a][003] {Linkage} 0x2 '_Z3fooPKijb'; [0x0000000071][003] {Block}; [0x0000000071][004] {Range} Lines 5:8 [0x000000001c:0x000000002f]; [0x000000007e][004] 5 {Variable} 'CONSTANT' -> [0x00000000c3]'const INTEGER'; [0x000000007e][005] {Coverage} 100.00%; [0x000000007f][005] {Location}; [0x000000007f][006] {Entry} Stack Offset: -28 (0xffffffffffffffe4) [DW_OP_fbreg]. The **{Range}** attribute describe the line ranges for a logical scope.; For this case, the function **foo** is within the lines **2** and **9**. The **{Coverage}** and **{Location}** attributes describe the debug; location and coverage for logical symbols. For optimized code, the; coverage value decreases and it affects the program debuggability. EXIT STATUS; -----------; :program:`llvm-debuginfo-analyzer` returns 0 if the input files were; parsed and printed successfully. Otherwise, it returns 1. SEE ALSO; --------; :manpage:`llvm-dwarfdump`; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-debuginfo-analyzer.rst:59798,optimiz,optimized,59798,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-debuginfo-analyzer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-debuginfo-analyzer.rst,1,['optimiz'],['optimized']
Performance,"6 x float>, align 16; 	store <16 x float> %A, <16 x float>* %tmp; 	%s = bitcast <16 x float>* %tmp to i8*; 	%s2 = bitcast <16 x float>* %tmp2 to i8*; 	call void @llvm.memcpy.i64(i8* %s, i8* %s2, i64 64, i32 16); 	%R = load <16 x float>* %tmp2; 	ret <16 x float> %R; }. declare void @llvm.memcpy.i64(i8* nocapture, i8* nocapture, i64, i32) nounwind. which compiles to:. _foo:; 	subl	$140, %esp; 	movaps	%xmm3, 112(%esp); 	movaps	%xmm2, 96(%esp); 	movaps	%xmm1, 80(%esp); 	movaps	%xmm0, 64(%esp); 	movl	60(%esp), %eax; 	movl	%eax, 124(%esp); 	movl	56(%esp), %eax; 	movl	%eax, 120(%esp); 	movl	52(%esp), %eax; <many many more 32-bit copies>; 	movaps	(%esp), %xmm0; 	movaps	16(%esp), %xmm1; 	movaps	32(%esp), %xmm2; 	movaps	48(%esp), %xmm3; 	addl	$140, %esp; 	ret. On Nehalem, it may even be cheaper to just use movups when unaligned than to; fall back to lower-granularity chunks. //===---------------------------------------------------------------------===//. Implement processor-specific optimizations for parity with GCC on these; processors. GCC does two optimizations:. 1. ix86_pad_returns inserts a noop before ret instructions if immediately; preceded by a conditional branch or is the target of a jump.; 2. ix86_avoid_jump_misspredicts inserts noops in cases where a 16-byte block of; code contains more than 3 branches.; ; The first one is done for all AMDs, Core2, and ""Generic""; The second one is done for: Atom, Pentium Pro, all AMDs, Pentium 4, Nocona,; Core 2, and ""Generic"". //===---------------------------------------------------------------------===//; Testcase:; int x(int a) { return (a&0xf0)>>4; }. Current output:; 	movl	4(%esp), %eax; 	shrl	$4, %eax; 	andl	$15, %eax; 	ret. Ideal output:; 	movzbl	4(%esp), %eax; 	shrl	$4, %eax; 	ret. //===---------------------------------------------------------------------===//. Re-implement atomic builtins __sync_add_and_fetch() and __sync_sub_and_fetch; properly. When the return value is not used (i.e. only care about the value in the; mem",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:32762,optimiz,optimizations,32762,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,2,['optimiz'],['optimizations']
Performance,"6 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.frem.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.frem.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point remainder of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.frem``' intrinsic performs floating-point remainder (:ref:`frem <i_frem>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.frem.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = frem <4 x float> %a, %b; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fneg:. '``llvm.vp.fneg.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fneg.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fneg.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fneg.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point negation of a vector of floating-point values. Arguments:; """""""""""""""""""". The first operand and the result have the same vector",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:739803,perform,performed,739803,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"6/7),; reduction will be performed using default iterative strategy.; Intrinsic is currently only implemented for i32. llvm.amdgcn.wave.reduce.umax Performs an arithmetic unsigned max reduction on the unsigned values; provided by each lane in the wavefront.; Intrinsic takes a hint for reduction strategy using second operand; 0: Target default preference,; 1: `Iterative strategy`, and; 2: `DPP`.; If target does not support the DPP operations (e.g. gfx6/7),; reduction will be performed using default iterative strategy.; Intrinsic is currently only implemented for i32. llvm.amdgcn.udot2 Provides direct access to v_dot2_u32_u16 across targets which; support such instructions. This performs unsigned dot product; with two v2i16 operands, summed with the third i32 operand. The; i1 fourth operand is used to clamp the output. llvm.amdgcn.udot4 Provides direct access to v_dot4_u32_u8 across targets which; support such instructions. This performs unsigned dot product; with two i32 operands (holding a vector of 4 8bit values), summed; with the third i32 operand. The i1 fourth operand is used to clamp; the output. llvm.amdgcn.udot8 Provides direct access to v_dot8_u32_u4 across targets which; support such instructions. This performs unsigned dot product; with two i32 operands (holding a vector of 8 4bit values), summed; with the third i32 operand. The i1 fourth operand is used to clamp; the output. llvm.amdgcn.sdot2 Provides direct access to v_dot2_i32_i16 across targets which; support such instructions. This performs signed dot product; with two v2i16 operands, summed with the third i32 operand. The; i1 fourth operand is used to clamp the output.; When applicable (e.g. no clamping), this is lowered into; v_dot2c_i32_i16 for targets which support it. llvm.amdgcn.sdot4 Provides direct access to v_dot4_i32_i8 across targets which; support such instructions. This performs signed dot product; with two i32 operands (holding a vector of 4 8bit values), summed; with the third i32 operan",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:39504,perform,performs,39504,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performs']
Performance,"64 or BPF_JMP,; 4th bit encodes source operand. ::. BPF_X 0x1 use src_reg register as source operand; BPF_K 0x0 use 32 bit immediate as source operand. and four MSB bits store operation code. ::. BPF_ADD 0x0 add; BPF_SUB 0x1 subtract; BPF_MUL 0x2 multiply; BPF_DIV 0x3 divide; BPF_OR 0x4 bitwise logical OR; BPF_AND 0x5 bitwise logical AND; BPF_LSH 0x6 left shift; BPF_RSH 0x7 right shift (zero extended); BPF_NEG 0x8 arithmetic negation; BPF_MOD 0x9 modulo; BPF_XOR 0xa bitwise logical XOR; BPF_MOV 0xb move register to register; BPF_ARSH 0xc right shift (sign extended); BPF_END 0xd endianness conversion. If BPF_CLASS(code) == BPF_JMP, BPF_OP(code) is one of. ::. BPF_JA 0x0 unconditional jump; BPF_JEQ 0x1 jump ==; BPF_JGT 0x2 jump >; BPF_JGE 0x3 jump >=; BPF_JSET 0x4 jump if (DST & SRC); BPF_JNE 0x5 jump !=; BPF_JSGT 0x6 jump signed >; BPF_JSGE 0x7 jump signed >=; BPF_CALL 0x8 function call; BPF_EXIT 0x9 function return. Instruction encoding (load, store); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; For load and store instructions the 8-bit 'code' field is divided as:. ::. +--------+--------+-------------------+; | 3 bits | 2 bits | 3 bits |; | mode | size | instruction class |; +--------+--------+-------------------+; (MSB) (LSB). Size modifier is one of. ::. BPF_W 0x0 word; BPF_H 0x1 half word; BPF_B 0x2 byte; BPF_DW 0x3 double word. Mode modifier is one of. ::. BPF_IMM 0x0 immediate; BPF_ABS 0x1 used to access packet data; BPF_IND 0x2 used to access packet data; BPF_MEM 0x3 memory; (reserved) 0x4; (reserved) 0x5; BPF_XADD 0x6 exclusive add. Packet data access (BPF_ABS, BPF_IND); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Two non-generic instructions: (BPF_ABS | <size> | BPF_LD) and; (BPF_IND | <size> | BPF_LD) which are used to access packet data.; Register R6 is an implicit input that must contain pointer to sk_buff.; Register R0 is an implicit output which contains the data fetched; from the packet. Registers R1-R5 are scratch registers and must not; be used to store the data ac",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:105051,load,load,105051,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,2,['load'],['load']
Performance,"6U); true();; }. The same transformation can work with an even modulo with the addition of a; rotate: rotate the result of the multiply to the right by the number of bits; which need to be zero for the condition to be true, and shrink the compare RHS; by the same amount. Unless the target supports rotates, though, that; transformation probably isn't worthwhile. The transformation can also easily be made to work with non-zero equality; comparisons: just transform, for example, ""n % 3 == 1"" to ""(n-1) % 3 == 0"". //===---------------------------------------------------------------------===//. Better mod/ref analysis for scanf would allow us to eliminate the vtable and a; bunch of other stuff from this example (see PR1604): . #include <cstdio>; struct test {; int val;; virtual ~test() {}; };. int main() {; test t;; std::scanf(""%d"", &t.val);; std::printf(""%d\n"", t.val);; }. //===---------------------------------------------------------------------===//. These functions perform the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:20837,perform,perform,20837,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['perform'],['perform']
Performance,"7-GFX9 ``flat_load/store/atomic`` instructions can report out of; vector memory order if they access LDS memory, and out of LDS operation order; if they access global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore, no special action is required for coherence between the; lanes of a single wavefront, or for coherence between wavefronts in the same; work-group. A ``buffer_wbinvl1_vol`` is required for coherence between; wavefronts executing in different work-groups as they may be executing on; different CUs.; * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a group of CUs. The scalar and vector L1 caches are not coherent. However,; scalar operations are used in a restricted way so do not impact the memory; model. See :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory operations use an L2 cache shared by all CUs on; the same agent.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel. Therefore, the vector and; scalar memory operations performed by wavefronts executing in different; work-groups (which may be executing on different CUs) of an agent can be; reordered relative to each other. A ``s_waitcnt vmcnt(0)`` is required to; ensure synchronization between vector memory operations of different CUs. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation and so can be used to meet the; requirements of acquire and release.; * The L2 cache can be kept coherent with other agents on some targets, or ranges; of virtual addresses can be set up to bypass it to ensure system coherence. Scalar memory operations are only used to access memory that is proven to not; change during the execution of the kernel dispatch. This includes constant; address space and global address space for program scope ``const`` varia",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:208478,cache,cache,208478,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"8 89 c8 mov %rcx,%rax; ca7fc8: 48 29 d0 sub %rdx,%rax; ca7fcb: 48 c1 c0 3d rol $0x3d,%rax; ca7fcf: 48 3d 7f 01 00 00 cmp $0x17f,%rax; ca7fd5: 0f 87 36 05 00 00 ja ca8511; ca7fdb: 48 8d 15 c0 0b f7 06 lea 0x6f70bc0(%rip),%rdx; ca7fe2: f6 04 10 10 testb $0x10,(%rax,%rdx,1); ca7fe6: 0f 84 25 05 00 00 je ca8511; ca7fec: ff 91 98 00 00 00 callq *0x98(%rcx); [...]; ca8511: 0f 0b ud2. The compiler relies on co-operation from the linker in order to assemble; the bit vectors for the whole program. It currently does this using LLVM's; `type metadata`_ mechanism together with link-time optimization. .. _address point: https://itanium-cxx-abi.github.io/cxx-abi/abi.html#vtable-general; .. _type metadata: https://llvm.org/docs/TypeMetadata.html; .. _ByteArrayBuilder: https://llvm.org/docs/doxygen/html/structllvm_1_1ByteArrayBuilder.html. Optimizations; -------------. The scheme as described above is the fully general variant of the scheme.; Most of the time we are able to apply one or more of the following; optimizations to improve binary size or performance. In fact, if you try the above example with the current version of the; compiler, you will probably find that it will not use the described virtual; table layout or machine instructions. Some of the optimizations we are about; to introduce cause the compiler to use a different layout or a different; sequence of machine instructions. Stripping Leading/Trailing Zeros in Bit Vectors; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If a bit vector contains leading or trailing zeros, we can strip them from; the vector. The compiler will emit code to check if the pointer is in range; of the region covered by ones, and perform the bit vector check using a; truncated version of the bit vector. For example, the bit vectors for our; example class hierarchy will be emitted like this:. .. csv-table:: Bit Vectors for A, B, C; :header: Class, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14. A, , , 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, ,; B, ,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:3624,optimiz,optimizations,3624,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"8b032d0, ID#5):; Predecessors according to CFG: 0x8b0c5f0 (#3) 0x8b0a7c0 (#4); %reg1039 = PHI %reg1070, mbb<bb76.outer,0x8b0c5f0>, %reg1037, mbb<bb27,0x8b0a7c0>. Note ADDri is not a two-address instruction. However, its result %reg1037 is an; operand of the PHI node in bb76 and its operand %reg1039 is the result of the; PHI node. We should treat it as a two-address code and make sure the ADDri is; scheduled after any node that reads %reg1039. //===---------------------------------------------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retain",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:1768,load,load,1768,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt,1,['load'],['load']
Performance,8e9/docs/source/instruction_set_extensions.rst>`_ by OpenHW Group. All instructions are prefixed with `cv.` as described in the specification. ``XCVelw``; LLVM implements `version 1.0.0 of the CORE-V Event load custom instructions specification <https://github.com/openhwgroup/cv32e40p/blob/master/docs/source/instruction_set_extensions.rst>`_ by OpenHW Group. All instructions are prefixed with `cv.` as described in the specification. These instructions are only available for riscv32 at this time. ``XCVmac``; LLVM implements `version 1.0.0 of the CORE-V Multiply-Accumulate (MAC) custom instructions specification <https://github.com/openhwgroup/cv32e40p/blob/4f024fe4b15a68b76615b0630c07a6745c620da7/docs/source/instruction_set_extensions.rst>`_ by OpenHW Group. All instructions are prefixed with `cv.mac` as described in the specification. These instructions are only available for riscv32 at this time. ``XCVmem``; LLVM implements `version 1.0.0 of the CORE-V Post-Increment load and stores custom instructions specification <https://github.com/openhwgroup/cv32e40p/blob/master/docs/source/instruction_set_extensions.rst>`_ by OpenHW Group. All instructions are prefixed with `cv.` as described in the specification. These instructions are only available for riscv32 at this time. ``XCValu``; LLVM implements `version 1.0.0 of the Core-V ALU custom instructions specification <https://github.com/openhwgroup/cv32e40p/blob/4f024fe4b15a68b76615b0630c07a6745c620da7/docs/source/instruction_set_extensions.rst>`_ by Core-V. All instructions are prefixed with `cv.` as described in the specification. These instructions are only available for riscv32 at this time. ``XCVsimd``; LLVM implements `version 1.0.0 of the CORE-V SIMD custom instructions specification <https://github.com/openhwgroup/cv32e40p/blob/cv32e40p_v1.3.2/docs/source/instruction_set_extensions.rst>`_ by OpenHW Group. All instructions are prefixed with `cv.` as described in the specification. ``XCVbi``; LLVM implements `version,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/RISCVUsage.rst:18895,load,load,18895,interpreter/llvm-project/llvm/docs/RISCVUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/RISCVUsage.rst,1,['load'],['load']
Performance,"91 98 00 00 00 callq *0x98(%rcx); [...]; ca8511: 0f 0b ud2. The compiler relies on co-operation from the linker in order to assemble; the bit vectors for the whole program. It currently does this using LLVM's; `type metadata`_ mechanism together with link-time optimization. .. _address point: https://itanium-cxx-abi.github.io/cxx-abi/abi.html#vtable-general; .. _type metadata: https://llvm.org/docs/TypeMetadata.html; .. _ByteArrayBuilder: https://llvm.org/docs/doxygen/html/structllvm_1_1ByteArrayBuilder.html. Optimizations; -------------. The scheme as described above is the fully general variant of the scheme.; Most of the time we are able to apply one or more of the following; optimizations to improve binary size or performance. In fact, if you try the above example with the current version of the; compiler, you will probably find that it will not use the described virtual; table layout or machine instructions. Some of the optimizations we are about; to introduce cause the compiler to use a different layout or a different; sequence of machine instructions. Stripping Leading/Trailing Zeros in Bit Vectors; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If a bit vector contains leading or trailing zeros, we can strip them from; the vector. The compiler will emit code to check if the pointer is in range; of the region covered by ones, and perform the bit vector check using a; truncated version of the bit vector. For example, the bit vectors for our; example class hierarchy will be emitted like this:. .. csv-table:: Bit Vectors for A, B, C; :header: Class, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14. A, , , 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, ,; B, , , , , , , , 1, , , , , , ,; C, , , , , , , , , , , , , 1, ,. Short Inline Bit Vectors; ~~~~~~~~~~~~~~~~~~~~~~~~. If the vector is sufficiently short, we can represent it as an inline constant; on x86. This saves us a few instructions when reading the correct element; of the bit vector. If the bit vector fits in 32 bits",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:3875,optimiz,optimizations,3875,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,1,['optimiz'],['optimizations']
Performance,": ""; <<min.userState().value(1); <<"" ""<<e1.first<<"" ""<<e1.second<<std::endl;; std::cout<<""par2: ""; <<min.userState().value(""area""); <<"" ""<<e2.first<<"" ""<<e2.second<<std::endl;; }; }. {; // demonstrate how to use the CONTOURs. // create Minuit parameters with names; MnUserParameters upar;; upar.add(""mean"", mean, 0.1);; upar.add(""sigma"", rms, 0.1);; upar.add(""area"", area, 0.1);. // create Migrad minimizer; MnMigrad migrad(theFCN, upar);. // minimize; FunctionMinimum min = migrad();. // create contours factory with FCN and minimum; MnContours contours(theFCN, min);. // 70% confidence level for 2 parameters contour; // around the minimum; theFCN.setErrorDef(2.41);; std::vector<std::pair<double,double> > cont =; contours(0, 1, 20);. // 95% confidence level for 2 parameters contour; theFCN.setErrorDef(5.99);; std::vector<std::pair<double,double> > cont4 =; contours(0, 1, 20);. // plot the contours; MnPlot plot;; cont4.insert(cont4.end(), cont.begin(), cont.end());; plot(min.userState().value(""mean""),; min.userState().value(""sigma""),; cont4);; }. return 0;; };; ```. [^1]: ROOT @bib-ROOT uses its own version of the Fortran M when this; manual was written. However an interface for this version exists and; the library can be loaded dynamically on demand. [^2]: The *internal error matrix* maintained by M is transformed for the; user into *external coordinates*, but the numbering of rows and; columns is of course still according to internal parameter; numbering, since one does not want rows and columns corresponding to; parameters which are not variable. The transformation therefore; affects only parameters with limits; if there are no limits,; internal and external error matrices are the same. [^3]: For example, if $\mbox{a}$ and $\mbox{b}$ are double; precision variables, the statement $\mbox{a = 2*b}$ is not good; programming, but happens to do what the user probably intended,; whereas the statement $\mbox{a = b + 2/3}$ almost certainly will; not do what the user intended.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md:83552,load,loaded,83552,documentation/minuit2/Minuit2.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md,1,['load'],['loaded']
Performance,": FaultingPCOffset; uint32 : HandlerPCOffset; }; }. FailtKind describes the reason of expected fault. Currently three kind; of faults are supported:. 1. ``FaultMaps::FaultingLoad`` - fault due to load from memory.; 2. ``FaultMaps::FaultingLoadStore`` - fault due to instruction load and store.; 3. ``FaultMaps::FaultingStore`` - fault due to store to memory. The ``ImplicitNullChecks`` pass; ===============================. The ``ImplicitNullChecks`` pass transforms explicit control flow for; checking if a pointer is ``null``, like:. .. code-block:: llvm. %ptr = call i32* @get_ptr(); %ptr_is_null = icmp i32* %ptr, null; br i1 %ptr_is_null, label %is_null, label %not_null, !make.implicit !0. not_null:; %t = load i32, i32* %ptr; br label %do_something_with_t. is_null:; call void @HFC(); unreachable. !0 = !{}. to control flow implicit in the instruction loading or storing through; the pointer being null checked:. .. code-block:: llvm. %ptr = call i32* @get_ptr(); %t = load i32, i32* %ptr ;; handler-pc = label %is_null; br label %do_something_with_t. is_null:; call void @HFC(); unreachable. This transform happens at the ``MachineInstr`` level, not the LLVM IR; level (so the above example is only representative, not literal). The; ``ImplicitNullChecks`` pass runs during codegen, if; ``-enable-implicit-null-checks`` is passed to ``llc``. The ``ImplicitNullChecks`` pass adds entries to the; ``__llvm_faultmaps`` section described above as needed. ``make.implicit`` metadata; --------------------------. Making null checks implicit is an aggressive optimization, and it can; be a net performance pessimization if too many memory operations end; up faulting because of it. A language runtime typically needs to; ensure that only a negligible number of implicit null checks actually; fault once the application has reached a steady state. A standard way; of doing this is by healing failed implicit null checks into explicit; null checks via code patching or recompilation. It follows that t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FaultMaps.rst:2614,load,load,2614,interpreter/llvm-project/llvm/docs/FaultMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FaultMaps.rst,1,['load'],['load']
Performance,": LLVM's SmallVectorBase; is templated over the type of fSize and fCapacity. It contains the parts of `RVec` that do not depend on the value; type.; No other classes in the hierarchy can contain data members! We expect the memory after `SmallVectorBase` to be; occupied by the small inline buffer. `SmallVectorTemplateCommon<T>`; - `getFirstEl()`: returns the address of the beginning of the small buffer; - `begin()`, `end()`, `front()`, `back()`, etc. Basically the same as the corresponding LLVM class.; It contains the parts that are independent of whether T is a POD or not. `SmallVectorTemplateBase<T, bool TriviallyCopiable>` and the specialization `SmallVectorTemplateBase<T, true>`; - `grow()`, `uninitialized_copy`, `uninitialized_move`, `push_back()`, `pop_back()`. This class contains the parts of `RVec` that can be optimized for trivially copiable types.; In particular, destruction can be skipped and memcpy can be used in place of copy/move construction.; These optimizations are inherited from LLVM's SmallVector. `RVecImpl<T>`; The analogous of LLVM's `SmallVectorImpl`, it factors out of `RVec` the parts that are independent of; the small buffer size, to limit the amount of code generated and provide a way to slice the small buffer; size when passing around `RVec` objects. `RVecN<T, N>`; It aggregates `RVecImpl` and `SmallVectorStorage` (see below) through public inheritance.; `N` is the small buffer size. `RVec<T>`; Inherits from `RVecN` and fixes the small buffer size `N` to a reasonable default.; We expect most users to use this type and only very rarely switch to `RVecN` to tweak the small buffer size. ### Helper types. - `SmallVectorAlignmentAndSize`: used to figure out the offset of the first small-buffer element in; `SmallVectorTemplateCommon::getFirstEl`; - `SmallVectorStorage`: properly aligned ""small buffer"" storage. It's a separate type so that it can be specialized to; be properly aligned also for the case of small buffer size = 0; - `RVecInlineStorage",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/vecops/ARCHITECTURE.md:2987,optimiz,optimizations,2987,math/vecops/ARCHITECTURE.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/vecops/ARCHITECTURE.md,1,['optimiz'],['optimizations']
Performance,": Loading from 0x[[#%x,ADDR:]]; ; CHECK-SAME: to 0x[[#ADDR + 7]]. The above example would match the text:. .. code-block:: gas. load r5, [r0]; load r6, [r1]; Loading from 0xa0463440 to 0xa0463447. but would not match the text:. .. code-block:: gas. load r5, [r0]; load r7, [r1]; Loading from 0xa0463440 to 0xa0463443. Due to ``7`` being unequal to ``5 + 1`` and ``a0463443`` being unequal to; ``a0463440 + 7``. A numeric variable can also be defined to the result of a numeric expression,; in which case the numeric expression constraint is checked and if verified the; variable is assigned to the value. The unified syntax for both checking a; numeric expression and capturing its value into a numeric variable is thus; ``[[#%<fmtspec>,<NUMVAR>: <constraint> <expr>]]`` with each element as; described previously. One can use this syntax to make a testcase more; self-describing by using variables instead of values:. .. code-block:: gas. ; CHECK: mov r[[#REG_OFFSET:]], 0x[[#%X,FIELD_OFFSET:12]]; ; CHECK-NEXT: load r[[#]], [r[[#REG_BASE:]], r[[#REG_OFFSET]]]. which would match:. .. code-block:: gas. mov r4, 0xC; load r6, [r5, r4]. The ``--enable-var-scope`` option has the same effect on numeric variables as; on string variables. Important note: In its current implementation, an expression cannot use a; numeric variable defined earlier in the same CHECK directive. FileCheck Pseudo Numeric Variables; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Sometimes there's a need to verify output that contains line numbers of the; match file, e.g. when testing compiler diagnostics. This introduces a certain; fragility of the match file structure, as ""``CHECK:``"" lines contain absolute; line numbers in the same file, which have to be updated whenever line numbers; change due to text addition or deletion. To support this case, FileCheck expressions understand the ``@LINE`` pseudo; numeric variable which evaluates to the line number of the CHECK pattern where; it is found. This way match patterns can be ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst:33201,load,load,33201,interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,1,['load'],['load']
Performance,": Rationale. Mismatches with returned results will cause over-retains or over-releases,; depending on the direction. Again, the rule about function calls is really; just an application of the existing C/C++ rule about calling functions; through an incompatible function type. .. _arc.objects.operands.unretained-returns:. Unretained return values; ^^^^^^^^^^^^^^^^^^^^^^^^. A method or function which returns a retainable object type but does not return; a retained value must ensure that the object is still valid across the return; boundary. When returning from such a function or method, ARC retains the value at the; point of evaluation of the return statement, then leaves all local scopes, and; then balances out the retain while ensuring that the value lives across the; call boundary. In the worst case, this may involve an ``autorelease``, but; callers must not assume that the value is actually in the autorelease pool. ARC performs no extra mandatory work on the caller side, although it may elect; to do something to shorten the lifetime of the returned value. .. admonition:: Rationale. It is common in non-ARC code to not return an autoreleased value; therefore; the convention does not force either path. It is convenient to not be; required to do unnecessary retains and autoreleases; this permits; optimizations such as eliding retain/autoreleases when it can be shown that; the original pointer will still be valid at the point of return. A method or function may be marked with; ``__attribute__((ns_returns_autoreleased))`` to indicate that it returns a; pointer which is guaranteed to be valid at least as long as the innermost; autorelease pool. There are no additional semantics enforced in the definition; of such a method; it merely enables optimizations in callers. .. _arc.objects.operands.casts:. Bridged casts; ^^^^^^^^^^^^^. A :arc-term:`bridged cast` is a C-style cast annotated with one of three; keywords:. * ``(__bridge T) op`` casts the operand to the destination typ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:21899,perform,performs,21899,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['performs']
Performance,": llvm. %res = call i4 @llvm.udiv.fix.i4(i4 6, i4 2, i32 0) ; %res = 3 (6 / 2 = 3); %res = call i4 @llvm.udiv.fix.i4(i4 6, i4 4, i32 1) ; %res = 3 (3 / 2 = 1.5); %res = call i4 @llvm.udiv.fix.i4(i4 1, i4 -8, i32 4) ; %res = 2 (0.0625 / 0.5 = 0.125). ; The result in the following could be rounded up to 1 or down to 0.5; %res = call i4 @llvm.udiv.fix.i4(i4 3, i4 4, i32 1) ; %res = 2 (or 1) (1.5 / 2 = 0.75). '``llvm.sdiv.fix.sat.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.sdiv.fix.sat``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.sdiv.fix.sat.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.sdiv.fix.sat.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.sdiv.fix.sat.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.sdiv.fix.sat.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.sdiv.fix.sat``' family of intrinsic functions perform signed; fixed point saturating division on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. ``%a`` and ``%b`` are the two; values that will undergo signed fixed point division. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs fixed point division on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. The maximum value this operation can clamp to is the largest signed value; representable by the bit width of the first 2 arguments. The minimum value is the; smallest signed value representable by this bit width. It is undefined behavi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:634418,perform,perform,634418,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,":. '``llvm.vp.mul.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.mul.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.mul.nxv46i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.mul.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated integer multiplication of two vectors of integers. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of integer type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """"""""""""""""""""; The '``llvm.vp.mul``' intrinsic performs integer multiplication; (:ref:`mul <i_mul>`) of the first and second vector operand on each enabled; lane. The result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.mul.v4i32(<4 x i32> %a, <4 x i32> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = mul <4 x i32> %a, %b; %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_sdiv:. '``llvm.vp.sdiv.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.sdiv.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.sdiv.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.sdiv.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overvi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:701130,perform,performs,701130,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,":. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same CU but may be; executed by different SIMDs. The exception is when in tgsplit execution mode; when the wavefronts may be executed by different SIMDs in different CUs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it. The exception is when in tgsplit execution mode when no LDS; is allocated as wavefronts of the same work-group can be in different CUs.; * All LDS operations of a CU are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that ``flat_load/store/atomic`` instructions can report out of vector memory; order if they access LDS memory, and out of LDS operation order if they access; global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore:. * No special action is required for coherence between the lanes of a single; wavefront. * No special action is required for coherence between wavefronts in the same; work-group since they execute on the same CU. The",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:235058,perform,performed,235058,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['perform'],['performed']
Performance,":. ::. foo not inlined into bar because it should never be inlined; (cost=never): noinline function attribute. ``Analysis``. Remarks that describe the result of an analysis, that can bring more; information to the user regarding the generated code. :Example:. ::. 16 stack bytes in function. ::. 10 instructions in function. Enabling optimization remarks; =============================. There are two modes that are supported for enabling optimization remarks in; LLVM: through remark diagnostics, or through serialized remarks. Remark diagnostics; ------------------. Optimization remarks can be emitted as diagnostics. These diagnostics will be; propagated to front-ends if desired, or emitted by tools like :doc:`llc; <CommandGuide/llc>` or :doc:`opt <CommandGuide/opt>`. .. option:: -pass-remarks=<regex>. Enables optimization remarks from passes whose name match the given (POSIX); regular expression. .. option:: -pass-remarks-missed=<regex>. Enables missed optimization remarks from passes whose name match the given; (POSIX) regular expression. .. option:: -pass-remarks-analysis=<regex>. Enables optimization analysis remarks from passes whose name match the given; (POSIX) regular expression. Serialized remarks; ------------------. While diagnostics are useful during development, it is often more useful to; refer to optimization remarks post-compilation, typically during performance; analysis. For that, LLVM can serialize the remarks produced for each compilation unit to; a file that can be consumed later. By default, the format of the serialized remarks is :ref:`YAML; <yamlremarks>`, and it can be accompanied by a :ref:`section <remarkssection>`; in the object files to easily retrieve it. :doc:`llc <CommandGuide/llc>` and :doc:`opt <CommandGuide/opt>` support the; following options:. ``Basic options``. .. option:: -pass-remarks-output=<filename>. Enables the serialization of remarks to a file specified in <filename>. By default, the output is serialized to :ref:`YAML <yamlr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst:1664,optimiz,optimization,1664,interpreter/llvm-project/llvm/docs/Remarks.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst,1,['optimiz'],['optimization']
Performance,":. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8833,load,load,8833,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,4,['load'],['load']
Performance,":. declare float @llvm.vp.reduce.fadd.v4f32(float <start_value>, <4 x float> <val>, <4 x i1> <mask>, i32 <vector_length>); declare double @llvm.vp.reduce.fadd.nxv8f64(double <start_value>, <vscale x 8 x double> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point ``ADD`` reduction of a vector and a scalar starting; value, returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; floating-point type equal to the result type. The second operand is the vector; on which the reduction is performed and must be a vector of floating-point; values whose element type is the result/start type. The third operand is the; vector mask and is a vector of boolean values with the same number of elements; as the vector operand. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.fadd``' intrinsic performs the floating-point ``ADD``; reduction (:ref:`llvm.vector.reduce.fadd <int_vector_reduce_fadd>`) of the; vector operand ``val`` on each enabled lane, adding it to the scalar; ``start_value``. Disabled lanes are treated as containing the neutral value; ``-0.0`` (i.e. having no effect on the reduction operation). If no lanes are; enabled, the resulting value will be equal to ``start_value``. To ignore the start value, the neutral value can be used. See the unpredicated version (:ref:`llvm.vector.reduce.fadd; <int_vector_reduce_fadd>`) for more detail on the semantics of the reduction. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call float @llvm.vp.reduce.fadd.v4f32(float %start, <4 x float> %a, <4 x i1> %mask, i32 %evl); ; %r is equivalent to %also.r, where lanes greater than or equal to %evl; ; are treated as though %mask were false for those lanes. %masked.a = select <4 x i1> %mask, <4 x float> %a, <4 x float> <float -0.0, float -0.0, float -0.0, float -0.0>; %also.r = call float @llvm.vector.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:751365,perform,performs,751365,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,":. declare float @llvm.vp.reduce.fmul.v4f32(float <start_value>, <4 x float> <val>, <4 x i1> <mask>, i32 <vector_length>); declare double @llvm.vp.reduce.fmul.nxv8f64(double <start_value>, <vscale x 8 x double> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point ``MUL`` reduction of a vector and a scalar starting; value, returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; floating-point type equal to the result type. The second operand is the vector; on which the reduction is performed and must be a vector of floating-point; values whose element type is the result/start type. The third operand is the; vector mask and is a vector of boolean values with the same number of elements; as the vector operand. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.fmul``' intrinsic performs the floating-point ``MUL``; reduction (:ref:`llvm.vector.reduce.fmul <int_vector_reduce_fmul>`) of the; vector operand ``val`` on each enabled lane, multiplying it by the scalar; `start_value``. Disabled lanes are treated as containing the neutral value; ``1.0`` (i.e. having no effect on the reduction operation). If no lanes are; enabled, the resulting value will be equal to the starting value. To ignore the start value, the neutral value can be used. See the unpredicated version (:ref:`llvm.vector.reduce.fmul; <int_vector_reduce_fmul>`) for more detail on the semantics. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call float @llvm.vp.reduce.fmul.v4f32(float %start, <4 x float> %a, <4 x i1> %mask, i32 %evl); ; %r is equivalent to %also.r, where lanes greater than or equal to %evl; ; are treated as though %mask were false for those lanes. %masked.a = select <4 x i1> %mask, <4 x float> %a, <4 x float> <float 1.0, float 1.0, float 1.0, float 1.0>; %also.r = call float @llvm.vector.reduce.fmul.v4f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:755577,perform,performs,755577,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"://www.sphinx-doc.org/en/stable/ and published to; http://cppyy.readthedocs.io/en/latest/index.html using a webhook. To create; the docs::. $ pip install sphinx_rtd_theme; Collecting sphinx_rtd_theme; ...; Successfully installed sphinx-rtd-theme-0.2.4; $ cd docs; $ make html. The Python code in this module supports:. * Interfacing to the correct backend for CPython or PyPy.; * Pythonizations (TBD). Cppyy-backend; -------------. The ``cppyy-backend`` module contains two areas:. * A patched copy of cling; * Wrapper code. Package structure; -----------------; .. _package-structure:. There are four PyPA packages involved in a full installation, with the; following structure::. (A) _cppyy (PyPy); / \; (1) cppyy (3) cppyy-backend -- (4) cppyy-cling; \ /; (2) CPyCppyy (CPython). The user-facing package is always ``cppyy`` (1).; It is used to select the other (versioned) required packages, based on the; python interpreter for which it is being installed. Below (1) follows a bifurcation based on interpreter.; This is needed for functionality and performance: for CPython, there is the; CPyCppyy package (2).; It is written in C++, makes use of the Python C-API, and installs as a Python; extension module.; For PyPy, there is the builtin module ``_cppyy`` (A).; This is not a PyPA package.; It is written in RPython as it needs access to low-level pointers, JIT hints,; and the ``_cffi_backend`` backend module (itself builtin). Shared again across interpreters is the backend, which is split in a small; wrapper (3) and a large package that contains Cling/LLVM (4).; The former is still under development and expected to be updated frequently.; It is small enough to download and build very quickly.; The latter, however, takes a long time to build, but since it is very stable,; splitting it off allows the creation of binary wheels that need updating; only infrequently (expected about twice a year). All code is publicly available; see the; :doc:`section on repositories <repositories>`.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/packages.rst:1329,perform,performance,1329,bindings/pyroot/cppyy/cppyy/doc/source/packages.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/packages.rst,1,['perform'],['performance']
Performance,":: MCJIT-load.png. The PassManager::run call causes the MC code generation mechanisms to emit; a complete relocatable binary object image (either in either ELF or MachO; format, depending on the target) into the ObjectBufferStream object, which; is flushed to complete the process. If an ObjectCache is being used, the; image will be passed to the ObjectCache here. At this point, the ObjectBufferStream contains the raw object image.; Before the code can be executed, the code and data sections from this; image must be loaded into suitable memory, relocations must be applied and; memory permission and code cache invalidation (if required) must be completed. Object Loading; ==============. Once an object image has been obtained, either through code generation or; having been retrieved from an ObjectCache, it is passed to RuntimeDyld to; be loaded. The RuntimeDyld wrapper class examines the object to determine; its file format and creates an instance of either RuntimeDyldELF or; RuntimeDyldMachO (both of which derive from the RuntimeDyldImpl base; class) and calls the RuntimeDyldImpl::loadObject method to perform that; actual loading. .. image:: MCJIT-dyld-load.png. RuntimeDyldImpl::loadObject begins by creating an ObjectImage instance; from the ObjectBuffer it received. ObjectImage, which wraps the; ObjectFile class, is a helper class which parses the binary object image; and provides access to the information contained in the format-specific; headers, including section, symbol and relocation information. RuntimeDyldImpl::loadObject then iterates through the symbols in the; image. Information about common symbols is collected for later use. For; each function or data symbol, the associated section is loaded into memory; and the symbol is stored in a symbol table map data structure. When the; iteration is complete, a section is emitted for the common symbols. Next, RuntimeDyldImpl::loadObject iterates through the sections in the; object image and for each section iterates",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:3921,load,loadObject,3921,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,3,"['load', 'perform']","['loadObject', 'loading', 'perform']"
Performance,":: c++. typedef float m4x4_t __attribute__((matrix_type(4, 4)));. m4x4_t f(m4x4_t a, m4x4_t b) {; a += b;; a -= b;; a *= b;; a += 23;; a -= 12;; return a;; }. The matrix type extension supports explicit casts. Implicit type conversion between matrix types is not allowed. .. code-block:: c++. typedef int ix5x5 __attribute__((matrix_type(5, 5)));; typedef float fx5x5 __attribute__((matrix_type(5, 5)));. fx5x5 f1(ix5x5 i, fx5x5 f) {; return (fx5x5) i;; }. template <typename X>; using matrix_4_4 = X __attribute__((matrix_type(4, 4)));. void f2() {; matrix_5_5<double> d;; matrix_5_5<int> i;; i = (matrix_5_5<int>)d;; i = static_cast<matrix_5_5<int>>(d);; }. Half-Precision Floating Point; =============================. Clang supports three half-precision (16-bit) floating point types:; ``__fp16``, ``_Float16`` and ``__bf16``. These types are supported; in all language modes, but their support differs between targets.; A target is said to have ""native support"" for a type if the target; processor offers instructions for directly performing basic arithmetic; on that type. In the absence of native support, a type can still be; supported if the compiler can emulate arithmetic on the type by promoting; to ``float``; see below for more information on this emulation. * ``__fp16`` is supported on all targets. The special semantics of this; type mean that no arithmetic is ever performed directly on ``__fp16`` values;; see below. * ``_Float16`` is supported on the following targets:. * 32-bit ARM (natively on some architecture versions); * 64-bit ARM (AArch64) (natively on ARMv8.2a and above); * AMDGPU (natively); * SPIR (natively); * X86 (if SSE2 is available; natively if AVX512-FP16 is also available); * RISC-V (natively if Zfh or Zhinx is available). * ``__bf16`` is supported on the following targets (currently never natively):. * 32-bit ARM; * 64-bit ARM (AArch64); * RISC-V; * X86 (when SSE2 is available). (For X86, SSE2 is available on 64-bit and all recent 32-bit processors.). `",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:30225,perform,performing,30225,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['perform'],['performing']
Performance,":: console. $ clang -mllvm -force-vector-interleave=2 ...; $ opt -loop-vectorize -force-vector-interleave=2 ... Pragma loop hint directives; ^^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``#pragma clang loop`` directive allows loop vectorization hints to be; specified for the subsequent for, while, do-while, or c++11 range-based for; loop. The directive allows vectorization and interleaving to be enabled or; disabled. Vector width as well as interleave count can also be manually; specified. The following example explicitly enables vectorization and; interleaving:. .. code-block:: c++. #pragma clang loop vectorize(enable) interleave(enable); while(...) {; ...; }. The following example implicitly enables vectorization and interleaving by; specifying a vector width and interleaving count:. .. code-block:: c++. #pragma clang loop vectorize_width(2) interleave_count(2); for(...) {; ...; }. See the Clang; `language extensions; <https://clang.llvm.org/docs/LanguageExtensions.html#extensions-for-loop-hint-optimizations>`_; for details. Diagnostics; -----------. Many loops cannot be vectorized including loops with complicated control flow,; unvectorizable types, and unvectorizable calls. The loop vectorizer generates; optimization remarks which can be queried using command line options to identify; and diagnose loops that are skipped by the loop-vectorizer. Optimization remarks are enabled using:. ``-Rpass=loop-vectorize`` identifies loops that were successfully vectorized. ``-Rpass-missed=loop-vectorize`` identifies loops that failed vectorization and; indicates if vectorization was specified. ``-Rpass-analysis=loop-vectorize`` identifies the statements that caused; vectorization to fail. If in addition ``-fsave-optimization-record`` is; provided, multiple causes of vectorization failure may be listed (this behavior; might change in the future). Consider the following loop:. .. code-block:: c++. #pragma clang loop vectorize(enable); for (int i = 0; i < Length; i++) {; switch(A[i]) {; ca",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst:2452,optimiz,optimizations,2452,interpreter/llvm-project/llvm/docs/Vectorizers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst,1,['optimiz'],['optimizations']
Performance,"::. ;; This instruction unconditionally stores data vector in multiple addresses; call @llvm.masked.scatter.v8i32.v8p0(<8 x i32> %value, <8 x ptr> %ptrs, i32 4, <8 x i1> <true, true, .. true>). ;; It is equivalent to a list of scalar stores; %val0 = extractelement <8 x i32> %value, i32 0; %val1 = extractelement <8 x i32> %value, i32 1; ..; %val7 = extractelement <8 x i32> %value, i32 7; %ptr0 = extractelement <8 x ptr> %ptrs, i32 0; %ptr1 = extractelement <8 x ptr> %ptrs, i32 1; ..; %ptr7 = extractelement <8 x ptr> %ptrs, i32 7; ;; Note: the order of the following stores is important when they overlap:; store i32 %val0, ptr %ptr0, align 4; store i32 %val1, ptr %ptr1, align 4; ..; store i32 %val7, ptr %ptr7, align 4. Masked Vector Expanding Load and Compressing Store Intrinsics; -------------------------------------------------------------. LLVM provides intrinsics for expanding load and compressing store operations. Data selected from a vector according to a mask is stored in consecutive memory addresses (compressed store), and vice-versa (expanding load). These operations effective map to ""if (cond.i) a[j++] = v.i"" and ""if (cond.i) v.i = a[j++]"" patterns, respectively. Note that when the mask starts with '1' bits followed by '0' bits, these operations are identical to :ref:`llvm.masked.store <int_mstore>` and :ref:`llvm.masked.load <int_mload>`. .. _int_expandload:. '``llvm.masked.expandload.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. Several values of integer, floating point or pointer data type are loaded from consecutive memory addresses and stored into the elements of a vector according to the mask. ::. declare <16 x float> @llvm.masked.expandload.v16f32 (ptr <ptr>, <16 x i1> <mask>, <16 x float> <passthru>); declare <2 x i64> @llvm.masked.expandload.v2i64 (ptr <ptr>, <2 x i1> <mask>, <2 x i64> <passthru>). Overview:; """""""""""""""""". Reads a number of scalar values sequentially from memory location pr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:854785,load,load,854785,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"::Interface getInterface(Ast &ast) {; SymbolFlagsMap Symbols;; // Find all the symbols in the AST and for each of them; // add it to the Symbols map.; Symbols[mangler(someNameFromAST)] =; JITSymbolFlags(JITSymbolFlags::Exported | JITSymbolFlags::Callable);; return MaterializationUnit::Interface(std::move(Symbols), nullptr);; }; };. Take look at the source code of `Building A JIT's Chapter 4 <tutorial/BuildingAJIT4.html>`_ for a complete example. How to use ThreadSafeModule and ThreadSafeContext; -------------------------------------------------. ThreadSafeModule and ThreadSafeContext are wrappers around Modules and; LLVMContexts respectively. A ThreadSafeModule is a pair of a; std::unique_ptr<Module> and a (possibly shared) ThreadSafeContext value. A; ThreadSafeContext is a pair of a std::unique_ptr<LLVMContext> and a lock.; This design serves two purposes: providing a locking scheme and lifetime; management for LLVMContexts. The ThreadSafeContext may be locked to prevent; accidental concurrent access by two Modules that use the same LLVMContext.; The underlying LLVMContext is freed once all ThreadSafeContext values pointing; to it are destroyed, allowing the context memory to be reclaimed as soon as; the Modules referring to it are destroyed. ThreadSafeContexts can be explicitly constructed from a; std::unique_ptr<LLVMContext>:. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadSafeModules can be constructed from a pair of a std::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple ThreadSafeModules:. .. code-block:: c++. ThreadSafeModule TSM1(; std::make_unique<Module>(""M1"", *TSCtx.getContext()), TSCtx);. ThreadSafeModule TSM2(; std::make_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the co",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:28293,concurren,concurrent,28293,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['concurren'],['concurrent']
Performance,"::MinimizerOptions::SetDefaultMinimizer()`, or alternatively in the `.rootrc` file by adding for example `Root.Fitter: Minuit2` to select Minuit2. ### Code modernization by using `std::string` in RooFit interfaces. The following lesser-used RooFit functions now return a `std::string` instead of a `const char*`, potentially requiring the update of your code:. - [std::string RooCmdConfig::missingArgs() const](https://root.cern/doc/v628/classRooCmdConfig.html#aec50335293c45a507d347c604bf9651f); ### Uniquely identifying RooArgSet and RooDataSet objects. Before v6.28, it was ensured that no `RooArgSet` and `RooDataSet` objects on the heap were located at an address that had already been used for an instance of the same class before.; With v6.28, this is not guaranteed anymore.; Hence, if your code uses pointer comparisons to uniquely identify RooArgSet or RooDataSet instances, please consider using the new `RooArgSet::uniqueId()` or `RooAbsData::uniqueId()`. ### Introducing binned likelihood fit optimization in HistFactory. In a binned likelihood fit, it is possible to skip the PDF normalization when; the unnormalized binned PDF can be interpreted directly in terms of event; yields. This is now done by default for HistFactory models, which; results in great speedups for binned fits with many channels. Some RooFit users; like ATLAS were already using this for a long time. To disable this optimization when using the `hist2workspace` executable, add the `-disable_binned_fit_optimization` command line argument.; Directly in C++, you can also set the `binnedFitOptimization` to `false` in the; HistFactory configuration as follows:; ```C++; RooStats::HistFactory::MakeModelAndMeasurementFast(measurement, {.binnedFitOptimization=false});; ```; If your compiler doesn't support aggregate initialization with designators, you; need to create and edit the configuration struct explicitely:; ```C++; RooStats::HistFactory::HistoToWorkspaceFactoryFast::Configuration hfCfg;; hfCfg.binnedFit",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md:20122,optimiz,optimization,20122,README/ReleaseNotes/v628/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md,1,['optimiz'],['optimization']
Performance,"::get(opcode)::ImplicitUses``. Pre-colored registers impose; constraints on any register allocation algorithm. The register allocator must; make sure that none of them are overwritten by the values of virtual registers; while still alive. Mapping virtual registers to physical registers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. There are two ways to map virtual registers to physical registers (or to memory; slots). The first way, that we will call *direct mapping*, is based on the use; of methods of the classes ``TargetRegisterInfo``, and ``MachineOperand``. The; second way, that we will call *indirect mapping*, relies on the ``VirtRegMap``; class in order to insert loads and stores sending and getting values to and from; memory. The direct mapping provides more flexibility to the developer of the register; allocator; however, it is more error prone, and demands more implementation; work. Basically, the programmer will have to specify where load and store; instructions should be inserted in the target function being compiled in order; to get and store values in memory. To assign a physical register to a virtual; register present in a given operand, use ``MachineOperand::setReg(p_reg)``. To; insert a store instruction, use ``TargetInstrInfo::storeRegToStackSlot(...)``,; and to insert a load instruction, use ``TargetInstrInfo::loadRegFromStackSlot``. The indirect mapping shields the application developer from the complexities of; inserting load and store instructions. In order to map a virtual register to a; physical one, use ``VirtRegMap::assignVirt2Phys(vreg, preg)``. In order to map; a certain virtual register to memory, use; ``VirtRegMap::assignVirt2StackSlot(vreg)``. This method will return the stack; slot where ``vreg``'s value will be located. If it is necessary to map another; virtual register to the same stack slot, use; ``VirtRegMap::assignVirt2StackSlot(vreg, stack_location)``. One important point; to consider when using the indirect mapping, is that",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:64516,load,load,64516,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['load'],['load']
Performance,"::mapSectionAddress is called, MCJIT passes the call on to; RuntimeDyldImpl (via its Dyld member). RuntimeDyldImpl stores the new; address in an internal data structure but does not update the code at this; time, since other sections are likely to change. When the client is finished remapping section addresses, it will call; MCJIT::finalizeObject to complete the remapping process. Final Preparations; ==================. When MCJIT::finalizeObject is called, MCJIT calls; RuntimeDyld::resolveRelocations. This function will attempt to locate any; external symbols and then apply all relocations for the object. External symbols are resolved by calling the memory manager's; getPointerToNamedFunction method. The memory manager will return the; address of the requested symbol in the target address space. (Note, this; may not be a valid pointer in the host process.) RuntimeDyld will then; iterate through the list of relocations it has stored which are associated; with this symbol and invoke the resolveRelocation method which, through an; format-specific implementation, will apply the relocation to the loaded; section memory. Next, RuntimeDyld::resolveRelocations iterates through the list of; sections and for each section iterates through a list of relocations that; have been saved which reference that symbol and call resolveRelocation for; each entry in this list. The relocation list here is a list of; relocations for which the symbol associated with the relocation is located; in the section associated with the list. Each of these locations will; have a target location at which the relocation will be applied that is; likely located in a different section. .. image:: MCJIT-resolve-relocations.png. Once relocations have been applied as described above, MCJIT calls; RuntimeDyld::getEHFrameSection, and if a non-zero result is returned; passes the section data to the memory manager's registerEHFrames method.; This allows the memory manager to call any desired target-specific; func",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:7514,load,loaded,7514,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,1,['load'],['loaded']
Performance,"::unique_ptr<Module>; and a ThreadSafeContext value. ThreadSafeContext values may be shared between; multiple ThreadSafeModules:. .. code-block:: c++. ThreadSafeModule TSM1(; std::make_unique<Module>(""M1"", *TSCtx.getContext()), TSCtx);. ThreadSafeModule TSM2(; std::make_unique<Module>(""M2"", *TSCtx.getContext()), TSCtx);. Before using a ThreadSafeContext, clients should ensure that either the context; is only accessible on the current thread, or that the context is locked. In the; example above (where the context is never locked) we rely on the fact that both; ``TSM1`` and ``TSM2``, and TSCtx are all created on one thread. If a context is; going to be shared between threads then it must be locked before any accessing; or creating any Modules attached to it. E.g. .. code-block:: c++. ThreadSafeContext TSCtx(std::make_unique<LLVMContext>());. ThreadPool TP(NumThreads);; JITStack J;. for (auto &ModulePath : ModulePaths) {; TP.async(; [&]() {; auto Lock = TSCtx.getLock();; auto M = loadModuleOnContext(ModulePath, TSCtx.getContext());; J.addModule(ThreadSafeModule(std::move(M), TSCtx));; });; }. TP.wait();. To make exclusive access to Modules easier to manage the ThreadSafeModule class; provides a convenience function, ``withModuleDo``, that implicitly (1) locks the; associated context, (2) runs a given function object, (3) unlocks the context,; and (3) returns the result generated by the function object. E.g. .. code-block:: c++. ThreadSafeModule TSM = getModule(...);. // Dump the module:; size_t NumFunctionsInModule =; TSM.withModuleDo(; [](Module &M) { // <- Context locked before entering lambda.; return M.size();; } // <- Context unlocked after leaving.; );. Clients wishing to maximize possibilities for concurrent compilation will want; to create every new ThreadSafeModule on a new ThreadSafeContext. For this; reason a convenience constructor for ThreadSafeModule is provided that implicitly; constructs a new ThreadSafeContext value from a std::unique_ptr<LLVMContext>:.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:29775,load,loadModuleOnContext,29775,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['load'],['loadModuleOnContext']
Performance,":; """""""""""""""""""". The maximum value this operation can clamp to is the largest signed value; representable by the bit width of the arguments. The minimum value is the; smallest signed value representable by this bit width. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.sshl.sat.i4(i4 2, i4 1) ; %res = 4; %res = call i4 @llvm.sshl.sat.i4(i4 2, i4 2) ; %res = 7; %res = call i4 @llvm.sshl.sat.i4(i4 -5, i4 1) ; %res = -8; %res = call i4 @llvm.sshl.sat.i4(i4 -1, i4 1) ; %res = -2. '``llvm.ushl.sat.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.ushl.sat``; on integers or vectors of integers of any bit width. ::. declare i16 @llvm.ushl.sat.i16(i16 %a, i16 %b); declare i32 @llvm.ushl.sat.i32(i32 %a, i32 %b); declare i64 @llvm.ushl.sat.i64(i64 %a, i64 %b); declare <4 x i32> @llvm.ushl.sat.v4i32(<4 x i32> %a, <4 x i32> %b). Overview; """""""""""""""""". The '``llvm.ushl.sat``' family of intrinsic functions perform unsigned; saturating left shift on the first argument. Arguments; """""""""""""""""""". The arguments (``%a`` and ``%b``) and the result may be of integer types of any; bit width, but they must have the same bit width. ``%a`` is the value to be; shifted, and ``%b`` is the amount to shift by. If ``b`` is (statically or; dynamically) equal to or larger than the integer bit width of the arguments,; the result is a :ref:`poison value <poisonvalues>`. If the arguments are; vectors, each vector element of ``a`` is shifted by the corresponding shift; amount in ``b``. Semantics:; """""""""""""""""""". The maximum value this operation can clamp to is the largest unsigned value; representable by the bit width of the arguments. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.ushl.sat.i4(i4 2, i4 1) ; %res = 4; %res = call i4 @llvm.ushl.sat.i4(i4 3, i4 3) ; %res = 15. Fixed Point Arithmetic Intrinsics; ---------------------------------. A fixed point number represents a real data type for a number that has",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:618006,perform,perform,618006,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,":; """""""""""""""""". The '``llvm.vp.trunc``' intrinsic truncates its first operand to the return; type. The operation has a mask and an explicit vector length parameter. Arguments:; """""""""""""""""""". The '``llvm.vp.trunc``' intrinsic takes a value to cast as its first operand.; The return type is the type to cast the value to. Both types must be vector of; :ref:`integer <t_integer>` type. The bit size of the value must be larger than; the bit size of the return type. The second operand is the vector mask. The; return type, the value to cast, and the vector mask have the same number of; elements. The third operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.trunc``' intrinsic truncates the high order bits in value and; converts the remaining bits to return type. Since the source size must be larger; than the destination size, '``llvm.vp.trunc``' cannot be a *no-op cast*. It will; always truncate bits. The conversion is performed on lane positions below the; explicit vector length and where the vector mask is true. Masked-off lanes are; ``poison``. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i16> @llvm.vp.trunc.v4i16.v4i32(<4 x i32> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = trunc <4 x i32> %a to <4 x i16>; %also.r = select <4 x i1> %mask, <4 x i16> %t, <4 x i16> poison. .. _int_vp_zext:. '``llvm.vp.zext.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.zext.v16i32.v16i16 (<16 x i16> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.zext.nxv4i32.nxv4i16 (<vscale x 4 x i16> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". The '``llvm.vp.zext``' intrinsic zero extends its first operand to the return; type. The operation has a mask and an explicit vector length parameter. Arguments:; """""""""""""""""""". The '``llvm.vp.zext``' intrin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:797117,perform,performed,797117,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,":; """""""""""""". ::. declare void @llvm.ubsantrap(i8 immarg) cold noreturn nounwind. Overview:; """""""""""""""""". The '``llvm.ubsantrap``' intrinsic. Arguments:; """""""""""""""""""". An integer describing the kind of failure detected. Semantics:; """""""""""""""""""". This intrinsic is lowered to code which is intended to cause an execution trap,; embedding the argument into encoding of that trap somehow to discriminate; crashes if possible. Equivalent to ``@llvm.trap`` for targets that do not support this behaviour. '``llvm.stackprotector``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.stackprotector(ptr <guard>, ptr <slot>). Overview:; """""""""""""""""". The ``llvm.stackprotector`` intrinsic takes the ``guard`` and stores it; onto the stack at ``slot``. The stack slot is adjusted to ensure that it; is placed on the stack before local variables. Arguments:; """""""""""""""""""". The ``llvm.stackprotector`` intrinsic requires two pointer arguments.; The first argument is the value loaded from the stack guard; ``@__stack_chk_guard``. The second variable is an ``alloca`` that has; enough space to hold the value of the guard. Semantics:; """""""""""""""""""". This intrinsic causes the prologue/epilogue inserter to force the position of; the ``AllocaInst`` stack slot to be before local variables on the stack. This is; to ensure that if a local variable on the stack is overwritten, it will destroy; the value of the guard. When the function exits, the guard on the stack is; checked against the original guard by ``llvm.stackprotectorcheck``. If they are; different, then ``llvm.stackprotectorcheck`` causes the program to abort by; calling the ``__stack_chk_fail()`` function. '``llvm.stackguard``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare ptr @llvm.stackguard(). Overview:; """""""""""""""""". The ``llvm.stackguard`` intrinsic returns the system stack guard value. It should not be generated by frontends, since it is only for internal usage.; The reason why we crea",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:930061,load,loaded,930061,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,":; C() { p = new int; }; ~C() { delete p; }; };. WinAPI. Name, DescriptionExampleProgress. WinAPI.CreateProcess; (C); CreateProcess(): if the first parameter ; lpApplicationName is NULL then the executable name must be in the; white space-delimited string pointed to by lpCommandLine.; If the executable or path name has a space in it, there is a risk that a; different executable could be run because of the way the function parses; spaces.; Source: ; MSDN: CreateProcess function, Security Remarks. #include <windows.h>. void test() {; STARTUPINFO si;; PROCESS_INFORMATION pi;; CreateProcess(NULL, TEXT(""C:\\Program Files\\App -L -S""),; NULL, NULL, TRUE, 0, NULL, NULL, &si, π);; // warn; }. WinAPI.LoadLibrary; (C); The SearchPath() function is used to retrieve a path to a DLL for; a subsequent LoadLibrary() call.; Source: ; MSDN: LoadLibrary function, Security Remarks. #include <windows.h>. HINSTANCE test() {; char filePath[100];; SearchPath(NULL, ""file.dll"", NULL, 100, filePath, NULL);; return LoadLibrary(filePath); // warn; }. WinAPI.WideCharToMultiByte; (C); Buffer overrun while calling WideCharToMultiByte(). The size of; the input buffer equals the number of characters in the Unicode string, while; the size of the output buffer equals the number of bytes.; Source: ; MSDN: WideCharToMultiByte function. #include <windows.h>. void test() {; wchar_t ws[] = L""abc"";; char s[3];; WideCharToMultiByte(CP_UTF8, 0, ws, -1, s,; 3, NULL, NULL); // warn; }. optimization. Name, DescriptionExampleProgress. optimization.PassConstObjByValue; (C, C++); Optimization: It is more effective to pass constant parameter by reference to; avoid unnecessary object copying. struct A {};. void f(const struct A a); // warn. optimization.PostfixIncIter; (C++); Optimization: It is more effective to use prefix increment operator with; iterator.; Source: Scott Meyers ""More Effective C++"", item 6:; Distinguish between prefix and postfix forms of increment and decrement; operators. #include <vector>. void",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html:26754,Load,LoadLibrary,26754,interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,1,['Load'],['LoadLibrary']
Performance,":; int foo (int x) { return (x & 65535) | 255; }. Should compile into:. _foo:; movzwl 4(%esp), %eax; orl $255, %eax; ret. instead of:; _foo:; 	movl	$65280, %eax; 	andl	4(%esp), %eax; 	orl	$255, %eax; 	ret. //===---------------------------------------------------------------------===//. We're codegen'ing multiply of long longs inefficiently:. unsigned long long LLM(unsigned long long arg1, unsigned long long arg2) {; return arg1 * arg2;; }. We compile to (fomit-frame-pointer):. _LLM:; 	pushl	%esi; 	movl	8(%esp), %ecx; 	movl	16(%esp), %esi; 	movl	%esi, %eax; 	mull	%ecx; 	imull	12(%esp), %esi; 	addl	%edx, %esi; 	imull	20(%esp), %ecx; 	movl	%esi, %edx; 	addl	%ecx, %edx; 	popl	%esi; 	ret. This looks like a scheduling deficiency and lack of remat of the load from; the argument area. ICC apparently produces:. movl 8(%esp), %ecx; imull 12(%esp), %ecx; movl 16(%esp), %eax; imull 4(%esp), %eax ; addl %eax, %ecx ; movl 4(%esp), %eax; mull 12(%esp) ; addl %ecx, %edx; ret. Note that it remat'd loads from 4(esp) and 12(esp). See this GCC PR:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=17236. //===---------------------------------------------------------------------===//. We can fold a store into ""zeroing a reg"". Instead of:. xorl %eax, %eax; movl %eax, 124(%esp). we should get:. movl $0, 124(%esp). if the flags of the xor are dead. Likewise, we isel ""x<<1"" into ""add reg,reg"". If reg is spilled, this should; be folded into: shl [mem], 1. //===---------------------------------------------------------------------===//. In SSE mode, we turn abs and neg into a load from the constant pool plus a xor; or and instruction, for example:. 	xorpd	LCPI1_0, %xmm2. However, if xmm2 gets spilled, we end up with really ugly code like this:. 	movsd	(%esp), %xmm0; 	xorpd	LCPI1_0, %xmm0; 	movsd	%xmm0, (%esp). Since we 'know' that this is a 'neg', we can actually ""fold"" the spill into; the neg/abs instruction, turning it into an *integer* operation, like this:. 	xorl 2147483648, [mem+4] ## 214748364",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:21737,load,loads,21737,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,2,['load'],['loads']
Performance,":CreateModelUpdater()`), the following steps are taken:. 1. On calling `RUpdater::BeginUpdate()`, all `REntry` instances belonging to the underlying RNTupleModel are invalidated.; 2. After adding the desired additional fields, calling `RUpdater::CommitUpdate()` will add the relevant fields to the footer's [schema extension record frame](./BinaryFormatSpecification.md#schema-extensions-record-frame).; 1. The principal columns of top-level fields and record subfields will have a non-zero first element index.; These columns are referred to as ""deferred columns"".; In particular, columns in a subfield tree of collections or variants are _not_ stored as deferred columns (see next point).; 2. All other columns belonging to the added (sub)fields will be written as usual.; 3. `RNTuple(Writer|Model)::CreateEntry()` or `RNTupleModel::CreateBareEntry()` must be used to create an `REntry` matching the new model.; 4. Writing continues as described in steps 2-5 above. ### Reading Case; The reverse process is performed on reading (e.g. `RNTupleReader::LoadEntry()`, `RNTupleView` call operator). By default, the page source uses an `RClusterPool` to asynchronously read-ahead data.; When a page of a certain cluster is required, the cluster pool reads pages of _active_ columns.; For instance, if only certain fields are used (e.g., through an imposed model), only the pages of columns connected to those fields are read.; Columns can be dynamically added (e.g. during event iteration, a new field view is created in a reader).; The cluster pool reads ahead a limited number of clusters given by the _cluster bunch size_ option (default = 1).; The read-ahead uses vector reads.; For the file backend, it additionally coalesces close read requests and uses uring reads when available. The page source can be restricted to a certain entry range.; This allows for optimizing the page lists that are being read.; Additionally, it allows for optimizing the cluster pool to not read-ahead beyond the limits",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md:21949,perform,performed,21949,tree/ntuple/v7/doc/Architecture.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md,1,['perform'],['performed']
Performance,":Math:::GSLIntegrator` |. #### ROOT::Math:::GaussIntegrator. It uses the most basic Gaussian integration algorithm, it uses the 8-point and the 16-point Gaussian; quadrature approximations. It is derived from the `DGAUSS` routine of the *CERNLIB* by S. Kolbig.; This class; Here is an example of using directly the `GaussIntegrator` class. ```{.cpp}; #include ""TF1.h""; #include ""Math/WrappedTF1.h""; #include ""Math/GaussIntegrator.h"". int main(); {; TF1 f(""Sin Function"", ""sin(x)"", 0, TMath::Pi());; ROOT::Math::WrappedTF1 wf1(f);. ROOT::Math::GaussIntegrator ig;. ig.SetFunction(wf1, false);; ig.SetRelTolerance(0.001);. cout << ig.Integral(0, TMath::PiOver2()) << endl;. return 0;; }; ```; #### ROOT::Math::GaussLegendreIntegrator. This class implementes the Gauss-Legendre quadrature formulas. This sort of numerical methods requieres that the user specifies the number of intermediate function points; used in the calculation of the integral. It will automatically determine the coordinates and weights of such points before performing the integration.; We can use the example above, but replacing the creation of a `ROOT::Math::GaussIntegrator` object with `ROOT::Math::GaussLegendreIntegrator`. #### ROOT::Math::GSLIntegrator. This is a wrapper for the *QUADPACK* integrator implemented in the GSL library. It supports several integration methods that can be chosen in construction time.; The default type is adaptive integration with singularity applying a Gauss-Kronrod 21-point integration rule. For a detail description of the GSL methods visit the GSL user guide; This class implements the best algorithms for numerical integration for one dimensional functions. We encourage the use it as the main option, bearing in mind that it uses code from the; GSL library, wich is provided in the *MathMore* library of ROOT. The interface to use is the same as above. We have now the possibility to specify a different integration algorithm in the constructor of the `ROOT::Math::GSLIntegrator` clas",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:54377,perform,performing,54377,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['perform'],['performing']
Performance,":Math::SVector`** classes for describing matrices and vectors of; arbitrary dimensions and of arbitrary type. The classes are templated on; the scalar type and on the size, like number of rows and columns for a; matrix . Therefore, the matrix/vector dimension has to be known at; compile time. An advantage of using the dimension as template parameters; is that the correctness of dimension in the matrix/vector operations can; be checked at compile time. `SMatrix` supports, since ROOT v5.10, symmetric matrices using a storage; class (**`ROOT::Math::MatRepSym`**) which contains only the `N*(N+1)/2`; independent element of a `NxN` symmetric matrix. It is not in the; mandate of this package to provide complete linear algebra; functionality. It provides basic matrix and vector functions such as; matrix-matrix, matrix-vector, vector-vector operations, plus some extra; functionality for square matrices, like inversion and determinant; calculation. The inversion is based on the optimized Cramer method for; squared matrices of size up to `6x6`. The `SMatrix` package contains only header files. Normally one does not; need to build any library. In the ROOT distribution a library,; `libSmatrix` is produced with the C++ dictionary information for squared; and symmetric matrices and vectors up to dimension 7 and based on; **`Double_t`**, **`Float_t`** and **`Double32_t`**. The following; paragraphs describe the main characteristics of the matrix and vector; classes. More detailed information about the `SMatrix` classes API is; available in the; [online reference documentation](online reference documentation). ### Example: Vector Class (SVector). The template class **`ROOT::Math::SVector`** represents `n`-dimensional; vectors for objects of arbitrary type. This class has 2 template; parameters, which define at compile time, its properties: 1) type of the; contained elements (for example *float* or *double*); 2) size of the; vector. The use of this dictionary is mandatory if one want ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:101241,optimiz,optimized,101241,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['optimiz'],['optimized']
Performance,":arc-term:`bridged cast` is a C-style cast annotated with one of three; keywords:. * ``(__bridge T) op`` casts the operand to the destination type ``T``. If; ``T`` is a retainable object pointer type, then ``op`` must have a; non-retainable pointer type. If ``T`` is a non-retainable pointer type,; then ``op`` must have a retainable object pointer type. Otherwise the cast; is ill-formed. There is no transfer of ownership, and ARC inserts no retain; operations.; * ``(__bridge_retained T) op`` casts the operand, which must have retainable; object pointer type, to the destination type, which must be a non-retainable; pointer type. ARC retains the value, subject to the usual optimizations on; local values, and the recipient is responsible for balancing that +1.; * ``(__bridge_transfer T) op`` casts the operand, which must have; non-retainable pointer type, to the destination type, which must be a; retainable object pointer type. ARC will release the value at the end of; the enclosing full-expression, subject to the usual optimizations on local; values. These casts are required in order to transfer objects in and out of ARC; control; see the rationale in the section on :ref:`conversion of retainable; object pointers <arc.objects.restrictions.conversion>`. Using a ``__bridge_retained`` or ``__bridge_transfer`` cast purely to convince; ARC to emit an unbalanced retain or release, respectively, is poor form. .. _arc.objects.restrictions:. Restrictions; ------------. .. _arc.objects.restrictions.conversion:. Conversion of retainable object pointers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. In general, a program which attempts to implicitly or explicitly convert a; value of retainable object pointer type to any non-retainable type, or; vice-versa, is ill-formed. For example, an Objective-C object pointer shall; not be converted to ``void*``. As an exception, cast to ``intptr_t`` is; allowed because such casts are not transferring ownership. The :ref:`bridged; casts <arc.objec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:23853,optimiz,optimizations,23853,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimizations']
Performance,":function<std::shared_ptr<Module>(std::shared_ptr<Module>)>;. IRTransformLayer<decltype(CompileLayer), OptimizeFunction> OptimizeLayer;. std::unique_ptr<JITCompileCallbackManager> CompileCallbackManager;; CompileOnDemandLayer<decltype(OptimizeLayer)> CODLayer;. public:; using ModuleHandle = decltype(CODLayer)::ModuleHandleT;. First we need to include the CompileOnDemandLayer.h header, then add two new; members: a std::unique_ptr<JITCompileCallbackManager> and a CompileOnDemandLayer,; to our class. The CompileCallbackManager member is used by the CompileOnDemandLayer; to create the compile callback needed for each function. .. code-block:: c++. KaleidoscopeJIT(); : TM(EngineBuilder().selectTarget()), DL(TM->createDataLayout()),; ObjectLayer([]() { return std::make_shared<SectionMemoryManager>(); }),; CompileLayer(ObjectLayer, SimpleCompiler(*TM)),; OptimizeLayer(CompileLayer,; [this](std::shared_ptr<Module> M) {; return optimizeModule(std::move(M));; }),; CompileCallbackManager(; orc::createLocalCompileCallbackManager(TM->getTargetTriple(), 0)),; CODLayer(OptimizeLayer,; [this](Function &F) { return std::set<Function*>({&F}); },; *CompileCallbackManager,; orc::createLocalIndirectStubsManagerBuilder(; TM->getTargetTriple())) {; llvm::sys::DynamicLibrary::LoadLibraryPermanently(nullptr);; }. Next we have to update our constructor to initialize the new members. To create; an appropriate compile callback manager we use the; createLocalCompileCallbackManager function, which takes a TargetMachine and an; ExecutorAddr to call if it receives a request to compile an unknown; function. In our simple JIT this situation is unlikely to come up, so we'll; cheat and just pass '0' here. In a production quality JIT you could give the; address of a function that throws an exception in order to unwind the JIT'd; code's stack. Now we can construct our CompileOnDemandLayer. Following the pattern from; previous layers we start by passing a reference to the next layer down in our; stack -- ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:5178,optimiz,optimizeModule,5178,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,1,['optimiz'],['optimizeModule']
Performance,":sup:`1` Visual Studio; Windows x64 x86-64 Visual Studio; ================== ===================== =============. .. note::. #. Code generation supported for Pentium processors and up; #. Code generation supported for 32-bit ABI only; #. To use LLVM modules on Win32-based system, you may configure LLVM; with ``-DBUILD_SHARED_LIBS=On``. Note that Debug builds require a lot of time and disk space. An LLVM-only build; will need about 1-3 GB of space. A full build of LLVM and Clang will need around; 15-20 GB of disk space. The exact space requirements will vary by system. (It; is so large because of all the debugging information and the fact that the; libraries are statically linked into multiple tools). If you are space-constrained, you can build only selected tools or only; selected targets. The Release build requires considerably less space. The LLVM suite *may* compile on other platforms, but it is not guaranteed to do; so. If compilation is successful, the LLVM utilities should be able to; assemble, disassemble, analyze, and optimize LLVM bitcode. Code generation; should work as well, although the generated native code may not work on your; platform. Software; --------. Compiling LLVM requires that you have several software packages installed. The; table below lists those required packages. The Package column is the usual name; for the software package that LLVM depends on. The Version column provides; ""known to work"" versions of the package. The Notes column describes how LLVM; uses the package and provides other details. =========================================================== ============ ==========================================; Package Version Notes; =========================================================== ============ ==========================================; `CMake <http://cmake.org/>`__ >=3.20.0 Makefile/workspace generator; `python <http://www.python.org/>`_ >=3.6 Automated test suite\ :sup:`1`; `zlib <http://zlib.net>`_ >=1.2.3.4 Compression libr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst:9920,optimiz,optimize,9920,interpreter/llvm-project/llvm/docs/GettingStarted.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst,1,['optimiz'],['optimize']
Performance,"; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.smul.fix.i4(i4 3, i4 2, i32 0) ; %res = 6 (2 x 3 = 6); %res = call i4 @llvm.smul.fix.i4(i4 3, i4 2, i32 1) ; %res = 3 (1.5 x 1 = 1.5); %res = call i4 @llvm.smul.fix.i4(i4 3, i4 -2, i32 1) ; %res = -3 (1.5 x -1 = -1.5). ; The result in the following could be rounded up to -2 or down to -2.5; %res = call i4 @llvm.smul.fix.i4(i4 3, i4 -3, i32 1) ; %res = -5 (or -4) (1.5 x -1.5 = -2.25). '``llvm.umul.fix.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.umul.fix``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.umul.fix.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.umul.fix.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.umul.fix.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.umul.fix.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.umul.fix``' family of intrinsic functions perform unsigned; fixed point multiplication on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. The arguments may also work with; int vectors of the same length and int size. ``%a`` and ``%b`` are the two; values that will undergo unsigned fixed point multiplication. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs unsigned fixed point multiplication on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. It is undefined behavior if the result value does not fit within the range of; the fixed point type. Examples; """""""""""""""""". .. code-block",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:623607,perform,perform,623607,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"; # prebuilt/A.pcm prebuilt/B.pcm. Note that with explicit or prebuilt modules, we are responsible for, and should be particularly careful about the compatibility of our modules.; Using mismatching compilation options and modules may lead to issues. .. code-block:: sh. clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -DENABLE_A; # use.c:4:10: warning: implicit declaration of function 'a' is invalid in C99 [-Wimplicit-function-declaration]; # return a(x);; # ^; # 1 warning generated. So we need to maintain multiple versions of prebuilt modules. We can do so using a manual module mapping, or pointing to a different prebuilt module cache path. For example:. .. code-block:: sh. rm -rf prebuilt ; mkdir prebuilt ; rm -rf prebuilt_a ; mkdir prebuilt_a; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt -fdisable-module-hash; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt_a -fdisable-module-hash -DENABLE_A; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt_a -DENABLE_A. Instead of managing the different module versions manually, we can build implicit modules in a given cache path (using ``-fmodules-cache-path``), and reuse them as prebuilt implicit modules by passing ``-fprebuilt-module-path`` and ``-fprebuilt-implicit-modules``. .. code-block:: sh. rm -rf prebuilt; mkdir prebuilt; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt -DENABLE_A; find prebuilt -name ""*.pcm""; # prebuilt/1AYBIGPM8R2GA/A-3L1K4LUA6O31.pcm; # prebuilt/1AYBIGPM8R2GA/B-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/A-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/B-3L1K4LUA6O31.pcm; clang -cc1 -emit-obj -o use",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:21890,cache,cache-path,21890,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['cache'],['cache-path']
Performance,"; $$. Remarkable feature of algorithm is the technique for step restriction.; For an initial value of parameter ${\vec\theta}^0$ a parallelepiped; $P_0$ is built with the center at ${\vec\theta}^0$ and axes parallel; to coordinate axes $\theta_i$. The lengths of parallelepiped sides; along i-th axis is $2b_i$, where $b_i$ is such a value that the; functions $f_j(\vec\theta)$ are quasi-linear all over the; parallelepiped. FUMILI takes into account simple linear inequalities in the form:. $$ \theta_i^{min}\le\theta_i\le\theta^{max}_i$$. They form parallelepiped $P$ ($P_0$ may be deformed by $P$). Very; similar step formulae are used in FUMILI for negative logarithm of; the likelihood function with the same idea - linearization of function; argument. ## Neural Networks. ### Introduction. Neural Networks are used in various fields for data analysis and; classification, both for research and commercial institutions. Some; randomly chosen examples are image analysis, financial movements'; predictions and analysis, or sales forecast and product shipping; optimization. In particles physics neural networks are mainly used for; classification tasks (signal over background discrimination). A vast; majority of commonly used neural networks are multilayer perceptrons.; This implementation of multilayer perceptrons is inspired from the; `MLPfit` package, which remains one of the fastest tools for neural; networks studies. ### The MLP. The multilayer perceptron is a simple feed-forward network with the; following structure showed on the left. ![](pictures/0300008D.png). It is made of neurons characterized by a bias and weighted links in; between - let's call those links synapses. The input neurons receive; the inputs, normalize them and forward them to the first hidden layer.; Each neuron in any subsequent layer first computes a linear; combination of the outputs of the previous layer. The output of the; neuron is then function of that combination with f being linear for; output ne",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md:70114,optimiz,optimization,70114,documentation/users-guide/FittingHistograms.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md,1,['optimiz'],['optimization']
Performance,"; % llvm-lit .; # Use the profile data for compilation and actual benchmark run:; % cmake -DTEST_SUITE_PROFILE_GENERATE=OFF \; -DTEST_SUITE_PROFILE_USE=ON \; -DTEST_SUITE_RUN_TYPE=ref \; .; % make; % llvm-lit -o result.json .; ```. To use Clang frontend's PGO instead of LLVM IR PGO, set `-DTEST_SUITE_USE_IR_PGO=OFF`. The `TEST_SUITE_RUN_TYPE` setting only affects the SPEC benchmark suites. Cross Compilation and External Devices; --------------------------------------. ### Compilation. CMake allows to cross compile to a different target via toolchain files. More; information can be found here:. - [https://llvm.org/docs/lnt/tests.html#cross-compiling](https://llvm.org/docs/lnt/tests.html#cross-compiling). - [https://cmake.org/cmake/help/latest/manual/cmake-toolchains.7.html](https://cmake.org/cmake/help/latest/manual/cmake-toolchains.7.html). Cross compilation from macOS to iOS is possible with the; `test-suite/cmake/caches/target-target-*-iphoneos-internal.cmake` CMake cache; files; this requires an internal iOS SDK. ### Running. There are two ways to run the tests in a cross compilation setting:. - Via SSH connection to an external device: The `TEST_SUITE_REMOTE_HOST` option; should be set to the SSH hostname. The executables and data files need to be; transferred to the device after compilation. This is typically done via the; `rsync` make target. After this, the lit runner can be used on the host; machine. It will prefix the benchmark and verification command lines with an; `ssh` command. Example:. ```bash; % cmake -G Ninja -D CMAKE_C_COMPILER=path/to/clang \; -C ../test-suite/cmake/caches/target-arm64-iphoneos-internal.cmake \; -D CMAKE_BUILD_TYPE=Release \; -D TEST_SUITE_REMOTE_HOST=mydevice \; ../test-suite; % ninja; % ninja rsync; % llvm-lit -j1 -o result.json .; ```. - You can specify a simulator for the target machine with the; `TEST_SUITE_RUN_UNDER` setting. The lit runner will prefix all benchmark; invocations with it. Running the test-suite via LNT; ------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md:11462,cache,cache,11462,interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,1,['cache'],['cache']
Performance,"; * - `Compiler Research - Calling C++ libraries from a D-written DSL: A cling/cppyy-based approach <https://www.youtube.com/watch?v=7teqrCNzrD8>`_; - *Alexandru Militaru* 2021 Compiler-Research Meeting; - This video presents D and C++ interoperability through SIL-Cling architecture. .. list-table:: Interactive CUDA C++ with Cling:; :widths: 25 25 50; :header-rows: 1. * - Link; - Info ; - Description; * - `Adding CUDA® Support to Cling: JIT Compile to GPUs <https://www.youtube.com/watch?v=XjjZRhiFDVs>`_; - *Simeon Ehrig* 2020 LLVM Developer Meeting; - Interactive CUDA-C++ through Cling is presented. Cling-CUDA architecture is discussed in detail, and an example of interactive simulation for laser plasma applications is shown. . .. list-table:: C++ in Jupyter Notebook - Xeus Cling:; :widths: 25 25 50; :header-rows: 1; ; * - Link; - Info ; - Description; * - `Interactive C++ code development using C++Explorer and GitHub Classroom for educational purposes <https://www.youtube.com/watch?v=HBgF2Yr0foA>`_; - *Patrick Diehl* 2020 Youtube; - C++Explorer is a novel teaching environment based on Jupyterhub and Cling, adapted to teaching C++ programming and source code management.; * - `Deep dive into the Xeus-based Cling kernel for Jupyter <https://www.youtube.com/watch?v=kx3wvKk4Qss>`_; - *Vassil Vassilev* 2021 Youtube; - Xeus-Cling is a Cling-based notebook kernel which delivers interactive C++. ; * - `Xeus-Cling: Run C++ code in Jupyter Notebook <https://www.youtube.com/watch?v=4fcKlJ_5QQk>`_ ; - *LearnOpenCV* 2019 Youtube; - In this demo, you will learn an example of C++ code in Jupyter Notebook using Xeus-Cling kernel. . .. list-table:: Clad:; :widths: 25 25 50; :header-rows: 1; ; * - Link; - Info ; - Description; * - `Clad: Automatic differentiation plugin for C++ <https://clad.readthedocs.io/en/latest/index.html>`_ ; - Read The Docs webpage; - Clad is a plugin for Cling. It allows to perform Automatic Differentiation (AD) on multivariate functions and functor objects. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/references.rst:7765,perform,perform,7765,interpreter/cling/docs/chapters/references.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/references.rst,1,['perform'],['perform']
Performance,"; * Do not automatically setup read cache during `TTree::Fill()`. This fixes [ROOT-8031].; * Make sure the option ""PARA"" in `TTree::Draw` is used with at least tow variables [ROOT-8196].; * The with `goff` option one can use as many variables as needed. There no more; limitation, like with the options `para`and `candle`.; * Fix detection of errors that appears in nested TTreeFormula [ROOT-8218]; * Better basket size optimization by taking into account meta data and rounding up to next 512 bytes, ensuring a complete cluster fits into a single basket. ### Fast Cloning. We added a cache specifically for the fast option of the TTreeCloner to significantly reduce the run-time when fast-cloning remote files to address [ROOT-5078]. It can be controlled from the `TTreeCloner`, `TTree::CopyEntries` or `hadd` interfaces. The new cache is enabled by default, to update the size of the cache or disable it from `TTreeCloner` use: `TTreeCloner::SetCacheSize`. To do the same from `TTree::CopyEntries` add to the option string ""cachesize=SIZE"". To update the size of the cache or disable it from `hadd`, use the command line option `-cachesize SIZE`. `SIZE` shouyld be given in number bytes and can be expressed in 'human readable form' (number followed by size unit like MB, MiB, GB or GiB, etc. or SIZE can be set zero to disable the cache. ### Other Changes. * Update `TChain::LoadTree` so that the user call back routine is actually called for each input file even those containing `TTree` objects with no entries.; * Repair setting the branch address of a leaflist style branch taking directly the address of the struct. (Note that leaflist is nonetheless still deprecated and declaring the struct to the interpreter and passing the object directly to create the branch is much better).; * Provide an implicitly parallel implementation of `TTree::GetEntry`. The approach is based on creating a task per top-level branch in order to do the reading, unzipping and deserialisation in parallel. In add",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md:9934,cache,cachesize,9934,README/ReleaseNotes/v608/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md,1,['cache'],['cachesize']
Performance,"; - 1:; Use a single thread only (disable multi-threading); - all:; Use one thread per logical core (uses all hyper-threads). Incremental; -----------; .. _incremental:. ThinLTO supports fast incremental builds through the use of a cache,; which currently must be enabled through a linker option. - gold (as of LLVM 4.0):; ``-Wl,-plugin-opt,cache-dir=/path/to/cache``; - ld64 (supported since clang 3.9 and Xcode 8) and Mach-O ld64.lld (as of LLVM; 15.0):; ``-Wl,-cache_path_lto,/path/to/cache``; - ELF ld.lld (as of LLVM 5.0):; ``-Wl,--thinlto-cache-dir=/path/to/cache``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocache:/path/to/cache``. Cache Pruning; -------------. To help keep the size of the cache under control, ThinLTO supports cache; pruning. Cache pruning is supported with gold, ld64, and lld, but currently only; gold and lld allow you to control the policy with a policy string. The cache; policy must be specified with a linker option. - gold (as of LLVM 6.0):; ``-Wl,-plugin-opt,cache-policy=POLICY``; - ELF ld.lld (as of LLVM 5.0), Mach-O ld64.lld (as of LLVM 15.0):; ``-Wl,--thinlto-cache-policy=POLICY``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocachepolicy:POLICY``. A policy string is a series of key-value pairs separated by ``:`` characters.; Possible key-value pairs are:. - ``cache_size=X%``: The maximum size for the cache directory is ``X`` percent; of the available space on the disk. Set to 100 to indicate no limit,; 50 to indicate that the cache size will not be left over half the available; disk space. A value over 100 is invalid. A value of 0 disables the percentage; size-based pruning. The default is 75%. - ``cache_size_bytes=X``, ``cache_size_bytes=Xk``, ``cache_size_bytes=Xm``,; ``cache_size_bytes=Xg``:; Sets the maximum size for the cache directory to ``X`` bytes (or KB, MB,; GB respectively). A value over the amount of available space on the disk; will be reduced to the amount of available space. A value of 0 disables; the byte size-based pruning. T",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:5398,cache,cache-policy,5398,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,1,['cache'],['cache-policy']
Performance,"; - 32-bit floating-point value. * - ``double``; - 64-bit floating-point value. * - ``fp128``; - 128-bit floating-point value (113-bit significand). * - ``x86_fp80``; - 80-bit floating-point value (X87). * - ``ppc_fp128``; - 128-bit floating-point value (two 64-bits). The binary format of half, float, double, and fp128 correspond to the; IEEE-754-2008 specifications for binary16, binary32, binary64, and binary128; respectively. X86_amx Type; """""""""""""""""""""""". :Overview:. The x86_amx type represents a value held in an AMX tile register on an x86; machine. The operations allowed on it are quite limited. Only few intrinsics; are allowed: stride load and store, zero and dot product. No instruction is; allowed for this type. There are no arguments, arrays, pointers, vectors; or constants of this type. :Syntax:. ::. x86_amx. X86_mmx Type; """""""""""""""""""""""". :Overview:. The x86_mmx type represents a value held in an MMX register on an x86; machine. The operations allowed on it are quite limited: parameters and; return values, load and store, and bitcast. User-specified MMX; instructions are represented as intrinsic or asm calls with arguments; and/or results of this type. There are no arrays, vectors or constants; of this type. :Syntax:. ::. x86_mmx. .. _t_pointer:. Pointer Type; """""""""""""""""""""""". :Overview:. The pointer type ``ptr`` is used to specify memory locations. Pointers are; commonly used to reference objects in memory. Pointer types may have an optional address space attribute defining; the numbered address space where the pointed-to object resides. For; example, ``ptr addrspace(5)`` is a pointer to address space 5.; In addition to integer constants, ``addrspace`` can also reference one of the; address spaces defined in the :ref:`datalayout string<langref_datalayout>`.; ``addrspace(""A"")`` will use the alloca address space, ``addrspace(""G"")``; the default globals address space and ``addrspace(""P"")`` the program address; space. The default address space is number zero. The sema",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:170468,load,load,170468,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. flat_atomic sc1=1; 4. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; have; completed b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:324768,load,loads,324768,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. 3. buffer_gl0_inv. - If OpenCL omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkmcnt(0) &; vm/vscnt(0). - If CU wavefront execution; mode, omit vm/vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; - system 2. s_waitcnt vm/vscnt(0). - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - agent - generic 1. flat_atomic; - system 2. s_waitcnt vm/vscnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; following; buffer_gl*_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL and; address space is; not generic, omit;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:350402,cache,caches,350402,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; atomicrmw acquire - workgroup - local 1. ds/flat_atomic; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. atomicrmw acquire - agent - global 1. buffer/global_atomic; - system 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - agent - generic 1. flat_atomic; - system 2. s_waitcnt vmcnt(0) &; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:215369,load,loads,215369,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_atomic; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:263166,load,load,263166,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; - Must happen before; the following; buffer_gl0_inv.; - Ensures that the; acquire-fence-paired; atomic has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; acquire-fence-paired-atomic. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acq_rel - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_gl*_inv.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the caches. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_gl0_inv;;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:372318,load,load,372318,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_atomic; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be spli",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:263041,load,load,263041,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; ----------------------. SequentiallyConsistent (``seq_cst`` in IR) provides Acquire semantics for loads; and Release semantics for stores. Additionally, it guarantees that a total; ordering exists between all SequentiallyConsistent operations. Relevant standard; This corresponds to the C++/C ``memory_order_seq_cst``, Java volatile, and; the gcc-compatible ``__sync_*`` builtins which do not specify otherwise. Notes for frontends; If a frontend is exposing atomic operations, these are much easier to reason; about for the programmer than other kinds of operations, and using them is; generally a practical performance tradeoff. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. For; SequentiallyConsistent loads and stores, the same reorderings are allowed as; for Acquire loads and Release stores, except that SequentiallyConsistent; operations may not be reordered. Notes for code generation; SequentiallyConsistent loads minimally require the same barriers as Acquire; operations and SequentiallyConsistent stores require Release; barriers. Additionally, the code generator must enforce ordering between; SequentiallyConsistent stores followed by SequentiallyConsistent loads. This; is usually done by emitting either a full fence before the loads or a full; fence after the stores; which is preferred varies by architecture. Atomics and IR optimization; ===========================. Predicates for optimizer writers to query:. * ``isSimple()``: A load or store which is not volatile or atomic. This is; what, for example, memcpyopt would check for operations it might transform. * ``isUnordered()``: A load or store which is not volatile and at most; Unordered. This would be checked, for example, by LICM before hoisting an; operation. * ``mayReadFromMemory()``/``mayWriteToMemory()``: Existing predicate, but note; that they return true for any operation which is volatile or at least; Monotonic. * ``isStrongerThan`` / ``isAtLeastOrStrongerThan``: ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:15359,load,loads,15359,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['loads']
Performance,"; ---------. Monotonic is the weakest level of atomicity that can be used in synchronization; primitives, although it does not provide any general synchronization. It; essentially guarantees that if you take all the operations affecting a specific; address, a consistent ordering exists. Relevant standard; This corresponds to the C++/C ``memory_order_relaxed``; see those; standards for the exact definition. Notes for frontends; If you are writing a frontend which uses this directly, use with caution. The; guarantees in terms of synchronization are very weak, so make sure these are; only used in a pattern which you know is correct. Generally, these would; either be used for atomic operations which do not protect other memory (like; an atomic counter), or along with a ``fence``. Notes for optimizers; In terms of the optimizer, this can be treated as a read+write on the relevant; memory location (and alias analysis will take advantage of that). In addition,; it is legal to reorder non-atomic and Unordered loads around Monotonic; loads. CSE/DSE and a few other optimizations are allowed, but Monotonic; operations are unlikely to be used in ways which would make those; optimizations useful. Notes for code generation; Code generation is essentially the same as that for unordered for loads and; stores. No fences are required. ``cmpxchg`` and ``atomicrmw`` are required; to appear as a single operation. Acquire; -------. Acquire provides a barrier of the sort necessary to acquire a lock to access; other memory with normal loads and stores. Relevant standard; This corresponds to the C++/C ``memory_order_acquire``. It should also be; used for C++/C ``memory_order_consume``. Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Acquire only provides a semantic guarantee when paired with a Release; operation. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. It is; also possible to move stores from be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:11053,load,loads,11053,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,2,['load'],['loads']
Performance,"; ... float x;; ... float y;; ... float z;; ... };; ...; ... std::vector<Atom> atoms = {{1, 2, 3}, {2, 3, 4}, {3, 4, 5}, {4, 5, 6}, {5, 6, 7}};; ... """"""); ...; >>> @numba.njit; >>> def lj_numba_scalar(r):; ... sr6 = (1./r)**6; ... pot = 4.*(sr6*sr6 - sr6); ... return pot. >>> @numba.njit; >>> def distance_numba_scalar(atom1, atom2):; ... dx = atom2.x - atom1.x; ... dy = atom2.y - atom1.y; ... dz = atom2.z - atom1.z; ...; ... r = (dx * dx + dy * dy + dz * dz) ** 0.5; ...; ... return r; ...; >>> def potential_numba_scalar(cluster):; ... energy = 0.0; ... for i in range(cluster.size() - 1):; ... for j in range(i + 1, cluster.size()):; ... r = distance_numba_scalar(cluster[i], cluster[j]); ... e = lj_numba_scalar(r); ... energy += e; ...; ... return energy; ...; >>> print(""Total lennard jones potential ="", potential_numba_scalar(cppyy.gbl.atoms)); Total lennard jones potential = -0.5780277345740283. Overhead; --------. The main overhead of JITing Numba traces is in the type annotation in Numba; itself, optimization of the IR and assembly by the backend less so.; (There is also a non-negligible cost to Numba initialization, which is why; ``cppyy`` does not provide automatic extension hooks.); The use of ``cppyy`` bound C++, which relies on the same Numba machinery,; does not change that, since the reflection-based lookups are in C++ and; comparatively very fast.; For example, there is no appreciable difference in wall clock time to JIT a; trace using Numba's included math functions (from module ``math`` or; ``numpy``) or one that uses C++ bound ones whether from the standard library; or a templated versions from e.g. Eigen.; Use of very complex template expressions may change this balance, but in; principle, wherever it makes sense in the first place to use Numba JITing, it; is also fine, performance-wise, to use ``cppyy`` bound C++ inside the trace. A second important overhead is in unboxing Python proxies of C++ objects,; in particular when passed as an argument to a Nu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/numba.rst:9339,optimiz,optimization,9339,bindings/pyroot/cppyy/cppyy/doc/source/numba.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/numba.rst,1,['optimiz'],['optimization']
Performance,"; ...; }. define i32 @never_instrument() uwtable ""function-instrument""=""xray-never"" {; ; ...; }. You can also set the ``xray-instruction-threshold`` attribute and provide a; numeric string value for how many instructions should be in the function before; it gets instrumented. .. code-block:: llvm. define i32 @maybe_instrument() uwtable ""xray-instruction-threshold""=""2"" {; ; ...; }. Special Case File; -----------------. Attributes can be imbued through the use of special case files instead of; adding them to the original source files. You can use this to mark certain; functions and classes to be never, always, or instrumented with first-argument; logging from a file. The file's format is described below:. .. code-block:: bash. # Comments are supported; [always]; fun:always_instrument; fun:log_arg1=arg1 # Log the first argument for the function. [never]; fun:never_instrument. These files can be provided through the ``-fxray-attr-list=`` flag to clang.; You may have multiple files loaded through multiple instances of the flag. XRay Runtime Library; --------------------. The XRay Runtime Library is part of the compiler-rt project, which implements; the runtime components that perform the patching and unpatching of inserted; instrumentation points. When you use ``clang`` to link your binaries and the; ``-fxray-instrument`` flag, it will automatically link in the XRay runtime. The default implementation of the XRay runtime will enable XRay instrumentation; before ``main`` starts, which works for applications that have a short; lifetime. This implementation also records all function entry and exit events; which may result in a lot of records in the resulting trace. Also by default the filename of the XRay trace is ``xray-log.XXXXXX`` where the; ``XXXXXX`` part is randomly generated. These options can be controlled through the ``XRAY_OPTIONS`` environment; variable, where we list down the options and their defaults below. +-------------------+-----------------+--------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRay.rst:4494,load,loaded,4494,interpreter/llvm-project/llvm/docs/XRay.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRay.rst,1,['load'],['loaded']
Performance,"; // #5: error: implicit loading of libA is currently unsupported. ```. This pattern is not only used in the ROOT prompt but in I/O hotspots such as; `ShowMembers` and `TClass::IsA`. A naive implementation of this feature would require inclusion of all reachable; library descriptors (aka header files) at ROOT startup time. Of course this is; not feasible and ROOT inserts a set of optimizations to fence itself from the; costly full header inclusion. Unfortunately, several of them are home-grown and; in a few cases inaccurate (eg line #5) causing a noticeable technical debt. Here we will briefly describe the three common layers of optimizations: ROOT PCH,; ROOTMAP and RDICT. The ROOT precompiled header (PCH) reduces the CPU and memory cost for ROOT's; most used libraries. The precompiled header technology is well-understood since; decades [[4]]. It is an efficient on-disk representation of the state of the; compiler after parsing a set of headers. It can be loaded before starting the; next instance to avoid doing redundant work. At build time, rootcling (ROOT's; dictionary generator) creates such PCH file which is attached at ROOT startup; time. Its major drawback is the fact that if third-party users want to include; their libraries, they have to recompile it every time there is a change. RDICT files store some useful information (in particular about class offsets) in; ROOT files to avoid the potentially expensive call to the interpreter if the; information is not the PCH. For example, ROOT's libGeom and other third-party; code. This is done to circumvent the costly call to `ShowMembers` which will; require parsing. ROOTMAP files reduce parsing for code which is not in the PCH. Consider; `foo::bar` and `S` are defined in `libFoo`'s `Foo.h`:; ```cpp; // Foo.h; namespace foo { struct bar{}; }; struct S{};; ```. ```bash; # libFoo.rootmap; { decls }; namespace foo { }; struct S;; ; [ libFoo.so ]; # List of selected classes; class bar; struct S; ```. ```cpp; // G__Foo.cxx ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:6156,load,loaded,6156,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,1,['load'],['loaded']
Performance,"; 15. Support direct reading of objects from sub-sub-directories.; 16. Introduce demo.htm, which demonstrates online usage of JSROOT.; 17. One could use demo.htm directly with THttpServer providing address like:; <http://localhost:8080/jsrootsys/demo/demo.htm?addr=../../Files/job1.root/hpx/root.json.gz&layout=3x3>; 18. Also for online server process url options like 'item', 'items', 'layout'; 19. Possibility to generate URL, which reproduces opened page with layout and drawn items. ### August 2014; 1. All communication between server and browser done with JSON format.; 2. Fix small error in dtree.js - one should always set; last sibling (_ls) property while tree can be dynamically changed.; 3. In JSRootCore.js provide central function, which handles different kinds; of XMLHttpRequest. Use only async requests, also when getting file header.; 4. Fully reorganize data management in file/tree/directory/collection hierarchical; display. Now complete description collected in HPainter class and decoupled from; visualization, performed with dTree.js.; 5. Remove all global variables from the code.; 6. Automatic scripts/style loading handled via JSROOT.loadScript() function.; One can specify arbitrary scripts list, which asynchronously loaded by browser.; 7. Method to build simple GUI changed and more simplified :). The example in index.htm.; While loadScript and AssertPrerequisites functions moved to JSROOT, one; can easily build many different kinds of GUIs, reusing provided JSRootCore.js functions.; 8. In example.htm also use AssertPrerequisites to load necessary scripts.; This helps to keep code up-to-date even by big changes in JavaScript code.; 9. Provide monitoring of online THttpServer with similar interface as for ROOT files.; 10. Fix several errors in TKey Streamer, use member names as in ROOT itself.; 11. Keep the only version identifier JSROOT.version for JS code; 12. One can specify in JSROOT.AssertPrerequisites functionality which is required.; One could specify",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:74298,perform,performed,74298,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['perform'],['performed']
Performance,"; 3. A younger store is not allowed to pass an older store.; 4. A younger store is not allowed to pass an older load. By default, the LSUnit optimistically assumes that loads do not alias; (`-noalias=true`) store operations. Under this assumption, younger loads are; always allowed to pass older stores. Essentially, the LSUnit does not attempt; to run any alias analysis to predict when loads and stores do not alias with; each other. Note that, in the case of write-combining memory, rule 3 could be relaxed to; allow reordering of non-aliasing store operations. That being said, at the; moment, there is no way to further relax the memory model (``-noalias`` is the; only option). Essentially, there is no option to specify a different memory; type (e.g., write-back, write-combining, write-through; etc.) and consequently; to weaken, or strengthen, the memory model. Other limitations are:. * The LSUnit does not know when store-to-load forwarding may occur.; * The LSUnit does not know anything about cache hierarchy and memory types.; * The LSUnit does not know how to identify serializing operations and memory; fences. The LSUnit does not attempt to predict if a load or store hits or misses the L1; cache. It only knows if an instruction ""MayLoad"" and/or ""MayStore."" For; loads, the scheduling model provides an ""optimistic"" load-to-use latency (which; usually matches the load-to-use latency for when there is a hit in the L1D). :program:`llvm-mca` does not (on its own) know about serializing operations or; memory-barrier like instructions. The LSUnit used to conservatively use an; instruction's ""MayLoad"", ""MayStore"", and unmodeled side effects flags to; determine whether an instruction should be treated as a memory-barrier. This was; inaccurate in general and was changed so that now each instruction has an; IsAStoreBarrier and IsALoadBarrier flag. These flags are mca specific and; default to false for every instruction. If any instruction should have either of; these flags set, i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:41034,cache,cache,41034,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['cache'],['cache']
Performance,"; 8. Provide example of custom entries in histogram context menu; 9. Provide alternative external location for zstd-codec, let use zstd even when not found locally; 10. Let skip HEAD requests when reading files, adding ""^"" symbol to file name (#223); 11. Show long histogram names in stats box when possible; 12. Fix logic how ""ndiv"" parameter of TAxis is handled, showing really the configured number of ticks; 13. Fix problem with curved TGraph drawings (#218); 14. Fix problems with TGraph drawing updates; 15. Base version for ROOT 6.26 release. ## Changes in 6.2.2; 1. Fix - proper fill TH1 which drawn with line option; 2. Fix - object drawing from inspector; 3. Fix - error with filling data of TGeoTrack in ""extract tracks"" example; 4. Fix - error in pad items context menu; 5. Fix - assigned dropped item name only when new painter created. ## Changes in 6.2.1; 1. Fix logy and logz handling on lego plots; 2. Fix error in statistic calculations for projections; 3. Fix zstd-codec loading with minified jsroot scripts. ## Changes in 6.2.0; 1. Support fully interactive second X/Y axis for histograms, graphs, functions and spline; 2. Support X+, Y+, RX, RY draw options for TF1; 3. Remove deprecated JSRootCore.js script, one have to use JSRoot.core.js; 4. Upgrade three.js to r127; 5. Upgrade d3.js to 6.7.0; 6. Implement ""nozoomx"" and ""nozoomy"" draw options for TPad; 7. Implement ""frame"" draw option for TGaxis - fix position of axis relative to the frame; 8. Preserve position of TPaletteAxis, if provided with histogram; make default position like in ROOT; 9. Support basic TLatex symbols in lego plos axis title; 10. Use frame margins when create 3D lego drawings; 11. Implement ""nomargins"" draw option for pad/canvas; 12. Support custom mouse click/dblcklick handlers in lego plots; 13. Implement marker styles 35 - 49; 14. Let switch orthographic camera in geometry via control gui (#217); 15. Fix drawing of custom markers on 3D, also in node.js (#205). ## Changes in 6.1.1; 1. Fix b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:22541,load,loading,22541,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loading']
Performance,"; 9. Fix problem of simultaneous move TGeo drawings and canvas in flexible layout. ## Changes in 4.2; 1. Significant performance improvements in 3D drawings - TGeo/TH2/TH3; 2. Implement TGeoPara, TGeoGtra, TGeoXtru and TGeoEltu shapes; 3. Optimize (reduce vertices number) for others TGeo shapes; 4. Correct rotation/translation/scaling of TGeo nodes; 5. Workaround for axis reflection (not directly supported in three.js); 6. Support array of objects in I/O (like in TAxis3D); 7. Correct reading of multi-dim arrays like Double_t fXY[8][2];; 8. Provide canvas toolbar for actions like savepng or unzoom; 9. Implement JSROOT.resize() function to let resize drawing after changes in page layout; 10. Fix error with title display/update. ## Changes in 4.1; 1. Introduce object inspector - one could browse object members of any class; 2. Let draw sub-items from TCanvas list of primitives like sub-pad or TLatex; 3. Provide possibility to save drawn SVG canvas as PNG; 4. TGraph drawing optimization - limit number of drawn points; 5. Implement painter for TPolyMarker3D; 6. Improve drawing and update of TMultiGraph; 7. Reorganize 3D drawing of TH2/TH3 histograms, allow to mix 2D and 3D display together; 8. Support overlay of 3D graphic over SVG canvas (used for IE); 9. Fix problems and improve flex(ible) layout. ## Changes in 4.0; 1. New TGeo classes support:; - browsing through volumes hierarchy; - changing visibility flags; - drawing of selected volumes; 2. New 'flex' layout:; - create frames like in Multi Document Interface; - one could move/resize/minimize/maximize such frames; 3. Significant (factor 4) I/O performance improvement:; - use ArrayBuffer class in HTTP requests instead of String; - use native arrays (like Int32Array) for array data members; - highly optimize streamer infos handling; 4. TH2 drawing optimization:; - if there are too many non-empty bins, combine them together; - when zoom-in, all original bins will be displayed separately; - let draw big TH2 histogram fas",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:59104,optimiz,optimization,59104,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['optimiz'],['optimization']
Performance,"; :ref:`expression<amdgpu_synid_expression>`; is used for an operand which has a different type or size. .. _amdgpu_synid_int_conv:. Conversion of Integer Values; ----------------------------. Instruction operands may be specified as 64-bit; :ref:`integer numbers<amdgpu_synid_integer_number>` or; :ref:`absolute expressions<amdgpu_synid_absolute_expression>`.; These values are converted to the; :ref:`expected operand type<amdgpu_syn_instruction_type>`; using the following steps:. 1. *Validation*. Assembler checks if the input value may be truncated; without loss to the required *truncation width* (see the table below).; There are two cases when this operation is enabled:. * The truncated bits are all 0.; * The truncated bits are all 1 and the value after truncation has its MSB bit set. In all other cases, the assembler triggers an error. 2. *Conversion*. The input value is converted to the expected type; as described in the table below. Depending on operand kind, this conversion; is performed by either assembler or AMDGPU H/W (or both). ============== ================= =============== ====================================================================; Expected type Truncation Width Conversion Description; ============== ================= =============== ====================================================================; i16, u16, b16 16 num.u16 Truncate to 16 bits.; i32, u32, b32 32 num.u32 Truncate to 32 bits.; i64 32 {-1,num.i32} Truncate to 32 bits and then sign-extend the result to 64 bits.; u64, b64 32 {0,num.u32} Truncate to 32 bits and then zero-extend the result to 64 bits.; f16 16 num.u16 Use low 16 bits as an f16 value.; f32 32 num.u32 Use low 32 bits as an f32 value.; f64 32 {num.u32,0} Use low 32 bits of the number as high 32 bits; of the result; low 32 bits of the result are zeroed.; ============== ================= =============== ====================================================================. Examples of enabled conversions:. .. parsed-litera",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUOperandSyntax.rst:35175,perform,performed,35175,interpreter/llvm-project/llvm/docs/AMDGPUOperandSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUOperandSyntax.rst,1,['perform'],['performed']
Performance,"; ;; ; [%ptr + 0]: 00100001 (0x21); ; [%ptr + 1]: 01010011 (0x53). When ``<N*M>`` isn't evenly divisible by the byte size the exact memory layout; is unspecified (just like it is for an integral type of the same size). This; is because different targets could put the padding at different positions when; the type size is smaller than the type's store size. :Syntax:. ::. < <# elements> x <elementtype> > ; Fixed-length vector; < vscale x <# elements> x <elementtype> > ; Scalable vector. The number of elements is a constant integer value larger than 0;; elementtype may be any integer, floating-point or pointer type. Vectors; of size zero are not allowed. For scalable vectors, the total number of; elements is a constant multiple (called vscale) of the specified number; of elements; vscale is a positive integer that is unknown at compile time; and the same hardware-dependent constant for all scalable vectors at run; time. The size of a specific scalable vector type is thus constant within; IR, even if the exact size in bytes cannot be determined until run time. :Examples:. +------------------------+----------------------------------------------------+; | ``<4 x i32>`` | Vector of 4 32-bit integer values. |; +------------------------+----------------------------------------------------+; | ``<8 x float>`` | Vector of 8 32-bit floating-point values. |; +------------------------+----------------------------------------------------+; | ``<2 x i64>`` | Vector of 2 64-bit integer values. |; +------------------------+----------------------------------------------------+; | ``<4 x ptr>`` | Vector of 4 pointers |; +------------------------+----------------------------------------------------+; | ``<vscale x 4 x i32>`` | Vector with a multiple of 4 32-bit integer values. |; +------------------------+----------------------------------------------------+. .. _t_label:. Label Type; ^^^^^^^^^^. :Overview:. The label type represents code labels. :Syntax:. ::. label. .. _t_token:. Token ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:177461,scalab,scalable,177461,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"; ======================; Thread Safety Analysis; ======================. Introduction; ============. Clang Thread Safety Analysis is a C++ language extension which warns about; potential race conditions in code. The analysis is completely static (i.e.; compile-time); there is no run-time overhead. The analysis is still; under active development, but it is mature enough to be deployed in an; industrial setting. It is being developed by Google, in collaboration with; CERT/SEI, and is used extensively in Google's internal code base. Thread safety analysis works very much like a type system for multi-threaded; programs. In addition to declaring the *type* of data (e.g. ``int``, ``float``,; etc.), the programmer can (optionally) declare how access to that data is; controlled in a multi-threaded environment. For example, if ``foo`` is; *guarded by* the mutex ``mu``, then the analysis will issue a warning whenever; a piece of code reads or writes to ``foo`` without first locking ``mu``.; Similarly, if there are particular routines that should only be called by; the GUI thread, then the analysis will warn if other threads call those; routines. Getting Started; ----------------. .. code-block:: c++. #include ""mutex.h"". class BankAccount {; private:; Mutex mu;; int balance GUARDED_BY(mu);. void depositImpl(int amount) {; balance += amount; // WARNING! Cannot write balance without locking mu.; }. void withdrawImpl(int amount) REQUIRES(mu) {; balance -= amount; // OK. Caller must have locked mu.; }. public:; void withdraw(int amount) {; mu.Lock();; withdrawImpl(amount); // OK. We've locked mu.; } // WARNING! Failed to unlock mu. void transferFrom(BankAccount& b, int amount) {; mu.Lock();; b.withdrawImpl(amount); // WARNING! Calling withdrawImpl() requires locking b.mu.; depositImpl(amount); // OK. depositImpl() has no requirements.; mu.Unlock();; }; };. This example demonstrates the basic concepts behind the analysis. The; ``GUARDED_BY`` attribute declares that a thread must lo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSafetyAnalysis.rst:188,race condition,race conditions,188,interpreter/llvm-project/clang/docs/ThreadSafetyAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSafetyAnalysis.rst,3,"['multi-thread', 'race condition']","['multi-threaded', 'race conditions']"
Performance,"; ==========================================. Introduction; ============. The :ref:`inalloca <attr_inalloca>` attribute is designed to allow; taking the address of an aggregate argument that is being passed by; value through memory. Primarily, this feature is required for; compatibility with the Microsoft C++ ABI. Under that ABI, class; instances that are passed by value are constructed directly into; argument stack memory. Prior to the addition of inalloca, calls in LLVM; were indivisible instructions. There was no way to perform intermediate; work, such as object construction, between the first stack adjustment; and the final control transfer. With inalloca, all arguments passed in; memory are modelled as a single alloca, which can be stored to prior to; the call. Unfortunately, this complicated feature comes with a large; set of restrictions designed to bound the lifetime of the argument; memory around the call. For now, it is recommended that frontends and optimizers avoid producing; this construct, primarily because it forces the use of a base pointer.; This feature may grow in the future to allow general mid-level; optimization, but for now, it should be regarded as less efficient than; passing by value with a copy. Intended Usage; ==============. The example below is the intended LLVM IR lowering for some C++ code; that passes two default-constructed ``Foo`` objects to ``g`` in the; 32-bit Microsoft C++ ABI. .. code-block:: c++. // Foo is non-trivial.; struct Foo { int a, b; Foo(); ~Foo(); Foo(const Foo &); };; void g(Foo a, Foo b);; void f() {; g(Foo(), Foo());; }. .. code-block:: text. %struct.Foo = type { i32, i32 }; declare void @Foo_ctor(%struct.Foo* %this); declare void @Foo_dtor(%struct.Foo* %this); declare void @g(<{ %struct.Foo, %struct.Foo }>* inalloca %memargs). define void @f() {; entry:; %base = call i8* @llvm.stacksave(); %memargs = alloca <{ %struct.Foo, %struct.Foo }>; %b = getelementptr <{ %struct.Foo, %struct.Foo }>* %memargs, i32 1; call voi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InAlloca.rst:1061,optimiz,optimizers,1061,interpreter/llvm-project/llvm/docs/InAlloca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InAlloca.rst,1,['optimiz'],['optimizers']
Performance,"; >>> for i in range(0,10):; ... v.push_back(i); ...; >>> for i in v:; ... print(i, end=' '); 1 2 3 4 5 6 7 8 9; >>>; >>> list(v); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; >>>; ```. The parameters to the template instantiation can either be an actual; type or value (as is used here, ""int""), or a string representation of; the parameters (e.g. ""'double'""), or a mixture of both (e.g. ""'TCanvas,; 0'"" or ""'double', 0"" ). The ""std::vector\<int\>"" class is one of the; classes builtin by default into the Cling extension dlls. You will get a; non-functional class (instances of which can still be passed around to; C++) if the corresponding dictionary doesn't exist. #### Access to ROOT Globals. Most globals and global functions can be imported directly from the; ROOT.py module, but some common ones (most notably **`gMinuit`**,; although that variable now exists at startup from release 5.08 onward); do not exist yet at program startup, as they exist in modules that are; loaded later (e.g. through the auto-loading mechanism). An example; session should make this clear:. ``` {.cpp}; >>> from ROOT import *; >>> gROOT # directly available; <ROOT.TROOT object at 0x399c30>; >>> gMinuit # library not yet loaded: not available; Traceback (most recent call last):; File ""<stdin>"", line 1, in ?; NameError: name 'gMinuit' is not defined; >>> TMinuit # use of TMinuit class forces auto-loading; <class '__main__.TMinuit'>; >>> gMinuit # now gMinuit is available; <__main__.TMinuit object at 0x1458c70>; >>> not not gMinuit # but it is the null pointer, until set; False; >>> g = TMinuit(); >>> not not gMinuit; True; ```. It is also possible to create globals interactively, either by executing; a Cling macro, or by a call to `gROOT.ProcessLine()`. These globals are; made available in the same way: either use them directly after creation; in 'from ROOT import \*' more, or get them from the ROOT namespace after; an 'import ROOT'. As of 5.08, the behaviour of ROOT globals is the same as python globals,; which",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:13124,load,loading,13124,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['load'],['loading']
Performance,"; BEGIN_HTML; <h2>Minuit2 Package</h2>. <b>Minuit2</b> is a new object-oriented implementation, written in C++, of the popular MINUIT minimization package.<p>; These new version provides basically all the functionality present in the old Fortran version, with almost equivalent numerical accuracy and computational performances. Furthermore, it contains new functionality, like the possibility to set single side parameter limits or the FUMILI algorithm, which is an optimized method for least square and log likelihood minimizations. The package has been originally developed by M. Winkler and F. James. More information on the new C++ version can be found on the <a href=""http://www.cern.ch/minuit"">MINUIT Web Site</a> and in particular the online doc can be found here <a href=""http://www.cern.ch/mathlibs/sw/html/Minuit2.html"">here</a>.<p>; Minuit2, originally developed in the SEAL project, is now distributed within ROOT. The API has been then changed in this new version to follow the ROOT coding convention (function names starting with capital letters) and the classes have been moved inside the namespace <em>ROOT::Minuit2</em>. In addition, the ROOT distribution contains classes like TFitterMinuit and TFitterFumili needed to integrate Minuit2 in the ROOT framework. <p>; In the latest version (from 5.17.08) a new class has been introduced, ROOT::Minuit2::Minuit2Minimizer, which implements the interface; ROOT::Math::Minimizer from <a href=""http://www.cern.ch/mathlibs/sw/html/MathCore.html"">MathCore</a>.; . It can be instantiates also using the ROOT plug-in manager. It is a convenient entry point for using Minuit2. <h3>References</h3>; <p>; <ol>; <li>; F. James, <em>Fortran MINUIT Reference Manual</em> (<a href=""https://cern-tex.web.cern.ch/cern-tex/minuit/minmain.html"">html</a>); </li>; <li>; F. James and M. Winkler, <em>C++ MINUIT User's Guide</em> (<a href=""http://seal.cern.ch/documents/minuit/mnusersguide.pdf"">pdf</a>); </li>; <li>; F. James, <em>Minuit Tutorial on Functio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/minuit2/doc/index.txt:315,perform,performances,315,math/minuit2/doc/index.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/minuit2/doc/index.txt,4,"['optimiz', 'perform']","['optimized', 'performances']"
Performance,"; DistSampler::Generate(n, data). The data set; generation can be unbinned or binned in the; given range (only equidistant bins are currently supported); Sampling of 1D or multi-dim distributions is supported via; the same interface; Derived classes; implementing this interface are not provided by MathCore but by; other libraries and they can be instantiated using the plugin; manager. Implementations based on Unuran and Foam exist.; The tutorial math/multidimSampling.C is an example on; how to use this class. New class ROOT::Math::GoFTest for goodness of fit; tests of unbinned data; ; The class implements the Kolmogorov-Smirnov and; Anderson-Darling tests for two samples (data vs data ) and; one sample (data vs distribution); For the data vs distribution test, the user can compare using a; predefined distributions (Gaussian, LogNormal or Exponential) or; by passing a user defined PDF or CDF.; Example 1: perform a 2 sample GoF test from two arrays,; sample1[n1] and sample2[n2] containing the data; ; ROOT::Math::GoFTest goftest(n1, sample1, n2, sample2);; double pValueAD = goftest.AndersonDarling2SamplesTest();; double pValueKS = goftest.KolmogorovSmirnov2SamplesTest();; ; The class can return optionally also the test statistics instead of; the p value.; Example 2: perform a 1 sample test with a pre-defined; distribution starting from a data set sample[n]. ROOT::Math::GoFTest goftest(n, sample, ROOT::Math::GoFTest::kGaussian);; double pValueAD = goftest.AndersonDarlingTest();; double pValueKS = goftest.KolmogorovSmirnovTest();; . Example 3: perform a 1 sample test with a user-defined; distribution provided as cdf; ; ROOT::Math::Functor1D cdf_func(&ROOT::Math::landau_cdf);; ROOT::Math::GofTest goftest(n, sample, cdf_func, ROOT::Math::GoFTest::kCDF);; double pValueAD = goftest.AndersonDarlingTest();; . Example 4: perform a 1 sample test with a user-defined; distribution provided as pdf. Note that in this case to avoid; integration problems is sometimes recommended to gi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v528/index.html:1151,perform,perform,1151,math/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v528/index.html,2,['perform'],['perform']
Performance,"; LangRef; LibFuzzer; MarkedUpDisassembly; MIRLangRef; OptBisect; PCSectionsMetadata; PDB/index; PointerAuth; ScudoHardenedAllocator; MemTagSanitizer; Security; SecurityTransparencyReports; SegmentedStacks; StackMaps; SpeculativeLoadHardening; Statepoints; SymbolizerMarkupFormat; SystemLibrary; TestingGuide; TransformMetadata; TypeMetadata; XRay; XRayExample; XRayFDRFormat; YamlIO. API Reference; -------------. `Doxygen generated documentation <https://llvm.org/doxygen/>`_; (`classes <https://llvm.org/doxygen/inherits.html>`_). :doc:`HowToUseAttributes`; Answers some questions about the new Attributes infrastructure. LLVM Reference; --------------. ======================; Command Line Utilities; ======================. :doc:`LLVM Command Guide <CommandGuide/index>`; A reference manual for the LLVM command line utilities (""man"" pages for LLVM; tools). :doc:`Bugpoint`; Automatic bug finder and test-case reducer description and usage; information. :doc:`OptBisect`; A command line option for debugging optimization-induced failures. :doc:`SymbolizerMarkupFormat`; A reference for the log symbolizer markup accepted by ``llvm-symbolizer``. :doc:`The Microsoft PDB File Format <PDB/index>`; A detailed description of the Microsoft PDB (Program Database) file format. ==================; Garbage Collection; ==================. :doc:`GarbageCollection`; The interfaces source-language compilers should use for compiling GC'd; programs. :doc:`Statepoints`; This describes a set of experimental extensions for garbage; collection support. =========; LibFuzzer; =========. :doc:`LibFuzzer`; A library for writing in-process guided fuzzers. :doc:`FuzzingLLVM`; Information on writing and using Fuzzers to find bugs in LLVM. ========; LLVM IR; ========. :doc:`LLVM Language Reference Manual <LangRef>`; Defines the LLVM intermediate representation and the assembly form of the; different nodes. :doc:`InAlloca`; Description of the ``inalloca`` argument attribute. :doc:`BitCodeFormat`; This describ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Reference.rst:1477,optimiz,optimization-induced,1477,interpreter/llvm-project/llvm/docs/Reference.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Reference.rst,1,['optimiz'],['optimization-induced']
Performance,"; Large code bases will benefit from more advanced features such as; :doc:`pythonizations <pythonizations>` for a cleaner interface to clients;; precompiled modules for faster parsing and reduced memory usage;; "":ref:`dictionaries <dictionaries>`"" to package locations and manage; dependencies; and mapping files for automatic, lazy, loading.; You can, however, get very far with just the basics and it may even be; completely sufficient for small packages with fewer classes. cppyy works by parsing C++ definitions through ``cling``, generating tiny; wrapper codes to honor compile-time features and create standardized; interfaces, then compiling/linking those wrappers with the ``clang`` JIT.; It thus requires only those two ingredients: *C++ definitions* and; *linker symbols*.; All cppyy uses, the basic and the more advanced, are variations on the; theme of bringing these two together at the point of use. Definitions typically live in header files and symbols in libraries.; Headers can be loaded with ``cppyy.include`` and libraries with the; ``cppyy.load_library`` call.; Loading the header is sufficient to start exploring, with ``cppyy.gbl`` the; starting point of all things C++, while the linker symbols are only needed at ; the point of first use. Here is an example using the `zlib`_ library, which is likely available on; your system:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('zlib.h') # bring in C++ definitions; >>> cppyy.load_library('libz') # load linker symbols; >>> cppyy.gbl.zlibVersion() # use a zlib API; '1.2.11'; >>>. Since header files can include other header files, it is easy to aggregate; all relevant ones into a single header to include.; If there are project-specific include paths, you can add those paths through; ``cppyy.add_include_path``.; If a header is C-only and not set for use with C++, use ``cppyy.c_include``,; which adds ``extern ""C""`` around the header. Library files can be aggregated by linking all relevant ones to a single; li",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/starting.rst:1112,load,loaded,1112,bindings/pyroot/cppyy/cppyy/doc/source/starting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/starting.rst,1,['load'],['loaded']
Performance,"; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the; value read by the; fence-paired-atomic. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:303244,load,load,303244,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,; NAD; Digit separators following non-octal prefix; Clang 3.5. 1948; NAD; exception-specification of replacement global new; Clang 3.5. 1949; CD4; “sequenced after” instead of “sequenced before”; Unknown. 1950; NAD; Restructuring description of ranks of conversion sequences; Unknown. 1951; CD4; Cv-qualification and literal types; Unknown. 1952; CD4; Constant expressions and library undefined behavior; Unknown. 1953; open; Data races and common initial sequence; Not resolved. 1954; tentatively ready; typeid null dereference check in subexpressions; Unknown. 1955; CD4; #elif with invalid controlling expression; Unknown. 1956; CD4; Reuse of storage of automatic variables; Unknown. 1957; NAD; decltype(auto) with direct-list-initialization; Unknown. 1958; CD4; decltype(auto) with parenthesized initializer; Unknown. 1959; CD4; Inadvertently inherited copy constructor; Clang 3.9. 1960; NAD; Visibility of entity named in class-scope using-declaration; No. 1961; C++17; Potentially-concurrent actions within a signal handler; Unknown. 1962; open; Type of __func__; Not resolved. 1963; CD4; Implementation-defined identifier characters; Unknown. 1964; NAD; opaque-enum-declaration in alias-declaration?; Unknown. 1965; drafting; Explicit casts to reference types; Not resolved. 1966; CD4; Colon following enumeration elaborated-type-specifier; Clang 11. 1967; CD4; Temporary lifetime and move-elision; Unknown. 1968; NAD; Address of typeid in constant expressions; No. 1969; CD6; Missing exclusion of ~S as an ordinary function name; Unknown. 1970; NAD; Ambiguity resolution for (T())*x; Unknown. 1971; CD4; Unclear disambiguation of destructor and operator~; Unknown. 1972; CD6; Identifier character restrictions in non-identifiers; Unknown. 1973; DRWP; Which parameter-declaration-clause in a lambda-expression?; Unknown. 1974; NAD; Redundant specification of non-type typename-specifier; Unknown. 1975; CD4; Permissible declarations for exception-specifications; Unknown. 1976; NAD; Ambiguity ,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html:133193,concurren,concurrent,133193,interpreter/llvm-project/clang/www/cxx_dr_status.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html,2,['concurren'],['concurrent']
Performance,"; Python functions can be used, including for dynamically constructing; classes. The C++ code used for the examples below can be found; :doc:`here <cppyy_features_header>`, and it is assumed that that code is; loaded at the start of any session.; Download it, save it under the name ``features.h``, and load it:. .. code-block:: python. >>> import cppyy; >>> cppyy.include('features.h'); >>>. Function argument type conversions follow the expected rules, with implicit; conversions allowed, including between Python builtin types and STL types,; but it is rather more efficient to make conversions explicit. `Free functions`; ----------------. All bound C++ code starts off from the global C++ namespace, represented in; Python by ``gbl``.; This namespace, as any other namespace, is treated as a module after it has; been loaded.; Thus, we can directly import C++ functions from it and other namespaces that; themselves may contain more functions.; All lookups on namespaces are done lazily, thus if loading more headers bring; in more functions (incl. new overloads), these become available dynamically. .. code-block:: python. >>> from cppyy.gbl import global_function, Namespace; >>> global_function == Namespace.global_function; False; >>> from cppyy.gbl.Namespace import global_function; >>> global_function == Namespace.global_function; True; >>> from cppyy.gbl import global_function; >>>. Free functions can be bound to a class, following the same rules as apply to; Python functions: unless marked as static, they will turn into member; functions when bound to an instance, but act as static functions when called; through the class.; Consider this example:. .. code-block:: python. >>> from cppyy.gbl import Concrete, call_abstract_method; >>> c = Concrete(); >>> Concrete.callit = call_abstract_method; >>> Concrete.callit(c); called Concrete::abstract_method; >>> c.callit(); called Concrete::abstract_method; >>> Concrete.callit = staticmethod(call_abstract_method); >>> c.callit(); Trac",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/functions.rst:1111,load,loading,1111,bindings/pyroot/cppyy/cppyy/doc/source/functions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/functions.rst,1,['load'],['loading']
Performance,"; TBranchElement:; *Br 17 :fH : TH1F* *; *Entries : 20 : Total Size= 19334 bytes File Size = 1671 *; *Baskets : 2 : Basket Size= 16000 bytes Compression= 11.29 *; *............................................................................*; *Br 18 :fTriggerBits : TBits *; *Entries : 20 : Total Size= 1398 bytes File Size = 400 *; *Baskets : 1 : Basket Size= 16000 bytes Compression= 2.23 *; *............................................................................*; *Br 19 :fIsValid : Bool_t *; *Entries : 20 : Total Size= 582 bytes File Size = 92 *; *Baskets : 1 : Basket Size= 16000 bytes Compression= 1.00 *. Add a new function TBranch::SetStatus It is much faster to call this function in case of a Tree with many branches; instead of calling TTree::SetBranchStatus.; Implement TTreeCache::Print that shows information like:; // ******TreeCache statistics for file: cms2.root ******; // Number of branches in the cache ...: 1093; // Cache Efficiency ..................: 0.997372; // Cache Efficiency Rel...............: 1.000000; // Learn entries......................: 100; // Reading............................: 72761843 bytes in 7 transactions; // Readahead..........................: 256000 bytes with overhead = 0 bytes; // Average transaction................: 10394.549000 Kbytes; // Number of blocks in current cache..: 210, total size: 6280352; This function can be called directly from TTree: T->PrintCacheStats();. Add support for variable size array of object in a TTree (when the owner of the array is split.); And many other bug fixes, security fixes, thread safety and performance improvements ; see the svn log for details. TTree Scan and Draw. Insured that the generated histogram as an integral bin width when plotting a string or integer.; Improved the output of TTree::Scan by inserting a blank space whenever a value is not available because there is no proper row in a friend.; (Previously it was re-printing the previous value). This required changes in ; When the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v528/index.html:5857,Cache,Cache,5857,tree/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v528/index.html,1,['Cache'],['Cache']
Performance,"; TGeoVolume *module2 = TGeoVolume::Import(""file.root"", ""MOD2"");; top->AddNode(module1, 1, new TGeoTranslation(0,0,100));; top->AddNode(module2, 1, new TGeoTranslation(0,0,-100));; // One should close oneself the geometry; geom->CloseGeometry();; ```. ### GDML. Few lines above word GDML was used. GDML stands for **G**eometry; **D**escription **M**arkup **L**anguage. It is an application-independent; geometry description format based on XML. It is mainly used for geometry; interchange between ROOT and Geant4 framework. More details about this; project can be found http://gdml.web.cern.ch. This feature; (importing/exporting from/to gdml file format) is disabled by default in; ROOT installation. To enable this feature add `--enable-gdml` option to; `./configure` script call. ## Navigation Algorithms. This section will describe the main methods and algorithms used for; implementing the navigation features within the geometrical modeller.; This includes navigation queries at shape level, global geometrical; queries and optimization mechanisms. ### Finding the State Corresponding to a Location (x,y,z). For reminder, a geometry state is a ‘touchable' object in the geometry; hierarchy. It is represented by a path like: **/TOP\_1/A\_1/B\_3/C\_1**,; where **B\_3** for instance is a copy of volume **B** positioned inside; volume **A**. A state is always associated to a transformation matrix; **M** of the touchable with respect to the global reference frame; (obtained by piling-up all local transformations of nodes in the branch; with respect to their containers). The current state and the; corresponding global matrix are updated whenever the geometry depth is; modified. The global transformations corresponding to all nodes in the; current branch are kept in an array: (**MTOP\_1, MA\_1, MB\_3, ...**). ![Navigation in the geometry hierarchy](pictures/080001E6.png). The elementary operations for changing the state are:. ``` {.cpp}; TGeoManager::CdUp();; TGeoManager::CdDown(i);; TG",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:154763,optimiz,optimization,154763,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['optimiz'],['optimization']
Performance,; TMVA/GiniIndexWithLaplace.h; TMVA/HyperParameterOptimisation.h; TMVA/IFitterTarget.h; TMVA/IMethod.h; TMVA/Interval.h; TMVA/IPruneTool.h; TMVA/KDEKernel.h; TMVA/LDA.h; TMVA/LogInterval.h; TMVA/LossFunction.h; TMVA/MCFitter.h; TMVA/MethodANNBase.h; TMVA/MethodBase.h; TMVA/MethodBayesClassifier.h; TMVA/MethodBDT.h; TMVA/MethodBoost.h; TMVA/MethodCategory.h; TMVA/MethodCFMlpANN_def.h; TMVA/MethodCFMlpANN.h; TMVA/MethodCFMlpANN_Utils.h; TMVA/MethodCompositeBase.h; TMVA/MethodCrossValidation.h; TMVA/MethodCuts.h; TMVA/MethodDL.h; TMVA/MethodDNN.h; TMVA/MethodDT.h; TMVA/MethodFDA.h; TMVA/MethodFisher.h; TMVA/MethodHMatrix.h; TMVA/MethodKNN.h; TMVA/MethodLD.h; TMVA/MethodLikelihood.h; TMVA/MethodMLP.h; TMVA/MethodPDEFoam.h; TMVA/MethodPDERS.h; TMVA/MethodRuleFit.h; TMVA/MethodSVM.h; TMVA/MethodTMlpANN.h; TMVA/MinuitFitter.h; TMVA/MinuitWrapper.h; TMVA/MisClassificationError.h; TMVA/ModulekNN.h; TMVA/Monitoring.h; TMVA/MsgLogger.h; TMVA/NeuralNet.h; TMVA/Node.h; TMVA/NodekNN.h; TMVA/OptimizeConfigParameters.h; TMVA/Option.h; TMVA/OptionMap.h; TMVA/Pattern.h; TMVA/PDEFoamCell.h; TMVA/PDEFoamDecisionTreeDensity.h; TMVA/PDEFoamDecisionTree.h; TMVA/PDEFoamDensityBase.h; TMVA/PDEFoamDiscriminantDensity.h; TMVA/PDEFoamDiscriminant.h; TMVA/PDEFoamEventDensity.h; TMVA/PDEFoamEvent.h; TMVA/PDEFoam.h; TMVA/PDEFoamKernelBase.h; TMVA/PDEFoamKernelGauss.h; TMVA/PDEFoamKernelLinN.h; TMVA/PDEFoamKernelTrivial.h; TMVA/PDEFoamMultiTarget.h; TMVA/PDEFoamTargetDensity.h; TMVA/PDEFoamTarget.h; TMVA/PDEFoamVect.h; TMVA/PDF.h; TMVA/QuickMVAProbEstimator.h; TMVA/Ranking.h; TMVA/Reader.h; TMVA/RegressionVariance.h; TMVA/ResultsClassification.h; TMVA/Results.h; TMVA/ResultsMulticlass.h; TMVA/ResultsRegression.h; TMVA/ROCCalc.h; TMVA/ROCCurve.h; TMVA/RootFinder.h; TMVA/RuleCut.h; TMVA/RuleEnsemble.h; TMVA/RuleFitAPI.h; TMVA/RuleFit.h; TMVA/RuleFitParams.h; TMVA/Rule.h; TMVA/SdivSqrtSplusB.h; TMVA/SeparationBase.h; TMVA/SimulatedAnnealingFitter.h; TMVA/SimulatedAnnealing.h; TMVA/SVEvent.h; TMVA/SVKe,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/CMakeLists.txt:2489,Optimiz,OptimizeConfigParameters,2489,tmva/tmva/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/CMakeLists.txt,1,['Optimiz'],['OptimizeConfigParameters']
Performance,"; The high 32 bits of the flat addressing shared aperture base.; Only used by GFX8 to allow conversion between shared segment; and flat addresses. See :ref:`amdgpu-amdhsa-kernel-prolog-flat-scratch`. ""hidden_queue_ptr""; A global memory address space pointer to the ROCm runtime; ``struct amd_queue_t`` structure for the HSA queue of the; associated dispatch AQL packet. It is only required for pre-GFX9; devices for the trap handler ABI (see :ref:`amdgpu-amdhsa-trap-handler-abi`). ====================== ============== ========= ================================. .. Kernel Dispatch; ~~~~~~~~~~~~~~~. The HSA architected queuing language (AQL) defines a user space memory interface; that can be used to control the dispatch of kernels, in an agent independent; way. An agent can have zero or more AQL queues created for it using an HSA; compatible runtime (see :ref:`amdgpu-os`), in which AQL packets (all of which; are 64 bytes) can be placed. See the *HSA Platform System Architecture; Specification* [HSA]_ for the AQL queue mechanics and packet layouts. The packet processor of a kernel agent is responsible for detecting and; dispatching HSA kernels from the AQL queues associated with it. For AMD GPUs the; packet processor is implemented by the hardware command processor (CP),; asynchronous dispatch controller (ADC) and shader processor input controller; (SPI). An HSA compatible runtime can be used to allocate an AQL queue object. It uses; the kernel mode driver to initialize and register the AQL queue with CP. To dispatch a kernel the following actions are performed. This can occur in the; CPU host program, or from an HSA kernel executing on a GPU. 1. A pointer to an AQL queue for the kernel agent on which the kernel is to be; executed is obtained.; 2. A pointer to the kernel descriptor (see; :ref:`amdgpu-amdhsa-kernel-descriptor`) of the kernel to execute is obtained.; It must be for a kernel that is contained in a code object that was loaded; by an HSA compatible runtime on th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:149274,queue,queue,149274,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"; The result of :ref:`freeze instruction <i_freeze>` is well defined regardless; of its operand. .. _blockaddress:. Addresses of Basic Blocks; -------------------------. ``blockaddress(@function, %block)``. The '``blockaddress``' constant computes the address of the specified; basic block in the specified function. It always has an ``ptr addrspace(P)`` type, where ``P`` is the address space; of the function containing ``%block`` (usually ``addrspace(0)``). Taking the address of the entry block is illegal. This value only has defined behavior when used as an operand to the; ':ref:`indirectbr <i_indirectbr>`' or for comparisons against null. Pointer; equality tests between labels addresses results in undefined behavior ---; though, again, comparison against null is ok, and no label is equal to the null; pointer. This may be passed around as an opaque pointer sized value as long as; the bits are not inspected. This allows ``ptrtoint`` and arithmetic to be; performed on these values so long as the original value is reconstituted before; the ``indirectbr`` instruction. Finally, some targets may provide defined semantics when using the value; as the operand to an inline assembly, but that is target specific. .. _dso_local_equivalent:. DSO Local Equivalent; --------------------. ``dso_local_equivalent @func``. A '``dso_local_equivalent``' constant represents a function which is; functionally equivalent to a given function, but is always defined in the; current linkage unit. The resulting pointer has the same type as the underlying; function. The resulting pointer is permitted, but not required, to be different; from a pointer to the function, and it may have different values in different; translation units. The target function may not have ``extern_weak`` linkage. ``dso_local_equivalent`` can be implemented as such:. - If the function has local linkage, hidden visibility, or is; ``dso_local``, ``dso_local_equivalent`` can be implemented as simply a pointer; to the function",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:200689,perform,performed,200689,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"; This operator produces a DAG node with the same arguments as *dag*, but with its; operator replaced with *op*. Example: ``!setdagop((foo 1, 2), bar)`` results in ``(bar 1, 2)``. ``!shl(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* left logically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!size(``\ *a*\ ``)``; This operator produces the size of the string, list, or dag *a*.; The size of a DAG is the number of arguments; the operator does not count. ``!sra(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right arithmetically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!srl(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right logically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!strconcat(``\ *str1*\ ``,`` *str2*\ ``, ...)``; This operator concatenates the string arguments *str1*, *str2*, etc., and; produces the resulting string. ``!sub(``\ *a*\ ``,`` *b*\ ``)``; This operator subtracts *b* from *a* and produces the arithmetic difference. ``!subst(``\ *target*\ ``,`` *repl*\ ``,`` *value*\ ``)``; This operator replaces all occurrences of the *target* in the *value* with; the *repl* and produces the resulting value. The *value* can; be a string, in which case substring substitution is performed. The *value* can be a record name, in which case the operator produces the *repl*; record if the *target* record name equals the *value* record name; otherwise it; produces the *value*. ``!substr(``\ *string*\ ``,`` *start*\ [``,`` *length*]\ ``)``; This operator extracts a substring of the given *string*. The starting; position of the substring is specified by *start*, which can range; between 0 and the length o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:72085,perform,performed,72085,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,1,['perform'],['performed']
Performance,"; [5] - JFPU0; [6] - JFPU1; [7] - JLAGU; [8] - JMul; [9] - JSAGU; [10] - JSTC; [11] - JVALU0; [12] - JVALU1; [13] - JVIMUL. Resource pressure per iteration:; [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13]; - - - 2.00 1.00 2.00 1.00 - - - - - - -. Resource pressure by instruction:; [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] Instructions:; - - - - 1.00 - 1.00 - - - - - - - vmulps	%xmm0, %xmm1, %xmm2; - - - 1.00 - 1.00 - - - - - - - - vhaddps	%xmm2, %xmm2, %xmm3; - - - 1.00 - 1.00 - - - - - - - - vhaddps	%xmm3, %xmm3, %xmm4. According to this report, the dot-product kernel has been executed 300 times,; for a total of 900 simulated instructions. The total number of simulated micro; opcodes (uOps) is also 900. The report is structured in three main sections. The first section collects a; few performance numbers; the goal of this section is to give a very quick; overview of the performance throughput. Important performance indicators are; **IPC**, **uOps Per Cycle**, and **Block RThroughput** (Block Reciprocal; Throughput). Field *DispatchWidth* is the maximum number of micro opcodes that are dispatched; to the out-of-order backend every simulated cycle. For processors with an; in-order backend, *DispatchWidth* is the maximum number of micro opcodes issued; to the backend every simulated cycle. IPC is computed dividing the total number of simulated instructions by the total; number of cycles. Field *Block RThroughput* is the reciprocal of the block throughput. Block; throughput is a theoretical quantity computed as the maximum number of blocks; (i.e. iterations) that can be executed per simulated clock cycle in the absence; of loop carried dependencies. Block throughput is superiorly limited by the; dispatch rate, and the availability of hardware resources. In the absence of loop-carried data dependencies, the observed IPC tends to a; theoretical maximum which can be computed by dividing the number of instructions; of a single iteration by the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:15786,perform,performance,15786,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['perform'],['performance']
Performance,"; ``CHECK:`` directives only accept lines corresponding to the body of the; ``@C_ctor_base`` function, even if the patterns match lines found later in; the file. Furthermore, if one of these three ``CHECK:`` directives fail,; FileCheck will recover by continuing to the next block, allowing multiple test; failures to be detected in a single invocation. There is no requirement that ``CHECK-LABEL:`` directives contain strings that; correspond to actual syntactic labels in a source or output language: they must; simply uniquely match a single line in the file being verified. ``CHECK-LABEL:`` directives cannot contain variable definitions or uses. Directive modifiers; ~~~~~~~~~~~~~~~~~~~. A directive modifier can be append to a directive by following the directive; with ``{<modifier>}`` where the only supported value for ``<modifier>`` is; ``LITERAL``. The ``LITERAL`` directive modifier can be used to perform a literal match. The; modifier results in the directive not recognizing any syntax to perform regex; matching, variable capture or any substitutions. This is useful when the text; to match would require excessive escaping otherwise. For example, the; following will perform literal matches rather than considering these as; regular expressions:. .. code-block:: text. Input: [[[10, 20]], [[30, 40]]]; Output %r10: [[10, 20]]; Output %r10: [[30, 40]]. ; CHECK{LITERAL}: [[[10, 20]], [[30, 40]]]; ; CHECK-DAG{LITERAL}: [[30, 40]]; ; CHECK-DAG{LITERAL}: [[10, 20]]. FileCheck Regex Matching Syntax; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. All FileCheck directives take a pattern to match.; For most uses of FileCheck, fixed string matching is perfectly sufficient. For; some things, a more flexible form of matching is desired. To support this,; FileCheck allows you to specify regular expressions in matching strings,; surrounded by double braces: ``{{yourregex}}``. FileCheck implements a POSIX; regular expression matcher; it supports Extended POSIX regular expressions; (ERE). Because we w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst:24070,perform,perform,24070,interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,1,['perform'],['perform']
Performance,"; ``RegisterRegAlloc::FunctionPassCtor``. In the same file add the ""installing""; declaration, in the form:. .. code-block:: c++. static RegisterRegAlloc myRegAlloc(""myregalloc"",; ""my register allocator help string"",; createMyRegisterAllocator);. Note the two spaces prior to the help string produces a tidy result on the; :option:`-help` query. .. code-block:: console. $ llc -help; ...; -regalloc - Register allocator to use (default=linearscan); =linearscan - linear scan register allocator; =local - local register allocator; =simple - simple register allocator; =myregalloc - my register allocator help string; ... And that's it. The user is now free to use ``-regalloc=myregalloc`` as an; option. Registering instruction schedulers is similar except use the; ``RegisterScheduler`` class. Note that the; ``RegisterScheduler::FunctionPassCtor`` is significantly different from; ``RegisterRegAlloc::FunctionPassCtor``. To force the load/linking of your register allocator into the; :program:`llc`/:program:`lli` tools, add your creator function's global; declaration to ``Passes.h`` and add a ""pseudo"" call line to; ``llvm/Codegen/LinkAllCodegenComponents.h``. Creating new registries; -----------------------. The easiest way to get started is to clone one of the existing registries; we; recommend ``llvm/CodeGen/RegAllocRegistry.h``. The key things to modify are; the class name and the ``FunctionPassCtor`` type. Then you need to declare the registry. Example: if your pass registry is; ``RegisterMyPasses`` then define:. .. code-block:: c++. MachinePassRegistry<RegisterMyPasses::FunctionPassCtor> RegisterMyPasses::Registry;. And finally, declare the command line option for your passes. Example:. .. code-block:: c++. cl::opt<RegisterMyPasses::FunctionPassCtor, false,; RegisterPassParser<RegisterMyPasses> >; MyPassOpt(""mypass"",; cl::init(&createDefaultMyPass),; cl::desc(""my pass option help""));. Here the command option is ""``mypass``"", with ``createDefaultMyPass`` as the; default creator",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:50918,load,load,50918,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['load'],['load']
Performance,"; ```. This will collapse the register to zero or one, and everything but the offset; in the addressing mode to be less than or equal to 9. This means the full; address can only be guaranteed to be less than `(1 << 31) + 9`. The OS may wish; to protect an extra page of the low address space to account for this. ##### Optimizations. A very large portion of the cost for this approach comes from checking loads in; this way, so it is important to work to optimize this. However, beyond making; the instruction sequences to *apply* the checks efficient (for example by; avoiding `pushfq` and `popfq` sequences), the only significant optimization is; to check fewer loads without introducing a vulnerability. We apply several; techniques to accomplish that. ###### Don't check loads from compile-time constant stack offsets. We implement this optimization on x86 by skipping the checking of loads which; use a fixed frame pointer offset. The result of this optimization is that patterns like reloading a spilled; register or accessing a global field don't get checked. This is a very; significant performance win. ###### Don't check dependent loads. A core part of why this mitigation strategy works is that it establishes a; data-flow check on the loaded address. However, this means that if the address; itself was already loaded using a checked load, there is no need to check a; dependent load provided it is within the same basic block as the checked load,; and therefore has no additional predicates guarding it. Consider code like the; following:; ```; ... .LBB0_4: # %danger; movq (%rcx), %rdi; movl (%rdi), %edx; ```. This will get transformed into:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; orq %rax, %rcx # Mask the pointer if misspeculating.; movq (%rcx), %rdi # Hardened load.; movl (%rdi), %edx # Unhardened load due to dependent addr.; ```. This doesn't check the load through `%rdi` as that pointer is dependent on a; checked load already. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:35125,optimiz,optimization,35125,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['optimiz'],['optimization']
Performance,"; acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - global 1. buffer_wbl2. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:269923,cache,cache,269923,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"; and flat addresses. See :ref:`amdgpu-amdhsa-kernel-prolog-flat-scratch`. ""hidden_queue_ptr""; A global memory address space pointer to the ROCm runtime; ``struct amd_queue_t`` structure for the HSA queue of the; associated dispatch AQL packet. It is only required for pre-GFX9; devices for the trap handler ABI (see :ref:`amdgpu-amdhsa-trap-handler-abi`). ====================== ============== ========= ================================. .. Kernel Dispatch; ~~~~~~~~~~~~~~~. The HSA architected queuing language (AQL) defines a user space memory interface; that can be used to control the dispatch of kernels, in an agent independent; way. An agent can have zero or more AQL queues created for it using an HSA; compatible runtime (see :ref:`amdgpu-os`), in which AQL packets (all of which; are 64 bytes) can be placed. See the *HSA Platform System Architecture; Specification* [HSA]_ for the AQL queue mechanics and packet layouts. The packet processor of a kernel agent is responsible for detecting and; dispatching HSA kernels from the AQL queues associated with it. For AMD GPUs the; packet processor is implemented by the hardware command processor (CP),; asynchronous dispatch controller (ADC) and shader processor input controller; (SPI). An HSA compatible runtime can be used to allocate an AQL queue object. It uses; the kernel mode driver to initialize and register the AQL queue with CP. To dispatch a kernel the following actions are performed. This can occur in the; CPU host program, or from an HSA kernel executing on a GPU. 1. A pointer to an AQL queue for the kernel agent on which the kernel is to be; executed is obtained.; 2. A pointer to the kernel descriptor (see; :ref:`amdgpu-amdhsa-kernel-descriptor`) of the kernel to execute is obtained.; It must be for a kernel that is contained in a code object that was loaded; by an HSA compatible runtime on the kernel agent with which the AQL queue is; associated.; 3. Space is allocated for the kernel arguments using the HSA compati",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:149420,queue,queues,149420,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queues']
Performance,"; and greater than 0.0 to enable cache. This value can be overridden by the; environment variable ROOT_TTREECACHE_SIZE. The resource variable TTreeCache.Prefill sets the default TTreeCache prefilling; type. The prefill type may be: 0 for no prefilling and 1 to prefill all; the branches. It can be overridden by the environment variable ROOT_TTREECACHE_PREFILL. In particular the default can be set back to the same as in version 5 by; setting TTreeCache.Size (or ROOT_TTREECACHE_SIZE) and TTreeCache.Prefill; (or ROOT_TTREECACHE_PREFILL) both to zero. TTree methods which are expected to modify a cache, like AddBranchToCache, will; attempt to setup a cache of default size if one does not exist, irrespective of; whether the auto cache creation is enabled. Additionally several methods giving; control of the cache have changed return type from void to Int_t, to be able to; return a code to indicate if there was an error. Usually TTree::SetCacheSize will no longer reset the list of branches to be; cached (either set or previously learnt) nor restart the learning phase.; The learning phase is restarted when a new cache is created, e.g. after having; removed a cache with SetCacheSize(0). ### TSelectorDraw. The axis titles in case of a `x:y:z` plot with the option `COLZ` were not correct. ### TParallelCoordVar. Change the format used to print the variables limit for ||-Coord to `%g`. It was; `%6.4f` before. ## Histogram Libraries. ### TFormula. - New version of the TFormula class based on Cling. Formula expressions are now used to create functions which are passed to Cling to be Just In Time compiled.; The expression is therefore compiled using Clang/LLVVM which will give execution time as compiled code and in addition correctness of the result obtained.; - This class is not 100% backward compatible with the old TFormula class, which is still available in ROOT as =ROOT::v5::TFormula=.; Some of the TFormula member funtions available in version 5, such as =Analyze= and =AnalyzeFun",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:14739,cache,cached,14739,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,1,['cache'],['cached']
Performance,"; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local. 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); Must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - s_waitcnt vscnt(0); Must happen after; preceding; global/generic store; atomic/; atomicrmw-no-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:375875,load,load,375875,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"; are, above all, a structuring technique for compiler code. All LLVM passes are subclasses of the `Pass; <https://llvm.org/doxygen/classllvm_1_1Pass.html>`_ class, which implement; functionality by overriding virtual methods inherited from ``Pass``. Depending; on how your pass works, you should inherit from the :ref:`ModulePass; <writing-an-llvm-pass-ModulePass>` , :ref:`CallGraphSCCPass; <writing-an-llvm-pass-CallGraphSCCPass>`, :ref:`FunctionPass; <writing-an-llvm-pass-FunctionPass>` , or :ref:`LoopPass; <writing-an-llvm-pass-LoopPass>`, or :ref:`RegionPass; <writing-an-llvm-pass-RegionPass>` classes, which gives the system more; information about what your pass does, and how it can be combined with other; passes. One of the main features of the LLVM Pass Framework is that it; schedules passes to run in an efficient way based on the constraints that your; pass meets (which are indicated by which class they derive from). We start by showing you how to construct a pass, everything from setting up the; code, to compiling, loading, and executing it. After the basics are down, more; advanced features are discussed. .. warning::; This document deals with the legacy pass manager. LLVM uses the new pass; manager for the optimization pipeline (the codegen pipeline; still uses the legacy pass manager), which has its own way of defining; passes. For more details, see :doc:`WritingAnLLVMNewPMPass` and; :doc:`NewPassManager`. Quick Start --- Writing hello world; ===================================. Here we describe how to write the ""hello world"" of passes. The ""Hello"" pass is; designed to simply print out the name of non-external functions that exist in; the program being compiled. It does not modify the program at all, it just; inspects it. The source code and files for this pass are available in the LLVM; source tree in the ``lib/Transforms/Hello`` directory. .. _writing-an-llvm-pass-makefile:. Setting up the build environment; --------------------------------. First, confi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:1525,load,loading,1525,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['load'],['loading']
Performance,"; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after any; preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_store; sc0=1 sc1=1; atomicrmw release - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic sc0=1; atomicrmw release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - agent - global 1. buffer_wbl2 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:310408,load,load,310408,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; buffer/global/flat_store; nt=1 sc0=1 sc1=1; GFX942; buffer/global/flat_store; nt=1. - volatile. 1. buffer/global/flat_store; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic sc0=1; load atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_load; load atomic monotonic - agent - global 1. buffer/global/flat_load; - generic sc1=1; load atomic monotonic - system - global 1. buffer/global/flat_load; - generic sc0=1 sc1=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; store atomic monotonic - workgroup - global 1. buffer/global/flat_store; - generic sc0=1; store atomic monotonic - agent - global 1. buffer/global/flat_store; - generic sc1=1; store atomic monotonic - system - global 1. buffer/global/flat_store; - generic sc0=1 sc1=1; store atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_store; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; atomicrmw monotonic - system - global 1. buffer/global/flat_atomic; - generic sc1=1;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:294247,load,load,294247,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_inv.; - Ensures the atomicrmw; has completed; before invalidating; the cache. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - g",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:299526,load,loads,299526,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"; changed to not optimize uses at build time, in order to provide the option to; reduce compile-time if the walking is not necessary in a pass. Most users call; the new API ``ensureOptimizedUses()`` to keep the previous behavior and do a; one-time optimization of ``MemoryUse``\ s, if this was not done before.; New pass users are recommended to call ``ensureOptimizedUses()``. Initially it was not possible to optimize ``MemoryDef``\ s in the same way, as we; restricted ``MemorySSA`` to one operand per access.; This was changed and ``MemoryDef``\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<Memo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:12594,cache,cache,12594,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['cache'],['cache']
Performance,"; check at the level of a single volume by using `option`=""`d`"" or; `option`=""`d<number>`"" to perform overlap checking by sampling the; volume with \<`number`\> random points (default 1 million). This; produces also a picture showing in red the overlapping region and; estimates the volume of the overlaps. An extrusion A) is declared in any of the following cases:. - At least one of the vertices of the daughter mesh representation is; outside the mother volume (in fact its shape) and having a safety; distance to the mother greater than the desired value;; - At least one of the mother vertices is contained also by one of its; daughters, in the same conditions. An overlap B) is declared if:. - At least one vertex of a positioned volume mesh is contained (having; a safety bigger than the accepted maximum value) by other positioned; volume inside the same container. The check is performed also by; inverting the candidates. The code is highly optimized to avoid checking candidates that are far; away in space by performing a fast check on their bounding boxes. Once; the checking tool is fired-up inside a volume or at top level, the list; of overlaps (visible as Illegal overlaps inside a **`TBrowser`**) held; by the manager class will be filled with **`TGeoOverlap`** objects; containing a full description of the detected overlaps. The list is; sorted in the decreasing order of the overlapping distance, extrusions; coming first. An overlap object name represents the full description of; the overlap, containing both candidate node names and a letter; (x-extrusion, o-overlap) representing the type. Double-clicking an; overlap item in a **`TBrowser`** produces a picture of the overlap; containing only the two overlapping nodes (one in blue and one in green); and having the critical vertices represented by red points. The picture; can be rotated/zoomed or drawn in X3d as any other view. Calling; `gGeoManager->PrintOverlaps()` prints the list of overlaps. ### Graphical Checking M",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:134240,optimiz,optimized,134240,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,2,"['optimiz', 'perform']","['optimized', 'performing']"
Performance,"; completed before; performing the; store that is being; released. 2. GFX940, GFX941; buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store; sc0=1; store atomic release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_store; store atomic release - agent - global 1. buffer_wbl2 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory have; completed before; performing the; store that is being; released. 3. GFX940, GFX941; buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store; sc1=1; store atomic release - system - global 1. buffer_wbl2 sc0=1 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after any; preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:308454,load,load,308454,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; details.**. #### Creating / Obtaining Viewer Handle. External viewers are bound to a **`TPad`** object (this may be removed; as a requirement in the future). You can create or obtain the current; viewer handle via the method:. ``` {.cpp}; TVirtualViewer3D * v = gPad->GetViewer3D(""type"");; ```. Here the ""type"" string defines the viewer type - currently one of:. - ""`ogl`"" : External GL viewer. - ""`x3d`"": External X3D viewer. - ""`pad`"": Pad viewer. If no type is passed (null string), and there is no current viewer, then; the type is defaulted to ""`pad`"". If no type is passed and there is a; current viewer, then this is returned - hence once a viewer is created; it can be obtained elsewhere by:. ``` {.cpp}; TVirtualViewer3D * v = gPad->GetViewer3D();; ```. #### Opening / Closing Scenes. Objects must be added to viewer between `BeginScene()` and `EndScene()`; calls e.g. ``` {.cpp}; viewer->BeginScene();; // Add objects; viewer ->EndScene();; ```. These calls enable the viewer to suspend redraws, and perform internal; caching/setup. If the object you attach to the pad derives from; **`TAtt3D`**, then the pad will take responsibility for calling; `BeginScene()` and `EndScene()` for you. You can always test if the; scene is already open for object addition with:. ``` {.cpp}; viewer->BuildingScene();; ```. ![Overview of 3D viewer architecture](pictures/030000DF.png). Note: the x3d viewer does not support rebuilding of scenes - objects; added after the first Open/Close Scene pair will be ignored. #### Describing Objects - Filling TBuffer3D. The viewers behind the **`TVirtualViewer3D`** interface differ greatly; in their capabilities e.g. - Some support native shape (e.g. spheres/tubes in OpenGL) and can; draw these based on an abstract description. Others always require a; tessellation description based on **`TBuffer3D`**'s `kRaw` /; `kRawSizes` points/lines/segments sections. - Some need the 3D object positions in the master (world) frame,; others can cope with local frames",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:125702,perform,perform,125702,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['perform'],['perform']
Performance,"; each; level includes all the guarantees of the previous level except for; Acquire/Release. (See also `LangRef Ordering <LangRef.html#ordering>`_.). .. _NotAtomic:. NotAtomic; ---------. NotAtomic is the obvious, a load or store which is not atomic. (This isn't; really a level of atomicity, but is listed here for comparison.) This is; essentially a regular load or store. If there is a race on a given memory; location, loads from that location return undef. Relevant standard; This is intended to match shared variables in C/C++, and to be used in any; other context where memory access is necessary, and a race is impossible. (The; precise definition is in `LangRef Memory Model <LangRef.html#memmodel>`_.). Notes for frontends; The rule is essentially that all memory accessed with basic loads and stores; by multiple threads should be protected by a lock or other synchronization;; otherwise, you are likely to run into undefined behavior. If your frontend is; for a ""safe"" language like Java, use Unordered to load and store any shared; variable. Note that NotAtomic volatile loads and stores are not properly; atomic; do not try to use them as a substitute. (Per the C/C++ standards,; volatile does provide some limited guarantees around asynchronous signals, but; atomics are generally a better solution.). Notes for optimizers; Introducing loads to shared variables along a codepath where they would not; otherwise exist is allowed; introducing stores to shared variables is not. See; `Optimization outside atomic`_. Notes for code generation; The one interesting restriction here is that it is not allowed to write to; bytes outside of the bytes relevant to a store. This is mostly relevant to; unaligned stores: it is not allowed in general to convert an unaligned store; into two aligned stores of the same width as the unaligned store. Backends are; also expected to generate an i8 store as an i8 store, and not an instruction; which writes to surrounding bytes. (If you are writing a ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:6808,load,load,6808,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['load']
Performance,"; editor. Then, it scans all object base classes searching the; corresponding object editors. When it finds one, it makes an instance of; the base class editor too. Once the object editor is in place, it sets the user interface elements; according to the object's status. After that, it is ready to interact; with the object following the user actions. The graphics editor gives an intuitive way to edit objects in a canvas; with immediate feedback. Complexity of some object editors is reduced by; hiding GUI elements and revealing them only on users' requests. An object in the canvas is selected by clicking on it with the left; mouse button. Its name is displayed on the top of the editor frame in; red color. If the editor frame needs more space than the canvas window,; a vertical scroll bar appears for easy navigation. \image html ged.png width=800px. **Histogram, pad and axis editors**. ### Editor Design Elements. The next rules describe the path to follow when creating your own object; editor that will be recognized and loaded by the graphics editor in; ROOT, i.e. it will be included as a part of it. (a) Derive the code of your object editor from the base editor class; **`TGedFrame`**. (b) Keep the correct naming convention: the name of the object editor; should be the object class name concatenated with the word `‘Editor'`. (c) Provide a default constructor. (d) Use the signals/slots communication mechanism for event processing. (e) Implement the virtual method `SetModel(TObject *obj)` where all; widgets are set with the current object's attributes. This method is; called when the editor receives a signal from the canvas saying that an; object is the selected. (f) Implement all necessary slots and connect them to appropriate; signals that GUI widgets send out. The GUI classes in ROOT are developed; to emit signals whenever they change a state that others might be; interested. As we noted already, the signals/slots communication; mechanism allows total independence of",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/ged/doc/index.md:3472,load,loaded,3472,gui/ged/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/ged/doc/index.md,1,['load'],['loaded']
Performance,"; else; return DLSGOrErr.takeError();. // IR added to JD can now link against all symbols exported by the library; // at '/path/to/lib'.; CompileLayer.add(JD, loadModule(...));. The ``DynamicLibrarySearchGenerator`` utility can also be constructed with a; filter function to restrict the set of symbols that may be reflected. For; example, to expose an allowed set of symbols from the main process:. .. code-block:: c++. const DataLayout &DL = getDataLayout();; MangleAndInterner Mangle(ES, DL);. auto &JD = ES.createJITDylib(""main"");. DenseSet<SymbolStringPtr> AllowList({; Mangle(""puts""),; Mangle(""gets""); });. // Use GetForCurrentProcess with a predicate function that checks the; // allowed list.; JD.addGenerator(cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(; DL.getGlobalPrefix(),; [&](const SymbolStringPtr &S) { return AllowList.count(S); })));. // IR added to JD can now link against any symbols exported by the process; // and contained in the list.; CompileLayer.add(JD, loadModule(...));. References to process or library symbols could also be hardcoded into your IR; or object files using the symbols' raw addresses, however symbolic resolution; using the JIT symbol tables should be preferred: it keeps the IR and objects; readable and reusable in subsequent JIT sessions. Hardcoded addresses are; difficult to read, and usually only good for one session. Roadmap; =======. ORC is still undergoing active development. Some current and future works are; listed below. Current Work; ------------. 1. **TargetProcessControl: Improvements to in-tree support for out-of-process; execution**. The ``TargetProcessControl`` API provides various operations on the JIT; target process (the one which will execute the JIT'd code), including; memory allocation, memory writes, function execution, and process queries; (e.g. for the target triple). By targeting this API new components can be; developed which will work equally well for in-process and out-of-process; JITing. 2. **OR",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:34383,load,loadModule,34383,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['load'],['loadModule']
Performance,"; endif(). # if CMAKE_CXX_STANDARD is still set after the cache unset above it means that the user requested it; # and we allow it to be set to something newer than the required standard but otherwise we fail.; if(DEFINED CMAKE_CXX_STANDARD AND CMAKE_CXX_STANDARD LESS ${LLVM_REQUIRED_CXX_STANDARD}); message(FATAL_ERROR ""Requested CMAKE_CXX_STANDARD=${CMAKE_CXX_STANDARD} which is less than the required ${LLVM_REQUIRED_CXX_STANDARD}.""); endif(). set(CMAKE_CXX_STANDARD ${LLVM_REQUIRED_CXX_STANDARD} CACHE STRING ""C++ standard to conform to""); set(CMAKE_CXX_STANDARD_REQUIRED YES). if (CYGWIN); # Cygwin is a bit stricter and lack things like 'strdup', 'stricmp', etc in; # c++xx mode.; set(CMAKE_CXX_EXTENSIONS YES); else(); set(CMAKE_CXX_EXTENSIONS NO); endif(). if (NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES); message(FATAL_ERROR ""; No build type selected. You need to pass -DCMAKE_BUILD_TYPE=<type> in order to configure LLVM.; Available options are:; * -DCMAKE_BUILD_TYPE=Release - For an optimized build with no assertions or debug info.; * -DCMAKE_BUILD_TYPE=Debug - For an unoptimized build with assertions and debug info.; * -DCMAKE_BUILD_TYPE=RelWithDebInfo - For an optimized build with no assertions but with debug info.; * -DCMAKE_BUILD_TYPE=MinSizeRel - For a build optimized for size instead of speed.; Learn more about these options in our documentation at https://llvm.org/docs/CMake.html#cmake-build-type; ""); endif(). # Set default build type for cmake's try_compile module.; # CMake 3.17 or newer sets CMAKE_DEFAULT_BUILD_TYPE to one of the; # items from CMAKE_CONFIGURATION_TYPES. Logic below can be further; # simplified once LLVM's minimum CMake version is updated to 3.17.; if(CMAKE_DEFAULT_BUILD_TYPE); set(CMAKE_TRY_COMPILE_CONFIGURATION ${CMAKE_DEFAULT_BUILD_TYPE}); else(); if(CMAKE_CONFIGURATION_TYPES); list(GET CMAKE_CONFIGURATION_TYPES 0 CMAKE_TRY_COMPILE_CONFIGURATION); elseif(CMAKE_BUILD_TYPE); set(CMAKE_TRY_COMPILE_CONFIGURATION ${CMAKE_BUILD_TYPE});",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:4021,optimiz,optimized,4021,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['optimiz'],['optimized']
Performance,"; filling in; ``COMPUTE_PGM_RSRC1.CDBG_USER``.; 26 1 bit FP16_OVFL GFX6-GFX8; Reserved, must be 0.; GFX9-GFX11; Wavefront starts execution; with specified fp16 overflow; mode. - If 0, fp16 overflow generates; +/-INF values.; - If 1, fp16 overflow that is the; result of an +/-INF input value; or divide by 0 produces a +/-INF,; otherwise clamps computed; overflow to +/-MAX_FP16 as; appropriate. Used by CP to set up; ``COMPUTE_PGM_RSRC1.FP16_OVFL``.; 28:27 2 bits Reserved, must be 0.; 29 1 bit WGP_MODE GFX6-GFX9; Reserved, must be 0.; GFX10-GFX11; - If 0 execute work-groups in; CU wavefront execution mode.; - If 1 execute work-groups on; in WGP wavefront execution mode. See :ref:`amdgpu-amdhsa-memory-model`. Used by CP to set up; ``COMPUTE_PGM_RSRC1.WGP_MODE``.; 30 1 bit MEM_ORDERED GFX6-GFX9; Reserved, must be 0.; GFX10-GFX11; Controls the behavior of the; s_waitcnt's vmcnt and vscnt; counters. - If 0 vmcnt reports completion; of load and atomic with return; out of order with sample; instructions, and the vscnt; reports the completion of; store and atomic without; return in order.; - If 1 vmcnt reports completion; of load, atomic with return; and sample instructions in; order, and the vscnt reports; the completion of store and; atomic without return in order. Used by CP to set up; ``COMPUTE_PGM_RSRC1.MEM_ORDERED``.; 31 1 bit FWD_PROGRESS GFX6-GFX9; Reserved, must be 0.; GFX10-GFX11; - If 0 execute SIMD wavefronts; using oldest first policy.; - If 1 execute SIMD wavefronts to; ensure wavefronts will make some; forward progress. Used by CP to set up; ``COMPUTE_PGM_RSRC1.FWD_PROGRESS``.; 32 **Total size 4 bytes**; ======= ===================================================================================================================. .. .. table:: compute_pgm_rsrc2 for GFX6-GFX12; :name: amdgpu-amdhsa-compute_pgm_rsrc2-gfx6-gfx12-table. ======= ======= =============================== ===========================================================================; Bits Siz",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:169598,load,load,169598,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"; find prebuilt -name ""*.pcm""; # prebuilt/1AYBIGPM8R2GA/A-3L1K4LUA6O31.pcm; # prebuilt/1AYBIGPM8R2GA/B-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/A-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/B-3L1K4LUA6O31.pcm; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -DENABLE_A. Finally we want to allow implicit modules for configurations that were not prebuilt. When using the clang driver a module cache path is implicitly selected. Using ``-cc1``, we simply add use the ``-fmodules-cache-path`` option. .. code-block:: sh. clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache -DENABLE_A; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -fmodules-cache-path=cache -DENABLE_A -DOTHER_OPTIONS. This way, a single directory containing multiple variants of modules can be prepared and reused. The options configuring the module cache are independent of other options. Module Semantics; ================. Modules are modeled as if each submodule were a separate translation unit, and a module import makes names from the other translation unit visible. Each submodule starts with a new preprocessor state and an empty translation unit. .. note::. This behavior is currently only approximated when building a module with submodules. Entities within a submodule that has already been built are visible when building later submodules in that module. This can lead to fragile modules that depend on the build order used for the submodules of the module, and should",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:23696,cache,cache-path,23696,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,2,['cache'],"['cache', 'cache-path']"
Performance,"; global, which is known to be somewhere outside the module. Globals; with ``available_externally`` linkage are allowed to be discarded at; will, and allow inlining and other optimizations. This linkage type is; only allowed on definitions, not declarations.; ``linkonce``; Globals with ""``linkonce``"" linkage are merged with other globals of; the same name when linkage occurs. This can be used to implement; some forms of inline functions, templates, or other code which must; be generated in each translation unit that uses it, but where the; body may be overridden with a more definitive definition later.; Unreferenced ``linkonce`` globals are allowed to be discarded. Note; that ``linkonce`` linkage does not actually allow the optimizer to; inline the body of this function into callers because it doesn't; know if this definition of the function is the definitive definition; within the program or whether it will be overridden by a stronger; definition. To enable inlining and other optimizations, use; ""``linkonce_odr``"" linkage.; ``weak``; ""``weak``"" linkage has the same merging semantics as ``linkonce``; linkage, except that unreferenced globals with ``weak`` linkage may; not be discarded. This is used for globals that are declared ""weak""; in C source code.; ``common``; ""``common``"" linkage is most similar to ""``weak``"" linkage, but they; are used for tentative definitions in C, such as ""``int X;``"" at; global scope. Symbols with ""``common``"" linkage are merged in the; same way as ``weak symbols``, and they may not be deleted if; unreferenced. ``common`` symbols may not have an explicit section,; must have a zero initializer, and may not be marked; ':ref:`constant <globalvars>`'. Functions and aliases may not have; common linkage. .. _linkage_appending:. ``appending``; ""``appending``"" linkage may only be applied to global variables of; pointer to array type. When two global variables with appending; linkage are linked together, the two global arrays are appended; togethe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:9319,optimiz,optimizations,9319,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizations']
Performance,"; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - global 1. buffer/global_atomic; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:300779,load,load,300779,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - generic 1. flat_atomic sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw; with an equal ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:302089,cache,caches,302089,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"; invocation of ""loadHTMLString:baseURL:"".; [self.bodyView loadHTMLString:html baseURL:NULL];. Matcher<ObjCMessageExpr>isClassMessage; Returns true when the Objective-C message is sent to a class. Example; matcher = objcMessageExpr(isClassMessage()); matches; [NSString stringWithFormat:@""format""];; but not; NSString *x = @""hello"";; [x containsString:@""h""];. Matcher<ObjCMessageExpr>isInstanceMessage; Returns true when the Objective-C message is sent to an instance. Example; matcher = objcMessageExpr(isInstanceMessage()); matches; NSString *x = @""hello"";; [x containsString:@""h""];; but not; [NSString stringWithFormat:@""format""];. Matcher<ObjCMessageExpr>matchesSelectorStringRef RegExp, Regex::RegexFlags Flags = NoFlags; Matches ObjC selectors whose name contains; a substring matched by the given RegExp.; matcher = objCMessageExpr(matchesSelector(""loadHTMLStringmatches the outer message expr in the code below, but NOT the message; invocation for self.bodyView.; [self.bodyView loadHTMLString:html baseURL:NULL];. If the matcher is used in clang-query, RegexFlags parameter; should be passed as a quoted string. e.g: ""NoFlags"".; Flags can be combined with '|' example ""IgnoreCase | BasicRegex"". Matcher<ObjCMessageExpr>numSelectorArgsunsigned N; Matches when the selector has the specified number of arguments. matcher = objCMessageExpr(numSelectorArgs(0));; matches self.bodyView in the code below. matcher = objCMessageExpr(numSelectorArgs(2));; matches the invocation of ""loadHTMLString:baseURL:"" but not that; of self.bodyView; [self.bodyView loadHTMLString:html baseURL:NULL];. Matcher<ObjCMethodDecl>isClassMethod; Returns true when the Objective-C method declaration is a class method. Example; matcher = objcMethodDecl(isClassMethod()); matches; @interface I + (void)foo; @end; but not; @interface I - (void)bar; @end. Matcher<ObjCMethodDecl>isDefinition; Matches if a declaration has a body attached. Example matches A, va, fa; class A {};; class B; // Doesn't match, as it has no b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html:108123,load,loadHTMLString,108123,interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,2,['load'],['loadHTMLString']
Performance,"; key in memory and deleted the object at the end, but the container is; not owner by default, so all objects inside leaked. PROOF-Lite. Fix a couple of memory leaks showing up when running; repeated queries; Fix a problem in TProofServ::CopyFromCache affecting; the case where the sandbox dir has a '.' and the macro name has no '.',; e.g. compiled selectors in PROOF-Lite. TProofOutputFile. Fix a problem with the determination of the fDir member; affecting mostly PROOF-Lite; Fix a serious issue whose net effect was to delete the; outputfile just after having open it. XrdProofd plugin. Make sure that the limit on the number of old; sessions is applied whenever a new session is started and not only when; the daemon is started.; Fix the behaviour of the xpd.allowedusers directive: if; at least one of these directives is present, users in the password file; are not allowed by default but must be explicitly appear in one; xpd.allowedusers directive  ; Fix a source for memory leak in; XrdProofdProtocol::SendMsg; Optimize the usage of strings in a few places. DataSet manager. Correctly classify as TTree all TTree derived classes; (e.g. TNtuple's); Fix a problem in saving the end-point URL for local; files; Improve realtime notification during 'verify'. TProofDraw. Fix a problem with the axis ranges of the underlying; histogram in PolyMarker3D; Allow to use the default pad instead of forcing; creation of one pad per object; Add wrapper to handle the feedback default canvas. TEventIter. Fix a problem with changing the tree cache size: the; size was reset to the default value after the first file. TDataSetManagerFile. Solve a consistency problem in checking URLs for; duplication when adding them to the relevant TFileInfo; During dataset validation, do not fail on duplications; but notify and add them to the bad file list. TPacketizerAdaptive, TPacketizer. Improve data node / worker matching by always using the; host FQDN. TPacketizerUnit, TEventIter. Make sure that the entry; n",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html:7884,Optimiz,Optimize,7884,proof/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html,1,['Optimiz'],['Optimize']
Performance,"; know that ``foo`` and ``test`` were not mutually recursive for the poll to be; redundant. In practice, you'd probably want to your poll definition to contain; a conditional branch of some form. At the moment, PlaceSafepoints can insert safepoint polls at method entry and; loop backedges locations. Extending this to work with return polls would be; straight forward if desired. PlaceSafepoints includes a number of optimizations to avoid placing safepoint; polls at particular sites unless needed to ensure timely execution of a poll; under normal conditions. PlaceSafepoints does not attempt to ensure timely; execution of a poll under worst case conditions such as heavy system paging. The implementation of a safepoint poll action is specified by looking up a; function of the name ``gc.safepoint_poll`` in the containing Module. The body; of this function is inserted at each poll site desired. While calls or invokes; inside this method are transformed to a ``gc.statepoints``, recursive poll; insertion is not performed. This pass is useful for any language frontend which only has to support; garbage collection semantics at safepoints. If you need other abstract; frame information at safepoints (e.g. for deoptimization or introspection),; you can insert safepoint polls in the frontend. If you have the later case,; please ask on llvm-dev for suggestions. There's been a good amount of work; done on making such a scheme work well in practice which is not yet documented; here. Supported Architectures; =======================. Support for statepoint generation requires some code for each backend.; Today, only Aarch64 and X86_64 are supported. .. _OpenWork:. Limitations and Half Baked Ideas; ================================. Mixing References and Raw Pointers; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Support for languages which allow unmanaged pointers to garbage collected; objects (i.e. pass a pointer to an object to a C routine) in the abstract; machine model. At the moment, the be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:33334,perform,performed,33334,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,1,['perform'],['performed']
Performance,"; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - s_waitcnt vscnt(0); Must happen after; preceding; global/generic store; atomic/; atomicrmw-no-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global/local; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; load atomic seq_cst - workgroup - local. 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); Must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not nee",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:375350,load,load,375350,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - generic 1. flat_atomic sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-at",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:302177,load,load,302177,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; loads will not see; stale data. atomicrmw acq_rel - agent - global 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - global 1. buffer_wbl2. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw tha",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:270006,load,load,270006,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; mode, omit.; - Use vmcnt(0) if atomic with; return and vscnt(0) if; atomic with no-return.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 4. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atom",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:365117,perform,performing,365117,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"; number, and each offset, interpreted as a signed number, does not wrap the; unsigned address space and remains *in bounds* of the allocated object.; As a corollary, if the added offset is non-negative, the addition does not; wrap in an unsigned sense (``nuw``).; * In cases where the base is a vector of pointers, the ``inbounds`` keyword; applies to each of the computations element-wise. Note that ``getelementptr`` with all-zero indices is always considered to be; ``inbounds``, even if the base pointer does not point to an allocated object.; As a corollary, the only pointer in bounds of the null pointer in the default; address space is the null pointer itself. These rules are based on the assumption that no allocated object may cross; the unsigned address space boundary, and no allocated object may be larger; than half the pointer index type space. If the ``inrange`` keyword is present before any index, loading from or; storing to any pointer derived from the ``getelementptr`` has undefined; behavior if the load or store would access memory outside of the bounds of; the element selected by the index marked as ``inrange``. The result of a; pointer comparison or ``ptrtoint`` (including ``ptrtoint``-like operations; involving memory) involving a pointer derived from a ``getelementptr`` with; the ``inrange`` keyword is undefined, with the exception of comparisons; in the case where both operands are in the range of the element selected; by the ``inrange`` keyword, inclusive of the address one past the end of; that element. Note that the ``inrange`` keyword is currently only allowed; in constant ``getelementptr`` expressions. The getelementptr instruction is often confusing. For some more insight; into how it works, see :doc:`the getelementptr FAQ <GetElementPtr>`. Example:; """""""""""""""". .. code-block:: llvm. %aptr = getelementptr {i32, [12 x i8]}, ptr %saptr, i64 0, i32 1; %vptr = getelementptr {i32, <2 x i8>}, ptr %svptr, i64 0, i32 1, i32 1; %eptr = getelementptr [12 x i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:438913,load,loading,438913,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['load'],"['load', 'loading']"
Performance,"; numeric string value for how many instructions should be in the function before; it gets instrumented. .. code-block:: llvm. define i32 @maybe_instrument() uwtable ""xray-instruction-threshold""=""2"" {; ; ...; }. Special Case File; -----------------. Attributes can be imbued through the use of special case files instead of; adding them to the original source files. You can use this to mark certain; functions and classes to be never, always, or instrumented with first-argument; logging from a file. The file's format is described below:. .. code-block:: bash. # Comments are supported; [always]; fun:always_instrument; fun:log_arg1=arg1 # Log the first argument for the function. [never]; fun:never_instrument. These files can be provided through the ``-fxray-attr-list=`` flag to clang.; You may have multiple files loaded through multiple instances of the flag. XRay Runtime Library; --------------------. The XRay Runtime Library is part of the compiler-rt project, which implements; the runtime components that perform the patching and unpatching of inserted; instrumentation points. When you use ``clang`` to link your binaries and the; ``-fxray-instrument`` flag, it will automatically link in the XRay runtime. The default implementation of the XRay runtime will enable XRay instrumentation; before ``main`` starts, which works for applications that have a short; lifetime. This implementation also records all function entry and exit events; which may result in a lot of records in the resulting trace. Also by default the filename of the XRay trace is ``xray-log.XXXXXX`` where the; ``XXXXXX`` part is randomly generated. These options can be controlled through the ``XRAY_OPTIONS`` environment; variable, where we list down the options and their defaults below. +-------------------+-----------------+---------------+------------------------+; | Option | Type | Default | Description |; +===================+=================+===============+========================+; | patch_premain | ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRay.rst:4692,perform,perform,4692,interpreter/llvm-project/llvm/docs/XRay.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/XRay.rst,1,['perform'],['perform']
Performance,"; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also does something similar to constpool load:; LPCRELL0:; ldr r0, LCPI1_0; =>; ldr r0, pc, #((LCPI1_0-(LPCRELL0+4))&0xfffffffc). //===---------------------------------------------------------------------===//. We compile the following:. define i16 @func_entry_2E_ce(i32 %i) {; switch i32 %i, label %bb12.exitStub [; i32 0, label %bb4.exitStub; i32 1, label %bb9.exitStub; i32 2, label %bb4.exitStub; i32 3, label %bb4.exitStub; i32 7, label %bb9.exitStub; i32 8, label %bb.exitStub; i32 9, label %bb9.exitStub; ]. bb12.exitStub:; ret i16 0. bb4.exitStub:; ret i16 1. bb9.exitStub:; ret i16 2. bb.exitStub:; ret i16 3; }. into:. _func_entry_2E_ce:; mov r2, #1; lsl r2, r0; cmp r0, #9; bhi LBB1_4 @bb12.exitStub; LBB1_1: @newFuncRoot; mov r1, #13; tst r2, r1; bne LBB1_5 @bb4.exitStub; LBB1_2: @newFuncRoot; ldr r1, LCPI1_0; tst r2, r1; bne LBB1_6 @bb9.exitStub; LBB1_3: @newFuncRoot; mov r1, #1; lsl r1, r1, #8; tst r2, r1; bne LBB1_7 @bb.exitStub; LBB1_4: @bb12.exitStub; mov r0, #0; bx lr; LBB1_5: @bb4.exitStub; mov r0, #1; bx lr; LBB1_6: @bb9.exitStub",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:2034,load,load,2034,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,2,['load'],['load']
Performance,"; option(LLVM_ENABLE_LLVM_LIBC ""Set to on to link all LLVM executables against LLVM libc, assuming it is accessible by the host compiler."" OFF); option(LLVM_STATIC_LINK_CXX_STDLIB ""Statically link the standard library."" OFF); option(LLVM_ENABLE_LLD ""Use lld as C and C++ linker."" OFF); option(LLVM_ENABLE_PEDANTIC ""Compile with pedantic enabled."" ON); option(LLVM_ENABLE_WERROR ""Fail and stop if a warning is triggered."" OFF). option(LLVM_ENABLE_DUMP ""Enable dump functions even when assertions are disabled"" OFF); option(LLVM_UNREACHABLE_OPTIMIZE ""Optimize llvm_unreachable() as undefined behavior (default), guaranteed trap when OFF"" ON). if( NOT uppercase_CMAKE_BUILD_TYPE STREQUAL ""DEBUG"" ); option(LLVM_ENABLE_ASSERTIONS ""Enable assertions"" OFF); else(); option(LLVM_ENABLE_ASSERTIONS ""Enable assertions"" ON); endif(). option(LLVM_ENABLE_EXPENSIVE_CHECKS ""Enable expensive checks"" OFF). # While adding scalable vector support to LLVM, we temporarily want to; # allow an implicit conversion of TypeSize to uint64_t, and to allow; # code to get the fixed number of elements from a possibly scalable vector.; # This CMake flag enables a more strict mode where it asserts that the type; # is not a scalable vector type.; #; # Enabling this flag makes it easier to find cases where the compiler makes; # assumptions on the size being 'fixed size', when building tests for; # SVE/SVE2 or other scalable vector architectures.; option(LLVM_ENABLE_STRICT_FIXED_SIZE_VECTORS; ""Enable assertions that type is not scalable in implicit conversion from TypeSize to uint64_t and calls to getNumElements"" OFF). set(LLVM_ABI_BREAKING_CHECKS ""WITH_ASSERTS"" CACHE STRING; ""Enable abi-breaking checks. Can be WITH_ASSERTS, FORCE_ON or FORCE_OFF.""). option(LLVM_FORCE_USE_OLD_TOOLCHAIN; ""Set to ON to force using an old, unsupported host toolchain."" OFF). set(LLVM_LOCAL_RPATH """" CACHE FILEPATH; ""If set, an absolute path added as rpath on binaries that do not already contain an executable-relative rpath.""). option",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:25491,scalab,scalable,25491,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,2,['scalab'],['scalable']
Performance,"; particular some code that used to run with CINT will either issue new; warnings or new compilation errors. For example when CINT was parsing; Namespace::%Symbol it would not only apply the C++ search rules but also; search in the outer scopes and for this example could actually return; ::%Symbol instead of (as Cling now does) issuing a compilation error. #### Template class names; Cling no longer supports refering to a class template instantiation of a; class template that has all default template parameter without the \<\>.; With:. ``` {.cpp}; template <typename T = int> class templt {};; ```. With Cling (and any standard compliant compiler), using `*templt<>*` is; allowed (but `*templt*` is not). #### Namespace prefix of template parameters; Given `namespace N { class A; template <typename T> class B;}`, the name; `N::B<N::A>` is no longer ""shortened"" to `N::B<A>`. This affects the forward; and backward compatibility of files. #### Implicit dynamic up-casts; CINT would perform automatic upcasts to derived classes under certain contexts:. ``` {.cpp}; TH1* h1 = hpx; TH1F* h1f = h1;; ```. Cling does not allow this anymore. We might add this feature later if demand exists ([ROOT-4802](https://sft.its.cern.ch/jira/browse/ROOT-4802)). #### Using symbols that are only available at runtime: load libFoo; foo(); CINT was processing macros line by line; Cling compiles code.; When calling a function (or in general using a symbol) that is provided by a library loaded at runtime,; Cling will in some cases report an unresolved symbol:. ``` {.cpp}; #include ""Event.h""; void dynload() {; gSystem->Load(""libEvent"");; new Event();; }; ```. You will currently have to provide a rootmap file for libEvent (which also requires include; guards for Event.h). This might get fixed in a later version ([ROOT-4691](https://sft.its.cern.ch/jira/browse/ROOT-4691)). #### Using identifiers that are only available at runtime: gROOT->LoadMacro(""foo.h""); foo(); CINT was processing macros line by line; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/doc/v600/index.md:3426,perform,perform,3426,core/doc/v600/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/doc/v600/index.md,1,['perform'],['perform']
Performance,"; pruning. Cache pruning is supported with gold, ld64, and lld, but currently only; gold and lld allow you to control the policy with a policy string. The cache; policy must be specified with a linker option. - gold (as of LLVM 6.0):; ``-Wl,-plugin-opt,cache-policy=POLICY``; - ELF ld.lld (as of LLVM 5.0), Mach-O ld64.lld (as of LLVM 15.0):; ``-Wl,--thinlto-cache-policy=POLICY``; - COFF lld-link (as of LLVM 6.0):; ``/lldltocachepolicy:POLICY``. A policy string is a series of key-value pairs separated by ``:`` characters.; Possible key-value pairs are:. - ``cache_size=X%``: The maximum size for the cache directory is ``X`` percent; of the available space on the disk. Set to 100 to indicate no limit,; 50 to indicate that the cache size will not be left over half the available; disk space. A value over 100 is invalid. A value of 0 disables the percentage; size-based pruning. The default is 75%. - ``cache_size_bytes=X``, ``cache_size_bytes=Xk``, ``cache_size_bytes=Xm``,; ``cache_size_bytes=Xg``:; Sets the maximum size for the cache directory to ``X`` bytes (or KB, MB,; GB respectively). A value over the amount of available space on the disk; will be reduced to the amount of available space. A value of 0 disables; the byte size-based pruning. The default is no byte size-based pruning. Note that ThinLTO will apply both size-based pruning policies simultaneously,; and changing one does not affect the other. For example, a policy of; ``cache_size_bytes=1g`` on its own will cause both the 1GB and default 75%; policies to be applied unless the default ``cache_size`` is overridden. - ``cache_size_files=X``:; Set the maximum number of files in the cache directory. Set to 0 to indicate; no limit. The default is 1000000 files. - ``prune_after=Xs``, ``prune_after=Xm``, ``prune_after=Xh``: Sets the; expiration time for cache files to ``X`` seconds (or minutes, hours; respectively). When a file hasn't been accessed for ``prune_after`` seconds,; it is removed from the cache. A value of",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:6182,cache,cache,6182,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,1,['cache'],['cache']
Performance,"; release.; * An L2 cache can be kept coherent with other L2 caches by using the MTYPE RW; (read-write) for memory local to the L2, and MTYPE NC (non-coherent) with; the PTE C-bit set for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by the PTE C-bit.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequent access from the GPU will automatically invalidate or writeback; the CPU cache due to the L2 probe filter.; * To ensure coherence of local memory writes of CUs with different L1 caches; in the same agent a ``buffer_wbl2`` is required. It does nothing if the; agent is configured to have a single L2, or will writeback dirty L2 cache; lines if configured to have multiple L2 caches.; * To ensure coherence of local memory writes of CUs in different agents a; ``buffer_wbl2 sc1`` is required. It will writeback dirty L2 cache lines.; * To ensure coherence of local memory reads of CUs with different L1 caches; in the same agent a ``buffer_inv sc1`` is required. It does nothing if the; agent is configured to have a single L2, or will invalidate non-local L2; cache lines if configured to have multiple L2 caches.; * To ensure coherence of local memory reads of CUs in different agents a; ``buffer_inv sc0 sc1`` is required. It will invalidate non-local L2 cache; lines if configured to have multiple L2 caches. * PCIe access from the GPU to the CPU can be kept coherent by using the MTYPE; UC (uncached) which bypasses the L2. Scalar memory operations are only used to access memory that is proven to not; change during the execution of the kernel dispatch. This includes constant; address space and global address space for program scope ``const`` variables.; Therefore, the kernel machine code does not have to maintain the scalar cache to; ensure it is coherent with the vector caches. The scalar and vector",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:289183,cache,cache,289183,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic. atomicrmw acq_rel - workgroup - local 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:223968,load,load,223968,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; sandbox file removal; file upload, download. TProofDraw. Allow to set a color, size, size, width for lines,; area, markers; the attributes are transmitted via the input list and; automatically derived from the ones of the chain. Support automatic creation of a dataset out of files; created on the worker nodes by worker processes. The implementation is; an extension of the functionality of the class TProofOutputFile used; for merging via file.; Add the possibility to enable/disable the tree cache and; to change its size on per-query base; two new parameters are available:. PROOF_UseTreeCache   ; Int_t       ; Enable (0) or Disable (1) the tree cache (default 1); PROOF_CacheSize      ; Long64_t     Cache size in bytes; (default 10000000). Examples:;        ; a) to disable the cache for the next run enter:;                                 ; proof->SetParameter(""PROOF_UseTreeCache"", 0);        ; b) to set the cache size to 20M;                                 ; proof->SetParameter(""PROOF_CacheSize"", 20000000);  Add the parameter; PROOF_UseParallelUnzip to toggle the use of the parallel unzip; (default off for now); to enable it add the following call;            ;            ;        ;  proof->SetParameter(""PROOF_UseParallelUnzip"", 1).  Add the possibility to give indications about; the number of workers at startup.;  E.g.;        1. To; start max 5 workers;             ; TProof::Open(""<master>"",""workers=5"");        2. To; start max 2 workers per physical machine;             ; TProof::Open(""<master>"",""workers=2x"");      This is useful in general when; running tests (equivalent but quicker then full startup;      followed by; TProof::SetParallel(n) or TProof::DeactivateWorker(...)).; Add support for the worker SysInfo_t in TSlaveInfo; (obtained via TProof::GetListOfSlaveInfos()); Add new submerger functionality to speed up the merging; phase. At the end of the query, a set of workers are promoted; submergers and assigned a sub-set of workers to merge. Once each; sub-me",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html:907,cache,cache,907,proof/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v526/index.html,4,['cache'],['cache']
Performance,"; sequence of loads, shifts and masks (or load-right + load-left on MIPS) for; all cases where the load / store does not have a sufficiently high alignment; in the IR. The alignment is used to guarantee the alignment on allocas and globals,; though in most cases this is unnecessary (most targets have a sufficiently; high default alignment that they’ll be fine). It is also used to provide a; contract to the back end saying ‘either this load/store has this alignment, or; it is undefined behavior’. This means that the back end is free to emit; instructions that rely on that alignment (and mid-level optimizers are free to; perform transforms that require that alignment). For x86, it doesn’t make; much difference, as almost all instructions are alignment-independent. For; MIPS, it can make a big difference. Note that if your loads and stores are atomic, the backend will be unable to; lower an under aligned access into a sequence of natively aligned accesses.; As a result, alignment is mandatory for atomic loads and stores. Other Things to Consider; ^^^^^^^^^^^^^^^^^^^^^^^^. #. Use ptrtoint/inttoptr sparingly (they interfere with pointer aliasing; analysis), prefer GEPs. #. Prefer globals over inttoptr of a constant address - this gives you; dereferencability information. In MCJIT, use getSymbolAddress to provide; actual address. #. Be wary of ordered and atomic memory operations. They are hard to optimize; and may not be well optimized by the current optimizer. Depending on your; source language, you may consider using fences instead. #. If calling a function which is known to throw an exception (unwind), use; an invoke with a normal destination which contains an unreachable; instruction. This form conveys to the optimizer that the call returns; abnormally. For an invoke which neither returns normally or requires unwind; code in the current function, you can use a noreturn call instruction if; desired. This is generally not required because the optimizer will convert; an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:5958,load,loads,5958,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['load'],['loads']
Performance,"; serv->SetItemField(""/"", ""_drawopt"", ""colz"");; ```. In such case URL parameters are not required - specified item will be displayed automatically when web page is opened.; One also can configure to display several items at once. For that one also can configure layout of the drawing area:. ```cpp; serv->SetItemField(""/"", ""_layout"", ""grid2x2""); // layout for drawing area; serv->SetItemField(""/"", ""_drawitem"", ""[Files/job1.root/hpxpy,Files/job1.root/hpx]""); // items; serv->SetItemField(""/"", ""_drawopt"", ""[colz,hist]""); // options; ```. One also can change appearance of hierarchy browser on the left side of the web page:. ```cpp; serv->SetItemField(""/"", ""_browser"", ""off""); // allowed ""fix"" (default), ""float"", ""no"", ""off""; serv->SetItemField(""/"", ""_toptitle"", ""Custom title""); // title of web page, shown when browser off; ```. If necessary, one also can automatically open ROOT file when web page is opened:. ```cpp; serv->SetItemField(""/"", ""_loadfile"", ""currentdir/hsimple.root""); // name of ROOT file to load; ```. ## Configuring user access. By default, the http server is open for anonymous access. One could restrict the access to the server for authenticated users only. First of all, one should create a password file, using the **htdigest** utility. ```bash; [shell] htdigest -c .htdigest domain_name user_name; ```. It is recommended not to use special symbols in domain or user names. Several users can be add to the "".htdigest"" file. When starting the server, the following arguments should be specified:. ```cpp; auto serv = new THttpServer(""http:8080?auth_file=.htdigest&auth_domain=domain_name"");; ```. After that, the web browser will automatically request to input a name/password for the domain ""domain_name"". Based on authorized accounts, one could restrict or enable access to some elements in the server objects hierarchy, using `THttpServer::Restrict()` method. For instance, one could hide complete folder from 'guest' account:. ```cpp; serv->Restrict(""/Folder"", ""hidden=gu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/HttpServer/HttpServer.md:8086,load,load,8086,documentation/HttpServer/HttpServer.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/HttpServer/HttpServer.md,1,['load'],['load']
Performance,"; set(LLVM_EXEGESIS_TARGETS); if (LLVM_TARGETS_TO_BUILD MATCHES ""X86""); list(APPEND LLVM_EXEGESIS_TARGETS ""X86""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""AArch64""); list(APPEND LLVM_EXEGESIS_TARGETS ""AArch64""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""PowerPC""); list(APPEND LLVM_EXEGESIS_TARGETS ""PowerPC""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""Mips""); list(APPEND LLVM_EXEGESIS_TARGETS ""Mips""); endif(). set(LLVM_EXEGESIS_TARGETS ${LLVM_EXEGESIS_TARGETS} PARENT_SCOPE). foreach(t ${LLVM_EXEGESIS_TARGETS}); add_subdirectory(${t}); endforeach(). set(LLVM_LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; ExecutionEngine; GlobalISel; MC; MCA; MCDisassembler; MCParser; Object; ObjectYAML; OrcJIT; RuntimeDyld; Support; TargetParser; ). set(libs); if(LLVM_ENABLE_LIBPFM AND HAVE_LIBPFM); list(APPEND libs pfm); endif(); if(HAVE_LIBRT); list(APPEND libs rt); endif(). add_llvm_library(LLVMExegesis; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Analysis.cpp; Assembler.cpp; BenchmarkResult.cpp; BenchmarkRunner.cpp; Clustering.cpp; CodeTemplate.cpp; DisassemblerHelper.cpp; Error.cpp; LatencyBenchmarkRunner.cpp; LlvmState.cpp; MCInstrDescView.cpp; ParallelSnippetGenerator.cpp; PerfHelper.cpp; RegisterAliasing.cpp; RegisterValue.cpp; SchedClassResolution.cpp; SerialSnippetGenerator.cpp; SnippetFile.cpp; SnippetGenerator.cpp; SnippetRepetitor.cpp; SubprocessMemory.cpp; Target.cpp; UopsBenchmarkRunner.cpp. LINK_LIBS ${libs}. DEPENDS; intrinsics_gen; ); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt:1096,Latency,LatencyBenchmarkRunner,1096,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt,1,['Latency'],['LatencyBenchmarkRunner']
Performance,"; small overhead for each allocation. The checksum is computed using a CRC32 (made faster with hardware support); of the global secret, the chunk pointer itself, and the 8 bytes of header with; the checksum field zeroed out. It is not intended to be cryptographically; strong. The header is atomically loaded and stored to prevent races. This is important; as two consecutive chunks could belong to different threads. We work on local; copies and use compare-exchange primitives to update the headers in the heap; memory, and avoid any type of double-fetching. Randomness; ----------; Randomness is a critical factor to the additional security provided by the; allocator. The allocator trusts the memory mapping primitives of the OS to; provide pages at (mostly) non-predictable locations in memory, as well as the; binaries to be compiled with ASLR. In the event one of those assumptions is; incorrect, the security will be greatly reduced. Scudo further randomizes how; blocks are allocated in the Primary, can randomize how caches are assigned to; threads. Memory reclaiming; -----------------; Primary and Secondary allocators have different behaviors with regard to; reclaiming. While Secondary mapped allocations can be unmapped on deallocation,; it isn't the case for the Primary, which could lead to a steady growth of the; RSS of a process. To counteract this, if the underlying OS allows it, pages; that are covered by contiguous free memory blocks in the Primary can be; released: this generally means they won't count towards the RSS of a process and; be zero filled on subsequent accesses). This is done in the deallocation path,; and several options exist to tune this behavior. Usage; =====. Platform; --------; If using Fuchsia or an Android version greater than 11, your memory allocations; are already service by Scudo (note that Android Svelte configurations still use; jemalloc). Library; -------; The allocator static library can be built from the LLVM tree thanks to the; ``scud",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst:4856,cache,caches,4856,interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,1,['cache'],['caches']
Performance,"; src/TStreamerInfoReadBuffer.cxx; src/TStreamerInfoWriteBuffer.cxx; src/TZIPFile.cxx; $<TARGET_OBJECTS:RootPcmObjs>; LIBRARIES; ${CMAKE_DL_LIBS}; DEPENDENCIES; Core; Thread; ). target_include_directories(RIO PRIVATE ${CMAKE_SOURCE_DIR}/core/clib/res); target_link_libraries(RIO PUBLIC ${ROOT_ATOMIC_LIBS}). if(builtin_nlohmannjson); target_include_directories(RIO PRIVATE ${CMAKE_SOURCE_DIR}/builtins); else(); target_link_libraries(RIO PRIVATE nlohmann_json::nlohmann_json); endif(). if(root7); set(RIO_EXTRA_HEADERS ROOT/RFile.hxx); target_sources(RIO PRIVATE v7/src/RFile.cxx); endif(). if(uring); target_link_libraries(RIO PUBLIC ${LIBURING_LIBRARY}); target_include_directories(RIO PRIVATE ${LIBURING_INCLUDE_DIR}); endif(). ROOT_GENERATE_DICTIONARY(G__RIO; ROOT/RRawFile.hxx; ROOT/RRawFileTFile.hxx; ${rawfile_local_headers}; ROOT/TBufferMerger.hxx; TArchiveFile.h; TBufferFile.h; TBufferText.h; TBufferIO.h; TBufferJSON.h; TCollectionProxyFactory.h; TContainerConverters.h; TEmulatedMapProxy.h; TEmulatedCollectionProxy.h; TDirectoryFile.h; TFileCacheRead.h; TFileMerger.h; TFree.h; TFileCacheWrite.h; TFilePrefetch.h; TFile.h; TFPBlock.h; TGenCollectionStreamer.h; TGenCollectionProxy.h; TKey.h; TKeyMapFile.h; TLockFile.h; TMemFile.h; TMapFile.h; TMakeProject.h; TStreamerInfoActions.h; TVirtualCollectionIterators.h; TStreamerInfo.h; TZIPFile.h; ${RIO_EXTRA_HEADERS}; STAGE1; MODULE; RIO; LINKDEF; LinkDef.h; OPTIONS; -writeEmptyRootPCM; DEPENDENCIES; Core; Thread; ). # TStreamerInfoReadBuffer in O0 needs 6k on the stack. It is called; # recursively, quickly exhausting the stack. Prevent that by forcing; # the many scope-local vars to share their stack space / become; # registers, thanks to the optimizer.; if(MSVC); set_source_files_properties(src/TStreamerInfoReadBuffer.cxx COMPILE_FLAGS $<IF:$<CONFIG:Debug>,""/Od"",""/O2"">); else(); set_source_files_properties(src/TStreamerInfoReadBuffer.cxx COMPILE_FLAGS ""-O3""); endif(). ROOT_INSTALL_HEADERS(). ROOT_ADD_TEST_SUBDIRECTORY(test); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/CMakeLists.txt:3156,optimiz,optimizer,3156,io/io/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/CMakeLists.txt,2,['optimiz'],['optimizer']
Performance,"; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; sc1=1; 4. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acq_rel - agent - generic 1. buffer_wbl2 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:321947,load,load,321947,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acq_rel - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before; the following; buffer_inv.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup - generic ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:317488,load,load,317488,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acq_rel - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global_atomic; 3. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before; the following; buffer_wbinvl1_vol.; - Ensures any; following global; data read is no; older than the; atomicrmw value; being acquired. 4. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. atomicrmw acq_rel - workgroup ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:266965,load,load,266965,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; those of :doc:`llc <CommandGuide/llc>` and the triple is required. For example,; the following command would fuzz AArch64 with :doc:`GlobalISel/index`:. .. code-block:: shell. % bin/llvm-isel-fuzzer <corpus-dir> -ignore_remaining_args=1 -mtriple aarch64 -global-isel -O0. Some flags can also be specified in the binary name itself in order to support; OSS Fuzz, which has trouble with required arguments. To do this, you can copy; or move ``llvm-isel-fuzzer`` to ``llvm-isel-fuzzer--x-y-z``, separating options; from the binary name using ""--"". The valid options are architecture names; (``aarch64``, ``x86_64``), optimization levels (``O0``, ``O2``), or specific; keywords, like ``gisel`` for enabling global instruction selection. In this; mode, the same example could be run like so:. .. code-block:: shell. % bin/llvm-isel-fuzzer--aarch64-O0-gisel <corpus-dir>. llvm-opt-fuzzer; ---------------. A |LLVM IR fuzzer| aimed at finding bugs in optimization passes. It receives optimization pipeline and runs it for each fuzzer input. Interface of this fuzzer almost directly mirrors ``llvm-isel-fuzzer``. Both; ``mtriple`` and ``passes`` arguments are required. Passes are specified in a; format suitable for the new pass manager. You can find some documentation about; this format in the doxygen for ``PassBuilder::parsePassPipeline``. .. code-block:: shell. % bin/llvm-opt-fuzzer <corpus-dir> -ignore_remaining_args=1 -mtriple x86_64 -passes instcombine. Similarly to the ``llvm-isel-fuzzer`` arguments in some predefined configurations; might be embedded directly into the binary file name:. .. code-block:: shell. % bin/llvm-opt-fuzzer--x86_64-instcombine <corpus-dir>. llvm-mc-assemble-fuzzer; -----------------------. A |generic fuzzer| that fuzzes the MC layer's assemblers by treating inputs as; target specific assembly. Note that this fuzzer has an unusual command line interface which is not fully; compatible with all of libFuzzer's features. Fuzzer arguments must be passed; after ``--f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst:3419,optimiz,optimization,3419,interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,1,['optimiz'],['optimization']
Performance,"; type; for an indirect function call it is the function signature. The; mapping from a type to an identifier is an ABI detail. In the current,; experimental, implementation the identifier of type T is calculated as; follows:. - Obtain the mangled name for ""typeinfo name for T"".; - Calculate MD5 hash of the name as a string.; - Reinterpret the first 8 bytes of the hash as a little-endian; 64-bit integer. It is possible, but unlikely, that collisions in the; ``CallSiteTypeId`` hashing will result in weaker CFI checks that would; still be conservatively correct. CFI_Check; ---------. In the general case, only the target DSO knows whether the call to; function ``f`` with type ``CallSiteTypeId`` is valid or not. To; export this information, every DSO implements. .. code-block:: none. void __cfi_check(uint64 CallSiteTypeId, void *TargetAddr, void *DiagData). This function provides external modules with access to CFI checks for; the targets inside this DSO. For each known ``CallSiteTypeId``, this; function performs an ``llvm.type.test`` with the corresponding type; identifier. It reports an error if the type is unknown, or if the; check fails. Depending on the values of compiler flags; ``-fsanitize-trap`` and ``-fsanitize-recover``, this function may; print an error, abort and/or return to the caller. ``DiagData`` is an; opaque pointer to the diagnostic information about the error, or; ``null`` if the caller does not provide this information. The basic implementation is a large switch statement over all values; of CallSiteTypeId supported by this DSO, and each case is similar to; the InlinedFastCheck() in the basic CFI mode. CFI Shadow; ----------. To route CFI checks to the target DSO's __cfi_check function, a; mapping from possible virtual / indirect call targets to the; corresponding __cfi_check functions is maintained. This mapping is; implemented as a sparse array of 2 bytes for every possible page (4096; bytes) of memory. The table is kept readonly most of the time.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:21134,perform,performs,21134,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,1,['perform'],['performs']
Performance,"; use ``__has_feature((objc_arc_weak))``; to test whether they are enabled. Unsafe references are enabled; unconditionally. ARC-style weak and unsafe references cannot be used; when Objective-C garbage collection is enabled. Except as noted below, the language rules for the ``__weak`` and; ``__unsafe_unretained`` qualifiers (and the ``weak`` and; ``unsafe_unretained`` property attributes) are just as laid out; in the :doc:`ARC specification <AutomaticReferenceCounting>`.; In particular, note that some classes do not support forming weak; references to their instances, and note that special care must be; taken when storing weak references in memory where initialization; and deinitialization are outside the responsibility of the compiler; (such as in ``malloc``-ed memory). Loading from a ``__weak`` variable always implicitly retains the; loaded value. In non-ARC modes, this retain is normally balanced; by an implicit autorelease. This autorelease can be suppressed; by performing the load in the receiver position of a ``-retain``; message send (e.g. ``[weakReference retain]``); note that this performs; only a single retain (the retain done when primitively loading from; the weak reference). For the most part, ``__unsafe_unretained`` in non-ARC modes is just the; default behavior of variables and therefore is not needed. However,; it does have an effect on the semantics of block captures: normally,; copying a block which captures an Objective-C object or block pointer; causes the captured pointer to be retained or copied, respectively,; but that behavior is suppressed when the captured variable is qualified; with ``__unsafe_unretained``. Note that the ``__weak`` qualifier formerly meant the GC qualifier in; all non-ARC modes and was silently ignored outside of GC modes. It now; means the ARC-style qualifier in all non-GC modes and is no longer; allowed if not enabled by either ``-fobjc-arc`` or ``-fobjc-weak``.; It is expected that ``-fobjc-weak`` will eventually be enab",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:72694,perform,performing,72694,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,2,"['load', 'perform']","['load', 'performing']"
Performance,"; warranty; and distribute a copy of this License along with the; Library. You may charge a fee for the physical act of transferring a copy,; and you may at your option offer warranty protection in exchange for a; fee. 2. You may modify your copy or copies of the Library or any portion; of it, thus forming a work based on the Library, and copy and; distribute such modifications or work under the terms of Section 1; above, provided that you also meet all of these conditions:. a) The modified work must itself be a software library. b) You must cause the files modified to carry prominent notices; stating that you changed the files and the date of any change. c) You must cause the whole of the work to be licensed at no; charge to all third parties under the terms of this License. d) If a facility in the modified Library refers to a function or a; table of data to be supplied by an application program that uses; the facility, other than as an argument passed when the facility; is invoked, then you must make a good faith effort to ensure that,; in the event an application does not supply such function or; table, the facility still operates, and performs whatever part of; its purpose remains meaningful. (For example, a function in a library to compute square roots has; a purpose that is entirely well-defined independent of the; application. Therefore, Subsection 2d requires that any; application-supplied function or table used by this function must; be optional: if the application does not supply it, the square; root function must still compute square roots.). These requirements apply to the modified work as a whole. If; identifiable sections of that work are not derived from the Library,; and can be reasonably considered independent and separate works in; themselves, then this License, and its terms, do not apply to those; sections when you distribute them as separate works. But when you; distribute the same sections as part of a whole which is a work based; on the Library",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/LICENSE.TXT:11573,perform,performs,11573,interpreter/cling/LICENSE.TXT,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/LICENSE.TXT,1,['perform'],['performs']
Performance,"; will not see stale; global data. atomicrmw acq_rel - agent - generic 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acq_rel - singlethread *none* *none*; - wavefront; fence acq_rel - workgroup *none* 1. s_waitcnt lgkmcnt(0). - If OpenCL and; address space is; not generic, omit.; - However,; since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Must happen after; any preceding; local/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that all; memory operations; to local have; completed before; performing any; following global; memory operations.; - Ensures that the; preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before following; global ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:227163,load,load,227163,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"; with the following CMake arguments:. - ``-DCMAKE_C_COMPILER=/path/to/stage2/clang`` - Use the Clang we built in; step 2.; - ``-DCMAKE_CXX_COMPILER=/path/to/stage2/clang++`` - Same as above. If your users are fans of debug info, you may want to consider using; ``-DCMAKE_BUILD_TYPE=RelWithDebInfo`` instead of; ``-DCMAKE_BUILD_TYPE=Release``. This will grant better coverage of; debug info pieces of clang, but will take longer to complete and will; result in a much larger build directory. It's recommended to build the ``all`` target with your instrumented Clang,; since more coverage is often better. b. You should now have a few ``*.profraw`` files in; ``path/to/stage2/profiles/``. You need to merge these using; ``llvm-profdata`` (even if you only have one! The profile merge transforms; profraw into actual profile data, as well). This can be done with; ``/path/to/stage1/llvm-profdata merge; -output=/path/to/output/profdata.prof path/to/stage2/profiles/*.profraw``. 4. Now, build your final, PGO-optimized Clang. To do this, you'll want to pass; the following additional arguments to CMake. - ``-DLLVM_PROFDATA_FILE=/path/to/output/profdata.prof`` - Use the PGO; profile from the previous step.; - ``-DCMAKE_C_COMPILER=/path/to/stage1/clang`` - Use the Clang we built in; step 1.; - ``-DCMAKE_CXX_COMPILER=/path/to/stage1/clang++`` - Same as above. From here, you can build whatever targets you need. .. note::; You may see warnings about a mismatched profile in the build output. These; are generally harmless. To silence them, you can add; ``-DCMAKE_C_FLAGS='-Wno-backend-plugin'; -DCMAKE_CXX_FLAGS='-Wno-backend-plugin'`` to your CMake invocation. Congrats! You now have a Clang built with profile-guided optimizations, and you; can delete all but the final build directory if you'd like. If this worked well for you and you plan on doing it often, there's a slight; optimization that can be made: LLVM and Clang have a tool called tblgen that's; built and run during the build process. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst:6007,optimiz,optimized,6007,interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,1,['optimiz'],['optimized']
Performance,"; }; uint32 : Padding (only if required to align to 8 byte); }. The first byte of each location encodes a type that indicates how to; interpret the ``RegNum`` and ``Offset`` fields as follows:. ======== ========== =================== ===========================; Encoding Type Value Description; -------- ---------- ------------------- ---------------------------; 0x1 Register Reg Value in a register; 0x2 Direct Reg + Offset Frame index value; 0x3 Indirect [Reg + Offset] Spilled value; 0x4 Constant Offset Small constant; 0x5 ConstIndex Constants[Offset] Large constant; ======== ========== =================== ===========================. In the common case, a value is available in a register, and the; ``Offset`` field will be zero. Values spilled to the stack are encoded; as ``Indirect`` locations. The runtime must load those values from a; stack address, typically in the form ``[BP + Offset]``. If an; ``alloca`` value is passed directly to a stack map intrinsic, then; LLVM may fold the frame index into the stack map as an optimization to; avoid allocating a register or stack slot. These frame indices will be; encoded as ``Direct`` locations in the form ``BP + Offset``. LLVM may; also optimize constants by emitting them directly in the stack map,; either in the ``Offset`` of a ``Constant`` location or in the constant; pool, referred to by ``ConstantIndex`` locations. At each callsite, a ""liveout"" register list is also recorded. These; are the registers that are live across the stackmap and therefore must; be saved by the runtime. This is an important optimization when the; patchpoint intrinsic is used with a calling convention that by default; preserves most registers as callee-save. Each entry in the liveout register list contains a DWARF register; number and size in bytes. The stackmap format deliberately omits; specific subregister information. Instead the runtime must interpret; this information conservatively. For example, if the stackmap reports; one byte at ``%r",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst:14111,optimiz,optimization,14111,interpreter/llvm-project/llvm/docs/StackMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst,1,['optimiz'],['optimization']
Performance,";. // We haven't made any control flow changes, any analyses that only care about the control flow are still valid.; PreservedAnalyses PA;; PA.preserveSet<CFGAnalyses>();; return PA;. The pass manager will call the analysis manager's ``invalidate()`` method; with the pass's returned ``PreservedAnalyses``. This can be also done; manually within the pass:. .. code-block:: c++. FooModulePass::run(Module& M, ModuleAnalysisManager& AM) {; auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();. // Invalidate all analysis results for function F1.; FAM.invalidate(F1, PreservedAnalyses::none());. // Invalidate all analysis results across the entire module.; AM.invalidate(M, PreservedAnalyses::none());. // Clear the entry in the analysis manager for function F2 if we've completely removed it from the module.; FAM.clear(F2);. ...; }. One thing to note when accessing inner level IR analyses is cached results for; deleted IR. If a function is deleted in a module pass, its address is still used; as the key for cached analyses. Take care in the pass to either clear the; results for that function or not use inner analyses at all. ``AM.invalidate(M, PreservedAnalyses::none());`` will invalidate the inner; analysis manager proxy which will clear all cached analyses, conservatively; assuming that there are invalid addresses used as keys for cached analyses.; However, if you'd like to be more selective about which analyses are; cached/invalidated, you can mark the analysis manager proxy as preserved,; essentially saying that all deleted entries have been taken care of manually.; This should only be done with measurable compile time gains as it can be tricky; to make sure all the right analyses are invalidated. Implementing Analysis Invalidation; ==================================. By default, an analysis is invalidated if ``PreservedAnalyses`` says that; analyses on the IR unit it runs on are not preserved (see; ``AnalysisResultModel::invalidate()``). An analysis",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:14198,cache,cached,14198,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,1,['cache'],['cached']
Performance,";; // return a slice of the second column from; // (0,1): c2[0] = m(0,1); c2[1] = m(1,1); SVector2 c2 = m.SubCol<SVector2> (1,0);. // return a sub-matrix 2x2 with the upper left corner at(1,1); SMatrix22 subM = m.Sub<SMatrix22> (1,1);. // return the diagonal element in a SVector; SVector3 diag = m.Diagonal();. // return the upper(lower) block of the matrix m; SVector6 vub = m.UpperBlock(); // vub = [ 1, 2, 3, 5, 6, 9 ]; SVector6 vlb = m.LowerBlock(); // vlb = [ 1, 4, 5, 7, 8, 9 ]; ```. #### Linear Algebra Matrix Functions (Inversion, Determinant). Only limited linear algebra functionality is available for `SMatrix`. It; is possible for squared matrices `NxN`, to find the inverse or to; calculate the determinant. Different inversion algorithms are used if; the matrix is smaller than `6x6` or if it is symmetric. In the case of a; small matrix, a faster direct inversion is used. For a large; `(N>6) `symmetric matrix the Bunch-Kaufman diagonal pivoting method is; used while for a large `(N>6)` general matrix an LU factorization is; performed using the same algorithm as in the CERNLIB routine `dinv`. ``` {.cpp}; // Invert a NxN matrix.; // The inverted matrix replaces the existing one if the; // result is successful; bool ret = m.Invert(); // return the inverse matrix of m. // If the inversion fails ifail is different than zero ???; int ifail = 0;; ifail = m.Inverse(ifail);. // determinant of a square matrix - calculate the determinant; // modyfing the matrix content and returns it if the calculation; // was successful; double det;; bool ret = m.Det(det);. // calculate determinant by using a temporary matrix; preserves; // matrix content; bool ret = n.Det2(det);; ```. ### Example: Matrix and Vector Functions and Operators. #### Matrix and Vector Operators. The **`ROOT::Math::SVector`** and **`ROOT::Math::SMatrix`** classes; define the following operators described below. The `m1`, `m2`, `m3` are; vectors or matrices of the same type (and size) and `a` is a scalar; value:",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:112375,perform,performed,112375,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['perform'],['performed']
Performance,";; movswl %ax, %eax -> not needed because eax is already sext'd; 	addl	$12, %esp; 	ret. //===----------------------------------------------------------------------===//; // What we have right now.; //===----------------------------------------------------------------------===//. Currently, these sorts of things are modelled by compiling a function to return; the small type and a signext/zeroext marker is used. For example, we compile; Z into:. define i32 @z() nounwind {; entry:; 	%0 = tail call signext i16 (...)* @y() nounwind; 	%1 = sext i16 %0 to i32; 	ret i32 %1; }. and b into:. define signext i16 @b() nounwind {; entry:; 	%0 = tail call i32 (...)* @a() nounwind		; <i32> [#uses=1]; 	%retval12 = trunc i32 %0 to i16		; <i16> [#uses=1]; 	ret i16 %retval12; }. This has some problems: 1) the actual precise semantics are really poorly; defined (see PR3779). 2) some targets might want the caller to extend, some; might want the callee to extend 3) the mid-level optimizer doesn't know the; size of the GPR, so it doesn't know that %0 is sign extended up to 32-bits ; here, and even if it did, it could not eliminate the sext. 4) the code; generator has historically assumed that the result is extended to i32, which is; a problem on PIC16 (and is also probably wrong on alpha and other 64-bit; targets). //===----------------------------------------------------------------------===//; // The proposal; //===----------------------------------------------------------------------===//. I suggest that we have the front-end fully lower out the ABI issues here to; LLVM IR. This makes it 100% explicit what is going on and means that there is; no cause for confusion. For example, the cases above should compile into:. define i32 @z() nounwind {; entry:; %0 = tail call i32 (...)* @y() nounwind; 	%1 = trunc i32 %0 to i16; %2 = sext i16 %1 to i32; ret i32 %2; }; define i32 @b() nounwind {; entry:; 	%0 = tail call i32 (...)* @a() nounwind; 	%retval12 = trunc i32 %0 to i16; 	%tmp = sext i16 %r",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt:2595,optimiz,optimizer,2595,interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,1,['optimiz'],['optimizer']
Performance,";; void (*copy_helper)(struct __block_literal_5 *dst, struct __block_literal_5 *src);; void (*dispose_helper)(struct __block_literal_5 *);; } __block_descriptor_5 = { 0, sizeof(struct __block_literal_5) __block_copy_5, __block_dispose_5 };. and:. .. code-block:: c. struct _block_byref_i i = {( .isa=NULL, .forwarding=&i, .flags=0, .size=sizeof(struct _block_byref_i), .captured_i=2 )};; struct __block_literal_5 _block_literal = {; &_NSConcreteStackBlock,; (1<<25)|(1<<29), <uninitialized>,; __block_invoke_5,; &__block_descriptor_5,; &i,; };. Importing ``__attribute__((NSObject))`` ``__block`` variables; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. A ``__block`` variable that is also marked ``__attribute__((NSObject))`` should; have ``byref_keep`` and ``byref_dispose`` helper functions that use; ``_Block_object_assign`` and ``_Block_object_dispose``. ``__block`` escapes; ^^^^^^^^^^^^^^^^^^^. Because ``Blocks`` referencing ``__block`` variables may have ``Block_copy()``; performed upon them the underlying storage for the variables may move to the; heap. In Objective-C Garbage Collection Only compilation environments the heap; used is the garbage collected one and no further action is required. Otherwise; the compiler must issue a call to potentially release any heap storage for; ``__block`` variables at all escapes or terminations of their scope. The call; should be:. .. code-block:: c. _Block_object_dispose(&_block_byref_foo, BLOCK_FIELD_IS_BYREF);. Nesting; ^^^^^^^. ``Blocks`` may contain ``Block`` literal expressions. Any variables used within; inner blocks are imported into all enclosing ``Block`` scopes even if the; variables are not used. This includes ``const`` imports as well as ``__block``; variables. Objective C Extensions to ``Blocks``; ====================================. Importing Objects; -----------------. Objects should be treated as ``__attribute__((NSObject))`` variables; all; ``copy_helper``, ``dispose_helper``, ``byref_keep``, and `",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Block-ABI-Apple.rst:17820,perform,performed,17820,interpreter/llvm-project/clang/docs/Block-ABI-Apple.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Block-ABI-Apple.rst,1,['perform'],['performed']
Performance,";; void f(int* a) {; int xtemp = x;; for (int i = 0; i < 100; i++) {; if (a[i]); xtemp += 1;; }; x = xtemp;; }. However, LLVM is not allowed to transform the former to the latter: it could; indirectly introduce undefined behavior if another thread can access ``x`` at; the same time. That thread would read `undef` instead of the value it was; expecting, which can lead to undefined behavior down the line. (This example is; particularly of interest because before the concurrency model was implemented,; LLVM would perform this transformation.). Note that speculative loads are allowed; a load which is part of a race returns; ``undef``, but does not have undefined behavior. Atomic instructions; ===================. For cases where simple loads and stores are not sufficient, LLVM provides; various atomic instructions. The exact guarantees provided depend on the; ordering; see `Atomic orderings`_. ``load atomic`` and ``store atomic`` provide the same basic functionality as; non-atomic loads and stores, but provide additional guarantees in situations; where threads and signals are involved. ``cmpxchg`` and ``atomicrmw`` are essentially like an atomic load followed by an; atomic store (where the store is conditional for ``cmpxchg``), but no other; memory operation can happen on any thread between the load and store. A ``fence`` provides Acquire and/or Release ordering which is not part; of another operation; it is normally used along with Monotonic memory; operations. A Monotonic load followed by an Acquire fence is roughly; equivalent to an Acquire load, and a Monotonic store following a; Release fence is roughly equivalent to a Release; store. SequentiallyConsistent fences behave as both an Acquire and a; Release fence, and additionally provide a total ordering with some; complicated guarantees, see the C++ standard for details. Frontends generating atomic instructions generally need to be aware of the; target to some degree; atomic instructions are guaranteed to be lock-fr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:4373,load,load,4373,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,2,['load'],"['load', 'loads']"
Performance,"<""fcomi"", (COM_FIr ST1)>;. // Simple alias.; def : InstAlias<""fcomi $reg"", (COM_FIr RST:$reg)>;. Instruction aliases can also have a Requires clause to make them subtarget; specific. If the back-end supports it, the instruction printer can automatically emit the; alias rather than what's being aliased. It typically leads to better, more; readable code. If it's better to print out what's being aliased, then pass a '0'; as the third parameter to the InstAlias definition. Instruction Matching; --------------------. .. note::. To Be Written. .. _Implementations of the abstract target description interfaces:; .. _implement the target description:. Target-specific Implementation Notes; ====================================. This section of the document explains features or design decisions that are; specific to the code generator for a particular target. .. _tail call section:. Tail call optimization; ----------------------. Tail call optimization, callee reusing the stack of the caller, is currently; supported on x86/x86-64, PowerPC, AArch64, and WebAssembly. It is performed on; x86/x86-64, PowerPC, and AArch64 if:. * Caller and callee have the calling convention ``fastcc``, ``cc 10`` (GHC; calling convention), ``cc 11`` (HiPE calling convention), ``tailcc``, or; ``swifttailcc``. * The call is a tail call - in tail position (ret immediately follows call and; ret uses value of call or is void). * Option ``-tailcallopt`` is enabled or the calling convention is ``tailcc``. * Platform-specific constraints are met. x86/x86-64 constraints:. * No variable argument lists are used. * On x86-64 when generating GOT/PIC code only module-local calls (visibility =; hidden or protected) are supported. PowerPC constraints:. * No variable argument lists are used. * No byval parameters are used. * On ppc32/64 GOT/PIC only module-local calls (visibility = hidden or protected); are supported. WebAssembly constraints:. * No variable argument lists are used. * The 'tail-call' target attribute ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:86251,optimiz,optimization,86251,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimization']
Performance,"</td>`; :raw-html:`<td>Saved LR</td>`; :raw-html:`</tr>`; :raw-html:`<tr>`; :raw-html:`<td>24</td>`; :raw-html:`<td>Reserved</td>`; :raw-html:`</tr>`; :raw-html:`<tr>`; :raw-html:`<td>32</td>`; :raw-html:`<td>Reserved</td>`; :raw-html:`</tr>`; :raw-html:`<tr>`; :raw-html:`<td>40</td>`; :raw-html:`<td>Saved FP (r31)</td>`; :raw-html:`</tr>`; :raw-html:`</table>`. The *parameter area* is used to store arguments being passed to a callee; function. Following the PowerPC ABI, the first few arguments are actually; passed in registers, with the space in the parameter area unused. However, if; there are not enough registers or the callee is a thunk or vararg function,; these register arguments can be spilled into the parameter area. Thus, the; parameter area must be large enough to store all the parameters for the largest; call sequence made by the caller. The size must also be minimally large enough; to spill registers r3-r10. This allows callees blind to the call signature,; such as thunks and vararg functions, enough space to cache the argument; registers. Therefore, the parameter area is minimally 32 bytes (64 bytes in 64; bit mode.) Also note that since the parameter area is a fixed offset from the; top of the frame, that a callee can access its split arguments using fixed; offsets from the stack pointer (or base pointer.). Combining the information about the linkage, parameter areas and alignment. A; stack frame is minimally 64 bytes in 32 bit mode and 128 bytes in 64 bit mode. The *dynamic area* starts out as size zero. If a function uses dynamic alloca; then space is added to the stack, the linkage and parameter areas are shifted to; top of stack, and the new space is available immediately below the linkage and; parameter areas. The cost of shifting the linkage and parameter areas is minor; since only the link value needs to be copied. The link value can be easily; fetched by adding the original frame size to the base pointer. Note that; allocations in the dynamic s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:99234,cache,cache,99234,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['cache'],['cache']
Performance,"<16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fadd.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fadd.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point addition of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fadd``' intrinsic performs floating-point addition (:ref:`fadd <i_fadd>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.fadd.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = fadd <4 x float> %a, %b; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fsub:. '``llvm.vp.fsub.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fsub.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fsub.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fsub.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point subtraction of two vecto",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:733447,perform,performed,733447,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"<16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fdiv.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fdiv.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point division of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fdiv``' intrinsic performs floating-point division (:ref:`fdiv <i_fdiv>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.fdiv.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = fdiv <4 x float> %a, %b; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_frem:. '``llvm.vp.frem.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.frem.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.frem.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.frem.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point remainder of two vectors",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:738217,perform,performed,738217,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"<4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fneg:. '``llvm.vp.fneg.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fneg.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fneg.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fneg.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point negation of a vector of floating-point values. Arguments:; """""""""""""""""""". The first operand and the result have the same vector of floating-point type.; The second operand is the vector mask and has the same number of elements as the; result vector type. The third operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fneg``' intrinsic performs floating-point negation (:ref:`fneg <i_fneg>`); of the first vector operand on each enabled lane. The result on disabled lanes; is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.fneg.v4f32(<4 x float> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = fneg <4 x float> %a; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fabs:. '``llvm.vp.fabs.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fabs.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fabs.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fabs.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point absolute value of a vector of floating-point v",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:741072,perform,performs,741072,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"<< 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===--------------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:24669,optimiz,optimized,24669,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,"<env>``. However, in order to standardize outputs for tools that consume bitcode; bundles, bundles written by the bundler internally use only the 4-field; target triple:. ``<arch><sub>-<vendor>-<sys>-<env>``. **target-id**; The canonical target ID of the code object. Present only if the target; supports a target ID. See :ref:`clang-target-id`. .. _code-object-composition:. Bundled Code Object Composition; -------------------------------. * Each entry of a bundled code object must have a different bundle entry ID.; * There can be multiple entries for the same processor provided they differ; in target feature settings.; * If there is an entry with a target feature specified as *Any*, then all; entries must specify that target feature as *Any* for the same processor. There may be additional target specific restrictions. .. _compatibility-bundle-entry-id:. Compatibility Rules for Bundle Entry ID; ---------------------------------------. A code object, specified using its Bundle Entry ID, can be loaded and; executed on a target processor, if:. * Their offload kinds are the same.; * Their target triples are compatible.; * Their Target IDs are compatible as defined in :ref:`compatibility-target-id`. .. _clang-target-id:. Target ID; =========. A target ID is used to indicate the processor and optionally its configuration,; expressed by a set of target features, that affect ISA generation. It is target; specific if a target ID is supported, or if the target triple alone is; sufficient to specify the ISA generation. It is used with the ``-mcpu=<target-id>`` and ``--offload-arch=<target-id>``; Clang compilation options to specify the kind of code to generate. It is also used as part of the bundle entry ID to identify the code object. See; :ref:`clang-bundle-entry-id`. Target ID syntax is defined by the following BNF syntax:. .. code::. <target-id> ::== <processor> ( "":"" <target-feature> ( ""+"" | ""-"" ) )*. Where:. **processor**; Is a the target specific processor or any alternat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst:10709,load,loaded,10709,interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,1,['load'],['loaded']
Performance,"<img src=""https://root-forum.cern.ch/uploads/default/original/2X/3/3fb82b650635bc6d61461f3c47f41786afad4548.png"" align=""right"" height=""50""/>. ## About. ROOT is a unified software package for the storage, processing, and analysis of ; scientific data: from its acquisition to the final visualization in form of highly ; customizable, publication-ready plots. It is reliable, performant and well supported,; easy to use and obtain, and strives to maximize the quantity and impact of scientific ; results obtained per unit cost, both of human effort and computing resources. ROOT provides a very efficient storage system for data models, ; that demonstrated to scale at the Large Hadron Collider experiments: Exabytes ; of scientific data are written in columnar ROOT format.; ROOT comes with histogramming capabilities in an arbitrary number of ; dimensions, curve fitting, statistical modelling, minimization, to allow; the easy setup of a data analysis system that can query and process the data; interactively or in batch mode, as well as a general parallel processing; framework, RDataFrame, that can considerably speed up an analysis, taking ; full advantage of multi-core and distributed systems. ROOT is performance critical software written in C++ and enables rapid prototyping ; powered by a unique C++ compliant interpreter called Cling. ; Cling also enables performant C++ type introspection which is a building block of automatic ; interoperability with Python. Thanks to PyROOT, leveraging the cppyy technology, ; ROOT offers efficient, on-demand C++/Python interoperability in a uniform cross-language ; execution environment. ROOT fully embraces open-source, it's made with passion by its community,; for the benefit of its community. [![License: LGPL v2.1+](https://img.shields.io/badge/License-LGPL%20v2.1+-blue.svg)](https://www.gnu.org/licenses/lgpl.html); [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5060/badge)](https://bestpractices.coreinfrastruct",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README.md:374,perform,performant,374,README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README.md,1,['perform'],['performant']
Performance,"= ""[8]"" \; source = ""Int_t *fArray; Int_t fN;"" \; target = ""fArray"" \; code = ""{ fArray = new Char_t[onfile.fN]; Char_t* gtc=fArray; Int_t* gti=onfile.fArray; \; for(Int_t i=0; i<onfile.fN; i++) *(gtc+i) = *(gti+i)+10; }""; #pragma read sourceClass = ""ACache"" targetClass = ""ACache"" version = ""[8]"" \; source = ""float fValues[3]"" \; target = ""fValues"" \; code = ""{ for(Int_t i=0; i<3; i++) fValues[i] = 1+onfile.fValues[i]; }"". Allow the seamless schema evolution from map<a,b> to vector<pair<a,b> >.; Avoid dropping information when reading a long written on a 64 bits platforms; and being read into a long long on a 32 bits platform (previously the higher; bits were lost due to passing through a 32 bits temporary long).; Migrate the functionality of TStreamerInfo::TagFile to a new interface TBuffer::TagStreamerInfo; so that TMessage can customize the behavior. TMessage now relies on this new interface; instead of TBuffer::IncrementLevel.; New option to hadd, -O requesting the (re)optimization of the basket size (by avoid the fast merge technique). The equivalent in TFileMerger is to call; merger->SetFastMethod(kFALSE); To make sure that the class emulation layer of ROOT does not double delete an object,; tell the StreamerElement representing one of the pointers pointing to the object; to never delete the object. For example:. TClass::AddRule(""HepMC::GenVertex m_event attributes=NotOwner"");. The handling of memory by the collection proxy has been improved in the case of a; collection of pointers which can now become owner of its content. The default, for backward compatibility reasons and to avoid double delete (at the expense; of memory leaks), the container of pointers are still not owning their content; unless they are a free standing container (i.e. itself not contained in another; object).; To make a container of pointers become owner of its content do something like:. TClass::AddRule(""ObjectVector<LHCb::MCRichDigitSummary> m_vector options=Owner"");. Added TKey::Reset ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v528/index.html:10921,optimiz,optimization,10921,io/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v528/index.html,2,['optimiz'],['optimization']
Performance,"= ======= ============= ======= =====. See also :ref:`langext-__builtin_shufflevector`, :ref:`langext-__builtin_convertvector`. .. [#] ternary operator(?:) has different behaviors depending on condition; operand's vector type. If the condition is a GNU vector (i.e. __vector_size__),; a NEON vector or an SVE vector, it's only available in C++ and uses normal bool; conversions (that is, != 0).; If it's an extension (OpenCL) vector, it's only available in C and OpenCL C.; And it selects base on signedness of the condition operands (OpenCL v1.1 s6.3.9).; .. [#] sizeof can only be used on vector length specific SVE types.; .. [#] Clang does not allow the address of an element to be taken while GCC; allows this. This is intentional for vectors with a boolean element type and; not implemented otherwise. Vector Builtins; ---------------. **Note: The implementation of vector builtins is work-in-progress and incomplete.**. In addition to the operators mentioned above, Clang provides a set of builtins; to perform additional operations on certain scalar and vector types. Let ``T`` be one of the following types:. * an integer type (as in C23 6.2.5p22), but excluding enumerated types and ``bool``; * the standard floating types float or double; * a half-precision floating point type, if one is supported on the target; * a vector type. For scalar types, consider the operation applied to a vector with a single element. *Vector Size*; To determine the number of elements in a vector, use ``__builtin_vectorelements()``.; For fixed-sized vectors, e.g., defined via ``__attribute__((vector_size(N)))`` or ARM; NEON's vector types (e.g., ``uint16x8_t``), this returns the constant number of; elements at compile-time. For scalable vectors, e.g., SVE or RISC-V V, the number of; elements is not known at compile-time and is determined at runtime. This builtin can; be used, e.g., to increment the loop-counter in vector-type agnostic loops. *Elementwise Builtins*. Each builtin returns a vector equi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:20915,perform,perform,20915,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['perform'],['perform']
Performance,"= B[0];; }. The runtime condition (``rtc``) checks that the array ``A`` and the; element `B[0]` do not alias. Currently, this transformation does not support followup-attributes. Loop Interchange; ----------------. Currently, the ``LoopInterchange`` pass does not use any metadata. Ambiguous Transformation Order; ==============================. If there multiple transformations defined, the order in which they are; executed depends on the order in LLVM's pass pipeline, which is subject; to change. The default optimization pipeline (anything higher than; ``-O0``) has the following order. When using the legacy pass manager:. - LoopInterchange (if enabled); - SimpleLoopUnroll/LoopFullUnroll (only performs full unrolling); - VersioningLICM (if enabled); - LoopDistribute; - LoopVectorizer; - LoopUnrollAndJam (if enabled); - LoopUnroll (partial and runtime unrolling). When using the legacy pass manager with LTO:. - LoopInterchange (if enabled); - SimpleLoopUnroll/LoopFullUnroll (only performs full unrolling); - LoopVectorizer; - LoopUnroll (partial and runtime unrolling). When using the new pass manager:. - SimpleLoopUnroll/LoopFullUnroll (only performs full unrolling); - LoopDistribute; - LoopVectorizer; - LoopUnrollAndJam (if enabled); - LoopUnroll (partial and runtime unrolling). Leftover Transformations; ========================. Forced transformations that have not been applied after the last; transformation pass should be reported to the user. The transformation; passes themselves cannot be responsible for this reporting because they; might not be in the pipeline, there might be multiple passes able to; apply a transformation (e.g. ``LoopInterchange`` and Polly) or a; transformation attribute may be 'hidden' inside another passes' followup; attribute. The pass ``-transform-warning`` (``WarnMissedTransformationsPass``); emits such warnings. It should be placed after the last transformation; pass. The current pass pipeline has a fixed order in which transformations; pa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst:14162,perform,performs,14162,interpreter/llvm-project/llvm/docs/TransformMetadata.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TransformMetadata.rst,1,['perform'],['performs']
Performance,"= [this]() { return cc; };; return l();; }; };; lambdaExpr(hasAnyCapture(lambdaCapture(capturesThis()))); matches `[this]() { return cc; }`. Matcher<LambdaCapture>isImplicit; Matches an entity that has been implicitly added by the compiler (e.g.; implicit default/copy constructors). Matcher<MemberExpr>isArrow; Matches member expressions that are called with '->' as opposed; to '.'. Member calls on the implicit this pointer match as called with '->'. Given; class Y {; void x() { this->x(); x(); Y y; y.x(); a; this->b; Y::b; }; template <class T> void f() { this->f<T>(); f<T>(); }; int a;; static int b;; };; template <class T>; class Z {; void x() { this->m; }; };; memberExpr(isArrow()); matches this->x, x, y.x, a, this->b; cxxDependentScopeMemberExpr(isArrow()); matches this->m; unresolvedMemberExpr(isArrow()); matches this->f<T>, f<T>. Matcher<NamedDecl>hasAnyNameStringRef, ..., StringRef; Matches NamedDecl nodes that have any of the specified names. This matcher is only provided as a performance optimization of hasName.; hasAnyName(a, b, c); is equivalent to, but faster than; anyOf(hasName(a), hasName(b), hasName(c)). Matcher<NamedDecl>hasExternalFormalLinkage; Matches a declaration that has external formal linkage. Example matches only z (matcher = varDecl(hasExternalFormalLinkage())); void f() {; int x;; static int y;; }; int z;. Example matches f() because it has external formal linkage despite being; unique to the translation unit as though it has internal likage; (matcher = functionDecl(hasExternalFormalLinkage())). namespace {; void f() {}; }. Matcher<NamedDecl>hasNameStringRef Name; Matches NamedDecl nodes that have the specified name. Supports specifying enclosing namespaces or classes by prefixing the name; with '<enclosing>::'.; Does not match typedefs of an underlying type with the given name. Example matches X (Name == ""X""); class X;. Example matches X (Name is one of ""::a::b::X"", ""a::b::X"", ""b::X"", ""X""); namespace a { namespace b { class X; } }. Matche",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html:100149,perform,performance,100149,interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,4,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"= i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimize",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:23248,optimiz,optimized,23248,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,"= select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_bitreverse:. '``llvm.vp.bitreverse.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.bitreverse.v16i32 (<16 x i32> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.bitreverse.nxv4i32 (<vscale x 4 x i32> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.bitreverse.v256i64 (<256 x i64> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated bitreverse of a vector of integers. Arguments:; """""""""""""""""""". The first operand and the result have the same vector of integer type. The; second operand is the vector mask and has the same number of elements as the; result vector type. The third operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.bitreverse``' intrinsic performs bitreverse (:ref:`bitreverse <int_bitreverse>`) of the first operand on each; enabled lane. The result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.bitreverse.v4i32(<4 x i32> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x i32> @llvm.bitreverse.v4i32(<4 x i32> %a); %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_bswap:. '``llvm.vp.bswap.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.bswap.v16i32 (<16 x i32> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.bswap.nxv4i32 (<vscale x 4 x i32> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.bswap.v256i64 (<256 x i64> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated bswap of a vector of integers. Arguments:",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:832209,perform,performs,832209,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"=//; // The proposal; //===----------------------------------------------------------------------===//. I suggest that we have the front-end fully lower out the ABI issues here to; LLVM IR. This makes it 100% explicit what is going on and means that there is; no cause for confusion. For example, the cases above should compile into:. define i32 @z() nounwind {; entry:; %0 = tail call i32 (...)* @y() nounwind; 	%1 = trunc i32 %0 to i16; %2 = sext i16 %1 to i32; ret i32 %2; }; define i32 @b() nounwind {; entry:; 	%0 = tail call i32 (...)* @a() nounwind; 	%retval12 = trunc i32 %0 to i16; 	%tmp = sext i16 %retval12 to i32; 	ret i32 %tmp; }. In this model, no functions will return an i1/i8/i16 (and on a x86-64 target; that extends results to i64, no i32). This solves the ambiguity issue, allows us ; to fully describe all possible ABIs, and now allows the optimizers to reason; about and eliminate these extensions. The one thing that is missing is the ability for the front-end and optimizer to; specify/infer the guarantees provided by the ABI to allow other optimizations.; For example, in the y/z case, since y is known to return a sign extended value,; the trunc/sext in z should be eliminable. This can be done by introducing new sext/zext attributes which mean ""I know; that the result of the function is sign extended at least N bits. Given this,; and given that it is stuck on the y function, the mid-level optimizer could; easily eliminate the extensions etc with existing functionality. The major disadvantage of doing this sort of thing is that it makes the ABI; lowering stuff even more explicit in the front-end, and that we would like to; eventually move to having the code generator do more of this work. However,; the sad truth of the matter is that this is a) unlikely to happen anytime in; the near future, and b) this is no worse than we have now with the existing; attributes. C compilers fundamentally have to reason about the target in many ways. ; This is ugly and horrib",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt:4002,optimiz,optimizer,4002,interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,2,['optimiz'],"['optimizations', 'optimizer']"
Performance,"=1 abbrevid=4 op0=5 op1=2/>; <Remark version codeid=2 abbrevid=5 op0=30/>; <String table codeid=3 abbrevid=6/> blob data = 'pass\\x00remark\\x00function\\x00path\\x00key\\x00value\\x00argpath\\x00'; </Meta>; <Remark BlockID=9 NumWords=8 BlockCodeSize=4>; <Remark header codeid=5 abbrevid=4 op0=2 op1=1 op2=0 op3=2/>; <Remark debug location codeid=6 abbrevid=5 op0=3 op1=99 op2=55/>; <Remark hotness codeid=7 abbrevid=6 op0=999999999/>; <Argument with debug location codeid=8 abbrevid=7 op0=4 op1=5 op2=6 op3=11 op4=66/>; </Remark>. opt-viewer; ==========. The ``opt-viewer`` directory contains a collection of tools that visualize and; summarize serialized remarks. The tools only support the ``yaml`` format. .. _optviewerpy:. opt-viewer.py; -------------. Output a HTML page which gives visual feedback on compiler interactions with; your program. :Examples:. ::. $ opt-viewer.py my_yaml_file.opt.yaml. ::. $ opt-viewer.py my_build_dir/. opt-stats.py; ------------. Output statistics about the optimization remarks in the input set. :Example:. ::. $ opt-stats.py my_yaml_file.opt.yaml. Total number of remarks 3. Top 10 remarks by pass:; inline 33%; asm-printer 33%; prologepilog 33%. Top 10 remarks:; asm-printer/InstructionCount 33%; inline/NoDefinition 33%; prologepilog/StackSize 33%. opt-diff.py; -----------. Produce a new YAML file which contains all of the changes in optimizations; between two YAML files. Typically, this tool should be used to do diffs between:. * new compiler + fixed source vs old compiler + fixed source; * fixed compiler + new source vs fixed compiler + old source. This diff file can be displayed using :ref:`opt-viewer.py <optviewerpy>`. :Example:. ::. $ opt-diff.py my_opt_yaml1.opt.yaml my_opt_yaml2.opt.yaml -o my_opt_diff.opt.yaml; $ opt-viewer.py my_opt_diff.opt.yaml. .. _remarkssection:. Emitting remark diagnostics in the object file; ==============================================. A section containing metadata on remark diagnostics will be emitted for the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst:15215,optimiz,optimization,15215,interpreter/llvm-project/llvm/docs/Remarks.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst,1,['optimiz'],['optimization']
Performance,"=19219); Write of size 4 at 0x7fcf47b21bc0 by thread T1:; #0 Thread1 tiny_race.c:4 (exe+0x00000000a360). Previous write of size 4 at 0x7fcf47b21bc0 by main thread:; #0 main tiny_race.c:10 (exe+0x00000000a3b4). Thread T1 (running) created at:; #0 pthread_create tsan_interceptors.cc:705 (exe+0x00000000c790); #1 main tiny_race.c:9 (exe+0x00000000a3a4). ``__has_feature(thread_sanitizer)``; ------------------------------------. In some cases one may need to execute different code depending on whether; ThreadSanitizer is enabled.; :ref:`\_\_has\_feature <langext-__has_feature-__has_extension>` can be used for; this purpose. .. code-block:: c. #if defined(__has_feature); # if __has_feature(thread_sanitizer); // code that builds only under ThreadSanitizer; # endif; #endif. ``__attribute__((no_sanitize(""thread"")))``; -----------------------------------------------. Some code should not be instrumented by ThreadSanitizer. One may use the; function attribute ``no_sanitize(""thread"")`` to disable instrumentation of plain; (non-atomic) loads/stores in a particular function. ThreadSanitizer still; instruments such functions to avoid false positives and provide meaningful stack; traces. This attribute may not be supported by other compilers, so we suggest; to use it together with ``__has_feature(thread_sanitizer)``. ``__attribute__((disable_sanitizer_instrumentation))``; --------------------------------------------------------. The ``disable_sanitizer_instrumentation`` attribute can be applied to functions; to prevent all kinds of instrumentation. As a result, it may introduce false; positives and incorrect stack traces. Therefore, it should be used with care,; and only if absolutely required; for example for certain code that cannot; tolerate any instrumentation and resulting side-effects. This attribute; overrides ``no_sanitize(""thread"")``. Ignorelist; ----------. ThreadSanitizer supports ``src`` and ``fun`` entity types in; :doc:`SanitizerSpecialCaseList`, that can be used to sup",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSanitizer.rst:2655,load,loads,2655,interpreter/llvm-project/clang/docs/ThreadSanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSanitizer.rst,1,['load'],['loads']
Performance,"== ======= ==================; Memory Space Name HSA Segment Hardware Address NULL Value; Name Name Size; ================= =========== ======== ======= ==================; Private private scratch 32 0x00000000; Local group LDS 32 0xFFFFFFFF; Global global global 64 0x0000000000000000; Constant constant *same as 64 0x0000000000000000; global*; Generic flat flat 64 0x0000000000000000; Region N/A GDS 32 *not implemented; for AMDHSA*; ================= =========== ======== ======= ==================. The global and constant memory spaces both use global virtual addresses, which; are the same virtual address space used by the CPU. However, some virtual; addresses may only be accessible to the CPU, some only accessible by the GPU,; and some by both. Using the constant memory space indicates that the data will not change during; the execution of the kernel. This allows scalar read instructions to be; used. The vector and scalar L1 caches are invalidated of volatile data before; each kernel dispatch execution to allow constant memory to change values between; kernel dispatches. The local memory space uses the hardware Local Data Store (LDS) which is; automatically allocated when the hardware creates work-groups of wavefronts, and; freed when all the wavefronts of a work-group have terminated. The data store; (DS) instructions can be used to access it. The private memory space uses the hardware scratch memory support. If the kernel; uses scratch, then the hardware allocates memory that is accessed using; wavefront lane dword (4 byte) interleaving. The mapping used from private; address to physical address is:. ``wavefront-scratch-base +; (private-address * wavefront-size * 4) +; (wavefront-lane-id * 4)``. There are different ways that the wavefront scratch base address is determined; by a wavefront (see :ref:`amdgpu-amdhsa-initial-kernel-execution-state`). This; memory can be accessed in an interleaved manner using buffer instruction with; the scratch buffer descriptor and ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:153941,cache,caches,153941,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"==. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <std",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:1153,optimiz,optimizer,1153,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,2,['optimiz'],"['optimizations', 'optimizer']"
Performance,"==//. Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how; to licm and cse the unnecessary load from cp#1. //===---------------------------------------------------------------------===//. The CMN instruction sets the flags like an ADD instruction, while CMP sets; them like a subtract. Therefore to be able to use CMN for comparisons other; than the Z bit, we'll need additional logic to reverse the conditionals; associated with the comparison. Perhaps a pseudo-instruction for the comparison,; with a post-codegen pass to clean up and handle the condition codes?; See PR5694 for testcase. //===---------------------------------------------------------------------===//. Given the following on armv5:; int test1(int A, int B) {; return (A&-8388481)|(B&8388480);; }. We currently generate:; 	ldr	r2, .LCPI0_0; 	and	r0, r0, r2; 	ldr	r2, .LCPI0_1; 	and	r1, r1, r2; 	orr	r0, r1, r0; 	bx	lr. We should be able to replace the second ldr+and with a bic (i.e. reuse the; constant which was already loaded). Not sure what's necessary to do that. //===---------------------------------------------------------------------===//. The code generated for bswap on armv4/5 (CPUs without rev) is less than ideal:. int a(int x) { return __builtin_bswap32(x); }. a:; 	mov	r1, #255, 24; 	mov	r2, #255, 16; 	and	r1, r1, r0, lsr #8; 	and	r2, r2, r0, lsl #8; 	orr	r1, r1, r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16; 	bic r1, r1, #0xff0000; 	mov r1, r1, lsr #8; 	eor r0, r1, r0, ror #8; 	bx	lr. A custom Thumb version would also be a slight improvement over the generic; version. //===---------------------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:18198,load,loaded,18198,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,2,['load'],['loaded']
Performance,"==; Opaque Pointers; ===============. The Opaque Pointer Type; =======================. Traditionally, LLVM IR pointer types have contained a pointee type. For example,; ``i32*`` is a pointer that points to an ``i32`` somewhere in memory. However,; due to a lack of pointee type semantics and various issues with having pointee; types, there is a desire to remove pointee types from pointers. The opaque pointer type project aims to replace all pointer types containing; pointee types in LLVM with an opaque pointer type. The new pointer type is; represented textually as ``ptr``. Some instructions still need to know what type to treat the memory pointed to by; the pointer as. For example, a load needs to know how many bytes to load from; memory and what type to treat the resulting value as. In these cases,; instructions themselves contain a type argument. For example the load; instruction from older versions of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra layer of checks ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:1005,load,load,1005,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst,1,['load'],['load']
Performance,"===----------------------------------------------------------------------===//. I suggest that we have the front-end fully lower out the ABI issues here to; LLVM IR. This makes it 100% explicit what is going on and means that there is; no cause for confusion. For example, the cases above should compile into:. define i32 @z() nounwind {; entry:; %0 = tail call i32 (...)* @y() nounwind; 	%1 = trunc i32 %0 to i16; %2 = sext i16 %1 to i32; ret i32 %2; }; define i32 @b() nounwind {; entry:; 	%0 = tail call i32 (...)* @a() nounwind; 	%retval12 = trunc i32 %0 to i16; 	%tmp = sext i16 %retval12 to i32; 	ret i32 %tmp; }. In this model, no functions will return an i1/i8/i16 (and on a x86-64 target; that extends results to i64, no i32). This solves the ambiguity issue, allows us ; to fully describe all possible ABIs, and now allows the optimizers to reason; about and eliminate these extensions. The one thing that is missing is the ability for the front-end and optimizer to; specify/infer the guarantees provided by the ABI to allow other optimizations.; For example, in the y/z case, since y is known to return a sign extended value,; the trunc/sext in z should be eliminable. This can be done by introducing new sext/zext attributes which mean ""I know; that the result of the function is sign extended at least N bits. Given this,; and given that it is stuck on the y function, the mid-level optimizer could; easily eliminate the extensions etc with existing functionality. The major disadvantage of doing this sort of thing is that it makes the ABI; lowering stuff even more explicit in the front-end, and that we would like to; eventually move to having the code generator do more of this work. However,; the sad truth of the matter is that this is a) unlikely to happen anytime in; the near future, and b) this is no worse than we have now with the existing; attributes. C compilers fundamentally have to reason about the target in many ways. ; This is ugly and horrible, but a fact of life. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt:4435,optimiz,optimizer,4435,interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,1,['optimiz'],['optimizer']
Performance,"===. This scheme checks that virtual calls take place using a vptr of the correct; dynamic type; that is, the dynamic type of the called object must be a; derived class of the static type of the object used to make the call.; This CFI scheme can be enabled on its own using ``-fsanitize=cfi-vcall``. For this scheme to work, all translation units containing the definition; of a virtual member function (whether inline or not), other than members; of :ref:`ignored <cfi-ignorelist>` types or types with public :doc:`LTO; visibility <LTOVisibility>`, must be compiled with ``-flto`` or ``-flto=thin``; enabled and be statically linked into the program. Performance; -----------. A performance overhead of less than 1% has been measured by running the; Dromaeo benchmark suite against an instrumented version of the Chromium; web browser. Another good performance benchmark for this mechanism is the; virtual-call-heavy SPEC 2006 xalancbmk. Note that this scheme has not yet been optimized for binary size; an increase; of up to 15% has been observed for Chromium. Bad Cast Checking; =================. This scheme checks that pointer casts are made to an object of the correct; dynamic type; that is, the dynamic type of the object must be a derived class; of the pointee type of the cast. The checks are currently only introduced; where the class being casted to is a polymorphic class. Bad casts are not in themselves control flow integrity violations, but they; can also create security vulnerabilities, and the implementation uses many; of the same mechanisms. There are two types of bad cast that may be forbidden: bad casts; from a base class to a derived class (which can be checked with; ``-fsanitize=cfi-derived-cast``), and bad casts from a pointer of; type ``void*`` or another unrelated type (which can be checked with; ``-fsanitize=cfi-unrelated-cast``). The difference between these two types of casts is that the first is defined; by the C++ standard to produce an undefined value, whil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst:5096,optimiz,optimized,5096,interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,1,['optimiz'],['optimized']
Performance,"====. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 3 Introduction; ======================. **Warning: This text is currently out of date due to ORC API updates.**. **The example code has been updated and can be used. The text will be updated; once the API churn dies down.**. Welcome to Chapter 3 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter discusses lazy JITing and shows you how to enable it by adding an ORC; CompileOnDemand layer the JIT from `Chapter 2 <BuildingAJIT2.html>`_. Lazy Compilation; ================. When we add a module to the KaleidoscopeJIT class from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each function in them to be compiled the; first time it is called. To do this, the CompileOnDemandLayer creates two small; utilities for e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:1165,perform,performance,1165,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,1,['perform'],['performance']
Performance,"====. Atomic operations are represented in the SelectionDAG with ``ATOMIC_*`` opcodes.; On architectures which use barrier instructions for all atomic ordering (like; ARM), appropriate fences can be emitted by the AtomicExpand Codegen pass if; ``shouldInsertFencesForAtomic()`` returns true. The MachineMemOperand for all atomic operations is currently marked as volatile;; this is not correct in the IR sense of volatile, but CodeGen handles anything; marked volatile very conservatively. This should get fixed at some point. One very important property of the atomic operations is that if your backend; supports any inline lock-free atomic operations of a given size, you should; support *ALL* operations of that size in a lock-free manner. When the target implements atomic ``cmpxchg`` or LL/SC instructions (as most do); this is trivial: all the other operations can be implemented on top of those; primitives. However, on many older CPUs (e.g. ARMv5, SparcV8, Intel 80386) there; are atomic load and store instructions, but no ``cmpxchg`` or LL/SC. As it is; invalid to implement ``atomic load`` using the native instruction, but; ``cmpxchg`` using a library call to a function that uses a mutex, ``atomic; load`` must *also* expand to a library call on such architectures, so that it; can remain atomic with regards to a simultaneous ``cmpxchg``, by using the same; mutex. AtomicExpandPass can help with that: it will expand all atomic operations to the; proper ``__atomic_*`` libcalls for any size above the maximum set by; ``setMaxAtomicSizeInBitsSupported`` (which defaults to 0). On x86, all atomic loads generate a ``MOV``. SequentiallyConsistent stores; generate an ``XCHG``, other stores generate a ``MOV``. SequentiallyConsistent; fences generate an ``MFENCE``, other fences do not cause any code to be; generated. ``cmpxchg`` uses the ``LOCK CMPXCHG`` instruction. ``atomicrmw xchg``; uses ``XCHG``, ``atomicrmw add`` and ``atomicrmw sub`` use ``XADD``, and all; other ``atomicrmw`` ope",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:19130,load,load,19130,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['load']
Performance,"====. The allocator will output an error message, and potentially terminate the; process, when an unexpected behavior is detected. The output usually starts with; ``""Scudo ERROR:""`` followed by a short summary of the problem that occurred as; well as the pointer(s) involved. Once again, Scudo is meant to be a mitigation,; and might not be the most useful of tools to help you root-cause the issue,; please consider `ASan <https://github.com/google/sanitizers/wiki/AddressSanitizer>`_; for this purpose. Here is a list of the current error messages and their potential cause:. - ``""corrupted chunk header""``: the checksum verification of the chunk header; has failed. This is likely due to one of two things: the header was; overwritten (partially or totally), or the pointer passed to the function is; not a chunk at all;. - ``""race on chunk header""``: two different threads are attempting to manipulate; the same header at the same time. This is usually symptomatic of a; race-condition or general lack of locking when performing operations on that; chunk;. - ``""invalid chunk state""``: the chunk is not in the expected state for a given; operation, eg: it is not allocated when trying to free it, or it's not; quarantined when trying to recycle it, etc. A double-free is the typical; reason this error would occur;. - ``""misaligned pointer""``: we strongly enforce basic alignment requirements, 8; bytes on 32-bit platforms, 16 bytes on 64-bit platforms. If a pointer passed; to our functions does not fit those, something is definitely wrong. - ``""allocation type mismatch""``: when the optional deallocation type mismatch; check is enabled, a deallocation function called on a chunk has to match the; type of function that was called to allocate it. Security implications of such; a mismatch are not necessarily obvious but situational at best;. - ``""invalid sized delete""``: when the C++14 sized delete operator is used, and; the optional check enabled, this indicates that the size passed when;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst:15567,perform,performing,15567,interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,1,['perform'],['performing']
Performance,"===== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; slc=1 dlc=1. - If GFX10, omit dlc=1. - volatile. 1. buffer/global/flat_load; glc=1 dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1 dlc=1. - If GFX10, omit dlc=1. - volatile. 1. buffer/global/flat_store; dlc=1. - If GFX10, omit dlc=1. 2. s_waitcnt vscnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic glc=1. - If CU wavefront execution; mode, omit glc=1. load atomic monotonic - singlethread - local 1. ds_load; - wavefront; - workgroup; load atomic monotonic - agent - global 1. buffer/global/flat_load; - system - generic glc=1 dlc=1. - If GFX11, omi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:344587,load,load,344587,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"=====; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; nt=1. - volatile. 1. buffer/global/flat_load; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. GFX940, GFX941; - constant buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store. - !volatile & nontemporal. 1. GFX940, GFX941; buffer/global/flat_store; nt=1 sc0=1 sc1=1; GFX942; buffer/global/flat_store; nt=1. - volatile. 1. buffer/global/flat_store; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic sc0=1; load atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_load; load atomic monotonic - agent - global 1. buffer/global/flat_load; - generic sc1=1; load atomic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:293517,load,load,293517,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"====== ==============================================================; unordered *none*; monotonic *none*; acquire - If a load atomic/atomicrmw then no following load/load; atomic/store/store atomic/atomicrmw/fence instruction can be; moved before the acquire.; - If a fence then same as load atomic, plus no preceding; associated fence-paired-atomic can be moved after the fence.; release - If a store atomic/atomicrmw then no preceding load/load; atomic/store/store atomic/atomicrmw/fence instruction can be; moved after the release.; - If a fence then same as store atomic, plus no following; associated fence-paired-atomic can be moved before the; fence.; acq_rel Same constraints as both acquire and release.; seq_cst - If a load atomic then same constraints as acquire, plus no; preceding sequentially consistent load atomic/store; atomic/atomicrmw/fence instruction can be moved after the; seq_cst.; - If a store atomic then the same constraints as release, plus; no following sequentially consistent load atomic/store; atomic/atomicrmw/fence instruction can be moved before the; seq_cst.; - If an atomicrmw/fence then same constraints as acq_rel.; ============ ==============================================================. The code sequences used to implement the memory model are defined in the; following sections:. * :ref:`amdgpu-amdhsa-memory-model-gfx6-gfx9`; * :ref:`amdgpu-amdhsa-memory-model-gfx90a`; * :ref:`amdgpu-amdhsa-memory-model-gfx942`; * :ref:`amdgpu-amdhsa-memory-model-gfx10-gfx11`. .. _amdgpu-amdhsa-memory-model-gfx6-gfx9:. Memory Model GFX6-GFX9; ++++++++++++++++++++++. For GFX6-GFX9:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same CU but may be; executed by different SIMDs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it.; * All LDS operations of a C",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:205640,load,load,205640,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"=======; Modules; =======. .. contents::; :local:. Introduction; ============; Most software is built using a number of software libraries, including libraries supplied by the platform, internal libraries built as part of the software itself to provide structure, and third-party libraries. For each library, one needs to access both its interface (API) and its implementation. In the C family of languages, the interface to a library is accessed by including the appropriate header files(s):. .. code-block:: c. #include <SomeLib.h>. The implementation is handled separately by linking against the appropriate library. For example, by passing ``-lSomeLib`` to the linker. Modules provide an alternative, simpler way to use software libraries that provides better compile-time scalability and eliminates many of the problems inherent to using the C preprocessor to access the API of a library. Problems with the current model; -------------------------------; The ``#include`` mechanism provided by the C preprocessor is a very poor way to access the API of a library, for a number of reasons:. * **Compile-time scalability**: Each time a header is included, the; compiler must preprocess and parse the text in that header and every; header it includes, transitively. This process must be repeated for; every translation unit in the application, which involves a huge; amount of redundant work. In a project with *N* translation units; and *M* headers included in each translation unit, the compiler is; performing *M x N* work even though most of the *M* headers are; shared among multiple translation units. C++ is particularly bad,; because the compilation model for templates forces a huge amount of; code into headers. * **Fragility**: ``#include`` directives are treated as textual; inclusion by the preprocessor, and are therefore subject to any; active macro definitions at the time of inclusion. If any of the; active macro definitions happens to collide with a name in the; library, it can b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:777,scalab,scalability,777,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,1,['scalab'],['scalability']
Performance,"=======; Remarks; =======. .. contents::; :local:. Introduction to the LLVM remark diagnostics; ===========================================. LLVM is able to emit diagnostics from passes describing whether an optimization; has been performed or missed for a particular reason, which should give more; insight to users about what the compiler did during the compilation pipeline. There are three main remark types:. ``Passed``. Remarks that describe a successful optimization performed by the compiler. :Example:. ::. foo inlined into bar with (cost=always): always inline attribute. ``Missed``. Remarks that describe an attempt to an optimization by the compiler that; could not be performed. :Example:. ::. foo not inlined into bar because it should never be inlined; (cost=never): noinline function attribute. ``Analysis``. Remarks that describe the result of an analysis, that can bring more; information to the user regarding the generated code. :Example:. ::. 16 stack bytes in function. ::. 10 instructions in function. Enabling optimization remarks; =============================. There are two modes that are supported for enabling optimization remarks in; LLVM: through remark diagnostics, or through serialized remarks. Remark diagnostics; ------------------. Optimization remarks can be emitted as diagnostics. These diagnostics will be; propagated to front-ends if desired, or emitted by tools like :doc:`llc; <CommandGuide/llc>` or :doc:`opt <CommandGuide/opt>`. .. option:: -pass-remarks=<regex>. Enables optimization remarks from passes whose name match the given (POSIX); regular expression. .. option:: -pass-remarks-missed=<regex>. Enables missed optimization remarks from passes whose name match the given; (POSIX) regular expression. .. option:: -pass-remarks-analysis=<regex>. Enables optimization analysis remarks from passes whose name match the given; (POSIX) regular expression. Serialized remarks; ------------------. While diagnostics are useful during development, it is oft",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst:208,optimiz,optimization,208,interpreter/llvm-project/llvm/docs/Remarks.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst,6,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"=======; ThinLTO; =======. .. contents::; :local:. Introduction; ============. *ThinLTO* compilation is a new type of LTO that is both scalable and; incremental. *LTO* (Link Time Optimization) achieves better; runtime performance through whole-program analysis and cross-module; optimization. However, monolithic LTO implements this by merging all; input into a single module, which is not scalable; in time or memory, and also prevents fast incremental compiles. In ThinLTO mode, as with regular LTO, clang emits LLVM bitcode after the; compile phase. The ThinLTO bitcode is augmented with a compact summary; of the module. During the link step, only the summaries are read and; merged into a combined summary index, which includes an index of function; locations for later cross-module function importing. Fast and efficient; whole-program analysis is then performed on the combined summary index. However, all transformations, including function importing, occur; later when the modules are optimized in fully parallel backends.; By default, linkers_ that support ThinLTO are set up to launch; the ThinLTO backends in threads. So the usage model is not affected; as the distinction between the fast serial thin link step and the backends; is transparent to the user. For more information on the ThinLTO design and current performance,; see the LLVM blog post `ThinLTO: Scalable and Incremental LTO; <http://blog.llvm.org/2016/06/thinlto-scalable-and-incremental-lto.html>`_.; While tuning is still in progress, results in the blog post show that; ThinLTO already performs well compared to LTO, in many cases matching; the performance improvement. Current Status; ==============. Clang/LLVM; ----------; .. _compiler:. The 3.9 release of clang includes ThinLTO support. However, ThinLTO; is under active development, and new features, improvements and bugfixes; are being added for the next release. For the latest ThinLTO support,; `build a recent version of clang and LLVM; <https://llvm.org/docs/",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:135,scalab,scalable,135,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,6,"['optimiz', 'perform', 'scalab']","['optimization', 'optimized', 'performance', 'performed', 'scalable']"
Performance,"======== ============ ============== ========== ================================. .. _amdgpu-amdhsa-memory-model-gfx90a:. Memory Model GFX90A; +++++++++++++++++++. For GFX90A:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same CU but may be; executed by different SIMDs. The exception is when in tgsplit execution mode; when the wavefronts may be executed by different SIMDs in different CUs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it. The exception is when in tgsplit execution mode when no LDS; is allocated as wavefronts of the same work-group can be in different CUs.; * All LDS operations of a CU are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that ``flat_load/store/atomic`` instructions can report out of vector memory; order if they access LDS memory, and out of LDS operation order if they access; global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore:. * No special action is required for cohe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:234991,queue,queues,234991,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queues']
Performance,"======== ============ ============== ========== ================================. .. _amdgpu-amdhsa-memory-model-gfx942:. Memory Model GFX942; +++++++++++++++++++. For GFX942:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same CU but may be; executed by different SIMDs. The exception is when in tgsplit execution mode; when the wavefronts may be executed by different SIMDs in different CUs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it. The exception is when in tgsplit execution mode when no LDS; is allocated as wavefronts of the same work-group can be in different CUs.; * All LDS operations of a CU are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that ``flat_load/store/atomic`` instructions can report out of vector memory; order if they access LDS memory, and out of LDS operation order if they access; global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore:. * No special action is required for cohe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:285139,queue,queues,285139,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queues']
Performance,"========. .. .. table:: AMDGPU Trap Handler for AMDHSA OS Code Object V4 and Above; :name: amdgpu-trap-handler-for-amdhsa-os-v4-onwards-table. =================== =============== ================ ================= =======================================; Usage Code Sequence GFX6-GFX8 Inputs GFX9-GFX11 Inputs Description; =================== =============== ================ ================= =======================================; reserved ``s_trap 0x00`` Reserved by hardware.; debugger breakpoint ``s_trap 0x01`` *none* *none* Reserved for debugger to use for; breakpoints. Causes wave to be halted; with the PC at the trap instruction.; The debugger is responsible to resume; the wave, including the instruction; that the breakpoint overwrote.; ``llvm.trap`` ``s_trap 0x02`` ``SGPR0-1``: *none* Causes wave to be halted with the PC at; ``queue_ptr`` the trap instruction. The associated; queue is signalled to put it into the; error state. When the queue is put in; the error state, the waves executing; dispatches on the queue will be; terminated.; ``llvm.debugtrap`` ``s_trap 0x03`` *none* *none* - If debugger not enabled then behaves; as a no-operation. The trap handler; is entered and immediately returns to; continue execution of the wavefront.; - If the debugger is enabled, causes; the debug trap to be reported by the; debugger and the wavefront is put in; the halt state with the PC at the; instruction. The debugger must; increment the PC and resume the wave.; reserved ``s_trap 0x04`` Reserved.; reserved ``s_trap 0x05`` Reserved.; reserved ``s_trap 0x06`` Reserved.; reserved ``s_trap 0x07`` Reserved.; reserved ``s_trap 0x08`` Reserved.; reserved ``s_trap 0xfe`` Reserved.; reserved ``s_trap 0xff`` Reserved.; =================== =============== ================ ================= =======================================. .. _amdgpu-amdhsa-function-call-convention:. Call Convention; ~~~~~~~~~~~~~~~. .. note::. This section is currently incomplete and has inaccuracies. It is WI",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:384919,queue,queue,384919,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['queue'],['queue']
Performance,"========; GWP-ASan; ========. .. contents::; :local:; :depth: 2. Introduction; ============. GWP-ASan is a sampled allocator framework that assists in finding use-after-free; and heap-buffer-overflow bugs in production environments. It informally is a; recursive acronym, ""**G**\WP-ASan **W**\ill **P**\rovide **A**\llocation; **SAN**\ity"". GWP-ASan is based on the classic; `Electric Fence Malloc Debugger <https://linux.die.net/man/3/efence>`_, with a; key adaptation. Notably, we only choose a very small percentage of allocations; to sample, and apply guard pages to these sampled allocations only. The sampling; is small enough to allow us to have very low performance overhead. There is a small, tunable memory overhead that is fixed for the lifetime of the; process. This is approximately ~40KiB per process using the default settings,; depending on the average size of your allocations. GWP-ASan vs. ASan; =================. Unlike `AddressSanitizer <https://clang.llvm.org/docs/AddressSanitizer.html>`_,; GWP-ASan does not induce a significant performance overhead. ASan often requires; the use of dedicated canaries to be viable in production environments, and as; such is often impractical. GWP-ASan is only capable of finding a subset of the memory issues detected by; ASan. Furthermore, GWP-ASan's bug detection capabilities are only probabilistic.; As such, we recommend using ASan over GWP-ASan in testing, as well as anywhere; else that guaranteed error detection is more valuable than the 2x execution; slowdown/binary size bloat. For the majority of production environments, this; impact is too high, and GWP-ASan proves extremely useful. Design; ======. **Please note:** The implementation of GWP-ASan is largely in-flux, and these; details are subject to change. There are currently other implementations of; GWP-ASan, such as the implementation featured in; `Chromium <https://cs.chromium.org/chromium/src/components/gwp_asan/>`_. The; long-term support goal is to ensure feature-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GwpAsan.rst:662,perform,performance,662,interpreter/llvm-project/llvm/docs/GwpAsan.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GwpAsan.rst,1,['perform'],['performance']
Performance,"=========. The SYCL specification represents pointers to disjoint memory regions using C++; wrapper classes on an accelerator to enable compilation with a standard C++; toolchain and a SYCL compiler toolchain. Section 3.8.2 of SYCL 2020; specification defines; `memory model <https://www.khronos.org/registry/SYCL/specs/sycl-2020/html/sycl-2020.html#_sycl_device_memory_model>`_\ ,; section 4.7.7 - `address space classes <https://www.khronos.org/registry/SYCL/specs/sycl-2020/html/sycl-2020.html#_address_space_classes>`_; and section 5.9 covers `address space deduction <https://www.khronos.org/registry/SYCL/specs/sycl-2020/html/sycl-2020.html#_address_space_deduction>`_.; The SYCL specification allows two modes of address space deduction: ""generic as; default address space"" (see section 5.9.3) and ""inferred address space"" (see; section 5.9.4). Current implementation supports only ""generic as default address; space"" mode. SYCL borrows its memory model from OpenCL however SYCL doesn't perform; the address space qualifier inference as detailed in; `OpenCL C v3.0 6.7.8 <https://www.khronos.org/registry/OpenCL/specs/3.0-unified/html/OpenCL_C.html#addr-spaces-inference>`_. The default address space is ""generic-memory"", which is a virtual address space; that overlaps the global, local, and private address spaces. SYCL mode enables; following conversions:. - explicit conversions to/from the default address space from/to the address; space-attributed type; - implicit conversions from the address space-attributed type to the default; address space; - explicit conversions to/from the global address space from/to the; ``__attribute__((opencl_global_device))`` or; ``__attribute__((opencl_global_host))`` address space-attributed type; - implicit conversions from the ``__attribute__((opencl_global_device))`` or; ``__attribute__((opencl_global_host))`` address space-attributed type to the; global address space. All named address spaces are disjoint and sub-sets of default address space",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SYCLSupport.rst:1515,perform,perform,1515,interpreter/llvm-project/clang/docs/SYCLSupport.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SYCLSupport.rst,1,['perform'],['perform']
Performance,"=========. This document is the central repository for all information pertaining to debug; information in LLVM. It describes the :ref:`actual format that the LLVM debug; information takes <format>`, which is useful for those interested in creating; front-ends or dealing directly with the information. Further, this document; provides specific examples of what debug information for C/C++ looks like. Philosophy behind LLVM debugging information; --------------------------------------------. The idea of the LLVM debugging information is to capture how the important; pieces of the source-language's Abstract Syntax Tree map onto LLVM code.; Several design aspects have shaped the solution that appears here. The; important ones are:. * Debugging information should have very little impact on the rest of the; compiler. No transformations, analyses, or code generators should need to; be modified because of debugging information. * LLVM optimizations should interact in :ref:`well-defined and easily described; ways <intro_debugopt>` with the debugging information. * Because LLVM is designed to support arbitrary programming languages,; LLVM-to-LLVM tools should not need to know anything about the semantics of; the source-level-language. * Source-level languages are often **widely** different from one another.; LLVM should not put any restrictions of the flavor of the source-language,; and the debugging information should work with any language. * With code generator support, it should be possible to use an LLVM compiler; to compile a program to native machine code and standard debugging; formats. This allows compatibility with traditional machine-code level; debuggers, like GDB or DBX. The approach used by the LLVM implementation is to use a small set of; :ref:`intrinsic functions <format_common_intrinsics>` to define a mapping; between LLVM program objects and the source-level objects. The description of; the source-level program is maintained in LLVM metadata in an; :ref:`impl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:1083,optimiz,optimizations,1083,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimizations']
Performance,"=========; SafeStack; =========. .. contents::; :local:. Introduction; ============. SafeStack is an instrumentation pass that protects programs against attacks; based on stack buffer overflows, without introducing any measurable performance; overhead. It works by separating the program stack into two distinct regions:; the safe stack and the unsafe stack. The safe stack stores return addresses,; register spills, and local variables that are always accessed in a safe way,; while the unsafe stack stores everything else. This separation ensures that; buffer overflows on the unsafe stack cannot be used to overwrite anything; on the safe stack. SafeStack is a part of the `Code-Pointer Integrity (CPI) Project; <https://dslab.epfl.ch/research/cpi/>`_. Performance; -----------. The performance overhead of the SafeStack instrumentation is less than 0.1% on; average across a variety of benchmarks (see the `Code-Pointer Integrity; <https://dslab.epfl.ch/pubs/cpi.pdf>`__ paper for details). This is mainly; because most small functions do not have any variables that require the unsafe; stack and, hence, do not need unsafe stack frames to be created. The cost of; creating unsafe stack frames for large functions is amortized by the cost of; executing the function. In some cases, SafeStack actually improves the performance. Objects that end up; being moved to the unsafe stack are usually large arrays or variables that are; used through multiple stack frames. Moving such objects away from the safe; stack increases the locality of frequently accessed values on the stack, such; as register spills, return addresses, and small local variables. Compatibility; -------------. Most programs, static libraries, or individual files can be compiled; with SafeStack as is. SafeStack requires basic runtime support, which, on most; platforms, is implemented as a compiler-rt library that is automatically linked; in when the program is compiled with SafeStack. Linking a DSO with SafeStack is not curr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SafeStack.rst:230,perform,performance,230,interpreter/llvm-project/clang/docs/SafeStack.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SafeStack.rst,2,['perform'],['performance']
Performance,"==========. .. Kernel Dispatch; ~~~~~~~~~~~~~~~. The HSA architected queuing language (AQL) defines a user space memory interface; that can be used to control the dispatch of kernels, in an agent independent; way. An agent can have zero or more AQL queues created for it using an HSA; compatible runtime (see :ref:`amdgpu-os`), in which AQL packets (all of which; are 64 bytes) can be placed. See the *HSA Platform System Architecture; Specification* [HSA]_ for the AQL queue mechanics and packet layouts. The packet processor of a kernel agent is responsible for detecting and; dispatching HSA kernels from the AQL queues associated with it. For AMD GPUs the; packet processor is implemented by the hardware command processor (CP),; asynchronous dispatch controller (ADC) and shader processor input controller; (SPI). An HSA compatible runtime can be used to allocate an AQL queue object. It uses; the kernel mode driver to initialize and register the AQL queue with CP. To dispatch a kernel the following actions are performed. This can occur in the; CPU host program, or from an HSA kernel executing on a GPU. 1. A pointer to an AQL queue for the kernel agent on which the kernel is to be; executed is obtained.; 2. A pointer to the kernel descriptor (see; :ref:`amdgpu-amdhsa-kernel-descriptor`) of the kernel to execute is obtained.; It must be for a kernel that is contained in a code object that was loaded; by an HSA compatible runtime on the kernel agent with which the AQL queue is; associated.; 3. Space is allocated for the kernel arguments using the HSA compatible runtime; allocator for a memory region with the kernarg property for the kernel agent; that will execute the kernel. It must be at least 16-byte aligned.; 4. Kernel argument values are assigned to the kernel argument memory; allocation. The layout is defined in the *HSA Programmer's Language; Reference* [HSA]_. For AMDGPU the kernel execution directly accesses the; kernel argument memory in the same way constant memory",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:149823,perform,performed,149823,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"===========+========================================================================+; | void* | addr | Address of global symbol within device image (function or global) |; +---------+------------+------------------------------------------------------------------------+; | char* | name | Name of the symbol |; +---------+------------+------------------------------------------------------------------------+; | size_t | size | Size of the entry info (0 if it is a function) |; +---------+------------+------------------------------------------------------------------------+; | int32_t | flags | Flags associated with the entry (see :ref:`table-offload_entry_flags`) |; +---------+------------+------------------------------------------------------------------------+; | int32_t | reserved | Reserved, to be used by the runtime library. |; +---------+------------+------------------------------------------------------------------------+. The address of the global symbol will be set to the device pointer value by the; runtime once the device image is loaded. The flags are set to indicate the; handling required for the offloading entry. If the offloading entry is an entry; to a target region it can have one of the following :ref:`entry flags; <table-offload_entry_flags>`. .. table:: Target Region Entry Flags; :name: table-offload_entry_flags. +----------------------------------+-------+-----------------------------------------+; | Name | Value | Description |; +==================================+=======+=========================================+; | OMPTargetRegionEntryTargetRegion | 0x00 | Mark the entry as generic target region |; +----------------------------------+-------+-----------------------------------------+; | OMPTargetRegionEntryCtor | 0x02 | Mark the entry as a global constructor |; +----------------------------------+-------+-----------------------------------------+; | OMPTargetRegionEntryDtor | 0x04 | Mark the entry as a global destructor |; +----------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst:4819,load,loaded,4819,interpreter/llvm-project/clang/docs/OffloadingDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst,1,['load'],['loaded']
Performance,"===========. In the common case, a value is available in a register, and the; ``Offset`` field will be zero. Values spilled to the stack are encoded; as ``Indirect`` locations. The runtime must load those values from a; stack address, typically in the form ``[BP + Offset]``. If an; ``alloca`` value is passed directly to a stack map intrinsic, then; LLVM may fold the frame index into the stack map as an optimization to; avoid allocating a register or stack slot. These frame indices will be; encoded as ``Direct`` locations in the form ``BP + Offset``. LLVM may; also optimize constants by emitting them directly in the stack map,; either in the ``Offset`` of a ``Constant`` location or in the constant; pool, referred to by ``ConstantIndex`` locations. At each callsite, a ""liveout"" register list is also recorded. These; are the registers that are live across the stackmap and therefore must; be saved by the runtime. This is an important optimization when the; patchpoint intrinsic is used with a calling convention that by default; preserves most registers as callee-save. Each entry in the liveout register list contains a DWARF register; number and size in bytes. The stackmap format deliberately omits; specific subregister information. Instead the runtime must interpret; this information conservatively. For example, if the stackmap reports; one byte at ``%rax``, then the value may be in either ``%al`` or; ``%ah``. It doesn't matter in practice, because the runtime will; simply save ``%rax``. However, if the stackmap reports 16 bytes at; ``%ymm0``, then the runtime can safely optimize by saving only; ``%xmm0``. The stack map format is a contract between an LLVM SVN revision and; the runtime. It is currently experimental and may change in the short; term, but minimizing the need to update the runtime is; important. Consequently, the stack map design is motivated by; simplicity and extensibility. Compactness of the representation is; secondary because the runtime is expected to ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst:14649,optimiz,optimization,14649,interpreter/llvm-project/llvm/docs/StackMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst,1,['optimiz'],['optimization']
Performance,"============ ============ ============== ========== ================================. .. _amdgpu-amdhsa-memory-model-gfx10-gfx11:. Memory Model GFX10-GFX11; ++++++++++++++++++++++++. For GFX10-GFX11:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple work-group processors (WGP).; * Each WGP has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same; WGP. In CU wavefront execution mode the wavefronts may be executed by; different SIMDs in the same CU. In WGP wavefront execution mode the; wavefronts may be executed by different SIMDs in different CUs in the same; WGP.; * Each WGP has a single LDS memory shared by the wavefronts of the work-groups; executing on it.; * All LDS operations of a WGP are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; WGP. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations.; Completion of load/store/sample operations are reported to a wavefront in; execution order of other load/store/sample operations performed by that; wavefront.; * The vector memory operations access a vector L0 cache. There is a single L0; cache per CU. Each SIMD of a CU accesses the same L0 cache. Therefore, no; special action is required for coherence between the lanes of a single; wavefront",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:335981,queue,queues,335981,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queues']
Performance,============ ==============================================================; Vendor Description; ============ ==============================================================; ``amd`` Can be used for all AMD GPU usage.; ``mesa3d`` Can be used if the OS is ``mesa3d``.; ============ ==============================================================. .. table:: AMDGPU Operating Systems; :name: amdgpu-os. ============== ============================================================; OS Description; ============== ============================================================; *<empty>* Defaults to the *unknown* OS.; ``amdhsa`` Compute kernels executed on HSA [HSA]_ compatible runtimes; such as:. - AMD's ROCm™ runtime [AMD-ROCm]_ using the *rocm-amdhsa*; loader on Linux. See *AMD ROCm Platform Release Notes*; [AMD-ROCm-Release-Notes]_ for supported hardware and; software.; - AMD's PAL runtime using the *pal-amdhsa* loader on; Windows. ``amdpal`` Graphic shaders and compute kernels executed on AMD's PAL; runtime using the *pal-amdpal* loader on Windows and Linux; Pro.; ``mesa3d`` Graphic shaders and compute kernels executed on AMD's Mesa; 3D runtime using the *mesa-mesa3d* loader on Linux.; ============== ============================================================. .. table:: AMDGPU Environments; :name: amdgpu-environment-table. ============ ==============================================================; Environment Description; ============ ==============================================================; *<empty>* Default.; ============ ==============================================================. .. _amdgpu-processors:. Processors; ----------. Use the Clang options ``-mcpu=<target-id>`` or ``--offload-arch=<target-id>`` to; specify the AMDGPU processor together with optional target features. See; :ref:`amdgpu-target-id` and :ref:`amdgpu-target-features` for AMD GPU target; specific information. Every processor supports every OS ABI (see :ref:`amdgpu-os`) with the following excep,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:2700,load,loader,2700,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loader']
Performance,"=============. The HLSL language is insufficiently documented, and not formally specified.; Documentation is available on `Microsoft's website; <https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl>`_.; The language syntax is similar enough to C and C++ that carefully written C and; C++ code is valid HLSL. HLSL has some key differences from C & C++ which we will; need to handle in Clang. HLSL is not a conforming or valid extension or superset of C or C++. The; language has key incompatibilities with C and C++, both syntactically and; semantically. An Aside on GPU Languages; -------------------------. Due to HLSL being a GPU targeted language HLSL is a Single Program Multiple Data; (SPMD) language relying on the implicit parallelism provided by GPU hardware.; Some language features in HLSL enable programmers to take advantage of the; parallel nature of GPUs in a hardware abstracted language. HLSL also prohibits some features of C and C++ which can have catastrophic; performance or are not widely supportable on GPU hardware or drivers. As an; example, register spilling is often excessively expensive on GPUs, so HLSL; requires all functions to be inlined during code generation, and does not; support a runtime calling convention. Pointers & References; ---------------------. HLSL does not support referring to values by address. Semantically all variables; are value-types and behave as such. HLSL disallows the pointer dereference; operators (unary ``*``, and ``->``), as well as the address of operator (unary; &). While HLSL disallows pointers and references in the syntax, HLSL does use; reference types in the AST, and we intend to use pointer decay in the AST in; the Clang implementation. HLSL ``this`` Keyword; ---------------------. HLSL does support member functions, and (in HLSL 2021) limited operator; overloading. With member function support, HLSL also has a ``this`` keyword. The; ``this`` keyword is an example of one of the places where HLSL",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/HLSLSupport.rst:6025,perform,performance,6025,interpreter/llvm-project/clang/docs/HLSL/HLSLSupport.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/HLSLSupport.rst,1,['perform'],['performance']
Performance,"=============; Calling Convention Description; =============================== ==========================================================; ``ccc`` The C calling convention. Used by default.; See :ref:`amdgpu-amdhsa-function-call-convention-non-kernel-functions`; for more details. ``fastcc`` The fast calling convention. Mostly the same as the ``ccc``. ``coldcc`` The cold calling convention. Mostly the same as the ``ccc``. ``amdgpu_cs`` Used for Mesa/AMDPAL compute shaders.; ..TODO::; Describe. ``amdgpu_cs_chain`` Similar to ``amdgpu_cs``, with differences described below. Functions with this calling convention cannot be called directly. They must; instead be launched via the ``llvm.amdgcn.cs.chain`` intrinsic. Arguments are passed in SGPRs, starting at s0, if they have the ``inreg``; attribute, and in VGPRs otherwise, starting at v8. Using more SGPRs or VGPRs; than available in the subtarget is not allowed. On subtargets that use; a scratch buffer descriptor (as opposed to ``scratch_{load,store}_*`` instructions),; the scratch buffer descriptor is passed in s[48:51]. This limits the; SGPR / ``inreg`` arguments to the equivalent of 48 dwords; using more; than that is not allowed. The return type must be void.; Varargs, sret, byval, byref, inalloca, preallocated are not supported. Values in scalar registers as well as v0-v7 are not preserved. Values in; VGPRs starting at v8 are not preserved for the active lanes, but must be; saved by the callee for inactive lanes when using WWM. Wave scratch is ""empty"" at function boundaries. There is no stack pointer input; or output value, but functions are free to use scratch starting from an initial; stack pointer. Calls to ``amdgpu_gfx`` functions are allowed and behave like they; do in ``amdgpu_cs`` functions. All counters (``lgkmcnt``, ``vmcnt``, ``storecnt``, etc.) are presumed in an; unknown state at function entry. A function may have multiple exits (e.g. one chain exit and one plain ``ret void``; for when the wave ends), but",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:53198,load,load,53198,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"=============; Hwreg Value Syntax Description; ==================================== ===============================================================================; hwreg({0..63}) All bits of a register indicated by the register *id*.; hwreg(<*name*>) All bits of a register indicated by the register *name*.; hwreg({0..63}, {0..31}, {1..32}) Register bits indicated by the register *id*, first bit *offset* and *size*.; hwreg(<*name*>, {0..31}, {1..32}) Register bits indicated by the register *name*, first bit *offset* and *size*.; ==================================== ===============================================================================. Numeric values may be specified as positive :ref:`integer numbers<amdgpu_synid_integer_number>`; or :ref:`absolute expressions<amdgpu_synid_absolute_expression>`. Predefined register *names* include:. ============================== ==========================================; Name Description; ============================== ==========================================; HW_REG_MODE Shader writable mode bits.; HW_REG_STATUS Shader read-only status.; HW_REG_TRAPSTS Trap status.; HW_REG_HW_ID1 Id of wave, simd, compute unit, etc.; HW_REG_HW_ID2 Id of queue, pipeline, etc.; HW_REG_GPR_ALLOC Per-wave SGPR and VGPR allocation.; HW_REG_LDS_ALLOC Per-wave LDS allocation.; HW_REG_IB_STS Counters of outstanding instructions.; HW_REG_SH_MEM_BASES Memory aperture.; HW_REG_FLAT_SCR_LO flat_scratch_lo register.; HW_REG_FLAT_SCR_HI flat_scratch_hi register.; ============================== ==========================================. Examples:. .. parsed-literal::. reg = 1; offset = 2; size = 4; hwreg_enc = reg | (offset << 6) | ((size - 1) << 11). s_getreg_b32 s2, 0x1881; s_getreg_b32 s2, hwreg_enc // the same as above; s_getreg_b32 s2, hwreg(1, 2, 4) // the same as above; s_getreg_b32 s2, hwreg(reg, offset, size) // the same as above. s_getreg_b32 s2, hwreg(15); s_getreg_b32 s2, hwreg(51, 1, 31); s_getreg_b32 s2, hwreg(HW_REG_LDS_ALLOC, 0, 1); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_hwreg.rst:2139,queue,queue,2139,interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_hwreg.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPU/gfx11_hwreg.rst,1,['queue'],['queue']
Performance,"=============; bin/clang-proto-fuzzer CORPUS_DIR. Arguments can be specified after -ignore_remaining_args=1 to modify the compiler; invocation. For example, the following command line will fuzz LLVM with a; custom optimization level and target triple:; bin/clang-proto-fuzzer CORPUS_DIR -ignore_remaining_args=1 -O3 -triple \; arm64apple-ios9. To translate a clang-proto-fuzzer corpus output to C++:; bin/clang-proto-to-cxx CORPUS_OUTPUT_FILE. ===================; llvm-proto-fuzzer; ===================; Like, clang-proto-fuzzer, llvm-proto-fuzzer is also a protobuf-mutator based; fuzzer. It receives as input a cxx_loop_proto which it then converts into a; string of valid LLVM IR: a function with either a single loop or two nested; loops. It then creates a new string of IR by running optimization passes over; the original IR. Currently, it only runs a loop-vectorize pass but more passes; can easily be added to the fuzzer. Once there are two versions of the input; function (optimized and not), llvm-proto-fuzzer uses LLVM's JIT Engine to; compile both functions. Lastly, it runs both functions on a suite of inputs and; checks that both functions behave the same on all inputs. In this way,; llvm-proto-fuzzer can find not only compiler crashes, but also miscompiles; originating from LLVM's optimization passes. llvm-proto-fuzzer is built very similarly to clang-proto-fuzzer. You can run the; fuzzer with the following command:; bin/clang-llvm-proto-fuzzer CORPUS_DIR. To translate a cxx_loop_proto file into LLVM IR do:; bin/clang-loop-proto-to-llvm CORPUS_OUTPUT_FILE; To translate a cxx_loop_proto file into C++ do:; bin/clang-loop-proto-to-cxx CORPUS_OUTPUT_FILE. Note: To get a higher number of executions per second with llvm-proto-fuzzer it; helps to build it without ASan instrumentation and with the -O2 flag. Because; the fuzzer is not only compiling code, but also running it, as the inputs get; large, the time necessary to fuzz one input can get very high.; Example:; cmake .. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/README.txt:4635,optimiz,optimized,4635,interpreter/llvm-project/clang/tools/clang-fuzzer/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/README.txt,1,['optimiz'],['optimized']
Performance,"==============. .. .. table:: compute_pgm_rsrc3 for GFX12; :name: amdgpu-amdhsa-compute_pgm_rsrc3-gfx12-table. ======= ======= =============================== ===========================================================================; Bits Size Field Name Description; ======= ======= =============================== ===========================================================================; 3:0 4 bits RESERVED Reserved, must be 0.; 11:4 8 bits INST_PREF_SIZE Number of instruction bytes to prefetch, starting at the kernel's entry; point instruction, before wavefront starts execution. The value is 0..255; with a granularity of 128 bytes.; 12 1 bit RESERVED Reserved, must be 0.; 13 1 bit GLG_EN If 1, group launch guarantee will be enabled for this dispatch; 30:14 17 bits RESERVED Reserved, must be 0.; 31 1 bit IMAGE_OP If 1, the kernel execution contains image instructions. If executed as; part of a graphics pipeline, image read instructions will stall waiting; for any necessary ``WAIT_SYNC`` fence to be performed in order to; indicate that earlier pipeline stages have completed writing to the; image. Not used for compute kernels that are not part of a graphics pipeline and; must be 0.; 32 **Total size 4 bytes.**; ======= ===================================================================================================================. .. .. table:: Floating Point Rounding Mode Enumeration Values; :name: amdgpu-amdhsa-floating-point-rounding-mode-enumeration-values-table. ====================================== ===== ==============================; Enumeration Name Value Description; ====================================== ===== ==============================; FLOAT_ROUND_MODE_NEAR_EVEN 0 Round Ties To Even; FLOAT_ROUND_MODE_PLUS_INFINITY 1 Round Toward +infinity; FLOAT_ROUND_MODE_MINUS_INFINITY 2 Round Toward -infinity; FLOAT_ROUND_MODE_ZERO 3 Round Toward 0; ====================================== ===== ==============================. .. table:: Extended FLT_ROUNDS En",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:179333,perform,performed,179333,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"==============; LTO Visibility; ==============. *LTO visibility* is a property of an entity that specifies whether it can be; referenced from outside the current LTO unit. A *linkage unit* is a set of; translation units linked together into an executable or DSO, and a linkage; unit's *LTO unit* is the subset of the linkage unit that is linked together; using link-time optimization; in the case where LTO is not being used, the; linkage unit's LTO unit is empty. Each linkage unit has only a single LTO unit. The LTO visibility of a class is used by the compiler to determine which; classes the whole-program devirtualization (``-fwhole-program-vtables``) and; control flow integrity (``-fsanitize=cfi-vcall`` and ``-fsanitize=cfi-mfcall``); features apply to. These features use whole-program information, so they; require the entire class hierarchy to be visible in order to work correctly. If any translation unit in the program uses either of the whole-program; devirtualization or control flow integrity features, it is effectively an ODR; violation to define a class with hidden LTO visibility in multiple linkage; units. A class with public LTO visibility may be defined in multiple linkage; units, but the tradeoff is that the whole-program devirtualization and; control flow integrity features can only be applied to classes with hidden LTO; visibility. A class's LTO visibility is treated as an ODR-relevant property; of its definition, so it must be consistent between translation units. In translation units built with LTO, LTO visibility is based on the; class's symbol visibility as expressed at the source level (i.e. the; ``__attribute__((visibility(""..."")))`` attribute, or the ``-fvisibility=``; flag) or, on the Windows platform, the dllimport and dllexport attributes. When; targeting non-Windows platforms, classes with a visibility other than hidden; visibility receive public LTO visibility. When targeting Windows, classes; with dllimport or dllexport attributes receive publ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LTOVisibility.rst:371,optimiz,optimization,371,interpreter/llvm-project/clang/docs/LTOVisibility.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LTOVisibility.rst,1,['optimiz'],['optimization']
Performance,"===============. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 2 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 2 of the ""Building an ORC-based JIT in LLVM"" tutorial. In; `Chapter 1 <BuildingAJIT1.html>`_ of this series we examined a basic JIT; class, KaleidoscopeJIT, that could take LLVM IR modules as input and produce; executable code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:1232,optimiz,optimization,1232,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimization']
Performance,"===============; Opaque Pointers; ===============. The Opaque Pointer Type; =======================. Traditionally, LLVM IR pointer types have contained a pointee type. For example,; ``i32*`` is a pointer that points to an ``i32`` somewhere in memory. However,; due to a lack of pointee type semantics and various issues with having pointee; types, there is a desire to remove pointee types from pointers. The opaque pointer type project aims to replace all pointer types containing; pointee types in LLVM with an opaque pointer type. The new pointer type is; represented textually as ``ptr``. Some instructions still need to know what type to treat the memory pointed to by; the pointer as. For example, a load needs to know how many bytes to load from; memory and what type to treat the resulting value as. In these cases,; instructions themselves contain a type argument. For example the load; instruction from older versions of LLVM. .. code-block:: llvm. load i64* %p. becomes. .. code-block:: llvm. load i64, ptr %p. Address spaces are still used to distinguish between different kinds of pointers; where the distinction is relevant for lowering (e.g. data vs function pointers; have different sizes on some architectures). Opaque pointers are not changing; anything related to address spaces and lowering. For more information, see; `DataLayout <LangRef.html#langref-datalayout>`_. Opaque pointers in non-default; address space are spelled ``ptr addrspace(N)``. This was proposed all the way back in; `2015 <https://lists.llvm.org/pipermail/llvm-dev/2015-February/081822.html>`_. Issues with explicit pointee types; ==================================. LLVM IR pointers can be cast back and forth between pointers with different; pointee types. The pointee type does not necessarily represent the actual; underlying type in memory. In other words, the pointee type carries no real; semantics. Historically LLVM was some sort of type-safe subset of C. Having pointee types; provided an extra laye",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:707,load,load,707,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst,4,['load'],['load']
Performance,"===============; ShadowCallStack; ===============. .. contents::; :local:. Introduction; ============. ShadowCallStack is an instrumentation pass, currently only implemented for; aarch64, that protects programs against return address overwrites; (e.g. stack buffer overflows.) It works by saving a function's return address; to a separately allocated 'shadow call stack' in the function prolog in; non-leaf functions and loading the return address from the shadow call stack; in the function epilog. The return address is also stored on the regular stack; for compatibility with unwinders, but is otherwise unused. The aarch64 implementation is considered production ready, and; an `implementation of the runtime`_ has been added to Android's libc; (bionic). An x86_64 implementation was evaluated using Chromium and was found; to have critical performance and security deficiencies--it was removed in; LLVM 9.0. Details on the x86_64 implementation can be found in the; `Clang 7.0.1 documentation`_. .. _`implementation of the runtime`: https://android.googlesource.com/platform/bionic/+/808d176e7e0dd727c7f929622ec017f6e065c582/libc/bionic/pthread_create.cpp#128; .. _`Clang 7.0.1 documentation`: https://releases.llvm.org/7.0.1/tools/clang/docs/ShadowCallStack.html. Comparison; ----------. To optimize for memory consumption and cache locality, the shadow call; stack stores only an array of return addresses. This is in contrast to other; schemes, like :doc:`SafeStack`, that mirror the entire stack and trade-off; consuming more memory for shorter function prologs and epilogs with fewer; memory accesses. `Return Flow Guard`_ is a pure software implementation of shadow call stacks; on x86_64. Like the previous implementation of ShadowCallStack on x86_64, it is; inherently racy due to the architecture's use of the stack for calls and; returns. Intel `Control-flow Enforcement Technology`_ (CET) is a proposed hardware; extension that would add native support to use a shadow stack to store/c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ShadowCallStack.rst:421,load,loading,421,interpreter/llvm-project/clang/docs/ShadowCallStack.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ShadowCallStack.rst,2,"['load', 'perform']","['loading', 'performance']"
Performance,"================; LeakSanitizer; ================. .. contents::; :local:. Introduction; ============. LeakSanitizer is a run-time memory leak detector. It can be combined with; :doc:`AddressSanitizer` to get both memory error and leak detection, or; used in a stand-alone mode. LSan adds almost no performance overhead; until the very end of the process, at which point there is an extra leak; detection phase. Usage; =====. :doc:`AddressSanitizer`: integrates LeakSanitizer and enables it by default on; supported platforms. .. code-block:: console. $ cat memory-leak.c; #include <stdlib.h>; void *p;; int main() {; p = malloc(7);; p = 0; // The memory is leaked here.; return 0;; }; % clang -fsanitize=address -g memory-leak.c ; ASAN_OPTIONS=detect_leaks=1 ./a.out; ==23646==ERROR: LeakSanitizer: detected memory leaks; Direct leak of 7 byte(s) in 1 object(s) allocated from:; #0 0x4af01b in __interceptor_malloc /projects/compiler-rt/lib/asan/asan_malloc_linux.cc:52:3; #1 0x4da26a in main memory-leak.c:4:7; #2 0x7f076fd9cec4 in __libc_start_main libc-start.c:287; SUMMARY: AddressSanitizer: 7 byte(s) leaked in 1 allocation(s). To use LeakSanitizer in stand-alone mode, link your program with; ``-fsanitize=leak`` flag. Make sure to use ``clang`` (not ``ld``) for the; link step, so that it would link in proper LeakSanitizer run-time library; into the final executable. Supported Platforms; ===================. * Android aarch64/i386/x86_64; * Fuchsia aarch64/x86_64; * Linux arm/aarch64/mips64/ppc64/ppc64le/riscv64/s390x/i386/x86\_64; * macOS aarch64/i386/x86\_64; * NetBSD i386/x86_64. More Information; ================. `<https://github.com/google/sanitizers/wiki/AddressSanitizerLeakSanitizer>`_; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LeakSanitizer.rst:299,perform,performance,299,interpreter/llvm-project/clang/docs/LeakSanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LeakSanitizer.rst,1,['perform'],['performance']
Performance,"================; MemorySanitizer; ================. .. contents::; :local:. Introduction; ============. MemorySanitizer is a detector of uninitialized reads. It consists of a; compiler instrumentation module and a run-time library. Typical slowdown introduced by MemorySanitizer is **3x**. How to build; ============. Build LLVM/Clang with `CMake <https://llvm.org/docs/CMake.html>`_. Usage; =====. Simply compile and link your program with ``-fsanitize=memory`` flag.; The MemorySanitizer run-time library should be linked to the final; executable, so make sure to use ``clang`` (not ``ld``) for the final; link step. When linking shared libraries, the MemorySanitizer run-time; is not linked, so ``-Wl,-z,defs`` may cause link errors (don't use it; with MemorySanitizer). To get a reasonable performance add ``-O1`` or; higher. To get meaningful stack traces in error messages add; ``-fno-omit-frame-pointer``. To get perfect stack traces you may need; to disable inlining (just use ``-O1``) and tail call elimination; (``-fno-optimize-sibling-calls``). .. code-block:: console. % cat umr.cc; #include <stdio.h>. int main(int argc, char** argv) {; int* a = new int[10];; a[5] = 0;; if (a[argc]); printf(""xx\n"");; return 0;; }. % clang -fsanitize=memory -fno-omit-frame-pointer -g -O2 umr.cc. If a bug is detected, the program will print an error message to; stderr and exit with a non-zero exit code. .. code-block:: console. % ./a.out; WARNING: MemorySanitizer: use-of-uninitialized-value; #0 0x7f45944b418a in main umr.cc:6; #1 0x7f45938b676c in __libc_start_main libc-start.c:226. By default, MemorySanitizer exits on the first detected error. If you; find the error report hard to understand, try enabling; :ref:`origin tracking <msan-origins>`. ``__has_feature(memory_sanitizer)``; ------------------------------------. In some cases one may need to execute different code depending on; whether MemorySanitizer is enabled. :ref:`\_\_has\_feature; <langext-__has_feature-__has_extension>` can b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MemorySanitizer.rst:795,perform,performance,795,interpreter/llvm-project/clang/docs/MemorySanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MemorySanitizer.rst,2,"['optimiz', 'perform']","['optimize-sibling-calls', 'performance']"
Performance,"==================. * Use a high resolution timer, e.g. perf under linux. * Run the benchmark multiple times to be able to recognize noise. * Disable as many processes or services as possible on the target system. * Disable frequency scaling, turbo boost and address space; randomization (see OS specific section). * Static link if the OS supports it. That avoids any variation that; might be introduced by loading dynamic libraries. This can be done; by passing ``-DLLVM_BUILD_STATIC=ON`` to cmake. * Try to avoid storage. On some systems you can use tmpfs. Putting the; program, inputs and outputs on tmpfs avoids touching a real storage; system, which can have a pretty big variability. To mount it (on linux and freebsd at least)::. mount -t tmpfs -o size=<XX>g none dir_to_mount. Linux; =====. * Disable address space randomization::. echo 0 > /proc/sys/kernel/randomize_va_space. * Set scaling_governor to performance::. for i in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; do; echo performance > /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; done. * Use https://github.com/lpechacek/cpuset to reserve cpus for just the; program you are benchmarking. If using perf, leave at least 2 cores; so that perf runs in one and your program in another::. cset shield -c N1,N2 -k on. This will move all threads out of N1 and N2. The ``-k on`` means; that even kernel threads are moved out. * Disable the SMT pair of the cpus you will use for the benchmark. The; pair of cpu N can be found in; ``/sys/devices/system/cpu/cpuN/topology/thread_siblings_list`` and; disabled with::. echo 0 > /sys/devices/system/cpu/cpuX/online. * Run the program with::. cset shield --exec -- perf stat -r 10 <cmd>. This will run the command after ``--`` in the isolated cpus. The; particular perf command runs the ``<cmd>`` 10 times and reports; statistics. With these in place you can expect perf variations of less than 0.1%. Linux Intel; -----------. * Disable turbo mode::. echo 1 > /sys/devices/syst",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Benchmarking.rst:1451,perform,performance,1451,interpreter/llvm-project/llvm/docs/Benchmarking.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Benchmarking.rst,1,['perform'],['performance']
Performance,"==================. Clang supports additional attributes that are useful for documenting program; invariants and rules for static analysis tools, such as the `Clang Static; Analyzer <https://clang-analyzer.llvm.org/>`_. These attributes are documented; in the analyzer's `list of source-level annotations; <https://clang-analyzer.llvm.org/annotations.html>`_. Extensions for Dynamic Analysis; ===============================. Use ``__has_feature(address_sanitizer)`` to check if the code is being built; with :doc:`AddressSanitizer`. Use ``__has_feature(thread_sanitizer)`` to check if the code is being built; with :doc:`ThreadSanitizer`. Use ``__has_feature(memory_sanitizer)`` to check if the code is being built; with :doc:`MemorySanitizer`. Use ``__has_feature(dataflow_sanitizer)`` to check if the code is being built; with :doc:`DataFlowSanitizer`. Use ``__has_feature(safe_stack)`` to check if the code is being built; with :doc:`SafeStack`. Extensions for selectively disabling optimization; =================================================. Clang provides a mechanism for selectively disabling optimizations in functions; and methods. To disable optimizations in a single function definition, the GNU-style or C++11; non-standard attribute ``optnone`` can be used. .. code-block:: c++. // The following functions will not be optimized.; // GNU-style attribute; __attribute__((optnone)) int foo() {; // ... code; }; // C++11 attribute; [[clang::optnone]] int bar() {; // ... code; }. To facilitate disabling optimization for a range of function definitions, a; range-based pragma is provided. Its syntax is ``#pragma clang optimize``; followed by ``off`` or ``on``. All function definitions in the region between an ``off`` and the following; ``on`` will be decorated with the ``optnone`` attribute unless doing so would; conflict with explicit attributes already present on the function (e.g. the; ones that control inlining). .. code-block:: c++. #pragma clang optimize off; // This funct",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:158491,optimiz,optimization,158491,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimization']
Performance,"==================; Available Checkers; ==================. The analyzer performs checks that are categorized into families or ""checkers"". The default set of checkers covers a variety of checks targeted at finding security and API usage bugs,; dead code, and other logic errors. See the :ref:`default-checkers` checkers list below. In addition to these, the analyzer contains a number of :ref:`alpha-checkers` (aka *alpha* checkers).; These checkers are under development and are switched off by default. They may crash or emit a higher number of false positives. The :ref:`debug-checkers` package contains checkers for analyzer developers for debugging purposes. .. contents:: Table of Contents; :depth: 4. .. _default-checkers:. Default Checkers; ----------------. .. _core-checkers:. core; ^^^^; Models core language features and contains general-purpose checkers such as division by zero,; null pointer dereference, usage of uninitialized values, etc.; *These checkers must be always switched on as other checker rely on them.*. .. _core-BitwiseShift:. core.BitwiseShift (C, C++); """""""""""""""""""""""""""""""""""""""""""""""""""". Finds undefined behavior caused by the bitwise left- and right-shift operator; operating on integer types. By default, this checker only reports situations when the right operand is; either negative or larger than the bit width of the type of the left operand;; these are logically unsound. Moreover, if the pedantic mode is activated by; ``-analyzer-config core.BitwiseShift:Pedantic=true``, then this checker also; reports situations where the _left_ operand of a shift operator is negative or; overflow occurs during the right shift of a signed value. (Most compilers; handle these predictably, but the C standard and the C++ standards before C++20; say that they're undefined behavior. In the C++20 standard these constructs are; well-defined, so activating pedantic mode in C++20 has no effect.). **Examples**. .. code-block:: cpp. static_assert(sizeof(int) == 4, ""assuming 32-bit in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst:73,perform,performs,73,interpreter/llvm-project/clang/docs/analyzer/checkers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst,1,['perform'],['performs']
Performance,"==================; Matrix Types; ==================. .. contents::; :local:. .. _matrixtypes:. Clang provides a C/C++ language extension that allows users to directly express; fixed-size 2-dimensional matrices as language values and perform arithmetic on; them. This feature is currently experimental, and both its design and its; implementation are in flux. Draft Specification; ===================. Matrix Type; -----------. A matrix type is a scalar type with an underlying *element type*, a constant; number of *rows*, and a constant number of *columns*. Matrix types with the same; element type, rows, and columns are the same type. A value of a matrix type; includes storage for ``rows * columns`` values of the *element type*. The; internal layout, overall size and alignment are implementation-defined. The maximum of the product of the number of rows and columns is; implementation-defined. If that implementation-defined limit is exceeded, the; program is ill-formed. Currently, the element type of a matrix is only permitted to be one of the; following types:. * an integer type (as in C23 6.2.5p22), but excluding enumerated types and ``bool``; * the standard floating types ``float`` or ``double``; * a half-precision floating point type, if one is supported on the target. Other types may be supported in the future. Matrix Type Attribute; ---------------------. Matrix types can be declared by adding the ``matrix_type`` attribute to the; declaration of a *typedef* (or a C++ alias declaration). The underlying type; of the *typedef* must be a valid matrix element type. The; attribute takes two arguments, both of which must be integer constant; expressions that evaluate to a value greater than zero. The first specifies the; number of rows, and the second specifies the number of columns. The underlying; type of the *typedef* becomes a matrix type with the given dimensions and an; element type of the former underlying type. If a declaration of a *typedef-name* has a ``matrix_typ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MatrixTypes.rst:234,perform,perform,234,interpreter/llvm-project/clang/docs/MatrixTypes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MatrixTypes.rst,1,['perform'],['perform']
Performance,"==================; Vectorization Plan; ==================. .. contents::; :local:. Abstract; ========; The vectorization transformation can be rather complicated, involving several; potential alternatives, especially for outer-loops [1]_ but also possibly for; innermost loops. These alternatives may have significant performance impact,; both positive and negative. A cost model is therefore employed to identify the; best alternative, including the alternative of avoiding any transformation; altogether. The Vectorization Plan is an explicit model for describing vectorization; candidates. It serves for both optimizing candidates including estimating their; cost reliably, and for performing their final translation into IR. This; facilitates dealing with multiple vectorization candidates. High-level Design; =================. Vectorization Workflow; ----------------------; VPlan-based vectorization involves three major steps, taking a ""scenario-based; approach"" to vectorization planning:. 1. Legal Step: check if a loop can be legally vectorized; encode constraints and; artifacts if so.; 2. Plan Step:. a. Build initial VPlans following the constraints and decisions taken by; Legal Step 1, and compute their cost.; b. Apply optimizations to the VPlans, possibly forking additional VPlans.; Prune sub-optimal VPlans having relatively high cost.; 3. Execute Step: materialize the best VPlan. Note that this is the only step; that modifies the IR. Design Guidelines; -----------------; In what follows, the term ""input IR"" refers to code that is fed into the; vectorizer whereas the term ""output IR"" refers to code that is generated by the; vectorizer. The output IR contains code that has been vectorized or ""widened""; according to a loop Vectorization Factor (VF), and/or loop unroll-and-jammed; according to an Unroll Factor (UF).; The design of VPlan follows several high-level guidelines:. 1. Analysis-like: building and manipulating VPlans must not modify the input IR.; In particular,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/VectorizationPlan.rst:319,perform,performance,319,interpreter/llvm-project/llvm/docs/VectorizationPlan.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/VectorizationPlan.rst,3,"['optimiz', 'perform']","['optimizing', 'performance', 'performing']"
Performance,"=================== ================================. neg; ~~~. See a description :ref:`here<amdgpu_synid_neg>`. .. _amdgpu_synid_wait_exp:. wait_exp; ~~~~~~~~. Specifies a wait on the EXP counter before issuing the current instruction.; The counter must be less than or equal to this value before the instruction is issued.; If set to 7, no wait is performed. The default value is zero. This is a safe value, but it may be suboptimal. ================ ======================================================; Syntax Description; ================ ======================================================; wait_exp:{0..7} An additional wait on the EXP counter before; issuing this instruction.; ================ ======================================================. .. _amdgpu_synid_wait_vdst:. wait_vdst; ~~~~~~~~~. Specifies a wait on the VA_VDST counter before issuing the current instruction.; The counter must be less than or equal to this value before the instruction is issued.; If set to 15, no wait is performed. The default value is zero. This is a safe value, but it may be suboptimal. ================== ======================================================; Syntax Description; ================== ======================================================; wait_vdst:{0..15} An additional wait on the VA_VDST counter before; issuing this instruction.; ================== ======================================================. DPP8 Modifiers; --------------. .. _amdgpu_synid_dpp8_sel:. dpp8_sel; ~~~~~~~~. Selects which lanes to pull data from, within a group of 8 lanes. This is a mandatory modifier.; There is no default value. The *dpp8_sel* modifier must specify exactly 8 values.; The first value selects which lane to read from to supply data into lane 0.; The second value controls lane 1 and so on. Each value may be specified as either; an :ref:`integer number<amdgpu_synid_integer_number>` or; an :ref:`absolute expression<amdgpu_synid_absolute_expression>`. =======================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst:36634,perform,performed,36634,interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,1,['perform'],['performed']
Performance,"=================== =========================================================; Syntax Description; =============================== =========================================================; dim:SQ_RSRC_IMG_1D One-dimensional image.; dim:SQ_RSRC_IMG_2D Two-dimensional image.; dim:SQ_RSRC_IMG_3D Three-dimensional image.; dim:SQ_RSRC_IMG_CUBE Cubemap array.; dim:SQ_RSRC_IMG_1D_ARRAY One-dimensional image array.; dim:SQ_RSRC_IMG_2D_ARRAY Two-dimensional image array.; dim:SQ_RSRC_IMG_2D_MSAA Two-dimensional multi-sample auto-aliasing image.; dim:SQ_RSRC_IMG_2D_MSAA_ARRAY Two-dimensional multi-sample auto-aliasing image array.; =============================== =========================================================. dlc; ~~~. See a description :ref:`here<amdgpu_synid_dlc>`. Miscellaneous Modifiers; -----------------------. .. _amdgpu_synid_dlc:. dlc; ~~~. Controls device level cache policy for memory operations. Used for synchronization.; When specified, forces operation to bypass device level cache, making the operation device; level coherent. By default, instructions use device level cache. ======================================== ================================================; Syntax Description; ======================================== ================================================; dlc Bypass device level cache.; ======================================== ================================================. .. _amdgpu_synid_glc:. glc; ~~~. For atomic opcodes, this modifier indicates that the instruction returns the value from memory; before the operation. For other opcodes, it is used together with :ref:`slc<amdgpu_synid_slc>`; to specify cache policy. The default value is off (0). ======================================== ================================================; Syntax Description; ======================================== ================================================; glc Set glc bit to 1.; ======================================== =========================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst:17176,cache,cache,17176,interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,1,['cache'],['cache']
Performance,"===================; HLSL Resource Types; ===================. .. contents::; :local:. Introduction; ============. HLSL Resources are runtime-bound data that is provided as input, output or both; to shader programs written in HLSL. Resource Types in HLSL provide key user; abstractions for reading and writing resource data. Implementation Details; ======================. In Clang resource types are forward declared by the ``HLSLExternalSemaSource``; on initialization. They are then lazily completed when ``requiresCompleteType``; is called later in Sema. Resource types are templated class declarations. The template parameter; specifies the expected return type of resource loads, and the expected parameter; type for stores. In Clang's AST and code generation, resource types are classes that store a; pointer of the template parameter type. The pointer is populated from a call to; ``__builtin_hlsl_create_handle``, and treated as a pointer to an array of typed; data through until lowering in the backend. Resource types are annotated with the ``HLSLResource`` attribute, which drives; code generation for resource binding metadata. The ``hlsl`` metadata nodes are; transformed in the backend to the binding information expected by the target; runtime.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/ResourceTypes.rst:679,load,loads,679,interpreter/llvm-project/clang/docs/HLSL/ResourceTypes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/ResourceTypes.rst,1,['load'],['loads']
Performance,"===================; Misexpect; ===================; .. contents::. .. toctree::; :maxdepth: 1. When developers use ``llvm.expect`` intrinsics, i.e., through use of; ``__builtin_expect(...)``, they are trying to communicate how their code is; expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect diagnostics are intended to help developers identify and address; these situations, by comparing the branch weights added by the ``llvm.expect``; intrinsic to those collected through profiling. Whenever these values are; mismatched, a diagnostic is surfaced to the user. Details on how the checks; operate in the LLVM backed can be found in LLVM's documentation. By default MisExpect checking is quite strict, because the use of the; ``llvm.expect`` intrinsic is designed for specialized cases, where the outcome; of a condition is severely skewed. As a result, the optimizer can be extremely; aggressive, which can result in performance degradation if the outcome is less; predictable than the annotation suggests. Even when the annotation is correct; 90% of the time, it may be beneficial to either remove the annotation or to use; a different intrinsic that can communicate the probability more directly. Because this may be too strict, MisExpect diagnostics are not enabled by; default, and support an additional flag to tolerate some deviation from the; exact thresholds. The ``-fdiagnostic-misexpect-tolerance=N`` accepts; deviations when comparing branch weights within ``N%`` of the expected values.; So passing ``-fdiagnostic-misexpect-tolerance=5`` will not report diagnosti",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MisExpect.rst:280,optimiz,optimizer,280,interpreter/llvm-project/clang/docs/MisExpect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/MisExpect.rst,2,['optimiz'],['optimizer']
Performance,"===================; Misexpect; ===================; .. contents::. .. toctree::; :maxdepth: 1. When developers use ``llvm.expect`` intrinsics, i.e., through use of; ``__builtin_expect(...)``, they are trying to communicate how their code is; expected to behave at runtime to the optimizer. These annotations, however, can; be incorrect for a variety of reasons: changes to the code base invalidate them; silently, the developer mis-annotated them (e.g., using ``LIKELY`` instead of; ``UNLIKELY``), or perhaps they assumed something incorrectly when they wrote; the annotation. Regardless of why, it is useful to detect these situations so; that the optimizer can make more useful decisions about the code. MisExpect; diagnostics are intended to help developers identify and address these; situations, by comparing the use of the ``llvm.expect`` intrinsic to the ground; truth provided by a profiling input. The MisExpect checks in the LLVM backend follow a simple procedure: if there is; a mismatch between the branch weights collected during profiling and those; supplied by an ``llvm.expect`` intrinsic, then it will emit a diagnostic; message to the user. The most natural place to perform the verification is just prior to when; branch weights are assigned to the target instruction in the form of; branch weight metadata. There are 3 key places in the LLVM backend where branch weights are; created and assigned based on profiling information or the use of the; ``llvm.expect`` intrinsic, and our implementation focuses on these; places to perform the verification. We calculate the threshold for emitting MisExpect related diagnostics; based on the values the compiler assigns to ``llvm.expect`` intrinsics,; which can be set through the ``-likely-branch-weight`` and; ``-unlikely-branch-weight`` LLVM options. During verification, if the; profile weights mismatch the calculated threshold, then we will emit a; remark or warning detailing a potential performance regression. The; diagnostic al",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst:280,optimiz,optimizer,280,interpreter/llvm-project/llvm/docs/MisExpect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MisExpect.rst,2,['optimiz'],['optimizer']
Performance,"====================; Constant Interpreter; ====================. .. contents::; :local:. Introduction; ============. The constexpr interpreter aims to replace the existing tree evaluator in; clang, improving performance on constructs which are executed inefficiently; by the evaluator. The interpreter is activated using the following flags:. * ``-fexperimental-new-constant-interpreter`` enables the interpreter,; emitting an error if an unsupported feature is encountered. Bytecode Compilation; ====================. Bytecode compilation is handled in ``ByteCodeStmtGen.h`` for statements; and ``ByteCodeExprGen.h`` for expressions. The compiler has two different; backends: one to generate bytecode for functions (``ByteCodeEmitter``) and; one to directly evaluate expressions as they are compiled, without; generating bytecode (``EvalEmitter``). All functions are compiled to; bytecode, while toplevel expressions used in constant contexts are directly; evaluated since the bytecode would never be reused. This mechanism aims to; pave the way towards replacing the evaluator, improving its performance on; functions and loops, while being just as fast on single-use toplevel; expressions. The interpreter relies on stack-based, strongly-typed opcodes. The glue; logic between the code generator, along with the enumeration and; description of opcodes, can be found in ``Opcodes.td``. The opcodes are; implemented as generic template methods in ``Interp.h`` and instantiated; with the relevant primitive types by the interpreter loop or by the; evaluating emitter. Primitive Types; ---------------. * ``PT_{U|S}int{8|16|32|64}``. Signed or unsigned integers of a specific bit width, implemented using; the ```Integral``` type. * ``PT_{U|S}intFP``. Signed or unsigned integers of an arbitrary, but fixed width used to; implement integral types which are required by the target, but are not; supported by the host. Under the hood, they rely on APValue. The; ``Integral`` specialisation for these typ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ConstantInterpreter.rst:209,perform,performance,209,interpreter/llvm-project/clang/docs/ConstantInterpreter.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ConstantInterpreter.rst,1,['perform'],['performance']
Performance,"====================; The LLVM gold plugin; ====================. Introduction; ============. Building with link time optimization requires cooperation from; the system linker. LTO support on Linux systems is available via the; `gold linker`_ which supports LTO via plugins. This is the same mechanism; used by the `GCC LTO`_ project. The LLVM gold plugin implements the gold plugin interface on top of; :ref:`libLTO`. The same plugin can also be used by other tools such as; ``ar`` and ``nm``. Note that ld.bfd from binutils version 2.21.51.0.2; and above also supports LTO via plugins. However, usage of the LLVM; gold plugin with ld.bfd is not tested and therefore not officially; supported or recommended. As of LLVM 15, the gold plugin will ignore bitcode from the ``.llvmbc``; section inside of ELF object files. However, LTO with bitcode files; is still supported. .. _`gold linker`: http://sourceware.org/binutils; .. _`GCC LTO`: http://gcc.gnu.org/wiki/LinkTimeOptimization; .. _`gold plugin interface`: http://gcc.gnu.org/wiki/whopr/driver. .. _lto-how-to-build:. How to build it; ===============. You need to have gold with plugin support and build the LLVMgold plugin.; The gold linker is installed as ld.gold. To see whether gold is the default; on your system, run ``/usr/bin/ld -v``. It will report ""GNU; gold"" or else ""GNU ld"" if not. If gold is already installed at; ``/usr/bin/ld.gold``, one option is to simply make that the default by; backing up your existing ``/usr/bin/ld`` and creating a symbolic link; with ``ln -s /usr/bin/ld.gold /usr/bin/ld``. Alternatively, you can build; with clang's ``-fuse-ld=gold`` or add ``-fuse-ld=gold`` to LDFLAGS, which will; cause the clang driver to invoke ``/usr/bin/ld.gold`` directly. If you have gold installed, check for plugin support by running; ``/usr/bin/ld.gold -plugin``. If it complains ""missing argument"" then; you have plugin support. If not, and you get an error such as ""unknown option"",; then you will either need to build gol",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GoldPlugin.rst:118,optimiz,optimization,118,interpreter/llvm-project/llvm/docs/GoldPlugin.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GoldPlugin.rst,1,['optimiz'],['optimization']
Performance,"====================; Writing an LLVM Pass; ====================. .. program:: opt. .. contents::; :local:. Introduction --- What is a pass?; ================================. The LLVM Pass Framework is an important part of the LLVM system, because LLVM; passes are where most of the interesting parts of the compiler exist. Passes; perform the transformations and optimizations that make up the compiler, they; build the analysis results that are used by these transformations, and they; are, above all, a structuring technique for compiler code. All LLVM passes are subclasses of the `Pass; <https://llvm.org/doxygen/classllvm_1_1Pass.html>`_ class, which implement; functionality by overriding virtual methods inherited from ``Pass``. Depending; on how your pass works, you should inherit from the :ref:`ModulePass; <writing-an-llvm-pass-ModulePass>` , :ref:`CallGraphSCCPass; <writing-an-llvm-pass-CallGraphSCCPass>`, :ref:`FunctionPass; <writing-an-llvm-pass-FunctionPass>` , or :ref:`LoopPass; <writing-an-llvm-pass-LoopPass>`, or :ref:`RegionPass; <writing-an-llvm-pass-RegionPass>` classes, which gives the system more; information about what your pass does, and how it can be combined with other; passes. One of the main features of the LLVM Pass Framework is that it; schedules passes to run in an efficient way based on the constraints that your; pass meets (which are indicated by which class they derive from). We start by showing you how to construct a pass, everything from setting up the; code, to compiling, loading, and executing it. After the basics are down, more; advanced features are discussed. .. warning::; This document deals with the legacy pass manager. LLVM uses the new pass; manager for the optimization pipeline (the codegen pipeline; still uses the legacy pass manager), which has its own way of defining; passes. For more details, see :doc:`WritingAnLLVMNewPMPass` and; :doc:`NewPassManager`. Quick Start --- Writing hello world; ===================================. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:333,perform,perform,333,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,2,"['optimiz', 'perform']","['optimizations', 'perform']"
Performance,"====================; Writing an LLVM Pass; ====================. .. program:: opt. .. contents::; :local:. Introduction --- What is a pass?; ================================. The LLVM pass framework is an important part of the LLVM system, because LLVM; passes are where most of the interesting parts of the compiler exist. Passes; perform the transformations and optimizations that make up the compiler, they; build the analysis results that are used by these transformations, and they; are, above all, a structuring technique for compiler code. Unlike passes under the legacy pass manager where the pass interface is; defined via inheritance, passes under the new pass manager rely on; concept-based polymorphism, meaning there is no explicit interface (see; comments in ``PassManager.h`` for more details). All LLVM passes inherit from; the CRTP mix-in ``PassInfoMixin<PassT>``. The pass should have a ``run()``; method which returns a ``PreservedAnalyses`` and takes in some unit of IR; along with an analysis manager. For example, a function pass would have a; ``PreservedAnalyses run(Function &F, FunctionAnalysisManager &AM);`` method. We start by showing you how to construct a pass, from setting up the build,; creating the pass, to executing and testing it. Looking at existing passes is; always a great way to learn details. .. warning::; This document deals with the new pass manager. LLVM uses the legacy pass; manager for the codegen pipeline. For more details, see; :doc:`WritingAnLLVMPass` and :doc:`NewPassManager`. Quick Start --- Writing hello world; ===================================. Here we describe how to write the ""hello world"" of passes. The ""HelloWorld""; pass is designed to simply print out the name of non-external functions that; exist in the program being compiled. It does not modify the program at all,; it just inspects it. The code below already exists; feel free to create a pass with a different; name alongside the HelloWorld source files. .. _writing-an-llvm-",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMNewPMPass.rst:333,perform,perform,333,interpreter/llvm-project/llvm/docs/WritingAnLLVMNewPMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMNewPMPass.rst,2,"['optimiz', 'perform']","['optimizations', 'perform']"
Performance,"=====================; Clang Offload Bundler; =====================. .. contents::; :local:. .. _clang-offload-bundler:. Introduction; ============. For heterogeneous single source programming languages, use one or more; ``--offload-arch=<target-id>`` Clang options to specify the target IDs of the; code to generate for the offload code regions. The tool chain may perform multiple compilations of a translation unit to; produce separate code objects for the host and potentially multiple offloaded; devices. The ``clang-offload-bundler`` tool may be used as part of the tool; chain to combine these multiple code objects into a single bundled code object. The tool chain may use a bundled code object as an intermediate step so that; each tool chain step consumes and produces a single file as in traditional; non-heterogeneous tool chains. The bundled code object contains the code objects; for the host and all the offload devices. A bundled code object may also be used to bundle just the offloaded code; objects, and embedded as data into the host code object. The host compilation; includes an ``init`` function that will use the runtime corresponding to the; offload kind (see :ref:`clang-offload-kind-table`) to load the offload code; objects appropriate to the devices present when the host program is executed. :program:`clang-offload-bundler` is located in; `clang/tools/clang-offload-bundler`. .. code-block:: console. $ clang-offload-bundler -help; OVERVIEW: A tool to bundle several input files of the specified type <type>; referring to the same source file but different targets into a single; one. The resulting file can also be unbundled into different files by; this tool if -unbundle is provided. USAGE: clang-offload-bundler [options]. OPTIONS:. Generic Options:. --help - Display available options (--help-hidden for more); --help-list - Display list of available options (--help-list-hidden for more); --version - Display the version of this program. clang-offload-bundler opti",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst:366,perform,perform,366,interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,1,['perform'],['perform']
Performance,"====================== ============== ========= ================================. .. Kernel Dispatch; ~~~~~~~~~~~~~~~. The HSA architected queuing language (AQL) defines a user space memory interface; that can be used to control the dispatch of kernels, in an agent independent; way. An agent can have zero or more AQL queues created for it using an HSA; compatible runtime (see :ref:`amdgpu-os`), in which AQL packets (all of which; are 64 bytes) can be placed. See the *HSA Platform System Architecture; Specification* [HSA]_ for the AQL queue mechanics and packet layouts. The packet processor of a kernel agent is responsible for detecting and; dispatching HSA kernels from the AQL queues associated with it. For AMD GPUs the; packet processor is implemented by the hardware command processor (CP),; asynchronous dispatch controller (ADC) and shader processor input controller; (SPI). An HSA compatible runtime can be used to allocate an AQL queue object. It uses; the kernel mode driver to initialize and register the AQL queue with CP. To dispatch a kernel the following actions are performed. This can occur in the; CPU host program, or from an HSA kernel executing on a GPU. 1. A pointer to an AQL queue for the kernel agent on which the kernel is to be; executed is obtained.; 2. A pointer to the kernel descriptor (see; :ref:`amdgpu-amdhsa-kernel-descriptor`) of the kernel to execute is obtained.; It must be for a kernel that is contained in a code object that was loaded; by an HSA compatible runtime on the kernel agent with which the AQL queue is; associated.; 3. Space is allocated for the kernel arguments using the HSA compatible runtime; allocator for a memory region with the kernarg property for the kernel agent; that will execute the kernel. It must be at least 16-byte aligned.; 4. Kernel argument values are assigned to the kernel argument memory; allocation. The layout is defined in the *HSA Programmer's Language; Reference* [HSA]_. For AMDGPU the kernel execution directl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:149761,queue,queue,149761,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"======================. In the example above, every variable assignment uniquely corresponds to a; memory store to the variable's position on the stack. However in heavily; optimized code LLVM promotes most variables into SSA values, which can; eventually be placed in physical registers or memory locations. To track SSA; values through compilation, when objects are promoted to SSA values an; ``llvm.dbg.value`` intrinsic is created for each assignment, recording the; variable's new location. Compared with the ``llvm.dbg.declare`` intrinsic:. * A dbg.value terminates the effect of any preceding dbg.values for (any; overlapping fragments of) the specified variable.; * The dbg.value's position in the IR defines where in the instruction stream; the variable's value changes.; * Operands can be constants, indicating the variable is assigned a; constant value. Care must be taken to update ``llvm.dbg.value`` intrinsics when optimization; passes alter or move instructions and blocks -- the developer could observe such; changes reflected in the value of variables when debugging the program. For any; execution of the optimized program, the set of variable values presented to the; developer by the debugger should not show a state that would never have existed; in the execution of the unoptimized program, given the same input. Doing so; risks misleading the developer by reporting a state that does not exist,; damaging their understanding of the optimized program and undermining their; trust in the debugger. Sometimes perfectly preserving variable locations is not possible, often when a; redundant calculation is optimized out. In such cases, a ``llvm.dbg.value``; with operand ``poison`` should be used, to terminate earlier variable locations; and let the debugger present ``optimized out`` to the developer. Withholding; these potentially stale variable values from the developer diminishes the; amount of available debug information, but increases the reliability of the; remaining inf",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:18768,optimiz,optimization,18768,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimization']
Performance,"======================. The inputs for a fuzz target are generated via random mutations of a; :ref:`corpus <libfuzzer-corpus>`. There are a few options for the kinds of; mutations that a fuzzer in LLVM might want. .. _fuzzing-llvm-generic:. Generic Random Fuzzing; ----------------------. The most basic form of input mutation is to use the built in mutators of; LibFuzzer. These simply treat the input corpus as a bag of bits and make random; mutations. This type of fuzzer is good for stressing the surface layers of a; program, and is good at testing things like lexers, parsers, or binary; protocols. Some of the in-tree fuzzers that use this type of mutator are `clang-fuzzer`_,; `clang-format-fuzzer`_, `llvm-as-fuzzer`_, `llvm-dwarfdump-fuzzer`_,; `llvm-mc-assemble-fuzzer`_, and `llvm-mc-disassemble-fuzzer`_. .. _fuzzing-llvm-protobuf:. Structured Fuzzing using ``libprotobuf-mutator``; ------------------------------------------------. We can use libprotobuf-mutator_ in order to perform structured fuzzing and; stress deeper layers of programs. This works by defining a protobuf class that; translates arbitrary data into structurally interesting input. Specifically, we; use this to work with a subset of the C++ language and perform mutations that; produce valid C++ programs in order to exercise parts of clang that are more; interesting than parser error handling. To build this kind of fuzzer you need `protobuf`_ and its dependencies; installed, and you need to specify some extra flags when configuring the build; with :doc:`CMake <CMake>`. For example, `clang-proto-fuzzer`_ can be enabled by; adding ``-DCLANG_ENABLE_PROTO_FUZZER=ON`` to the flags described in; :ref:`building-fuzzers`. The only in-tree fuzzer that uses ``libprotobuf-mutator`` today is; `clang-proto-fuzzer`_. .. _libprotobuf-mutator: https://github.com/google/libprotobuf-mutator; .. _protobuf: https://github.com/google/protobuf. .. _fuzzing-llvm-ir:. Structured Fuzzing of LLVM IR; ---------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst:6502,perform,perform,6502,interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,1,['perform'],['perform']
Performance,"======================; Control Flow Integrity; ======================. .. toctree::; :hidden:. ControlFlowIntegrityDesign. .. contents::; :local:. Introduction; ============. Clang includes an implementation of a number of control flow integrity (CFI); schemes, which are designed to abort the program upon detecting certain forms; of undefined behavior that can potentially allow attackers to subvert the; program's control flow. These schemes have been optimized for performance,; allowing developers to enable them in release builds. To enable Clang's available CFI schemes, use the flag ``-fsanitize=cfi``.; You can also enable a subset of available :ref:`schemes <cfi-schemes>`.; As currently implemented, all schemes rely on link-time optimization (LTO);; so it is required to specify ``-flto``, and the linker used must support LTO,; for example via the `gold plugin`_. To allow the checks to be implemented efficiently, the program must; be structured such that certain object files are compiled with CFI; enabled, and are statically linked into the program. This may preclude; the use of shared libraries in some cases. The compiler will only produce CFI checks for a class if it can infer hidden; LTO visibility for that class. LTO visibility is a property of a class that; is inferred from flags and attributes. For more details, see the documentation; for :doc:`LTO visibility <LTOVisibility>`. The ``-fsanitize=cfi-{vcall,nvcall,derived-cast,unrelated-cast}`` flags; require that a ``-fvisibility=`` flag also be specified. This is because the; default visibility setting is ``-fvisibility=default``, which would disable; CFI checks for classes without visibility attributes. Most users will want; to specify ``-fvisibility=hidden``, which enables CFI checks for such classes. Experimental support for :ref:`cross-DSO control flow integrity; <cfi-cross-dso>` exists that does not require classes to have hidden LTO; visibility. This cross-DSO support has unstable ABI at this time. .. _g",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst:456,optimiz,optimized,456,interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,3,"['optimiz', 'perform']","['optimization', 'optimized', 'performance']"
Performance,"========================; Advice on Packaging LLVM; ========================. .. contents::; :local:. Overview; ========. LLVM sets certain default configure options to make sure our developers don't; break things for constrained platforms. These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM class",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:678,optimiz,optimized,678,interpreter/llvm-project/llvm/docs/Packaging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst,2,['optimiz'],['optimized']
Performance,"========================; Debugging C++ Coroutines; ========================. .. contents::; :local:. Introduction; ============. For performance and other architectural reasons, the C++ Coroutines feature in; the Clang compiler is implemented in two parts of the compiler. Semantic; analysis is performed in Clang, and Coroutine construction and optimization; takes place in the LLVM middle-end. However, this design forces us to generate insufficient debugging information.; Typically, the compiler generates debug information in the Clang frontend, as; debug information is highly language specific. However, this is not possible; for Coroutine frames because the frames are constructed in the LLVM middle-end. To mitigate this problem, the LLVM middle end attempts to generate some debug; information, which is unfortunately incomplete, since much of the language; specific information is missing in the middle end. This document describes how to use this debug information to better debug; coroutines. Terminology; ===========. Due to the recent nature of C++20 Coroutines, the terminology used to describe; the concepts of Coroutines is not settled. This section defines a common,; understandable terminology to be used consistently throughout this document. coroutine type; --------------. A `coroutine function` is any function that contains any of the Coroutine; Keywords `co_await`, `co_yield`, or `co_return`. A `coroutine type` is a; possible return type of one of these `coroutine functions`. `Task` and; `Generator` are commonly referred to coroutine types. coroutine; ---------. By technical definition, a `coroutine` is a suspendable function. However,; programmers typically use `coroutine` to refer to an individual instance.; For example:. .. code-block:: c++. std::vector<Task> Coros; // Task is a coroutine type.; for (int i = 0; i < 3; i++); Coros.push_back(CoroTask()); // CoroTask is a coroutine function, which; // would return a coroutine type 'Task'. In practice, we typical",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst:134,perform,performance,134,interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,3,"['optimiz', 'perform']","['optimization', 'performance', 'performed']"
Performance,========================; Many Tests lit Example; ========================. This directory contains a trivial lit test suite configuration that defines a; custom test format which just generates a large (N=10000) number of tests that; do a small amount of work in the Python test execution code. This test suite is useful for testing the performance of lit on large numbers of; tests.; ,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt:338,perform,performance,338,interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt,2,['perform'],['performance']
Performance,"========================; Scudo Hardened Allocator; ========================. .. contents::; :local:; :depth: 2. Introduction; ============. The Scudo Hardened Allocator is a user-mode allocator, originally based on LLVM; Sanitizers'; `CombinedAllocator <https://github.com/llvm/llvm-project/blob/main/compiler-rt/lib/sanitizer_common/sanitizer_allocator_combined.h>`_.; It aims at providing additional mitigation against heap based vulnerabilities,; while maintaining good performance. Scudo is currently the default allocator in; `Fuchsia <https://fuchsia.dev/>`_, and in `Android <https://www.android.com/>`_; since Android 11. The name ""Scudo"" comes from the Italian word for; `shield <https://www.collinsdictionary.com/dictionary/italian-english/scudo>`_; (and Escudo in Spanish). Design; ======. Allocator; ---------; Scudo was designed with security in mind, but aims at striking a good balance; between security and performance. It was designed to be highly tunable and; configurable, and while we provide some default configurations, we encourage; consumers to come up with the parameters that will work best for their use; cases. The allocator combines several components that serve distinct purposes:. - the Primary allocator: fast and efficient, it services smaller allocation; sizes by carving reserved memory regions into blocks of identical size. There; are currently two Primary allocators implemented, specific to 32 and 64 bit; architectures. It is configurable via compile time options. - the Secondary allocator: slower, it services larger allocation sizes via the; memory mapping primitives of the underlying operating system. Secondary backed; allocations are surrounded by Guard Pages. It is also configurable via compile; time options. - the thread specific data Registry: defines how local caches operate for each; thread. There are currently two models implemented: the exclusive model where; each thread holds its own caches (using the ELF TLS); or the shared model; where t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst:474,perform,performance,474,interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,2,['perform'],['performance']
Performance,"=========================. .. contents::; :local:. Introduction; ============. This document aims to provide a high-level overview of the design and; implementation of the ORC JIT APIs. Except where otherwise stated all discussion; refers to the modern ORCv2 APIs (available since LLVM 7). Clients wishing to; transition from OrcV1 should see Section :ref:`transitioning_orcv1_to_orcv2`. Use-cases; =========. ORC provides a modular API for building JIT compilers. There are a number; of use cases for such an API. For example:. 1. The LLVM tutorials use a simple ORC-based JIT class to execute expressions; compiled from a toy language: Kaleidoscope. 2. The LLVM debugger, LLDB, uses a cross-compiling JIT for expression; evaluation. In this use case, cross compilation allows expressions compiled; in the debugger process to be executed on the debug target process, which may; be on a different device/architecture. 3. In high-performance JITs (e.g. JVMs, Julia) that want to make use of LLVM's; optimizations within an existing JIT infrastructure. 4. In interpreters and REPLs, e.g. Cling (C++) and the Swift interpreter. By adopting a modular, library-based design we aim to make ORC useful in as many; of these contexts as possible. Features; ========. ORC provides the following features:. **JIT-linking**; ORC provides APIs to link relocatable object files (COFF, ELF, MachO) [1]_; into a target process at runtime. The target process may be the same process; that contains the JIT session object and jit-linker, or may be another process; (even one running on a different machine or architecture) that communicates; with the JIT via RPC. **LLVM IR compilation**; ORC provides off the shelf components (IRCompileLayer, SimpleCompiler,; ConcurrentIRCompiler) that make it easy to add LLVM IR to a JIT'd process. **Eager and lazy compilation**; By default, ORC will compile symbols as soon as they are looked up in the JIT; session object (``ExecutionSession``). Compiling eagerly by default make",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:1068,optimiz,optimizations,1068,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['optimiz'],['optimizations']
Performance,"=========================; Dependence Graphs in LLVM; =========================. .. contents::; :local:. Introduction; ============; Dependence graphs are useful tools in compilers for analyzing relationships; between various program elements to help guide optimizations. The ideas; behind these graphs are described in papers [1]_ and [2]_. The implementation of these ideas in LLVM may be slightly different than; what is mentioned in the papers. These differences are documented in; the `implementation details <implementation-details_>`_. .. _DataDependenceGraph:. Data Dependence Graph; =====================; In its simplest form the Data Dependence Graph (or DDG) represents data; dependencies between individual instructions. Each node in such a graph; represents a single instruction and is referred to as an ""atomic"" node.; It is also possible to combine some atomic nodes that have a simple; def-use dependency between them into larger nodes that contain multiple-; instructions. As described in [1]_ the DDG uses graph abstraction to group nodes; that are part of a strongly connected component of the graph; into special nodes called pi-blocks. pi-blocks represent cycles of data; dependency that prevent reordering transformations. Since any strongly; connected component of the graph is a maximal subgraph of all the nodes; that form a cycle, pi-blocks are at most one level deep. In other words,; no pi-blocks are nested inside another pi-block, resulting in a; hierarchical representation that is at most one level deep. For example, consider the following:. .. code-block:: c++. for (int i = 1; i < n; i++) {; b[i] = c[i] + b[i-1];; }. This code contains a statement that has a loop carried dependence on; itself creating a cycle in the DDG. The figure below illustrates; how the cycle of dependency is carried through multiple def-use relations; and a memory access dependency. .. image:: cycle.png. The DDG corresponding to this example would have a pi-block that contains; all the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/DependenceGraphs/index.rst:257,optimiz,optimizations,257,interpreter/llvm-project/llvm/docs/DependenceGraphs/index.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/DependenceGraphs/index.rst,1,['optimiz'],['optimizations']
Performance,"==========================; Auto-Vectorization in LLVM; ==========================. .. contents::; :local:. LLVM has two vectorizers: The :ref:`Loop Vectorizer <loop-vectorizer>`,; which operates on Loops, and the :ref:`SLP Vectorizer; <slp-vectorizer>`. These vectorizers; focus on different optimization opportunities and use different techniques.; The SLP vectorizer merges multiple scalars that are found in the code into; vectors while the Loop Vectorizer widens instructions in loops; to operate on multiple consecutive iterations. Both the Loop Vectorizer and the SLP Vectorizer are enabled by default. .. _loop-vectorizer:. The Loop Vectorizer; ===================. Usage; -----. The Loop Vectorizer is enabled by default, but it can be disabled; through clang using the command line flag:. .. code-block:: console. $ clang ... -fno-vectorize file.c. Command line flags; ^^^^^^^^^^^^^^^^^^. The loop vectorizer uses a cost model to decide on the optimal vectorization factor; and unroll factor. However, users of the vectorizer can force the vectorizer to use; specific values. Both 'clang' and 'opt' support the flags below. Users can control the vectorization SIMD width using the command line flag ""-force-vector-width"". .. code-block:: console. $ clang -mllvm -force-vector-width=8 ...; $ opt -loop-vectorize -force-vector-width=8 ... Users can control the unroll factor using the command line flag ""-force-vector-interleave"". .. code-block:: console. $ clang -mllvm -force-vector-interleave=2 ...; $ opt -loop-vectorize -force-vector-interleave=2 ... Pragma loop hint directives; ^^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``#pragma clang loop`` directive allows loop vectorization hints to be; specified for the subsequent for, while, do-while, or c++11 range-based for; loop. The directive allows vectorization and interleaving to be enabled or; disabled. Vector width as well as interleave count can also be manually; specified. The following example explicitly enables vectorization and; interl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst:293,optimiz,optimization,293,interpreter/llvm-project/llvm/docs/Vectorizers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Vectorizers.rst,1,['optimiz'],['optimization']
Performance,"==========================; Exception Handling in LLVM; ==========================. .. contents::; :local:. Introduction; ============. This document is the central repository for all information pertaining to; exception handling in LLVM. It describes the format that LLVM exception; handling information takes, which is useful for those interested in creating; front-ends or dealing directly with the information. Further, this document; provides specific examples of what exception handling information is used for in; C and C++. Itanium ABI Zero-cost Exception Handling; ----------------------------------------. Exception handling for most programming languages is designed to recover from; conditions that rarely occur during general use of an application. To that end,; exception handling should not interfere with the main flow of an application's; algorithm by performing checkpointing tasks, such as saving the current pc or; register state. The Itanium ABI Exception Handling Specification defines a methodology for; providing outlying data in the form of exception tables without inlining; speculative exception handling code in the flow of an application's main; algorithm. Thus, the specification is said to add ""zero-cost"" to the normal; execution of an application. A more complete description of the Itanium ABI exception handling runtime; support of can be found at `Itanium C++ ABI: Exception Handling; <http://itanium-cxx-abi.github.io/cxx-abi/abi-eh.html>`_. A description of the; exception frame format can be found at `Exception Frames; <http://refspecs.linuxfoundation.org/LSB_3.0.0/LSB-Core-generic/LSB-Core-generic/ehframechpt.html>`_,; with details of the DWARF 4 specification at `DWARF 4 Standard; <http://dwarfstd.org/Dwarf4Std.php>`_. A description for the C++ exception; table formats can be found at `Exception Handling Tables; <http://itanium-cxx-abi.github.io/cxx-abi/exceptions.pdf>`_. Setjmp/Longjmp Exception Handling; ---------------------------------. Setjmp/Lon",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExceptionHandling.rst:869,perform,performing,869,interpreter/llvm-project/llvm/docs/ExceptionHandling.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExceptionHandling.rst,1,['perform'],['performing']
Performance,"==========================; Offload Kind Description; ============= ==============================================================; host Host code object. ``clang-offload-bundler`` always includes; this entry as the first bundled code object entry. For an; embedded bundled code object this entry is not used by the; runtime and so is generally an empty code object. hip Offload code object for the HIP language. Used for all; HIP language offload code objects when the; ``clang-offload-bundler`` is used to bundle code objects as; intermediate steps of the tool chain. Also used for AMD GPU; code objects before ABI version V4 when the; ``clang-offload-bundler`` is used to create a *fat binary*; to be loaded by the HIP runtime. The fat binary can be; loaded directly from a file, or be embedded in the host code; object as a data section with the name ``.hip_fatbin``. hipv4 Offload code object for the HIP language. Used for AMD GPU; code objects with at least ABI version V4 when the; ``clang-offload-bundler`` is used to create a *fat binary*; to be loaded by the HIP runtime. The fat binary can be; loaded directly from a file, or be embedded in the host code; object as a data section with the name ``.hip_fatbin``. openmp Offload code object for the OpenMP language extension.; ============= ==============================================================. **target-triple**; The target triple of the code object. See `Target Triple; <https://clang.llvm.org/docs/CrossCompilation.html#target-triple>`_. The bundler accepts target triples with or without the optional environment; field:. ``<arch><sub>-<vendor>-<sys>``, or; ``<arch><sub>-<vendor>-<sys>-<env>``. However, in order to standardize outputs for tools that consume bitcode; bundles, bundles written by the bundler internally use only the 4-field; target triple:. ``<arch><sub>-<vendor>-<sys>-<env>``. **target-id**; The canonical target ID of the code object. Present only if the target; supports a target ID. See :ref:`clang-target",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst:9098,load,loaded,9098,interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangOffloadBundler.rst,1,['load'],['loaded']
Performance,"==========================; Vector Predication Roadmap; ==========================. .. contents:: Table of Contents; :depth: 3; :local:. Motivation; ==========. This proposal defines a roadmap towards native vector predication in LLVM,; specifically for vector instructions with a mask and/or an explicit vector; length. LLVM currently has no target-independent means to model predicated; vector instructions for modern SIMD ISAs such as AVX512, ARM SVE, the RISC-V V; extension and NEC SX-Aurora. Only some predicated vector operations, such as; masked loads and stores, are available through intrinsics [MaskedIR]_. The Vector Predication (VP) extensions is a concrete RFC and prototype; implementation to achieve native vector predication in LLVM. The VP prototype; and all related discussions can be found in the VP patch on Phabricator; [VPRFC]_. Roadmap; =======. 1. IR-level VP intrinsics; -------------------------. - There is a consensus on the semantics/instruction set of VP.; - VP intrinsics and attributes are available on IR level.; - TTI has capability flags for VP (``supportsVP()``?,; ``haveActiveVectorLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/In",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:554,load,loads,554,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,1,['load'],['loads']
Performance,"===========================; ORC Design and Implementation; ===============================. .. contents::; :local:. Introduction; ============. This document aims to provide a high-level overview of the design and; implementation of the ORC JIT APIs. Except where otherwise stated all discussion; refers to the modern ORCv2 APIs (available since LLVM 7). Clients wishing to; transition from OrcV1 should see Section :ref:`transitioning_orcv1_to_orcv2`. Use-cases; =========. ORC provides a modular API for building JIT compilers. There are a number; of use cases for such an API. For example:. 1. The LLVM tutorials use a simple ORC-based JIT class to execute expressions; compiled from a toy language: Kaleidoscope. 2. The LLVM debugger, LLDB, uses a cross-compiling JIT for expression; evaluation. In this use case, cross compilation allows expressions compiled; in the debugger process to be executed on the debug target process, which may; be on a different device/architecture. 3. In high-performance JITs (e.g. JVMs, Julia) that want to make use of LLVM's; optimizations within an existing JIT infrastructure. 4. In interpreters and REPLs, e.g. Cling (C++) and the Swift interpreter. By adopting a modular, library-based design we aim to make ORC useful in as many; of these contexts as possible. Features; ========. ORC provides the following features:. **JIT-linking**; ORC provides APIs to link relocatable object files (COFF, ELF, MachO) [1]_; into a target process at runtime. The target process may be the same process; that contains the JIT session object and jit-linker, or may be another process; (even one running on a different machine or architecture) that communicates; with the JIT via RPC. **LLVM IR compilation**; ORC provides off the shelf components (IRCompileLayer, SimpleCompiler,; ConcurrentIRCompiler) that make it easy to add LLVM IR to a JIT'd process. **Eager and lazy compilation**; By default, ORC will compile symbols as soon as they are looked up in the JIT; sessio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:999,perform,performance,999,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['perform'],['performance']
Performance,"============================ ================================================; lwe Enables LOD warning.; ======================================== ================================================. .. _amdgpu_synid_da:. da; ~~. Specifies if an array index must be sent to TA. By default, the array index is not sent. ======================================== ================================================; Syntax Description; ======================================== ================================================; da Send an array index to TA.; ======================================== ================================================. .. _amdgpu_synid_d16:. d16; ~~~. Specifies data size: 16 or 32 bits (32 bits by default). ======================================== ================================================; Syntax Description; ======================================== ================================================; d16 Enables 16-bits data mode. On loads, convert data in memory to 16-bit; format before storing it in VGPRs. For stores, convert 16-bit data in VGPRs to; 32 bits before writing the values to memory. Note that GFX8.0 does not support data packing.; Each 16-bit data element occupies 1 VGPR. GFX8.1 and GFX9+ support data packing.; Each pair of 16-bit data elements; occupies 1 VGPR.; ======================================== ================================================. .. _amdgpu_synid_a16:. a16; ~~~. Specifies the size of image address components: 16 or 32 bits (32 bits by default). ======================================== ================================================; Syntax Description; ======================================== ================================================; a16 Enables 16-bits image address components.; ======================================== ================================================. .. _amdgpu_synid_dim:. dim; ~~~. Specifies surface dimension. This is a mandatory modifier. There is no default value. ===================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst:14397,load,loads,14397,interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,1,['load'],['loads']
Performance,"============================; Clang Compiler User's Manual; ============================. .. include:: <isonum.txt>. .. contents::; :local:. Introduction; ============. The Clang Compiler is an open-source compiler for the C family of; programming languages, aiming to be the best in class implementation of; these languages. Clang builds on the LLVM optimizer and code generator,; allowing it to provide high-quality optimization and code generation; support for many targets. For more general information, please see the; `Clang Web Site <https://clang.llvm.org>`_ or the `LLVM Web; Site <https://llvm.org>`_. This document describes important notes about using Clang as a compiler; for an end-user, documenting the supported features, command line; options, etc. If you are interested in using Clang to build a tool that; processes code, please see :doc:`InternalsManual`. If you are interested in the; `Clang Static Analyzer <https://clang-analyzer.llvm.org>`_, please see its web; page. Clang is one component in a complete toolchain for C family languages.; A separate document describes the other pieces necessary to; :doc:`assemble a complete toolchain <Toolchain>`. Clang is designed to support the C family of programming languages,; which includes :ref:`C <c>`, :ref:`Objective-C <objc>`, :ref:`C++ <cxx>`, and; :ref:`Objective-C++ <objcxx>` as well as many dialects of those. For; language-specific information, please see the corresponding language; specific section:. - :ref:`C Language <c>`: K&R C, ANSI C89, ISO C90, ISO C94 (C89+AMD1), ISO; C99 (+TC1, TC2, TC3).; - :ref:`Objective-C Language <objc>`: ObjC 1, ObjC 2, ObjC 2.1, plus; variants depending on base language.; - :ref:`C++ Language <cxx>`; - :ref:`Objective C++ Language <objcxx>`; - :ref:`OpenCL Kernel Language <opencl>`: OpenCL C 1.0, 1.1, 1.2, 2.0, 3.0,; and C++ for OpenCL 1.0 and 2021. In addition to these base languages and their dialects, Clang supports a; broad variety of language extensions, which are documente",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:351,optimiz,optimizer,351,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,2,['optimiz'],"['optimization', 'optimizer']"
Performance,"============================; Global Instruction Selection; ============================. .. warning::; This document is a work in progress. It reflects the current state of the; implementation, as well as open design and implementation issues. .. contents::; :local:; :depth: 1. Introduction; ============. GlobalISel is a framework that provides a set of reusable passes and utilities; for instruction selection --- translation from LLVM IR to target-specific; Machine IR (MIR). GlobalISel is intended to be a replacement for SelectionDAG and FastISel, to; solve three major problems:. * **Performance** --- SelectionDAG introduces a dedicated intermediate; representation, which has a compile-time cost. GlobalISel directly operates on the post-isel representation used by the; rest of the code generator, MIR.; It does require extensions to that representation to support arbitrary; incoming IR: :ref:`gmir`. * **Granularity** --- SelectionDAG and FastISel operate on individual basic; blocks, losing some global optimization opportunities. GlobalISel operates on the whole function. * **Modularity** --- SelectionDAG and FastISel are radically different and share; very little code. GlobalISel is built in a way that enables code reuse. For instance, both the; optimized and fast selectors share the :ref:`pipeline`, and targets can; configure that pipeline to better suit their needs. Design and Implementation Reference; ===================================. More information on the design and implementation of GlobalISel can be found in; the following sections. .. toctree::; :maxdepth: 1. GMIR; GenericOpcode; MIRPatterns; Pipeline; Porting; Resources. More information on specific passes can be found in the following sections:. .. toctree::; :maxdepth: 1. IRTranslator; Legalizer; RegBankSelect; InstructionSelect; KnownBits. .. _progress:. Progress and Future Work; ========================. The initial goal is to replace FastISel on AArch64. The next step will be to; replace SelectionDA",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/index.rst:1017,optimiz,optimization,1017,interpreter/llvm-project/llvm/docs/GlobalISel/index.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/index.rst,1,['optimiz'],['optimization']
Performance,"=============================; Advanced Build Configurations; =============================. .. contents::; :local:. Introduction; ============. `CMake <http://www.cmake.org/>`_ is a cross-platform build-generator tool. CMake; does not build the project, it generates the files needed by your build tool; (GNU make, Visual Studio, etc.) for building LLVM. If **you are a new contributor**, please start with the :doc:`GettingStarted` or; :doc:`CMake` pages. This page is intended for users doing more complex builds. Many of the examples below are written assuming specific CMake Generators.; Unless otherwise explicitly called out these commands should work with any CMake; generator. Many of the build configurations mentioned on this documentation page can be; utilized by using a CMake cache. A CMake cache is essentially a configuration; file that sets the necessary flags for a specific build configuration. The caches; for Clang are located in :code:`/clang/cmake/caches` within the monorepo. They; can be passed to CMake using the :code:`-C` flag as demonstrated in the examples; below along with additional configuration flags. Bootstrap Builds; ================. The Clang CMake build system supports bootstrap (aka multi-stage) builds. At a; high level a multi-stage build is a chain of builds that pass data from one; stage into the next. The most common and simple version of this is a traditional; bootstrap build. In a simple two-stage bootstrap build, we build clang using the system compiler,; then use that just-built clang to build clang again. In CMake this simplest form; of a bootstrap build can be configured with a single option,; CLANG_ENABLE_BOOTSTRAP. .. code-block:: console. $ cmake -G Ninja -DCMAKE_BUILD_TYPE=Release \; -DCLANG_ENABLE_BOOTSTRAP=On \; -DLLVM_ENABLE_PROJECTS=""clang"" \; <path to source>/llvm; $ ninja stage2. This command itself isn't terribly useful because it assumes default; configurations for each stage. The next series of examples utilize CMake cac",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:790,cache,cache,790,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,4,['cache'],"['cache', 'caches']"
Performance,"=============================; Offloading Design & Internals; =============================. .. contents::; :local:. Introduction; ============. This document describes the Clang driver and code generation steps for creating; offloading applications. Clang supports offloading to various architectures; using programming models like CUDA, HIP, and OpenMP. The purpose of this; document is to illustrate the steps necessary to create an offloading; application using Clang. OpenMP Offloading; =================. Clang supports OpenMP target offloading to several different architectures such; as NVPTX, AMDGPU, X86_64, Arm, and PowerPC. Offloading code is generated by; Clang and then executed using the ``libomptarget`` runtime and the associated; plugin for the target architecture, e.g. ``libomptarget.rtl.cuda``. This section; describes the steps necessary to create a functioning device image that can be; loaded by the OpenMP runtime. More information on the OpenMP runtimes can be; found at the `OpenMP documentation page <https://openmp.llvm.org>`__. .. _Offloading Overview:. Offloading Overview; -------------------. The goal of offloading compilation is to create an executable device image that; can be run on the target device. OpenMP offloading creates executable images by; compiling the input file for both the host and the target device. The output; from the device phase then needs to be embedded into the host to create a fat; object. A special tool then needs to extract the device code from the fat; objects, run the device linking step, and embed the final image in a symbol the; host runtime library can use to register the library and access the symbols on; the device. Compilation Process; ^^^^^^^^^^^^^^^^^^^. The compiler performs the following high-level actions to generate OpenMP; offloading code:. * Compile the input file for the host to produce a bitcode file. Lower ``#pragma; omp target`` declarations to :ref:`offloading entries <Generating Offloading; Entries>` and",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst:910,load,loaded,910,interpreter/llvm-project/clang/docs/OffloadingDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst,1,['load'],['loaded']
Performance,"==============================; "".name"" string Kernel argument name.; "".type_name"" string Kernel argument type name.; "".size"" integer Required Kernel argument size in bytes.; "".offset"" integer Required Kernel argument offset in; bytes. The offset must be a; multiple of the alignment; required by the argument.; "".value_kind"" string Required Kernel argument kind that; specifies how to set up the; corresponding argument.; Values include:. ""by_value""; The argument is copied; directly into the kernarg. ""global_buffer""; A global address space pointer; to the buffer data is passed; in the kernarg. ""dynamic_shared_pointer""; A group address space pointer; to dynamically allocated LDS; is passed in the kernarg. ""sampler""; A global address space; pointer to a S# is passed in; the kernarg. ""image""; A global address space; pointer to a T# is passed in; the kernarg. ""pipe""; A global address space pointer; to an OpenCL pipe is passed in; the kernarg. ""queue""; A global address space pointer; to an OpenCL device enqueue; queue is passed in the; kernarg. ""hidden_global_offset_x""; The OpenCL grid dispatch; global offset for the X; dimension is passed in the; kernarg. ""hidden_global_offset_y""; The OpenCL grid dispatch; global offset for the Y; dimension is passed in the; kernarg. ""hidden_global_offset_z""; The OpenCL grid dispatch; global offset for the Z; dimension is passed in the; kernarg. ""hidden_none""; An argument that is not used; by the kernel. Space needs to; be left for it, but it does; not need to be set up. ""hidden_printf_buffer""; A global address space pointer; to the runtime printf buffer; is passed in kernarg. Mutually; exclusive with; ""hidden_hostcall_buffer""; before Code Object V5. ""hidden_hostcall_buffer""; A global address space pointer; to the runtime hostcall buffer; is passed in kernarg. Mutually; exclusive with; ""hidden_printf_buffer""; before Code Object V5. ""hidden_default_queue""; A global address space pointer; to the OpenCL device enqueue; queue that should be use",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:137422,queue,queue,137422,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['queue'],['queue']
Performance,"==============================; LLVM Language Reference Manual; ==============================. .. contents::; :local:; :depth: 3. Abstract; ========. This document is a reference manual for the LLVM assembly language. LLVM; is a Static Single Assignment (SSA) based representation that provides; type safety, low-level operations, flexibility, and the capability of; representing 'all' high-level languages cleanly. It is the common code; representation used throughout all phases of the LLVM compilation; strategy. Introduction; ============. The LLVM code representation is designed to be used in three different; forms: as an in-memory compiler IR, as an on-disk bitcode representation; (suitable for fast loading by a Just-In-Time compiler), and as a human; readable assembly language representation. This allows LLVM to provide a; powerful intermediate representation for efficient compiler; transformations and analysis, while providing a natural means to debug; and visualize the transformations. The three different forms of LLVM are; all equivalent. This document describes the human readable; representation and notation. The LLVM representation aims to be light-weight and low-level while; being expressive, typed, and extensible at the same time. It aims to be; a ""universal IR"" of sorts, by being at a low enough level that; high-level ideas may be cleanly mapped to it (similar to how; microprocessors are ""universal IR's"", allowing many source languages to; be mapped to them). By providing type information, LLVM can be used as; the target of optimizations: for example, through pointer analysis, it; can be proven that a C automatic variable is never accessed outside of; the current function, allowing it to be promoted to a simple SSA value; instead of a memory location. .. _wellformed:. Well-Formedness; ---------------. It is important to note that this document describes 'well formed' LLVM; assembly language. There is a difference between what the parser accepts; and what is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:710,load,loading,710,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loading']
Performance,"===============================; Building a Distribution of LLVM; ===============================. .. contents::; :local:. Introduction; ============. This document is geared toward people who want to build and package LLVM and any; combination of LLVM sub-project tools for distribution. This document covers; useful features of the LLVM build system as well as best practices and general; information about packaging LLVM. If you are new to CMake you may find the :doc:`CMake` or :doc:`CMakePrimer`; documentation useful. Some of the things covered in this document are the inner; workings of the builds described in the :doc:`AdvancedBuilds` document. General Distribution Guidance; =============================. When building a distribution of a compiler it is generally advised to perform a; bootstrap build of the compiler. That means building a ""stage 1"" compiler with; your host toolchain, then building the ""stage 2"" compiler using the ""stage 1""; compiler. This is done so that the compiler you distribute benefits from all the; bug fixes, performance optimizations and general improvements provided by the; new compiler. In deciding how to build your distribution there are a few trade-offs that you; will need to evaluate. The big two are:. #. Compile time of the distribution against performance of the built compiler. #. Binary size of the distribution against performance of the built compiler. The guidance for maximizing performance of the generated compiler is to use LTO,; PGO, and statically link everything. This will result in an overall larger; distribution, and it will take longer to generate, but it provides the most; opportunity for the compiler to optimize. The guidance for minimizing distribution size is to dynamically link LLVM and; Clang libraries into the tools to reduce code duplication. This will come at a; substantial performance penalty to the generated binary both because it reduces; optimization opportunity, and because dynamic linking requires resolving s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst:787,perform,perform,787,interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BuildingADistribution.rst,1,['perform'],['perform']
Performance,"===============================; How To Use Instruction Mappings; ===============================. .. contents::; :local:. Introduction; ============. This document contains information about adding instruction mapping support; for a target. The motivation behind this feature comes from the need to switch; between different instruction formats during various optimizations. One approach; could be to use switch cases which list all the instructions along with formats; they can transition to. However, it has large maintenance overhead; because of the hardcoded instruction names. Also, whenever a new instruction is; added in the .td files, all the relevant switch cases should be modified; accordingly. Instead, the same functionality could be achieved with TableGen and; some support from the .td files for a fraction of maintenance cost. ``InstrMapping`` Class Overview; ===============================. TableGen uses relationship models to map instructions with each other. These; models are described using ``InstrMapping`` class as a base. Each model sets; various fields of the ``InstrMapping`` class such that they can uniquely; describe all the instructions using that model. TableGen parses all the relation; models and uses the information to construct relation tables which relate; instructions with each other. These tables are emitted in the; ``XXXInstrInfo.inc`` file along with the functions to query them. Following; is the definition of ``InstrMapping`` class defined in Target.td file:. .. code-block:: text. class InstrMapping {; // Used to reduce search space only to the instructions using this; // relation model.; string FilterClass;. // List of fields/attributes that should be same for all the instructions in; // a row of the relation table. Think of this as a set of properties shared; // by all the instructions related by this relationship.; list<string> RowFields = [];. // List of fields/attributes that are same for all the instructions; // in a column of the relat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUseInstrMappings.rst:361,optimiz,optimizations,361,interpreter/llvm-project/llvm/docs/HowToUseInstrMappings.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUseInstrMappings.rst,1,['optimiz'],['optimizations']
Performance,"===============================; MCJIT Design and Implementation; ===============================. Introduction; ============. This document describes the internal workings of the MCJIT execution; engine and the RuntimeDyld component. It is intended as a high level; overview of the implementation, showing the flow and interactions of; objects throughout the code generation and dynamic loading process. Engine Creation; ===============. In most cases, an EngineBuilder object is used to create an instance of; the MCJIT execution engine. The EngineBuilder takes an llvm::Module; object as an argument to its constructor. The client may then set various; options that we control the later be passed along to the MCJIT engine,; including the selection of MCJIT as the engine type to be created.; Of particular interest is the EngineBuilder::setMCJITMemoryManager; function. If the client does not explicitly create a memory manager at; this time, a default memory manager (specifically SectionMemoryManager); will be created when the MCJIT engine is instantiated. Once the options have been set, a client calls EngineBuilder::create to; create an instance of the MCJIT engine. If the client does not use the; form of this function that takes a TargetMachine as a parameter, a new; TargetMachine will be created based on the target triple associated with; the Module that was used to create the EngineBuilder. .. image:: MCJIT-engine-builder.png. EngineBuilder::create will call the static MCJIT::createJIT function,; passing in its pointers to the module, memory manager and target machine; objects, all of which will subsequently be owned by the MCJIT object. The MCJIT class has a member variable, Dyld, which contains an instance of; the RuntimeDyld wrapper class. This member will be used for; communications between MCJIT and the actual RuntimeDyldImpl object that; gets created when an object is loaded. .. image:: MCJIT-creation.png. Upon creation, MCJIT holds a pointer to the Module object th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst:388,load,loading,388,interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MCJITDesignAndImplementation.rst,1,['load'],['loading']
Performance,"================================; Fuzzing LLVM libraries and tools; ================================. .. contents::; :local:; :depth: 2. Introduction; ============. The LLVM tree includes a number of fuzzers for various components. These are; built on top of :doc:`LibFuzzer <LibFuzzer>`. In order to build and run these; fuzzers, see :ref:`building-fuzzers`. Available Fuzzers; =================. clang-fuzzer; ------------. A |generic fuzzer| that tries to compile textual input as C++ code. Some of the; bugs this fuzzer has reported are `on bugzilla`__ and `on OSS Fuzz's; tracker`__. __ https://llvm.org/pr23057; __ https://bugs.chromium.org/p/oss-fuzz/issues/list?q=proj-llvm+clang-fuzzer. clang-proto-fuzzer; ------------------. A |protobuf fuzzer| that compiles valid C++ programs generated from a protobuf; class that describes a subset of the C++ language. This fuzzer accepts clang command line options after `ignore_remaining_args=1`.; For example, the following command will fuzz clang with a higher optimization; level:. .. code-block:: shell. % bin/clang-proto-fuzzer <corpus-dir> -ignore_remaining_args=1 -O3. clang-format-fuzzer; -------------------. A |generic fuzzer| that runs clang-format_ on C++ text fragments. Some of the; bugs this fuzzer has reported are `on bugzilla`__; and `on OSS Fuzz's tracker`__. .. _clang-format: https://clang.llvm.org/docs/ClangFormat.html; __ https://llvm.org/pr23052; __ https://bugs.chromium.org/p/oss-fuzz/issues/list?q=proj-llvm+clang-format-fuzzer. llvm-as-fuzzer; --------------. A |generic fuzzer| that tries to parse text as :doc:`LLVM assembly <LangRef>`.; Some of the bugs this fuzzer has reported are `on bugzilla`__. __ https://llvm.org/pr24639. llvm-dwarfdump-fuzzer; ---------------------. A |generic fuzzer| that interprets inputs as object files and runs; :doc:`llvm-dwarfdump <CommandGuide/llvm-dwarfdump>` on them. Some of the bugs; this fuzzer has reported are `on OSS Fuzz's tracker`__. __ https://bugs.chromium.org/p/oss-fuzz/i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst:1013,optimiz,optimization,1013,interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FuzzingLLVM.rst,1,['optimiz'],['optimization']
Performance,"==================================; Benchmarking tips; ==================================. Introduction; ============. For benchmarking a patch we want to reduce all possible sources of; noise as much as possible. How to do that is very OS dependent. Note that low noise is required, but not sufficient. It does not; exclude measurement bias. See; https://www.cis.upenn.edu/~cis501/papers/producing-wrong-data.pdf for; example. General; ================================. * Use a high resolution timer, e.g. perf under linux. * Run the benchmark multiple times to be able to recognize noise. * Disable as many processes or services as possible on the target system. * Disable frequency scaling, turbo boost and address space; randomization (see OS specific section). * Static link if the OS supports it. That avoids any variation that; might be introduced by loading dynamic libraries. This can be done; by passing ``-DLLVM_BUILD_STATIC=ON`` to cmake. * Try to avoid storage. On some systems you can use tmpfs. Putting the; program, inputs and outputs on tmpfs avoids touching a real storage; system, which can have a pretty big variability. To mount it (on linux and freebsd at least)::. mount -t tmpfs -o size=<XX>g none dir_to_mount. Linux; =====. * Disable address space randomization::. echo 0 > /proc/sys/kernel/randomize_va_space. * Set scaling_governor to performance::. for i in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; do; echo performance > /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; done. * Use https://github.com/lpechacek/cpuset to reserve cpus for just the; program you are benchmarking. If using perf, leave at least 2 cores; so that perf runs in one and your program in another::. cset shield -c N1,N2 -k on. This will move all threads out of N1 and N2. The ``-k on`` means; that even kernel threads are moved out. * Disable the SMT pair of the cpus you will use for the benchmark. The; pair of cpu N can be found in; ``/sys/devices/system/cpu/cpuN/topology/t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Benchmarking.rst:858,load,loading,858,interpreter/llvm-project/llvm/docs/Benchmarking.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Benchmarking.rst,1,['load'],['loading']
Performance,"==================================; Stack Safety Analysis; ==================================. Introduction; ============. The Stack Safety Analysis determines if stack allocated variables can be; considered 'safe' from memory access bugs. The primary purpose of the analysis is to be used by sanitizers to avoid; unnecessary instrumentation of 'safe' variables. SafeStack is going to be the; first user. 'safe' variables can be defined as variables that can not be used out-of-scope; (e.g. use-after-return) or accessed out of bounds. In the future it can be; extended to track other variable properties. E.g. we plan to extend; implementation with a check to make sure that variable is always initialized; before every read to optimize use-of-uninitialized-memory checks. How it works; ============. The analysis is implemented in two stages:. The intra-procedural, or 'local', stage performs a depth-first search inside; functions to collect all uses of each alloca, including loads/stores and uses as; arguments functions. After this stage we know which parts of the alloca are used; by functions itself but we don't know what happens after it is passed as; an argument to another function. The inter-procedural, or 'global', stage, resolves what happens to allocas after; they are passed as function arguments. This stage performs a depth-first search; on function calls inside a single module and propagates allocas usage through; functions calls. When used with ThinLTO, the global stage performs a whole program analysis over; the Module Summary Index. Testing; =======. The analysis is covered with lit tests. We expect that users can tolerate false classification of variables as; 'unsafe' when in-fact it's 'safe'. This may lead to inefficient code. However, we; can't accept false 'safe' classification which may cause sanitizers to miss actual; bugs in instrumented code. To avoid that we want additional validation tool. AddressSanitizer may help with this validation. We can instrument ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackSafetyAnalysis.rst:729,optimiz,optimize,729,interpreter/llvm-project/llvm/docs/StackSafetyAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackSafetyAnalysis.rst,3,"['load', 'optimiz', 'perform']","['loads', 'optimize', 'performs']"
Performance,"====================================; Getting Started with the LLVM System; ====================================. .. contents::; :local:. Overview; ========. Welcome to the LLVM project!. The LLVM project has multiple components. The core of the project is; itself called ""LLVM"". This contains all of the tools, libraries, and header; files needed to process intermediate representations and converts it into; object files. Tools include an assembler, disassembler, bitcode analyzer, and; bitcode optimizer. It also contains basic regression tests. C-like languages use the `Clang <https://clang.llvm.org/>`_ front end. This; component compiles C, C++, Objective C, and Objective C++ code into LLVM bitcode; -- and from there into object files, using LLVM. Other components include:; the `libc++ C++ standard library <https://libcxx.llvm.org>`_,; the `LLD linker <https://lld.llvm.org>`_, and more. Getting the Source Code and Building LLVM; =========================================. #. Check out LLVM (including subprojects like Clang):. * ``git clone https://github.com/llvm/llvm-project.git``; * Or, on windows:. ``git clone --config core.autocrlf=false; https://github.com/llvm/llvm-project.git``; * To save storage and speed-up the checkout time, you may want to do a; `shallow clone <https://git-scm.com/docs/git-clone#Documentation/git-clone.txt---depthltdepthgt>`_.; For example, to get the latest revision of the LLVM project, use. ``git clone --depth 1 https://github.com/llvm/llvm-project.git``. * You are likely only interested in the main branch moving forward, if; you don't want `git fetch` (or `git pull`) to download user branches, use:. ``sed 's#fetch = +refs/heads/\*:refs/remotes/origin/\*#fetch = +refs/heads/main:refs/remotes/origin/main#' -i llvm-project/.git/config``. #. Configure and build LLVM and Clang:. * ``cd llvm-project``; * ``cmake -S llvm -B build -G <generator> [options]``. Some common build system generators are:. * ``Ninja`` --- for generating `Ninja <https://",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst:497,optimiz,optimizer,497,interpreter/llvm-project/llvm/docs/GettingStarted.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst,1,['optimiz'],['optimizer']
Performance,"====================================; LLVM bugpoint tool: design and usage; ====================================. .. contents::; :local:. Description; ===========. ``bugpoint`` narrows down the source of problems in LLVM tools and passes. It; can be used to debug three types of failures: optimizer crashes, miscompilations; by optimizers, or bad native code generation (including problems in the static; and JIT compilers). It aims to reduce large test cases to small, useful ones.; For example, if ``opt`` crashes while optimizing a file, it will identify the; optimization (or combination of optimizations) that causes the crash, and reduce; the file down to a small example which triggers the crash. For detailed case scenarios, such as debugging ``opt``, or one of the LLVM code; generators, see :doc:`HowToSubmitABug`. Design Philosophy; =================. ``bugpoint`` is designed to be a useful tool without requiring any hooks into; the LLVM infrastructure at all. It works with any and all LLVM passes and code; generators, and does not need to ""know"" how they work. Because of this, it may; appear to do stupid things or miss obvious simplifications. ``bugpoint`` is; also designed to trade off programmer time for computer time in the; compiler-debugging process; consequently, it may take a long period of; (unattended) time to reduce a test case, but we feel it is still worth it. Note; that ``bugpoint`` is generally very quick unless debugging a miscompilation; where each test of the program (which requires executing it) takes a long time. Automatic Debugger Selection; ----------------------------. ``bugpoint`` reads each ``.bc`` or ``.ll`` file specified on the command line; and links them together into a single module, called the test program. If any; LLVM passes are specified on the command line, it runs these passes on the test; program. If any of the passes crash, or if they produce malformed output (which; causes the verifier to abort), ``bugpoint`` starts the `crash d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst:289,optimiz,optimizer,289,interpreter/llvm-project/llvm/docs/Bugpoint.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Bugpoint.rst,5,['optimiz'],"['optimization', 'optimizations', 'optimizer', 'optimizers', 'optimizing']"
Performance,"====================================; LLVM's Analysis and Transform Passes; ====================================. .. contents::; :local:. Introduction; ============. This document serves as a high level summary of the optimization features that; LLVM provides. Optimizations are implemented as Passes that traverse some; portion of a program to either collect information or transform the program.; The table below divides the passes that LLVM provides into three categories.; Analysis passes compute information that other passes can use or for debugging; or program visualization purposes. Transform passes can use (or invalidate); the analysis passes. Transform passes all mutate the program in some way.; Utility passes provides some utility but don't otherwise fit categorization.; For example passes to extract functions to bitcode or write a module to bitcode; are neither analysis nor transform passes. The table of contents above; provides a quick summary of each pass and links to the more complete pass; description later in the document. Analysis Passes; ===============. This section describes the LLVM Analysis Passes. ``aa-eval``: Exhaustive Alias Analysis Precision Evaluator; ----------------------------------------------------------. This is a simple N^2 alias analysis accuracy evaluator. Basically, for each; function in the program, it simply queries to see how the alias analysis; implementation answers alias queries between each pair of pointers in the; function. This is inspired and adapted from code by: Naveen Neelakantam, Francesco; Spadini, and Wojciech Stryjewski. ``basic-aa``: Basic Alias Analysis (stateless AA impl); ------------------------------------------------------. A basic alias analysis pass that implements identities (two different globals; cannot alias, etc), but does no stateful analysis. ``basiccg``: Basic CallGraph Construction; -----------------------------------------. Yet to be written. .. _passes-da:. ``da``: Dependence Analysis; ------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:218,optimiz,optimization,218,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['optimiz'],['optimization']
Performance,"====================================; ``threadId`` ``@llvm.nvvm.read.ptx.sreg.tid.*``; ``blockIdx`` ``@llvm.nvvm.read.ptx.sreg.ctaid.*``; ``blockDim`` ``@llvm.nvvm.read.ptx.sreg.ntid.*``; ``gridDim`` ``@llvm.nvvm.read.ptx.sreg.nctaid.*``; ============ =====================================. Barriers; --------. '``llvm.nvvm.barrier0``'; ^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". .. code-block:: llvm. declare void @llvm.nvvm.barrier0(). Overview:; """""""""""""""""". The '``@llvm.nvvm.barrier0()``' intrinsic emits a PTX ``bar.sync 0``; instruction, equivalent to the ``__syncthreads()`` call in CUDA. Other Intrinsics; ----------------. For the full set of NVPTX intrinsics, please see the; ``include/llvm/IR/IntrinsicsNVVM.td`` file in the LLVM source tree. .. _libdevice:. Linking with Libdevice; ======================. The CUDA Toolkit comes with an LLVM bitcode library called ``libdevice`` that; implements many common mathematical functions. This library can be used as a; high-performance math library for any compilers using the LLVM NVPTX target.; The library can be found under ``nvvm/libdevice/`` in the CUDA Toolkit and; there is a separate version for each compute architecture. For a list of all math functions implemented in libdevice, see; `libdevice Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_. To accommodate various math-related compiler flags that can affect code; generation of libdevice code, the library code depends on a special LLVM IR; pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:7740,perform,performance,7740,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,1,['perform'],['performance']
Performance,"=====================================; Garbage Collection with LLVM; =====================================. .. contents::; :local:. Abstract; ========. This document covers how to integrate LLVM into a compiler for a language which; supports garbage collection. **Note that LLVM itself does not provide a; garbage collector.** You must provide your own. Quick Start; ============. First, you should pick a collector strategy. LLVM includes a number of built; in ones, but you can also implement a loadable plugin with a custom definition.; Note that the collector strategy is a description of how LLVM should generate; code such that it interacts with your collector and runtime, not a description; of the collector itself. Next, mark your generated functions as using your chosen collector strategy.; From c++, you can call:. .. code-block:: c++. F.setGC(<collector description name>);. This will produce IR like the following fragment:. .. code-block:: llvm. define void @foo() gc ""<collector description name>"" { ... }. When generating LLVM IR for your functions, you will need to:. * Use ``@llvm.gcread`` and/or ``@llvm.gcwrite`` in place of standard load and; store instructions. These intrinsics are used to represent load and store; barriers. If you collector does not require such barriers, you can skip; this step. * Use the memory allocation routines provided by your garbage collector's; runtime library. * If your collector requires them, generate type maps according to your; runtime's binary interface. LLVM is not involved in the process. In; particular, the LLVM type system is not suitable for conveying such; information though the compiler. * Insert any coordination code required for interacting with your collector.; Many collectors require running application code to periodically check a; flag and conditionally call a runtime function. This is often referred to; as a safepoint poll. You will need to identify roots (i.e. references to heap objects your collector; needs to kno",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:497,load,loadable,497,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['load'],['loadable']
Performance,"=====================================; Performance Tips for Frontend Authors; =====================================. .. contents::; :local:; :depth: 2. Abstract; ========. The intended audience of this document is developers of language frontends; targeting LLVM IR. This document is home to a collection of tips on how to; generate IR that optimizes well. IR Best Practices; =================. As with any optimizer, LLVM has its strengths and weaknesses. In some cases,; surprisingly small changes in the source IR can have a large effect on the; generated code. Beyond the specific items on the list below, it's worth noting that the most; mature frontend for LLVM is Clang. As a result, the further your IR gets from; what Clang might emit, the less likely it is to be effectively optimized. It; can often be useful to write a quick C program with the semantics you're trying; to model and see what decisions Clang's IRGen makes about what IR to emit.; Studying Clang's CodeGen directory can also be a good source of ideas. Note; that Clang and LLVM are explicitly version locked so you'll need to make sure; you're using a Clang built from the same git revision or release as the LLVM; library you're using. As always, it's *strongly* recommended that you track; tip of tree development, particularly during bring up of a new project. The Basics; ^^^^^^^^^^^. #. Make sure that your Modules contain both a data layout specification and; target triple. Without these pieces, non of the target specific optimization; will be enabled. This can have a major effect on the generated code quality. #. For each function or global emitted, use the most private linkage type; possible (private, internal or linkonce_odr preferably). Doing so will; make LLVM's inter-procedural optimizations much more effective. #. Avoid high in-degree basic blocks (e.g. basic blocks with dozens or hundreds; of predecessors). Among other issues, the register allocator is known to; perform badly with confronted with suc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:341,optimiz,optimizes,341,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,3,['optimiz'],"['optimized', 'optimizer', 'optimizes']"
Performance,==========================================. .. table:: AMDGPU Vendors; :name: amdgpu-vendor-table. ============ ==============================================================; Vendor Description; ============ ==============================================================; ``amd`` Can be used for all AMD GPU usage.; ``mesa3d`` Can be used if the OS is ``mesa3d``.; ============ ==============================================================. .. table:: AMDGPU Operating Systems; :name: amdgpu-os. ============== ============================================================; OS Description; ============== ============================================================; *<empty>* Defaults to the *unknown* OS.; ``amdhsa`` Compute kernels executed on HSA [HSA]_ compatible runtimes; such as:. - AMD's ROCm™ runtime [AMD-ROCm]_ using the *rocm-amdhsa*; loader on Linux. See *AMD ROCm Platform Release Notes*; [AMD-ROCm-Release-Notes]_ for supported hardware and; software.; - AMD's PAL runtime using the *pal-amdhsa* loader on; Windows. ``amdpal`` Graphic shaders and compute kernels executed on AMD's PAL; runtime using the *pal-amdpal* loader on Windows and Linux; Pro.; ``mesa3d`` Graphic shaders and compute kernels executed on AMD's Mesa; 3D runtime using the *mesa-mesa3d* loader on Linux.; ============== ============================================================. .. table:: AMDGPU Environments; :name: amdgpu-environment-table. ============ ==============================================================; Environment Description; ============ ==============================================================; *<empty>* Default.; ============ ==============================================================. .. _amdgpu-processors:. Processors; ----------. Use the Clang options ``-mcpu=<target-id>`` or ``--offload-arch=<target-id>`` to; specify the AMDGPU processor together with optional target features. See; :ref:`amdgpu-target-id` and :ref:`amdgpu-target-features` for AMD GPU target; specifi,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:2579,load,loader,2579,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loader']
Performance,"==========================================; Design and Usage of the InAlloca Attribute; ==========================================. Introduction; ============. The :ref:`inalloca <attr_inalloca>` attribute is designed to allow; taking the address of an aggregate argument that is being passed by; value through memory. Primarily, this feature is required for; compatibility with the Microsoft C++ ABI. Under that ABI, class; instances that are passed by value are constructed directly into; argument stack memory. Prior to the addition of inalloca, calls in LLVM; were indivisible instructions. There was no way to perform intermediate; work, such as object construction, between the first stack adjustment; and the final control transfer. With inalloca, all arguments passed in; memory are modelled as a single alloca, which can be stored to prior to; the call. Unfortunately, this complicated feature comes with a large; set of restrictions designed to bound the lifetime of the argument; memory around the call. For now, it is recommended that frontends and optimizers avoid producing; this construct, primarily because it forces the use of a base pointer.; This feature may grow in the future to allow general mid-level; optimization, but for now, it should be regarded as less efficient than; passing by value with a copy. Intended Usage; ==============. The example below is the intended LLVM IR lowering for some C++ code; that passes two default-constructed ``Foo`` objects to ``g`` in the; 32-bit Microsoft C++ ABI. .. code-block:: c++. // Foo is non-trivial.; struct Foo { int a, b; Foo(); ~Foo(); Foo(const Foo &); };; void g(Foo a, Foo b);; void f() {; g(Foo(), Foo());; }. .. code-block:: text. %struct.Foo = type { i32, i32 }; declare void @Foo_ctor(%struct.Foo* %this); declare void @Foo_dtor(%struct.Foo* %this); declare void @g(<{ %struct.Foo, %struct.Foo }>* inalloca %memargs). define void @f() {; entry:; %base = call i8* @llvm.stacksave(); %memargs = alloca <{ %struct.Foo, %struc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InAlloca.rst:615,perform,perform,615,interpreter/llvm-project/llvm/docs/InAlloca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/InAlloca.rst,1,['perform'],['perform']
Performance,==========================================; ``amd`` Can be used for all AMD GPU usage.; ``mesa3d`` Can be used if the OS is ``mesa3d``.; ============ ==============================================================. .. table:: AMDGPU Operating Systems; :name: amdgpu-os. ============== ============================================================; OS Description; ============== ============================================================; *<empty>* Defaults to the *unknown* OS.; ``amdhsa`` Compute kernels executed on HSA [HSA]_ compatible runtimes; such as:. - AMD's ROCm™ runtime [AMD-ROCm]_ using the *rocm-amdhsa*; loader on Linux. See *AMD ROCm Platform Release Notes*; [AMD-ROCm-Release-Notes]_ for supported hardware and; software.; - AMD's PAL runtime using the *pal-amdhsa* loader on; Windows. ``amdpal`` Graphic shaders and compute kernels executed on AMD's PAL; runtime using the *pal-amdpal* loader on Windows and Linux; Pro.; ``mesa3d`` Graphic shaders and compute kernels executed on AMD's Mesa; 3D runtime using the *mesa-mesa3d* loader on Linux.; ============== ============================================================. .. table:: AMDGPU Environments; :name: amdgpu-environment-table. ============ ==============================================================; Environment Description; ============ ==============================================================; *<empty>* Default.; ============ ==============================================================. .. _amdgpu-processors:. Processors; ----------. Use the Clang options ``-mcpu=<target-id>`` or ``--offload-arch=<target-id>`` to; specify the AMDGPU processor together with optional target features. See; :ref:`amdgpu-target-id` and :ref:`amdgpu-target-features` for AMD GPU target; specific information. Every processor supports every OS ABI (see :ref:`amdgpu-os`) with the following exceptions:. * ``amdhsa`` is not supported in ``r600`` architecture (see :ref:`amdgpu-architecture-table`). .. table:: AMDGPU Processo,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:2841,load,loader,2841,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loader']
Performance,"=============================================; Building a JIT: Per-function Lazy Compilation; =============================================. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 3 Introduction; ======================. **Warning: This text is currently out of date due to ORC API updates.**. **The example code has been updated and can be used. The text will be updated; once the API churn dies down.**. Welcome to Chapter 3 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter discusses lazy JITing and shows you how to enable it by adding an ORC; CompileOnDemand layer the JIT from `Chapter 2 <BuildingAJIT2.html>`_. Lazy Compilation; ================. When we add a module to the KaleidoscopeJIT class from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:931,optimiz,optimized,931,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,1,['optimiz'],['optimized']
Performance,"==============================================; JSON Compilation Database Format Specification; ==============================================. This document describes a format for specifying how to replay single; compilations independently of the build system. Background; ==========. Tools based on the C++ Abstract Syntax Tree need full information how to; parse a translation unit. Usually this information is implicitly; available in the build system, but running tools as part of the build; system is not necessarily the best solution:. - Build systems are inherently change driven, so running multiple tools; over the same code base without changing the code does not fit into; the architecture of many build systems.; - Figuring out whether things have changed is often an IO bound; process; this makes it hard to build low latency end user tools based; on the build system.; - Build systems are inherently sequential in the build graph, for; example due to generated source code. While tools that run; independently of the build still need the generated source code to; exist, running tools multiple times over unchanging source does not; require serialization of the runs according to the build dependency; graph. Supported Systems; =================. Clang has the ability to generate compilation database fragments via; ``-MJ argument <clang -MJ\<arg>>``. You can concatenate those; fragments together between ``[`` and ``]`` to create a compilation database. Currently `CMake <https://cmake.org>`_ (since 2.8.5) supports generation; of compilation databases for Unix Makefile builds (Ninja builds in the; works) with the option ``CMAKE_EXPORT_COMPILE_COMMANDS``. For projects on Linux, there is an alternative to intercept compiler; calls with a tool called `Bear <https://github.com/rizsotto/Bear>`_. `Bazel <https://bazel.build>`_ can export a compilation database via; `this extractor extension; <https://github.com/hedronvision/bazel-compile-commands-extractor>`_.; Bazel is otherwise",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/JSONCompilationDatabase.rst:832,latency,latency,832,interpreter/llvm-project/clang/docs/JSONCompilationDatabase.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/JSONCompilationDatabase.rst,1,['latency'],['latency']
Performance,"==============================================; Kaleidoscope: Adding JIT and Optimizer Support; ==============================================. .. contents::; :local:. Chapter 4 Introduction; ======================. Welcome to Chapter 4 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Chapters 1-3 described the implementation; of a simple language and added support for generating LLVM IR. This; chapter describes two new techniques: adding optimizer support to your; language, and adding JIT compiler support. These additions will; demonstrate how to get nice, efficient code for the Kaleidoscope; language. Trivial Constant Folding; ========================. Our demonstration for Chapter 3 is elegant and easy to extend.; Unfortunately, it does not produce wonderful code. The IRBuilder,; however, does give us obvious optimizations when compiling simple code:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; ret double %addtmp; }. This code is not a literal transcription of the AST built by parsing the; input. That would be:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 2.000000e+00, 1.000000e+00; %addtmp1 = fadd double %addtmp, %x; ret double %addtmp1; }. Constant folding, as seen above, in particular, is a very common and; very important optimization: so much so that many language implementors; implement constant folding support in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:464,optimiz,optimizer,464,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,2,['optimiz'],"['optimizations', 'optimizer']"
Performance,"==============================================; LLVM Atomic Instructions and Concurrency Guide; ==============================================. .. contents::; :local:. Introduction; ============. LLVM supports instructions which are well-defined in the presence of threads and; asynchronous signals. The atomic instructions are designed specifically to provide readable IR and; optimized code generation for the following:. * The C++ ``<atomic>`` header and C ``<stdatomic.h>`` headers. These; were originally added in C++11 and C11. The memory model has been; subsequently adjusted to correct errors in the initial; specification, so LLVM currently intends to implement the version; specified by C++20. (See the `C++20 draft standard; <https://isocpp.org/files/papers/N4860.pdf>`_ or the unofficial; `latest C++ draft <https://eel.is/c++draft/>`_. A `C2x draft; <https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3047.pdf>`_ is; also available, though the text has not yet been updated with the; errata corrected by C++20.). * Proper semantics for Java-style memory, for both ``volatile`` and regular; shared variables. (`Java Specification; <http://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html>`_). * gcc-compatible ``__sync_*`` builtins. (`Description; <https://gcc.gnu.org/onlinedocs/gcc/_005f_005fsync-Builtins.html>`_). * Other scenarios with atomic semantics, including ``static`` variables with; non-trivial constructors in C++. Atomic and volatile in the IR are orthogonal; ""volatile"" is the C/C++ volatile,; which ensures that every volatile load and store happens and is performed in the; stated order. A couple examples: if a SequentiallyConsistent store is; immediately followed by another SequentiallyConsistent store to the same; address, the first store can be erased. This transformation is not allowed for a; pair of volatile stores. On the other hand, a non-volatile non-atomic load can; be moved across a volatile load freely, but not an Acquire load. This document is int",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:378,optimiz,optimized,378,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['optimiz'],['optimized']
Performance,"==============================================; Using ARM NEON instructions in big endian mode; ==============================================. .. contents::; :local:. Introduction; ============. Generating code for big endian ARM processors is for the most part straightforward. NEON loads and stores however have some interesting properties that make code generation decisions less obvious in big endian mode. The aim of this document is to explain the problem with NEON loads and stores, and the solution that has been implemented in LLVM. In this document the term ""vector"" refers to what the ARM ABI calls a ""short vector"", which is a sequence of items that can fit in a NEON register. This sequence can be 64 or 128 bits in length, and can constitute 8, 16, 32 or 64 bit items. This document refers to A64 instructions throughout, but is almost applicable to the A32/ARMv7 instruction sets also. The ABI format for passing vectors in A32 is slightly different to A64. Apart from that, the same concepts apply. Example: C-level intrinsics -> assembly; ---------------------------------------. It may be helpful first to illustrate how C-level ARM NEON intrinsics are lowered to instructions. This trivial C function takes a vector of four ints and sets the zero'th lane to the value ""42""::. #include <arm_neon.h>; int32x4_t f(int32x4_t p) {; return vsetq_lane_s32(42, p, 0);; }. arm_neon.h intrinsics generate ""generic"" IR where possible (that is, normal IR instructions not ``llvm.arm.neon.*`` intrinsic calls). The above generates::. define <4 x i32> @f(<4 x i32> %p) {; %vset_lane = insertelement <4 x i32> %p, i32 42, i32 0; ret <4 x i32> %vset_lane; }. Which then becomes the following trivial assembly::. f: // @f; movz	w8, #0x2a; ins 	v0.s[0], w8; ret. Problem; =======. The main problem is how vectors are represented in memory and in registers. First, a recap. The ""endianness"" of an item affects its representation in memory only. In a register, a number is just a sequence of bits - 64",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:285,load,loads,285,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,2,['load'],['loads']
Performance,"===============================================. The code sequences used to implement the memory model are defined in the; following sections:. * :ref:`amdgpu-amdhsa-memory-model-gfx6-gfx9`; * :ref:`amdgpu-amdhsa-memory-model-gfx90a`; * :ref:`amdgpu-amdhsa-memory-model-gfx942`; * :ref:`amdgpu-amdhsa-memory-model-gfx10-gfx11`. .. _amdgpu-amdhsa-memory-model-gfx6-gfx9:. Memory Model GFX6-GFX9; ++++++++++++++++++++++. For GFX6-GFX9:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same CU but may be; executed by different SIMDs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it.; * All LDS operations of a CU are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that for GFX7-GFX9 ``flat_load/store/atomic`` instructions can report out of; vector memory order if they access LDS memory, and out of LDS operation order; if they access global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore, no special action is requir",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:206818,queue,queues,206818,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queues']
Performance,"==================================================. Introduction and Warning; ========================. During the course of using LLVM, you may wish to customize it for your research; project or for experimentation. At this point, you may realize that you need to; add something to LLVM, whether it be a new fundamental type, a new intrinsic; function, or a whole new instruction. When you come to this realization, stop and think. Do you really need to extend; LLVM? Is it a new fundamental capability that LLVM does not support at its; current incarnation or can it be synthesized from already pre-existing LLVM; elements? If you are not sure, ask on the `LLVM forums; <https://discourse.llvm.org>`_. The reason is that; extending LLVM will get involved as you need to update all the different passes; that you intend to use with your extension, and there are ``many`` LLVM analyses; and transformations, so it may be quite a bit of work. Adding an `intrinsic function`_ is far easier than adding an; instruction, and is transparent to optimization passes. If your added; functionality can be expressed as a function call, an intrinsic function is the; method of choice for LLVM extension. Before you invest a significant amount of effort into a non-trivial extension,; **ask on the list** if what you are looking to do can be done with; already-existing infrastructure, or if maybe someone else is already working on; it. You will save yourself a lot of time and effort by doing so. .. _intrinsic function:. Adding a new intrinsic function; ===============================. Adding a new intrinsic function to LLVM is much easier than adding a new; instruction. Almost all extensions to LLVM should start as an intrinsic; function and then be turned into an instruction if warranted. #. ``llvm/docs/LangRef.html``:. Document the intrinsic. Decide whether it is code generator specific and; what the restrictions are. Talk to other people about it so that you are; sure it's a good idea. #. ``llvm/",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendingLLVM.rst:1173,optimiz,optimization,1173,interpreter/llvm-project/llvm/docs/ExtendingLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendingLLVM.rst,1,['optimiz'],['optimization']
Performance,"==================================================; Kaleidoscope: Extending the Language: Control Flow; ==================================================. .. contents::; :local:. Chapter 5 Introduction; ======================. Welcome to Chapter 5 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Parts 1-4 described the implementation of; the simple Kaleidoscope language and included support for generating; LLVM IR, followed by optimizations and a JIT compiler. Unfortunately, as; presented, Kaleidoscope is mostly useless: it has no control flow other; than call and return. This means that you can't have conditional; branches in the code, significantly limiting its power. In this episode; of ""build that compiler"", we'll extend Kaleidoscope to have an; if/then/else expression plus a simple 'for' loop. If/Then/Else; ============. Extending Kaleidoscope to support if/then/else is quite straightforward.; It basically requires adding support for this ""new"" concept to the; lexer, parser, AST, and LLVM code emitter. This example is nice, because; it shows how easy it is to ""grow"" a language over time, incrementally; extending it as new ideas are discovered. Before we get going on ""how"" we add this extension, let's talk about; ""what"" we want. The basic idea is that we want to be able to write this; sort of thing:. ::. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. In Kaleidoscope, every construct is an expression: there are no; statements. As such, the if/then/else expression needs to return a value; like any other. Since we're using a mostly functional form, we'll have; it evaluate its conditional, then return the 'then' or 'else' value; based on how the condition was resolved. This is very similar to the C; ""?:"" expression. The semantics of the if/then/else expression is that it evaluates the; condition to a boolean equality value: 0.0 is considered to be false and; everything else is considered to be true. If the condition is true, the; first ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:453,optimiz,optimizations,453,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,1,['optimiz'],['optimizations']
Performance,"====================================================; Using -opt-bisect-limit to debug optimization errors; ====================================================; .. contents::; :local:; :depth: 1. Introduction; ============. The -opt-bisect-limit option provides a way to disable all optimization passes; above a specified limit without modifying the way in which the Pass Managers; are populated. The intention of this option is to assist in tracking down; problems where incorrect transformations during optimization result in incorrect; run-time behavior. This feature is implemented on an opt-in basis. Passes which can be safely; skipped while still allowing correct code generation call a function to; check the opt-bisect limit before performing optimizations. Passes which; either must be run or do not modify the IR do not perform this check and are; therefore never skipped. Generally, this means analysis passes, passes; that are run at CodeGenOptLevel::None and passes which are required for register; allocation. The -opt-bisect-limit option can be used with any tool, including front ends; such as clang, that uses the core LLVM library for optimization and code; generation. The exact syntax for invoking the option is discussed below. This feature is not intended to replace other debugging tools such as bugpoint.; Rather it provides an alternate course of action when reproducing the problem; requires a complex build infrastructure that would make using bugpoint; impractical or when reproducing the failure requires a sequence of; transformations that is difficult to replicate with tools like opt and llc. Getting Started; ===============. The -opt-bisect-limit command line option can be passed directly to tools such; as opt, llc and lli. The syntax is as follows:. ::. <tool name> [other options] -opt-bisect-limit=<limit>. If a value of -1 is used the tool will perform all optimizations but a message; will be printed to stderr for each optimization that could be skipped; in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst:87,optimiz,optimization,87,interpreter/llvm-project/llvm/docs/OptBisect.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OptBisect.rst,6,"['optimiz', 'perform']","['optimization', 'optimizations', 'perform', 'performing']"
Performance,"======================================================; LLVM Link Time Optimization: Design and Implementation; ======================================================. .. contents::; :local:. Description; ===========. LLVM features powerful intermodular optimizations which can be used at link; time. Link Time Optimization (LTO) is another name for intermodular; optimization when performed during the link stage. This document describes the; interface and design between the LTO optimizer and the linker. Design Philosophy; =================. The LLVM Link Time Optimizer provides complete transparency, while doing; intermodular optimization, in the compiler tool chain. Its main goal is to let; the developer take advantage of intermodular optimizations without making any; significant changes to the developer's makefiles or build system. This is; achieved through tight integration with the linker. In this model, the linker; treats LLVM bitcode files like native object files and allows mixing and; matching among them. The linker uses `libLTO`_, a shared object, to handle LLVM; bitcode files. This tight integration between the linker and LLVM optimizer; helps to do optimizations that are not possible in other models. The linker; input allows the optimizer to avoid relying on conservative escape analysis. .. _libLTO-example:. Example of link time optimization; ---------------------------------. The following example illustrates the advantages of LTO's integrated approach; and clean interface. This example requires a system linker which supports LTO; through the interface described in this document. Here, clang transparently; invokes system linker. * Input source file ``a.c`` is compiled into LLVM bitcode form.; * Input source file ``main.c`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:254,optimiz,optimizations,254,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,6,"['optimiz', 'perform']","['optimization', 'optimizations', 'optimizer', 'performed']"
Performance,"=======================================================; Building a JIT: Starting out with KaleidoscopeJIT; =======================================================. .. contents::; :local:. Chapter 1 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 1 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; tutorial runs through the implementation of a JIT compiler using LLVM's; On-Request-Compilation (ORC) APIs. It begins with a simplified version of the; KaleidoscopeJIT class used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials and then; introduces new features like concurrent compilation, optimization, lazy; compilation and remote execution. The goal of this tutorial is to introduce you to LLVM's ORC JIT APIs, show how; these APIs interact with other parts of LLVM, and to teach you how to recombine; them to build a custom JIT that is suited to your use-case. The structure of the tutorial is:. - Chapter #1: Investigate the simple KaleidoscopeJIT class. This will; introduce some of the basic concepts of the ORC JIT APIs, including the; idea of an ORC *Layer*. - `Chapter #2 <BuildingAJIT2.html>`_: Extend the basic KaleidoscopeJIT by adding; a new layer that will optimize IR and generated code. - `Chapter #3 <BuildingAJIT3.html>`_: Further extend the JIT by adding a; Compile-On-Demand layer to lazily compile IR. - `Chapter #4 <BuildingAJIT4.html>`_: Improve the laziness of our JIT by; replacing the Compile-On-Demand layer with a custom layer that uses the ORC; Compile Callbacks API directly to defer IR-generation until functions are; called. - `Chapter #5 <BuildingAJIT5.html>`_: Add process isolation by JITing code into; a remote process with reduced privileges using the JIT Remote APIs. To provide input for our JIT we will us",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:821,concurren,concurrent,821,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,2,"['concurren', 'optimiz']","['concurrent', 'optimization']"
Performance,"=======================================================; Kaleidoscope: Extending the Language: Mutable Variables; =======================================================. .. contents::; :local:. Chapter 7 Introduction; ======================. Welcome to Chapter 7 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. In chapters 1 through 6, we've built a; very respectable, albeit simple, `functional programming; language <http://en.wikipedia.org/wiki/Functional_programming>`_. In our; journey, we learned some parsing techniques, how to build and represent; an AST, how to build LLVM IR, and how to optimize the resultant code as; well as JIT compile it. While Kaleidoscope is interesting as a functional language, the fact; that it is functional makes it ""too easy"" to generate LLVM IR for it. In; particular, a functional language makes it very easy to build LLVM IR; directly in `SSA; form <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; Since LLVM requires that the input code be in SSA form, this is a very; nice property and it is often unclear to newcomers how to generate code; for an imperative language with mutable variables. The short (and happy) summary of this chapter is that there is no need; for your front-end to build SSA form: LLVM provides highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:620,optimiz,optimize,620,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['optimiz'],['optimize']
Performance,"=============================================================; How To Build Clang and LLVM with Profile-Guided Optimizations; =============================================================. Introduction; ============. PGO (Profile-Guided Optimization) allows your compiler to better optimize code; for how it actually runs. Users report that applying this to Clang and LLVM can; decrease overall compile time by 20%. This guide walks you through how to build Clang with PGO, though it also applies; to other subprojects, such as LLD. If you want to build other software with PGO, see the `end-user documentation; for PGO <https://clang.llvm.org/docs/UsersManual.html#profile-guided-optimization>`_. Using preconfigured CMake caches; ================================. See https://llvm.org/docs/AdvancedBuilds.html#multi-stage-pgo. Using the script; ================. We have a script at ``utils/collect_and_build_with_pgo.py``. This script is; tested on a few Linux flavors, and requires a checkout of LLVM, Clang, and; compiler-rt. Despite the name, it performs four clean builds of Clang, so it; can take a while to run to completion. Please see the script's ``--help`` for; more information on how to run it, and the different options available to you.; If you want to get the most out of PGO for a particular use-case (e.g. compiling; a specific large piece of software), please do read the section below on; 'benchmark' selection. Please note that this script is only tested on a few Linux distros. Patches to; add support for other platforms, as always, are highly appreciated. :). This script also supports a ``--dry-run`` option, which causes it to print; important commands instead of running them. Selecting 'benchmarks'; ======================. PGO does best when the profiles gathered represent how the user plans to use the; compiler. Notably, highly accurate profiles of llc building x86_64 code aren't; incredibly helpful if you're going to be targeting ARM. By default, the script above ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst:282,optimiz,optimize,282,interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,3,"['cache', 'optimiz']","['caches', 'optimization', 'optimize']"
Performance,"==================================================================; Getting Started with the LLVM System using Microsoft Visual Studio; ==================================================================. .. contents::; :local:. Overview; ========; Welcome to LLVM on Windows! This document only covers LLVM on Windows using; Visual Studio, not WSL, mingw or cygwin. In order to get started, you first need; to know some basic information. There are many different projects that compose LLVM. The first piece is the; LLVM suite. This contains all of the tools, libraries, and header files needed; to use LLVM. It contains an assembler, disassembler, bitcode analyzer and; bitcode optimizer. It also contains basic regression tests that can be used to; test the LLVM tools and the Clang front end. The second piece is the `Clang <https://clang.llvm.org/>`_ front end. This; component compiles C, C++, Objective C, and Objective C++ code into LLVM; bitcode. Clang typically uses LLVM libraries to optimize the bitcode and emit; machine code. LLVM fully supports the COFF object file format, which is; compatible with all other existing Windows toolchains. There are more LLVM projects which this document does not discuss. Requirements; ============; Before you begin to use the LLVM system, review the requirements given; below. This may save you some trouble by knowing ahead of time what hardware; and software you will need. Hardware; --------; Any system that can adequately run Visual Studio 2019 is fine. The LLVM; source tree including the git index consumes approximately 3GB.; Object files, libraries and executables consume approximately 5GB in; Release mode and much more in Debug mode. SSD drive and >16GB RAM are; recommended. Software; --------; You will need `Visual Studio <https://visualstudio.microsoft.com/>`_ 2019 or; later, with the latest Update installed. Visual Studio Community Edition; suffices. You will also need the `CMake <http://www.cmake.org/>`_ build system since it; ge",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStartedVS.rst:679,optimiz,optimizer,679,interpreter/llvm-project/llvm/docs/GettingStartedVS.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStartedVS.rst,2,['optimiz'],"['optimize', 'optimizer']"
Performance,"===================================================================; Cross-compilation using Clang; ===================================================================. Introduction; ============. This document will guide you in choosing the right Clang options; for cross-compiling your code to a different architecture. It assumes you; already know how to compile the code in question for the host architecture,; and that you know how to choose additional include and library paths. However, this document is *not* a ""how to"" and won't help you setting your; build system or Makefiles, nor choosing the right CMake options, etc.; Also, it does not cover all the possible options, nor does it contain; specific examples for specific architectures. For a concrete example, the; `instructions for cross-compiling LLVM itself; <https://llvm.org/docs/HowToCrossCompileLLVM.html>`_ may be of interest. After reading this document, you should be familiar with the main issues; related to cross-compilation, and what main compiler options Clang provides; for performing cross-compilation. Cross compilation issues; ========================. In GCC world, every host/target combination has its own set of binaries,; headers, libraries, etc. So, it's usually simple to download a package; with all files in, unzip to a directory and point the build system to; that compiler, that will know about its location and find all it needs to; when compiling your code. On the other hand, Clang/LLVM is natively a cross-compiler, meaning that; one set of programs can compile to all targets by setting the ``-target``; option. That makes it a lot easier for programmers wishing to compile to; different platforms and architectures, and for compiler developers that; only have to maintain one build system, and for OS distributions, that; need only one set of main packages. But, as is true to any cross-compiler, and given the complexity of; different architectures, OS's and options, it's not always easy finding; the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CrossCompilation.rst:1053,perform,performing,1053,interpreter/llvm-project/clang/docs/CrossCompilation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CrossCompilation.rst,1,['perform'],['performing']
Performance,"=ON \; -DTEST_SUITE_USE_IR_PGO=ON \; -DTEST_SUITE_RUN_TYPE=train \; ../test-suite; % make; % llvm-lit .; # Use the profile data for compilation and actual benchmark run:; % cmake -DTEST_SUITE_PROFILE_GENERATE=OFF \; -DTEST_SUITE_PROFILE_USE=ON \; -DTEST_SUITE_RUN_TYPE=ref \; .; % make; % llvm-lit -o result.json .; ```. To use Clang frontend's PGO instead of LLVM IR PGO, set `-DTEST_SUITE_USE_IR_PGO=OFF`. The `TEST_SUITE_RUN_TYPE` setting only affects the SPEC benchmark suites. Cross Compilation and External Devices; --------------------------------------. ### Compilation. CMake allows to cross compile to a different target via toolchain files. More; information can be found here:. - [https://llvm.org/docs/lnt/tests.html#cross-compiling](https://llvm.org/docs/lnt/tests.html#cross-compiling). - [https://cmake.org/cmake/help/latest/manual/cmake-toolchains.7.html](https://cmake.org/cmake/help/latest/manual/cmake-toolchains.7.html). Cross compilation from macOS to iOS is possible with the; `test-suite/cmake/caches/target-target-*-iphoneos-internal.cmake` CMake cache; files; this requires an internal iOS SDK. ### Running. There are two ways to run the tests in a cross compilation setting:. - Via SSH connection to an external device: The `TEST_SUITE_REMOTE_HOST` option; should be set to the SSH hostname. The executables and data files need to be; transferred to the device after compilation. This is typically done via the; `rsync` make target. After this, the lit runner can be used on the host; machine. It will prefix the benchmark and verification command lines with an; `ssh` command. Example:. ```bash; % cmake -G Ninja -D CMAKE_C_COMPILER=path/to/clang \; -C ../test-suite/cmake/caches/target-arm64-iphoneos-internal.cmake \; -D CMAKE_BUILD_TYPE=Release \; -D TEST_SUITE_REMOTE_HOST=mydevice \; ../test-suite; % ninja; % ninja rsync; % llvm-lit -j1 -o result.json .; ```. - You can specify a simulator for the target machine with the; `TEST_SUITE_RUN_UNDER` setting. The lit run",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md:11408,cache,caches,11408,interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,1,['cache'],['caches']
Performance,"=init; src:bad/init/files/*=init. Suppressing memory leaks; ------------------------. Memory leak reports produced by :doc:`LeakSanitizer` (if it is run as a part; of AddressSanitizer) can be suppressed by a separate file passed as. .. code-block:: bash. LSAN_OPTIONS=suppressions=MyLSan.supp. which contains lines of the form `leak:<pattern>`. Memory leak will be; suppressed if pattern matches any function name, source file name, or; library name in the symbolized stack trace of the leak report. See; `full documentation; <https://github.com/google/sanitizers/wiki/AddressSanitizerLeakSanitizer#suppressions>`_; for more details. Code generation control; =======================. Instrumentation code outlining; ------------------------------. By default AddressSanitizer inlines the instrumentation code to improve the; run-time performance, which leads to increased binary size. Using the; (clang flag ``-fsanitize-address-outline-instrumentation` default: ``false``); flag forces all code instrumentation to be outlined, which reduces the size; of the generated code, but also reduces the run-time performance. Limitations; ===========. * AddressSanitizer uses more real memory than a native run. Exact overhead; depends on the allocations sizes. The smaller the allocations you make the; bigger the overhead is.; * AddressSanitizer uses more stack memory. We have seen up to 3x increase.; * On 64-bit platforms AddressSanitizer maps (but not reserves) 16+ Terabytes of; virtual address space. This means that tools like ``ulimit`` may not work as; usually expected.; * Static linking of executables is not supported. Supported Platforms; ===================. AddressSanitizer is supported on:. * Linux i386/x86\_64 (tested on Ubuntu 12.04); * macOS 10.7 - 10.11 (i386/x86\_64); * iOS Simulator; * Android ARM; * NetBSD i386/x86\_64; * FreeBSD i386/x86\_64 (tested on FreeBSD 11-current); * Windows 8.1+ (i386/x86\_64). Ports to various other platforms are in progress. Current Status; ========",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AddressSanitizer.rst:12188,perform,performance,12188,interpreter/llvm-project/clang/docs/AddressSanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AddressSanitizer.rst,1,['perform'],['performance']
Performance,"> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.vp.load.*``' intrinsic is the vector length predicated version of; the :ref:`llvm.masked.load <int_mload>` intrinsic. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is a; vector of boolean values with the same number of elements as the return type.; The third is the explicit vector length of the operation. The return type and; underlying type of the base pointer are the same vector types. The :ref:`align <attr_align>` parameter attribute can be provided for the first; operand. Semantics:; """""""""""""""""""". The '``llvm.vp.load``' intrinsic reads a vector from memory in the same way as; the '``llvm.masked.load``' intrinsic, where the mask is taken from the; combination of the '``mask``' and '``evl``' operands in the usual VP way.; Certain '``llvm.masked.load``' operands do not have corresponding operands in; '``llvm.vp.load``': the '``passthru``' operand is implicitly ``poison``; the; '``alignment``' operand is taken as the ``align`` parameter attribute, if; provided. The default alignment is taken as the ABI alignment of the return; type as specified by the :ref:`datalayout string<langref_datalayout>`. Examples:; """""""""""""""""". .. code-block:: text. %r = call <8 x i8> @llvm.vp.load.v8i8.p0(ptr align 2 %ptr, <8 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %also.r = call <8 x i8> @llvm.masked.load.v8i8.p0(ptr %ptr, i32 2, <8 x i1> %mask, <8 x i8> poison). .. _int_vp_store:. '``llvm.vp.store``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare void @llvm.vp.store.v4f32.p0(<4 x float> %val, ptr %ptr, <4 x i1> %mask, i32 %evl); declare void @llvm.vp.store.nxv2i16.p0(<vscale x 2 x i16> %val, ptr %ptr, <vscale x 2 x i1> %mask, i32 %evl); declare void @llvm.vp.store.v8f32.p1(<8 x float> %val, ptr addrspace(1) %ptr, <8 x i1> %mask, i32 %evl); declare void @llvm.vp.s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:784315,load,load,784315,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"> asi = 0; // asi not currently used; bits<5> rs2;; let op = opVal;; let op3 = op3val;; let Inst{13} = 0; // i field = 0; let Inst{12-5} = asi; // address space identifier; let Inst{4-0} = rs2;; }. ``F3_1`` assigns a value to ``op`` and ``op3`` fields, and defines the ``rs2``; field. Therefore, a ``F3_1`` format instruction will require a definition for; ``rd``, ``rs1``, and ``rs2`` in order to fully specify the instruction encoding. The ``XNORrr`` instruction then provides those three operands in its; OutOperandList and InOperandList, which bind to the corresponding fields, and; thus complete the instruction encoding. For some instructions, a single operand may contain sub-operands. As shown; earlier, the instruction ``LDrr`` uses an input operand of type ``MEMrr``. This; operand type contains two register sub-operands, defined by the; ``MIOperandInfo`` value to be ``(ops IntRegs, IntRegs)``. .. code-block:: text. def LDrr : F3_1 <3, 0b000000, (outs IntRegs:$rd), (ins (MEMrr $rs1, $rs2):$addr),; ""ld [$addr], $dst"",; [(set i32:$dst, (load ADDRrr:$addr))]>;. As this instruction is also the ``F3_1`` format, it will expect operands named; ``rd``, ``rs1``, and ``rs2`` as well. In order to allow this, a complex operand; can optionally give names to each of its sub-operands. In this example; ``MEMrr``'s first sub-operand is named ``$rs1``, the second ``$rs2``, and the; operand as a whole is also given the name ``$addr``. When a particular instruction doesn't use all the operands that the instruction; format defines, a constant value may instead be bound to one or all. For; example, the ``RDASR`` instruction only takes a single register operand, so we; assign a constant zero to ``rs2``:. .. code-block:: text. let rs2 = 0 in; def RDASR : F3_1<2, 0b101000,; (outs IntRegs:$rd), (ins ASRRegs:$rs1),; ""rd $rs1, $rd"", []>;. Instruction Operand Name Mapping; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. TableGen will also generate a function called getNamedOperandIdx() which; can be used to lo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:39202,load,load,39202,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['load'],['load']
Performance,"> cppyy.cppdef(r""""""\; ... void process_data(double) { std::cerr << ""processing double\n""; }; ... void process_data(int32_t) { std::cerr << ""processing int\n""; }""""""); True; >>> cppyy.gbl.process_data(2**32) # too large for int32_t type; processing double; >>>. There are two rounds to run-time overload resolution.; The first round considers all overloads in sorted order, with promotion but; no implicit conversion allowed.; The sorting is based on priority scores of each overload.; Higher priority is given to overloads with argument types that can be; promoted or align better with Python types.; E.g. ``int`` is preferred over ``double`` and ``double`` is preferred over; ``float``.; If argument conversion fails for all overloads during this round *and* at; least one argument converter has indicated that it can do implicit; conversion, a second round is tried where implicit conversion, including; instantiation of temporaries, is allowed.; The implicit creation of temporaries, although convenient, can be costly in; terms of run-time performance. During some template calls, implicit conversion is not allowed, giving; preference to new instantiations (as is the case in C++).; If, however, a previously instantiated overload is available and would match; with promotion, it is preferred over a (costly) new instantiation, unless a; template overload is explicitly selected using template arguments.; For example:. .. code-block:: python. >>> cppyy.cppdef(r""""""\; ... template<typename T>; ... T process_T(T t) { return t; }""""""); True; >>> type(cppyy.gbl.process_T(1.0)); <class 'float'>; >>> type(cppyy.gbl.process_T(1)) # selects available ""double"" overload; <class 'float'>; >>> type(cppyy.gbl.process_T[int](1)) # explicit selection of ""int"" overload; <class 'int'>; >>>. The template parameters used for instantiation can depend on the argument; values.; For example, if the type of an argument is Python ``int``, but its value is; too large for a 4-byte C++ ``int``, the template may be ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/functions.rst:7607,perform,performance,7607,bindings/pyroot/cppyy/cppyy/doc/source/functions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/functions.rst,1,['perform'],['performance']
Performance,"> poison. .. _int_vp_nearbyint:. '``llvm.vp.nearbyint.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.nearbyint.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.nearbyint.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.nearbyint.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point nearbyint of a vector of floating-point values. Arguments:; """""""""""""""""""". The first operand and the result have the same vector of floating-point type.; The second operand is the vector mask and has the same number of elements as the; result vector type. The third operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.nearbyint``' intrinsic performs floating-point nearbyint; (:ref:`nearbyint <int_nearbyint>`) of the first vector operand on each enabled lane.; The result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.nearbyint.v4f32(<4 x float> %a, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.nearbyint.v4f32(<4 x float> %a); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_round:. '``llvm.vp.round.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.round.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.round.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.round.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floatin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:826439,perform,performs,826439,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"> {; };; public:; my_stream2() {; this->init(new my_streambuf);; }; };. void test() {; my_stream1<char> *p1 = new my_stream1<char>;; my_stream2<char> *p2 = new my_stream2<char>;; p1->narrow('a', 'b'); // warn; p2->narrow('a', 'b'); // ok; }. undefbehavior.MinusOnePosType; (C++); Undefined behavior: passing -1 to any streambuf/; istream/ostream member that accepts a value of; type traits::pos_type result in undefined behavior.; Source: C++03 27.4.3.2p3; C++11 27.5.4.2p3. #include <fstream>. class my_streambuf : public std::streambuf {; void f() {; seekpos(-1); // warn; }; };. #include <fstream>. void test() {; std::filebuf fb;; std::istream in(&fb);; std::filebuf::off_type pos(-1);; in.seekg(pos); // warn; }. different. Name, DescriptionExampleProgress. different.SuccessiveAssign; (C); Successive assign to a variable. int test() {; int i;; i=1;; i=2; // warn; return i;; }. different.NullDerefStmtOrder; (C); Dereferencing of the null pointer might take place. Checking the pointer for; null should be performed first.; Note: possibly an enhancement to ; core.NullDereference. struct S {; int x;; };. struct S* f();. void test() {; struct S *p1 = f();; int x1 = p1->x; // warn; if (p1) {};. struct S *p2 = f();; int x2 = p2->x; // ok; }. different.NullDerefCondOrder; (C); Dereferencing of the null pointer might take place. Checking the pointer for; null should be performed first.; Note: possibly an enhancement to ; core.NullDereference. struct S {int i;};. struct S* f();. void test() {; struct S *p = f();; if (p->i && p) {}; // warn; }. different.MultipleAccessors; (C++); Identical accessor bodies. Possibly a misprint. class A {; int i;; int j;; public:; int getI() { return i; }; int getJ() { return i; } // warn; };. class A {; int i;; int j;; public:; void setI(int& ii) { i = ii; }; void setJ(int& jj) { i = jj; } // warn; };. different.AccessorsForPublic; (C++); Accessors exist for a public class field. Should this field really be; public?. class A {; public:; int i; // war",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html:19923,perform,performed,19923,interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,2,['perform'],['performed']
Performance,">' must be a pointer to; that type. If the ``cmpxchg`` is marked as ``volatile``, then the; optimizer is not allowed to modify the number or order of execution of; this ``cmpxchg`` with other :ref:`volatile operations <volatile>`. The success and failure :ref:`ordering <ordering>` arguments specify how this; ``cmpxchg`` synchronizes with other atomic operations. Both ordering parameters; must be at least ``monotonic``, the failure ordering cannot be either; ``release`` or ``acq_rel``. A ``cmpxchg`` instruction can also take an optional; "":ref:`syncscope <syncscope>`"" argument. Note: if the alignment is not greater or equal to the size of the `<value>`; type, the atomic operation is likely to require a lock and have poor; performance. The alignment is only optional when parsing textual IR; for in-memory IR, it is; always present. If unspecified, the alignment is assumed to be equal to the; size of the '<value>' type. Note that this default alignment assumption is; different from the alignment used for the load/store instructions when align; isn't specified. The pointer passed into cmpxchg must have alignment greater than or; equal to the size in memory of the operand. Semantics:; """""""""""""""""""". The contents of memory at the location specified by the '``<pointer>``' operand; is read and compared to '``<cmp>``'; if the values are equal, '``<new>``' is; written to the location. The original value at the location is returned,; together with a flag indicating success (true) or failure (false). If the cmpxchg operation is marked as ``weak`` then a spurious failure is; permitted: the operation may not write ``<new>`` even if the comparison; matched. If the cmpxchg operation is strong (the default), the i1 value is 1 if and only; if the value loaded equals ``cmp``. A successful ``cmpxchg`` is a read-modify-write instruction for the purpose of; identifying release sequences. A failed ``cmpxchg`` is equivalent to an atomic; load with an ordering parameter determined the second ord",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:427518,load,load,427518,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,">* %tmp; 	%s = bitcast <16 x float>* %tmp to i8*; 	%s2 = bitcast <16 x float>* %tmp2 to i8*; 	call void @llvm.memcpy.i64(i8* %s, i8* %s2, i64 64, i32 16); 	%R = load <16 x float>* %tmp2; 	ret <16 x float> %R; }. declare void @llvm.memcpy.i64(i8* nocapture, i8* nocapture, i64, i32) nounwind. which compiles to:. _foo:; 	subl	$140, %esp; 	movaps	%xmm3, 112(%esp); 	movaps	%xmm2, 96(%esp); 	movaps	%xmm1, 80(%esp); 	movaps	%xmm0, 64(%esp); 	movl	60(%esp), %eax; 	movl	%eax, 124(%esp); 	movl	56(%esp), %eax; 	movl	%eax, 120(%esp); 	movl	52(%esp), %eax; <many many more 32-bit copies>; 	movaps	(%esp), %xmm0; 	movaps	16(%esp), %xmm1; 	movaps	32(%esp), %xmm2; 	movaps	48(%esp), %xmm3; 	addl	$140, %esp; 	ret. On Nehalem, it may even be cheaper to just use movups when unaligned than to; fall back to lower-granularity chunks. //===---------------------------------------------------------------------===//. Implement processor-specific optimizations for parity with GCC on these; processors. GCC does two optimizations:. 1. ix86_pad_returns inserts a noop before ret instructions if immediately; preceded by a conditional branch or is the target of a jump.; 2. ix86_avoid_jump_misspredicts inserts noops in cases where a 16-byte block of; code contains more than 3 branches.; ; The first one is done for all AMDs, Core2, and ""Generic""; The second one is done for: Atom, Pentium Pro, all AMDs, Pentium 4, Nocona,; Core 2, and ""Generic"". //===---------------------------------------------------------------------===//; Testcase:; int x(int a) { return (a&0xf0)>>4; }. Current output:; 	movl	4(%esp), %eax; 	shrl	$4, %eax; 	andl	$15, %eax; 	ret. Ideal output:; 	movzbl	4(%esp), %eax; 	shrl	$4, %eax; 	ret. //===---------------------------------------------------------------------===//. Re-implement atomic builtins __sync_add_and_fetch() and __sync_sub_and_fetch; properly. When the return value is not used (i.e. only care about the value in the; memory), x86 does not have to use add to implement these. In",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:32831,optimiz,optimizations,32831,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,2,['optimiz'],['optimizations']
Performance,">, <4 x i1> <mask>, i32 <vector_length>); declare i16 @llvm.vp.reduce.or.nxv8i16(i16 <start_value>, <vscale x 8 x i16> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated integer ``OR`` reduction of a vector and a scalar starting value,; returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; integer type equal to the result type. The second operand is the vector on; which the reduction is performed and must be a vector of integer values whose; element type is the result/start type. The third operand is the vector mask and; is a vector of boolean values with the same number of elements as the vector; operand. The fourth operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.or``' intrinsic performs the integer ``OR`` reduction; (:ref:`llvm.vector.reduce.or <int_vector_reduce_or>`) of the vector operand; ``val`` on each enabled lane, performing an '``or``' of that with the scalar; ``start_value``. Disabled lanes are treated as containing the neutral value; ``0`` (i.e. having no effect on the reduction operation). If the vector length; is zero, the result is the start value. To ignore the start value, the neutral value can be used. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call i32 @llvm.vp.reduce.or.v4i32(i32 %start, <4 x i32> %a, <4 x i1> %mask, i32 %evl); ; %r is equivalent to %also.r, where lanes greater than or equal to %evl; ; are treated as though %mask were false for those lanes. %masked.a = select <4 x i1> %mask, <4 x i32> %a, <4 x i32> <i32 0, i32 0, i32 0, i32 0>; %reduction = call i32 @llvm.vector.reduce.or.v4i32(<4 x i32> %masked.a); %also.r = or i32 %reduction, %start. .. _int_vp_reduce_xor:. '``llvm.vp.reduce.xor.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.vp.reduce.xor.v4i32(i32 <start_value>, <4 x i32",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:759917,perform,performing,759917,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performing']
Performance,">. void test() {; STARTUPINFO si;; PROCESS_INFORMATION pi;; CreateProcess(NULL, TEXT(""C:\\Program Files\\App -L -S""),; NULL, NULL, TRUE, 0, NULL, NULL, &si, π);; // warn; }. WinAPI.LoadLibrary; (C); The SearchPath() function is used to retrieve a path to a DLL for; a subsequent LoadLibrary() call.; Source: ; MSDN: LoadLibrary function, Security Remarks. #include <windows.h>. HINSTANCE test() {; char filePath[100];; SearchPath(NULL, ""file.dll"", NULL, 100, filePath, NULL);; return LoadLibrary(filePath); // warn; }. WinAPI.WideCharToMultiByte; (C); Buffer overrun while calling WideCharToMultiByte(). The size of; the input buffer equals the number of characters in the Unicode string, while; the size of the output buffer equals the number of bytes.; Source: ; MSDN: WideCharToMultiByte function. #include <windows.h>. void test() {; wchar_t ws[] = L""abc"";; char s[3];; WideCharToMultiByte(CP_UTF8, 0, ws, -1, s,; 3, NULL, NULL); // warn; }. optimization. Name, DescriptionExampleProgress. optimization.PassConstObjByValue; (C, C++); Optimization: It is more effective to pass constant parameter by reference to; avoid unnecessary object copying. struct A {};. void f(const struct A a); // warn. optimization.PostfixIncIter; (C++); Optimization: It is more effective to use prefix increment operator with; iterator.; Source: Scott Meyers ""More Effective C++"", item 6:; Distinguish between prefix and postfix forms of increment and decrement; operators. #include <vector>. void test() {; std::vector<int> v;; std::vector<int>::const_iterator it;; for(it = v.begin();; it != v.end(); it++) {}; // warn; }. optimization.MultipleCallsStrlen; (C); Optimization: multiple calls to strlen() for a string in an; expression. It is more effective to hold a value returned; from strlen() in a temporary variable. #include <string.h>. void test(const char* s) {; if (strlen(s) > 0 &&; strlen(s) < 7) {}; // warn; }. optimization.StrLengthCalculation; (C++); Optimization: it is more efficient to use string::",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html:27264,optimiz,optimization,27264,interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,2,['optimiz'],['optimization']
Performance,">> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:27474,optimiz,optimized,27474,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,">registerCallbacks(*ThePIC, TheMAM.get());; ... After initializing the global module ``TheModule`` and the FunctionPassManager,; we need to initialize other parts of the framework. The four AnalysisManagers; allow us to add analysis passes that run across the four levels of the IR; hierarchy. PassInstrumentationCallbacks and StandardInstrumentations are; required for the pass instrumentation framework, which allows developers to; customize what happens between passes. Once these managers are set up, we use a series of ""addPass"" calls to add a; bunch of LLVM transform passes:. .. code-block:: c++. // Add transform passes.; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->addPass(InstCombinePass());; // Reassociate expressions.; TheFPM->addPass(ReassociatePass());; // Eliminate Common SubExpressions.; TheFPM->addPass(GVNPass());; // Simplify the control flow graph (deleting unreachable blocks, etc).; TheFPM->addPass(SimplifyCFGPass());. In this case, we choose to add four optimization passes.; The passes we choose here are a pretty standard set; of ""cleanup"" optimizations that are useful for a wide variety of code. I won't; delve into what they do but, believe me, they are a good starting place :). Next, we register the analysis passes used by the transform passes. .. code-block:: c++. // Register analysis passes used in these transform passes.; PassBuilder PB;; PB.registerModuleAnalyses(*TheMAM);; PB.registerFunctionAnalyses(*TheFAM);; PB.crossRegisterProxies(*TheLAM, *TheFAM, *TheCGAM, *TheMAM);; }. Once the PassManager is set up, we need to make use of it. We do this by; running it after our newly created function is constructed (in; ``FunctionAST::codegen()``), but before it is returned to the client:. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder.CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. // Optimize the function.; TheFPM->",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:7427,optimiz,optimization,7427,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,1,['optimiz'],['optimization']
Performance,"? Description; ================= ============== ========= ================================; ""Name"" string Kernel argument name.; ""TypeName"" string Kernel argument type name.; ""Size"" integer Required Kernel argument size in bytes.; ""Align"" integer Required Kernel argument alignment in; bytes. Must be a power of two.; ""ValueKind"" string Required Kernel argument kind that; specifies how to set up the; corresponding argument.; Values include:. ""ByValue""; The argument is copied; directly into the kernarg. ""GlobalBuffer""; A global address space pointer; to the buffer data is passed; in the kernarg. ""DynamicSharedPointer""; A group address space pointer; to dynamically allocated LDS; is passed in the kernarg. ""Sampler""; A global address space; pointer to a S# is passed in; the kernarg. ""Image""; A global address space; pointer to a T# is passed in; the kernarg. ""Pipe""; A global address space pointer; to an OpenCL pipe is passed in; the kernarg. ""Queue""; A global address space pointer; to an OpenCL device enqueue; queue is passed in the; kernarg. ""HiddenGlobalOffsetX""; The OpenCL grid dispatch; global offset for the X; dimension is passed in the; kernarg. ""HiddenGlobalOffsetY""; The OpenCL grid dispatch; global offset for the Y; dimension is passed in the; kernarg. ""HiddenGlobalOffsetZ""; The OpenCL grid dispatch; global offset for the Z; dimension is passed in the; kernarg. ""HiddenNone""; An argument that is not used; by the kernel. Space needs to; be left for it, but it does; not need to be set up. ""HiddenPrintfBuffer""; A global address space pointer; to the runtime printf buffer; is passed in kernarg. Mutually; exclusive with; ""HiddenHostcallBuffer"". ""HiddenHostcallBuffer""; A global address space pointer; to the runtime hostcall buffer; is passed in kernarg. Mutually; exclusive with; ""HiddenPrintfBuffer"". ""HiddenDefaultQueue""; A global address space pointer; to the OpenCL device enqueue; queue that should be used by; the kernel by default is; passed in the kernarg. ""HiddenComp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:124183,queue,queue,124183,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['queue'],['queue']
Performance,"@bb.exitStub; mov r0, #3; bx lr; LBB1_8:; .align 2; LCPI1_0:; .long 642. gcc compiles to:. 	cmp	r0, #9; 	@ lr needed for prologue; 	bhi	L2; 	ldr	r3, L11; 	mov	r2, #1; 	mov	r1, r2, asl r0; 	ands	r0, r3, r2, asl r0; 	movne	r0, #2; 	bxne	lr; 	tst	r1, #13; 	beq	L9; L3:; 	mov	r0, r2; 	bx	lr; L9:; 	tst	r1, #256; 	movne	r0, #3; 	bxne	lr; L2:; 	mov	r0, #0; 	bx	lr; L12:; 	.align 2; L11:; 	.long	642; . GCC is doing a couple of clever things here:; 1. It is predicating one of the returns. This isn't a clear win though: in; cases where that return isn't taken, it is replacing one condbranch with; two 'ne' predicated instructions.; 2. It is sinking the shift of ""1 << i"" into the tst, and using ands instead of; tst. This will probably require whole function isel.; 3. GCC emits:; 	tst	r1, #256; we emit:; mov r1, #1; lsl r1, r1, #8; tst r2, r1. //===---------------------------------------------------------------------===//. When spilling in thumb mode and the sp offset is too large to fit in the ldr /; str offset field, we load the offset from a constpool entry and add it to sp:. ldr r2, LCPI; add r2, sp; ldr r2, [r2]. These instructions preserve the condition code which is important if the spill; is between a cmp and a bcc instruction. However, we can use the (potentially); cheaper sequence if we know it's ok to clobber the condition register. add r2, sp, #255 * 4; add r2, #132; ldr r2, [r2, #7 * 4]. This is especially bad when dynamic alloca is used. The all fixed size stack; objects are referenced off the frame pointer with negative offsets. See; oggenc for an example. //===---------------------------------------------------------------------===//. Poor codegen test/CodeGen/ARM/select.ll f7:. 	ldr r5, LCPI1_0; LPC0:; 	add r5, pc; 	ldr r6, LCPI1_1; 	ldr r2, LCPI1_2; 	mov r3, r6; 	mov lr, pc; 	bx r5. //===---------------------------------------------------------------------===//. Make register allocator / spiller smarter so we can re-materialize ""mov r, imm"",; etc. Almost all Thumb",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:4100,load,load,4100,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,2,['load'],['load']
Performance,"@llvm.gcwrite(i8* %value, i8* %object, i8** %derived). For write barriers, LLVM provides the ``llvm.gcwrite`` intrinsic function. It; has exactly the same semantics as a non-volatile ``store`` to the derived; pointer (the third argument). The exact code generated is specified by the; Function's selected :ref:`GC strategy <plugin>`. Many important algorithms require write barriers, including generational and; concurrent collectors. Additionally, write barriers could be used to implement; reference counting. Read barrier: ``llvm.gcread``; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: llvm. i8* @llvm.gcread(i8* %object, i8** %derived). For read barriers, LLVM provides the ``llvm.gcread`` intrinsic function. It has; exactly the same semantics as a non-volatile ``load`` from the derived pointer; (the second argument). The exact code generated is specified by the Function's; selected :ref:`GC strategy <plugin>`. Read barriers are needed by fewer algorithms than write barriers, and may have a; greater performance impact since pointer reads are more frequent than writes. .. _plugin:. .. _builtin-gc-strategies:. Built In GC Strategies; ======================. LLVM includes built in support for several varieties of garbage collectors. The Shadow Stack GC; ----------------------. To use this collector strategy, mark your functions with:. .. code-block:: c++. F.setGC(""shadow-stack"");. Unlike many GC algorithms which rely on a cooperative code generator to compile; stack maps, this algorithm carefully maintains a linked list of stack roots; [:ref:`Henderson2002 <henderson02>`]. This so-called ""shadow stack"" mirrors the; machine stack. Maintaining this data structure is slower than using a stack map; compiled into the executable as constant data, but has a significant portability; advantage because it requires no special support from the target code generator,; and does not require tricky platform-specific code to crawl the machine stack. The tradeoff for this simplicity and por",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:15737,perform,performance,15737,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['perform'],['performance']
Performance,"@print(i32 %n.addr) #4; %2 = call i8 @llvm.coro.suspend(token none, i1 false); switch i8 %2, label %suspend [i8 0, label %loop.resume; i8 1, label %cleanup]; loop.resume:; %inc = add nsw i32 %n.addr, 1; %sub = xor i32 %n.addr, -1; call void @print(i32 %sub); %3 = call i8 @llvm.coro.suspend(token none, i1 false); switch i8 %3, label %suspend [i8 0, label %loop; i8 1, label %cleanup]. In this case, the coroutine frame would include a suspend index that will; indicate at which suspend point the coroutine needs to resume. .. code-block:: llvm. %f.frame = type { ptr, ptr, i32, i32 }. The resume function will use an index to jump to an appropriate basic block and will look; as follows:. .. code-block:: llvm. define internal fastcc void @f.Resume(ptr %FramePtr) {; entry.Resume:; %index.addr = getelementptr inbounds %f.Frame, ptr %FramePtr, i64 0, i32 2; %index = load i8, ptr %index.addr, align 1; %switch = icmp eq i8 %index, 0; %n.addr = getelementptr inbounds %f.Frame, ptr %FramePtr, i64 0, i32 3; %n = load i32, ptr %n.addr, align 4. br i1 %switch, label %loop.resume, label %loop. loop.resume:; %sub = sub nsw i32 0, %n; call void @print(i32 %sub); br label %suspend; loop:; %inc = add nsw i32 %n, 1; store i32 %inc, ptr %n.addr, align 4; tail call void @print(i32 %inc); br label %suspend. suspend:; %storemerge = phi i8 [ 0, %loop ], [ 1, %loop.resume ]; store i8 %storemerge, ptr %index.addr, align 1; ret void; }. If different cleanup code needs to get executed for different suspend points,; a similar switch will be in the `f.destroy` function. .. note ::. Using suspend index in a coroutine state and having a switch in `f.resume` and; `f.destroy` is one of the possible implementation strategies. We explored; another option where a distinct `f.resume1`, `f.resume2`, etc. are created for; every suspend point, and instead of storing an index, the resume and destroy; function pointers are updated at every suspend. Early testing showed that the; current approach is easier on the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst:19418,load,load,19418,interpreter/llvm-project/llvm/docs/Coroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst,1,['load'],['load']
Performance,"A specific shape can be created; stand-alone:. ``` {.cpp}; TGeoBBox *box = new TGeoBBox(""s_box"",halfX,halfY,halfZ); // named; TGeoTube *tub = new TGeoTube(rmin,rmax,halfZ); // no name; //... (See all specific shape constructors); ```. Sometimes it is much easier to create a volume having a given shape in; one step, since shapes are not directly linked in the geometrical tree; but volumes are:. ``` {.cpp}; TGeoVolume *vol_box = gGeoManager->MakeBox(""BOX_VOL"",pmed,halfX,; halfY,halfZ);; TGeoVolume *vol_tub = gGeoManager->MakeTube(""TUB_VOL"",pmed,rmin,; rmax,halfZ);; // ...(See MakeXXX() utilities in TGeoManager class); ```. ### Dividing Shapes. Shapes can generally be divided along a given axis. Supported axes are:; `X`, `Y`, `Z`, `Rxy`, `Phi`, `Rxyz`. A given shape cannot be divided; however on any axis. The general rule is that that divisions are; possible on whatever axis that produces still known shapes as slices.; The division of shapes are performed by the call `TGeoShape::Divide()`,; but this operation can be done only via `TGeoVolume::Divide()` method.; In other words, the algorithm for dividing a specific shape is known by; the shape object, but is always invoked in a generic way from the volume; level. Details on how to do that can be found in the paragraph ‘Dividing; volumes'. One can see how all division options are interpreted and which; their result inside specific shape classes is. ### Parametric Shapes. Shapes generally have a set of parameters that is well defined at build; time. In fact, when the final geometrical hierarchy is assembled and the; geometry is closed, all constituent shapes `MUST`**have well defined and; valid parameters. In order to ease-up geometry creation, some; parameterizations are however allowed. For instance let's suppose that we need to define several volumes having; exactly the same properties but different sizes. A way to do this would; be to create as many different volumes and shapes. The modeller allows; however the definit",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:53950,perform,performed,53950,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performed']
Performance,"AG-based instruction selection consists of the following steps:. #. `Build initial DAG`_ --- This stage performs a simple translation from the; input LLVM code to an illegal SelectionDAG. #. `Optimize SelectionDAG`_ --- This stage performs simple optimizations on the; SelectionDAG to simplify it, and recognize meta instructions (like rotates; and ``div``/``rem`` pairs) for targets that support these meta operations.; This makes the resultant code more efficient and the `select instructions; from DAG`_ phase (below) simpler. #. `Legalize SelectionDAG Types`_ --- This stage transforms SelectionDAG nodes; to eliminate any types that are unsupported on the target. #. `Optimize SelectionDAG`_ --- The SelectionDAG optimizer is run to clean up; redundancies exposed by type legalization. #. `Legalize SelectionDAG Ops`_ --- This stage transforms SelectionDAG nodes to; eliminate any operations that are unsupported on the target. #. `Optimize SelectionDAG`_ --- The SelectionDAG optimizer is run to eliminate; inefficiencies introduced by operation legalization. #. `Select instructions from DAG`_ --- Finally, the target instruction selector; matches the DAG operations to target instructions. This process translates; the target-independent input DAG into another DAG of target instructions. #. `SelectionDAG Scheduling and Formation`_ --- The last phase assigns a linear; order to the instructions in the target-instruction DAG and emits them into; the MachineFunction being compiled. This step uses traditional prepass; scheduling techniques. After all of these steps are complete, the SelectionDAG is destroyed and the; rest of the code generation passes are run. One of the most common ways to debug these steps is using ``-debug-only=isel``,; which prints out the DAG, along with other information like debug info,; after each of these steps. Alternatively, ``-debug-only=isel-dump`` shows only; the DAG dumps, but the results can be filtered by function names using; ``-filter-print-funcs=",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:37852,optimiz,optimizer,37852,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimizer']
Performance,"ARMv8.2a and above); * AMDGPU (natively); * SPIR (natively); * X86 (if SSE2 is available; natively if AVX512-FP16 is also available); * RISC-V (natively if Zfh or Zhinx is available). * ``__bf16`` is supported on the following targets (currently never natively):. * 32-bit ARM; * 64-bit ARM (AArch64); * RISC-V; * X86 (when SSE2 is available). (For X86, SSE2 is available on 64-bit and all recent 32-bit processors.). ``__fp16`` and ``_Float16`` both use the binary16 format from IEEE; 754-2008, which provides a 5-bit exponent and an 11-bit significand; (counting the implicit leading 1). ``__bf16`` uses the `bfloat16; <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format>`_ format,; which provides an 8-bit exponent and an 8-bit significand; this is the same; exponent range as `float`, just with greatly reduced precision. ``_Float16`` and ``__bf16`` follow the usual rules for arithmetic; floating-point types. Most importantly, this means that arithmetic operations; on operands of these types are formally performed in the type and produce; values of the type. ``__fp16`` does not follow those rules: most operations; immediately promote operands of type ``__fp16`` to ``float``, and so; arithmetic operations are defined to be performed in ``float`` and so result in; a value of type ``float`` (unless further promoted because of other operands).; See below for more information on the exact specifications of these types. When compiling arithmetic on ``_Float16`` and ``__bf16`` for a target without; native support, Clang will perform the arithmetic in ``float``, inserting; extensions and truncations as necessary. This can be done in a way that; exactly matches the operation-by-operation behavior of native support,; but that can require many extra truncations and extensions. By default,; when emulating ``_Float16`` and ``__bf16`` arithmetic using ``float``, Clang; does not truncate intermediate operands back to their true type unless the; operand is the result of an explic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:31793,perform,performed,31793,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['perform'],['performed']
Performance,"ARS)`. Going; on and opening our packs, we will obviously find out some empty ones,; otherwise, something is very wrong... We will call these leaves (by; analogy with a tree structure). On the other hand, any volume is a small world by itself - what we need; to do is to take it out and to ignore all the rest since it is a; self-contained object. In fact, the modeller can act like this,; considering a given volume as temporary MARS, but we will describe this; feature later on. Let us focus on the biggest pack - it is mandatory to; define one. Consider the simplest geometry that is made of a single box.; Here is an example on how to build it:. ### Example 1: Creating the World. We first need to load the geometry library. This is not needed if one; does `make map` in root folder. ``` {.cpp}; root[] gSystem->Load(""libGeom"");; ```. Second, we have to create an instance of the geometry manager class.; This takes care of all the modeller components, performing several tasks; to insure geometry validity and containing the user interface for; building and interacting with the geometry. After its creation, the; geometry manager class can be accessed with the global; ***`gGeoManager`***:. ``` {.cpp}; root[] new TGeoManager(""world"", ""the simplest geometry"");; ```. We want to create a single volume in our geometry, but since any volume; needs to have an associated medium, we will create a dummy one. You can; safely ignore the following lines for the time being, since materials; and media will be explained in detail later on. ``` {.cpp}; root[] TGeoMaterial *mat = new TGeoMaterial(""Vacuum"",0,0,0);; root[] TGeoMedium *med = new TGeoMedium(""Vacuum"",1,mat);; ```. We can finally make our volume having a box shape. Note that the world; volume does not need to be a box - it can be any other shape. Generally,; boxes and tubes are the most recommendable shapes for this purpose due; to their fast navigation algorithms. ``` {.cpp}; root[] TGeoVolume *top=gGeoManager->MakeBox(""Top"",med,10.,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:2384,perform,performing,2384,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performing']
Performance,"Activation function is now supported in the SOFIE Keras parser. ## 2D Graphics Libraries. - Introduce `TAxis::ChangeLabelByValue` to set custom label defined by axis value. It works also; when axis zooming changes and position and index of correspondent axis label changes as well.; `TAxis::ChangeLabel` method to change axis label by index works as before. - Introduce `TCanvas::SaveAll` method. Allows to store several pads at once into different image file formats.; File name can include printf qualifier to code pad number. Also allows to store all pads in single PDF; or single ROOT file. Significantly improves performance when creating many image files using web graphics. - Introduce `TCanvas::UpdateAsync` method. In case of web-based canvas triggers update of the canvas on the client side,; but does not wait that real update is completed. Avoids blocking of caller thread.; Have to be used if called from other web-based widget to avoid logical dead-locks.; In case of normal canvas just canvas->Update() is performed. - The Delaunay triangles (used by TGraph2D) were computed by the external package `triangle.c`; included in the ROOT distribution. This package had several issues:; - It was not maintained anymore.; - Its license was not compatible with LGPL; This code is now replaced by the [CDT package](https://github.com/artem-ogre/CDT) which is; properly maintained and has a license (MLP) compatible with LGPL. It will appear in 6.03.02. ## Machine Learning integration. - ROOT now offers functionality to extract batches of events out of a dataset for use in common ML training workflows. For example, one can generate PyTorch tensors from a TTree. The functionality is available through the `RBatchGenerator` class and can be seamlessly integrated in user code, for example:; ```python; # Returns two generators that return training and validation batches as PyTorch tensors.; gen_train, gen_validation = ROOT.TMVA.Experimental.CreatePyTorchGenerators(; tree_name, file_name, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v630/index.md:20213,perform,performed,20213,README/ReleaseNotes/v630/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v630/index.md,1,['perform'],['performed']
Performance,"Additional; memory dependence analysis is required to make that determination. As a fail; safe mechanism, this causes loops that were originally parallel to be considered; sequential (if optimization passes that are unaware of the parallel semantics; insert new memory instructions into the loop body). Example of a loop that is considered parallel due to its correct use of; both ``llvm.access.group`` and ``llvm.loop.parallel_accesses``; metadata types. .. code-block:: llvm. for.body:; ...; %val0 = load i32, ptr %arrayidx, !llvm.access.group !1; ...; store i32 %val0, ptr %arrayidx1, !llvm.access.group !1; ...; br i1 %exitcond, label %for.end, label %for.body, !llvm.loop !0. for.end:; ...; !0 = distinct !{!0, !{!""llvm.loop.parallel_accesses"", !1}}; !1 = distinct !{}. It is also possible to have nested parallel loops:. .. code-block:: llvm. outer.for.body:; ...; %val1 = load i32, ptr %arrayidx3, !llvm.access.group !4; ...; br label %inner.for.body. inner.for.body:; ...; %val0 = load i32, ptr %arrayidx1, !llvm.access.group !3; ...; store i32 %val0, ptr %arrayidx2, !llvm.access.group !3; ...; br i1 %exitcond, label %inner.for.end, label %inner.for.body, !llvm.loop !1. inner.for.end:; ...; store i32 %val1, ptr %arrayidx4, !llvm.access.group !4; ...; br i1 %exitcond, label %outer.for.end, label %outer.for.body, !llvm.loop !2. outer.for.end: ; preds = %for.body; ...; !1 = distinct !{!1, !{!""llvm.loop.parallel_accesses"", !3}} ; metadata for the inner loop; !2 = distinct !{!2, !{!""llvm.loop.parallel_accesses"", !3, !4}} ; metadata for the outer loop; !3 = distinct !{} ; access group for instructions in the inner loop (which are implicitly contained in outer loop as well); !4 = distinct !{} ; access group for instructions in the outer, but not the inner loop. .. _langref_llvm_loop_mustprogress:. '``llvm.loop.mustprogress``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``llvm.loop.mustprogress`` metadata indicates that this loop is required to; terminate, unwind, or inter",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:313681,load,load,313681,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the function. The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once again:. .. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass());; ... It is interesting to see what the code looks like before and after the; mem2reg optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. Here there is only one variable (x, the i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:16921,optimiz,optimization,16921,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['optimiz'],['optimization']
Performance,"By Chris:. LLVM has been designed with two primary goals in mind. First we strive to ; enable the best possible division of labor between static and dynamic ; compilers, and second, we need a flexible and powerful interface ; between these two complementary stages of compilation. We feel that ; providing a solution to these two goals will yield an excellent solution ; to the performance problem faced by modern architectures and programming ; languages. A key insight into current compiler and runtime systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:378,perform,performance,378,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,2,['perform'],['performance']
Performance,"C++.; It uses LLVM along with a C++ interpreter (e.g., Cling) to enable features like; run-time instantiation of C++ templates, cross-inheritance, callbacks,; auto-casting, transparent use of smart pointers, etc. In a nutshell, this feature enables a new way of developing code, paving the; way for language interoperability and easier interactive programming. Implementation Details; ======================. Interpreter as a REPL vs. as a Library; --------------------------------------. 1 - If we're using the interpreter in interactive (REPL) mode, it will dump; the value (i.e., value printing). .. code-block:: console. if (LastValue.isValid()) {; if (!V) {; LastValue.dump();; LastValue.clear();; } else; *V = std::move(LastValue);; }. 2 - If we're using the interpreter as a library, then it will pass the value; to the user. Incremental AST Consumer; ------------------------. The ``IncrementalASTConsumer`` class wraps the original code generator; ``ASTConsumer`` and it performs a hook, to traverse all the top-level decls, to; look for expressions to synthesize, based on the ``isSemiMissing()`` condition. If this condition is found to be true, then ``Interp.SynthesizeExpr()`` will be; invoked. **Note:** Following is a sample code snippet. Actual code may vary over time. .. code-block:: console. for (Decl *D : DGR); if (auto *TSD = llvm::dyn_cast<TopLevelStmtDecl>(D);; TSD && TSD->isSemiMissing()); TSD->setStmt(Interp.SynthesizeExpr(cast<Expr>(TSD->getStmt())));. return Consumer->HandleTopLevelDecl(DGR);. The synthesizer will then choose the relevant expression, based on its type. Communication between Compiled Code and Interpreted Code; --------------------------------------------------------. In Clang-Repl there is **interpreted code**, and this feature adds a 'value'; runtime that can talk to the **compiled code**. Following is an example where the compiled code interacts with the interpreter; code. The execution results of an expression are stored in the object 'V' of;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangRepl.rst:12013,perform,performs,12013,interpreter/llvm-project/clang/docs/ClangRepl.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangRepl.rst,1,['perform'],['performs']
Performance,"C64, it returns a; compile-time-known constant value. The return value type of :ref:`llvm.get.dynamic.area.offset <int_get_dynamic_area_offset>`; must match the target's default address space's (address space 0) pointer type. '``llvm.prefetch``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.prefetch(ptr <address>, i32 <rw>, i32 <locality>, i32 <cache type>). Overview:; """""""""""""""""". The '``llvm.prefetch``' intrinsic is a hint to the code generator to; insert a prefetch instruction if supported; otherwise, it is a noop.; Prefetches have no effect on the behavior of the program but can change; its performance characteristics. Arguments:; """""""""""""""""""". ``address`` is the address to be prefetched, ``rw`` is the specifier; determining if the fetch should be for a read (0) or write (1), and; ``locality`` is a temporal locality specifier ranging from (0) - no; locality, to (3) - extremely local keep in cache. The ``cache type``; specifies whether the prefetch is performed on the data (1) or; instruction (0) cache. The ``rw``, ``locality`` and ``cache type``; arguments must be constant integers. Semantics:; """""""""""""""""""". This intrinsic does not modify the behavior of the program. In; particular, prefetches cannot trap and do not produce a value. On; targets that support this intrinsic, the prefetch can provide hints to; the processor cache for better performance. '``llvm.pcmarker``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.pcmarker(i32 <id>). Overview:; """""""""""""""""". The '``llvm.pcmarker``' intrinsic is a method to export a Program; Counter (PC) in a region of code to simulators and other tools. The; method is target specific, but it is expected that the marker will use; exported symbols to transmit the PC of the marker. The marker makes no; guarantees that it will remain with any specific instruction after; optimizations. It is possible that the presence of a marker will inhibit; optimizations. The intende",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:523588,cache,cache,523588,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,3,"['cache', 'perform']","['cache', 'performed']"
Performance,"C; Rint.Logon: rootlogon.C; Rint.Logoff: rootlogoff.C; ...; Rint.Canvas.MoveOpaque: false; Rint.Canvas.HighLightColor: 5; ```. The various options are explained in `$ROOTSYS/etc/system.rootrc`. The; `.rootrc` file contents are combined. For example, if the flag to use; true type fonts is set to true in the `system.rootrc` file, you have; to set explicitly it false in your local `.rootrc` file if you do not; want to use true type fonts. Removing the `UseTTFonts `statement in; the local `.rootrc` file will not disable true fonts. The value of the; environment variable `ROOTDEBUG` overrides the value in the `.rootrc`; file at startup. Its value is used to set ***`gDebug`*** and helps for; quick turn on debug mode in **`TROOT`** startup. ROOT looks for scripts in the path specified in the `.rootrc` file in; the `Root.Macro.Path` variable. You can expand this path to hold your; own directories. ### Logon and Logoff Scripts. The `rootlogon.C` and `rootlogoff.C` files are scripts loaded and; executed at start-up and shutdown. The `rootalias.C` file is loaded; but not executed. It typically contains small utility functions. For; example, the `rootalias.C` script that comes with the ROOT; distributions (located in `$ROOTSYS/tutorials)` defines the function; `edit(char *file)`. This allows the user to call the editor from the; command line. This particular function will start the VI editor if the; environment variable `EDITOR` is not set. ``` {.cpp}; root[0] edit(""c1.C""); ```. For more details, see `$ROOTSYS/tutorials/rootalias.C`. ### History File. You can use the up and down arrow at the command line, to access the; previous and next command. The commands are recorded in the history; file `$HOME/.root_hist`. It is a text file, and you can edit, cut, and; paste from it. You can specify the history file in the `system.rootrc`; file, by setting the `Rint.History `option. You can also turn off the; command logging in the `system.rootrc` file with the option:; `Rint.History: -`. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/GettingStarted.md:37840,load,loaded,37840,documentation/users-guide/GettingStarted.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/GettingStarted.md,1,['load'],['loaded']
Performance,"CL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. **Acquire-Release Atomic**; ------------------------------------------------------------------------------------; atomicrmw acq_rel - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acq_rel - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acq_rel - workgroup - global 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - Must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:265785,load,load,265785,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"CMake Caches; ============. This directory contains CMake cache scripts that pre-populate the CMakeCache in; a build directory with commonly used settings. You can use the caches files with the following CMake invocation:. cmake -G <build system>; -C <path to cache file>; [additional CMake options (i.e. -DCMAKE_INSTALL_PREFIX=<install path>)]; <path to llvm>. Options specified on the command line will override options in the cache files. The following cache files exist. Apple-stage1; ------------. The Apple stage1 cache configures a two stage build similar to how Apple builds; the clang shipped with Xcode. The build files generated from this invocation has; a target named ""stage2"" which performs an LTO build of clang. The Apple-stage2 cache can be used directly to match the build settings Apple; uses in shipping builds without doing a full bootstrap build. PGO; ---. The PGO CMake cache can be used to generate a multi-stage instrumented compiler.; You can configure your build directory with the following invocation of CMake:. cmake -G <generator> -C <path_to_clang>/cmake/caches/PGO.cmake <source dir>. After configuration the following additional targets will be generated:. stage2-instrumented:; Builds a stage1 x86 compiler, runtime, and required tools (llvm-config,; llvm-profdata) then uses that compiler to build an instrumented stage2 compiler. stage2-instrumented-generate-profdata:; Depends on ""stage2-instrumented"" and will use the instrumented compiler to; generate profdata based on the training files in <clang>/utils/perf-training. stage2:; Depends on ""stage2-instrumented-generate-profdata"" and will use the stage1; compiler with the stage2 profdata to build a PGO-optimized compiler. stage2-check-llvm:; Depends on stage2 and runs check-llvm using the stage3 compiler. stage2-check-clang:; Depends on stage2 and runs check-clang using the stage3 compiler. stage2-check-all:; Depends on stage2 and runs check-all using the stage3 compiler. stage2-test-suite:; Depends on ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/cmake/caches/README.txt:58,cache,cache,58,interpreter/llvm-project/clang/cmake/caches/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/cmake/caches/README.txt,9,"['cache', 'perform']","['cache', 'caches', 'performs']"
Performance,"CSE/DSE and a few other optimizations are allowed, but Monotonic; operations are unlikely to be used in ways which would make those; optimizations useful. Notes for code generation; Code generation is essentially the same as that for unordered for loads and; stores. No fences are required. ``cmpxchg`` and ``atomicrmw`` are required; to appear as a single operation. Acquire; -------. Acquire provides a barrier of the sort necessary to acquire a lock to access; other memory with normal loads and stores. Relevant standard; This corresponds to the C++/C ``memory_order_acquire``. It should also be; used for C++/C ``memory_order_consume``. Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Acquire only provides a semantic guarantee when paired with a Release; operation. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. It is; also possible to move stores from before an Acquire load or read-modify-write; operation to after it, and move non-Acquire loads from before an Acquire; operation to after it. Notes for code generation; Architectures with weak memory ordering (essentially everything relevant today; except x86 and SPARC) require some sort of fence to maintain the Acquire; semantics. The precise fences required varies widely by architecture, but for; a simple implementation, most architectures provide a barrier which is strong; enough for everything (``dmb`` on ARM, ``sync`` on PowerPC, etc.). Putting; such a fence after the equivalent Monotonic operation is sufficient to; maintain Acquire semantics for a memory operation. Release; -------. Release is similar to Acquire, but with a barrier of the sort necessary to; release a lock. Relevant standard; This corresponds to the C++/C ``memory_order_release``. Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Release only provides a semantic guarantee when paired with an Acquire; operation. Notes ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:12053,load,load,12053,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,2,['load'],"['load', 'loads']"
Performance,"CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - generic 1. flat_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If CU wavefront execution; mode, omit vmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; the following; buffer_gl0_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1 dlc=1. - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_gl*_inv.; - Ensures the load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1 dlc=1; - system; - If GFX11, omit dlc=1. 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_gl*_invl.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atom",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:347578,load,load,347578,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0) & vscnt(0). - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 2. buffer/global/flat_atomic; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address sp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:359535,load,load,359535,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"Cb` trigger. The program stress takes one argument, the number of events to process.; The default is 1000 events. Be aware that executing stress with 1000; events*will create several files consuming about 100 MB of disk space;*; running stress with 30 events will consume about 20 MB. The disk space; is released once stress is done. There are two ways to run `stress`:. From the system prompt or from the ROOT prompt using the interpreter. ``` {.cpp}; > cd $ROOTSYS/test; > stress // default 1000 events; > stress 30 // test with 30 events; ```. Start ROOT with the batch mode option (-b) to suppress the graphic; output. ``` {.cpp}; > root -b; root[] .L stress.cxx; root[] stress(1000)// test with 1000 events; root[] stress(30)// test with 30 events; ```. The output of stress includes a pass/fail conclusion for each test, the; total number of bytes read and written, and the elapsed real and CPU; time. It also calculates a performance index for your machine relative; to a reference machine a DELL Inspiron 7500 (Pentium III 600 MHz) with; 256 MB of memory and 18GB IDE disk in ROOTMARKS. Higher ROOTMARKS means; better performance. The reference machine has 200 ROOTMARKS, so the; sample run below with 53.7 ROOTMARKS is about four times slower than the; reference machine. Here is a sample run:. ``` {.cpp}; % root -b; root[] .x stress.cxx(30). Test 1 : Functions, Random Numbers, Histogram Fits............. OK; Test 2 : Check size & compression factor of a Root file........ OK; Test 3 : Purge, Reuse of gaps in TFile......................... OK; Test 4 : Test of 2-d histograms, functions, 2-d fits........... OK; Test 5 : Test graphics & PostScript ............................OK; Test 6 : Test subdirectories in a Root file.................... OK; Test 7 : TNtuple, selections, TCutG, TEventList.......... OK; Test 8 : Trees split and compression modes..................... OK; Test 9 : Analyze Event.root file of stress 8................... OK; Test 10 : Create 10 files starting from Ev",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/TutorialsandTests.md:14341,perform,performance,14341,documentation/users-guide/TutorialsandTests.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/TutorialsandTests.md,1,['perform'],['performance']
Performance,"Check that NSLocalizedString macros include a comment for context. .. code-block:: objc. - (void)test {; NSString *string = NSLocalizedString(@""LocalizedString"", nil); // warn; NSString *string2 = NSLocalizedString(@""LocalizedString"", @"" ""); // warn; NSString *string3 = NSLocalizedStringWithDefaultValue(; @""LocalizedString"", nil, [[NSBundle alloc] init], nil,@""""); // warn; }. .. _optin-osx-cocoa-localizability-NonLocalizedStringChecker:. optin.osx.cocoa.localizability.NonLocalizedStringChecker (ObjC); """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Warns about uses of non-localized NSStrings passed to UI methods expecting localized NSStrings. .. code-block:: objc. NSString *alarmText =; NSLocalizedString(@""Enabled"", @""Indicates alarm is turned on"");; if (!isEnabled) {; alarmText = @""Disabled"";; }; UILabel *alarmStateLabel = [[UILabel alloc] init];. // Warning: User-facing text should use localized string macro; [alarmStateLabel setText:alarmText];. .. _optin-performance-GCDAntipattern:. optin.performance.GCDAntipattern; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Check for performance anti-patterns when using Grand Central Dispatch. .. _optin-performance-Padding:. optin.performance.Padding; """"""""""""""""""""""""""""""""""""""""""""""""""; Check for excessively padded structs. .. _optin-portability-UnixAPI:. optin.portability.UnixAPI; """"""""""""""""""""""""""""""""""""""""""""""""""; Finds implementation-defined behavior in UNIX/Posix functions. .. _security-checkers:. security; ^^^^^^^^. Security related checkers. .. _security-cert-env-InvalidPtr:. security.cert.env.InvalidPtr; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". Corresponds to SEI CERT Rules `ENV31-C <https://wiki.sei.cmu.edu/confluence/display/c/ENV31-C.+Do+not+rely+on+an+environment+pointer+following+an+operation+that+may+invalidate+it>`_ and `ENV34-C <https://wiki.sei.cmu.edu/confluence/display/c/ENV34-C.+Do+not+store+pointers+returned+by+certain+functions>`_. * **ENV31-C**:; Rule is about the possible problem with ``main`` function's third argument",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst:19306,perform,performance-GCDAntipattern,19306,interpreter/llvm-project/clang/docs/analyzer/checkers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst,1,['perform'],['performance-GCDAntipattern']
Performance,"Clang generates an access function to access C++-style TLS. The access; function generally has an entry block, an exit block and an initialization; block that is run at the first time. The entry and exit blocks can access; a few TLS IR variables, each access will be lowered to a platform-specific; sequence. This calling convention aims to minimize overhead in the caller by; preserving as many registers as possible (all the registers that are; preserved on the fast path, composed of the entry and exit blocks). This calling convention behaves identical to the `C` calling convention on; how arguments and return values are passed, but it uses a different set of; caller/callee-saved registers. Given that each platform has its own lowering sequence, hence its own set; of preserved registers, we can't use the existing `PreserveMost`. - On X86-64 the callee preserves all general purpose registers, except for; RDI and RAX.; ""``tailcc``"" - Tail callable calling convention; This calling convention ensures that calls in tail position will always be; tail call optimized. This calling convention is equivalent to fastcc,; except for an additional guarantee that tail calls will be produced; whenever possible. `Tail calls can only be optimized when this, the fastcc,; the GHC or the HiPE convention is used. <CodeGenerator.html#tail-call-optimization>`_; This calling convention does not support varargs and requires the prototype of; all callees to exactly match the prototype of the function definition.; ""``swiftcc``"" - This calling convention is used for Swift language.; - On X86-64 RCX and R8 are available for additional integer returns, and; XMM2 and XMM3 are available for additional FP/vector returns.; - On iOS platforms, we use AAPCS-VFP calling convention.; ""``swifttailcc``""; This calling convention is like ``swiftcc`` in most respects, but also the; callee pops the argument area of the stack so that mandatory tail calls are; possible as in ``tailcc``.; ""``cfguard_checkcc``"" - Win",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:21026,optimiz,optimized,21026,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimized']
Performance,"Cling interprets C++; ====================. .. figure:: images/fig1.jpeg. **Cling** is an interactive C++ interpreter built on top of `Clang; <https://clang.llvm.org/>`_ and `LLVM <https://llvm.org/>`_. It uses LLVM's; *Just-In-Time* (`JIT <https://en.wikipedia.org/wiki/Just-in-time_compilation>`_); compiler to provide a fast and optimized compilation pipeline. Cling uses the; `read-eval-print-loop; <https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop>`_; (**REPL**) approach, making rapid application development in C++ possible,; avoiding the classic edit-compile-run-debug cycle approach. Cling's last release, download instructions, dependencies, and any other useful; information for developers can be found on `Cling's GitHub webpage; <https://github.com/vgvassilev/cling>`_. Find out more about **Interpreting C++** on the `Compiler Research Group; <https://compiler-research.org/>`_'s webpage.; . Table of Contents; -----------------. .. toctree::; :numbered:; ; chapters/background; chapters/interactivity; chapters/why_interpreting; chapters/implementation; chapters/REPL; chapters/grammar; chapters/applications; chapters/conclusion; chapters/references; . .. note::. This project is under active development.; Cling has its documentation hosted on Read the Docs. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/index.rst:332,optimiz,optimized,332,interpreter/cling/docs/index.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/index.rst,1,['optimiz'],['optimized']
Performance,"Command Line; ============. Cling has its own command line, which looks like any other Unix shell. The; emacs-like command line editor is what we call interactive command line or; interactive shell. Once we start Cling it automatically includes several header files and its own; runtime universe. Thus it creates the minimal environment for the user to start. Grammar; -------. Cling is capable to parse everything that `Clang <https://clang.llvm.org/>`_ can; do. In addition, Cling can parse some interpreter-specific C++ extensions. Metaprocessor; -------------. Cling Metaprocessor provides convenient and easy to use interface for changing; the interpreter’s internal state or for executing handy commands. Cling provides; the following metaprocessor commands:. **syntax: .(command)**, where command is:. .. code:: bash. x filename.cxx; ; loads filename and calls void filename() if defined;. .. code:: bash. L library | filename.cxx; ; loads library or filename.cxx;. .. code:: bash. printAST; ; (DEBUG ONLY) shows the abstract syntax tree after each processed entity;. .. code:: bash. I path; ; adds an include path;. .. code:: bash. .@ . Cancels the multiline input;. .. code:: bash. .dynamicExtensions. Turns on cling's dynamic extensions. This in turn enables the dynamic lookup and; the late resolving of the identifier. With that option cling tries to heal the; compile-time failed lookups at runtime.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/grammar.rst:843,load,loads,843,interpreter/cling/docs/chapters/grammar.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/grammar.rst,2,['load'],['loads']
Performance,"Conclusion; ----------. Cling is not just an interpreter, and is not just a REPL: it is a C/C++; JIT-compiler that can be embedded to your software for efficient incremental; execution of C++. Cling allows you to decide how much you want to compile; statically and how much to defer for the target platform. Cling enables; reflection and introspection information in high-performance systems such as; ROOT, or Xeus Jupyter, where it provides efficient code for performance-critical; tasks where hot-spot regions can be annotated with specific optimization; levels. You ca find more information regarding Cling's internal architecture,; functionment, user-cases, and Cling's based project into the References Chapter.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/conclusion.rst:372,perform,performance,372,interpreter/cling/docs/chapters/conclusion.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/conclusion.rst,3,"['optimiz', 'perform']","['optimization', 'performance', 'performance-critical']"
Performance,"Connection to Other Vector Classes. The 3D and 4D vectors of the `GenVector` package can be constructed and; assigned from any vector which satisfies the following requisites:. - for 3D vectors implementing the `x()`, `y()` and `z()` methods. - for Lorentz vectors implementing the `x()`, `y()`, `z()` and `t()`; methods. ``` {.cpp}; CLHEP::Hep3Vector hv;; XYZVector v1(hv); //create 3D vector from; //CLHEP 3D Vector; HepGeom::Point3D hp;; XYZPoint p1(hp); //create a 3D p; ```. ## Linear Algebra: SMatrix Package. The ROOT Linear algebra package is documented in a separate chapter (see; ""Linear Algebra in ROOT""). `SMatrix` is a C++ package, for high; performance vector and matrix computations. It has been introduced in; ROOT v5.08. It is optimized for describing small matrices and vectors; and It can be used only in problems when the size of the matrices is; known at compile time, like in the tracking reconstruction of physics; experiments. It is based on a C++ technique, called expression; templates, to achieve an high level optimization. The C++ templates can; be used to implement vector and matrix expressions such that these; expressions can be transformed at compile time to code which is; equivalent to hand optimized code in a low-level language like FORTRAN; or C (see for example T. Veldhuizen, Expression Templates, C++ Report,; 1995). The `SMatrix` has been developed initially by T. Glebe in; Max-Planck-Institut, Heidelberg, as part of the `HeraB` analysis; framework. A subset of the original package has been now incorporated in; the ROOT distribution, with the aim to provide a stand-alone and high; performance matrix package. The API of the current package differs from; the original one, in order to be compliant to the ROOT coding; conventions. `SMatrix` contains the generic **`ROOT::Math::SMatrix`** and; **`ROOT::Math::SVector`** classes for describing matrices and vectors of; arbitrary dimensions and of arbitrary type. The classes are templated on; the scalar ty",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md:99448,optimiz,optimization,99448,documentation/users-guide/MathLibraries.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/MathLibraries.md,1,['optimiz'],['optimization']
Performance,"D = ES.createJITDylib(""libFoo.dylib"");. The JITDylib is owned by the ``ExecutionEngine`` instance and will be freed; when it is destroyed. How to remove code; ------------------. To remove an individual module from a JITDylib it must first be added using an; explicit ``ResourceTracker``. The module can then be removed by calling; ``ResourceTracker::remove``:. .. code-block:: c++. auto &JD = ... ;; auto M = ... ;. auto RT = JD.createResourceTracker();; Layer.add(RT, std::move(M)); // Add M to JD, tracking resources with RT. RT.remove(); // Remove M from JD. Modules added directly to a JITDylib will be tracked by that JITDylib's default; resource tracker. All code can be removed from a JITDylib by calling ``JITDylib::clear``. This; leaves the cleared JITDylib in an empty but usable state. JITDylibs can be removed by calling ``ExecutionSession::removeJITDylib``. This; clears the JITDylib and then puts it into a defunct state. No further operations; can be performed on the JITDylib, and it will be destroyed as soon as the last; handle to it is released. An example of how to use the resource management APIs can be found at; ``llvm/examples/OrcV2Examples/LLJITRemovableCode``. How to add the support for custom program representation; --------------------------------------------------------; In order to add the support for a custom program representation, a custom ``MaterializationUnit``; for the program representation, and a custom ``Layer`` are needed. The Layer will have two; operations: ``add`` and ``emit``. The ``add`` operation takes an instance of your program; representation, builds one of your custom ``MaterializationUnits`` to hold it, then adds it; to a ``JITDylib``. The emit operation takes a ``MaterializationResponsibility`` object and an; instance of your program representation and materializes it, usually by compiling it and handing; the resulting object off to an ``ObjectLinkingLayer``. Your custom ``MaterializationUnit`` will have two operations: ``materiali",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst:24347,perform,performed,24347,interpreter/llvm-project/llvm/docs/ORCv2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ORCv2.rst,1,['perform'],['performed']
Performance,D2; Position of attribute-specifier in declarator syntax; Unknown. 980; CD2; Explicit instantiation of a member of a class template; Unknown. 981; C++11; Constexpr constructor templates and literal types; Unknown. 982; NAD; Initialization with an empty initializer list; Unknown. 983; CD2; Ambiguous pointer-to-member constant; Unknown. 984; CD2; “Deduced type” is unclear in auto type deduction; Unknown. 985; C++11; Alternative tokens and user-defined literals; Unknown. 986; CD2; Transitivity of using-directives versus qualified lookup; Unknown. 987; CD4; Which declarations introduce namespace members?; Unknown. 988; CD2; Reference-to-reference collapsing with decltype; Unknown. 989; CD2; Misplaced list-initialization example; Unknown. 990; CD2; Value initialization with multiple initializer-list constructors; Clang 3.5. 991; CD2; Reference parameters of constexpr functions and constructors; Unknown. 992; NAD; Inheriting explicitness; Unknown. 993; C++11; Freedom to perform instantiation at the end of the translation unit; Unknown. 994; C++11; braced-init-list as a default argument; Unknown. 995; CD2; Incorrect example for using-declaration and explicit instantiation; Unknown. 996; C++11; Ambiguous partial specializations of member class templates; Unknown. 997; C++11; Argument-dependent lookup and dependent function template parameter types; Unknown. 998; dup; Function parameter transformations and template functions; Unknown. 999; CD2; “Implicit” or “implied” object argument/parameter?; Unknown. 1000; CD2; Mistaking member typedefs for constructors; Unknown. 1001; drafting; Parameter type adjustment in dependent parameter types; Not resolved. 1002; NAD; Pack expansion for function arguments; Unknown. 1003; CD3; Acceptable definitions of main; Unknown. 1004; C++11; Injected-class-names as arguments for template template parameters; Clang 5. 1005; NAD; Qualified name resolution in member functions of class templates; Unknown. 1006; C++11; std::nullptr_t as a non-type ,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html:65280,perform,perform,65280,interpreter/llvm-project/clang/www/cxx_dr_status.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/cxx_dr_status.html,2,['perform'],['perform']
Performance,"DGPU_REL32_HI`` Static 11 ``word32`` (S + A - P) >> 32; *reserved* 12; ``R_AMDGPU_RELATIVE64`` Dynamic 13 ``word64`` B + A; ``R_AMDGPU_REL16`` Static 14 ``word16`` ((S + A - P) - 4) / 4; ========================== ======= ===== ========== ==============================. ``R_AMDGPU_ABS32_LO`` and ``R_AMDGPU_ABS32_HI`` are only supported by; the ``mesa3d`` OS, which does not support ``R_AMDGPU_ABS64``. There is no current OS loader support for 32-bit programs and so; ``R_AMDGPU_ABS32`` is not used. .. _amdgpu-loaded-code-object-path-uniform-resource-identifier:. Loaded Code Object Path Uniform Resource Identifier (URI); ---------------------------------------------------------. The AMD GPU code object loader represents the path of the ELF shared object from; which the code object was loaded as a textual Uniform Resource Identifier (URI).; Note that the code object is the in memory loaded relocated form of the ELF; shared object. Multiple code objects may be loaded at different memory; addresses in the same process from the same ELF shared object. The loaded code object path URI syntax is defined by the following BNF syntax:. .. code::. code_object_uri ::== file_uri | memory_uri; file_uri ::== ""file://"" file_path [ range_specifier ]; memory_uri ::== ""memory://"" process_id range_specifier; range_specifier ::== [ ""#"" | ""?"" ] ""offset="" number ""&"" ""size="" number; file_path ::== URI_ENCODED_OS_FILE_PATH; process_id ::== DECIMAL_NUMBER; number ::== HEX_NUMBER | DECIMAL_NUMBER | OCTAL_NUMBER. **number**; Is a C integral literal where hexadecimal values are prefixed by ""0x"" or ""0X"",; and octal values by ""0"". **file_path**; Is the file's path specified as a URI encoded UTF-8 string. In URI encoding,; every character that is not in the regular expression ``[a-zA-Z0-9/_.~-]`` is; encoded as two uppercase hexadecimal digits proceeded by ""%"". Directories in; the path are separated by ""/"". **offset**; Is a 0-based byte offset to the start of the code object. For a file URI, it; is f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:83117,load,loaded,83117,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loaded']
Performance,"DS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations.; Completion of load/store/sample operations are reported to a wavefront in; execution order of other load/store/sample operations performed by that; wavefront.; * The vector memory operations access a vector L0 cache. There is a single L0; cache per CU. Each SIMD of a CU accesses the same L0 cache. Therefore, no; special action is required for coherence between the lanes of a single; wavefront. However, a ``buffer_gl0_inv`` is required for coherence between; wavefronts executing in the same work-group as they may be executing on SIMDs; of different CUs that access different L0s. A ``buffer_gl0_inv`` is also; required for coherence between wavefronts executing in different work-groups; as they may be executing on different WGPs.; * The scalar memory operations access a scalar L0 cache shared by all wavefronts; on a WGP. The scalar and vector L0 caches are not coherent. However, scalar; operations are used in a restricted way so do not impact the memory model. See; :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory L0 caches use an L1 cache shared by all WGPs on; the same SA. Therefore, no special action is required for coherence between; the wavefronts of a single work-group. However, a ``buffer_gl1_inv`` is; required for coherence between wavefronts executing in different work-groups; as they may be executing on different SAs that access different L1s.; * The L1 caches have independent quadrants to service disjoint ranges of virtual; addresses.; * Each L0 cache has a separate request queue per L1 quadrant. Therefore, the; vector and scalar memory operations performed by different wavefronts, whether; executing in the same or different work-groups (which may be executing on; different CUs accessing different L0s), can be reordered relativ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:337373,cache,cache,337373,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"Date: Fri, 1 Jun 2001 16:38:17 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Interesting: GCC passes. Take a look at this document (which describes the order of optimizations; that GCC performs):. http://gcc.gnu.org/onlinedocs/gcc_17.html. The rundown is that after RTL generation, the following happens:. 1 . [t] jump optimization (jumps to jumps, etc); 2 . [t] Delete unreachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:217,optimiz,optimizations,217,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,4,"['optimiz', 'perform']","['optimization', 'optimizations', 'performs']"
Performance,"Date: Fri, 1 Jun 2001 17:08:44 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: RE: Interesting: GCC passes. > That is very interesting. I agree that some of these could be done on LLVM; > at link-time, but it is the extra time required that concerns me. Link-time; > optimization is severely time-constrained. If we were to reimplement any of these optimizations, I assume that we; could do them a translation unit at a time, just as GCC does now. This; would lead to a pipeline like this:. Static optimizations, xlation unit at a time:; .c --GCC--> .llvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mix",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:321,optimiz,optimization,321,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,6,['optimiz'],"['optimization', 'optimizations']"
Performance,"Date: Fri, 6 Jul 2001 16:56:56 -0500; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: lowering the IR. BTW, I do think that we should consider lowering the IR as you said. I; didn't get time to raise it today, but it comes up with the SPARC; move-conditional instruction. I don't think we want to put that in the core; VM -- it is a little too specialized. But without a corresponding; conditional move instruction in the VM, it is pretty difficult to maintain a; close mapping between VM and machine code. Other architectures may have; other such instructions. What I was going to suggest was that for a particular processor, we define; additional VM instructions that match some of the unusual opcodes on the; processor but have VM semantics otherwise, i.e., all operands are in SSA; form and typed. This means that we can re-generate core VM code from the; more specialized code any time we want (so that portability is not lost). Typically, a static compiler like gcc would generate just the core VM, which; is relatively portable. Anyone (an offline tool, the linker, etc., or even; the static compiler itself if it chooses) can transform that into more; specialized target-specific VM code for a particular architecture. If the; linker does it, it can do it after all machine-independent optimizations.; This would be the most convenient, but not necessary. The main benefit of lowering will be that we will be able to retain a close; mapping between VM and machine code. --Vikram. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-07-06-LoweringIRForCodeGen.txt:1338,optimiz,optimizations,1338,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-07-06-LoweringIRForCodeGen.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-07-06-LoweringIRForCodeGen.txt,1,['optimiz'],['optimizations']
Performance,"Date: Tue, 13 Feb 2001 13:29:52 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: LLVM Concerns... I've updated the documentation to include load store and allocation; instructions (please take a look and let me know if I'm on the right; track):. file:/home/vadve/lattner/llvm/docs/LangRef.html#memoryops. I have a couple of concerns I would like to bring up:. 1. Reference types; Right now, I've spec'd out the language to have a pointer type, which; works fine for lots of stuff... except that Java really has; references: constrained pointers that cannot be manipulated: added and; subtracted, moved, etc... Do we want to have a type like this? It; could be very nice for analysis (pointer always points to the start of; an object, etc...) and more closely matches Java semantics. The; pointer type would be kept for C++ like semantics. Through analysis,; C++ pointers could be promoted to references in the LLVM; representation. 2. Our ""implicit"" memory references in assembly language:; After thinking about it, this model has two problems:; A. If you do pointer analysis and realize that two stores are; independent and can share the same memory source object, there is; no way to represent this in either the bytecode or assembly.; B. When parsing assembly/bytecode, we effectively have to do a full; SSA generation/PHI node insertion pass to build the dependencies; when we don't want the ""pinned"" representation. This is not; cool.; I'm tempted to make memory references explicit in both the assembly and; bytecode to get around this... what do you think?. -Chris. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-Memory.txt:193,load,load,193,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-Memory.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-Memory.txt,1,['load'],['load']
Performance,"Date: Tue, 18 Sep 2001 00:38:37 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Idea for a simple, useful link time optimization. In C++ programs, exceptions suck, and here's why:. 1. In virtually all function calls, you must assume that the function; throws an exception, unless it is defined as 'nothrow'. This means; that every function call has to have code to invoke dtors on objects; locally if one is thrown by the function. Most functions don't throw; exceptions, so this code is dead [with all the bad effects of dead; code, including icache pollution].; 2. Declaring a function nothrow causes catch blocks to be added to every; call that isnot provably nothrow. This makes them very slow.; 3. Extra extraneous exception edges reduce the opportunity for code; motion.; 4. EH is typically implemented with large lookup tables. Ours is going to; be much smaller (than the ""standard"" way of doing it) to start with,; but eliminating it entirely would be nice. :); 5. It is physically impossible to correctly put (accurate, correct); exception specifications on generic, templated code. But it is trivial; to analyze instantiations of said code.; 6. Most large C++ programs throw few exceptions. Most well designed; programs only throw exceptions in specific planned portions of the; code. Given our _planned_ model of handling exceptions, all of this would be; pretty trivial to eliminate through some pretty simplistic interprocedural; analysis. The DCE factor alone could probably be pretty significant. The; extra code motion opportunities could also be exploited though... Additionally, this optimization can be implemented in a straight forward; conservative manner, allowing libraries to be optimized or individual; files even (if there are leaf functions visible in the translation unit; that are called). I think it's a reasonable optimization that hasn't really been addressed; (because assembly is way too low level for this), and ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt:170,optimiz,optimization,170,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt,1,['optimiz'],['optimization']
Performance,"Date: Wed, 20 Jun 2001 12:32:22 -0500; From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: .NET vs. our VM. One significant difference between .NET CLR and our VM is that the CLR; includes full information about classes and inheritance. In fact, I just; sat through the paper on adding templates to .NET CLR, and the speaker; indicated that the goal seems to be to do simple static compilation (very; little lowering or optimization). Also, the templates implementation in CLR; ""relies on dynamic class loading and JIT compilation"". This is an important difference because I think there are some significant; advantages to have a much lower level VM layer, and do significant static; analysis and optimization. I also talked to the lead guy for KAI's C++ compiler (Arch Robison) and he; said that SGI and other commercial compilers have included options to export; their *IR* next to the object code (i.e., .il files) and use them for; link-time code generation. In fact, he said that the .o file was nearly; empty and was entirely generated from the .il at link-time. But he agreed; that this limited the link-time interprocedural optimization to modules; compiled by the same compiler, whereas our approach allows us to link and; optimize modules from multiple different compilers. (Also, of course, they; don't do anything for runtime optimization). All issues to bring up in Related Work. --Vikram. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt:457,optimiz,optimization,457,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt,6,"['load', 'optimiz']","['loading', 'optimization', 'optimize']"
Performance,"Date: Wed, 31 Jan 2001 12:04:33 -0600; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLV",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:565,optimiz,optimization,565,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,3,['optimiz'],"['optimization', 'optimizations']"
Performance,"Debugify:. Mutation testing for IR-level transformations; ---------------------------------------------. An IR test case for a transformation can, in many cases, be automatically; mutated to test debug info handling within that transformation. This is a; simple way to test for proper debug info handling. The ``debugify`` utility pass; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``debugify`` testing utility is just a pair of passes: ``debugify`` and; ``check-debugify``. The first applies synthetic debug information to every instruction of the; module, and the second checks that this DI is still available after an; optimization has occurred, reporting any errors/warnings while doing so. The instructions are assigned sequentially increasing line locations, and are; immediately used by debug value intrinsics everywhere possible. For example, here is a module before:. .. code-block:: llvm. define void @f(i32* %x) {; entry:; %x.addr = alloca i32*, align 8; store i32* %x, i32** %x.addr, align 8; %0 = load i32*, i32** %x.addr, align 8; store i32 10, i32* %0, align 4; ret void; }. and after running ``opt -debugify``:. .. code-block:: llvm. define void @f(i32* %x) !dbg !6 {; entry:; %x.addr = alloca i32*, align 8, !dbg !12; call void @llvm.dbg.value(metadata i32** %x.addr, metadata !9, metadata !DIExpression()), !dbg !12; store i32* %x, i32** %x.addr, align 8, !dbg !13; %0 = load i32*, i32** %x.addr, align 8, !dbg !14; call void @llvm.dbg.value(metadata i32* %0, metadata !11, metadata !DIExpression()), !dbg !14; store i32 10, i32* %0, align 4, !dbg !15; ret void, !dbg !16; }. !llvm.dbg.cu = !{!0}; !llvm.debugify = !{!3, !4}; !llvm.module.flags = !{!5}. !0 = distinct !DICompileUnit(language: DW_LANG_C, file: !1, producer: ""debugify"", isOptimized: true, runtimeVersion: 0, emissionKind: FullDebug, enums: !2); !1 = !DIFile(filename: ""debugify-sample.ll"", directory: ""/""); !2 = !{}; !3 = !{i32 5}; !4 = !{i32 2}; !5 = !{i32 2, !""Debug Info Version"", i32 3}; !6 = distinct !DISubprogram(name: ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst:10275,load,load,10275,interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,1,['load'],['load']
Performance,"DefMap.txt` to contain the name of the `.ast` files instead of the source files:. .. code-block:: bash. $ sed -i -e ""s/.cpp/.cpp.ast/g"" externalDefMap.txt. We still have to further modify the `externalDefMap.txt` file to contain relative paths:. .. code-block:: bash. $ sed -i -e ""s|$(pwd)/||g"" externalDefMap.txt. Now everything is available for the CTU analysis.; We have to feed Clang with CTU specific extra arguments:. .. code-block:: bash. $ pwd; /path/to/your/project; $ clang++ --analyze \; -Xclang -analyzer-config -Xclang experimental-enable-naive-ctu-analysis=true \; -Xclang -analyzer-config -Xclang ctu-dir=. \; -Xclang -analyzer-output=plist-multi-file \; main.cpp; main.cpp:5:12: warning: Division by zero; return 3 / foo();; ~~^~~~~~~; 1 warning generated.; $ # The plist file with the result is generated.; $ ls -F; compile_commands.json externalDefMap.txt foo.ast foo.cpp foo.cpp.ast main.cpp main.plist; $. This manual procedure is error-prone and not scalable, therefore to analyze real projects it is recommended to use; `CodeChecker` or `scan-build-py`. Automated CTU Analysis with CodeChecker; #######################################; The `CodeChecker <https://github.com/Ericsson/codechecker>`_ project fully supports automated CTU analysis with Clang.; Once we have set up the `PATH` environment variable and we activated the python `venv` then it is all it takes:. .. code-block:: bash. $ CodeChecker analyze --ctu compile_commands.json -o reports; $ ls -F; compile_commands.json foo.cpp foo.cpp.ast main.cpp reports/; $ tree reports; reports; ├── compile_cmd.json; ├── compiler_info.json; ├── foo.cpp_53f6fbf7ab7ec9931301524b551959e2.plist; ├── main.cpp_23db3d8df52ff0812e6e5a03071c8337.plist; ├── metadata.json; └── unique_compile_commands.json. 0 directories, 6 files; $. The `plist` files contain the results of the analysis, which may be viewed with the regular analysis tools.; E.g. one may use `CodeChecker parse` to view the results in command line:. .. code-block::",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/user-docs/CrossTranslationUnit.rst:4050,scalab,scalable,4050,interpreter/llvm-project/clang/docs/analyzer/user-docs/CrossTranslationUnit.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/user-docs/CrossTranslationUnit.rst,1,['scalab'],['scalable']
Performance,"Disable buffer security check; /GS Enable buffer security check (default); /Gs Use stack probes (default); /Gs<value> Set stack probe size (default 4096); /guard:<value> Enable Control Flow Guard with /guard:cf,; or only the table with /guard:cf,nochecks.; Enable EH Continuation Guard with /guard:ehcont; /Gv Set __vectorcall as a default calling convention; /Gw- Don't put each data item in its own section; /Gw Put each data item in its own section; /GX- Disable exception handling; /GX Enable exception handling; /Gy- Don't put each function in its own section (default); /Gy Put each function in its own section; /Gz Set __stdcall as a default calling convention; /help Display available options; /imsvc <dir> Add directory to system include search path, as if part of %INCLUDE%; /I <dir> Add directory to include search path; /J Make char type unsigned; /LDd Create debug DLL; /LD Create DLL; /link <options> Forward options to the linker; /MDd Use DLL debug run-time; /MD Use DLL run-time; /MTd Use static debug run-time; /MT Use static run-time; /O0 Disable optimization; /O1 Optimize for size (same as /Og /Os /Oy /Ob2 /GF /Gy); /O2 Optimize for speed (same as /Og /Oi /Ot /Oy /Ob2 /GF /Gy); /Ob0 Disable function inlining; /Ob1 Only inline functions which are (explicitly or implicitly) marked inline; /Ob2 Inline functions as deemed beneficial by the compiler; /Od Disable optimization; /Og No effect; /Oi- Disable use of builtin functions; /Oi Enable use of builtin functions; /Os Optimize for size; /Ot Optimize for speed; /Ox Deprecated (same as /Og /Oi /Ot /Oy /Ob2); use /O2 instead; /Oy- Disable frame pointer omission (x86 only, default); /Oy Enable frame pointer omission (x86 only); /O<flags> Set multiple /O flags at once; e.g. '/O2y-' for '/O2 /Oy-'; /o <file or directory> Set output file or directory (ends in / or \); /P Preprocess to file; /Qvec- Disable the loop vectorization passes; /Qvec Enable the loop vectorization passes; /showFilenames- Don't print the name of each ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:174019,optimiz,optimization,174019,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,2,['optimiz'],['optimization']
Performance,"E ""${CLANG_VENDOR} clang""); else(); set(TOOL_INFO_NAME ""clang""); endif(). set(TOOL_INFO_UTI ""${CLANG_VENDOR_UTI}""); set(TOOL_INFO_VERSION ""${CLANG_VERSION}""); set(TOOL_INFO_BUILD_VERSION ""${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}""). set(TOOL_INFO_PLIST_OUT ""${CMAKE_CURRENT_BINARY_DIR}/${TOOL_INFO_PLIST}""). if(LLVM_TOOL_LLVM_DRIVER_BUILD AND clang IN_LIST LLVM_DRIVER_TOOLS); set(TARGET_NAME llvm-driver); else(); set(TARGET_NAME clang); endif(). target_link_libraries(${TARGET_NAME}; PRIVATE; ""-Wl,-sectcreate,__TEXT,__info_plist,\""${TOOL_INFO_PLIST_OUT}\""""); configure_file(""${TOOL_INFO_PLIST}.in"" ""${TOOL_INFO_PLIST_OUT}"" @ONLY). set(TOOL_INFO_UTI); set(TOOL_INFO_NAME); set(TOOL_INFO_VERSION); set(TOOL_INFO_BUILD_VERSION); endif(). if(CLANG_ORDER_FILE AND; (LLVM_LINKER_IS_APPLE OR LLVM_LINKER_IS_GOLD OR LLVM_LINKER_IS_LLD)); include(LLVMCheckLinkerFlag). if (LLVM_LINKER_IS_APPLE OR (LLVM_LINKER_IS_LLD AND APPLE)); set(LINKER_ORDER_FILE_OPTION ""-Wl,-order_file,${CLANG_ORDER_FILE}""); elseif (LLVM_LINKER_IS_GOLD); set(LINKER_ORDER_FILE_OPTION ""-Wl,--section-ordering-file,${CLANG_ORDER_FILE}""); elseif (LLVM_LINKER_IS_LLD); set(LINKER_ORDER_FILE_OPTION ""-Wl,--symbol-ordering-file,${CLANG_ORDER_FILE}""); endif(). # This is a test to ensure the actual order file works with the linker.; llvm_check_linker_flag(CXX ${LINKER_ORDER_FILE_OPTION} LINKER_ORDER_FILE_WORKS). # Passing an empty order file disables some linker layout optimizations.; # To work around this and enable workflows for re-linking when the order file; # changes we check during configuration if the file is empty, and make it a; # configuration dependency.; file(READ ${CLANG_ORDER_FILE} ORDER_FILE LIMIT 20); if(""${ORDER_FILE}"" STREQUAL ""\n""); set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${CLANG_ORDER_FILE}); elseif(LINKER_ORDER_FILE_WORKS); target_link_libraries(clang PRIVATE ${LINKER_ORDER_FILE_OPTION}); set_target_properties(clang PROPERTIES LINK_DEPENDS ${CLANG_ORDER_FILE}); endif(); endif(); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/driver/CMakeLists.txt:2797,optimiz,optimizations,2797,interpreter/llvm-project/clang/tools/driver/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/driver/CMakeLists.txt,1,['optimiz'],['optimizations']
Performance,"E RW and CC memory will; never be stale in L2 due to; the memory probes. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/; atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the; value read by the; fence-paired-atomic. 2. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must hap",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:253336,load,load,253336,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"E RW; (read-write) for memory local to the L2, and MTYPE NC (non-coherent) with; the PTE C-bit set for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by the PTE C-bit.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequent access from the GPU will automatically invalidate or writeback; the CPU cache due to the L2 probe filter.; * To ensure coherence of local memory writes of CUs with different L1 caches; in the same agent a ``buffer_wbl2`` is required. It does nothing if the; agent is configured to have a single L2, or will writeback dirty L2 cache; lines if configured to have multiple L2 caches.; * To ensure coherence of local memory writes of CUs in different agents a; ``buffer_wbl2 sc1`` is required. It will writeback dirty L2 cache lines.; * To ensure coherence of local memory reads of CUs with different L1 caches; in the same agent a ``buffer_inv sc1`` is required. It does nothing if the; agent is configured to have a single L2, or will invalidate non-local L2; cache lines if configured to have multiple L2 caches.; * To ensure coherence of local memory reads of CUs in different agents a; ``buffer_inv sc0 sc1`` is required. It will invalidate non-local L2 cache; lines if configured to have multiple L2 caches. * PCIe access from the GPU to the CPU can be kept coherent by using the MTYPE; UC (uncached) which bypasses the L2. Scalar memory operations are only used to access memory that is proven to not; change during the execution of the kernel dispatch. This includes constant; address space and global address space for program scope ``const`` variables.; Therefore, the kernel machine code does not have to maintain the scalar cache to; ensure it is coherent with the vector caches. The scalar and vector caches are; invalidated between kernel dispatches by CP since constant address space",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:289266,cache,caches,289266,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"ECACHE_SIZE. The resource variable TTreeCache.Prefill sets the default TTreeCache prefilling; type. The prefill type may be: 0 for no prefilling and 1 to prefill all; the branches. It can be overridden by the environment variable ROOT_TTREECACHE_PREFILL. In particular the default can be set back to the same as in version 5 by; setting TTreeCache.Size (or ROOT_TTREECACHE_SIZE) and TTreeCache.Prefill; (or ROOT_TTREECACHE_PREFILL) both to zero. TTree methods which are expected to modify a cache, like AddBranchToCache, will; attempt to setup a cache of default size if one does not exist, irrespective of; whether the auto cache creation is enabled. Additionally several methods giving; control of the cache have changed return type from void to Int_t, to be able to; return a code to indicate if there was an error. Usually TTree::SetCacheSize will no longer reset the list of branches to be; cached (either set or previously learnt) nor restart the learning phase.; The learning phase is restarted when a new cache is created, e.g. after having; removed a cache with SetCacheSize(0). ### TSelectorDraw. The axis titles in case of a `x:y:z` plot with the option `COLZ` were not correct. ### TParallelCoordVar. Change the format used to print the variables limit for ||-Coord to `%g`. It was; `%6.4f` before. ## Histogram Libraries. ### TFormula. - New version of the TFormula class based on Cling. Formula expressions are now used to create functions which are passed to Cling to be Just In Time compiled.; The expression is therefore compiled using Clang/LLVVM which will give execution time as compiled code and in addition correctness of the result obtained.; - This class is not 100% backward compatible with the old TFormula class, which is still available in ROOT as =ROOT::v5::TFormula=.; Some of the TFormula member funtions available in version 5, such as =Analyze= and =AnalyzeFunction= are not available in the new TFormula class.; On the other hand formula expressions which were valid ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:14856,cache,cache,14856,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,1,['cache'],['cache']
Performance,"EDUCE_OR; * G_VECREDUCE_XOR; * G_VECREDUCE_SMAX; * G_VECREDUCE_SMIN; * G_VECREDUCE_UMAX; * G_VECREDUCE_UMIN. Integer reductions may have a result type larger than the vector element type.; However, the reduction is performed using the vector element type and the value; in the top bits is unspecified. Memory Operations; -----------------. G_LOAD, G_SEXTLOAD, G_ZEXTLOAD; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Generic load. Expects a MachineMemOperand in addition to explicit; operands. If the result size is larger than the memory size, the; high bits are undefined, sign-extended, or zero-extended respectively. Only G_LOAD is valid if the result is a vector type. If the result is larger; than the memory size, the high elements are undefined (i.e. this is not a; per-element, vector anyextload). Unlike in SelectionDAG, atomic loads are expressed with the same; opcodes as regular loads. G_LOAD, G_SEXTLOAD and G_ZEXTLOAD may all; have atomic memory operands. G_INDEXED_LOAD; ^^^^^^^^^^^^^^. Generic indexed load. Combines a GEP with a load. $newaddr is set to $base + $offset.; If $am is 0 (post-indexed), then the value is loaded from $base; if $am is 1 (pre-indexed); then the value is loaded from $newaddr. G_INDEXED_SEXTLOAD; ^^^^^^^^^^^^^^^^^^. Same as G_INDEXED_LOAD except that the load performed is sign-extending, as with G_SEXTLOAD. G_INDEXED_ZEXTLOAD; ^^^^^^^^^^^^^^^^^^. Same as G_INDEXED_LOAD except that the load performed is zero-extending, as with G_ZEXTLOAD. G_STORE; ^^^^^^^. Generic store. Expects a MachineMemOperand in addition to explicit; operands. If the stored value size is greater than the memory size,; the high bits are implicitly truncated. If this is a vector store, the; high elements are discarded (i.e. this does not function as a per-lane; vector, truncating store). G_INDEXED_STORE; ^^^^^^^^^^^^^^^. Combines a store with a GEP. See description of G_INDEXED_LOAD for indexing behaviour. G_ATOMIC_CMPXCHG_WITH_SUCCESS; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Generic atomic c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst:16102,load,load,16102,interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,1,['load'],['load']
Performance,"EGRATED_CRT_ALLOC}\\ide\\vs2019\\mimalloc.sln""); endif(); set(system_libs ${system_libs} ""${MIMALLOC_LIB}"" ""-INCLUDE:malloc""); endif(); endif(). # FIXME: We are currently guarding AIX headers with _XOPEN_SOURCE=700.; # See llvm/CMakeLists.txt. However, we need _SC_NPROCESSORS_ONLN in; # unistd.h and it is guarded by _ALL_SOURCE, so we remove the _XOPEN_SOURCE; # guard here. We should remove the guards all together once AIX cleans up; # the system headers.; if (UNIX AND ${CMAKE_SYSTEM_NAME} MATCHES ""AIX""); remove_definitions(""-D_XOPEN_SOURCE=700""); endif(). add_subdirectory(BLAKE3). add_llvm_component_library(LLVMSupport; ABIBreak.cpp; AMDGPUMetadata.cpp; APFixedPoint.cpp; APFloat.cpp; APInt.cpp; APSInt.cpp; ARMBuildAttrs.cpp; ARMAttributeParser.cpp; ARMWinEH.cpp; Allocator.cpp; AutoConvert.cpp; Base64.cpp; BalancedPartitioning.cpp; BinaryStreamError.cpp; BinaryStreamReader.cpp; BinaryStreamRef.cpp; BinaryStreamWriter.cpp; BlockFrequency.cpp; BranchProbability.cpp; BuryPointer.cpp; CachePruning.cpp; Caching.cpp; circular_raw_ostream.cpp; Chrono.cpp; COM.cpp; CodeGenCoverage.cpp; CommandLine.cpp; Compression.cpp; CRC.cpp; ConvertUTF.cpp; ConvertEBCDIC.cpp; ConvertUTFWrapper.cpp; CrashRecoveryContext.cpp; CSKYAttributes.cpp; CSKYAttributeParser.cpp; DataExtractor.cpp; Debug.cpp; DebugCounter.cpp; DeltaAlgorithm.cpp; DivisionByConstantInfo.cpp; DAGDeltaAlgorithm.cpp; DJB.cpp; ELFAttributeParser.cpp; ELFAttributes.cpp; Error.cpp; ErrorHandling.cpp; ExtensibleRTTI.cpp; FileCollector.cpp; FileUtilities.cpp; FileOutputBuffer.cpp; FloatingPointMode.cpp; FoldingSet.cpp; FormattedStream.cpp; FormatVariadic.cpp; GlobPattern.cpp; GraphWriter.cpp; Hashing.cpp; InitLLVM.cpp; InstructionCost.cpp; IntEqClasses.cpp; IntervalMap.cpp; JSON.cpp; KnownBits.cpp; LEB128.cpp; LineIterator.cpp; Locale.cpp; LockFileManager.cpp; ManagedStatic.cpp; MathExtras.cpp; MemAlloc.cpp; MemoryBuffer.cpp; MemoryBufferRef.cpp; MD5.cpp; MSP430Attributes.cpp; MSP430AttributeParser.cpp; NativeFormatting.cpp;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:5950,Cache,CachePruning,5950,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,1,['Cache'],['CachePruning']
Performance,"EG_FS`` and ``__SEG_GS``; indicate their support. PowerPC Language Extensions; ---------------------------. Set the Floating Point Rounding Mode; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; PowerPC64/PowerPC64le supports the builtin function ``__builtin_setrnd`` to set; the floating point rounding mode. This function will use the least significant; two bits of integer argument to set the floating point rounding mode. .. code-block:: c++. double __builtin_setrnd(int mode);. The effective values for mode are:. - 0 - round to nearest; - 1 - round to zero; - 2 - round to +infinity; - 3 - round to -infinity. Note that the mode argument will modulo 4, so if the integer argument is greater; than 3, it will only use the least significant two bits of the mode.; Namely, ``__builtin_setrnd(102))`` is equal to ``__builtin_setrnd(2)``. PowerPC cache builtins; ^^^^^^^^^^^^^^^^^^^^^^. The PowerPC architecture specifies instructions implementing cache operations.; Clang provides builtins that give direct programmer access to these cache; instructions. Currently the following builtins are implemented in clang:. ``__builtin_dcbf`` copies the contents of a modified block from the data cache; to main memory and flushes the copy from the data cache. **Syntax**:. .. code-block:: c. void __dcbf(const void* addr); /* Data Cache Block Flush */. **Example of Use**:. .. code-block:: c. int a = 1;; __builtin_dcbf (&a);. Extensions for Static Analysis; ==============================. Clang supports additional attributes that are useful for documenting program; invariants and rules for static analysis tools, such as the `Clang Static; Analyzer <https://clang-analyzer.llvm.org/>`_. These attributes are documented; in the analyzer's `list of source-level annotations; <https://clang-analyzer.llvm.org/annotations.html>`_. Extensions for Dynamic Analysis; ===============================. Use ``__has_feature(address_sanitizer)`` to check if the code is being built; with :doc:`AddressSanitizer`. Use ``__has_",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:157075,cache,cache,157075,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['cache'],['cache']
Performance,"EV`` | ``LD1`` |; +-------------------------------+-------------------------------+---------------------+; | AAPCS | ``LDR`` | ``LD1 + REV`` |; +-------------------------------+-------------------------------+---------------------+; | Alignment for strict mode | ``LDR`` / ``LD1 + REV`` | ``LD1`` |; +-------------------------------+-------------------------------+---------------------+. Neither approach is perfect, and choosing one boils down to choosing the lesser of two evils. The issue with lane ordering, it was decided, would have to change target-agnostic compiler passes and would result in a strange IR in which lane indices were reversed. It was decided that this was worse than the changes that would have to be made to support ``LD1``, so ``LD1`` was chosen as the canonical vector load instruction (and by inference, ``ST1`` for vector stores). Implementation; ==============. There are 3 parts to the implementation:. 1. Predicate ``LDR`` and ``STR`` instructions so that they are never allowed to be selected to generate vector loads and stores. The exception is one-lane vectors [1]_ - these by definition cannot have lane ordering problems so are fine to use ``LDR``/``STR``. 2. Create code generation patterns for bitconverts that create ``REV`` instructions. 3. Make sure appropriate bitconverts are created so that vector values get passed over call boundaries as 1-element vectors (which is the same as if they were loaded with ``LDR``). Bitconverts; -----------. .. image:: ARM-BE-bitcastfail.png; :align: right. The main problem with the ``LD1`` solution is dealing with bitconverts (or bitcasts, or reinterpret casts). These are pseudo instructions that only change the compiler's interpretation of data, not the underlying data itself. A requirement is that if data is loaded and then saved again (called a ""round trip""), the memory contents should be the same after the store as before the load. If a vector is loaded and is then bitconverted to a different vector type b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:9339,load,loads,9339,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,1,['load'],['loads']
Performance,"E_BUILD_TYPE=RelWithDebInfo`` instead of; ``-DCMAKE_BUILD_TYPE=Release``. This will grant better coverage of; debug info pieces of clang, but will take longer to complete and will; result in a much larger build directory. It's recommended to build the ``all`` target with your instrumented Clang,; since more coverage is often better. b. You should now have a few ``*.profraw`` files in; ``path/to/stage2/profiles/``. You need to merge these using; ``llvm-profdata`` (even if you only have one! The profile merge transforms; profraw into actual profile data, as well). This can be done with; ``/path/to/stage1/llvm-profdata merge; -output=/path/to/output/profdata.prof path/to/stage2/profiles/*.profraw``. 4. Now, build your final, PGO-optimized Clang. To do this, you'll want to pass; the following additional arguments to CMake. - ``-DLLVM_PROFDATA_FILE=/path/to/output/profdata.prof`` - Use the PGO; profile from the previous step.; - ``-DCMAKE_C_COMPILER=/path/to/stage1/clang`` - Use the Clang we built in; step 1.; - ``-DCMAKE_CXX_COMPILER=/path/to/stage1/clang++`` - Same as above. From here, you can build whatever targets you need. .. note::; You may see warnings about a mismatched profile in the build output. These; are generally harmless. To silence them, you can add; ``-DCMAKE_C_FLAGS='-Wno-backend-plugin'; -DCMAKE_CXX_FLAGS='-Wno-backend-plugin'`` to your CMake invocation. Congrats! You now have a Clang built with profile-guided optimizations, and you; can delete all but the final build directory if you'd like. If this worked well for you and you plan on doing it often, there's a slight; optimization that can be made: LLVM and Clang have a tool called tblgen that's; built and run during the build process. While it's potentially nice to build; this for coverage as part of step 3, none of your other builds should benefit; from building it. You can pass the CMake option; ``-DLLVM_NATIVE_TOOL_DIR=/path/to/stage1/bin``; to steps 2 and onward to avoid these useless rebuilds.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst:6719,optimiz,optimizations,6719,interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"Each scalar type has a parent type, which must also; be a scalar type or the TBAA root. Via this parent relation, scalar types; within a TBAA root form a tree. **Struct type descriptors** denote types; that contain a sequence of other type descriptors, at known offsets. These; contained type descriptors can either be struct type descriptors themselves; or scalar type descriptors. **Access tags** are metadata nodes attached to load and store instructions.; Access tags use type descriptors to describe the *location* being accessed; in terms of the type system of the higher level language. Access tags are; tuples consisting of a base type, an access type and an offset. The base; type is a scalar type descriptor or a struct type descriptor, the access; type is a scalar type descriptor, and the offset is a constant integer. The access tag ``(BaseTy, AccessTy, Offset)`` can describe one of two; things:. * If ``BaseTy`` is a struct type, the tag describes a memory access (load; or store) of a value of type ``AccessTy`` contained in the struct type; ``BaseTy`` at offset ``Offset``. * If ``BaseTy`` is a scalar type, ``Offset`` must be 0 and ``BaseTy`` and; ``AccessTy`` must be the same; and the access tag describes a scalar; access with scalar type ``AccessTy``. We first define an ``ImmediateParent`` relation on ``(BaseTy, Offset)``; tuples this way:. * If ``BaseTy`` is a scalar type then ``ImmediateParent(BaseTy, 0)`` is; ``(ParentTy, 0)`` where ``ParentTy`` is the parent of the scalar type as; described in the TBAA metadata. ``ImmediateParent(BaseTy, Offset)`` is; undefined if ``Offset`` is non-zero. * If ``BaseTy`` is a struct type then ``ImmediateParent(BaseTy, Offset)``; is ``(NewTy, NewOffset)`` where ``NewTy`` is the type contained in; ``BaseTy`` at offset ``Offset`` and ``NewOffset`` is ``Offset`` adjusted; to be relative within that inner type. A memory access with an access tag ``(BaseTy1, AccessTy1, Offset1)``; aliases a memory access with an access tag ``(BaseTy2",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:275104,load,load,275104,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"Enable debug output""), cl::Hidden, cl::location(DebugFlag));. In the above example, we specify ""``true``"" as the second argument to the; `cl::opt`_ template, indicating that the template should not maintain a copy of; the value itself. In addition to this, we specify the `cl::location`_; attribute, so that ``DebugFlag`` is automatically set. Option Attributes; -----------------. This section describes the basic attributes that you can specify on options. * The option name attribute (which is required for all options, except; `positional options`_) specifies what the option name is. This option is; specified in simple double quotes:. .. code-block:: c++. cl::opt<bool> Quiet(""quiet"");. .. _cl::desc(...):. * The **cl::desc** attribute specifies a description for the option to be; shown in the ``-help`` output for the program. This attribute supports; multi-line descriptions with lines separated by '\n'. .. _cl::value_desc:. * The **cl::value_desc** attribute specifies a string that can be used to; fine tune the ``-help`` output for a command line option. Look `here`_ for an; example. .. _cl::init:. * The **cl::init** attribute specifies an initial value for a `scalar`_; option. If this attribute is not specified then the command line option value; defaults to the value created by the default constructor for the; type. .. warning::. If you specify both **cl::init** and **cl::location** for an option, you; must specify **cl::location** first, so that when the command-line parser; sees **cl::init**, it knows where to put the initial value. (You will get an; error at runtime if you don't put them in the right order.). .. _cl::location:. * The **cl::location** attribute where to store the value for a parsed command; line option if using external storage. See the section on `Internal vs; External Storage`_ for more information. .. _cl::aliasopt:. * The **cl::aliasopt** attribute specifies which option a `cl::alias`_ option is; an alias for. .. _cl::values:. * The **cl::values",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst:36141,tune,tune,36141,interpreter/llvm-project/llvm/docs/CommandLine.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst,1,['tune'],['tune']
Performance,"Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 2. buffer/global/flat_atomic; fence release - singlethread *none* *none*; - wavefront; fence release - workgroup *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store atomic/; atomicrmw.; - Must happen before; any following store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Ensures that all; memory operations; have; completed before; performing the; following; fence-paired-atomic. fence release - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; spa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:360892,load,load,360892,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Configuration options for MVA method :. Configuration options reference for MVA method: SVM. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Gamma No 1 − RBF kernel parameter: Gamma (size of the Kernel). C No 1 − Cost parameter. Tol No 0.01 − Tolerance parameter. MaxIter No 1000 − Maximum number of training loops. Configuration options for MVA method :. Configuration options reference for MVA method: CFMlpANN. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No Fa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:8135,perform,performance,8135,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performance']
Performance,"Example; matcher = objcMessageExpr(isInstanceMessage()); matches; NSString *x = @""hello"";; [x containsString:@""h""];; but not; [NSString stringWithFormat:@""format""];. Matcher<ObjCMessageExpr>matchesSelectorStringRef RegExp, Regex::RegexFlags Flags = NoFlags; Matches ObjC selectors whose name contains; a substring matched by the given RegExp.; matcher = objCMessageExpr(matchesSelector(""loadHTMLStringmatches the outer message expr in the code below, but NOT the message; invocation for self.bodyView.; [self.bodyView loadHTMLString:html baseURL:NULL];. If the matcher is used in clang-query, RegexFlags parameter; should be passed as a quoted string. e.g: ""NoFlags"".; Flags can be combined with '|' example ""IgnoreCase | BasicRegex"". Matcher<ObjCMessageExpr>numSelectorArgsunsigned N; Matches when the selector has the specified number of arguments. matcher = objCMessageExpr(numSelectorArgs(0));; matches self.bodyView in the code below. matcher = objCMessageExpr(numSelectorArgs(2));; matches the invocation of ""loadHTMLString:baseURL:"" but not that; of self.bodyView; [self.bodyView loadHTMLString:html baseURL:NULL];. Matcher<ObjCMethodDecl>isClassMethod; Returns true when the Objective-C method declaration is a class method. Example; matcher = objcMethodDecl(isClassMethod()); matches; @interface I + (void)foo; @end; but not; @interface I - (void)bar; @end. Matcher<ObjCMethodDecl>isDefinition; Matches if a declaration has a body attached. Example matches A, va, fa; class A {};; class B; // Doesn't match, as it has no body.; int va;; extern int vb; // Doesn't match, as it doesn't define the variable.; void fa() {}; void fb(); // Doesn't match, as it has no body.; @interface X; - (void)ma; // Doesn't match, interface is declaration.; @end; @implementation X; - (void)ma {}; @end. Usable as: Matcher<TagDecl>, Matcher<VarDecl>, Matcher<FunctionDecl>,; Matcher<ObjCMethodDecl>. Matcher<ObjCMethodDecl>isInstanceMethod; Returns true when the Objective-C method declaration is an instance m",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html:108620,load,loadHTMLString,108620,interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,2,['load'],['loadHTMLString']
Performance,"Expression field of the original debug intrinsic. The second form, ``DBG_VALUE_LIST``, appears thus:. .. code-block:: text. DBG_VALUE_LIST !123, !DIExpression(DW_OP_LLVM_arg, 0, DW_OP_LLVM_arg, 1, DW_OP_plus), %1, %2. And has the following operands:; * The first operand is the Variable field of the original debug intrinsic.; * The second operand is the Expression field of the original debug intrinsic.; * Any number of operands, from the 3rd onwards, record a sequence of variable; location operands, which may take any of the same values as the first; operand of the ``DBG_VALUE`` instruction above. These variable location; operands are inserted into the final DWARF Expression in positions indicated; by the DW_OP_LLVM_arg operator in the `DIExpression; <LangRef.html#diexpression>`_. The position at which the DBG_VALUEs are inserted should correspond to the; positions of their matching ``llvm.dbg.value`` intrinsics in the IR block. As; with optimization, LLVM aims to preserve the order in which variable; assignments occurred in the source program. However SelectionDAG performs some; instruction scheduling, which can reorder assignments (discussed below).; Function parameter locations are moved to the beginning of the function if; they're not already, to ensure they're immediately available on function entry. To demonstrate variable locations during instruction selection, consider; the following example:. .. code-block:: llvm. define i32 @foo(i32* %addr) {; entry:; call void @llvm.dbg.value(metadata i32 0, metadata !3, metadata !DIExpression()), !dbg !5; br label %bb1, !dbg !5. bb1: ; preds = %bb1, %entry; %bar.0 = phi i32 [ 0, %entry ], [ %add, %bb1 ]; call void @llvm.dbg.value(metadata i32 %bar.0, metadata !3, metadata !DIExpression()), !dbg !5; %addr1 = getelementptr i32, i32 *%addr, i32 1, !dbg !5; call void @llvm.dbg.value(metadata i32 *%addr1, metadata !3, metadata !DIExpression()), !dbg !5; %loaded1 = load i32, i32* %addr1, !dbg !5; %addr2 = getelementptr i32, i32",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:28904,optimiz,optimization,28904,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimization']
Performance,"Extended SSA form, and; attach various forms of information to operands that dominate specific; uses. It is not meant for general use, only for building temporary; renaming forms that require value splits at certain points. .. _type.test:. '``llvm.type.test``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i1 @llvm.type.test(ptr %ptr, metadata %type) nounwind memory(none). Arguments:; """""""""""""""""""". The first argument is a pointer to be tested. The second argument is a; metadata object representing a :doc:`type identifier <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.test`` intrinsic tests whether the given pointer is associated; with the given type identifier. .. _type.checked.load:. '``llvm.type.checked.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load(ptr %ptr, i32 %offset, metadata %type) nounwind memory(argmem: read). Arguments:; """""""""""""""""""". The first argument is a pointer from which to load a function pointer. The; second argument is the byte offset from which to load the function pointer. The; third argument is a metadata object representing a :doc:`type identifier; <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.checked.load`` intrinsic safely loads a function pointer from a; virtual table pointer using type metadata. This intrinsic is used to implement; control flow integrity in conjunction with virtual call optimization. The; virtual call optimization pass will optimize away ``llvm.type.checked.load``; intrinsics associated with devirtualized calls, thereby removing the type; check in cases where it is not needed to enforce the control flow integrity; constraint. If the given pointer is associated with a type metadata identifier, this; function returns true as the second element of its return value. (Note that; the function may also return true if the given pointer is not associated; with a type metadata identifier.) If the function's return val",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:937970,load,load,937970,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"Extensions Documentation; <https://github.com/KhronosGroup/Khronosdotorg/blob/main/api/opencl/assets/OpenCL_LangExt.pdf>`_. OpenCL-Specific Attributes; --------------------------. OpenCL support in Clang contains a set of attribute taken directly from the; specification as well as additional attributes. See also :doc:`AttributeReference`. nosvm; ^^^^^. Clang supports this attribute to comply to OpenCL v2.0 conformance, but it; does not have any effect on the IR. For more details reffer to the specification; `section 6.7.2; <https://www.khronos.org/registry/cl/specs/opencl-2.0-openclc.pdf#49>`_. opencl_unroll_hint; ^^^^^^^^^^^^^^^^^^. The implementation of this feature mirrors the unroll hint for C.; More details on the syntax can be found in the specification; `section 6.11.5; <https://www.khronos.org/registry/cl/specs/opencl-2.0-openclc.pdf#61>`_. convergent; ^^^^^^^^^^. To make sure no invalid optimizations occur for single program multiple data; (SPMD) / single instruction multiple thread (SIMT) Clang provides attributes that; can be used for special functions that have cross work item semantics.; An example is the subgroup operations such as `intel_sub_group_shuffle; <https://www.khronos.org/registry/cl/extensions/intel/cl_intel_subgroups.txt>`_. .. code-block:: c. // Define custom my_sub_group_shuffle(data, c); // that makes use of intel_sub_group_shuffle; r1 = ...; if (r0) r1 = computeA();; // Shuffle data from r1 into r3; // of threads id r2.; r3 = my_sub_group_shuffle(r1, r2);; if (r0) r3 = computeB();. with non-SPMD semantics this is optimized to the following equivalent code:. .. code-block:: c. r1 = ...; if (!r0); // Incorrect functionality! The data in r1; // have not been computed by all threads yet.; r3 = my_sub_group_shuffle(r1, r2);; else {; r1 = computeA();; r3 = my_sub_group_shuffle(r1, r2);; r3 = computeB();; }. Declaring the function ``my_sub_group_shuffle`` with the convergent attribute; would prevent this:. .. code-block:: c. my_sub_group_shuff",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:149966,optimiz,optimizations,149966,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"F); option(LLVM_UNREACHABLE_OPTIMIZE ""Optimize llvm_unreachable() as undefined behavior (default), guaranteed trap when OFF"" ON). if( NOT uppercase_CMAKE_BUILD_TYPE STREQUAL ""DEBUG"" ); option(LLVM_ENABLE_ASSERTIONS ""Enable assertions"" OFF); else(); option(LLVM_ENABLE_ASSERTIONS ""Enable assertions"" ON); endif(). option(LLVM_ENABLE_EXPENSIVE_CHECKS ""Enable expensive checks"" OFF). # While adding scalable vector support to LLVM, we temporarily want to; # allow an implicit conversion of TypeSize to uint64_t, and to allow; # code to get the fixed number of elements from a possibly scalable vector.; # This CMake flag enables a more strict mode where it asserts that the type; # is not a scalable vector type.; #; # Enabling this flag makes it easier to find cases where the compiler makes; # assumptions on the size being 'fixed size', when building tests for; # SVE/SVE2 or other scalable vector architectures.; option(LLVM_ENABLE_STRICT_FIXED_SIZE_VECTORS; ""Enable assertions that type is not scalable in implicit conversion from TypeSize to uint64_t and calls to getNumElements"" OFF). set(LLVM_ABI_BREAKING_CHECKS ""WITH_ASSERTS"" CACHE STRING; ""Enable abi-breaking checks. Can be WITH_ASSERTS, FORCE_ON or FORCE_OFF.""). option(LLVM_FORCE_USE_OLD_TOOLCHAIN; ""Set to ON to force using an old, unsupported host toolchain."" OFF). set(LLVM_LOCAL_RPATH """" CACHE FILEPATH; ""If set, an absolute path added as rpath on binaries that do not already contain an executable-relative rpath.""). option(LLVM_TEMPORARILY_ALLOW_OLD_TOOLCHAIN; ""Set to ON to only warn when using a toolchain which is about to be deprecated, instead of emitting an error."" OFF). option(LLVM_USE_INTEL_JITEVENTS; ""Use Intel JIT API to inform Intel(R) VTune(TM) Amplifier XE 2011 about JIT code""; OFF). if( LLVM_USE_INTEL_JITEVENTS ); # Verify we are on a supported platform; if( NOT CMAKE_SYSTEM_NAME MATCHES ""Windows"" AND NOT CMAKE_SYSTEM_NAME MATCHES ""Linux"" ); message(FATAL_ERROR; ""Intel JIT API support is available on Linux and W",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:26091,scalab,scalable,26091,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['scalab'],['scalable']
Performance,"F1`; instances. Here, we used a macro, some sort of lightweight program, that the; interpreter distributed with ROOT, Cling, is able to execute. This is a; rather extraordinary situation, since `C++` is not natively an interpreted; language! There is much more to say: chapter is indeed dedicated to; macros. ## Controlling ROOT ##. One more remark at this point: as every command you type into ROOT is; usually interpreted by Cling, an ""escape character"" is needed to pass; commands to ROOT directly. This character is the dot at the beginning of; a line:. ``` {.cpp}; root [1] .<command>; ```. This is a selection of the most common commands. - **quit root**, simply type `.q` or `.quit` or `.exit`. - obtain the full **list of commands**, use `.?` or `.help`. - **access the shell** of the operating system, type `.!<OS_command>`;; try, e.g. `.!ls` or `.!pwd`. - **execute a macro**, enter `.x <file_name>`; in the above example,; you might have used `.x slits.C` at the ROOT prompt. - **load a macro**, type `.L <file_name>`; in the above example, you; might instead have used the command `.L slits.C` followed by the; function call `slits();`. Note that after loading a macro all; functions and procedures defined therein are available at the ROOT; prompt. - **compile a macro**, type `.L <file_name>+`; ROOT is able to manage; for you the `C++` compiler behind the scenes and to produce machine; code starting from your macro. One could decide to compile a macro; in order to obtain better performance or to get nearer to the; production environment. ## Plotting Measurements ##. To display measurements in ROOT, including errors, there exists a; powerful class `TGraphErrors` with different types of constructors. In; the example here, we use data from the file `ExampleData.txt` in text; format:. ``` {.cpp}; root [0] TGraphErrors gr(""ExampleData.txt"");; root [1] gr.Draw(""AP"");; ```. You should see the output shown in Figure [2.2](#f22). [f22]: figures/TGraphErrors_Example.png ""f22""; <a na",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/ROOT_as_calculator.md:8226,load,load,8226,documentation/primer/ROOT_as_calculator.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/ROOT_as_calculator.md,1,['load'],['load']
Performance,"FLAT_SCRATCH_LO is in units of 256 bytes, the offset must be right; shifted by 8 before moving into FLAT_SCRATCH_HI. FLAT_SCRATCH_HI corresponds to SGPRn-4 on GFX7, and SGPRn-6 on GFX8 (where; SGPRn is the highest numbered SGPR allocated to the wavefront).; FLAT_SCRATCH_HI is multiplied by 256 (as it is in units of 256 bytes) and; added to ``SH_HIDDEN_PRIVATE_BASE_VIMID`` to calculate the per wavefront; FLAT SCRATCH BASE in flat memory instructions that access the scratch; aperture.; 2. The second word of Flat Scratch Init is 32-bit byte size of a single; work-items scratch memory usage. CP obtains this from the runtime, and it is always a multiple of DWORD. CP; checks that the value in the kernel dispatch packet Private Segment Byte; Size is not larger and requests the runtime to increase the queue's scratch; size if necessary. CP directly loads from the kernel dispatch packet Private Segment Byte Size; field and rounds up to a multiple of DWORD. Having CP load it once avoids; loading it at the beginning of every wavefront. The kernel prolog code must move it to FLAT_SCRATCH_LO which is SGPRn-3 on; GFX7 and SGPRn-5 on GFX8. FLAT_SCRATCH_LO is used as the FLAT SCRATCH SIZE; in flat memory instructions. * If the *Target Properties* column of :ref:`amdgpu-processor-table`; specifies *Absolute flat scratch*:. If the kernel or any function it calls may use flat operations to access; scratch memory, the prolog code must set up the FLAT_SCRATCH register pair; (FLAT_SCRATCH_LO/FLAT_SCRATCH_HI which are in SGPRn-4/SGPRn-3). Initialization; uses Flat Scratch Init and Scratch Wavefront Offset SGPR registers (see; :ref:`amdgpu-amdhsa-initial-kernel-execution-state`):. The Flat Scratch Init is the 64-bit address of the base of scratch backing; memory being managed by SPI for the queue executing the kernel dispatch. CP obtains this from the runtime. The kernel prolog must add the value of the wave's Scratch Wavefront Offset; and move the result as a 64-bit value to the FLAT_SCRAT",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:196167,load,load,196167,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],"['load', 'loading']"
Performance,"FP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; if (!BodyVal); return nullptr;. Finally, before returning, we restore the previous variable bindings:. .. code-block:: c++. // Pop all our variables from scope.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i); NamedValues[VarNames[i].first] = OldBindings[i];. // Return the body computation.; return BodyVal;; }. The end result of all of this is that we get properly scoped variable; definitions, and we even (trivially) allow mutation of them :). With this, we completed what we set out to do. Our nice iterative fib; example from the intro compiles and runs just fine. The mem2reg pass; optimizes all of our stack variables into SSA registers, inserting PHI; nodes where needed, and our front-end remains simple: no ""iterated; dominance frontier"" computation anywhere in sight. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; mutable variables and var/in support. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter7/toy.cpp; :language: c++. `Next: Compiling to Object Code <LangImpl08.html>`_. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:28838,optimiz,optimizes,28838,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['optimiz'],['optimizes']
Performance,"FX941; - constant buffer/global/flat_store; sc0=1 sc1=1; GFX942; buffer/global/flat_store. - !volatile & nontemporal. 1. GFX940, GFX941; buffer/global/flat_store; nt=1 sc0=1 sc1=1; GFX942; buffer/global/flat_store; nt=1. - volatile. 1. buffer/global/flat_store; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic sc0=1; load atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_load; load atomic monotonic - agent - global 1. buffer/global/flat_load; - generic sc1=1; load atomic monotonic - system - global 1. buffer/global/flat_load; - generic sc0=1 sc1=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; store atomic monotonic - workgroup - global 1. buffer/global/flat_store; - generic sc0=1; store atomic monotonic - agent - global 1. buffer/global/flat_store; - generic sc1=1; store atomic monotonic - system - global 1. buffer/global/flat_store; - generic sc0=1 sc1=1; store atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_store; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:294159,load,load,294159,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"File (#208); 3. Fix TGraph tooltips handling; 4. Fix TH2Poly tooltips handling. ## Changes in 6.0.0; 1. Major release with:; - incompatible changes in API; - heavy use of Promise class; - upgrade all used packages; 2. Use generic naming convention - all class names always starts from; capital letter like ""ObjectPainter"", all function names starts from small; letter like ""painter.getObjectHint()""; 3. Rename JSRootCore.js -> JSRoot.core.js, eliminate all URL parameters.; Loading of extra JSROOT functionality should be done via JSROOT.require() method; All other scripts uses similar naming convention.; 4. JSROOT.draw()/JSROOT.redraw() functions returns Promise, deprecate callback parameter; 5. Introduce JSROOT.httpRequest() function which returns Promise instance, deprecate; JSROOT.NewHttpRequest() function; 6. JSROOT.openFile() returns Promise with file instance, deprecate callback parameter; 7. Provide new code loader via JSROOT.require(); - introduces clean dependencies in JSROOT code; - by default uses plain script loading emulating require.js behavior; - can use require.js when available; - uses require() method when running inside node.js; - supports openui5 sap.ui.require loader if available before JSRoot.core.js; - deprecates old JSROOT.AssertPrerequisites() function; 8. Upgrade d3.js to v6.1.1, skip support of older versions; 9. Upgrade three.js to r121:; - SoftwareRenderer deprecated and removed; - let use WebGL for browser, batch and node.js (via headless-gl); - support r3d_gl, r3d_img, r3d_svg rendering options for TGeo and histograms; - keep support of SVGRendered as backup solution; 10. Upgrade MathJax.js to version 3.1.1; - reliably works in browser and node.js!; - all latex/mathjax related methods moved to special JSRoot.latex.js script, loaded on demand; 11. Update jquery to 3.5.1, openui5 to 1.82.2; 12. Use JS classes only in few places - performance is not good enough compared to Object.prototype; 13. Deprecate IE support; 14. Deprecate bower package",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:25531,load,loading,25531,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loading']
Performance,"FileCheck - Flexible pattern matching file verifier; ===================================================. .. program:: FileCheck. SYNOPSIS; --------. :program:`FileCheck` *match-filename* [*--check-prefix=XXX*] [*--strict-whitespace*]. DESCRIPTION; -----------. :program:`FileCheck` reads two files (one from standard input, and one; specified on the command line) and uses one to verify the other. This; behavior is particularly useful for the testsuite, which wants to verify that; the output of some tool (e.g. :program:`llc`) contains the expected information; (for example, a movsd from esp or whatever is interesting). This is similar to; using :program:`grep`, but it is optimized for matching multiple different; inputs in one file in a specific order. The ``match-filename`` file specifies the file that contains the patterns to; match. The file to verify is read from standard input unless the; :option:`--input-file` option is used. OPTIONS; -------. Options are parsed from the environment variable ``FILECHECK_OPTS``; and from the command line. .. option:: -help. Print a summary of command line options. .. option:: --check-prefix prefix. FileCheck searches the contents of ``match-filename`` for patterns to; match. By default, these patterns are prefixed with ""``CHECK:``"".; If you'd like to use a different prefix (e.g. because the same input; file is checking multiple different tool or options), the; :option:`--check-prefix` argument allows you to specify (without the trailing; ""``:``"") one or more prefixes to match. Multiple prefixes are useful for tests; which might change for different run options, but most lines remain the same. FileCheck does not permit duplicate prefixes, even if one is a check prefix; and one is a comment prefix (see :option:`--comment-prefixes` below). .. option:: --check-prefixes prefix1,prefix2,... An alias of :option:`--check-prefix` that allows multiple prefixes to be; specified as a comma separated list. .. option:: --comment-prefixes prefix",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst:678,optimiz,optimized,678,interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,1,['optimiz'],['optimized']
Performance,"FisherCuts No False − Use multivariate splits using the Fisher criterion. MinLinCorrForFisher No 0.8 − The minimum linear correlation between two variables demanded for use in Fisher criterion in node splitting. UseExclusiveVars No False − Variables already used in fisher criterion are not anymore analysed individually for node splitting. DoPreselection No False − and and apply automatic pre-selection for 100% efficient signal (bkg) cuts prior to training. RenormByClass No False − Individually re-normalize each event class to the original size after boosting. SigToBkgFraction No 1 − Sig to Bkg ratio used in Training (similar to NodePurityLimit, which cannot be used in real adaboost. PruneMethod No NoPruning NoPruning, ExpectedError, CostComplexity Note: for BDTs use small trees (e.g.MaxDepth=3) and NoPruning: Pruning: Method used for pruning (removal) of statistically insignificant branches . PruneStrength No 0 − Pruning strength. PruningValFraction No 0.5 − Fraction of events to use for optimizing automatic pruning. nEventsMin No 0 − deprecated: Use MinNodeSize (in % of training events) instead. GradBaggingFraction No 0.6 − deprecated: Use *BaggedSampleFraction* instead: Defines the fraction of events to be used in each iteration, e.g. when UseBaggedGrad=kTRUE. . UseNTrainEvents No 0 − deprecated: Use *BaggedSampleFraction* instead: Number of randomly picked training events used in randomised (and bagged) trees. NNodesMax No 0 − deprecated: Use MaxDepth instead to limit the tree size. Configuration options for MVA method :. Configuration options reference for MVA method: Boost. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PC",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:15088,optimiz,optimizing,15088,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['optimiz'],['optimizing']
Performance,"Flag(UInt_t(random+0.5));; event->SetTemperature(random+20.);; for(UChar_t m = 0; m < 10; m++) {; event->SetMeasure(m, Int_t(gRandom->Gaus(m,m+1)));; }; // continued...; // fill the matrix; for(UChar_t i0 = 0; i0 < 4; i0++) {; for(UChar_t i1 = 0; i1 < 4; i1++) {; event->SetMatrix(i0,i1,gRandom->Gaus(i0*i1,1));; }; }; // create and fill the Track objects; for (Int_t t = 0; t < ntrack; t++) event->AddTrack(random);; t4.Fill(); // Fill the tree; event->Clear(); // Clear before reloading event; }; f.Write(); // Write the file header; t4.Print(); // Print the tree contents; }; ```. ### Reading the Tree. First, we check if the shared library with the class definitions is; loaded. If not we load it. Then we read two branches, one for the number; of tracks and one for the entire event. We check the number of tracks; first, and if it meets our condition, we read the entire event. We show; the fist entry that meets the condition. ``` {.cpp}; void tree4r() {; // check if the event class is in the dictionary; // if it is not load the definition in libEvent.so; if (!TClassTable::GetDict(""Event"")) {; gSystem->Load(""$ROOTSYS/test/libEvent.so"");; }; // read the tree generated with tree4w. // note that we use ""new"" to create the TFile and TTree objects, because we; // want to keep these objects alive when we leave this function.; TFile *f = new TFile(""tree4.root"");; TTree *t4 = (TTree*)f->Get(""t4"");. // create a pointer to an event object for reading the branch values.; Event *event = new Event();; // get two branches and set the branch address; TBranch *bntrack = t4->GetBranch(""fNtrack"");; TBranch *branch = t4->GetBranch(""event_split"");; branch->SetAddress(&event);. Int_t nevent = t4->GetEntries();; Int_t nselected = 0;; Int_t nb = 0;; for (Int_t i=0; i<nevent; i++) {; //read branch ""fNtrack""only; bntrack->GetEntry(i);. // reject events with more than 587 tracks; if (event->GetNtrack() > 587)continue;. // read complete accepted event in memory; nb += t4->GetEntry(i);; nselected++;. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md:64290,load,load,64290,documentation/users-guide/Trees.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Trees.md,1,['load'],['load']
Performance,"FlushBaskets has been called.; This minimizes the number of cases where one has to seek backward when reading. This function may be called at the start of a program to change; the default value for fAutoFlush. CASE 1 : autof > 0. autof is the number of consecutive entries after which TTree::Fill will; flush all branch buffers to disk. CASE 2 : autof < 0. When filling the Tree the branch buffers will be flushed to disk when; more than autof bytes have been written to the file. At the first FlushBaskets; TTree::Fill will replace fAutoFlush by the current value of fEntries. Calling this function with autof < 0 is interesting when it is hard to estimate; the size of one entry. This value is also independent of the Tree. When calling SetAutoFlush with no arguments, the; default value is -30000000, ie that the first AutoFlush will be done when; 30 MBytes of data are written to the file. CASE 3 : autof = 0; The AutoFlush mechanism is disabled. Flushing the buffers at regular intervals optimize the location of; consecutive entries on the disk. Changed the default value of AutoSave from 10 to 30 MBytes. New class TTreePerfStats; This new class is an important tool to measure the I/O performance of a Tree.; It shows the locations in the file when reading a Tree. In particular it is easy; to see the performance of the Tree Cache. The results can be:. drawn in a canvas.; printed on standard output.; saved to a file for processing later. Example of use; {; TFile *f = TFile::Open(""RelValMinBias-GEN-SIM-RECO.root"");; T = (TTree*)f->Get(""Events"");; Long64_t nentries = T->GetEntries();; T->SetCacheSize(10000000);; T->AddBranchToCache(""*"");. TTreePerfStats *ps= new TTreePerfStats(""ioperf"",T);. for (Int_t i=0;i<nentries;i++) {; T->GetEntry(i);; }; ps->SaveAs(""atlas_perf.root"");; }. then, in a root interactive session, one can do:. root > TFile f(""atlas_perf.root"");; root > ioperf->Draw();; root > ioperf->Print();. The Draw or Print functions print the following information:. TreeCache ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html:7051,optimiz,optimize,7051,tree/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html,2,['optimiz'],['optimize']
Performance,"For GFX6-GFX9:. * Each agent has multiple shader arrays (SA).; * Each SA has multiple compute units (CU).; * Each CU has multiple SIMDs that execute wavefronts.; * The wavefronts for a single work-group are executed in the same CU but may be; executed by different SIMDs.; * Each CU has a single LDS memory shared by the wavefronts of the work-groups; executing on it.; * All LDS operations of a CU are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; CU. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations and; completion is reported to a wavefront in execution order. The exception is; that for GFX7-GFX9 ``flat_load/store/atomic`` instructions can report out of; vector memory order if they access LDS memory, and out of LDS operation order; if they access global memory.; * The vector memory operations access a single vector L1 cache shared by all; SIMDs a CU. Therefore, no special action is required for coherence between the; lanes of a single wavefront, or for coherence between wavefronts in the same; work-group. A ``buffer_wbinvl1_vol`` is required for coherence between; wavefronts executing in different work-groups as they may be executing on; different CUs.; * The scalar memory operations access a scalar L1 cache shared by all wavefronts; on a group of CUs. The scalar and vector L1 caches are not coheren",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:207311,perform,performed,207311,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"For example see tutorials/eve/cms_calo.C. Possible performance issues with ATI drivers (fglrx). In late 2007 ATI switched to a new driver architecture. With these; drivers a significant degradation of GL performance in selection mode,; up to a factor of 50, was observed. Both linux and Windows drivers; were affected. The issue has been resolved in the latest driver; versions. Eve; Major changes. Support for multiple, parallel OpenGL views that can show different; projections of the same event. Provide object selection and feedback highlight across all GL-views and; list-trees. New classes for visualization of calorimeter data,; TEveCaloXYZ, see tutorials/eve/cms_calo.C. Available; representations: 3D-cylindrical view, projected views r-phi and rho-z,; and lego-view (with dedicated event handler allowing detailed; inspection of the data). Support for compound objects in view of selection, highlight and; color managament (see class TEveCompound). Optimize updates of GL-scenes by introducing change-stamping bits; into TEveElement. See methods AddStamp() and; StampXyzz(). Added support for central management of visualization parameters; of objects. Instead of specifying visual attributes individually by; set-methods a single string tag can be used to retrieve all of them; with a single command, e.g.,; track->ApplyVizTag(""MuonTrack""). The parameter-database can; be saved as a CINT script, edited manually and loaded. This provides more; flexibility as different users can share the same code to; instantiate visualziation objects but still override visualization; parameters independently. See TEveElement::CopyVizParams(); and TEveManager::*VizDB() methods for more information. Minor changes, fixes and improvements. Improved handling of projected elements. For fish-eye projections, allow fixing of compression scale; beyond given distance from the center. Add support for step-function scaling of 2D-projections. This; allows arbitrary magnification of concentric regions in r-ph",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf3d/doc/v520/index.html:1955,Optimiz,Optimize,1955,graf3d/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf3d/doc/v520/index.html,1,['Optimiz'],['Optimize']
Performance,"For example, software pipelining; schedules multiple iterations in an interleaved fashion to allow the; instructions of one iteration to hide the latencies of the instructions of; another iteration. Another example is vectorization that can exploit SIMD; hardware to allow a single instruction to execute multiple iterations using; vector registers. Note that although this is similar to SIMT execution, the way a client debugger; uses the information is fundamentally different. In SIMT execution the debugger; needs to present the concurrent execution as distinct source language threads; that the user can list and switch focus between. With iteration concurrency; optimizations, such as software pipelining and vectorized SIMD, the debugger; must not present the concurrency as distinct source language threads. Instead,; it must inform the user that multiple loop iterations are executing in parallel; and allow the user to select between them. In general, SIMT execution fixes the number of concurrent executions per target; architecture thread. However, both software pipelining and SIMD vectorization; may vary the number of concurrent iterations for different loops executed by a; single source language thread. It is possible for the compiler to use both SIMT concurrency and iteration; concurrency techniques in the code of a single source language thread. Therefore, a DWARF operation is required to denote the current concurrent; iteration instance, much like ``DW_OP_push_object_address`` denotes the current; object. See ``DW_OP_LLVM_push_iteration`` in; :ref:`amdgpu-dwarf-literal-operations`. In addition, a way is needed for the compiler to communicate how many source; language loop iterations are executing concurrently. See; ``DW_AT_LLVM_iterations`` in :ref:`amdgpu-dwarf-low-level-information`. 2.20 DWARF Operation to Create Runtime Overlay Composite Location Description; -----------------------------------------------------------------------------. It is common in SIMD vec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:33960,concurren,concurrent,33960,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['concurren'],['concurrent']
Performance,"Fraser U) for suggesting; this.; ; Methods:. BDT: New Decision Tree Pruning algorithm: Cost; Complexity Pruning a la CART. Written by Doug Schouten; (Fraser U.). It replaces the old CostComplexity and; CostComplexity2 algorithms.; . BDT: New no splitting option (choosable with; NCuts<0) that finds best split point by first sorting the; events for each variable and then looping through all; events, placing the cuts always in the middle between two; of the sorted events, and finding the true possible; maximum separation gain in the training sample by cutting; on this variable.; . BDT, AdaBoost The beta parameter is now an; option (default is 1).; . BDT: The node purity at which a node is; classified as signal (respective background node) for; determining the error fraction in the pruning became a; parameter that can be set via the option NodePurityLimit; (default is 0.5).; . Dataset preparation:. First implementation of a new preprocessing method: transformation of the; variables first into a Gaussian distribution, then performing a decorrelation of; the ""Gaussianised"" variables. The transformation is again done by default such that; (by default) the signal distributions become Gaussian and are decorrelated. Note ; that simultaneous Gaussianisation and decorrelation of signal and background is ; only possible (and done) for methods, such as Likelihood, which test both hypotheses.; . Bug fixes:. Fix in Expected error pruning: Rather than multiplying both sides, the error on ; the node and the sub-tree, with the prune strength, now only the expected error ; of the sub-tree is scaled.; . Fix in FDA parsing of the input formula. There were problems when treating; more than 10 parameters (thanks to Hugh Skottowe for reporting this).; . Calculation of ""Separation"": fixed bin-shift and; normalisation bugs. Thanks to Dag Gillberg (Fraser U) for; spotting these.; . Fixed problem in ""SetSignal(Background)WeightExpression"":; signal (background weight expressions not existing in t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v522/index.html:1360,perform,performing,1360,tmva/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/doc/v522/index.html,2,['perform'],['performing']
Performance,"GC intrinsic lowering; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. As a part of lowering to the explicit model of relocations; RewriteStatepointsForGC performs GC specific lowering for the following; intrinsics:. * ``gc.get.pointer.base``; * ``gc.get.pointer.offset``; * ``llvm.memcpy.element.unordered.atomic.*``; * ``llvm.memmove.element.unordered.atomic.*``. There are two possible lowerings for the memcpy and memmove operations:; GC leaf lowering and GC parseable lowering. If a call is explicitly marked with; ""gc-leaf-function"" attribute the call is lowered to a GC leaf call to; '``__llvm_memcpy_element_unordered_atomic_*``' or; '``__llvm_memmove_element_unordered_atomic_*``' symbol. Such a call can not; take a safepoint. Otherwise, the call is made GC parseable by wrapping the; call into a statepoint. This makes it possible to take a safepoint during; copy operation. Note that a GC parseable copy operation is not required to; take a safepoint. For example, a short copy operation may be performed without; taking a safepoint. GC parseable calls to '``llvm.memcpy.element.unordered.atomic.*``',; '``llvm.memmove.element.unordered.atomic.*``' intrinsics are lowered to calls; to '``__llvm_memcpy_element_unordered_atomic_safepoint_*``',; '``__llvm_memmove_element_unordered_atomic_safepoint_*``' symbols respectively.; This way the runtime can provide implementations of copy operations with and; without safepoints. GC parseable lowering also involves adjusting the arguments for the call.; Memcpy and memmove intrinsics take derived pointers as source and destination; arguments. If a copy operation takes a safepoint it might need to relocate the; underlying source and destination objects. This requires the corresponding base; pointers to be available in the copy operation. In order to make the base; pointers available RewriteStatepointsForGC replaces derived pointers with base; pointer and offset pairs. For example:. .. code-block:: llvm. declare void @__llvm_memcpy_element_u",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst:30289,perform,performed,30289,interpreter/llvm-project/llvm/docs/Statepoints.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Statepoints.rst,1,['perform'],['performed']
Performance,"GNU-style or C++11; non-standard attribute ``optnone`` can be used. .. code-block:: c++. // The following functions will not be optimized.; // GNU-style attribute; __attribute__((optnone)) int foo() {; // ... code; }; // C++11 attribute; [[clang::optnone]] int bar() {; // ... code; }. To facilitate disabling optimization for a range of function definitions, a; range-based pragma is provided. Its syntax is ``#pragma clang optimize``; followed by ``off`` or ``on``. All function definitions in the region between an ``off`` and the following; ``on`` will be decorated with the ``optnone`` attribute unless doing so would; conflict with explicit attributes already present on the function (e.g. the; ones that control inlining). .. code-block:: c++. #pragma clang optimize off; // This function will be decorated with optnone.; int foo() {; // ... code; }. // optnone conflicts with always_inline, so bar() will not be decorated.; __attribute__((always_inline)) int bar() {; // ... code; }; #pragma clang optimize on. If no ``on`` is found to close an ``off`` region, the end of the region is the; end of the compilation unit. Note that a stray ``#pragma clang optimize on`` does not selectively enable; additional optimizations when compiling at low optimization levels. This feature; can only be used to selectively disable optimizations. The pragma has an effect on functions only at the point of their definition; for; function templates, this means that the state of the pragma at the point of an; instantiation is not necessarily relevant. Consider the following example:. .. code-block:: c++. template<typename T> T twice(T t) {; return 2 * t;; }. #pragma clang optimize off; template<typename T> T thrice(T t) {; return 3 * t;; }. int container(int a, int b) {; return twice(a) + thrice(b);; }; #pragma clang optimize on. In this example, the definition of the template function ``twice`` is outside; the pragma region, whereas the definition of ``thrice`` is inside the region.; The ``conta",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:159718,optimiz,optimize,159718,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimize']
Performance,"GetTopVolume()->Draw(""ogl"");; ~~~. For any questions or comments about the GDML->ROOT binding please contact ben.lloyd@cern.ch. ### ROOT->GDML. The TGeo to GDML converter allows to export ROOT geometries (TGeo; geometry trees) as GDML files. The writer module writes a GDML file; out of the 'in-memory' representation of the geometry. The actual; application-specific (ROOT) binding is implemented in ROOTwriter; module. It contains 'binding methods' for TGeo geometry classes which; can be exported in GDML format. Please refere to the comment part of; the ROOTwriter.py file for the list of presently supported TGeo; classes. The ROOTwriter class contains also three methods,; dumpMaterials, dumpSolids and examineVol which need to be called in; order to export materials, solids and geometry tree respectively. The TGeo to GDML converter is now interfaced to the; TGeoManager::Export method which automatically calls the appropriate; Python scripts whenever the geometry output file has the .gdml; extension. Alternatively, one can also use the ROOT->GDML converter directly from; the Python prompt (assuming the TGeo geometry has already been loaded; into memory in one or another way), for example:. ~~~ {.cpp}; from math import *. import ROOT; import writer; import ROOTwriter. # get TGeoManager and; # get the top volume of the existing (in-memory) geometry tree; geomgr = ROOT.gGeoManager; topV = geomgr.GetTopVolume(). # instanciate writer; gdmlwriter = writer.writer('mygeo.gdml'); binding = ROOTwriter.ROOTwriter(gdmlwriter). # dump materials; matlist = geomgr.GetListOfMaterials(); binding.dumpMaterials(matlist). # dump solids; shapelist = geomgr.GetListOfShapes(); binding.dumpSolids(shapelist). # dump geo tree; print 'Traversing geometry tree'; gdmlwriter.addSetup('default', '1.0', topV.GetName()); binding.examineVol(topV). # write file; gdmlwriter.writeFile(); ~~~. For all other functionality questions or comments, or even GDML in general,; please email Witold.Pokorski@cern.ch; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/gdml/doc/index.md:2818,load,loaded,2818,geom/gdml/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/gdml/doc/index.md,1,['load'],['loaded']
Performance,"Getting Started/Tutorials; =========================. For those new to the LLVM system. .. toctree::; :hidden:. CompilerWriterInfo; Frontend/PerformanceTips; GettingStarted; GettingStartedVS; ProgrammersManual; tutorial/index; MyFirstTypoFix. :doc:`GettingStarted`; Discusses how to get up and running quickly with the LLVM infrastructure.; Everything from unpacking and compilation of the distribution to execution; of some tools. :doc:`tutorial/index`; Tutorials about using LLVM. Includes a tutorial about making a custom; language with LLVM. :doc:`ProgrammersManual`; Introduction to the general layout of the LLVM sourcebase, important classes; and APIs, and some tips & tricks. :doc:`Frontend/PerformanceTips`; A collection of tips for frontend authors on how to generate IR; which LLVM is able to effectively optimize. :doc:`GettingStartedVS`; An addendum to the main Getting Started guide for those using Visual Studio; on Windows. :doc:`CompilerWriterInfo`; A list of helpful links for compiler writers. :doc:`MyFirstTypoFix`; This tutorial will guide you through the process of making a change to; LLVM, and contributing it back to the LLVM project.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStartedTutorials.rst:816,optimiz,optimize,816,interpreter/llvm-project/llvm/docs/GettingStartedTutorials.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStartedTutorials.rst,1,['optimiz'],['optimize']
Performance,"HF_ALLOC`` + ``SHF_WRITE``; ``.debug_``\ *\** ``SHT_PROGBITS`` *none*; ``.dynamic`` ``SHT_DYNAMIC`` ``SHF_ALLOC``; ``.dynstr`` ``SHT_PROGBITS`` ``SHF_ALLOC``; ``.dynsym`` ``SHT_PROGBITS`` ``SHF_ALLOC``; ``.got`` ``SHT_PROGBITS`` ``SHF_ALLOC`` + ``SHF_WRITE``; ``.hash`` ``SHT_HASH`` ``SHF_ALLOC``; ``.note`` ``SHT_NOTE`` *none*; ``.rela``\ *name* ``SHT_RELA`` *none*; ``.rela.dyn`` ``SHT_RELA`` *none*; ``.rodata`` ``SHT_PROGBITS`` ``SHF_ALLOC``; ``.shstrtab`` ``SHT_STRTAB`` *none*; ``.strtab`` ``SHT_STRTAB`` *none*; ``.symtab`` ``SHT_SYMTAB`` *none*; ``.text`` ``SHT_PROGBITS`` ``SHF_ALLOC`` + ``SHF_EXECINSTR``; ================== ================ =================================. These sections have their standard meanings (see [ELF]_) and are only generated; if needed. ``.debug``\ *\**; The standard DWARF sections. See :ref:`amdgpu-dwarf-debug-information` for; information on the DWARF produced by the AMDGPU backend. ``.dynamic``, ``.dynstr``, ``.dynsym``, ``.hash``; The standard sections used by a dynamic loader. ``.note``; See :ref:`amdgpu-note-records` for the note records supported by the AMDGPU; backend. ``.rela``\ *name*, ``.rela.dyn``; For relocatable code objects, *name* is the name of the section that the; relocation records apply. For example, ``.rela.text`` is the section name for; relocation records associated with the ``.text`` section. For linked shared code objects, ``.rela.dyn`` contains all the relocation; records from each of the relocatable code object's ``.rela``\ *name* sections. See :ref:`amdgpu-relocation-records` for the relocation records supported by; the AMDGPU backend. ``.text``; The executable machine code for the kernels and functions they call. Generated; as position independent code. See :ref:`amdgpu-code-conventions` for; information on conventions used in the isa generation. .. _amdgpu-note-records:. Note Records; ------------. The AMDGPU backend code object contains ELF note records in the ``.note``; section. The set of generated not",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:68316,load,loader,68316,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loader']
Performance,"HLSLShader`` attribute to the function with the; specified name. This allows code generation for entry functions to always key; off the presence of the ``HLSLShader`` attribute, regardless of what shader; profile you are compiling. In code generation, two functions are generated. One is the user defined; function, which is code generated as a mangled C++ function with internal; linkage following normal function code generation. The actual exported entry function which can be called by the GPU driver is a; ``void(void)`` function that isn't name mangled. In code generation we generate; the unmangled entry function to serve as the actual shader entry. The shader; entry function is annotated with the ``hlsl.shader`` function attribute; identifying the entry's pipeline stage. The body of the unmangled entry function contains first a call to execute global; constructors, then instantiations of the user-defined entry parameters with; their semantic values populated, and a call to the user-defined function.; After the call instruction the return value (if any) is saved using a; target-appropriate intrinsic for storing outputs (for DirectX, the; ``llvm.dx.store.output``). Lastly, any present global destructors will be called; immediately before the return. HLSL does not support C++ ``atexit``; registrations, instead calls to global destructors are compile-time generated. .. note::. HLSL support in Clang is currently focused on compute shaders, which do not; support output semantics. Support for output semantics will not be; implemented until other shader profiles are supported. Below is example IR that represents the planned implementation, subject to; change as the ``llvm.dx.store.output`` and ``llvm.dx.load.input`` intrinsics are; not yet implemented. .. code-block:: none. ; Function Attrs: norecurse; define void @main() #1 {; entry:; %0 = call i32 @llvm.dx.load.input.i32(...); %1 = call i32 @""?main@@YAXII@Z""(i32 %0); call @llvm.dx.store.output.i32(%1, ...); ret void; }. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/EntryFunctions.rst:2761,load,load,2761,interpreter/llvm-project/clang/docs/HLSL/EntryFunctions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/EntryFunctions.rst,2,['load'],['load']
Performance,"HttpServer(""http:127.0.0.1:8080""); If host has several network interfaces, one could select one for binding:; new THttpServer(""http:192.168.1.17:8080""). ### TNetXNGFileStager; Fixed ROOT-7703. This restores the behavior of Locate() to that found with; TXNetFileStager: Rather than return only the xrootd server's reply, the endpoint; hostname is looked up and Locate() returns the full url, including the path. ### TWebFile; Fixed ROOT-7809. Returns an error for a redirect which does not specify the new; URI, rather than going into a loop. Fixed ROOT-7817. Avoid a crash under some circumstances when trying to open an; invalid path. ## GUI Libraries. ## Montecarlo Libraries. ## Multi-processing. With this version we introduce a new module, core/multiproc, for multi-processing on multi-core machines. This module is based on fork technology and offers an interface inspired from Python multiprocessor module. The new interface, implemented in the class TProcPool, provides the possibility to perform in parallel a very generic set of tasks, described by macros, functions or lambdas. This illustrates the usage of lambdas:. ``` {.cpp}; {; TProcPool pool;; auto ten = pool.MapReduce([]() { return 1; }, 10, [](std::vector<int> v) { return std::accumulate(v.begin(), v.end(), 0); }); }; ```. And this how it can be used to generate ten histos and merge them:. ``` {.cpp}; {; TObject *CreateAndFillHists(); {. TH1F *h = new TH1F(""h"", """", 100, -3., 3.);; h->SetDirectory(0);; h->FillRandom(""gaus"", 1000);; return h;; }. TProcPool pool;; auto hist = pool.MapReduce(CreateAndFillHists, 10, PoolUtils::ReduceObjects);; hist->DrawClone();; }; ```. Tutorials illustrating other usages of the new class TProcPool are available under tutorials/multicore. ## Language Bindings. ### Notebooks; We provided integration of ROOT with the Jupyter technology, integrating ROOT with Python Notebooks and providing a ROOT Kernel like functionality - de facto an enhanced C++ web based shell. Tab completion, output a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v606/index.md:21293,perform,perform,21293,README/ReleaseNotes/v606/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v606/index.md,1,['perform'],['perform']
Performance,"ION 3.20.0); project(SimpleProject). find_package(LLVM REQUIRED CONFIG). message(STATUS ""Found LLVM ${LLVM_PACKAGE_VERSION}""); message(STATUS ""Using LLVMConfig.cmake in: ${LLVM_DIR}""). # Set your project compile flags.; # E.g. if using the C++ header files; # you will need to enable C++11 support; # for your compiler. include_directories(${LLVM_INCLUDE_DIRS}); separate_arguments(LLVM_DEFINITIONS_LIST NATIVE_COMMAND ${LLVM_DEFINITIONS}); add_definitions(${LLVM_DEFINITIONS_LIST}). # Now build our tools; add_executable(simple-tool tool.cpp). # Find the libraries that correspond to the LLVM components; # that we wish to use; llvm_map_components_to_libnames(llvm_libs support core irreader). # Link against LLVM libraries; target_link_libraries(simple-tool ${llvm_libs}). The ``find_package(...)`` directive when used in CONFIG mode (as in the above; example) will look for the ``LLVMConfig.cmake`` file in various locations (see; cmake manual for details). It creates a ``LLVM_DIR`` cache entry to save the; directory where ``LLVMConfig.cmake`` is found or allows the user to specify the; directory (e.g. by passing ``-DLLVM_DIR=/usr/lib/cmake/llvm`` to; the ``cmake`` command or by setting it directly in ``ccmake`` or ``cmake-gui``). This file is available in two different locations. * ``<LLVM_INSTALL_PACKAGE_DIR>/LLVMConfig.cmake`` where; ``<LLVM_INSTALL_PACKAGE_DIR>`` is the location where LLVM CMake modules are; installed as part of an installed version of LLVM. This is typically; ``cmake/llvm/`` within the lib directory. On Linux, this is typically; ``/usr/lib/cmake/llvm/LLVMConfig.cmake``. * ``<LLVM_BUILD_ROOT>/lib/cmake/llvm/LLVMConfig.cmake`` where; ``<LLVM_BUILD_ROOT>`` is the root of the LLVM build tree. **Note: this is only; available when building LLVM with CMake.**. If LLVM is installed in your operating system's normal installation prefix (e.g.; on Linux this is usually ``/usr/``) ``find_package(LLVM ...)`` will; automatically find LLVM if it is installed correctly. I",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:42303,cache,cache,42303,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,1,['cache'],['cache']
Performance,"IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; TransformLayer(ES, CompileLayer, optimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we introduce a new member, TransformLayer, which sits; on top of our CompileLayer. We initialize our OptimizeLayer with a reference to; the ExecutionSession and output layer (standard practice for layers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:4056,optimiz,optimizeModule,4056,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimizeModule']
Performance,"IRgen optimization opportunities. //===---------------------------------------------------------------------===//. The common pattern of; --; short x; // or char, etc; (x == 10); --; generates an zext/sext of x which can easily be avoided. //===---------------------------------------------------------------------===//. Bitfields accesses can be shifted to simplify masking and sign; extension. For example, if the bitfield width is 8 and it is; appropriately aligned then is is a lot shorter to just load the char; directly. //===---------------------------------------------------------------------===//. It may be worth avoiding creation of alloca's for formal arguments; for the common situation where the argument is never written to or has; its address taken. The idea would be to begin generating code by using; the argument directly and if its address is taken or it is stored to; then generate the alloca and patch up the existing code. In theory, the same optimization could be a win for block local; variables as long as the declaration dominates all statements in the; block. NOTE: The main case we care about this for is for -O0 -g compile time; performance, and in that scenario we will need to emit the alloca; anyway currently to emit proper debug info. So this is blocked by; being able to emit debug information which refers to an LLVM; temporary, not an alloca. //===---------------------------------------------------------------------===//. We should try and avoid generating basic blocks which only contain; jumps. At -O0, this penalizes us all the way from IRgen (malloc &; instruction overhead), all the way down through code generation and; assembly time. On 176.gcc:expr.ll, it looks like over 12% of basic blocks are just; direct branches!. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/README.txt:6,optimiz,optimization,6,interpreter/llvm-project/clang/lib/CodeGen/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/README.txt,4,"['load', 'optimiz', 'perform']","['load', 'optimization', 'performance']"
Performance,"IX ps c.ommand;; ```. With the `mhs3` example, you should be able to see a canvas with two; pads on it. Both pads keep histograms updated and filled by three; different threads. With the `CalcPi` example, you should be able to see; two threads calculating Pi with the given number of intervals as; precision. ### TThread in More Details. Cling is not thread safe yet, and it will block the execution of the; threads until it has finished executing. #### Asynchronous Actions. Different threads can work simultaneously with the same object. Some; actions can be dangerous. For example, when two threads create a; histogram object, ROOT allocates memory and puts them to the same; collection. If it happens at the same time, the results are; undetermined. To avoid this problem, the user has to synchronize these; actions with:. ``` {.cpp}; TThread::Lock() // Locking the following part of code; ... // Create an object, etc...; TThread::UnLock() // Unlocking; ```. The code between `Lock()` and `UnLock()` will be performed; uninterrupted. No other threads can perform actions or access; objects/collections while it is being executed. The methods; `TThread::Lock() `and **`TThread::UnLock()`** internally use a global; `TMutex` instance for locking. The user may also define their own **`TMutex`** `MyMutex` instance and may; locally protect their asynchronous actions by calling `MyMutex.Lock()` and; `MyMutex.UnLock().`. #### Synchronous Actions: TCondition. To synchronize the actions of different threads you can use the; **`TCondition`** class, which provides a signaling mechanism. The; **`TCondition`** instance must be accessible by all threads that need to; use it, i.e. it should be a global object (or a member of the class; which owns the threaded methods, see below). To create a; **`TCondition`** object, a **`TMutex`** instance is required for the; Wait and `TimedWait` locking methods. One can pass the address of an; external mutex to the **`TCondition`** constructor:. ``` {.cpp}; TM",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md:7070,perform,performed,7070,documentation/users-guide/Threads.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md,1,['perform'],['performed']
Performance,"I`` to; ``rootcling``/``genreflex`` during build time).; When relocating the shared library, move the .pcm with it.; Once support for C++ modules is fully fleshed out, access to the header file; will no longer be needed. .. _`rootcling manual`: https://root.cern.ch/root/html/guides/users-guide/AddingaClass.html#the-linkdef.h-file; .. _`helper script`: https://github.com/wlav/cppyy/blob/master/test/make_dict_win32.py. Class loader; ^^^^^^^^^^^^. Explicitly loading dictionaries is fine if this is hidden under the hood of; a Python package and thus transparently done on ``import``.; Otherwise, the automatic class loader is more convenient, as it allows direct; use without having to manually find and load dictionaries (assuming these are; locatable by the dynamic loader). The class loader utilizes so-called rootmap files, which by convention should; live alongside the dictionary shared library (and C++ module file).; These are simple text files, which map C++ entities (such as classes) to the; dictionaries and other libraries that need to be loaded for their use. With ``genreflex``, the mapping file can be automatically created with; ``--rootmap-lib=MyClassDict``, where ""MyClassDict"" is the name of the shared; library (without the extension) build from the dictionary file.; With ``rootcling``, create the same mapping file with; ``-rmf MyClassDict.rootmap -rml MyClassDict``.; It is necessary to provide the final library name explicitly, since it is; only in the separate linking step where these names are fixed and those names; may not match the default choice. With the mapping file in place, the above example can be rerun without; explicit loading of the dictionary:. .. code-block:: python. >>> import cppyy; >>> from cppyy.gbl import MyClass; >>> MyClass(42).get_int(); 42; >>>. .. _cppyy-generator:. Bindings collection; -------------------. ``cppyy-generator`` is a clang-based utility program which takes a set of C++; header files and generates a JSON output file describ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst:8786,load,loaded,8786,bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,1,['load'],['loaded']
Performance,"If one has a web server which already provides such JSON file, one could specify the URL to this file like:. <https://root.cern/js/latest/demo/update_draw.htm?addr=../httpserver.C/Canvases/c1/root.json.gz>. Here the same problem with [Cross-Origin Request](https://developer.mozilla.org/en/http_access_control) can appear. If the web server configuration cannot be changed, just copy JSROOT to the web server itself. ### Binary file-based monitoring (not recommended). Theoretically, one could use binary ROOT files to implement monitoring.; With such approach, a ROOT-based application creates and regularly updates content of a ROOT file, which can be accessed via normal web server. From the browser side, JSROOT could regularly read the specified objects and update their drawings. But such solution has three major caveats. First of all, one need to store the data of all objects, which only potentially could be displayed in the browser. In case of 10 objects it does not matter, but for 1000 or 100000 objects this will be a major performance penalty. With such big amount of data one will never achieve higher update rate. The second problem is I/O. To read the first object from the ROOT file, one need to perform several (about 5) file-reading operations via http protocol.; There is no http file locking mechanism (at least not for standard web servers),; therefore there is no guarantee that the file content is not changed/replaced between consequent read operations. Therefore, one should expect frequent I/O failures while trying to monitor data from ROOT binary files. There is a workaround for the problem - one could load the file completely and exclude many partial I/O operations by this. To achieve this with JSROOT, one should add ""+"" sign at the end of the file name. Of course, it only could work for small files. If somebody still wants to use monitoring of data from ROOT files, could try link like:. - <https://root.cern/js/latest/?nobrowser&file=../files/hsimple.root+&item",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md:32766,perform,performance,32766,documentation/JSROOT/JSROOT.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md,1,['perform'],['performance']
Performance,"If we were not permitted in any event to shorten the lifetime of the; object in ``x``, then we would not be able to eliminate this retain; and release unless we could prove that the message send could not; modify ``_ivar`` (or deallocate ``self``). Since message sends are; opaque to the optimizer, this is not possible, and so ARC's hands; would be almost completely tied. ARC makes no guarantees about the execution of a computation history; which contains undefined behavior. In particular, ARC makes no; guarantees in the presence of race conditions. ARC may assume that any retainable object pointers it receives or; generates are instantaneously valid from that point until a point; which, by the concurrency model of the host language, happens-after; the generation of the pointer and happens-before a release of that; object (possibly via an aliasing pointer or indirectly due to; destruction of a different object). .. admonition:: Rationale. There is very little point in trying to guarantee correctness in the; presence of race conditions. ARC does not have a stack-scanning; garbage collector, and guaranteeing the atomicity of every load and; store operation would be prohibitive and preclude a vast amount of; optimization. ARC may assume that non-ARC code engages in sensible balancing; behavior and does not rely on exact or minimum retain count values; except as guaranteed by ``__strong`` object invariants or +1 transfer; conventions. For example, if an object is provably double-retained; and double-released, ARC may eliminate the inner retain and release;; it does not need to guard against code which performs an unbalanced; release followed by a ""balancing"" retain. .. _arc.optimization.liveness:. Object liveness; ---------------. ARC may not allow a retainable object ``X`` to be deallocated at a; time ``T`` in a computation history if:. * ``X`` is the value stored in a ``__strong`` object ``S`` with; :ref:`precise lifetime semantics <arc.optimization.precise>`, or. * ``X",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:77977,race condition,race conditions,77977,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['race condition'],['race conditions']
Performance,"InTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). FitMethod No GA GA, SA, MC, MCEvents, MINUIT, EventScan Minimisation Method (GA, SA, and MC are the primary methods to be used; the others have been introduced for testing purposes and are depreciated). EffMethod No EffSel EffSel, EffPDF Selection Method. CutRangeMin Yes -1 − Minimum of allowed cut range (set per variable). CutRangeMax Yes -1 − Maximum of allowed cut range (set per variable). VarProp Yes NotEnforced NotEnforced, FMax, FMin, FSmart Categorisation of cuts. Configuration options for MVA method :. Configuration options reference for MVA method: PDEFoam. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). SigBgSeparate No False − Separate foams for signal and background. TailCut No 0.001 − Fraction of outlier events that are excluded from the foam in each dimension. VolFrac No 0.0666667 − Size of sampling box, used for density calculation during foam build-up (maximum value: 1.0 is equivalent to volume of entire foam). nActiveCells No 500 − Maximum number of active cells to be created by the foam. nSampl No 2000 − Num",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:26030,perform,performed,26030,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performed']
Performance,"Indicates that PAL will program this user-SGPR to contain the amount of LDS; space used for the ES/GS pseudo-ring-buffer for passing data between shader; stages.; 0x1000000B ViewId View id (32-bit unsigned integer) identifies a view of graphic; pipeline instancing.; 0x1000000C StreamOutTable 32-bit pointer to GPU memory containing the stream out target SRD table. This; can only appear for one shader stage per pipeline.; 0x1000000D PerShaderPerfData 32-bit pointer to GPU memory containing the per-shader performance data buffer.; 0x1000000F VertexBufferTable 32-bit pointer to GPU memory containing the vertex buffer SRD table. This can; only appear for one shader stage per pipeline.; 0x10000010 UavExportTable 32-bit pointer to GPU memory containing the UAV export SRD table. This can; only appear for one shader stage per pipeline (PS). These replace color targets; and are completely separate from any UAVs used by the shader. This is optional,; and only used by the PS when UAV exports are used to replace color-target; exports to optimize specific shaders.; 0x10000011 NggCullingData 64-bit pointer to GPU memory containing the hardware register data needed by; some NGG pipelines to perform culling. This value contains the address of the; first of two consecutive registers which provide the full GPU address.; 0x10000015 FetchShaderPtr 64-bit pointer to GPU memory containing the fetch shader subroutine.; ========== ================= ===============================================================================. .. _amdgpu-amdpal-code-object-metadata-user-data-per-shader-table-section:. Per-Shader Table; ################. Low 32 bits of the GPU address for an optional buffer in the ``.data``; section of the ELF. The high 32 bits of the address match the high 32 bits; of the shader's program counter. The buffer can be anything the shader compiler needs it for, and; allows each shader to have its own region of the ``.data`` section.; Typically, this could be a table of buffer ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:417880,optimiz,optimize,417880,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['optimiz'],['optimize']
Performance,"It may; report undefined symbol errors, read archive members, replace weak symbols, etc.; The linker is able to do this seamlessly even though it does not know the exact; content of input LLVM bitcode files. If dead code stripping is enabled then the; linker collects the list of live symbols. Phase 3 : Optimize Bitcode Files; --------------------------------. After symbol resolution, the linker tells the LTO shared object which symbols; are needed by native object files. In the example above, the linker reports; that only ``foo1()`` is used by native object files using; ``lto_codegen_add_must_preserve_symbol()``. Next the linker invokes the LLVM; optimizer and code generators using ``lto_codegen_compile()`` which returns a; native object file creating by merging the LLVM bitcode files and applying; various optimization passes. Phase 4 : Symbol Resolution after optimization; ----------------------------------------------. In this phase, the linker reads optimized a native object file and updates the; internal global symbol table to reflect any changes. The linker also collects; information about any changes in use of external symbols by LLVM bitcode; files. In the example above, the linker notes that ``foo4()`` is not used any; more. If dead code stripping is enabled then the linker refreshes the live; symbol information appropriately and performs dead code stripping. After this phase, the linker continues linking as if it never saw LLVM bitcode; files. .. _libLTO:. ``libLTO``; ==========. ``libLTO`` is a shared object that is part of the LLVM tools, and is intended; for use by a linker. ``libLTO`` provides an abstract C interface to use the LLVM; interprocedural optimizer without exposing details of LLVM's internals. The; intention is to keep the interface as stable as possible even when the LLVM; optimizer continues to evolve. It should even be possible for a completely; different compilation technology to provide a different libLTO that works with; their object fil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:7484,optimiz,optimized,7484,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,1,['optimiz'],['optimized']
Performance,"Itanium Name Demangler Library; ==============================. Introduction; ------------. This directory contains the generic itanium name demangler; library. The main purpose of the library is to demangle C++ symbols,; i.e. convert the string ""_Z1fv"" into ""f()"". You can also use the CRTP; base ManglingParser to perform some simple analysis on the mangled; name, or (in LLVM) use the opaque ItaniumPartialDemangler to query the; demangled AST. Why are there multiple copies of the this library in the source tree?; ---------------------------------------------------------------------. The canonical sources are in libcxxabi/src/demangle and some of the; files are copied to llvm/include/llvm/Demangle. The simple reason for; this comes from before the monorepo, and both [sub]projects need to; demangle symbols, but neither can depend on each other. * libcxxabi needs the demangler to implement __cxa_demangle, which is; part of the itanium ABI spec. * LLVM needs a copy for a bunch of places, and cannot rely on the; system's __cxa_demangle because it a) might not be available (i.e.,; on Windows), and b) may not be up-to-date on the latest language; features. The copy of the demangler in LLVM has some extra stuff that aren't; needed in libcxxabi (ie, the MSVC demangler, ItaniumPartialDemangler),; which depend on the shared generic components. Despite these; differences, we want to keep the ""core"" generic demangling library; identical between both copies to simplify development and testing. If you're working on the generic library, then do the work first in; libcxxabi, then run libcxxabi/src/demangle/cp-to-llvm.sh. This; script takes as an optional argument the path to llvm, and copies the; changes you made to libcxxabi over. Note that this script just; blindly overwrites all changes to the generic library in llvm, so be; careful. Because the core demangler needs to work in libcxxabi, everything; needs to be declared in an anonymous namespace (see; DEMANGLE_NAMESPACE_BEGIN), an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt:316,perform,perform,316,interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,1,['perform'],['perform']
Performance,"JSRootPainter.more.js script, where painters for auxiliary classes; will be implemented.; 7. Implement painter for TEllipse, TLine, TArrow classes; 8. Fix several problems with markers drawing; implement plus, asterisk, mult symbols.; 9. Implement custom layout, which allows to configure user-defined layout for displayed objects; 10. Fix errors with scaling of axis labels.; 11. Support also Y axis with custom labels like: http://jsroot.gsi.de/dev/?nobrowser&file=../files/atlas.root&item=LEDShapeHeightCorr_Gain0;1&opt=col. ## Changes in 3.7; 1. Support of X axis with custom labels like: http://jsroot.gsi.de/dev/?nobrowser&json=../files/hist_xlabels.json; 2. Extend functionality of JSROOT.addDrawFunc() function. One could register type-specific; `make_request` and `after_request` functions; `icon`, `prereq`, `script`, `monitor` properties.; This let add more custom elements to the generic gui, implemented with JSROOT.HierarchyPainter; 3. Provide full support of require.js. One could load now JSRootCore.js script like:. <script type=""text/javascript"" src=""require.js"" data-main=""scripts/JSRootCore.js""></script>. After this several modules are defined and can be used with syntax like:. require(['JSRootPainter'], function(jsroot) { /*any user code*/});. Also inside JSROOT require.js used to load all dependencies. ## Changes in 3.6; 1. Try to provide workaround for websites where require.js already loaded.; This makes problem by direct loading of jquery and jquery-ui; 2. Provide workaround for older version of jquery-ui; 3. Prompt for input of command arguments; 4. After command execution one could automatically reload hierarchy (_hreload property) or; update view of displayed object (_update_item property); 5. Use HierarchyPainter for implementing draw.htm. This let us handle; all different kinds of extra attributes in central place; 6. Fix problem in tabs layout - new tab should be add to direct child; 7. When drawing several tabs, activate frame before drawing - only the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:63500,load,load,63500,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['load']
Performance,L CPU; ROOT_EXECUTABLE(testBackpropagationDLCpu TestBackpropagationDLCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Backpropagation-DL-Cpu COMMAND testBackpropagationDLCpu). # DNN - Batch normalization; ROOT_EXECUTABLE(testBatchNormalizationCpu TestBatchNormalizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-BatchNormalization-Cpu COMMAND testBatchNormalizationCpu). # DNN - Optimization CPU; ROOT_EXECUTABLE(testOptimizationCpu TestOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Optimization-Cpu COMMAND testOptimizationCpu). # DNN - MethodDL SGD Optimization CPU; ROOT_EXECUTABLE(testMethodDLSGDOptimizationCpu TestMethodDLSGDOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-SGD-Optimization-Cpu COMMAND testMethodDLSGDOptimizationCpu). # DNN - MethodDL Adam Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdamOptimizationCpu TestMethodDLAdamOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adam-Optimization-Cpu COMMAND testMethodDLAdamOptimizationCpu TIMEOUT 1800). # DNN - MethodDL Adagrad Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdagradOptimizationCpu TestMethodDLAdagradOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adagrad-Optimization-Cpu COMMAND testMethodDLAdagradOptimizationCpu). # DNN - MethodDL RMSProp Optimization CPU; ROOT_EXECUTABLE(testMethodDLRMSPropOptimizationCpu TestMethodDLRMSPropOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-RMSProp-Optimization-Cpu COMMAND testMethodDLRMSPropOptimizationCpu). # DNN - MethodDL Adadelta Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdadeltaOptimizationCpu TestMethodDLAdadeltaOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adadelta-Optimization-Cpu COMMAND testMethodDLAdadeltaOptimizationCpu). # DNN - Regression CPU; ROOT_EXECUTABLE(testRegressionCpu TestRegressionMethodDL.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Regression-Cp,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt:5974,Optimiz,Optimization-Cpu,5974,tmva/tmva/test/DNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt,1,['Optimiz'],['Optimization-Cpu']
Performance,"L algorithms, which have better performances in term of CPU time and a template interface has been also added.; Some of the basic special mathematical functions of TMath, like the error function or the gamma and beta functions use now the Cephes implementation from Stephen L. Moshier, which is used as well by the ROOT::Math functions. This implementation has been found to be more accurate and in some cases more efficient in term of CPU time. More detailed information on the new mathematical functions can be found in this presentation from M. Slawinska at a ROOT team meeting. define the functions as template functions instead of having the same re-definition for all the various basic types. This is done for TMath::Mean,TMath::GeomMean, TMath::Median, TMath::KOrdStat; Use STL to implement the following algorithms:; ; TMath::Sort is re-implemented using std::sort.; TMath::BinarySearch is re-implemented using the STL algorithm std::lower_bound. The STL algorithms have been found for these cases to be perform better in term of CPU time. For some other algorithms like TMath::LocMin, TMath::LocMax or TMath::Permute the original implementation is faster than STL and has been maintained.; ; Add a generic iterator interface, similar to the STL algorithm interface, to the following TMath functions:; MinElement, MaxElement, LocMin, LocMax, Mean, GeomMean, RMS,; BinarySearch. The iterator interface for sort is called; TMath::SortItr to avoid a compilation ambiguity. For example,; for finding the mean of a std::vector<double> v, one simply needs to call TMath::Mean(v.begin(), v.end() ) .; ; Various changes have been applied to the mathematical functions to remove duplications in the implementation with the special and statistical functions defined in ROOT::Math. The functions which have been changed and thus they can return a sightly different result than before are:; ; TMath::Erf and TMath::Erfc call ROOT::Math::erf and ROOT::math::erfc which are implemented using the Cephes algo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/doc/v520/index.html:1938,perform,perform,1938,math/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/doc/v520/index.html,2,['perform'],['perform']
Performance,"L, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkmcnt(0). - If OpenCL and; address space is; not generic, omit.; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the; value read by the; fence-paired-atomic. fence acquire - agent *none* 1. s_waitcnt lgkmcnt(0) &; - system vmcnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:217401,load,load,217401,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"L2 cache. * The gfx942 can be configured as a number of smaller agents with each having; a single L2 shared by all CUs on the same agent, or as fewer (possibly one); larger agents with groups of CUs on each agent each sharing separate L2; caches.; * The L2 cache has independent channels to service disjoint ranges of virtual; addresses.; * Each CU has a separate request queue per channel for its associated L2.; Therefore, the vector and scalar memory operations performed by wavefronts; executing with different L1 caches and the same L2 cache can be reordered; relative to each other.; * A ``s_waitcnt vmcnt(0)`` is required to ensure synchronization between; vector memory operations of different CUs. It ensures a previous vector; memory operation has completed before executing a subsequent vector memory; or LDS operation and so can be used to meet the requirements of acquire and; release.; * An L2 cache can be kept coherent with other L2 caches by using the MTYPE RW; (read-write) for memory local to the L2, and MTYPE NC (non-coherent) with; the PTE C-bit set for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by the PTE C-bit.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequent access from the GPU will automatically invalidate or writeback; the CPU cache due to the L2 probe filter.; * To ensure coherence of local memory writes of CUs with different L1 caches; in the same agent a ``buffer_wbl2`` is required. It does nothing if the; agent is configured to have a single L2, or will writeback dirty L2 cache; lines if configured to have multiple L2 caches.; * To ensure coherence of local memory writes of CUs in different agents a; ``buffer_wbl2 sc1`` is required. It will writeback dirty L2 cache lines.; * To ensure coherence of local memory reads of CUs with different L1 caches; in the same ag",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:288195,cache,cache,288195,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['cache'],"['cache', 'caches']"
Performance,"L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:278865,load,load,278865,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,"['cache', 'load']","['cache', 'load']"
Performance,"LE_A; # use.c:4:10: warning: implicit declaration of function 'a' is invalid in C99 [-Wimplicit-function-declaration]; # return a(x);; # ^; # 1 warning generated. So we need to maintain multiple versions of prebuilt modules. We can do so using a manual module mapping, or pointing to a different prebuilt module cache path. For example:. .. code-block:: sh. rm -rf prebuilt ; mkdir prebuilt ; rm -rf prebuilt_a ; mkdir prebuilt_a; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt -fdisable-module-hash; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt_a -fdisable-module-hash -DENABLE_A; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt; clang -cc1 -emit-obj use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt_a -DENABLE_A. Instead of managing the different module versions manually, we can build implicit modules in a given cache path (using ``-fmodules-cache-path``), and reuse them as prebuilt implicit modules by passing ``-fprebuilt-module-path`` and ``-fprebuilt-implicit-modules``. .. code-block:: sh. rm -rf prebuilt; mkdir prebuilt; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fmodules-cache-path=prebuilt -DENABLE_A; find prebuilt -name ""*.pcm""; # prebuilt/1AYBIGPM8R2GA/A-3L1K4LUA6O31.pcm; # prebuilt/1AYBIGPM8R2GA/B-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/A-3L1K4LUA6O31.pcm; # prebuilt/VH0YZMF1OIRK/B-3L1K4LUA6O31.pcm; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules; clang -cc1 -emit-obj -o use.o use.c -fmodules -fimplicit-module-maps -fprebuilt-module-path=prebuilt -fprebuilt-implicit-modules -DENABLE_A. Finally we want to allow implicit modules for configurations that were not prebuilt. When using the clang driver a module cac",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst:22246,cache,cache,22246,interpreter/llvm-project/clang/docs/Modules.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Modules.rst,2,['cache'],"['cache', 'cache-path']"
Performance,"LINK_CXX_STDLIB ""Statically link the standard library."" OFF); option(LLVM_ENABLE_LLD ""Use lld as C and C++ linker."" OFF); option(LLVM_ENABLE_PEDANTIC ""Compile with pedantic enabled."" ON); option(LLVM_ENABLE_WERROR ""Fail and stop if a warning is triggered."" OFF). option(LLVM_ENABLE_DUMP ""Enable dump functions even when assertions are disabled"" OFF); option(LLVM_UNREACHABLE_OPTIMIZE ""Optimize llvm_unreachable() as undefined behavior (default), guaranteed trap when OFF"" ON). if( NOT uppercase_CMAKE_BUILD_TYPE STREQUAL ""DEBUG"" ); option(LLVM_ENABLE_ASSERTIONS ""Enable assertions"" OFF); else(); option(LLVM_ENABLE_ASSERTIONS ""Enable assertions"" ON); endif(). option(LLVM_ENABLE_EXPENSIVE_CHECKS ""Enable expensive checks"" OFF). # While adding scalable vector support to LLVM, we temporarily want to; # allow an implicit conversion of TypeSize to uint64_t, and to allow; # code to get the fixed number of elements from a possibly scalable vector.; # This CMake flag enables a more strict mode where it asserts that the type; # is not a scalable vector type.; #; # Enabling this flag makes it easier to find cases where the compiler makes; # assumptions on the size being 'fixed size', when building tests for; # SVE/SVE2 or other scalable vector architectures.; option(LLVM_ENABLE_STRICT_FIXED_SIZE_VECTORS; ""Enable assertions that type is not scalable in implicit conversion from TypeSize to uint64_t and calls to getNumElements"" OFF). set(LLVM_ABI_BREAKING_CHECKS ""WITH_ASSERTS"" CACHE STRING; ""Enable abi-breaking checks. Can be WITH_ASSERTS, FORCE_ON or FORCE_OFF.""). option(LLVM_FORCE_USE_OLD_TOOLCHAIN; ""Set to ON to force using an old, unsupported host toolchain."" OFF). set(LLVM_LOCAL_RPATH """" CACHE FILEPATH; ""If set, an absolute path added as rpath on binaries that do not already contain an executable-relative rpath.""). option(LLVM_TEMPORARILY_ALLOW_OLD_TOOLCHAIN; ""Set to ON to only warn when using a toolchain which is about to be deprecated, instead of emitting an error."" OFF). option(LL",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:25783,scalab,scalable,25783,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['scalab'],['scalable']
Performance,"LLVM NVPTX target.; The library can be found under ``nvvm/libdevice/`` in the CUDA Toolkit and; there is a separate version for each compute architecture. For a list of all math functions implemented in libdevice, see; `libdevice Users Guide <http://docs.nvidia.com/cuda/libdevice-users-guide/index.html>`_. To accommodate various math-related compiler flags that can affect code; generation of libdevice code, the library code depends on a special LLVM IR; pass (``NVVMReflect``) to handle conditional compilation within LLVM IR. This; pass looks for calls to the ``@__nvvm_reflect`` function and replaces them; with constants based on the defined reflection parameters. Such conditional; code often follows a pattern:. .. code-block:: c++. float my_function(float a) {; if (__nvvm_reflect(""FASTMATH"")); return my_function_fast(a);; else; return my_function_precise(a);; }. The default value for all unspecified reflection parameters is zero. The ``NVVMReflect`` pass should be executed early in the optimization; pipeline, immediately after the link stage. The ``internalize`` pass is also; recommended to remove unused math functions from the resulting PTX. For an; input IR module ``module.bc``, the following compilation flow is recommended:. 1. Save list of external functions in ``module.bc``; 2. Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``; 3. Internalize all functions not in list from (1); 4. Eliminate all unused internal functions; 5. Run ``NVVMReflect`` pass; 6. Run standard optimization pipeline. .. note::. ``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the; libdevice functions. It is possible to link two IR modules that have been; linked against libdevice using different reflection variables. Since the ``NVVMReflect`` pass replaces conditionals with constants, it will; often leave behind dead code of the form:. .. code-block:: llvm. entry:; ..; br i1 true, label %foo, label %bar; foo:; ..; bar:; ; Dead code; .. Therefore, it is recommended tha",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst:8794,optimiz,optimization,8794,interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NVPTXUsage.rst,1,['optimiz'],['optimization']
Performance,"LLVM Project Blog; - Cling enables eval-style programming for Data Science applications. Examples of ROOT and Xeus-Cling for data science are shown.; * - `Interactive C++ with Cling <https://blog.llvm.org/posts/2020-11-30-interactive-cpp-with-cling/>`_; - *Vassil Vassilev* 2020 The LLVM Project Blog; - This blog page briefly discusses the concept of interactive C++ by presenting Cling’s main features, such as wrapper functions, entity redefinition, error recovery. ; * - `Using the Cling C++ Interpreter on the Bela Platform <https://gist.github.com/jarmitage/6e411ae8746c04d6ecbee1cbc1ebdcd4>`_; - Jack Armitage 2019; - Cling has been installed on a BeagleBoard to bring live coding to the Bela interactive audio platform.; * - `Implementation of GlobalModuleIndex in ROOT and Cling <https://indico.cern.ch/event/840376/contributions/3525646/attachments/1895398/3127159/GSoC_Presentation__GMI.pdf>`_; - *Arpitha Raghunandan* 2012 Google Summer of Code GSoC; - GlobalModuleIndex can be used for improving ROOT’s and Cling’s performance ; * - `Example project using cling as library <https://github.com/root-project/cling/tree/master/tools/demo>`_; - *Axel Naumann* 2016 GitHub; - This video showcases how to use Cling as a library, and shows how to set up a simple CMake configuration that uses Cling.; * - `Cling C++ interpreter testdrive <https://www.youtube.com/watch?v=1IGTHusaJ18>`_; - *Mika* 2015 Youtube; - In this tutorial, a developer tries Cling for the first time by uploading a few simple C++ user-cases onto Cling, involving also the loading of external files; * - `Building an Order Book in C++ <https://www.youtube.com/watch?v=fxN4xEZvrxI>`_; - *Dimitri Nesteruk* 2015 Youtube; - This demo shows how to build a simple order book using C++, CLion, Google Test and, of course, Cling. ; * - `Cling C++ interpreter testdrive <https://www.youtube.com/watch?v=1IGTHusaJ18>`_; - Dimitri Nesteruk 2015 Youtube; - This tutorial describes Cling’s general features. You will learn how to star",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/references.rst:4347,perform,performance,4347,interpreter/cling/docs/chapters/references.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/references.rst,1,['perform'],['performance']
Performance,"LLVMPass.html>`_ document and the; `List of LLVM Passes <../../Passes.html>`_. For Kaleidoscope, we are currently generating functions on the fly, one; at a time, as the user types them in. We aren't shooting for the; ultimate optimization experience in this setting, but we also want to; catch the easy and quick stuff where possible. As such, we will choose; to run a few per-function optimizations as the user types the function; in. If we wanted to make a ""static Kaleidoscope compiler"", we would use; exactly the code we have now, except that we would defer running the; optimizer until the entire file has been parsed. In addition to the distinction between function and module passes, passes can be; divided into transform and analysis passes. Transform passes mutate the IR, and; analysis passes compute information that other passes can use. In order to add; a transform pass, all analysis passes it depends upon must be registered in; advance. In order to get per-function optimizations going, we need to set up a; `FunctionPassManager <../../WritingAnLLVMPass.html#what-passmanager-doesr>`_ to hold; and organize the LLVM optimizations that we want to run. Once we have; that, we can add a set of optimizations to run. We'll need a new; FunctionPassManager for each module that we want to optimize, so we'll; add to a function created in the previous chapter (``InitializeModule()``):. .. code-block:: c++. void InitializeModuleAndManagers(void) {; // Open a new context and module.; TheContext = std::make_unique<LLVMContext>();; TheModule = std::make_unique<Module>(""KaleidoscopeJIT"", *TheContext);; TheModule->setDataLayout(TheJIT->getDataLayout());. // Create a new builder for the module.; Builder = std::make_unique<IRBuilder<>>(*TheContext);. // Create new pass and analysis managers.; TheFPM = std::make_unique<FunctionPassManager>();; TheLAM = std::make_unique<LoopAnalysisManager>();; TheFAM = std::make_unique<FunctionAnalysisManager>();; TheCGAM = std::make_unique<CGSCCAnalysis",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:5178,optimiz,optimizations,5178,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,1,['optimiz'],['optimizations']
Performance,"LLimitMinMax, //specify limits; -1.,1.); //limit values; ```. **`TGNumberEntryField`** is a number entry input widget. ``` {.cpp}; Nent = new TGNumberEntryField(hgrunf2, kNENT_ID, 0.6,; TGNumberFormat::kNESRealThree,; TGNumberFormat::kNEAAnyNumber);; ```. **`TGNumberEntryField`** is a plain vanilla entry field, whereas; **`TGNumberEntry`** adds two small buttons to increase and decrease the; numerical value in the field. The number entry widgets also support; using the up and down cursor keys to change the numerical values. The; step size can be selected with control and shift keys:. - --small step (1 unit/factor of 3). - Shift medium step (10 units/factor of 10). - Controllarge step (100 units/factor of 30). - Shift+Controlhuge step (1000 units/factor of 100). The steps are either linear or logarithmic. The default behavior is set; when the entry field is created, but it can be changed by pressing the; alt key at the same time. ### Menus. Menus provide a list of commands or options helping the user to select; and to perform a task. The menu system classes are **`TGMenuBar`**,; **`TGMenuTitle`**, **`TGPopupMenu`**, and **`TGMenuEntry`**. The **`TGMenuBar`** class implements a menu bar widget. It is used to; specify and provide access to common and frequently used application; actions described in menu titles, implemented by **`TGMenuTitle`**; class. The menu bar is the highest-level of the menu system and it is a; starting point for all interactions. Also, it is always visible and; allows using the keyboard equivalents. The geometry of the menu bar is; automatically set to the parent widget, i.e. the menu bar automatically; resizes itself so that it has the same width as its parent (typically; **`TGMainFrame`**). The menu bar is as a container for its menus - objects of the type; **`TGPopupMenu.`** Popup menus can appear in a menu bar. They can be a; sub-menu of another popup menu (cascading menus) or can be standalone; (as a context menu). They are made of one or mo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md:68567,perform,perform,68567,documentation/users-guide/WritingGUI.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md,1,['perform'],['perform']
Performance,"LVM emits stack map data into the object code within a designated; :ref:`stackmap-section`. This stack map data contains a record for; each stack map. The record stores the stack map's instruction address; and contains an entry for each mapped value. Each entry encodes a; value's location as a register, stack offset, or constant. A patch point is an instruction address at which space is reserved for; patching a new instruction sequence at run time. Patch points look; much like calls to LLVM. They take arguments that follow a calling; convention and may return a value. They also imply stack map; generation, which allows the runtime to locate the patchpoint and; find the location of ``live values`` at that point. Motivation; ==========. This functionality is currently experimental but is potentially useful; in a variety of settings, the most obvious being a runtime (JIT); compiler. Example applications of the patchpoint intrinsics are; implementing an inline call cache for polymorphic method dispatch or; optimizing the retrieval of properties in dynamically typed languages; such as JavaScript. The intrinsics documented here are currently used by the JavaScript; compiler within the open source WebKit project, see the `FTL JIT; <https://trac.webkit.org/wiki/FTLJIT>`_, but they are designed to be; used whenever stack maps or code patching are needed. Because the; intrinsics have experimental status, compatibility across LLVM; releases is not guaranteed. The stack map functionality described in this document is separate; from the functionality described in; :ref:`stack-map`. `GCFunctionMetadata` provides the location of; pointers into a collected heap captured by the `GCRoot` intrinsic,; which can also be considered a ""stack map"". Unlike the stack maps; defined above, the `GCFunctionMetadata` stack map interface does not; provide a way to associate live register values of arbitrary type with; an instruction address, nor does it specify a format for the resulting; stack ma",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst:1758,cache,cache,1758,interpreter/llvm-project/llvm/docs/StackMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst,2,"['cache', 'optimiz']","['cache', 'optimizing']"
Performance,"LVM_active_lane`` *allows the compiler to; provide the means to determine the source language active lanes at any; program location. Typically, this attribute will use a loclist to express; different locations of the active lane mask at different program locations.*. If not present and ``DW_AT_LLVM_lanes`` is greater than 1, then the target; architecture execution mask is used. 7. A ``DW_TAG_subprogram``, ``DW_TAG_inlined_subroutine``, or; ``DW_TAG_entry_point`` debugger information entry may have a; ``DW_AT_LLVM_iterations`` attribute whose value is an integer constant or a; DWARF expression E. Its value is the number of source language loop; iterations executing concurrently by the target architecture for a single; source language thread of execution. *A compiler may generate code that executes more than one iteration of a; source language loop concurrently using optimization techniques such as; software pipelining or SIMD vectorization. The number of concurrent; iterations may vary for different loop nests in the same subprogram.; Typically, this attribute will use a loclist to express different values at; different program locations.*. If the attribute is an integer constant, then the value is the constant. The; DWARF is ill-formed if the constant is less than or equal to 0. Otherwise, E is evaluated with a context that has a result kind of a; location description, an unspecified object, the compilation unit that; contains E, an empty initial stack, and other context elements corresponding; to the source language thread of execution upon which the user is focused,; if any. The DWARF is ill-formed if the result is not a location description; comprised of one implicit location description, that when read as the; generic type, results in a value V that is less than or equal to 0. The; result of the attribute is the value V. If not present, the default value of 1 is used. A.3.4 Call Site Entries and Parameters; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. A.3.4.2 Call Sit",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:166097,concurren,concurrent,166097,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['concurren'],['concurrent']
Performance,"LViewer - render opaque; objects from all scenes first, then all transparent ones. Modularization of input event-handling in TGLViewer: all; event-handling is done by the TGLEventHandler class. One can; sub-class it now and modify behaviour of a given viewer. For; example, see TEveLegoEventHandler. Support highlighting of physical shapes for providing feedback and; showing selection. Minor changes, fixes and improvements. Improve saving of images from the GL-viewer so that the dialog; boxes and other windows do not result in black areas on the saved; image. The window must still be fully contained within the desktop. Improved camera controls. Three new orthographic cameras have; been added to TGLViewer, looking at the scene from another; side than the one present so far. Improved FTGL font management across rendering contexts and text; rendering support. New class TGLAxisPainter that can render 2D and 3D axes; via GL. For example see tutorials/eve/cms_calo.C. Possible performance issues with ATI drivers (fglrx). In late 2007 ATI switched to a new driver architecture. With these; drivers a significant degradation of GL performance in selection mode,; up to a factor of 50, was observed. Both linux and Windows drivers; were affected. The issue has been resolved in the latest driver; versions. Eve; Major changes. Support for multiple, parallel OpenGL views that can show different; projections of the same event. Provide object selection and feedback highlight across all GL-views and; list-trees. New classes for visualization of calorimeter data,; TEveCaloXYZ, see tutorials/eve/cms_calo.C. Available; representations: 3D-cylindrical view, projected views r-phi and rho-z,; and lego-view (with dedicated event handler allowing detailed; inspection of the data). Support for compound objects in view of selection, highlight and; color managament (see class TEveCompound). Optimize updates of GL-scenes by introducing change-stamping bits; into TEveElement. See methods AddStamp() a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf3d/doc/v520/index.html:1047,perform,performance,1047,graf3d/doc/v520/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf3d/doc/v520/index.html,2,['perform'],['performance']
Performance,"Line X1=0.000000 Y1=1.000000 X2=0.000000 Y2=0.000000; TLine X1=1.000000 Y1=2.000000 X2=0.000000 Y2=0.000000; TLine X1=2.000000 Y1=3.000000 X2=0.000000 Y2=0.000000; TLine X1=3.000000 Y1=4.000000 X2=0.000000 Y2=0.000000; TLine X1=4.000000 Y1=5.000000 X2=0.000000 Y2=0.000000; root [] .q; ```. Here we note:. - A multi-line command starts with a { and ends with a }.; - Inside continuation, every line has to be correctly terminated with a ; (like in ""real''; C++).; - All objects are created in *global* scope.; - There is no way to back up; you are better off writing a script.; - Use `.q` to exit root. ## Feeding Sources Files To ROOT: C++ Scripts. ROOT script files (often called ""Macros"") contain pure C++ code. They can contain a simple; sequence of statements like in the multi command line example given; above, but also arbitrarily complex class and function definitions. The most frequent interaction with the ROOT prompt uses `.x` to ""run"" a file:. ```; root [] .x myScript.C; ```. This loads myScript.C into the interpreter and calls the function `myScript()`.; You can pass arguments using `.x myScript.C(12, ""A String"")`. Alternatively you can load the script and then run a function explicitly:. ```; root [] .L myScript.C; root [] myScript(); ```. The above is equivalent to `.x myScript.C`. In a named script, the objects created on the stack are deleted when; the function exits. In a common scenario you; create a histogram in a named script on the stack. You draw the; histogram, but when the function exits the canvas is empty and the; histogram has disappeared. To avoid the histogram from disappearing you can; create it on the heap (by using new). This will leave the histogram; object intact, but the pointer in the named script scope will be; ""gone"". Since histograms (and trees) are added to the list of objects; in the current directory, you can always retrieve them to delete them; if needed. ``` {.cpp}; root[] TH1F *h = (TH1F*)gDirectory->Get(""myHist""); // or; root[] TH1F",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md:7881,load,loads,7881,documentation/users-guide/Cling.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md,1,['load'],['loads']
Performance,"ListOfFree()->After(idcur);; }; ```. Method 1 uses internally method 2. Method 2 works for all collection classes. **`TIter`** overloads; `operator()`. Methods 3 and 4 are specific for **`TList`**. Methods 2, 3 and 4 can also easily iterate backwards using either a; backward **`TIter`** (using argument `kIterBackward`) or by using; `LastLink()` and `lnk>Prev`() or by using the `Before()` method. ## The TObjArray Collection. A **`TObjArray`** is a collection which supports traditional array; semantics via the overloading of `operator[]`. Objects can be directly; accessed via an index. The array expands automatically when objects are; added. At creation time one specifies the default array size (default =; 16) and lower bound (default = 0). Resizing involves a re-allocation and; a copy of the old array to the new. This can be costly if done too; often. If possible, set initial size close to expected final size. Index; validity is always checked (if you are 100% sure and maximum performance; is needed you can use `UnCheckedAt()` instead of `At()` or; `operator[]`). If the stored objects are sort able the array can be; sorted using `Sort()`. Once sorted, efficient searching is possible via; the `BinarySearch()` method. The figure shows the internal data; structure of a **`TObjArray`**:. ![The internal data structure of a TObjArray](pictures/020001A7.jpg). Iterating can be done using a **`TIter`** iterator or via a simple for; loop:. ``` {.cpp}; for (int i = 0; i <= fArr.GetLast(); i++); if ((track = (TTrack*)fArr[i])) // or fArr.At(i); track->Draw();; ```. Main features of **`TObjArray`** are simple, well-known array semantics.; **Overhead per element**: none, except possible over sizing of `fCont`. ## TClonesArray An Array of Identical Objects. A **`TClonesArray`** is an array of identical (clone) objects. The; memory for the objects stored in the array is allocated only once in the; lifetime of the clones array. All objects must be of the same class. For; the rest thi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/CollectionClasses.md:15341,perform,performance,15341,documentation/users-guide/CollectionClasses.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/CollectionClasses.md,1,['perform'],['performance']
Performance,"Low-level code; ==============. .. toctree::; :hidden:. C code and older C++ code sometimes makes use of low-level features such as; pointers to builtin types, some of which do not have any Python equivalent; (e.g. ``unsigned short*``).; Furthermore, such codes tend to be ambiguous: the information from header; file is not sufficient to determine the full purpose.; For example, an ``int*`` type may refer to the address of a single ``int``; (an out-parameter, say) or it may refer to an array of ``int``, the ownership; of which is not clear either.; cppyy provides a few low-level helpers and integration with the Python; `ctypes module`_ to cover these cases. Use of these low-level helpers will obviously lead to very ""C-like"" code and; it is recommended to :doc:`pythonize <pythonizations>` the code, perhaps; using the Cling JIT and embedded C++. Note: the low-level module is not loaded by default (since its use is, or; should be, uncommon).; It needs to be imported explicitly:. .. code-block:: python. >>> import cppyy.ll; >>>. `LowLevelView`; --------------. Python has an elaborate array interface (buffer) specification, but no; standard library array type that completely implements it; instead, the; canonical Python array type is the NumPy one.; cppyy introduces the basic ``LowLevelView`` array class to avoid having a; direct dependency on NumPy and to guarantee zero copy.; The ``LowLevelView`` type gives access to array details such as the size,; type, etc. and allows reading/writing of array elements, both for interactive; use and through the buffer interface to allow NumPy to interface with them.; For more complex operations, it's recommended to copy from the; ``LowLevelView`` inta a NumPy array, or to create a NumPy view (see below,; under :ref:`NumPy Casts <npcasts>`). `C/C++ casts`; -------------. C++ instances are auto-casted to the most derived available type, so do not; require explicit casts even when a function returns a pointer to a base; class or interface",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/lowlevel.rst:889,load,loaded,889,bindings/pyroot/cppyy/cppyy/doc/source/lowlevel.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/lowlevel.rst,1,['load'],['loaded']
Performance,"M bitcode; files into a single archive library that can be linked into a program. However,; the archive can contain any kind of file. By default, :program:`llvm-ar`; generates a symbol table that makes linking faster because only the symbol; table needs to be consulted, not each individual file member of the archive. The :program:`llvm-ar` command can be used to *read* archive files in SVR4, GNU,; BSD , Big Archive, and Darwin format, and *write* in the GNU, BSD, Big Archive, and; Darwin style archive files. If an SVR4 format archive is used with the :option:`r`; (replace), :option:`d` (delete), :option:`m` (move) or :option:`q`; (quick update) operations, the archive will be reconstructed in the format; defined by :option:`--format`. Here's where :program:`llvm-ar` departs from previous :program:`ar`; implementations:. *The following option is not supported*. [f] - truncate inserted filenames. *The following options are ignored for compatibility*. --plugin=<string> - load a plugin which adds support for other file formats. [l] - ignored in :program:`ar`. *Symbol Table*. Since :program:`llvm-ar` supports bitcode files, the symbol table it creates; includes both native and bitcode symbols. *Deterministic Archives*. By default, :program:`llvm-ar` always uses zero for timestamps and UIDs/GIDs; to write archives in a deterministic mode. This is equivalent to the; :option:`D` modifier being enabled by default. If you wish to maintain; compatibility with other :program:`ar` implementations, you can pass the; :option:`U` modifier to write actual timestamps and UIDs/GIDs. *Windows Paths*. When on Windows :program:`llvm-ar` treats the names of archived *files* in the same; case sensitive manner as the operating system. When on a non-Windows machine; :program:`llvm-ar` does not consider character case. OPTIONS; -------. :program:`llvm-ar` operations are compatible with other :program:`ar`; implementations. However, there are a few modifiers (:option:`L`) that are not; found in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-ar.rst:1327,load,load,1327,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-ar.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-ar.rst,1,['load'],['load']
Performance,"M tools, relative to the *CMAKE_INSTALL_PREFIX*.; Defaults to *CMAKE_INSTALL_BINDIR*. **LLVM_UTILS_INSTALL_DIR**:STRING; The path to install auxiliary LLVM utilities, relative to the *CMAKE_INSTALL_PREFIX*.; Only matters if *LLVM_INSTALL_UTILS* is enabled.; Defaults to *LLVM_TOOLS_INSTALL_DIR*. **LLVM_EXAMPLES_INSTALL_DIR**:STRING; The path for examples of using LLVM, relative to the *CMAKE_INSTALL_PREFIX*.; Only matters if *LLVM_BUILD_EXAMPLES* is enabled.; Defaults to ""examples"". CMake Caches; ============. Recently LLVM and Clang have been adding some more complicated build system; features. Utilizing these new features often involves a complicated chain of; CMake variables passed on the command line. Clang provides a collection of CMake; cache scripts to make these features more approachable. CMake cache files are utilized using CMake's -C flag:. .. code-block:: console. $ cmake -C <path to cache file> <path to sources>. CMake cache scripts are processed in an isolated scope, only cached variables; remain set when the main configuration runs. CMake cached variables do not reset; variables that are already set unless the FORCE option is specified. A few notes about CMake Caches:. - Order of command line arguments is important. - -D arguments specified before -C are set before the cache is processed and; can be read inside the cache file; - -D arguments specified after -C are set after the cache is processed and; are unset inside the cache file. - All -D arguments will override cache file settings; - CMAKE_TOOLCHAIN_FILE is evaluated after both the cache file and the command; line arguments; - It is recommended that all -D options should be specified *before* -C. For more information about some of the advanced build configurations supported; via Cache files see :doc:`AdvancedBuilds`. Executing the Tests; ===================. Testing is performed when the *check-all* target is built. For instance, if you are; using Makefiles, execute this command in the root of you",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:38987,cache,cache,38987,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,2,['cache'],"['cache', 'cached']"
Performance,"MPONENTS``. This variable holds a space separated list of components that the LLVM; ``Makefiles`` pass to the ``llvm-config`` tool to generate a link line for; the program. For example, to link with all LLVM libraries use; ``LINK_COMPONENTS = all``. ``LIBS``. To link dynamic libraries, add ``-l<library base name>`` to the ``LIBS``; variable. The LLVM build system will look in the same places for dynamic; libraries as it does for static libraries. For example, to link ``libsample.so``, you would have the following line in; your ``Makefile``:. .. code-block:: makefile. LIBS += -lsample. Note that ``LIBS`` must occur in the Makefile after the inclusion of; ``Makefile.common``. Miscellaneous Variables; -----------------------. ``CFLAGS`` & ``CPPFLAGS``. This variable can be used to add options to the C and C++ compiler,; respectively. It is typically used to add options that tell the compiler; the location of additional directories to search for header files. It is highly suggested that you append to ``CFLAGS`` and ``CPPFLAGS`` as; opposed to overwriting them. The LLVM ``Makefiles`` may already have; useful options in them that you may not want to overwrite. Placement of Object Code; ========================. The final location of built libraries and executables will depend upon whether; you do a ``Debug``, ``Release``, or ``Profile`` build. Libraries. All libraries (static and dynamic) will be stored in; ``PROJ_OBJ_ROOT/<type>/lib``, where *type* is ``Debug``, ``Release``, or; ``Profile`` for a debug, optimized, or profiled build, respectively. Executables. All executables will be stored in ``PROJ_OBJ_ROOT/<type>/bin``, where *type*; is ``Debug``, ``Release``, or ``Profile`` for a debug, optimized, or; profiled build, respectively. Further Help; ============. If you have any questions or need any help creating an LLVM project, the LLVM; team would be more than happy to help. You can always post your questions to; the `Discourse forums; <https://discourse.llvm.org>`_.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Projects.rst:8802,optimiz,optimized,8802,interpreter/llvm-project/llvm/docs/Projects.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Projects.rst,2,['optimiz'],['optimized']
Performance,"M_PURGE but will force release all possible |; | | memory regardless of how long it takes. |; | | The value is ignored. |; +---------------------------+-------------------------------------------------------+; | M_MEMTAG_TUNING | Tunes the allocator's choice of memory tags to make |; | | it more likely that a certain class of memory errors |; | | will be detected. The value argument should be one of |; | | the enumerators of ``scudo_memtag_tuning``. |; +---------------------------+-------------------------------------------------------+; | M_THREAD_DISABLE_MEM_INIT | Tunes the per-thread memory initialization, 0 being |; | | the normal behavior, 1 disabling the automatic heap |; | | initialization. |; +---------------------------+-------------------------------------------------------+; | M_CACHE_COUNT_MAX | Set the maximum number of entries than can be cached |; | | in the Secondary cache. |; +---------------------------+-------------------------------------------------------+; | M_CACHE_SIZE_MAX | Sets the maximum size of entries that can be cached |; | | in the Secondary cache. |; +---------------------------+-------------------------------------------------------+; | M_TSDS_COUNT_MAX | Increases the maximum number of TSDs that can be used |; | | up to the limit specified at compile time. |; +---------------------------+-------------------------------------------------------+. Error Types; ===========. The allocator will output an error message, and potentially terminate the; process, when an unexpected behavior is detected. The output usually starts with; ``""Scudo ERROR:""`` followed by a short summary of the problem that occurred as; well as the pointer(s) involved. Once again, Scudo is meant to be a mitigation,; and might not be the most useful of tools to help you root-cause the issue,; please consider `ASan <https://github.com/google/sanitizers/wiki/AddressSanitizer>`_; for this purpose. Here is a list of the current error messages and their potential cause:.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst:14182,cache,cached,14182,interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ScudoHardenedAllocator.rst,2,['cache'],"['cache', 'cached']"
Performance,"MachineFunction being compiled. This step uses traditional prepass; scheduling techniques. After all of these steps are complete, the SelectionDAG is destroyed and the; rest of the code generation passes are run. One of the most common ways to debug these steps is using ``-debug-only=isel``,; which prints out the DAG, along with other information like debug info,; after each of these steps. Alternatively, ``-debug-only=isel-dump`` shows only; the DAG dumps, but the results can be filtered by function names using; ``-filter-print-funcs=<function names>``. One great way to visualize what is going on here is to take advantage of a few; LLC command line options. The following options pop up a window displaying the; SelectionDAG at specific times (if you only get errors printed to the console; while using this, you probably `need to configure your; system <ProgrammersManual.html#viewing-graphs-while-debugging-code>`_ to add support for it). * ``-view-dag-combine1-dags`` displays the DAG after being built, before the; first optimization pass. * ``-view-legalize-dags`` displays the DAG before Legalization. * ``-view-dag-combine2-dags`` displays the DAG before the second optimization; pass. * ``-view-isel-dags`` displays the DAG before the Select phase. * ``-view-sched-dags`` displays the DAG before Scheduling. The ``-view-sunit-dags`` displays the Scheduler's dependency graph. This graph; is based on the final SelectionDAG, with nodes that must be scheduled together; bundled into a single scheduling-unit node, and with immediate operands and; other nodes that aren't relevant for scheduling omitted. The option ``-filter-view-dags`` allows to select the name of the basic block; that you are interested to visualize and filters all the previous; ``view-*-dags`` options. .. _Build initial DAG:. Initial SelectionDAG Construction; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The initial SelectionDAG is na\ :raw-html:`&iuml;`\ vely peephole expanded from; the LLVM input by the ``SelectionDAG",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:39363,optimiz,optimization,39363,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimization']
Performance,"Manual Data Model Evolution Capabilities - the user documentation. 1. Overview. The automatic data model schema evolution implemented in ROOT makes it possible; to read back the serialized data object in the situation when the definition of; the classes those objects represent changed slightly (some of the data members were; removed or some new ones added). It is also possible to manually specify the rules; for more sophisticated data transformations done while reading to load the serialized; objects into data structures that changed quite significantly. ROOT provides two interface enabling users to specify the conversion rules. The; first way is to define a rule in the dictionary file and the second way is to insert; it to the TClass object using the C++ API. There are two types of conversion rules. The first of them, the normal rules, are; the ones that should be used in the most of the cases. They provide a buffered input; data and an address of the in-memory target object and allow user to specify the; conversion function mapping the data being read to the output format. The second type; of the rules, the raw rules, also provide the pointer to the target object but the; input is a raw TBuffer object containing the input data member declared as an input; to the rule. This type of a rule is provided mainly to handle the file format changes; that couldn't have been handled otherwise and in general should not be used unless there; is no other option. 2. The dictionaries. The most convenient place to specify the conversion rules is a dictionary. One can; do that either in CINT's LinkDef file or in the selection xml file being fed to genreflex.; The syntax of the rules is the following:. * For CINT dictionaries:. #pragma read \; sourceClass=""ClassA"" \; source=""double m_a; double m_b; double m_c"" \; version=""[4-5,7,9,12-]"" \; checksum=""[12345,123456]"" \; targetClass=""ClassB"" \; target=""m_x"" \; embed=""true"" \; include=""iostream,cstdlib"" \; code=""{m_x = onfile.m_a * onfil",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt:477,load,load,477,io/doc/DataModelEvolution.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt,2,['load'],['load']
Performance,"MemoryOperand(...)`` method. Care must be taken when; folding instructions; a folded instruction can be quite different from the; original instruction. See ``LiveIntervals::addIntervalsForSpills`` in; ``lib/CodeGen/LiveIntervalAnalysis.cpp`` for an example of its use. Built in register allocators; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The LLVM infrastructure provides the application developer with three different; register allocators:. * *Fast* --- This register allocator is the default for debug builds. It; allocates registers on a basic block level, attempting to keep values in; registers and reusing registers as appropriate. * *Basic* --- This is an incremental approach to register allocation. Live; ranges are assigned to registers one at a time in an order that is driven by; heuristics. Since code can be rewritten on-the-fly during allocation, this; framework allows interesting allocators to be developed as extensions. It is; not itself a production register allocator but is a potentially useful; stand-alone mode for triaging bugs and as a performance baseline. * *Greedy* --- *The default allocator*. This is a highly tuned implementation of; the *Basic* allocator that incorporates global live range splitting. This; allocator works hard to minimize the cost of spill code. * *PBQP* --- A Partitioned Boolean Quadratic Programming (PBQP) based register; allocator. This allocator works by constructing a PBQP problem representing; the register allocation problem under consideration, solving this using a PBQP; solver, and mapping the solution back to a register assignment. The type of register allocator used in ``llc`` can be chosen with the command; line option ``-regalloc=...``:. .. code-block:: bash. $ llc -regalloc=linearscan file.bc -o ln.s; $ llc -regalloc=fast file.bc -o fa.s; $ llc -regalloc=pbqp file.bc -o pbqp.s. .. _Prolog/Epilog Code Insertion:. Prolog/Epilog Code Insertion; ----------------------------. .. note::. To Be Written. Compact Unwind; --------------. Thro",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:70036,perform,performance,70036,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['perform'],['performance']
Performance,"Merger on TH1 and TTree.; (Soft dependencies still exist to be able to disable the; merging of TTrees and to be able to disable the AutoAdd; behavior of TH1). The object TFileMergeInfo can be used inside the Merge; function to pass information between runs of the Merge; (see below). In particular it contains:. TDirectory *fOutputDirectory; // Target directory where the merged object will be written.; Bool_t fIsFirst; // True if this is the first call to Merge for this series of object.; TString fOptions; // Additional text based option being passed down to customize the merge.; TObject *fUserData; // Place holder to pass extra information. This object will be deleted at the end of each series of objects. The default in TFileMerger is to call Merge for every object; in the series (i.e the collection has exactly one element) in; order to save memory (by not having all the object in memory; at the same time). However for histograms, the default is to first load all the; objects and then merge them in one go ; this is customizable; when creating the TFileMerger object. Asynchronous Prefetching; The prefetching mechanism uses two new classes (TFilePrefetch; and TFPBlock) to prefetch in advance a block of tree entries. There; is a thread which takes care of actually transferring the blocks and; making them available to the main requesting thread. Therefore, the time; spent by the main thread waiting for the data before processing considerably; decreases. Besides the prefetching mechanisms there is also a local; caching option which can be enabled by the user. Both capabilities are; disabled by default and must be explicitly enabled by the user. In order to enable the prefetching the user must set the rootrc environment; variable TFile.AsyncPrefetching as follows:; gEnv->SetValue(""TFile.AsyncPrefetching"", 1). Only when the; prefetching is enabled can the user set the local cache directory in; which the file transferred will be saved. For subsequent reads of the; same file ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/doc/v530/index.html:4723,load,load,4723,io/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/v530/index.html,2,['load'],['load']
Performance,"Minuit2*). The class supports also setting global default values for the options, by using the static functions `MinimizerOptions::SetDefault...` (for example `MinimizerOptions::SetDefaultPrintLevel(int )`). The; static functions can be also used to set the minimizer options when using `TH1::Fit` or `TGraph::Fit`.; The list of the current option values can be inspected by using `MinimizerOptions::Print`.; ```{.cpp}; ROOT::Math::MinimizerOptions() opt;; // print the default minimizer option values; opt.Print();; ```. In addition it is possible to provide extra options which might apply for a particular minimizer `MinimizerOptions::SetExtraOptions(const IOptions & )`.; See the documentation of the particular minimizer to use for the list of possible additional options available. ### Performing the Fit. Here we have now all the required input ingredients for the fit, the data and the function to fit.; Depending on these we have now several different way to perform the fit, using the corresponding methods of the; `ROOT::Fit::Fitter` class and depending on the type of input data. #### Available fit methods. * **Least-square fit**: `Fitter::LeastSquare(const BinData & )` or `Fitter::Fit(const Bindata &)`. It requires the user to pass a `BinData` object. It should be used when the data values follow a; Gaussian distribution. This fit method is implemented using the class `ROOT::Fit::Chi2FCN`.; * **Binned Likelihood fit** : `Fitter::LikelihoodFit(const Bindata & )`. The user needs to pass a `BinData` object. It should be used when the data values follow a Poisson or a multinomial; distribution. The Poisson case (extended fit) is the default and in this case the function normalization is also fit to the data. The Multi-nominal case can be selected by passing the optional; *extended* boolean flag as *false*. This method is implemented by the class `ROOT::Fit:::PoissonLikelihoodFCN`.; * **Un-Binned likelihood fit**: `Fitter::LikelihoodFit(const UnBindata &)`. The user needs to ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md:45469,perform,perform,45469,documentation/users-guide/FittingHistograms.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md,1,['perform'],['perform']
Performance,"Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic glc=1. - If not TgSplit execution; mode, omit glc=1. load atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_load; load atomic monotonic - agent - global 1. buffer/global/flat_load; - generic glc=1; load atomic monotonic - system - global 1. buffer/global/flat_load; - generic glc=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; store atomic monotonic - system - global 1. buffer/global/flat_store; - generic; store atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_store; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; atomicrmw monotonic - system - global 1. buffer/global/flat_atomic; - generic; atomicrmw monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_atomic; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - glob",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:244213,load,load,244213,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - global 1. buffer_wbl2 sc0=1 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. buffer/global_atomic; sc1=1; 4. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 5. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acq_rel - agent - generic 1. buffer_wbl2 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt v",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:321641,perform,performing,321641,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"N and ``op1``; is less than or equal to ``op2``.; #. ``one``: yields ``true`` if both operands are not a QNAN and ``op1``; is not equal to ``op2``.; #. ``ord``: yields ``true`` if both operands are not a QNAN.; #. ``ueq``: yields ``true`` if either operand is a QNAN or ``op1`` is; equal to ``op2``.; #. ``ugt``: yields ``true`` if either operand is a QNAN or ``op1`` is; greater than ``op2``.; #. ``uge``: yields ``true`` if either operand is a QNAN or ``op1`` is; greater than or equal to ``op2``.; #. ``ult``: yields ``true`` if either operand is a QNAN or ``op1`` is; less than ``op2``.; #. ``ule``: yields ``true`` if either operand is a QNAN or ``op1`` is; less than or equal to ``op2``.; #. ``une``: yields ``true`` if either operand is a QNAN or ``op1`` is; not equal to ``op2``.; #. ``uno``: yields ``true`` if either operand is a QNAN.; #. ``true``: always yields ``true``, regardless of operands. The ``fcmp`` instruction can also optionally take any number of; :ref:`fast-math flags <fastmath>`, which are optimization hints to enable; otherwise unsafe floating-point optimizations. Any set of fast-math flags are legal on an ``fcmp`` instruction, but the; only flags that have any effect on its semantics are those that allow; assumptions to be made about the values of input arguments; namely; ``nnan``, ``ninf``, and ``reassoc``. See :ref:`fastmath` for more information. Example:; """""""""""""""". .. code-block:: text. <result> = fcmp oeq float 4.0, 5.0 ; yields: result=false; <result> = fcmp one float 4.0, 5.0 ; yields: result=true; <result> = fcmp olt float 4.0, 5.0 ; yields: result=true; <result> = fcmp ueq double 1.0, 2.0 ; yields: result=false. .. _i_phi:. '``phi``' Instruction; ^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = phi [fast-math-flags] <ty> [ <val0>, <label0>], ... Overview:; """""""""""""""""". The '``phi``' instruction is used to implement the φ node in the SSA; graph representing the function. Arguments:; """""""""""""""""""". The type of the incoming values is specified ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:466245,optimiz,optimization,466245,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"N string, using `toJSON()` function. Like:. ```javascript; import { toJSON, openFile, makeSVG } from 'jsroot';; import { writeFileSync } from 'fs';. let file = await openFile(""https://root.cern/js/files/hsimple.root"");; let obj = await file.readObject(""hpx;1"");; let json = await toJSON(obj);; writrFileSync(""hpxpy.json"", json);; ```. Such JSON string could be parsed by any other JSROOT-based application. When WebGL rendering is used (lego plots or TGeo drawing), on the Linux one need to have `DISPLAY` correctly set; to make it working. To run JSROOT on headless machine, one have to use `xvfb-run` utility,; see also [here](https://github.com/stackgl/headless-gl#how-can-headless-gl-be-used-on-a-headless-linux-machine):. ```bash; [shell] xvfb-run -s ""-ac -screen 0 1280x1024x24"" node geomsvg.js; ```. ### Use with OpenUI5. [OpenUI5](http://openui5.org/) is a web toolkit for developers to ease and speed up the development of full-blown HTML5 web applications.; JSROOT provides `loadOpenui5` function to load supported OpenUI5:. ```javascript; <script type=""module"">; import { loadOpenui5 } from 'path_to_jsroot/modules/main.mjs';; let sap = await loadOpenui5();; sap.registerModulePath(""NavExample"", ""./"");; new sap.m.App ({; pages: [; new sap.m.Page({; title: ""Nav Container"",; enableScrolling : true,; content: [ new sap.ui.core.ComponentContainer({ name : ""NavExample"" })]; }); ]; }).placeAt(""content"");; </script>; ```. JSROOT uses <https://openui5.hana.ondemand.com/1.128.0/> when no other source is specified. There are small details when using OpenUI5 with THttpServer. First of all, location of JSROOT modules should be specified; as `/jsrootsys/modules/main.mjs`. And then trying to access files from local disk, one should specify `/currentdir/` folder:. ```javascript; jQuery.sap.registerModulePath(""NavExample"", ""/currentdir/"");; ```. JSROOT provides [example](https://root.cern/js/latest/demo/openui5/) showing usage of JSROOT drawing in the OpenUI5,; [source code](https://github",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md:46200,load,load,46200,documentation/JSROOT/JSROOT.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md,1,['load'],['load']
Performance,"N=256); * `TRandomMT64` - 64 bit Mersenenne Twister generator from the standard library (based on `std::mt19937_64`). This generates 64 bit random numbers, while `TRandom3` generates only 32 bit random numbers.; * `TRandomRanlux48` - 48 bit Ranlux generator. Note that `TRandom1` is a 24 bit generator. ; * Improve thread safety of `TMinuit` constructor [ROOT-8217]; * Vc has ben removed from the ROOT sources. If the option 'vc' is enabled, the package will be searched (by default),; alternatively the source tarfile can be downloded and build with the option 'builtin_vc'. ## TMVA Libraries. * New `DataLoader` class that allows flexibility in variable and dataset selection. ; * New Deep Neural Network. Three different versions are available, which can be selected with the 'Architecture' option. See also the tutorial`tmva/TMVAClassification.C` for using the new DNN.; * `Architecture=STANDARD` to select the earlier version.; * `Architecture=CPU` to select the newer version for CPU, but designed also for GPU and optimized for speed and with multi-class support. ; * `Architecture=GPU` to select the newer GPU version. Requires configuration of ROOT with CUDA or OpenCL enabled. ; * Support for Cross Validation (see tutorial `tmva/TMVACrossValidation` as an example).; * Support for Hyper-Parameter tuning for BDT and SVM methods.; * New Variable Importance algorithm independent of the MVA method.; * New Loss Function class for regression.; * Improvements in the SVM method: new kernel functions.; * New `ROCCurve` class. ; * New interface to Keras (PyKeras) available in the PyMVA library.; * Support for Jupyter notebooks; * Support for all the functionality available in GUI: preprocessing, variable correlations, classifier output.; * New classifier visualization for BDT, ANN and DNN.; * Interactive training for all methods. ## 2D Graphics Libraries. * In `TColor::SetPalette`, make sure the high quality palettes are defined; only once taking care of transparency. Also `CreateGradie",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md:12947,optimiz,optimized,12947,README/ReleaseNotes/v608/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md,1,['optimiz'],['optimized']
Performance,"NABLE_RUNTIMES ${LLVM_DEFAULT_RUNTIMES}); endif(); foreach(proj IN LISTS LLVM_ENABLE_RUNTIMES); if (NOT ""${proj}"" IN_LIST LLVM_SUPPORTED_RUNTIMES); message(FATAL_ERROR ""Runtime \""${proj}\"" is not a supported runtime. Supported runtimes are: ${LLVM_SUPPORTED_RUNTIMES}""); endif(); endforeach(). if (""libc"" IN_LIST LLVM_ENABLE_RUNTIMES); # To build the libc runtime, we need to be able to build few libc build; # tools from the ""libc"" project. So, we add it to the list of enabled; # projects.; if (NOT ""libc"" IN_LIST LLVM_ENABLE_PROJECTS); message(STATUS ""Enabling libc project to build libc build tools""); list(APPEND LLVM_ENABLE_PROJECTS ""libc""); endif(); endif(). # LLVM_ENABLE_PROJECTS_USED is `ON` if the user has ever used the; # `LLVM_ENABLE_PROJECTS` CMake cache variable. This exists for; # several reasons:; #; # * As an indicator that the `LLVM_ENABLE_PROJECTS` list is now the single; # source of truth for which projects to build. This means we will ignore user; # supplied `LLVM_TOOL_<project>_BUILD` CMake cache variables and overwrite; # them.; #; # * The case where the user previously had `LLVM_ENABLE_PROJECTS` set to a; # non-empty list but now the user wishes to disable building all other projects; # by setting `LLVM_ENABLE_PROJECTS` to an empty string. In that case we still; # need to set the `LLVM_TOOL_${upper_proj}_BUILD` variables so that we disable; # building all the projects that were previously enabled.; set(LLVM_ENABLE_PROJECTS_USED OFF CACHE BOOL """"); mark_as_advanced(LLVM_ENABLE_PROJECTS_USED). if (LLVM_ENABLE_PROJECTS_USED OR NOT LLVM_ENABLE_PROJECTS STREQUAL """"); set(LLVM_ENABLE_PROJECTS_USED ON CACHE BOOL """" FORCE); foreach(proj ${LLVM_KNOWN_PROJECTS} ${LLVM_EXTERNAL_PROJECTS}); string(TOUPPER ""${proj}"" upper_proj); string(REGEX REPLACE ""-"" ""_"" upper_proj ${upper_proj}); if (""${proj}"" IN_LIST LLVM_ENABLE_PROJECTS); message(STATUS ""${proj} project is enabled""); set(SHOULD_ENABLE_PROJECT TRUE); set(PROJ_DIR ""${CMAKE_CURRENT_SOURCE_DIR}/../${proj}""); if",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:8195,cache,cache,8195,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['cache'],['cache']
Performance,"NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. load atomic acquire - agent - generic 1. flat_load sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. load atomic acquire - system - generic 1. flat_load sc0=1 sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; the following; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_inv.; - Ensures the atomicrmw; has completed; before invalidating; the cache. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any followin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:298756,load,loads,298756,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,NN - Optimization CPU; ROOT_EXECUTABLE(testOptimizationCpu TestOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Optimization-Cpu COMMAND testOptimizationCpu). # DNN - MethodDL SGD Optimization CPU; ROOT_EXECUTABLE(testMethodDLSGDOptimizationCpu TestMethodDLSGDOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-SGD-Optimization-Cpu COMMAND testMethodDLSGDOptimizationCpu). # DNN - MethodDL Adam Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdamOptimizationCpu TestMethodDLAdamOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adam-Optimization-Cpu COMMAND testMethodDLAdamOptimizationCpu TIMEOUT 1800). # DNN - MethodDL Adagrad Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdagradOptimizationCpu TestMethodDLAdagradOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adagrad-Optimization-Cpu COMMAND testMethodDLAdagradOptimizationCpu). # DNN - MethodDL RMSProp Optimization CPU; ROOT_EXECUTABLE(testMethodDLRMSPropOptimizationCpu TestMethodDLRMSPropOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-RMSProp-Optimization-Cpu COMMAND testMethodDLRMSPropOptimizationCpu). # DNN - MethodDL Adadelta Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdadeltaOptimizationCpu TestMethodDLAdadeltaOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adadelta-Optimization-Cpu COMMAND testMethodDLAdadeltaOptimizationCpu). # DNN - Regression CPU; ROOT_EXECUTABLE(testRegressionCpu TestRegressionMethodDL.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Regression-Cpu COMMAND testRegressionCpu). #( old-dnn-test ); # DNN - DataLoader CPU; ROOT_EXECUTABLE(testDataLoaderCpu TestDataLoaderCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Data-Loader-Cpu COMMAND testDataLoaderCpu). # DNN - Minimization CPU; ROOT_EXECUTABLE(testMinimizationCpu TestMinimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Minimization-Cpu COMMAND testMinimizat,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt:6331,Optimiz,Optimization,6331,tmva/tmva/test/DNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt,1,['Optimiz'],['Optimization']
Performance,"NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND test_targets ${target}-${name}); endforeach(); set(test_targets ""${test_targets}"" PARENT_SCOPE); endif(). set(${name}_extra_args ${ARG_CMAKE_ARGS}); string(REPLACE "";"" ""|"" LLVM_ENABLE_RUNTIMES_PASSTHROUGH ""${LLVM_ENABLE_RUNTIMES}""); list(APPEND ${name}_extra_args -DLLVM_ENABLE_RUNTIMES=${LLVM_ENABLE_RUNTIMES_PASSTHROUGH}); list(APPEND ${name}_extra_args -DLLVM_USE_LINKER=${LLVM_USE_LINKER}). get_cmake_property(variable_names VARIABLES); foreach(extra_name IN ITEMS ${ARG_BASE_NAME} ${name}); foreach(variable_name ${variable_names}); string(FIND ""${variable_name}"" ""RUNTIMES_${extra_name}_"" out); if(""${out}"" EQUAL 0); string(REPLACE ""RUNTIMES_${extra_name}_"" """" new_name ${variable_name}); if(new_name STREQUAL CACHE_FILES); foreach(cache IN LISTS ${variable_name}); list(APPEND ${name}_extra_args -C ${cache}); endforeach(); else(); string(REPLACE "";"" ""|"" new_value ""${${variable_name}}""); list(APPEND ${name}_extra_args ""-D${new_name}=${new_value}""); endif(); endif(); endforeach(); endforeach(). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(runtimes-${name}; ${CMAKE_CURRENT_SOURCE_DIR}/../../runtimes; DEPENDS ${ARG_DEPENDS}; # Builtins were built separately above; CMAKE_ARGS -DCOMPILER_RT_BUILD_BUILTINS=OFF; -DLLVM_INCLUDE_TESTS=${LLVM_INCLUDE_TESTS}; -DLLVM_ENABLE_PROJECTS_USED=${LLVM_ENABLE_PROJECTS_USED}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:12917,cache,cache,12917,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,2,['cache'],['cache']
Performance,"New directive 'xpd.datadir' to better control the user data directories and their permission settings. In TPacketizerUnit, add the possibility to exactly share the number of cycles between the workers.; See the parameter PROOF_PacketizerFixedNum.Implement; a timer to terminate idle sessions. The timeout value is controlled by; the variable ProofServ.IdleTimeout (value in seconds). This variable; can be set for all sessions in the xproofd config file via the 'xpd.putrc' directive.; Add the possibility to control the use of sub-mergers with; the ROOTrc variable Proof.SubMergers. It has the same meaning of the; parameter 'PROOF_UseMergers'. The capabilities of the latter have been; extended: now -1 means disable the use of submergers (before  negative values were ignored and there was no way for the user to disable the use of submergers). . Packetizer optimizations: improved worked distribution when; the number of files left to be processed is smaller than the number of; workers and at least one file has a number of events significantly; larger than the average; better apply the upper/lower limits on the; expected packet processing time.; Add the possibility to single-out disk partitions in the; packetizer; this works adding the beginning of a path in the name; defining a new TFileNode (e.g. 'host://disk1' instead of 'host' only as; it was so far). These feature can be enabled by defining the rootrc; variable 'Packetizer.Partitions', e.g.;            Packetizer.Partitions  /disk1,/disk2,/disk3; Add to the output list the parameters used by the active packetizer. . In the PrintProgress function used to display a text progress; bar, show also the average reading rate in [k,M,G}bytes/s in addition; to the event processing rate. This is useful to have a feeling of the; rate when running of a remote machine in batch mode.; Add the possibility to control the resident and virtual; memory of a proofserv using 'ulimit', which has less limitations and; more flexibility than setrl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v528/index.html:7091,optimiz,optimizations,7091,proof/doc/v528/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v528/index.html,2,['optimiz'],['optimizations']
Performance,"Node(x,y,z); ```. Note that the current particle position can be set using; **`SetCurrentPosition(x,y,z)`** method of the manager class, in which; case **`FindNode()`** can be called without arguments. The method; returns a pointer to the *deepest node* that geometrically contains *P*; (in our case let us suppose it is *B\_3*). Since a node is just a; positioned volume, we can then get a pointer to the volume, medium or; material objects related to it. *Deepest* means that *B\_3* still; contains point *P* (as well as *A\_1* and *TOP\_1*), but none of the; daughters of volume **B** does. After finding out the node containing; the particle, one can check if the geometry state is different compared; to the last located point:. ``` {.cpp}; Bool_t *TGeoManager::IsSameLocation(); ```. The algorithm for finding where a point is located in geometry is; presented in the figure 17-36. It always starts by checking if the last computed modeller state is the; answer. This optimizes the search when continuously tracking a particle.; The main actions performed are:. - moving up and down in the logical node tree while updating the; current node and its global matrix; - converting the global position into the local frame of the current; node/volume; - checking whether the local position lies within the geometrical; shape of the current volume - if this is the case continue the; search downwards for the daughters of the current node, otherwise; search upwards its containers until the top level is reached.; - the number of candidate nodes to be checked at a given level is; minimized by an additional optimization structure: voxels. This is; effective even in case there is only one daughter of the current; volume.; - in case the current node is declared as possibly overlapping, the; method FindInCluster() is invoked. This method checks all different; possibilities within the cluster of overlapping candidates. One of; the candidates is prioritized if one of the following conditions id; fu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:157115,optimiz,optimizes,157115,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['optimiz'],['optimizes']
Performance,"O bitcode is augmented with a compact summary; of the module. During the link step, only the summaries are read and; merged into a combined summary index, which includes an index of function; locations for later cross-module function importing. Fast and efficient; whole-program analysis is then performed on the combined summary index. However, all transformations, including function importing, occur; later when the modules are optimized in fully parallel backends.; By default, linkers_ that support ThinLTO are set up to launch; the ThinLTO backends in threads. So the usage model is not affected; as the distinction between the fast serial thin link step and the backends; is transparent to the user. For more information on the ThinLTO design and current performance,; see the LLVM blog post `ThinLTO: Scalable and Incremental LTO; <http://blog.llvm.org/2016/06/thinlto-scalable-and-incremental-lto.html>`_.; While tuning is still in progress, results in the blog post show that; ThinLTO already performs well compared to LTO, in many cases matching; the performance improvement. Current Status; ==============. Clang/LLVM; ----------; .. _compiler:. The 3.9 release of clang includes ThinLTO support. However, ThinLTO; is under active development, and new features, improvements and bugfixes; are being added for the next release. For the latest ThinLTO support,; `build a recent version of clang and LLVM; <https://llvm.org/docs/CMake.html>`_. Linkers; -------; .. _linkers:; .. _linker:. ThinLTO is currently supported for the following linkers:. - **gold (via the gold-plugin)**:; Similar to monolithic LTO, this requires using; a `gold linker configured with plugins enabled; <https://llvm.org/docs/GoldPlugin.html>`_.; - **ld64**:; Starting with `Xcode 8 <https://developer.apple.com/xcode/>`_.; - **lld**:; Starting with r284050 for ELF, r298942 for COFF. Usage; =====. Basic; -----. To utilize ThinLTO, simply add the -flto=thin option to compile and link. E.g. .. code-block:: console",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst:1566,perform,performs,1566,interpreter/llvm-project/clang/docs/ThinLTO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThinLTO.rst,2,['perform'],"['performance', 'performs']"
Performance,"O is the location bit offset; specified by a single location description SL of L. The remaining bit; storage size RBSS of SL is the bit size of LS minus LO. *rbss(L)* is the; minimum RBSS of each single location description SL of L. The DWARF expression is ill-formed if *rbss(BL)* is less than BO plus BS. If BS is 0, then the operation pushes BL. If BO is 0 and BS equals *rbss(BL)*, then the operation pushes OL. Otherwise, the operation is equivalent to performing the following steps to; push a composite location description. *The composite location description is conceptually the base location; description BL with the overlay location description OL positioned as an; overlay starting at the overlay offset BO and covering overlay bit size BS.*. 1. If BO is not 0 then push BL followed by performing the ``DW_OP_bit_piece; BO, 0`` operation.; 2. Push OL followed by performing the ``DW_OP_bit_piece BS, 0`` operation.; 3. If *rbss(BL)* is greater than BO plus BS, push BL followed by performing; the ``DW_OP_bit_piece (rbss(BL) - BO - BS), (BO + BS)`` operation.; 4. Perform the ``DW_OP_LLVM_piece_end`` operation. .. _amdgpu-dwarf-location-list-expressions:. A.2.5.5 DWARF Location List Expressions; +++++++++++++++++++++++++++++++++++++++. .. note::. This section replaces DWARF Version 5 section 2.6.2. *To meet the needs of recent computer architectures and optimization techniques,; debugging information must be able to describe the location of an object whose; location changes over the object’s lifetime, and may reside at multiple; locations during parts of an object's lifetime. Location list expressions are; used in place of operation expressions whenever the object whose location is; being described has these requirements.*. A location list expression consists of a series of location list entries. Each; location list entry is one of the following kinds:. *Bounded location description*. This kind of location list entry provides an operation expression that; evaluates to the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:142174,perform,performing,142174,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['performing']
Performance,"ONS; -------. .. option:: --help. Print a summary of command line options. .. option:: --opcode-index=<LLVM opcode index>. Specify the opcode to measure, by index. Specifying `-1` will result; in measuring every existing opcode. See example 1 for details.; Either `opcode-index`, `opcode-name` or `snippets-file` must be set. .. option:: --opcode-name=<opcode name 1>,<opcode name 2>,... Specify the opcode to measure, by name. Several opcodes can be specified as; a comma-separated list. See example 1 for details.; Either `opcode-index`, `opcode-name` or `snippets-file` must be set. .. option:: --snippets-file=<filename>. Specify the custom code snippet to measure. See example 2 for details.; Either `opcode-index`, `opcode-name` or `snippets-file` must be set. .. option:: --mode=[latency|uops|inverse_throughput|analysis]. Specify the run mode. Note that some modes have additional requirements and options. `latency` mode can be make use of either RDTSC or LBR.; `latency[LBR]` is only available on X86 (at least `Skylake`).; To run in `latency` mode, a positive value must be specified; for `x86-lbr-sample-period` and `--repetition-mode=loop`. In `analysis` mode, you also need to specify at least one of the; `-analysis-clusters-output-file=` and `-analysis-inconsistencies-output-file=`. .. option:: --benchmark-phase=[prepare-snippet|prepare-and-assemble-snippet|assemble-measured-code|measure]. By default, when `-mode=` is specified, the generated snippet will be executed; and measured, and that requires that we are running on the hardware for which; the snippet was generated, and that supports performance measurements.; However, it is possible to stop at some stage before measuring. Choices are:; * ``prepare-snippet``: Only generate the minimal instruction sequence.; * ``prepare-and-assemble-snippet``: Same as ``prepare-snippet``, but also dumps an excerpt of the sequence (hex encoded).; * ``assemble-measured-code``: Same as ``prepare-and-assemble-snippet``. but also creates",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst:10143,latency,latency,10143,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,1,['latency'],['latency']
Performance,"OOL; ^^^^^^^^^^^^^^^. The address of an object in the constant pool. .. code-block:: none. %0:_(p0) = G_CONSTANT_POOL %const.0. Integer Extension and Truncation; --------------------------------. G_ANYEXT; ^^^^^^^^. Extend the underlying scalar type of an operation, leaving the high bits; unspecified. .. code-block:: none. %1:_(s32) = G_ANYEXT %0:_(s16). G_SEXT; ^^^^^^. Sign extend the underlying scalar type of an operation, copying the sign bit; into the newly-created space. .. code-block:: none. %1:_(s32) = G_SEXT %0:_(s16). G_SEXT_INREG; ^^^^^^^^^^^^. Sign extend the value from an arbitrary bit position, copying the sign bit; into all bits above it. This is equivalent to a shl + ashr pair with an; appropriate shift amount. $sz is an immediate (MachineOperand::isImm(); returns true) to allow targets to have some bitwidths legal and others; lowered. This opcode is particularly useful if the target has sign-extension; instructions that are cheaper than the constituent shifts as the optimizer is; able to make decisions on whether it's better to hang on to the G_SEXT_INREG; or to lower it and optimize the individual shifts. .. code-block:: none. %1:_(s32) = G_SEXT_INREG %0:_(s32), 16. G_ZEXT; ^^^^^^. Zero extend the underlying scalar type of an operation, putting zero bits; into the newly-created space. .. code-block:: none. %1:_(s32) = G_ZEXT %0:_(s16). G_TRUNC; ^^^^^^^. Truncate the underlying scalar type of an operation. This is equivalent to; G_EXTRACT for scalar types, but acts elementwise on vectors. .. code-block:: none. %1:_(s16) = G_TRUNC %0:_(s32). Type Conversions; ----------------. G_INTTOPTR; ^^^^^^^^^^. Convert an integer to a pointer. .. code-block:: none. %1:_(p0) = G_INTTOPTR %0:_(s32). G_PTRTOINT; ^^^^^^^^^^. Convert a pointer to an integer. .. code-block:: none. %1:_(s32) = G_PTRTOINT %0:_(p0). G_BITCAST; ^^^^^^^^^. Reinterpret a value as a new type. This is usually done without; changing any bits but this is not always the case due a subtlety in the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst:1979,optimiz,optimizer,1979,interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,2,['optimiz'],"['optimize', 'optimizer']"
Performance,"OOT enables; access from ROOT to any application or library that itself has Python; bindings, and it makes all ROOT functionality directly available from; the python interpreter. ## PyROOT Overview. The Python scripting language is widely used for scientific programming,; including high performance and distributed parallel code (see; <http://www.scipy.org>). It is the second most popular scripting; language (after Perl) and enjoys a wide-spread use as a ""glue language"":; practically every library and application these days comes with Python; bindings (and if not, they can be easily written or generated). `PyROOT`, a Python extension module, provides the bindings for the ROOT; class library in a generic way using the Cling dictionary. This way, it; allows the use of any ROOT classes from the Python interpreter, and thus; the ""glue-ing"" of ROOT libraries with any non-ROOT library or; applications that provide Python bindings. Further, `PyROOT` can be; loaded into the Cling interpreter to allow (as of now still rudimentary); access to Python classes. The best way to understand the benefits of; `PyROOT` is through a few examples. ### Glue-ing Applications. The `PyQt` library, see <http://www.riverbankcomputing.co.uk/pyqt>,; provides Python bindings for the Qt cross-platform GUI framework (; <http://www.trolltech.com>). With `PyROOT` and `PyQt`, adding ROOT; application layer code to a Qt GUI, becomes children play. The following; example shows how a Python class can be used to have ROOT code respond; to a click on a Qt widget. ``` {.cpp}; # Glue-ing Qt and ROOT through Python; import sys, ROOT; from qt import *. theApp = QApplication( sys.argv); box = QVBox(); box.resize(QSize(40,10).expandedTo(box.minimumSizeHint())). class myButton(QPushButton):; def __init__( self,label,master):; QPushButton.__init__(self,label,master); self.setFont( QFont('Times',18,QFont.Bold)). def browse(self):; self.b = ROOT.TBrowser(). bb = myButton('browser',box); QObject.connect( bb,SIGNAL('c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:1553,load,loaded,1553,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['load'],['loaded']
Performance,"OOT.NewHttpRequest() function; 6. JSROOT.openFile() returns Promise with file instance, deprecate callback parameter; 7. Provide new code loader via JSROOT.require(); - introduces clean dependencies in JSROOT code; - by default uses plain script loading emulating require.js behavior; - can use require.js when available; - uses require() method when running inside node.js; - supports openui5 sap.ui.require loader if available before JSRoot.core.js; - deprecates old JSROOT.AssertPrerequisites() function; 8. Upgrade d3.js to v6.1.1, skip support of older versions; 9. Upgrade three.js to r121:; - SoftwareRenderer deprecated and removed; - let use WebGL for browser, batch and node.js (via headless-gl); - support r3d_gl, r3d_img, r3d_svg rendering options for TGeo and histograms; - keep support of SVGRendered as backup solution; 10. Upgrade MathJax.js to version 3.1.1; - reliably works in browser and node.js!; - all latex/mathjax related methods moved to special JSRoot.latex.js script, loaded on demand; 11. Update jquery to 3.5.1, openui5 to 1.82.2; 12. Use JS classes only in few places - performance is not good enough compared to Object.prototype; 13. Deprecate IE support; 14. Deprecate bower package manager; 15. Add support of ZSTD compression - works only on https://root.cern/js/ website; 16. Add support of log2 scale for axes drawing, v7 can have arbitrary log base; 17. Improve TH2 col drawings for large number of bins - up to factor 5 faster; 18. Allow to move axis title to opposite position; 19. Fix zooming in color palette; 20. Implement monitoring of object inspector. ## Changes in 5.9.1; 1. Fix zooming in color palette; 2. Fix interactive update of TGraph painting on time scale; 3. Fix I/O error in reading std::map (#204); 4. Fix functionality of ""open all"" / ""close all"" GUI buttons. ## Changes in 5.9.0; 1. Support RX and RY drawing option together with COL of TH2; 2. Add support of #overline, #underline, #strike into TLatex parsing (#196); 3. Add support of TGeo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:26280,load,loaded,26280,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['load'],['loaded']
Performance,"OOTAllocator`, `TSchemaRuleProcessor`, `TStdBitsetHelper`, `TInitBehavior`, `TDefaultInitBehavior`, `DefineBehavior`, `THnBaseBrowsable`, `THnBaseBinIter`, `GenericShowMembers`, `TOperatorNewHelper` and `BranchProxy` implementations classes. Several definition where moved from the global or ROOT namespace to the ROOT::Details namespace as they are intended to be used in 'expert' level code and have a lower level of backward compatibility requirement. This includes `TCollectionProxyInfo`, `TSchemaRuleSet`. ## Interpreter. ROOT can now dump the context of STL collections, for instance `map<string,int>`. A few ROOT types print their content, too. Fixed the handling of the current directory in `#include` of system headers, avoid problem with local files named `new` or `vector`. Fixed the issue with the ROOT special variable where the objects were read from the file at each and every access by caching those object. See [ROOT-7830] for example. This release contains several bug fixes and improvements, notably in unloading and performance. > NOTE: The GCC 5 ABI is *not* supported yet, due to a lack of support in clang. ## I/O Libraries. ### hadd. We extended the `hadd` options to allow more control on the compression settings use for the; output file. In particular the new option -fk allows for a copy of the input; files with no decompressions/recompression of the TTree baskets even if they; do not match the requested compression setting. New options:. - `-ff` allows to force the compression setting to match the one from the first input; - `-fk[0-209]` allows to keep all the basket compressed as is and to compress the meta data with the given compression setting or the compression setting of the first input file.; - `-a` option append to existing file; - The verbosity level is now optional after -v. ### Command line utilities. We added command line utilities to streamline very common operations performed on root files, like listing their content or creating directories.; T",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v606/index.md:6619,perform,performance,6619,README/ReleaseNotes/v606/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v606/index.md,1,['perform'],['performance']
Performance,OOT_EXECUTABLE(testDerivativesCpu TestDerivativesCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Derivatives-Cpu COMMAND testDerivativesCpu). # DNN - Backpropagation CPU; ROOT_EXECUTABLE(testBackpropagationCpu TestBackpropagationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Backpropagation-Cpu COMMAND testBackpropagationCpu). # DNN - BackpropagationDL CPU; ROOT_EXECUTABLE(testBackpropagationDLCpu TestBackpropagationDLCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Backpropagation-DL-Cpu COMMAND testBackpropagationDLCpu). # DNN - Batch normalization; ROOT_EXECUTABLE(testBatchNormalizationCpu TestBatchNormalizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-BatchNormalization-Cpu COMMAND testBatchNormalizationCpu). # DNN - Optimization CPU; ROOT_EXECUTABLE(testOptimizationCpu TestOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Optimization-Cpu COMMAND testOptimizationCpu). # DNN - MethodDL SGD Optimization CPU; ROOT_EXECUTABLE(testMethodDLSGDOptimizationCpu TestMethodDLSGDOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-SGD-Optimization-Cpu COMMAND testMethodDLSGDOptimizationCpu). # DNN - MethodDL Adam Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdamOptimizationCpu TestMethodDLAdamOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adam-Optimization-Cpu COMMAND testMethodDLAdamOptimizationCpu TIMEOUT 1800). # DNN - MethodDL Adagrad Optimization CPU; ROOT_EXECUTABLE(testMethodDLAdagradOptimizationCpu TestMethodDLAdagradOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-Adagrad-Optimization-Cpu COMMAND testMethodDLAdagradOptimizationCpu). # DNN - MethodDL RMSProp Optimization CPU; ROOT_EXECUTABLE(testMethodDLRMSPropOptimizationCpu TestMethodDLRMSPropOptimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-MethodDL-RMSProp-Optimization-Cpu COMMAND testMethodDLRMSPropOptimizationCpu). # DNN - MethodDL Adadelta Optimization C,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt:5569,Optimiz,Optimization,5569,tmva/tmva/test/DNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt,1,['Optimiz'],['Optimization']
Performance,"OR; * G_VECREDUCE_SMAX; * G_VECREDUCE_SMIN; * G_VECREDUCE_UMAX; * G_VECREDUCE_UMIN. Integer reductions may have a result type larger than the vector element type.; However, the reduction is performed using the vector element type and the value; in the top bits is unspecified. Memory Operations; -----------------. G_LOAD, G_SEXTLOAD, G_ZEXTLOAD; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Generic load. Expects a MachineMemOperand in addition to explicit; operands. If the result size is larger than the memory size, the; high bits are undefined, sign-extended, or zero-extended respectively. Only G_LOAD is valid if the result is a vector type. If the result is larger; than the memory size, the high elements are undefined (i.e. this is not a; per-element, vector anyextload). Unlike in SelectionDAG, atomic loads are expressed with the same; opcodes as regular loads. G_LOAD, G_SEXTLOAD and G_ZEXTLOAD may all; have atomic memory operands. G_INDEXED_LOAD; ^^^^^^^^^^^^^^. Generic indexed load. Combines a GEP with a load. $newaddr is set to $base + $offset.; If $am is 0 (post-indexed), then the value is loaded from $base; if $am is 1 (pre-indexed); then the value is loaded from $newaddr. G_INDEXED_SEXTLOAD; ^^^^^^^^^^^^^^^^^^. Same as G_INDEXED_LOAD except that the load performed is sign-extending, as with G_SEXTLOAD. G_INDEXED_ZEXTLOAD; ^^^^^^^^^^^^^^^^^^. Same as G_INDEXED_LOAD except that the load performed is zero-extending, as with G_ZEXTLOAD. G_STORE; ^^^^^^^. Generic store. Expects a MachineMemOperand in addition to explicit; operands. If the stored value size is greater than the memory size,; the high bits are implicitly truncated. If this is a vector store, the; high elements are discarded (i.e. this does not function as a per-lane; vector, truncating store). G_INDEXED_STORE; ^^^^^^^^^^^^^^^. Combines a store with a GEP. See description of G_INDEXED_LOAD for indexing behaviour. G_ATOMIC_CMPXCHG_WITH_SUCCESS; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Generic atomic cmpxchg with internal succ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst:16130,load,load,16130,interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,1,['load'],['load']
Performance,"OR}.${LLVM_VERSION_PATCH}; LANGUAGES C CXX ASM). if (NOT DEFINED CMAKE_INSTALL_LIBDIR AND DEFINED LLVM_LIBDIR_SUFFIX); # Must go before `include(GNUInstallDirs)`.; set(CMAKE_INSTALL_LIBDIR ""lib${LLVM_LIBDIR_SUFFIX}""); endif(). # Must go after `DEFINED LLVM_LIBDIR_SUFFIX` check.; set(LLVM_LIBDIR_SUFFIX """" CACHE STRING ""Define suffix of library directory name (32/64)"" ). # Must go after `project(..)`.; include(GNUInstallDirs). # This C++ standard is required to build LLVM.; set(LLVM_REQUIRED_CXX_STANDARD 17). # If we find that the cache contains CMAKE_CXX_STANDARD it means that it's a old CMakeCache.txt; # and we can just inform the user and then reset it.; if($CACHE{CMAKE_CXX_STANDARD} AND $CACHE{CMAKE_CXX_STANDARD} LESS ${LLVM_REQUIRED_CXX_STANDARD}); message(WARNING ""Resetting cache value for CMAKE_CXX_STANDARD to ${LLVM_REQUIRED_CXX_STANDARD}""); unset(CMAKE_CXX_STANDARD CACHE); endif(). # if CMAKE_CXX_STANDARD is still set after the cache unset above it means that the user requested it; # and we allow it to be set to something newer than the required standard but otherwise we fail.; if(DEFINED CMAKE_CXX_STANDARD AND CMAKE_CXX_STANDARD LESS ${LLVM_REQUIRED_CXX_STANDARD}); message(FATAL_ERROR ""Requested CMAKE_CXX_STANDARD=${CMAKE_CXX_STANDARD} which is less than the required ${LLVM_REQUIRED_CXX_STANDARD}.""); endif(). set(CMAKE_CXX_STANDARD ${LLVM_REQUIRED_CXX_STANDARD} CACHE STRING ""C++ standard to conform to""); set(CMAKE_CXX_STANDARD_REQUIRED YES). if (CYGWIN); # Cygwin is a bit stricter and lack things like 'strdup', 'stricmp', etc in; # c++xx mode.; set(CMAKE_CXX_EXTENSIONS YES); else(); set(CMAKE_CXX_EXTENSIONS NO); endif(). if (NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES); message(FATAL_ERROR ""; No build type selected. You need to pass -DCMAKE_BUILD_TYPE=<type> in order to configure LLVM.; Available options are:; * -DCMAKE_BUILD_TYPE=Release - For an optimized build with no assertions or debug info.; * -DCMAKE_BUILD_TYPE=Debug - For an unoptimized bui",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:3071,cache,cache,3071,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['cache'],['cache']
Performance,"OT shell. For more difficult problems the full FOAM may be better. ### How to run application programs ?. The application program can be run in two modes: it can be simply; interpreted by CLING or compiled. The first method is simpler but; results in slower execution. The second method employs ACLiC -; The Automatic Compiler of Libraries, which automatizes the; process of compilation and linking. In $(ROOTSYS)/tutorials there are 3 demonstration programs:. #### foam_kanwa.C; is a simple example how to run FOAM in interactive; mode. To run this macro issue the following simple command from the; Linux shell:. ```; root foam_kanwa.C; ```. or from CLING:. ```; root [0] .x foam_kanwa.C; ```. Simulation will start and graphical canvas with plot; of the distribution function appear. In this example; we defined the distribution function simply as a global; function function Camel2. #### foam_demo.C; shows usage of FOAM in compiled mode, which is; the preferred method. The integrand function is defined; now as a Density method from class TFDISTR inheriting from; abstract class TFoamIntegrand. User can modify interface to; integrand function according to their needs but they should; always remember to define Density method which provides the; density distribution.; Enter CLING interpreter and type:. ```; root [0] gSystem->Load(""libFoam.so""); root [1] .x foam_demo.C+; ```. to load FOAM library, compile and execute macro foam_demo.C.; A shared object foam_demo_C.so is created in the current; directory. At the end of exploration phase FOAM object; including distribution function will be written to disk. #### foam_demopers.C; demonstrates persistency of FOAM classes.; To run this macro type:. ```; root [0] .x foam_demopers.C; ```. Program reads the FOAM object from disk, checks its; consistency and prints geometry of cells. Next starts the; the generation. It can be interpreted directly by CLING; because compiled TFDISTR class is already available in; `foam_demo_C.so` library. @}",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tutorials/foam/index.md:1845,load,load,1845,tutorials/foam/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/foam/index.md,1,['load'],['load']
Performance,"OT-8055]. - ##### TBufferJSON:; + support data members with `//[fN]` comment; + preliminary support of STL containers; + JSON data can be produced with `TObject::SaveAs()` method. ## TTree Libraries. * TChains can now be histogrammed without any C++ code, using the command line tool `rootdrawtree`. It is based on the new class `TSimpleAnalysis`.; * Do not automatically setup read cache during `TTree::Fill()`. This fixes [ROOT-8031].; * Make sure the option ""PARA"" in `TTree::Draw` is used with at least tow variables [ROOT-8196].; * The with `goff` option one can use as many variables as needed. There no more; limitation, like with the options `para`and `candle`.; * Fix detection of errors that appears in nested TTreeFormula [ROOT-8218]; * Better basket size optimization by taking into account meta data and rounding up to next 512 bytes, ensuring a complete cluster fits into a single basket. ### Fast Cloning. We added a cache specifically for the fast option of the TTreeCloner to significantly reduce the run-time when fast-cloning remote files to address [ROOT-5078]. It can be controlled from the `TTreeCloner`, `TTree::CopyEntries` or `hadd` interfaces. The new cache is enabled by default, to update the size of the cache or disable it from `TTreeCloner` use: `TTreeCloner::SetCacheSize`. To do the same from `TTree::CopyEntries` add to the option string ""cachesize=SIZE"". To update the size of the cache or disable it from `hadd`, use the command line option `-cachesize SIZE`. `SIZE` shouyld be given in number bytes and can be expressed in 'human readable form' (number followed by size unit like MB, MiB, GB or GiB, etc. or SIZE can be set zero to disable the cache. ### Other Changes. * Update `TChain::LoadTree` so that the user call back routine is actually called for each input file even those containing `TTree` objects with no entries.; * Repair setting the branch address of a leaflist style branch taking directly the address of the struct. (Note that leaflist is nonethe",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md:9493,cache,cache,9493,README/ReleaseNotes/v608/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v608/index.md,1,['cache'],['cache']
Performance,"OT/PIC code only module-local calls (visibility =; hidden or protected) are supported. PowerPC constraints:. * No variable argument lists are used. * No byval parameters are used. * On ppc32/64 GOT/PIC only module-local calls (visibility = hidden or protected); are supported. WebAssembly constraints:. * No variable argument lists are used. * The 'tail-call' target attribute is enabled. * The caller and callee's return types must match. The caller cannot; be void unless the callee is, too. AArch64 constraints:. * No variable argument lists are used. Example:. Call as ``llc -tailcallopt test.ll``. .. code-block:: llvm. declare fastcc i32 @tailcallee(i32 inreg %a1, i32 inreg %a2, i32 %a3, i32 %a4). define fastcc i32 @tailcaller(i32 %in1, i32 %in2) {; %l1 = add i32 %in1, %in2; %tmp = tail call fastcc i32 @tailcallee(i32 inreg %in1, i32 inreg %in2, i32 %in1, i32 %l1); ret i32 %tmp; }. Implications of ``-tailcallopt``:. To support tail call optimization in situations where the callee has more; arguments than the caller a 'callee pops arguments' convention is used. This; currently causes each ``fastcc`` call that is not tail call optimized (because; one or more of above constraints are not met) to be followed by a readjustment; of the stack. So performance might be worse in such cases. Sibling call optimization; -------------------------. Sibling call optimization is a restricted form of tail call optimization.; Unlike tail call optimization described in the previous section, it can be; performed automatically on any tail calls when ``-tailcallopt`` option is not; specified. Sibling call optimization is currently performed on x86/x86-64 when the; following constraints are met:. * Caller and callee have the same calling convention. It can be either ``c`` or; ``fastcc``. * The call is a tail call - in tail position (ret immediately follows call and; ret uses value of call or is void). * Caller and callee have matching return type or the callee result is not used. * If any of ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:87881,optimiz,optimization,87881,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimization']
Performance,"Offloading entry information (see :ref:`table-tgt_offload_entry_structure`) |; +----------------------------------+------------------------------------------------------------------------------+; | .llvm.offloading | Embedded device object file for the target device and architecture |; +----------------------------------+------------------------------------------------------------------------------+. .. _Device Linking:. Linking Target Device Code; --------------------------. Objects containing :ref:`table-offloading_sections` require special handling to; create an executable device image. This is done using a Clang tool, see; :doc:`ClangLinkerWrapper` for more information. This tool works as a wrapper; over the host linking job. It scans the input object files for the offloading; section ``.llvm.offloading``. The device files stored in this section are then; extracted and passed to the appropriate linking job. The linked device image is; then :ref:`wrapped <Device Binary Wrapping>` to create the symbols used to load; the device image and link it with the host. The linker wrapper tool supports linking bitcode files through link time; optimization (LTO). This is used whenever the object files embedded in the host; contain LLVM bitcode. Bitcode will be embedded for architectures that do not; support a relocatable object format, such as AMDGPU or SPIR-V, or if the user; requested it using the ``-foffload-lto`` flag. .. _Device Binary Wrapping:. Device Binary Wrapping; ----------------------. Various structures and functions are used to create the information necessary to; offload code on the device. We use the :ref:`linked device executable <Device; Linking>` with the corresponding offloading entries to create the symbols; necessary to load and execute the device image. Structure Types; ^^^^^^^^^^^^^^^. Several different structures are used to store offloading information. The; :ref:`device image structure <table-device_image_structure>` stores a single; linked device i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst:14141,load,load,14141,interpreter/llvm-project/clang/docs/OffloadingDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst,1,['load'],['load']
Performance,"OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 2. flat_atomic; 3. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 4. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - generic 1. buffer_wbl2. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; atomicrmw that is; being released. 3. flat_atomic; 4. s_waitcnt vmcnt(0) &; lgkmcnt(0).",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:272638,load,loads,272638,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"Operator()); matches 'return a + b'; with binaryOperator(); matching 'a + b'. Matcher<StmtExpr>hasAnySubstatementMatcher<Stmt> InnerMatcher; Matches compound statements where at least one substatement matches; a given matcher. Also matches StmtExprs that have CompoundStmt as children. Given; { {}; 1+2; }; hasAnySubstatement(compoundStmt()); matches '{ {}; 1+2; }'; with compoundStmt(); matching '{}'. Matcher<Stmt>alignOfExprMatcher<UnaryExprOrTypeTraitExpr> InnerMatcher; Same as unaryExprOrTypeTraitExpr, but only matching; alignof. Matcher<Stmt>forCallableMatcher<Decl> InnerMatcher; Matches declaration of the function, method, or block the statement; belongs to. Given:; F& operator=(const F& o) {; std::copy_if(o.begin(), o.end(), begin(), [](V v) { return v > 0; });; return *this;; }; returnStmt(forCallable(functionDecl(hasName(""operator="")))); matches 'return *this'; but does not match 'return v > 0'. Given:; -(void) foo {; int x = 1;; dispatch_sync(queue, ^{ int y = 2; });; }; declStmt(forCallable(objcMethodDecl())); matches 'int x = 1'; but does not match 'int y = 2'.; whereas declStmt(forCallable(blockDecl())); matches 'int y = 2'; but does not match 'int x = 1'. Matcher<Stmt>forFunctionMatcher<FunctionDecl> InnerMatcher; Matches declaration of the function the statement belongs to. Deprecated. Use forCallable() to correctly handle the situation when; the declaration is not a function (but a block or an Objective-C method).; forFunction() not only fails to take non-functions into account but also; may match the wrong declaration in their presence. Given:; F& operator=(const F& o) {; std::copy_if(o.begin(), o.end(), begin(), [](V v) { return v > 0; });; return *this;; }; returnStmt(forFunction(hasName(""operator=""))); matches 'return *this'; but does not match 'return v > 0'. Matcher<Stmt>sizeOfExprMatcher<UnaryExprOrTypeTraitExpr> InnerMatcher; Same as unaryExprOrTypeTraitExpr, but only matching; sizeof. Matcher<SubstTemplateTypeParmType>hasReplacementTypeMatcher<",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html:232455,queue,queue,232455,interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LibASTMatchersReference.html,2,['queue'],['queue']
Performance,"OrCreateFileForWriting``. In the case of the soft ""doesn't exist"" error,; this function would just create it and then open it for writing. This design principle needs to be maintained in ``lib/Support`` because it; avoids the propagation of soft error handling throughout the rest of LLVM.; Hard errors will generally just cause a termination for an LLVM tool so don't; be bashful about throwing them. Rules of thumb:. #. Don't throw soft errors, only hard errors. #. If you're tempted to throw a soft error, re-think the interface. #. Handle internally the most common normal/good/soft error conditions; so the rest of LLVM doesn't have to. No throw Specifications; -----------------------. None of the ``lib/Support`` interface functions may be declared with C++; ``throw()`` specifications on them. This requirement makes sure that the; compiler does not insert additional exception handling code into the interface; functions. This is a performance consideration: ``lib/Support`` functions are; at the bottom of many call chains and as such can be frequently called. We; need them to be as efficient as possible. However, no routines in the system; library should actually throw exceptions. Code Organization; -----------------. Implementations of the Support Library interface are separated by their general; class of operating system. Currently only Unix and Win32 classes are defined; but more could be added for other operating system classifications. To; distinguish which implementation to compile, the code in ``lib/Support`` uses; the ``LLVM_ON_UNIX`` and ``_WIN32`` ``#defines``. Each source file in; ``lib/Support``, after implementing the generic (operating system independent); functionality needs to include the correct implementation using a set of; ``#if defined(LLVM_ON_XYZ)`` directives. For example, if we had; ``lib/Support/Path.cpp``, we'd expect to see in that file:. .. code-block:: c++. #if defined(LLVM_ON_UNIX); #include ""Unix/Path.inc""; #endif; #if defined(_WIN32); #inc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SupportLibrary.rst:9615,perform,performance,9615,interpreter/llvm-project/llvm/docs/SupportLibrary.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SupportLibrary.rst,1,['perform'],['performance']
Performance,"Other options you can use are:. .. code-block:: bash. Use Ninja instead of Make: ""-G Ninja""; Build with assertions on: ""-DLLVM_ENABLE_ASSERTIONS=True""; Local (non-sudo) install path: ""-DCMAKE_INSTALL_PREFIX=$HOME/llvm/install""; CPU flags: ""DCMAKE_C_FLAGS=-mcpu=cortex-a15"" (same for CXX_FLAGS). After that, just typing ``make -jN`` or ``ninja`` will build everything.; ``make -jN check-all`` or ``ninja check-all`` will run all compiler tests. For; running the test suite, please refer to :doc:`TestingGuide`. #. If you are building LLVM/Clang on an ARM board with 1G of memory or less,; please use ``gold`` rather then GNU ``ld``. In any case it is probably a good; idea to set up a swap partition, too. .. code-block:: bash. $ sudo ln -sf /usr/bin/ld /usr/bin/ld.gold. #. ARM development boards can be unstable and you may experience that cores; are disappearing, caches being flushed on every big.LITTLE switch, and; other similar issues. To help ease the effect of this, set the Linux; scheduler to ""performance"" on **all** cores using this little script:. .. code-block:: bash. # The code below requires the package 'cpufrequtils' to be installed.; for ((cpu=0; cpu<`grep -c proc /proc/cpuinfo`; cpu++)); do; sudo cpufreq-set -c $cpu -g performance; done. Remember to turn that off after the build, or you may risk burning your; CPU. Most modern kernels don't need that, so only use it if you have; problems. #. Running the build on SD cards is ok, but they are more prone to failures; than good quality USB sticks, and those are more prone to failures than; external hard-drives (those are also a lot faster). So, at least, you; should consider to buy a fast USB stick. On systems with a fast eMMC,; that's a good option too. #. Make sure you have a decent power supply (dozens of dollars worth) that can; provide *at least* 4 amperes, this is especially important if you use USB; devices with your board. Externally powered USB/SATA harddrives are even; better than having a good power supply.;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildOnARM.rst:2551,perform,performance,2551,interpreter/llvm-project/llvm/docs/HowToBuildOnARM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildOnARM.rst,1,['perform'],['performance']
Performance,"Otherwise, if R is atomic, and all the writes R\ :sub:`byte` may; see are atomic, it chooses one of the values written. See the :ref:`Atomic; Memory Ordering Constraints <ordering>` section for additional; constraints on how the choice is made.; - Otherwise R\ :sub:`byte` returns ``undef``. R returns the value composed of the series of bytes it read. This; implies that some bytes within the value may be ``undef`` **without**; the entire value being ``undef``. Note that this only defines the; semantics of the operation; it doesn't mean that targets will emit more; than one instruction to read the series of bytes. Note that in cases where none of the atomic intrinsics are used, this; model places only one restriction on IR transformations on top of what; is required for single-threaded execution: introducing a store to a byte; which might not otherwise be stored is not allowed in general.; (Specifically, in the case where another thread might write to and read; from an address, introducing a store can change a load that may see; exactly one write into a load that may see multiple writes.). .. _ordering:. Atomic Memory Ordering Constraints; ----------------------------------. Atomic instructions (:ref:`cmpxchg <i_cmpxchg>`,; :ref:`atomicrmw <i_atomicrmw>`, :ref:`fence <i_fence>`,; :ref:`atomic load <i_load>`, and :ref:`atomic store <i_store>`) take; ordering parameters that determine which other atomic instructions on; the same address they *synchronize with*. These semantics implement; the Java or C++ memory models; if these descriptions aren't precise; enough, check those specs (see spec references in the; :doc:`atomics guide <Atomics>`). :ref:`fence <i_fence>` instructions; treat these orderings somewhat differently since they don't take an; address. See that instruction's documentation for details. For a simpler introduction to the ordering constraints, see the; :doc:`Atomics`. ``unordered``; The set of values that can be read is governed by the happens-before; part",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:152112,load,load,152112,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['load'],['load']
Performance,"PATH``) and the header file; is fully embedded or still accessible (e.g. through a path added to; ``cppyy.add_include_path`` at run-time, or with ``-I`` to; ``rootcling``/``genreflex`` during build time).; When relocating the shared library, move the .pcm with it.; Once support for C++ modules is fully fleshed out, access to the header file; will no longer be needed. .. _`rootcling manual`: https://root.cern.ch/root/html/guides/users-guide/AddingaClass.html#the-linkdef.h-file; .. _`helper script`: https://github.com/wlav/cppyy/blob/master/test/make_dict_win32.py. Class loader; ^^^^^^^^^^^^. Explicitly loading dictionaries is fine if this is hidden under the hood of; a Python package and thus transparently done on ``import``.; Otherwise, the automatic class loader is more convenient, as it allows direct; use without having to manually find and load dictionaries (assuming these are; locatable by the dynamic loader). The class loader utilizes so-called rootmap files, which by convention should; live alongside the dictionary shared library (and C++ module file).; These are simple text files, which map C++ entities (such as classes) to the; dictionaries and other libraries that need to be loaded for their use. With ``genreflex``, the mapping file can be automatically created with; ``--rootmap-lib=MyClassDict``, where ""MyClassDict"" is the name of the shared; library (without the extension) build from the dictionary file.; With ``rootcling``, create the same mapping file with; ``-rmf MyClassDict.rootmap -rml MyClassDict``.; It is necessary to provide the final library name explicitly, since it is; only in the separate linking step where these names are fixed and those names; may not match the default choice. With the mapping file in place, the above example can be rerun without; explicit loading of the dictionary:. .. code-block:: python. >>> import cppyy; >>> from cppyy.gbl import MyClass; >>> MyClass(42).get_int(); 42; >>>. .. _cppyy-generator:. Bindings collection; -----",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst:8521,load,loader,8521,bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,1,['load'],['loader']
Performance,"PDEFoam] [MVA::TMlpANN]. [Fitter_SA] [Fitter_MC] [Fitter_Minuit] [Fitter_GA]. [DataSetFactory] [PDF] [Factory]. Configuration options for MVA method :. Configuration options reference for MVA method: HMatrix. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Configuration options for MVA method :. Configuration options reference for MVA method: Fisher. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are include",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:2353,perform,performance,2353,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performance']
Performance,"PI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-weak addresses so this:. static int a;; void bar(int b) { a = b; }; void foo(unsigned char *c) {; *c = a;; }. So that . _foo:; lis r2, ha16(_a); la r2, lo16(_a)(r2); lbz r2, 3(r2); stb r2, 0(r3); blr. Becomes. _foo:; lis r2, ha16(_a+3); lbz r2, lo16(_a+3)(r2); stb r2, 0(r3); blr. ===-------------------------------------------------------------------------===. We should compile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:2500,optimiz,optimization,2500,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,2,['optimiz'],['optimization']
Performance,"PU; ROOT_EXECUTABLE(testDataLoaderCpu TestDataLoaderCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Data-Loader-Cpu COMMAND testDataLoaderCpu). # DNN - Minimization CPU; ROOT_EXECUTABLE(testMinimizationCpu TestMinimizationCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Minimization-Cpu COMMAND testMinimizationCpu). # tests using TReference architecture; if ( reference-tests). # DNN - Activation Functions; ROOT_EXECUTABLE(testActivationFunctions TestActivationFunctions.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Activation-Functions COMMAND testActivationFunctions). # DNN - Loss Functions; ROOT_EXECUTABLE(testLossFunctions TestLossFunctions.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Loss-Functions COMMAND testLossFunctions). # DNN - Derivatives; ROOT_EXECUTABLE(testDerivatives TestDerivatives.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Derivatives COMMAND testDerivatives). # DNN - Backpropagation; ROOT_EXECUTABLE(testBackpropagation TestBackpropagation.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Backpropagation COMMAND testBackpropagation). # DNN - Backpropagation DL; ROOT_EXECUTABLE(testBackpropagationDL TestBackpropagationDL.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Backpropagation-DL COMMAND testBackpropagationDL). # DNN - Batch normalization; ROOT_EXECUTABLE(testBatchNormalization TestBatchNormalization.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-BatchNormalization COMMAND testBatchNormalization). # DNN - DataLoader; ROOT_EXECUTABLE(testDataLoader TestDataLoader.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Data-Loader COMMAND testDataLoader). # DNN - Minimization; # ROOT_EXECUTABLE(testMinimization TestMinimization.cxx LIBRARIES ${Libraries}); # # this test takes more than 20 minutes on arm in non-optimised mode; # if (NOT (CMAKE_SYSTEM_PROCESSOR STREQUAL ""aarch64"" AND CMAKE_BUILD_TYPE STREQUAL ""Debug"") ); # ROOT_ADD_TEST(TMVA-DNN-Minimization COMMAND testMinimization); # endif(); endif(); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt:8660,Load,Loader,8660,tmva/tmva/test/DNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt,1,['Load'],['Loader']
Performance,"P_{\mathrm{ext}} - a + 1)^2 - 1} \\; P_{\mathrm{ext}} &=& a - 1 + \sqrt{P_{\mathrm{int}}^2 + 1} \end{aligned}$$. Upper bound $b$:. $$\begin{aligned}; P_{\mathrm{int}} &=& \pm\sqrt{(b - P_{\mathrm{ext}} + 1)^2 - 1} \\; P_{\mathrm{ext}} &=& b + 1 - \sqrt{P_{\mathrm{int}}^2 + 1} \end{aligned}$$. The transformation of course also affects the parameter error matrix, so; M does a transformation of the error matrix (and the ""parabolic""; parameter errors) when there are parameter limits. Users should however; realize that the transformation is only a linear approximation, and that; it cannot give a meaningful result if one or more parameters is very; close to a limit, where; $\partial P_{\mathrm{ext}} / \partial P_{\mathrm{int}} \approx 0$.; Therefore, it is recommended that:. - Limits on variable parameters should be used only when needed in; order to prevent the parameter from taking on unphysical values. - When a satisfactory minimum has been found using limits, the limits; should then be removed if possible, in order to perform or; re-perform the error analysis without limits. Further discussion of the effects of parameter limits may be found in; the last chapter. ## M strategy ##. At many places in the analysis of the $\mbox{FCN}$ (the user; provided function), M must decide whether to be ""safe"" and waste a few; function calls in order to know where it is, or to be ""fast"" and attempt; to get the requested results with the fewest possible calls at a certain; risk of not obtaining the precision desired by the user. In order to; allow the user to influence these decisions, there is a M class; MnStrategy (see [api:strategy]) which the user can use to put different; settings. In the current release, this MnStrategy can be instantiated; with three different minimization quality levels for low (0), medium (1); and high (2) quality. Default settings for iteration cycles and; tolerances are initialized then. The default setting is set for medium; quality. Value 0 (low) indicates",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md:10798,perform,perform,10798,documentation/minuit2/Minuit2.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/minuit2/Minuit2.md,2,['perform'],['perform']
Performance,"Patches applied to the release branch may only be applied by the release; manager, the official release testers or the code owners with approval from; the release manager. #. Release managers are encouraged, but not required, to get approval from code; owners before approving patches. If there is no code owner or the code owner; is unreachable then release managers can ask approval from patch reviewers or; other developers active in that area. #. *Before RC1* Patches should be limited to bug fixes, important optimization; improvements, or completion of features that were started before the branch; was created. As with all phases, release managers and code owners can reject; patches that are deemed too invasive. #. *Before RC2* Patches should be limited to bug fixes or backend specific; improvements that are determined to be very safe. #. *Before RC3/Final Major Release* Patches should be limited to critical; bugs or regressions. #. *Bug fix releases* Patches should be limited to bug fixes or very safe; and critical performance improvements. Patches must maintain both API and; ABI compatibility with the previous major release. Release Final Tasks; -------------------. The final stages of the release process involves tagging the ""final"" release; branch, updating documentation that refers to the release, and updating the; demo page. Update Documentation; ^^^^^^^^^^^^^^^^^^^^. Review the documentation in the release branch and ensure that it is up; to date. The ""Release Notes"" must be updated to reflect new features, bug; fixes, new known issues, and changes in the list of supported platforms.; The ""Getting Started Guide"" should be updated to reflect the new release; version number tag available from Subversion and changes in basic system; requirements. .. _tag:. Tag the LLVM Final Release; ^^^^^^^^^^^^^^^^^^^^^^^^^^. Tag the final release sources:. ::. $ git tag -sa llvmorg-X.Y.Z; $ git push https://github.com/llvm/llvm-project.git llvmorg-X.Y.Z. Update the LLVM Website",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToReleaseLLVM.rst:13580,perform,performance,13580,interpreter/llvm-project/llvm/docs/HowToReleaseLLVM.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToReleaseLLVM.rst,1,['perform'],['performance']
Performance,"PrintValue() const { printf(""value = %d\n"", fValue); }; };; void A::SetValue(Int_t value) { // Set new value; // Emit signal ""SetValue(Int_t)"" with a single parameter; if (value != fValue) {; fValue = value;; Emit(""SetValue(Int_t)"", fValue);; }; }; // Main program; #ifdef STANDALONE; int main(int argc, char **argv) {; A* a = new A();; A* b = new A();; a->Connect(""SetValue(Int_t)"", ""A"", b, ""SetValue(Int_t)"");; printf(""n******* Test of SetValue(Int_t) signal *******n"");; b->SetValue(10);; printf(""nt***** b before ******n"");; b->PrintValue();; a->SetValue(20);; printf(""t***** b after a->SetValue(20) ******n"");; b->PrintValue();; return 0;; }; #endif; ```. ACLiC simplifies this procedure and allows the dictionary generation by:. ``` {.cpp}; root[] .L tst.C++; ```. It will create the shared library `tst_C.so.`. The next line will create an executable:. **`` g++ -otst tst.C `root-config --cflags --libs` ./tst_C.so -DSTANDALONE ``**. The library `tst_C.so` is a dynamically loaded library and should be; located in `$LD_LIBRARY_PATH`. The current working directory should be; added to `$LD_LIBRARY_PATH` via:. **`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:./`**. To run it, you just do:. **`./tst`**. ## Widgets in Detail. ### Buttons. Buttons are a popular group of widgets designed to provide specific; interfaces for user interaction. **`TGButton`** is an abstract class; defining the general button behavior: width, height, state, its group,; tool tip text, etc. There are two main groups of buttons: command buttons with a text or; graphics inside that indicate the action to be accomplished and option; buttons well known as radio and check buttons that select or change; properties. The first group is presented in ROOT by; **`TGPictureButton`** and **`TGTextButton`** classes. They yield an; action as soon as they are clicked. It can be opening/closing a dialog; box or invoking a specific function in an application. Remember the Draw; button from the example. The radio and check button",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md:53710,load,loaded,53710,documentation/users-guide/WritingGUI.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md,1,['load'],['loaded']
Performance,"Q`` loaded from ``P``. Dependency applies only to values derived directly or indirectly from; a particular expression result and does not occur merely because a; separate pointer value dynamically aliases ``P``. Furthermore, this; dependency is not carried by values that are stored to objects. .. admonition:: Rationale. The restrictions on dependency are intended to make this analysis; feasible by an optimizer with only incomplete information about a; program. Essentially, dependence is carried to ""obvious"" uses of a; pointer. Merely passing a pointer argument to a function does not; itself cause dependence, but since generally the optimizer will not; be able to prove that the function doesn't depend on that parameter,; it will be forced to conservatively assume it does. Dependency propagates to values loaded from a pointer because those; values might be invalidated by deallocating the object. For; example, given the code ``__strong id x = p->ivar;``, ARC must not; move the release of ``p`` to between the load of ``p->ivar`` and the; retain of that value for storing into ``x``. Dependency does not propagate through stores of dependent pointer; values because doing so would allow dependency to outlive the; full-expression which produced the original value. For example, the; address of an instance variable could be written to some global; location and then freely accessed during the lifetime of the local,; or a function could return an inner pointer of an object and store; it to a local. These cases would be potentially impossible to; reason about and so would basically prevent any optimizations based; on imprecise lifetime. There are also uncommon enough to make it; reasonable to require the precise-lifetime annotation if someone; really wants to rely on them. Dependency does propagate through return values of pointer type.; The compelling source of need for this rule is a property accessor; which returns an un-autoreleased result; the calling function must; have the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:81143,load,load,81143,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['load'],['load']
Performance,"R block. As; with optimization, LLVM aims to preserve the order in which variable; assignments occurred in the source program. However SelectionDAG performs some; instruction scheduling, which can reorder assignments (discussed below).; Function parameter locations are moved to the beginning of the function if; they're not already, to ensure they're immediately available on function entry. To demonstrate variable locations during instruction selection, consider; the following example:. .. code-block:: llvm. define i32 @foo(i32* %addr) {; entry:; call void @llvm.dbg.value(metadata i32 0, metadata !3, metadata !DIExpression()), !dbg !5; br label %bb1, !dbg !5. bb1: ; preds = %bb1, %entry; %bar.0 = phi i32 [ 0, %entry ], [ %add, %bb1 ]; call void @llvm.dbg.value(metadata i32 %bar.0, metadata !3, metadata !DIExpression()), !dbg !5; %addr1 = getelementptr i32, i32 *%addr, i32 1, !dbg !5; call void @llvm.dbg.value(metadata i32 *%addr1, metadata !3, metadata !DIExpression()), !dbg !5; %loaded1 = load i32, i32* %addr1, !dbg !5; %addr2 = getelementptr i32, i32 *%addr, i32 %bar.0, !dbg !5; call void @llvm.dbg.value(metadata i32 *%addr2, metadata !3, metadata !DIExpression()), !dbg !5; %loaded2 = load i32, i32* %addr2, !dbg !5; %add = add i32 %bar.0, 1, !dbg !5; call void @llvm.dbg.value(metadata i32 %add, metadata !3, metadata !DIExpression()), !dbg !5; %added = add i32 %loaded1, %loaded2; %cond = icmp ult i32 %added, %bar.0, !dbg !5; br i1 %cond, label %bb1, label %bb2, !dbg !5. bb2: ; preds = %bb1; ret i32 0, !dbg !5; }. If one compiles this IR with ``llc -o - -start-after=codegen-prepare -stop-after=expand-isel-pseudos -mtriple=x86_64--``, the following MIR is produced:. .. code-block:: text. bb.0.entry:; successors: %bb.1(0x80000000); liveins: $rdi. %2:gr64 = COPY $rdi; %3:gr32 = MOV32r0 implicit-def dead $eflags; DBG_VALUE 0, $noreg, !3, !DIExpression(), debug-location !5. bb.1.bb1:; successors: %bb.1(0x7c000000), %bb.2(0x04000000). %0:gr32 = PHI %3, %bb.0, %1, %bb.1; DBG",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:29890,load,load,29890,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['load'],['load']
Performance,"R=<dir>. Like ``-fcrash-diagnostics-dir=<dir>``, specifies where to write the; crash diagnostics files, but with lower precedence than the option. Clang is also capable of generating preprocessed source file(s) and associated; run script(s) even without a crash. This is specially useful when trying to; generate a reproducer for warnings or errors while using modules. .. option:: -gen-reproducer. Generates preprocessed source files, a reproducer script and if relevant, a; cache containing: built module pcm's and all headers needed to rebuild the; same modules. .. _rpass:. Options to Emit Optimization Reports; ------------------------------------. Optimization reports trace, at a high-level, all the major decisions; done by compiler transformations. For instance, when the inliner; decides to inline function ``foo()`` into ``bar()``, or the loop unroller; decides to unroll a loop N times, or the vectorizer decides to; vectorize a loop body. Clang offers a family of flags which the optimizers can use to emit; a diagnostic in three cases:. 1. When the pass makes a transformation (`-Rpass`). 2. When the pass fails to make a transformation (`-Rpass-missed`). 3. When the pass determines whether or not to make a transformation; (`-Rpass-analysis`). NOTE: Although the discussion below focuses on `-Rpass`, the exact; same options apply to `-Rpass-missed` and `-Rpass-analysis`. Since there are dozens of passes inside the compiler, each of these flags; take a regular expression that identifies the name of the pass which should; emit the associated diagnostic. For example, to get a report from the inliner,; compile the code with:. .. code-block:: console. $ clang -O2 -Rpass=inline code.cc -o code; code.cc:4:25: remark: foo inlined into bar [-Rpass=inline]; int bar(int j) { return foo(j, j - 2); }; ^. Note that remarks from the inliner are identified with `[-Rpass=inline]`.; To request a report from every optimization pass, you should use; `-Rpass=.*` (in fact, you can use any vali",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:23910,optimiz,optimizers,23910,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizers']
Performance,"RANTIES, INCLUDING, BUT NOT LIMITED; TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR; PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS; BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR; CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE; GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION); HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT; LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY; OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF; SUCH DAMAGE. You are under no obligation whatsoever to provide any bug fixes,; patches, or upgrades to the features, functionality or performance of; the source code (""Enhancements"") to anyone; however, if you choose to; make your Enhancements available either publicly, or directly to; Lawrence Berkeley National Laboratory, without imposing a separate; written license agreement for such Enhancements, then you hereby grant; the following license: a non-exclusive, royalty-free perpetual license; to install, use, modify, prepare derivative works, incorporate into; other computer software, distribute, and sublicense such Enhancements; or derivative works thereof, in binary and source code form. Additional copyright holders; ----------------------------. In addition to LBNL/UC Berkeley, this package contains files copyrighted by; one or more of the following people and organizations, and licensed under; the same conditions (except for some compatible licenses as retained in the; source code):. * CERN; * Lucio Asnaghi; * Simone Bacchio; * Robert Bradshaw; * Ellis Breen; * Antonio Cuni; * Aditi Dutta; * Shaheed Haque; * Jonsomi; * Max Kolin; * Alvaro Moran; * Tarmo Pikaro; * Matti Picus; * Camille Scott; * Toby StClere-Smithe; * Stefan Wunsch. Conda-forge recipes were provided by Julian Rueth and Isuru Fernando. External code; -------------.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/license.rst:1901,perform,performance,1901,bindings/pyroot/cppyy/cppyy/doc/source/license.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/license.rst,1,['perform'],['performance']
Performance,"RANTIES, INCLUDING, BUT NOT LIMITED; TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR; PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS; BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR; CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE; GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION); HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT; LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY; OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF; SUCH DAMAGE. You are under no obligation whatsoever to provide any bug fixes,; patches, or upgrades to the features, functionality or performance of; the source code (""Enhancements"") to anyone; however, if you choose to; make your Enhancements available either publicly, or directly to; Lawrence Berkeley National Laboratory, without imposing a separate; written license agreement for such Enhancements, then you hereby grant; the following license: a non-exclusive, royalty-free perpetual license; to install, use, modify, prepare derivative works, incorporate into; other computer software, distribute, and sublicense such Enhancements; or derivative works thereof, in binary and source code form. Additional copyright holders; ----------------------------. In addition to LBNL/UC Berkeley, this package contains files copyrighted by; one or more of the following people and organizations, and licensed under; the same conditions (except for some compatible licenses as retained in the; source code):. CERN; Simone Bacchio; Robert Bradshaw; Ellis Breen; Antonio Cuni; Aditi Dutta; Shaheed Haque; Jonsomi; Alvaro Moran; Tarmo Pikaro; Matti Picus; Camille Scott. External code; -------------. The create_src_directory.py script will pull in ROOT and LLVM sources, which; are licensed differently:. LLVM: distributed under University of Illinois/NCSA Open S",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/LICENSE.txt:1855,perform,performance,1855,bindings/pyroot/cppyy/cppyy-backend/cling/LICENSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/cling/LICENSE.txt,1,['perform'],['performance']
Performance,RARIES(testArithmeticCuda ${Libraries} ); ROOT_ADD_TEST(TMVA-DNN-ArithmeticCuda COMMAND testArithmeticCuda). # DNN - DataLoader Cuda; add_executable(testDataLoaderCuda TestDataLoaderCuda.cxx); TARGET_LINK_LIBRARIES(testDataLoaderCuda ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-DataLoaderCuda COMMAND testDataLoaderCuda). # DNN - Optimization GPU. add_executable(testOptimizationCuda TestOptimizationCuda.cxx); TARGET_LINK_LIBRARIES(testOptimizationCuda ${Libraries} ); ROOT_ADD_TEST(TMVA-DNN-Optimization-Cuda COMMAND testOptimizationCuda). #Cuda tests using CUDNN; if (tmva-cudnn). # DNN - Batch normalization Cudnn; add_executable(testBatchNormalizationCudnn TestBatchNormalizationCudnn.cxx ); TARGET_LINK_LIBRARIES(testBatchNormalizationCudnn ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-BatchNormalization-Cudnn COMMAND testBatchNormalizationCudnn). # DNN Optimization GPU Cudnn. add_executable(testOptimizationCudnn TestOptimizationCudnn.cxx); TARGET_LINK_LIBRARIES(testOptimizationCudnn ${Libraries} ); ROOT_ADD_TEST(TMVA-DNN-Optimization-Cudnn COMMAND testOptimizationCudnn). # DNN - TensorDataLoader Cudnn; #add_executable(testTensorDataLoaderCudnn TestTensorDataLoaderCudnn.cxx); #TARGET_LINK_LIBRARIES(testTensorDataLoaderCudnn ${Libraries} ${DNN_CUDA_LIBRARIES}); #ROOT_ADD_TEST(TMVA-DNN-TensorDataLoaderCudnn COMMAND testTensorDataLoaderCudnn). endif(). endif (). #--- CPU tests. ----------------------------; #; # always run the Cpu tests. If tmva-cpu is off (no Blas or no imt); # they will work using TMatrix operations. # DNN - Arithmetic Functions CPU; ROOT_EXECUTABLE(testArithmeticCpu TestMatrixArithmeticCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Arithmetic-Cpu COMMAND testArithmeticCpu). # DNN - Activation Functions CPU; ROOT_EXECUTABLE(testActivationFunctionsCpu TestActivationFunctionsCpu.cxx LIBRARIES ${Libraries}); ROOT_ADD_TEST(TMVA-DNN-Activation-Functions-Cpu COMMAND testActivationFunctionsCpu). # DNN - Loss Functions CPU; ROOT_EXECUTABLE(testLossFunctionsCpu TestLos,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt:3483,Optimiz,Optimization-Cudnn,3483,tmva/tmva/test/DNN/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/CMakeLists.txt,1,['Optimiz'],['Optimization-Cudnn']
Performance,"RE MATCHES s390); set(ROOT_CLING_TARGET ""SystemZ""); elseif(ROOT_ARCHITECTURE MATCHES riscv64); set(ROOT_CLING_TARGET ""RISCV""); elseif(ROOT_ARCHITECTURE MATCHES freebsd OR ROOT_ARCHITECTURE MATCHES linux OR ROOT_ARCHITECTURE MATCHES macosx); set(ROOT_CLING_TARGET ""X86""); elseif(ROOT_ARCHITECTURE MATCHES win32 OR ROOT_ARCHITECTURE MATCHES win64); set(ROOT_CLING_TARGET ""X86""); else(); set(ROOT_CLING_TARGET ""all""); endif(). if(NOT ""${ROOT_CLING_TARGET}"" STREQUAL ""all""); string(APPEND ROOT_CLING_TARGET "";NVPTX""); endif(). if(MSVC); # FIXME: since Visual Studio v16.4.0 the /O2 optimization flag make many (25%) of the tests failing; # Try to re-enable /O2 after the upgrade of llvm & clang; if (MSVC_VERSION GREATER_EQUAL 1924 AND MSVC_VERSION LESS 1929); string(REPLACE ""-O2"" ""-O1 -Oi"" CMAKE_CXX_FLAGS_RELEASE ""${CMAKE_CXX_FLAGS_RELEASE}""); string(REPLACE ""-O2"" ""-O1 -Oi"" CMAKE_CXX_FLAGS_RELWITHDEBINFO ""${CMAKE_CXX_FLAGS_RELWITHDEBINFO}""); endif(); # replace dashes in the -EH* and -GR* flags with slashes (/EH* /GR*); string(REPLACE "" -EH"" "" /EH"" CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS}""); string(REPLACE "" -GR"" "" /GR"" CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS}""); set(CMAKE_EXE_LINKER_FLAGS ""${CMAKE_EXE_LINKER_FLAGS} /ignore:4049,4206,4217,4221""); set(CMAKE_SHARED_LINKER_FLAGS ""${CMAKE_SHARED_LINKER_FLAGS} /ignore:4049,4206,4217,4221""); set(CMAKE_MODULE_LINKER_FLAGS ""${CMAKE_MODULE_LINKER_FLAGS} /ignore:4049,4206,4217,4221""); endif(). set(LLVM_TARGETS_TO_BUILD ${ROOT_CLING_TARGET} CACHE STRING ""Semicolon-separated list of targets to build, or \""all\"".""). if(clingtest); message(""-- cling test suite enabled: llvm / clang symbols in libCling will be visible!""); set(CLING_INCLUDE_TESTS ON CACHE BOOL """" FORCE); # The path to cling passed through environment variable only relevant when building; # against external llvm. In that case, cling's testsuite cannot deduce the binary; # of cling relatively to the llvm tools folder.; if (NOT builtin_llvm); set(CLINGTEST_EXECUTABLE CLING=${CMAKE_CURRENT_BIN",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/CMakeLists.txt:5390,optimiz,optimization,5390,interpreter/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/CMakeLists.txt,1,['optimiz'],['optimization']
Performance,"REDUCE_SEQ_FADD, G_VECREDUCE_SEQ_FMUL; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The SEQ variants perform reductions in sequential order. The first operand is; an initial scalar accumulator value, and the second operand is the vector to reduce. G_VECREDUCE_FADD, G_VECREDUCE_FMUL; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. These reductions are relaxed variants which may reduce the elements in any order. G_VECREDUCE_FMAX, G_VECREDUCE_FMIN, G_VECREDUCE_FMAXIMUM, G_VECREDUCE_FMINIMUM; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. FMIN/FMAX/FMINIMUM/FMAXIMUM nodes can have flags, for NaN/NoNaN variants. Integer/bitwise reductions; ^^^^^^^^^^^^^^^^^^^^^^^^^^. * G_VECREDUCE_ADD; * G_VECREDUCE_MUL; * G_VECREDUCE_AND; * G_VECREDUCE_OR; * G_VECREDUCE_XOR; * G_VECREDUCE_SMAX; * G_VECREDUCE_SMIN; * G_VECREDUCE_UMAX; * G_VECREDUCE_UMIN. Integer reductions may have a result type larger than the vector element type.; However, the reduction is performed using the vector element type and the value; in the top bits is unspecified. Memory Operations; -----------------. G_LOAD, G_SEXTLOAD, G_ZEXTLOAD; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Generic load. Expects a MachineMemOperand in addition to explicit; operands. If the result size is larger than the memory size, the; high bits are undefined, sign-extended, or zero-extended respectively. Only G_LOAD is valid if the result is a vector type. If the result is larger; than the memory size, the high elements are undefined (i.e. this is not a; per-element, vector anyextload). Unlike in SelectionDAG, atomic loads are expressed with the same; opcodes as regular loads. G_LOAD, G_SEXTLOAD and G_ZEXTLOAD may all; have atomic memory operands. G_INDEXED_LOAD; ^^^^^^^^^^^^^^. Generic indexed load. Combines a GEP with a load. $newaddr is set to $base + $offset.; If $am is 0 (post-indexed), then the value is loaded from $base; if $am is 1 (pre-indexed); then the value is loaded from $newaddr. G_INDEXED_SEXTLOAD; ^^^^^^^^^^^^^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst:15311,perform,performed,15311,interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/GenericOpcode.rst,1,['perform'],['performed']
Performance,"RING; The path to install auxiliary LLVM utilities, relative to the *CMAKE_INSTALL_PREFIX*.; Only matters if *LLVM_INSTALL_UTILS* is enabled.; Defaults to *LLVM_TOOLS_INSTALL_DIR*. **LLVM_EXAMPLES_INSTALL_DIR**:STRING; The path for examples of using LLVM, relative to the *CMAKE_INSTALL_PREFIX*.; Only matters if *LLVM_BUILD_EXAMPLES* is enabled.; Defaults to ""examples"". CMake Caches; ============. Recently LLVM and Clang have been adding some more complicated build system; features. Utilizing these new features often involves a complicated chain of; CMake variables passed on the command line. Clang provides a collection of CMake; cache scripts to make these features more approachable. CMake cache files are utilized using CMake's -C flag:. .. code-block:: console. $ cmake -C <path to cache file> <path to sources>. CMake cache scripts are processed in an isolated scope, only cached variables; remain set when the main configuration runs. CMake cached variables do not reset; variables that are already set unless the FORCE option is specified. A few notes about CMake Caches:. - Order of command line arguments is important. - -D arguments specified before -C are set before the cache is processed and; can be read inside the cache file; - -D arguments specified after -C are set after the cache is processed and; are unset inside the cache file. - All -D arguments will override cache file settings; - CMAKE_TOOLCHAIN_FILE is evaluated after both the cache file and the command; line arguments; - It is recommended that all -D options should be specified *before* -C. For more information about some of the advanced build configurations supported; via Cache files see :doc:`AdvancedBuilds`. Executing the Tests; ===================. Testing is performed when the *check-all* target is built. For instance, if you are; using Makefiles, execute this command in the root of your build directory:. .. code-block:: console. $ make check-all. On Visual Studio, you may run tests by building the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst:39111,cache,cached,39111,interpreter/llvm-project/llvm/docs/CMake.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMake.rst,1,['cache'],['cached']
Performance,"RNTuple.; It contains a list of `RValue` objects that correspond to the top-level fields of the originating model.; The entry gives access to the shared pointers corresponding to the top-level fields.; It also provides functionality to bind application-provided pointers. An REntry can be passed to `RNTupleWriter::Fill()` and `RNTupleReader::LoadEntry()`.; Otherwise, the reader/writer uses the default entry of its model. An entry can safely outlive its originating model.; New objects cannot anymore be created (`EmplaceNewValue` will throw an exception), but the entry is still properly destructed. ### RNTupleWriter, RNTupleParallelWriter; The RNTupleWriter is the primary interface to create an RNTuple.; The writer takes ownership of a given model.; The writer can either add an RNTuple to an existing ROOT file (`RNTupleWriter::Append()`) or create a new ROOT file with an RNTuple (`RNTupleWriter::Recreate()`).; Once created, entries are added to an RNTuple either serially (`RNTupleWriter::Fill()`) or in concurrently in multiple threads with the `RNTupleParallelWriter`.; Once committed (e.g. by releasing the RNTupleWriter), the RNTuple is immutable and cannot be amended.; An RNTuple that is currently being written cannot be read. ### RNTupleReader; The RNTupleReader is the primary interface to read and inspect an RNTuple.; An RNTupleReader owns a model: either a model created from the on-disk information or an imposed, user-provided model.; The user-provided model can be limited to a subset of fields.; Data is populated to an explicit `REntry` or the model's default entry through `RNTupleReader::LoadEntry()`. The reader can create `RNTupleView` objects for the independent reading of individual fields.; The reader can create `RBulk` objects for bulk reading of individual fields. Additionally, the reader provides access to a cached copy of the descriptor.; It can display individual entries (`RNTupleReader::Show()`) and summary information (`RNTupleReader::PrintInfo()`). ##",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md:12488,concurren,concurrently,12488,tree/ntuple/v7/doc/Architecture.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/doc/Architecture.md,1,['concurren'],['concurrently']
Performance,"RNTupleParallelWriter` class has been added, providing (initial) support for parallel writing of RNTuples.; - A new static method `RFieldBase::Check()` has been added, which produces a support status report of a type with regards to RNTuple I/O.; - A new internal `RNTupleMerger` class has been added, enabling the merging of different page sources into one page sink. This also means that RNTuples can be merged through `hadd`.; - Zero-copy bulk reading has been added, with extra optimizations for `ROOT::RVec` fields.; - It is now possible to use the `RNTupleView` with an external address with type erasure, e.g.:; ```cpp; std::shared_ptr<void> data{new float()};; auto view = reader->GetView(""pt"", data);; ```; This enables use cases such as reading one specific entry of one specific field into a previously allocated memory location.; - Further integration with [RDataFrame](#rdataframe): it is now possible to create RDataFrame for chains of RNTuples. This addition also comes with improvements to the multi-threaded work scheduling.; - Many additional bug fixes and improvements. Please, report any issues regarding the above mentioned features should you encounter them. RNTuple is still in pre-production. The on-disk format is scheduled to be finalized by the end of 2024. Thus, we appreciate feedback and suggestions for improvement. ## Histogram Libraries. - Implement the FLT_MAX mechanism for `THStack::GetMaximum()` and `THStack::GetMiniumum()`.; - Print a warning when the range given to `TAxis::SetRange` is invalid.; - Fix projection name in `TH3` as requested [here](https://root-forum.cern.ch/t/project3d-letter-d-in-name-option/57612). ## Parallelism; - The ROOT::Experimental::TFuture template has been removed. ## RooFit Libraries. ### New CPU likelihood evaluation backend by default. The new vectorizing CPU evaluation backend is not the default for RooFit likelihoods.; Likelihood minimization is now up to 10x faster on a single CPU core. If you experience unexpected pro",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md:6262,multi-thread,multi-threaded,6262,README/ReleaseNotes/v632/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md,1,['multi-thread'],['multi-threaded']
Performance,"ROOT (and thus not; interfere with the system's shared linker).; The final ""Dynamic Path"" is now composed of these sources in order:; 1. `ROOT_LIBRARY_PATH` environment variable; 2. System specific shared linker environment variables like; `LD_LIBRARY_PATH`, `LIBPATH`, or `PATH`.; 3. Setting from rootrc; 4. ROOT's builtin library directory. ### Interpreter. - cling's LLVM is upgraded to version 9.0; - New interface to enable/disable optional cling features. Currently, it can be used to enable/disable support for redefinitions. See [this](https://github.com/root-project/cling/issues/360) issue for more information. ### Multithreading. - Fix an uninitialized variable in global read-write lock which could have caused deadlocks or crashes in some rare cases.; - Default global read-write lock transitioned to new implementation based on TBB thread local storage when TBB is available on supported platforms (all except Windows). This gives an O(10%) performance improvement for some typical RDataFrame scenarios with 256 threads due to reduced lock contention. ## I/O Libraries. - Exclusive use of the global lock is reduced or migrated to finer grained read and write locks in a few hotspots that occur during file opening/closing or task initialization in RDataFrame. This can lead to O(100x) improvements for some typical RDataFrame scenarios with 256 threads due to massively reduced lock contention. ## TTree Libraries. - `TTree` now supports the inclusion of leaves of types `long` and `unsigned long` (and therefore also `std::size_t` on most systems) also for branches in ""leaflist mode"". The corresponding leaflist letters are 'G' and 'g'.; - when looping over a `TTree` with a friend with a larger number of entries, `TTreeReader` now ends the event loop when the entries in the _main_ `TTree` are exhausted, consistently with other interfaces. See [#6518](https://github.com/root-project/root/issues/6518) for more details.; - `TTreeProcessorMT::SetMaxTasksPerFilePerWorker` is now d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md:4166,perform,performance,4166,README/ReleaseNotes/v624/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v624/index.md,1,['perform'],['performance']
Performance,"ROOT project allows:; - reading of binary and JSON ROOT files in JavaScript;; - drawing of different ROOT classes in web browsers;; - reading and drawing TTree data;; - using in node.js. ## Installing JSROOT. In most practical cases it is not necessary to install JSROOT - it can be used directly from project web sites <https://root.cern/js/> and <https://jsroot.gsi.de/>. When required, there are following alternatives to install JSROOT on other web servers:. - download and unpack [provided](https://github.com/root-project/jsroot/releases) packages (recommended); - use [npm](https://npmjs.com/package/jsroot) package manager and invoke `npm install jsroot`; - clone master branch from [repository](https://github.com/root-project/jsroot/). ## Drawing objects in JSROOT. [The main page](https://root.cern/js/latest/) of the JSROOT project provides the possibility to interactively open ROOT files and draw objects like histogram or canvas. To automate files loading and objects drawing, one can provide number of URL parameters in address string like:. - file - name of the file, which will be automatically open with page loading; - files - array of file names for loading; - json - name of JSON file with stored ROOT object like histogram or canvas; - item - item name to be displayed; - opt - drawing option for the item; - items - array of items name to be displayed; - opts - array of drawing options for the items; - expand - item name(s) to be expanded in the hierarchy browser; - focus - item name to be focused on in the hierarchy browser; - title - set browser title; - dir - list files in directory on http server, see https://github.com/root-project/jsroot/issues/283; - layout - can be 'simple', 'flex', 'tabs', 'gridNxM', 'horizNMK', 'vertNMK'; - browser - layout of the browser 'fix' (default), 'float', 'no' (hidden), 'off' (fully disabled); - nobrowser - do not display file browser (same as browser=no); - float - display floating browser (same as browser=float); - status - con",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md:988,load,loading,988,documentation/JSROOT/JSROOT.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md,1,['load'],['loading']
Performance,"ROOT-HEAD fails with ""cling interactive line includer >>>: fatal error: module file '[snip]/Vc.pcm' not found: module file not found""; * [[#14958](https://github.com/root-project/root/issues/14958)] - ROOT_HEAD failed with error message: Fail to detect cryptographic random generator; * [[#14921](https://github.com/root-project/root/issues/14921)] - ROOT Fails to build macOS 14.4 arm64 Xcode 15.3; * [[#14914](https://github.com/root-project/root/issues/14914)] - VecOps::Take with default argument doesn't check correctly the out of boundary condition; * [[#14910](https://github.com/root-project/root/issues/14910)] - hadd issue when using parallelization together with indirect file; * [[#14902](https://github.com/root-project/root/issues/14902)] - compilation error; * [[#14863](https://github.com/root-project/root/issues/14863)] - [hist] TH1::SaveAs missing default option argument causes compilation errors; * [[#14855](https://github.com/root-project/root/issues/14855)] - TRatioPlot crashes if loaded from the file; * [[#14842](https://github.com/root-project/root/issues/14842)] - TRatioplot gives ""different"" results with Web Graphics; * [[#14838](https://github.com/root-project/root/issues/14838)] - Problems with Confidence Band of TRatioPlot; * [[#14801](https://github.com/root-project/root/issues/14801)] - TEfficiency drawing to .C is broken; * [[#14793](https://github.com/root-project/root/issues/14793)] - 6.26 cannot read file written with 6.30.4; * [[#14772](https://github.com/root-project/root/issues/14772)] - [cling] Undocumented behaviour of root macro_C.so; * [[#14767](https://github.com/root-project/root/issues/14767)] - rootn.exe instant crash on startup; * [[#14765](https://github.com/root-project/root/issues/14765)] - TGenPhaseSpace weight normalization; * [[#14748](https://github.com/root-project/root/issues/14748)] - [ntuple] `Show` and `PrintInfo` does not work for friend RNTuples; * [[#14601](https://github.com/root-project/root/issues/14601)] - std::s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md:28604,load,loaded,28604,README/ReleaseNotes/v632/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v632/index.md,1,['load'],['loaded']
Performance,"ROOTQL; ======. This is a Quick Look plugin that allows quick inspection of the content; of ROOT files. Quick Look is available on MacOS X since version 10.5 (Leopard). To use QL; select a file icon in the Finder and hit the space bar. For all file types; supported by QL you will get a window showing the file content, for file types; not supported you will get a generic window showing some basic file info. The idea of QL is that file content can be shown without the heavy application; startup process. Generating a QL view of a ROOT file depends on the size of the; file and number of keys, but generally it is a quick operation. Get the binary for the ROOTQL plugin from:. ftp://root.cern/root/ROOTQL.tgz. To install the plugin, after untarring the above file, just drag the; ROOTQL.qlgenerator icon to either /Library/QuickLook or to ~/Library/QuickLook.; You may have to create that folder if it does not exist. Once installed; you may have to refresh the QL plugin cache by executing:; /usr/bin/qlmanage -r. To build from source, get it from git using:. git clone http://root.cern/git/root.git root; cd root/misc/rootql. Open the ROOTQL project in Xcode and click on ""Build"" (make sure the Active; Build Configuration is set to ""Release""). A command line short cut to open; the Xcode project is to type ""open ROOTQL.xcodeproj"" in the Terminal app.; Move the resulting plugin from the build/Release directory to either; the /Library/QuickLook or ~/Library/QuickLook directory. Cheers, Fons.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/misc/rootql/ReadMe.txt:974,cache,cache,974,misc/rootql/ReadMe.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/misc/rootql/ReadMe.txt,2,['cache'],['cache']
Performance,"RTLD_LAZY()`.; - `SetFCN(void*)` from TVirtualFitter, TFitter, TBackCompFitter, TMinuit; - `TFoam::SetRhoInt(void*)`. ### Core. - The enum constant `TRef::kNotComputed`, `TLink::kObjIsParent` were never used and have been removed.; - The enum constant `TClonesArray::kNoSplit` has not been used since v2.26 and has been removed. ## Interpreter. - Automatic declaration of variables (`h = new TH1F(...)`) is *only* available at the prompt. The side-effects of relying on this in source files is simply too grave. Due to a bug (ROOT-8538), automatically declared variables must currently reside on the top-most scope, i.e. not inside an `if` block etc.; - Improved the stack frame information generated by the JIT. By avoiding interleaving of the memory associated to multiple JIT module, the generation of stack trace involving jitted code and the catching of exception going through jitted code has been repaired.; - Interpreted code is now optimized; `.O 0/1/2/3` can be used to change the optimization level, as well as `#pragma cling optimize`.; - The prompt colors are now much more visible, both on terminals with light and dark background.; - Significant speedup of `TMethodCall`.; - One can now run `.x 12file-with@funny=name.C`; it will expect a function called `_12file_with_funny_name()`. ## Core Libraries. - See ""Build, Configuration and Testing Infrastructure"" below for changes in the directory structure.; - libCling now exports only a minimal set of symbols.; - Add support for std::array_view also for C++11 builds. The implementation has been modified to work before C++14.; - Added TCollection::Notify to allow notifying more than one object.; ```{.cpp}; TList formulas;; // Add several TTreeFormula to the list;; chain.SetNotify(&formulas);; ```; - For classes that need the `ClassDef` support, `ClassDefInline(ClassName, Version)` now provides it without the need for a dictionary source: all members injected by this `ClassDef` flavor are generated by the interpreter. ### `TObjS",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v610/index.md:1959,optimiz,optimization,1959,README/ReleaseNotes/v610/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v610/index.md,2,['optimiz'],"['optimization', 'optimize']"
Performance,"Range = NSMakeRange(0, [s1 length]);; return [s1 compare:s2 options:comparisonOptions; range:string1Range locale:currentLocale];; }];; NSLog(@""sorted: %@"", sorted);. This code relies on an implicit conversion from the type of the lambda; expression (an unnamed, local class type called the *closure type*) to the; corresponding block pointer type. The conversion itself is expressed by a; conversion operator in that closure type that produces a block pointer with the; same signature as the lambda itself, e.g.,. .. code-block:: objc. operator NSComparisonResult (^)(id, id)() const;. This conversion function returns a new block that simply forwards the two; parameters to the lambda object (which it captures by copy), then returns the; result. The returned block is first copied (with ``Block_copy``) and then; autoreleased. As an optimization, if a lambda expression is immediately; converted to a block pointer (as in the first example, above), then the block; is not copied and autoreleased: rather, it is given the same lifetime as a; block literal written at that point in the program, which avoids the overhead; of copying a block to the heap in the common case. The conversion from a lambda to a block pointer is only available in; Objective-C++, and not in C++ with blocks, due to its use of Objective-C memory; management (autorelease). Object Literals and Subscripting; --------------------------------. Clang provides support for :doc:`Object Literals and Subscripting; <ObjectiveCLiterals>` in Objective-C, which simplifies common Objective-C; programming patterns, makes programs more concise, and improves the safety of; container creation. There are several feature macros associated with object; literals and subscripting: ``__has_feature(objc_array_literals)`` tests the; availability of array literals; ``__has_feature(objc_dictionary_literals)``; tests the availability of dictionary literals;; ``__has_feature(objc_subscripting)`` tests the availability of object; subscriptin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:76372,optimiz,optimization,76372,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimization']
Performance,"Reorganize 3D drawing of TH2/TH3 histograms, allow to mix 2D and 3D display together; 8. Support overlay of 3D graphic over SVG canvas (used for IE); 9. Fix problems and improve flex(ible) layout. ## Changes in 4.0; 1. New TGeo classes support:; - browsing through volumes hierarchy; - changing visibility flags; - drawing of selected volumes; 2. New 'flex' layout:; - create frames like in Multi Document Interface; - one could move/resize/minimize/maximize such frames; 3. Significant (factor 4) I/O performance improvement:; - use ArrayBuffer class in HTTP requests instead of String; - use native arrays (like Int32Array) for array data members; - highly optimize streamer infos handling; 4. TH2 drawing optimization:; - if there are too many non-empty bins, combine them together; - when zoom-in, all original bins will be displayed separately; - let draw big TH2 histogram faster than in 1 sec; - optimization can be disabled by providing '&optimize=0' in URL; 5. TF1 drawing optimization:; - function 'compiled' only once; 6. Reorganize scripts structure:; - move all math functions to JSRootMath.js; - TH2, TF1, THStack and TMultiGraph painters moved into JSRootPainter.more.js script; - reduce size of scripts required for default functionality; 7. Update all basic libraries:; - d3.js - v3.5.9,; - jquery.js - v2.1.4,; - jquery-ui.js - v1.11.4,; - three.js - r73; 8. Implement ROOT6-like color palettes:; - all palettes in range 51...112 are implemented; - by default palette 57 is used; - one could change default palette with '&palette=111' in URL; - or palette can be specified in draw option like '&opt=colz,pal77'. ## Changes in 3.9; 1. Support non-equidistant bins for TH1/TH2 objects.; 2. Display entries count from histo.fEntries member, only when not set use computed value; 3. Support italic and bold text when used with MathJax; 4. Improve TF1 drawing - support exp function in TFormula, fix errors with logx scale, enable zoom-in, (re)calculate function points when zooming; 5. S",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:60220,optimiz,optimization,60220,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['optimiz'],['optimization']
Performance,"RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the function. The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once again:. .. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass());; ... It is interesting to see what the code looks like before and after the; mem2reg optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. Here there is only one variable (x, the input argument) but you can; still see the extremely simple-minded code generation strategy we are; using. In the entry block, an alloca is created, and the initial input; value is stored into it. Each reference to the variable does a reload; from the s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:17162,load,load,17162,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['load'],['load']
Performance,"S bits is retrieved from the location storage LS specified by; one of the single location descriptions SL of L. *If L, or the location description of any composite location description; part that is a subcomponent of L, has more than one single location; description, then any one of them can be selected as they are required to; all have the same value. For any single location description SL, bits are; retrieved from the associated storage location starting at the bit offset; specified by SL. For a composite location description, the retrieved bits; are the concatenation of the N bits from each composite location part PL,; where N is limited to the size of PL.*. V is pushed on the stack with the type T. .. note::. This definition makes it an evaluation error if L is a register location; description that has less than TS bits remaining in the register storage.; Particularly since these extensions extend location descriptions to have; a bit offset, it would be odd to define this as performing sign extension; based on the type, or be target architecture dependent, as the number of; remaining bits could be any number. This matches the GDB implementation; for ``DW_OP_deref_type``. These extensions define ``DW_OP_*breg*`` in terms of; ``DW_OP_regval_type``. ``DW_OP_regval_type`` is defined in terms of; ``DW_OP_regx``, which uses a 0 bit offset, and ``DW_OP_deref_type``.; Therefore, it requires the register size to be greater or equal to the; address size of the address space. This matches the GDB implementation for; ``DW_OP_*breg*``. The DWARF is ill-formed if D is not in the current compilation unit, D is; not a ``DW_TAG_base_type`` debugging information entry, or if TS divided by; 8 (the byte size) and rounded up to a whole number is not equal to S. .. note::. This definition allows the base type to be a bit size since there seems no; reason to restrict it. It is an evaluation error if any bit of the value is retrieved from the; undefined location storage or the offset o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:91838,perform,performing,91838,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['performing']
Performance,"SCV-LMUL M1; vadd.vv v2, v2, v2; vsetvli zero, a0, e8, m8, tu, mu; # LLVM-MCA-RISCV-LMUL M8; vadd.vv v2, v2, v2. Example of program with call to `vsetvl`:. .. code-block:: none. vsetvl rd, rs1, rs2; # LLVM-MCA-RISCV-LMUL M1; vadd.vv v12, v12, v12; vsetvl rd, rs1, rs2; # LLVM-MCA-RISCV-LMUL M4; vadd.vv v12, v12, v12. HOW LLVM-MCA WORKS; ------------------. :program:`llvm-mca` takes assembly code as input. The assembly code is parsed; into a sequence of MCInst with the help of the existing LLVM target assembly; parsers. The parsed sequence of MCInst is then analyzed by a ``Pipeline`` module; to generate a performance report. The Pipeline module simulates the execution of the machine code sequence in a; loop of iterations (default is 100). During this process, the pipeline collects; a number of execution related statistics. At the end of this process, the; pipeline generates and prints a report from the collected statistics. Here is an example of a performance report generated by the tool for a; dot-product of two packed float vectors of four elements. The analysis is; conducted for target x86, cpu btver2. The following result can be produced via; the following command using the example located at; ``test/tools/llvm-mca/X86/BtVer2/dot-product.s``:. .. code-block:: bash. $ llvm-mca -mtriple=x86_64-unknown-unknown -mcpu=btver2 -iterations=300 dot-product.s. .. code-block:: none. Iterations: 300; Instructions: 900; Total Cycles: 610; Total uOps: 900. Dispatch Width: 2; uOps Per Cycle: 1.48; IPC: 1.48; Block RThroughput: 2.0. Instruction Info:; [1]: #uOps; [2]: Latency; [3]: RThroughput; [4]: MayLoad; [5]: MayStore; [6]: HasSideEffects (U). [1] [2] [3] [4] [5] [6] Instructions:; 1 2 1.00 vmulps	%xmm0, %xmm1, %xmm2; 1 3 1.00 vhaddps	%xmm2, %xmm2, %xmm3; 1 3 1.00 vhaddps	%xmm3, %xmm3, %xmm4. Resources:; [0] - JALU0; [1] - JALU1; [2] - JDiv; [3] - JFPA; [4] - JFPM; [5] - JFPU0; [6] - JFPU1; [7] - JLAGU; [8] - JMul; [9] - JSAGU; [10] - JSTC; [11] - JVALU0; [12] - JVALU1; [13] ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:13911,perform,performance,13911,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['perform'],['performance']
Performance,"SRC_DIR}); set(LLVM_OBJ_ROOT ${LLVM_BINARY_DIR}); set(LLVM_CPPFLAGS ""${LLVM_DEFINITIONS}""); set(LLVM_CFLAGS ""${LLVM_C_STD_FLAG} ${LLVM_DEFINITIONS}""); # The language standard potentially affects the ABI/API of LLVM, so we want; # to make sure it is reported by llvm-config.; set(LLVM_CXXFLAGS ""${CMAKE_CXX${CMAKE_CXX_STANDARD}_STANDARD_COMPILE_OPTION} ${LLVM_CXX_STDLIB_FLAG} ${COMPILE_FLAGS} ${LLVM_DEFINITIONS}""); set(LLVM_BUILD_SYSTEM cmake); set(LLVM_HAS_RTTI ${LLVM_CONFIG_HAS_RTTI}); set(LLVM_DYLIB_VERSION ""${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX}""). # Use the C++ link flags, since they should be a superset of C link flags.; set(LLVM_LDFLAGS ""${CMAKE_CXX_LINK_FLAGS}""); set(LLVM_BUILDMODE ${CMAKE_BUILD_TYPE}); set(LLVM_SYSTEM_LIBS ${SYSTEM_LIBS}); string(REPLACE "";"" "" "" LLVM_TARGETS_BUILT ""${LLVM_TARGETS_TO_BUILD}""); llvm_canonicalize_cmake_booleans(; LLVM_BUILD_LLVM_DYLIB; LLVM_LINK_LLVM_DYLIB; LLVM_HAS_RTTI; BUILD_SHARED_LIBS); llvm_expand_pseudo_components(LLVM_DYLIB_COMPONENTS_expanded ""${LLVM_DYLIB_COMPONENTS}""); configure_file(${BUILDVARIABLES_SRCPATH} ${BUILDVARIABLES_OBJPATH} @ONLY). # Set build-time environment(s).; add_compile_definitions(CMAKE_CFG_INTDIR=""$<CONFIG>""). if(LLVM_ENABLE_MODULES); target_compile_options(llvm-config PUBLIC; ""-fmodules-ignore-macro=CMAKE_CFG_INTDIR""; ); endif(). # Add the dependency on the generation step.; add_file_dependencies(${CMAKE_CURRENT_SOURCE_DIR}/llvm-config.cpp ${BUILDVARIABLES_OBJPATH}). if(CMAKE_CROSSCOMPILING); if (LLVM_NATIVE_TOOL_DIR AND NOT LLVM_CONFIG_PATH); if (EXISTS ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); set(LLVM_CONFIG_PATH ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); endif(); endif(). if (NOT LLVM_CONFIG_PATH); build_native_tool(llvm-config LLVM_CONFIG_PATH); set(LLVM_CONFIG_PATH ""${LLVM_CONFIG_PATH}"" CACHE STRING """"). add_custom_target(NativeLLVMConfig DEPENDS ${LLVM_CONFIG_PATH}); add_dependencies(llvm-config NativeLLVMConfig); endif(); endif(); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:3893,CACHE,CACHE,3893,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,1,['CACHE'],['CACHE']
Performance,"SSA; graph representing the function. Arguments:; """""""""""""""""""". The type of the incoming values is specified with the first type field.; After this, the '``phi``' instruction takes a list of pairs as; arguments, with one pair for each predecessor basic block of the current; block. Only values of :ref:`first class <t_firstclass>` type may be used as; the value arguments to the PHI node. Only labels may be used as the; label arguments. There must be no non-phi instructions between the start of a basic block; and the PHI instructions: i.e. PHI instructions must be first in a basic; block. For the purposes of the SSA form, the use of each incoming value is; deemed to occur on the edge from the corresponding predecessor block to; the current block (but after any definition of an '``invoke``'; instruction's return value on the same edge). The optional ``fast-math-flags`` marker indicates that the phi has one; or more :ref:`fast-math-flags <fastmath>`. These are optimization hints; to enable otherwise unsafe floating-point optimizations. Fast-math-flags; are only valid for phis that return a floating-point scalar or vector; type, or an array (nested to any depth) of floating-point scalar or vector; types. Semantics:; """""""""""""""""""". At runtime, the '``phi``' instruction logically takes on the value; specified by the pair corresponding to the predecessor basic block that; executed just prior to the current block. Example:; """""""""""""""". .. code-block:: llvm. Loop: ; Infinite loop that counts from 0 on up...; %indvar = phi i32 [ 0, %LoopHeader ], [ %nextindvar, %Loop ]; %nextindvar = add i32 %indvar, 1; br label %Loop. .. _i_select:. '``select``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. <result> = select [fast-math flags] selty <cond>, <ty> <val1>, <ty> <val2> ; yields ty. selty is either i1 or {<N x i1>}. Overview:; """""""""""""""""". The '``select``' instruction is used to choose one value based on a; condition, without IR-level branching. Arguments:; """""""""""""""""""". The '``s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:468089,optimiz,optimization,468089,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"SUMMARY; -------. We met to discuss the LLVM instruction format and bytecode representation:. ISSUES RESOLVED; ---------------. 1. We decided that we shall use a flat namespace to represent our ; variables in SSA form, as opposed to having a two dimensional namespace; of the original variable and the SSA instance subscript. ARGUMENT AGAINST:; * A two dimensional namespace would be valuable when doing alias ; analysis because the extra information can help limit the scope of; analysis. ARGUMENT FOR:; * Including this information would require that all users of the LLVM; bytecode would have to parse and handle it. This would slow down the; common case and inflate the instruction representation with another; infinite variable space. REASONING:; * It was decided that because original variable sources could be; reconstructed from SSA form in linear time, that it would be an; unjustified expense for the common case to include the extra; information for one optimization. Alias analysis itself is typically; greater than linear in asymptotic complexity, so this extra analaysis; would not affect the runtime of the optimization in a significant; way. Additionally, this would be an unlikely optimization to do at; runtime. IDEAS TO CONSIDER; -----------------. 1. Including dominator information in the LLVM bytecode; representation. This is one example of an analysis result that may be; packaged with the bytecodes themselves. As a conceptual implementation ; idea, we could include an immediate dominator number for each basic block; in the LLVM bytecode program. Basic blocks could be numbered according; to the order of occurrence in the bytecode representation. 2. Including loop header and body information. This would facilitate; detection of intervals and natural loops. UNRESOLVED ISSUES ; ----------------- . 1. Will oSUIF provide enough of an infrastructure to support the research; that we will be doing? We know that it has less than stellar; performance, but hope that this will ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt:965,optimiz,optimization,965,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt,1,['optimiz'],['optimization']
Performance,"SUnit) to simulate the speculative; execution of loads and stores. Each load (or store) consumes an entry in the load (or store) queue. Users can; specify flags ``-lqueue`` and ``-squeue`` to limit the number of entries in the; load and store queues respectively. The queues are unbounded by default. The LSUnit implements a relaxed consistency model for memory loads and stores.; The rules are:. 1. A younger load is allowed to pass an older load only if there are no; intervening stores or barriers between the two loads.; 2. A younger load is allowed to pass an older store provided that the load does; not alias with the store.; 3. A younger store is not allowed to pass an older store.; 4. A younger store is not allowed to pass an older load. By default, the LSUnit optimistically assumes that loads do not alias; (`-noalias=true`) store operations. Under this assumption, younger loads are; always allowed to pass older stores. Essentially, the LSUnit does not attempt; to run any alias analysis to predict when loads and stores do not alias with; each other. Note that, in the case of write-combining memory, rule 3 could be relaxed to; allow reordering of non-aliasing store operations. That being said, at the; moment, there is no way to further relax the memory model (``-noalias`` is the; only option). Essentially, there is no option to specify a different memory; type (e.g., write-back, write-combining, write-through; etc.) and consequently; to weaken, or strengthen, the memory model. Other limitations are:. * The LSUnit does not know when store-to-load forwarding may occur.; * The LSUnit does not know anything about cache hierarchy and memory types.; * The LSUnit does not know how to identify serializing operations and memory; fences. The LSUnit does not attempt to predict if a load or store hits or misses the L1; cache. It only knows if an instruction ""MayLoad"" and/or ""MayStore."" For; loads, the scheduling model provides an ""optimistic"" load-to-use latency (which; usually",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:40416,load,loads,40416,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['load'],['loads']
Performance,"S`. Collect internal LLVM statistics. Appends `-save-stats=obj` when invoking the; compiler and makes the lit runner collect and merge the statistic files. - `TEST_SUITE_RUN_BENCHMARKS`. If this is set to `OFF` then lit will not actually run the tests but just; collect build statistics like compile time and code size. - `TEST_SUITE_USE_PERF`. Use the `perf` tool for time measurement instead of the `timeit` tool that; comes with the test-suite. The `perf` is usually available on linux systems. - `TEST_SUITE_SPEC2000_ROOT`, `TEST_SUITE_SPEC2006_ROOT`, `TEST_SUITE_SPEC2017_ROOT`, ... Specify installation directories of external benchmark suites. You can find; more information about expected versions or usage in the README files in the; `External` directory (such as `External/SPEC/README`). ### Common CMake Flags. - `-GNinja`. Generate build files for the ninja build tool. - `-Ctest-suite/cmake/caches/<cachefile.cmake>`. Use a CMake cache. The test-suite comes with several CMake caches which; predefine common or tricky build configurations. Displaying and Analyzing Results; --------------------------------. The `compare.py` script displays and compares result files. A result file is; produced when invoking lit with the `-o filename.json` flag. Example usage:. - Basic Usage:. ```text; % test-suite/utils/compare.py baseline.json; Warning: 'test-suite :: External/SPEC/CINT2006/403.gcc/403.gcc.test' has No metrics!; Tests: 508; Metric: exec_time. Program baseline. INT2006/456.hmmer/456.hmmer 1222.90; INT2006/464.h264ref/464.h264ref 928.70; ...; baseline; count 506.000000; mean 20.563098; std 111.423325; min 0.003400; 25% 0.011200; 50% 0.339450; 75% 4.067200; max 1222.896800; ```. - Show compile_time or text segment size metrics:. ```bash; % test-suite/utils/compare.py -m compile_time baseline.json; % test-suite/utils/compare.py -m size.__text baseline.json; ```. - Compare two result files and filter short running tests:. ```bash; % test-suite/utils/compare.py --filter-short ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md:7127,cache,caches,7127,interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteGuide.md,1,['cache'],['caches']
Performance,"See [ROOT-7830] for example. This release contains several bug fixes and improvements, notably in unloading and performance. > NOTE: The GCC 5 ABI is *not* supported yet, due to a lack of support in clang. ## I/O Libraries. ### hadd. We extended the `hadd` options to allow more control on the compression settings use for the; output file. In particular the new option -fk allows for a copy of the input; files with no decompressions/recompression of the TTree baskets even if they; do not match the requested compression setting. New options:. - `-ff` allows to force the compression setting to match the one from the first input; - `-fk[0-209]` allows to keep all the basket compressed as is and to compress the meta data with the given compression setting or the compression setting of the first input file.; - `-a` option append to existing file; - The verbosity level is now optional after -v. ### Command line utilities. We added command line utilities to streamline very common operations performed on root files, like listing their content or creating directories.; The command line utilities are:; - `rootbrowse`: to open the file in a TBrowser; - `rootcp`: to copy content from one file to another; - `rooteventselector`: to select a subset of the events in a tree contained in a file; - `rootls`: to list the content of a rootfile; - `rootmkdir`: to create a directory in a rootfile; - `rootmv`: to move content across files; - `rootprint`: to plot content (histograms, graphs) of files; - `rootrm`: to remove content from files; These utilities took inspiration from the well known *nix commands and all offer the `-h` switch which provides documentation for all options available and example invocation lines. ### TBufferFile. We updated TBuffer::Expand to properly shrink the buffer when requested, hence reducing memory usage in some cases. ### I/O New functionalities. We added support for template parameter packs in class name involved in the I/O. ## TTree Libraries. ### Improveme",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v606/index.md:7504,perform,performed,7504,README/ReleaseNotes/v606/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v606/index.md,1,['perform'],['performed']
Performance,"Sel is a framework that provides a set of reusable passes and utilities; for instruction selection --- translation from LLVM IR to target-specific; Machine IR (MIR). GlobalISel is intended to be a replacement for SelectionDAG and FastISel, to; solve three major problems:. * **Performance** --- SelectionDAG introduces a dedicated intermediate; representation, which has a compile-time cost. GlobalISel directly operates on the post-isel representation used by the; rest of the code generator, MIR.; It does require extensions to that representation to support arbitrary; incoming IR: :ref:`gmir`. * **Granularity** --- SelectionDAG and FastISel operate on individual basic; blocks, losing some global optimization opportunities. GlobalISel operates on the whole function. * **Modularity** --- SelectionDAG and FastISel are radically different and share; very little code. GlobalISel is built in a way that enables code reuse. For instance, both the; optimized and fast selectors share the :ref:`pipeline`, and targets can; configure that pipeline to better suit their needs. Design and Implementation Reference; ===================================. More information on the design and implementation of GlobalISel can be found in; the following sections. .. toctree::; :maxdepth: 1. GMIR; GenericOpcode; MIRPatterns; Pipeline; Porting; Resources. More information on specific passes can be found in the following sections:. .. toctree::; :maxdepth: 1. IRTranslator; Legalizer; RegBankSelect; InstructionSelect; KnownBits. .. _progress:. Progress and Future Work; ========================. The initial goal is to replace FastISel on AArch64. The next step will be to; replace SelectionDAG as the optimized ISel. ``NOTE``:; While we iterate on GlobalISel, we strive to avoid affecting the performance of; SelectionDAG, FastISel, or the other MIR passes. For instance, the types of; :ref:`gmir-gvregs` are stored in a separate table in ``MachineRegisterInfo``,; that is destroyed after :ref:`instructions",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/index.rst:1266,optimiz,optimized,1266,interpreter/llvm-project/llvm/docs/GlobalISel/index.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/index.rst,1,['optimiz'],['optimized']
Performance,"Setup a static PROOF cluster with PROOF on Demand; =================================================. Introduction; ------------. Using PROOF on Demand is our current recommended way of running a PROOF; cluster. The usage of PoD is in particular helpful for the following; reasons:. - **Sandboxing.** Each user get their own personal PROOF cluster,; separated from the others: a problem occurring on one personal; cluster does not affect the workflow of other users. - **Easier administration and self-servicing.** A user can restart their; personal PROOF cluster in case of troubles without waiting for a; system administrator's intervention. - **Efficient multiuser scheduling.** PROOF on Demand makes PROOF run on; top of an existing resource management system, moving the problem of; scheduling many concurrent users outside of PROOF. This guide particularly refers to the setup of a static PROOF cluster; running on physical hosts: the recommended setup is in practice the same; as the ready-to-go Virtual Analysis Facility. If you want to use PROOF; on the clouds there is no configuration to go through. Setup a resource management system; ----------------------------------. Although PROOF on Demand can run on a cluster of nodes without using a; resource management system (using `pod-ssh`), it is recommended to setup a; dedicated one to benefit from the scheduling in a multiuser environment, or a; dedicated queue on an existing one. As there's a variety of resource management systems, this guide does not cover; their setup. The RMS preconfigured for the Virtual Analysis Facility is; [HTCondor](http://research.cs.wisc.edu/htcondor/), which we recommend primarily; because it has dynamic addition of workers built in. Configuration steps for all nodes; ---------------------------------. ### Setup CernVM-FS. [CernVM-FS](http://cernvm.cern.ch/portal/filesystem) should be installed; on all machines as the preferred method for software distribution. > Configuration instructions for the",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/ConfigProofPoD.md:804,concurren,concurrent,804,proof/doc/confman/ConfigProofPoD.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/ConfigProofPoD.md,1,['concurren'],['concurrent']
Performance,"Since there; are no loop-carried dependencies, the observed `uOps Per Cycle` is expected to; approach 1.50 when the number of iterations tends to infinity. The delta between; the Dispatch Width (2.00), and the theoretical maximum uOp throughput (1.50) is; an indicator of a performance bottleneck caused by the lack of hardware; resources, and the *Resource pressure view* can help to identify the problematic; resource usage. The second section of the report is the `instruction info view`. It shows the; latency and reciprocal throughput of every instruction in the sequence. It also; reports extra information related to the number of micro opcodes, and opcode; properties (i.e., 'MayLoad', 'MayStore', and 'HasSideEffects'). Field *RThroughput* is the reciprocal of the instruction throughput. Throughput; is computed as the maximum number of instructions of a same type that can be; executed per clock cycle in the absence of operand dependencies. In this; example, the reciprocal throughput of a vector float multiply is 1; cycles/instruction. That is because the FP multiplier JFPM is only available; from pipeline JFPU1. Instruction encodings are displayed within the instruction info view when flag; `-show-encoding` is specified. Below is an example of `-show-encoding` output for the dot-product kernel:. .. code-block:: none. Instruction Info:; [1]: #uOps; [2]: Latency; [3]: RThroughput; [4]: MayLoad; [5]: MayStore; [6]: HasSideEffects (U); [7]: Encoding Size. [1] [2] [3] [4] [5] [6] [7] Encodings: Instructions:; 1 2 1.00 4 c5 f0 59 d0 vmulps	%xmm0, %xmm1, %xmm2; 1 4 1.00 4 c5 eb 7c da vhaddps	%xmm2, %xmm2, %xmm3; 1 4 1.00 4 c5 e3 7c e3 vhaddps	%xmm3, %xmm3, %xmm4. The `Encoding Size` column shows the size in bytes of instructions. The; `Encodings` column shows the actual instruction encodings (byte sequences in; hex). The third section is the *Resource pressure view*. This view reports; the average number of resource cycles consumed every iteration by instructions; for every ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:19070,throughput,throughput,19070,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['throughput'],['throughput']
Performance,"Size indicates how many additional clusters will be kept in memory).; - Added ```TBranchProxy::GetEntries``` to support leaflist variable size array and added ```TBranchProxy::GetArrayLength```.; - In ```TBranch::Streamer``` insured that we never steam already basket already written to disk. ### TDataFrame. #### New features; - Add `Alias`, a facility to specify an alternative name for a given column: `auto histo = mytdf.Alias(""myAlias"", ""myColumn"").Histo1D(""myAlias"");`. Especially useful for pyROOT users to deal with column names that are not valid C++ identifiers (e.g. `Filter(""1branch > 0"") --> Alias(""1branch"", ""branch1"").Filter(""branch1 > 0"")`.; - Add `Cache`, a facility to cache `TDataFrame`s in memory. All or some columns can be cached. Two versions of the method are proposed: one which allows to explicitly list the types of the columns and another one allowing to let the system infer them (the same mechanism of the `Snapshot` method). Only columns containing instances of classes which have a copy constructor can be cached.; - Add `DefineSlot`, a `Define` transformation that is aware of the multi-threading slot where the workload is executed; - Add `DefineSlotEntry`, a `Define` transformation that is aware of the multi-threading slot and of the current entry number; - Add `GetColumnsNames`: users can now get the names of the available columns coming from trees, data sources or `Define`d columns; - Add `OnPartialResult` and `OnPartialResultSlot`: users can now register one or more functions to be executed on partial results of TDF actions during the event loop.; This mechanism is meant to be used to inspect partial results of the analysis or print useful debug information.; For example, both in single- and multi-thread event loops, one can draw a result histogram and update the canvas every 100 entries like this:; ```c++; auto h = tdf.Histo1D(""x"");; TCanvas c(""c"",""x hist"");; h.OnPartialResult(100, [&c](TH1D &h_) { c.cd(); h_.Draw(); c.Update(); });; ```; See th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md:11329,cache,cached,11329,README/ReleaseNotes/v612/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md,1,['cache'],['cached']
Performance,"Suite Source Code; ------------------------------------. Unlike with autotools, with CMake your build type is defined at configuration.; If you want to change your build type, you can re-run cmake with the following; invocation:. .. code-block:: console. % cmake -G ""Unix Makefiles"" -DCMAKE_BUILD_TYPE=<type> SRC_ROOT. Between runs, CMake preserves the values set for all options. CMake has the; following build types defined:. Debug. These builds are the default. The build system will compile the tools and; libraries unoptimized, with debugging information, and asserts enabled. Release. For these builds, the build system will compile the tools and libraries; with optimizations enabled and not generate debug info. CMakes default; optimization level is -O3. This can be configured by setting the; ``CMAKE_CXX_FLAGS_RELEASE`` variable on the CMake command line. RelWithDebInfo. These builds are useful when debugging. They generate optimized binaries with; debug information. CMakes default optimization level is -O2. This can be; configured by setting the ``CMAKE_CXX_FLAGS_RELWITHDEBINFO`` variable on the; CMake command line. Once you have LLVM configured, you can build it by entering the *OBJ_ROOT*; directory and issuing the following command:. .. code-block:: console. % make. If the build fails, please `check here`_ to see if you are using a version of; GCC that is known not to compile LLVM. If you have multiple processors in your machine, you may wish to use some of the; parallel build options provided by GNU Make. For example, you could use the; command:. .. code-block:: console. % make -j2. There are several special targets which are useful when working with the LLVM; source code:. ``make clean``. Removes all files generated by the build. This includes object files,; generated C/C++ files, libraries, and executables. ``make install``. Installs LLVM header files, libraries, tools, and documentation in a hierarchy; under ``$PREFIX``, specified with ``CMAKE_INSTALL_PREFIX``,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst:29436,optimiz,optimization,29436,interpreter/llvm-project/llvm/docs/GettingStarted.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst,1,['optimiz'],['optimization']
Performance,"Syntax Description; =============================== =========================================================; dim:SQ_RSRC_IMG_1D One-dimensional image.; dim:SQ_RSRC_IMG_2D Two-dimensional image.; dim:SQ_RSRC_IMG_3D Three-dimensional image.; dim:SQ_RSRC_IMG_CUBE Cubemap array.; dim:SQ_RSRC_IMG_1D_ARRAY One-dimensional image array.; dim:SQ_RSRC_IMG_2D_ARRAY Two-dimensional image array.; dim:SQ_RSRC_IMG_2D_MSAA Two-dimensional multi-sample auto-aliasing image.; dim:SQ_RSRC_IMG_2D_MSAA_ARRAY Two-dimensional multi-sample auto-aliasing image array.; =============================== =========================================================. dlc; ~~~. See a description :ref:`here<amdgpu_synid_dlc>`. Miscellaneous Modifiers; -----------------------. .. _amdgpu_synid_dlc:. dlc; ~~~. Controls device level cache policy for memory operations. Used for synchronization.; When specified, forces operation to bypass device level cache, making the operation device; level coherent. By default, instructions use device level cache. ======================================== ================================================; Syntax Description; ======================================== ================================================; dlc Bypass device level cache.; ======================================== ================================================. .. _amdgpu_synid_glc:. glc; ~~~. For atomic opcodes, this modifier indicates that the instruction returns the value from memory; before the operation. For other opcodes, it is used together with :ref:`slc<amdgpu_synid_slc>`; to specify cache policy. The default value is off (0). ======================================== ================================================; Syntax Description; ======================================== ================================================; glc Set glc bit to 1.; ======================================== ================================================. .. _amdgpu_synid_lds:. lds; ~~~. Specifies where to st",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst:17270,cache,cache,17270,interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,1,['cache'],['cache']
Performance,"T MC, GA, SA, MINUIT Optimisation Method. Converger No None None, MINUIT FitMethod uses Converger to improve result. Configuration options for MVA method :. Configuration options reference for MVA method: LD. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included for testing and performance evaluation). Configuration options for MVA method :. Configuration options reference for MVA method: SVM. Option Array Default value Predefined values Description. V No False − Verbose output (short form of VerbosityLevel below - overrides the latter one). VerbosityLevel No Default Default, Debug, Verbose, Info, Warning, Error, Fatal Verbosity level. VarTransform No None − List of variable transformations performed before training, e.g., D_Background,P_Signal,G,N_AllClasses for: Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed). H No False − Print method-specific help message. CreateMVAPdfs No False − Create PDFs for classifier outputs (signal and background). IgnoreNegWeightsInTraining No False − Events with negative weights are ignored in the training (but are included f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html:7173,perform,performance,7173,documentation/tmva/UsersGuide/optionRef.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/tmva/UsersGuide/optionRef.html,2,['perform'],['performance']
Performance,"T at start-up ###. The behaviour of a ROOT session can be tailored with the options in the; `.rootrc` file. Examples of the tunable parameters are the ones related; to the operating and window system, to the fonts to be used, to the; location of start-up files. At start-up, ROOT looks for a `.rootrc` file; in the following order:. - `./.rootrc //local directory`. - `$HOME/.rootrc //user directory`. - `$ROOTSYS/etc/system.rootrc //global ROOT directory`. If more than one `.rootrc` files are found in the search paths above,; the options are merged, with precedence local, user, global. The parsing; and interpretation of this file is handled by the ROOT class `TEnv`.; Have a look to its documentation if you need such rather advanced; features. The file `.rootrc` defines the location of two rather; important files inspected at start-up: `rootalias.C` and `rootlogon.C`.; They can contain code that needs to be loaded and executed at ROOT; startup. `rootalias.C` is only loaded and best used to define some often; used functions. `rootlogon.C` contains code that will be executed at; startup: this file is extremely useful for example to pre-load a custom; style for the plots created with ROOT. This is done most easily by; creating a new `TStyle` object with your preferred settings, as; described in the class reference guide, and then use the command; `gROOT->SetStyle(""MyStyleName"");` to make this new style definition the; default one. As an example, have a look in the file `rootlogon.C` coming; with this tutorial. Another relevant file is `rootlogoff.C` that it; called when the session is finished. ### ROOT command history ###. Every command typed at the ROOT prompt is stored in a file `.root_hist`; in your home directory. ROOT uses this file to allow for navigation in; the command history with the up-arrow and down-arrow keys. It is also; convenient to extract successful ROOT commands with the help of a text; editor for use in your own macros. ### ROOT Global Pointers ###. All",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/ROOT_as_calculator.md:18868,load,loaded,18868,documentation/primer/ROOT_as_calculator.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/primer/ROOT_as_calculator.md,1,['load'],['loaded']
Performance,"TEST DISCOVERY; ~~~~~~~~~~~~~~. Once test suites are located, :program:`lit` recursively traverses the source; directory (following *test_source_root*) looking for tests. When :program:`lit`; enters a sub-directory, it first checks to see if a nested test suite is; defined in that directory. If so, it loads that test suite recursively,; otherwise it instantiates a local test config for the directory (see; :ref:`local-configuration-files`). Tests are identified by the test suite they are contained within, and the; relative path inside that suite. Note that the relative path may not refer to; an actual file on disk; some test formats (such as *GoogleTest*) define; ""virtual tests"" which have a path that contains both the path to the actual; test file and a subpath to identify the virtual test. .. _local-configuration-files:. LOCAL CONFIGURATION FILES; ~~~~~~~~~~~~~~~~~~~~~~~~~. When :program:`lit` loads a subdirectory in a test suite, it instantiates a; local test configuration by cloning the configuration for the parent directory; --- the root of this configuration chain will always be a test suite. Once the; test configuration is cloned :program:`lit` checks for a *lit.local.cfg* file; in the subdirectory. If present, this file will be loaded and can be used to; specialize the configuration for each individual directory. This facility can; be used to define subdirectories of optional tests, or to change other; configuration parameters --- for example, to change the test format, or the; suffixes which identify test files. SUBSTITUTIONS; ~~~~~~~~~~~~~. :program:`lit` allows patterns to be substituted inside RUN commands. It also; provides the following base set of substitutions, which are defined in; TestRunner.py:. ======================= ==============; Macro Substitution; ======================= ==============; %s source path (path to the file currently being run); %S source dir (directory of the file currently being run); %p same as %S; %{pathsep} path separator; %{",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lit.rst:18295,load,loads,18295,interpreter/llvm-project/llvm/docs/CommandGuide/lit.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/lit.rst,1,['load'],['loads']
Performance,"TK[i][t];; ; return 5;; }. We generate relatively atrocious code for this loop compared to gcc. We could also strength reduce the rem and the div:; http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf. ===-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* %b) {; entry:; %tmp = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp = load i32* %tmp ; <uint> [#uses=1]; %tmp3 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp4 = load i32* %tmp3 ; <uint> [#uses=1]; %tmp8 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=2]; %tmp9 = load i32* %tmp8 ; <uint> [#uses=1]; %tmp4.mask17 = shl i32 %tmp4, i8 1 ; <uint> [#uses=1]; %tmp1415 = and i32 %tmp4.mask17, 2147483648 ; <uint> [#uses=1]; %tmp.masked = and i32 %tmp, 2147483648 ; <uint> [#uses=1]; %tmp11 = or i32 %tmp1415, %tmp.masked ; <uint> [#uses=1]; %tmp12 = and i32 %tmp9, 2147483647 ; <uint> [#uses=1]; %tmp13 = or i32 %tmp12, %tmp11 ; <uint> [#uses=1]; store i32 %tmp13, i32* %tmp8; ret void; }. We emit:. _foo:; lwz r2, 0(r3); slwi r4, r2, 1; or r4, r4, r2; rlwimi r2, r4, 0, 0, 0; stw r2, 0(r3); blr. We could collapse a bunch of those ORs and ANDs and generate the following; equivalent code:. _foo:; lwz r2, 0(r3); rlwinm r4, r2, 1, 0, 0; or r2, r2, r4; stw r2, 0(r3); blr. ===-------------------------------------------------------------------------===. Consider a function like this:. float foo(float X) { return X + 1234.4123f; }. The FP constant ends up in the constant pool, so we need to get the LR register.; This ends up producing code like this:. _foo:; .LBB_foo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:5602,load,load,5602,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,2,['load'],['load']
Performance,"TString`, e.g.; `objString->GetString().ReplaceAll(""a"", ""b""); // WRONG!` This will now fail to compile, instead of not doing what the author of the; code expected. Please fix that code by using the `TObjString::String()` interface, which returns a non-const `TString&`:; `objString->String().ReplaceAll(""a"", ""b"");`. In extremely rare cases, this change breaks a valid use where the temporary `TString` was modified and then captured in a new `TString`; object before the destruction of the temporary: `TString str = objStr->GetString().ReplaceAll(""a"", ""b"");`. In these rare cases,; please use the new function `CopyString()` which clearly indicates that it involves a temporary. ## Histogram Libraries. - New class `THnChain` was added to provide a `TChain`-like experience when; working with `THnBase`'ed histograms (currently `THn` and `THnSparse`) from; many files, see [here](https://sft.its.cern.ch/jira/browse/ROOT-4515). This; allows to e.g., interactively adjust axis parameters before performing; projections from high-dimensional histograms,. ```{.cpp}; // Create a chain of histograms called `h`.; THnChain chain(""h"");. // Add files containing histograms `h` to `chain`.; chain->AddFile(""file1.root"");. chain->GetXaxis(1)->SetRangeUser(0.1, 0.2);. TH1* projection = chain->Projection(0); ```. ## Math Libraries. * Improve thread friendliness of the TMinuit class. ## RooFit Libraries. - Remove deprecated `RooComplex` superseded by `std::complex`. ## TTree Libraries. - `TTreeReader` now supports `TEntryList`s, `Double32_t` / `Float16_t`.; - `TTreeReader::SetLastEntry()` has been deprecated. Its name is misleading; please use `TTreePlayer::SetEntriesRange()` instead.; - `TTree::Branch()` now complains for wrong leaf list strings, e.g. ""value/F[4]"" (which should really be spelled as ""value[4]/F"").; - Allow reading of older version of TTreePerfStats (ROOT-8520); - In `TTree::OptimizeBaskets()` do not call GetBasket(0) to avoid disc reads; - It is now possible to define the precisio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v610/index.md:4527,perform,performing,4527,README/ReleaseNotes/v610/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v610/index.md,1,['perform'],['performing']
Performance,"TTeXDump class:; \par; \input{hpx.tex}; \end{document}; ```. Note the two directive needed at the top of the LaTeX file:. ```; \usepackage{tikz}; \usetikzlibrary{patterns}; ```. Then including the picture in the document is done with the; `\input` directive. The command `pdflatex simple.tex` will generate the corresponding pdf; file `simple.pdf`. ### X11 fonts. - A coverity fix in `Rotated.cxx` had a side effect on rotated text; drawn with X11 fonts. ### TCanvas and TPad. - `TPad::SaveAs` produces named macros in .C files.; - Change the way the string input is done in the Pad toolbar for text; and Pave Label. It doesn't use anymore the `TGX11` function `RequestString`.; Now the text appears directly as it will show and it is possible to; enter several text string. The input is not block in the `RequestString` event loop.; - The toolbar methods now work without XOR mode (useful for OpenGL()).; - A new ""vertex compression"" algorithm added to deal with complex histograms; (thousands/millions of bins - polygons with thousands/millions of vertices) -; optimization/fix for X11 crashes. ### TGaxis and TAxis. - The time axis behavior should now be correct along time zone and; summer saving time. A fix has been done with the of Philippe Gras; (CEA Saclay. IRFU/SEDI) and Julian Sitarek (IFAE). Time axis; transported from a time zone to an other in a ROOT file are correct; too. A new example test have been introduced to test the time axis; (timeonaxis3.C); - In some case the format use to build the axis labels was incorrect.; (cf: Jira report ROOT-5635).; - New static function to change the position of the ""power of 10""; near the axis. A static function is used instead of data members; in `TAxis` in order to keep the `TAxis` class small. Adding two; floating point numbers in that class (in fact in `TAttAxis`) would; have a none negligible effect on the Root files' sizes as there is; at least two axis per histogram and that there is often 1000th; histograms in a single file.; S",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf2d/doc/v600/index.md:2512,optimiz,optimization,2512,graf2d/doc/v600/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf2d/doc/v600/index.md,1,['optimiz'],['optimization']
Performance,"TYPE CC (cache coherent); and so the L2 cache will be coherent with the CPU and other agents. Scratch backing memory (which is used for the private address space) is accessed; with MTYPE NC_NV (non-coherent non-volatile). Since the private address space is; only accessed by a single thread, and is always write-before-read, there is; never a need to invalidate these entries from the L1 cache. Hence all cache; invalidates are done as ``*_vol`` to only invalidate the volatile cache lines. The code sequences used to implement the memory model for GFX6-GFX9 are defined; in table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx6-gfx9-table`. .. table:: AMDHSA Memory Model Code Sequences GFX6-GFX9; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx6-gfx9-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX6-GFX9; Space; ============ ============ ============== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; glc=1 slc=1. - volatile. 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1. - volatile. 1. buffer/global/flat_store; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:212018,load,load,212018,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"TargetMachine::registerPassBuilderCallbacks()`` is an example of a; backend adding passes to various parts of the pipeline. Pass plugins can also add passes into default pipelines. Different tools have; different ways of loading dynamic pass plugins. For example, ``opt; -load-pass-plugin=path/to/plugin.so`` loads a pass plugin into ``opt``. For; information on writing a pass plugin, see :doc:`WritingAnLLVMNewPMPass`. Using Analyses; ==============. LLVM provides many analyses that passes can use, such as a dominator tree.; Calculating these can be expensive, so the new pass manager has; infrastructure to cache analyses and reuse them when possible. When a pass runs on some IR, it also receives an analysis manager which it can; query for analyses. Querying for an analysis will cause the manager to check if; it has already computed the result for the requested IR. If it already has and; the result is still valid, it will return that. Otherwise it will construct a; new result by calling the analysis's ``run()`` method, cache it, and return it.; You can also ask the analysis manager to only return an analysis if it's; already cached. The analysis manager only provides analysis results for the same IR type as; what the pass runs on. For example, a function pass receives an analysis; manager that only provides function-level analyses. This works for many; passes which work on a fixed scope. However, some passes want to peek up or; down the IR hierarchy. For example, an SCC pass may want to look at function; analyses for the functions inside the SCC. Or it may want to look at some; immutable global analysis. In these cases, the analysis manager can provide a; proxy to an outer or inner level analysis manager. For example, to get a; ``FunctionAnalysisManager`` from a ``CGSCCAnalysisManager``, you can call. .. code-block:: c++. FunctionAnalysisManager &FAM =; AM.getResult<FunctionAnalysisManagerCGSCCProxy>(InitialC, CG); .getManager();. and use ``FAM`` as a typical ``Function",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst:7378,cache,cache,7378,interpreter/llvm-project/llvm/docs/NewPassManager.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/NewPassManager.rst,1,['cache'],['cache']
Performance,"The LLVM Compiler Infrastructure; ================================. This directory and its subdirectories contain source code for LLVM,; a toolkit for the construction of highly optimized compilers,; optimizers, and runtime environments. LLVM is open source software. You may freely distribute it under the terms of; the license agreement found in LICENSE.txt. Please see the documentation provided in docs/ for further; assistance with LLVM, and in particular docs/GettingStarted.rst for getting; started with LLVM and docs/README.txt for an overview of LLVM's; documentation setup. If you are writing a package for LLVM, see docs/Packaging.rst for our; suggestions.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/README.txt:178,optimiz,optimized,178,interpreter/llvm-project/llvm/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/README.txt,2,['optimiz'],"['optimized', 'optimizers']"
Performance,"The analyzer performs checks that are categorized into families or ""checkers"". The; default set of checkers covers a variety of checks targeted at finding security; and API usage bugs, dead code, and other logic errors. See the; Default Checkers list below. In addition to; these, the analyzer contains a number of ; Experimental (Alpha) Checkers. Writeups with examples of some of the bugs that the analyzer finds. Bug Finding With Clang: 5 Resources To Get You Started; Finding Memory Leaks With The LLVM/Clang Static Analyzer; Under the Microscope - The Clang Static Analyzer; Mike Ash - Using the Clang Static Analyzer. Default Checkers. Core Checkers model core language features and perform general-purpose checks such as division by zero, null pointer dereference, usage of uninitialized values, etc.; C++ Checkers perform C++-specific checks; Dead Code Checkers check for unused code; Nullability Checkers ; Optin Checkers ; OS X Checkers perform Objective-C-specific checks and check the use of Apple's SDKs (OS X and iOS); Security Checkers check for insecure API usage and perform checks based on the CERT Secure Coding Standards; Unix Checkers check the use of Unix and POSIX APIs. Core Checkers. Name, DescriptionExample. core.CallAndMessage; (C, C++, ObjC); Check for logical errors for function calls and Objective-C message expressions; (e.g., uninitialized arguments, null function pointers). // C; struct S {; int x;; };. void f(struct S s);. void test() {; struct S s;; f(s); // warn: passed-by-value arg contain uninitialized data; }. // C; void test() {; void (*foo)(void);; foo(); // warn: function pointer is uninitialized; }. // C; void test() {; void (*foo)(void);; foo = 0;; foo(); // warn: function pointer is null; }. // C++; class C {; public:; void f();; };. void test() {; C *pc;; pc->f(); // warn: object pointer is uninitialized; }. // C++; class C {; public:; void f();; };. void test() {; C *pc = 0;; pc->f(); // warn: object pointer is null; }. // Objective-C; @in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/available_checks.html:864,perform,perform,864,interpreter/llvm-project/clang/www/analyzer/available_checks.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/available_checks.html,6,['perform'],['perform']
Performance,"The design; factorizes the choice of the parameter points to be tested,�the choice of; the test statistic, and the generation of sampling distribution into; separate parts (described above). �Finally, the NeymanConstruction class; is simply in charge of using these parts (strategies) and constructing; the confidence belt and confidence intervals. �The ConfidenceBelt class; is still under development, but the current version works fine for; producing ConfidenceIntervals. �We are also working to make this class; work with parallelization approaches, which is not yet complete.; The FeldmanCousins class is a separate concrete implementation of the; IntervalCalculator interface. �It uses the NeymanConstruction internally,; and�enforces�specific choices of the test statistic and ordering; principle to realize the Unified intervals described by Feldman and; Cousins in their paper�Phys.Rev.D57:3873-3889,1998. In an extension to the technique discussed in Feldman and Cousins paper,; the FeldmanCousins class also performs a ""profile construction"" if their are nuisance parameters.; In this case, the parameters of interest are scanned in a regular grid. For each point in the grid; the calculator finds the best fit value of the nuisance parameters (given the data). The construction; is then only performed in this subspace of the parameters. As a result, the number of points in the; construction only scales in the number of parameters of interest, not in the number of nuisance parameters. Markov Chain Monte Carlo Interval; A flexible framework for Markov Chain Monte Carlo was added in this; release. The MCMCCalculator is a concrete implementation of the; IntervalCalculator interface. To use it one needs to specify the ProposalFunction.; There is a base class for ProposalFunctions and one concrete implementation: UniformProposal.; Support for other proposal functions will be added in the next release.; The MCMCCalculator scans the space of the parameters of interest and nuisance p",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html:28968,perform,performs,28968,roofit/doc/v524/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v524/index.html,2,['perform'],['performs']
Performance,"The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once again:. .. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass());; ... It is interesting to see what the code looks like before and after the; mem2reg optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. Here there is only one variable (x, the input argument) but you can; still see the extremely simple-minded code generation strategy we are; using. In the entry block, an alloca is created, and the initial input; value is stored into it. Each reference to the variable does a reload; from the stack. Also, note that we didn't modify the if/then/else; expression, so it still inserts a PHI node. While we could make an; alloca for it, it is actually easier to create a PHI node for it, so we; still just make the PHI. Here is the code after the mem2reg pass runs:. .. code-block:: llvm. define double ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:17436,load,load,17436,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['load'],['load']
Performance,"The key of the attack is to call this with `untrusted_offset_from_caller` that; is far outside of the bounds when the branch predictor will predict that it; will be in-bounds. In that case, the body of the `if` will be executed; speculatively, and may read secret data into `value` and leak it via a; cache-timing side channel when a dependent access is made to populate `value2`. ## High Level Mitigation Approach. While several approaches are being actively pursued to mitigate specific; branches and/or loads inside especially risky software (most notably various OS; kernels), these approaches require manual and/or static analysis aided auditing; of code and explicit source changes to apply the mitigation. They are unlikely; to scale well to large applications. We are proposing a comprehensive; mitigation approach that would apply automatically across an entire program; rather than through manual changes to the code. While this is likely to have a; high performance cost, some applications may be in a good position to take this; performance / security tradeoff. The specific technique we propose is to cause loads to be checked using; branchless code to ensure that they are executing along a valid control flow; path. Consider the following C-pseudo-code representing the core idea of a; predicate guarding potentially invalid loads:; ```; void leak(int data);; void example(int* pointer1, int* pointer2) {; if (condition) {; // ... lots of code ...; leak(*pointer1);; } else {; // ... more code ...; leak(*pointer2);; }; }; ```. This would get transformed into something resembling the following:; ```; uintptr_t all_ones_mask = std::numerical_limits<uintptr_t>::max();; uintptr_t all_zeros_mask = 0;; void leak(int data);; void example(int* pointer1, int* pointer2) {; uintptr_t predicate_state = all_ones_mask;; if (condition) {; // Assuming ?: is implemented using branchless logic...; predicate_state = !condition ? all_zeros_mask : predicate_state;; // ... lots of code ...; //; //",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:2909,perform,performance,2909,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,2,['perform'],['performance']
Performance,"There is ambiguity about how ``-ffp-contract``,; ``-funsafe-math-optimizations``, and ``-fno-unsafe-math-optimizations``; behave when combined. Explanation in :option:`-fno-fast-math` also applies; to these options. Defaults to ``-fno-unsafe-math-optimizations``. .. option:: -f[no-]finite-math-only. Allow floating-point optimizations that assume arguments and results are; not NaNs or +-Inf. ``-ffinite-math-only`` defines the; ``__FINITE_MATH_ONLY__`` preprocessor macro.; ``-ffinite-math-only`` implies:. * ``-fno-honor-infinities``; * ``-fno-honor-nans``. ``-ffno-inite-math-only`` implies:. * ``-fhonor-infinities``; * ``-fhonor-nans``. Defaults to ``-fno-finite-math-only``. .. option:: -f[no-]rounding-math. Force floating-point operations to honor the dynamically-set rounding mode by default. The result of a floating-point operation often cannot be exactly represented in the result type and therefore must be rounded. IEEE 754 describes different rounding modes that control how to perform this rounding, not all of which are supported by all implementations. C provides interfaces (``fesetround`` and ``fesetenv``) for dynamically controlling the rounding mode, and while it also recommends certain conventions for changing the rounding mode, these conventions are not typically enforced in the ABI. Since the rounding mode changes the numerical result of operations, the compiler must understand something about it in order to optimize floating point operations. Note that floating-point operations performed as part of constant initialization are formally performed prior to the start of the program and are therefore not subject to the current rounding mode. This includes the initialization of global variables and local ``static`` variables. Floating-point operations in these contexts will be rounded using ``FE_TONEAREST``. - The option ``-fno-rounding-math`` allows the compiler to assume that the rounding mode is set to ``FE_TONEAREST``. This is the default.; - The option ``-f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:60704,perform,perform,60704,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['perform'],['perform']
Performance,"These callbacks are emitted by -fsanitize-coverage=trace-cmp since 2017-08-11; void __sanitizer_cov_trace_const_cmp1(uint8_t Arg1, uint8_t Arg2);; void __sanitizer_cov_trace_const_cmp2(uint16_t Arg1, uint16_t Arg2);; void __sanitizer_cov_trace_const_cmp4(uint32_t Arg1, uint32_t Arg2);; void __sanitizer_cov_trace_const_cmp8(uint64_t Arg1, uint64_t Arg2);. // Called before a switch statement.; // Val is the switch operand.; // Cases[0] is the number of case constants.; // Cases[1] is the size of Val in bits.; // Cases[2:] are the case constants.; void __sanitizer_cov_trace_switch(uint64_t Val, uint64_t *Cases);. // Called before a division statement.; // Val is the second argument of division.; void __sanitizer_cov_trace_div4(uint32_t Val);; void __sanitizer_cov_trace_div8(uint64_t Val);. // Called before a GetElemementPtr (GEP) instruction; // for every non-constant array index.; void __sanitizer_cov_trace_gep(uintptr_t Idx);. // Called before a load of appropriate size. Addr is the address of the load.; void __sanitizer_cov_load1(uint8_t *addr);; void __sanitizer_cov_load2(uint16_t *addr);; void __sanitizer_cov_load4(uint32_t *addr);; void __sanitizer_cov_load8(uint64_t *addr);; void __sanitizer_cov_load16(__int128 *addr);; // Called before a store of appropriate size. Addr is the address of the store.; void __sanitizer_cov_store1(uint8_t *addr);; void __sanitizer_cov_store2(uint16_t *addr);; void __sanitizer_cov_store4(uint32_t *addr);; void __sanitizer_cov_store8(uint64_t *addr);; void __sanitizer_cov_store16(__int128 *addr);. Tracing control flow; ====================. With ``-fsanitize-coverage=control-flow`` the compiler will create a table to collect; control flow for each function. More specifically, for each basic block in the function,; two lists are populated. One list for successors of the basic block and another list for; non-intrinsic called functions. **TODO:** in the current implementation, indirect calls are not tracked; and are only marked with speci",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SanitizerCoverage.rst:11230,load,load,11230,interpreter/llvm-project/clang/docs/SanitizerCoverage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SanitizerCoverage.rst,1,['load'],['load']
Performance,"These settings are not optimal for most; desktop systems, and we hope that packagers (e.g., Redhat, Debian, MacPorts,; etc.) will tweak them. This document lists settings we suggest you tweak. LLVM's API changes with each release, so users are likely to want, for example,; both LLVM-2.6 and LLVM-2.7 installed at the same time to support apps developed; against each. Compile Flags; =============. LLVM runs much more quickly when it's optimized and assertions are removed.; However, such a build is currently incompatible with users who build without; defining ``NDEBUG``, and the lack of assertions makes it hard to debug problems; in user code. We recommend allowing users to install both optimized and debug; versions of LLVM in parallel. The following configure flags are relevant:. ``--disable-assertions``; Builds LLVM with ``NDEBUG`` defined. Changes the LLVM ABI. Also available; by setting ``DISABLE_ASSERTIONS=0|1`` in ``make``'s environment. This; defaults to enabled regardless of the optimization setting, but it slows; things down. ``--enable-debug-symbols``; Builds LLVM with ``-g``. Also available by setting ``DEBUG_SYMBOLS=0|1`` in; ``make``'s environment. This defaults to disabled when optimizing, so you; should turn it back on to let users debug their programs. ``--enable-optimized``; (For git checkouts) Builds LLVM with ``-O2`` and, by default, turns off; debug symbols. Also available by setting ``ENABLE_OPTIMIZED=0|1`` in; ``make``'s environment. This defaults to enabled when not in a; checkout. C++ Features; ============. RTTI; LLVM disables RTTI by default. Add ``REQUIRES_RTTI=1`` to your environment; while running ``make`` to re-enable it. This will allow users to build with; RTTI enabled and still inherit from LLVM classes. Shared Library; ==============. Configure with ``--enable-shared`` to build; ``libLLVM-<major>.<minor>.(so|dylib)`` and link the tools against it. This; saves lots of binary size at the cost of some startup time. Dependencies; ==========",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst:1240,optimiz,optimization,1240,interpreter/llvm-project/llvm/docs/Packaging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Packaging.rst,1,['optimiz'],['optimization']
Performance,"They; can be passed to CMake using the :code:`-C` flag as demonstrated in the examples; below along with additional configuration flags. Bootstrap Builds; ================. The Clang CMake build system supports bootstrap (aka multi-stage) builds. At a; high level a multi-stage build is a chain of builds that pass data from one; stage into the next. The most common and simple version of this is a traditional; bootstrap build. In a simple two-stage bootstrap build, we build clang using the system compiler,; then use that just-built clang to build clang again. In CMake this simplest form; of a bootstrap build can be configured with a single option,; CLANG_ENABLE_BOOTSTRAP. .. code-block:: console. $ cmake -G Ninja -DCMAKE_BUILD_TYPE=Release \; -DCLANG_ENABLE_BOOTSTRAP=On \; -DLLVM_ENABLE_PROJECTS=""clang"" \; <path to source>/llvm; $ ninja stage2. This command itself isn't terribly useful because it assumes default; configurations for each stage. The next series of examples utilize CMake cache; scripts to provide more complex options. By default, only a few CMake options will be passed between stages.; The list, called _BOOTSTRAP_DEFAULT_PASSTHROUGH, is defined in clang/CMakeLists.txt.; To force the passing of the variables between stages, use the -DCLANG_BOOTSTRAP_PASSTHROUGH; CMake option, each variable separated by a "";"". As example:. .. code-block:: console. $ cmake -G Ninja -DCMAKE_BUILD_TYPE=Release \; -DCLANG_ENABLE_BOOTSTRAP=On \; -DCLANG_BOOTSTRAP_PASSTHROUGH=""CMAKE_INSTALL_PREFIX;CMAKE_VERBOSE_MAKEFILE"" \; -DLLVM_ENABLE_PROJECTS=""clang"" \; <path to source>/llvm; $ ninja stage2. CMake options starting by ``BOOTSTRAP_`` will be passed only to the stage2 build.; This gives the opportunity to use Clang specific build flags.; For example, the following CMake call will enabled '-fno-addrsig' only during; the stage2 build for C and C++. .. code-block:: console. $ cmake [..] -DBOOTSTRAP_CMAKE_CXX_FLAGS='-fno-addrsig' -DBOOTSTRAP_CMAKE_C_FLAGS='-fno-addrsig' [..]. The c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:1998,cache,cache,1998,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['cache'],['cache']
Performance,"This is a simple example demonstrating how to use clang's facility for; providing AST consumers using a plugin. Build the plugin by running `make` in this directory. Once the plugin is built, you can run it using:; --; Linux:; $ clang -cc1 -load ../../Debug+Asserts/lib/libPrintFunctionNames.so -plugin print-fns some-input-file.c; $ clang -cc1 -load ../../Debug+Asserts/lib/libPrintFunctionNames.so -plugin print-fns -plugin-arg-print-fns help -plugin-arg-print-fns --example-argument some-input-file.c; $ clang -cc1 -load ../../Debug+Asserts/lib/libPrintFunctionNames.so -plugin print-fns -plugin-arg-print-fns -an-error some-input-file.c. Mac:; $ clang -cc1 -load ../../Debug+Asserts/lib/libPrintFunctionNames.dylib -plugin print-fns some-input-file.c; $ clang -cc1 -load ../../Debug+Asserts/lib/libPrintFunctionNames.dylib -plugin print-fns -plugin-arg-print-fns help -plugin-arg-print-fns --example-argument some-input-file.c; $ clang -cc1 -load ../../Debug+Asserts/lib/libPrintFunctionNames.dylib -plugin print-fns -plugin-arg-print-fns -an-error some-input-file.c; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/examples/PrintFunctionNames/README.txt:241,load,load,241,interpreter/llvm-project/clang/examples/PrintFunctionNames/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/examples/PrintFunctionNames/README.txt,6,['load'],['load']
Performance,"This is an overloaded intrinsic. You can use ``llvm.udiv.fix``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.udiv.fix.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.udiv.fix.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.udiv.fix.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.udiv.fix.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.udiv.fix``' family of intrinsic functions perform unsigned; fixed point division on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. The arguments may also work with; int vectors of the same length and int size. ``%a`` and ``%b`` are the two; values that will undergo unsigned fixed point division. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs fixed point division on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. It is undefined behavior if the result value does not fit within the range of; the fixed point type, or if the second argument is zero. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.udiv.fix.i4(i4 6, i4 2, i32 0) ; %res = 3 (6 / 2 = 3); %res = call i4 @llvm.udiv.fix.i4(i4 6, i4 4, i32 1) ; %res = 3 (3 / 2 = 1.5); %res = call i4 @llvm.udiv.fix.i4(i4 1, i4 -8, i32 4) ; %res = 2 (0.0625 / 0.5 = 0.125). ; The result in the following could be rounded up to 1 or down to 0.5; %res = call i4 @llvm.udiv.fix.i4(i4 3, i4 4, i32 1) ; %res = 2 (or 1) (1.5 / 2 = 0.75). '``llvm.sdiv.fix.sat.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overlo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:632931,perform,performs,632931,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"This is the Minuit2 fitter standalone extractor, from the [ROOT] toolkit. It uses [CMake] 3.1+ to build.; See `README.md` for information about building Minuit2. ## Extracting from the ROOT source. To extract, run the following commands from the `math/minuit2/build` directory:. ```bash; cmake .. -Dminuit2_standalone=ON; ```. This will fill in the `math/minuit2` directory with all the files needed for Minuit2, copied from the corresponding ROOT files, as part of the configure step.; At this point, you could continue to build (using `make`). Note that the CMake option `minuit2_inroot` will automatically be set to `ON` if you are inside the ROOT source tree. Setting `minuit2_standalone` requires that this be inside the ROOT source tree. As always, any manual setting of a cached variable in CMake will be remembered as long as the `CMakeCache.txt` file is not removed. Remember that after building a tarball or a binary package you should remove the copied files using:. ```bash; make purge; ```. Otherwise git shows the file as untracked, unless you explicitly remove their tracking yourself with a .gitignore file. ## Building a tarball. Minuit2 standalone also has support for CPack to make installers for different platforms. To build a source package:. ```bash; make package_source; ```. This will create a source file in several formats that you can distribute. Reminder: You **must** have used `-Dminuit2_standalone=ON` when you configured CMake, or many of the files will be missing. ## Building a binary. To build a binary package (add other generators with `-G`):; ```bash; make; make package; ```. ## Maintenance. If new files are needed by Minuit2 due to additions to [ROOT], they should be added to the source files lists in `src/Math/CMakeLists.txt` and `src/Minuit2/CMakeLists.txt` (depending on if it's a new Math or Minuit2 requirement). For testing, the main `test/CMakeLists.txt` is used by ROOT, and the `test/*/CMakeLists.txt` files are used by the standalone build. ## How",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/minuit2/DEVELOP.md:779,cache,cached,779,math/minuit2/DEVELOP.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/minuit2/DEVELOP.md,1,['cache'],['cached']
Performance,"This view is disabled by default. .. option:: -instruction-info. Enable the instruction info view. This is enabled by default. .. option:: -show-encoding. Enable the printing of instruction encodings within the instruction info view. .. option:: -show-barriers. Enable the printing of LoadBarrier and StoreBarrier flags within the; instruction info view. .. option:: -all-stats. Print all hardware statistics. This enables extra statistics related to the; dispatch logic, the hardware schedulers, the register file(s), and the retire; control unit. This option is disabled by default. .. option:: -all-views. Enable all the view. .. option:: -instruction-tables. Prints resource pressure information based on the static information; available from the processor model. This differs from the resource pressure; view because it doesn't require that the code is simulated. It instead prints; the theoretical uniform distribution of resource pressure for every; instruction in sequence. .. option:: -bottleneck-analysis. Print information about bottlenecks that affect the throughput. This analysis; can be expensive, and it is disabled by default. Bottlenecks are highlighted; in the summary view. Bottleneck analysis is currently not supported for; processors with an in-order backend. .. option:: -json. Print the requested views in valid JSON format. The instructions and the; processor resources are printed as members of special top level JSON objects.; The individual views refer to them by index. However, not all views are; currently supported. For example, the report from the bottleneck analysis is; not printed out in JSON. All the default views are currently supported. .. option:: -disable-cb. Force usage of the generic CustomBehaviour and InstrPostProcess classes rather; than using the target specific implementation. The generic classes never; detect any custom hazards or make any post processing modifications to; instructions. .. option:: -disable-im. Force usage of the generic Instr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:6694,bottleneck,bottleneck-analysis,6694,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['bottleneck'],['bottleneck-analysis']
Performance,"ThreadSanitizer; ===============. Introduction; ------------. ThreadSanitizer is a tool that detects data races. It consists of a compiler; instrumentation module and a run-time library. Typical slowdown introduced by; ThreadSanitizer is about **5x-15x**. Typical memory overhead introduced by; ThreadSanitizer is about **5x-10x**. How to build; ------------. Build LLVM/Clang with `CMake <https://llvm.org/docs/CMake.html>`_. Supported Platforms; -------------------. ThreadSanitizer is supported on the following OS:. * Android aarch64, x86_64; * Darwin arm64, x86_64; * FreeBSD; * Linux aarch64, x86_64, powerpc64, powerpc64le; * NetBSD. Support for other 64-bit architectures is possible, contributions are welcome.; Support for 32-bit platforms is problematic and is not planned. Usage; -----. Simply compile and link your program with ``-fsanitize=thread``. To get a; reasonable performance add ``-O1`` or higher. Use ``-g`` to get file names; and line numbers in the warning messages. Example:. .. code-block:: console. % cat projects/compiler-rt/lib/tsan/lit_tests/tiny_race.c; #include <pthread.h>; int Global;; void *Thread1(void *x) {; Global = 42;; return x;; }; int main() {; pthread_t t;; pthread_create(&t, NULL, Thread1, NULL);; Global = 43;; pthread_join(t, NULL);; return Global;; }. $ clang -fsanitize=thread -g -O1 tiny_race.c. If a bug is detected, the program will print an error message to stderr.; Currently, ThreadSanitizer symbolizes its output using an external; ``addr2line`` process (this will be fixed in future). .. code-block:: bash. % ./a.out; WARNING: ThreadSanitizer: data race (pid=19219); Write of size 4 at 0x7fcf47b21bc0 by thread T1:; #0 Thread1 tiny_race.c:4 (exe+0x00000000a360). Previous write of size 4 at 0x7fcf47b21bc0 by main thread:; #0 main tiny_race.c:10 (exe+0x00000000a3b4). Thread T1 (running) created at:; #0 pthread_create tsan_interceptors.cc:705 (exe+0x00000000c790); #1 main tiny_race.c:9 (exe+0x00000000a3a4). ``__has_feature(thread_sanitizer",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSanitizer.rst:885,perform,performance,885,interpreter/llvm-project/clang/docs/ThreadSanitizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ThreadSanitizer.rst,1,['perform'],['performance']
Performance,"Thu Jun 26 14:43:04 CDT 2003. Information about BinInterface; ------------------------------. Take in a set of instructions with some particular register; allocation. It allows you to add, modify, or delete some instructions,; in SSA form (kind of like LLVM's MachineInstrs.) Then re-allocate; registers. It assumes that the transformations you are doing are safe.; It does not update the mapping information or the LLVM representation; for the modified trace (so it would not, for instance, support; multiple optimization passes; passes have to be aware of and update; manually the mapping information.). The way you use it is you take the original code and provide it to; BinInterface; then you do optimizations to it, then you put it in the; trace cache. The BinInterface tries to find live-outs for traces so that it can do; register allocation on just the trace, and stitch the trace back into; the original code. It has to preserve the live-ins and live-outs when; it does its register allocation. (On exits from the trace we have; epilogues that copy live-outs back into the right registers, but; live-ins have to be in the right registers.). Limitations of BinInterface; ---------------------------. It does copy insertions for PHIs, which it infers from the machine; code. The mapping info inserted by LLC is not sufficient to determine; the PHIs. It does not handle integer or floating-point condition codes and it; does not handle floating-point register allocation. It is not aggressively able to use lots of registers. There is a problem with alloca: we cannot find our spill space for; spilling registers, normally allocated on the stack, if the trace; follows an alloca(). What might be an acceptable solution would be to; disable trace generation on functions that have variable-sized; alloca()s. Variable-sized allocas in the trace would also probably; screw things up. Because of the FP and alloca limitations, the BinInterface is; completely disabled right now. Demo; ----. This is ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt:510,optimiz,optimization,510,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt,3,"['cache', 'optimiz']","['cache', 'optimization', 'optimizations']"
Performance,"To-do; -----. * Keep the address of the constant pool in a register instead of forming its; address all of the time.; * We can fold small constant offsets into the %hi/%lo references to constant; pool addresses as well.; * When in V9 mode, register allocate %icc[0-3].; * Add support for isel'ing UMUL_LOHI instead of marking it as Expand.; * Emit the 'Branch on Integer Register with Prediction' instructions. It's; not clear how to write a pattern for this though:. float %t1(int %a, int* %p) {; %C = seteq int %a, 0; br bool %C, label %T, label %F; T:; store int 123, int* %p; br label %F; F:; ret float undef; }. codegens to this:. t1:; save -96, %o6, %o6; 1) subcc %i0, 0, %l0; 1) bne .LBBt1_2 ! F; nop; .LBBt1_1: ! T; or %g0, 123, %l0; st %l0, [%i1]; .LBBt1_2: ! F; restore %g0, %g0, %g0; retl; nop. 1) should be replaced with a brz in V9 mode. * Same as above, but emit conditional move on register zero (p192) in V9; mode. Testcase:. int %t1(int %a, int %b) {; %C = seteq int %a, 0; %D = select bool %C, int %a, int %b; ret int %D; }. * Emit MULX/[SU]DIVX instructions in V9 mode instead of fiddling; with the Y register, if they are faster. * Codegen bswap(load)/store(bswap) -> load/store ASI. * Implement frame pointer elimination, e.g. eliminate save/restore for; leaf fns.; * Fill delay slots. * Use %g0 directly to materialize 0. No instruction is required.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt:1166,load,load,1166,interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt,4,['load'],['load']
Performance,"U C17 mode, so it uses standard C99; semantics for the inline keyword. These semantics are different; from those in GNU C89 mode, which is the default mode in versions of GCC; prior to 5.0. For example, consider the following code:. inline int add(int i, int j) { return i + j; }. int main() {; int i = add(4, 5);; return i;; }. In C99, inline means that a function's definition is; provided only for inlining, and that there is another definition; (without inline) somewhere else in the program. That; means that this program is incomplete, because if add; isn't inlined (for example, when compiling without optimization), then; main will have an unresolved reference to that other; definition. Therefore we'll get a (correct) link-time error like this:. Undefined symbols:; ""_add"", referenced from:; _main in cc-y1jXIr.o. By contrast, GNU C89 mode (used by default in older versions of GCC) is the; C89 standard plus a lot of extensions. C89 doesn't have an inline; keyword, but GCC recognizes it as an extension and just treats it as a hint to; the optimizer.; There are several ways to fix this problem:. Change add to a static inline; function. This is usually the right solution if only one; translation unit needs to use the function. static; inline functions are always resolved within the translation; unit, so you won't have to add a non-inline definition; of the function elsewhere in your program.; Remove the inline keyword from this definition of; add. The inline keyword is not required; for a function to be inlined, nor does it guarantee that it will be.; Some compilers ignore it completely. Clang treats it as a mild; suggestion from the programmer.; Provide an external (non-inline) definition; of add somewhere else in your program. The two; definitions must be equivalent!; Compile in the GNU C89 dialect by adding; -std=gnu89 to the set of Clang options. This option is; only recommended if the program source cannot be changed or if the; program also relies on additional C89-s",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/compatibility.html:2484,optimiz,optimizer,2484,interpreter/llvm-project/clang/www/compatibility.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/compatibility.html,2,['optimiz'],['optimizer']
Performance,"Used Technology; ---------------. `LLVM <https://llvm.org/>`_ is a free, open-source compiler infrastructure under; the `Apache License 2.0 <https://www.apache.org/licenses/LICENSE-2.0>`_. It is; designed as a collection of tools including Front Ends parsers, Middle Ends; optimizers, and Back Ends to produce machine code out of those programs. `Clang <https://clang.llvm.org/>`_ is a front-end that uses a LLVM; license. Clang works by taking the source language (e.g. C++) and translating it; into an intermediate representation that is then received by the compiler back; end (i.e., the LLVM backend). Its library-based architecture makes it relatively; easy to adapt Clang and build new tools based on it. Cling inherits a number of; features from LLVM and Clang, such as: fast compiling and low memory use,; efficient C++ parsing, extremely clear and concise diagnostics, Just-In-Time; compilation, pluggable optimizers, and support for `GCC <https://gcc.gnu.org/>`_; extensions. Interpreters allow for exploration of software development at the rate of human; thought. Nevertheless, interpreter code can be slower than compiled code due to; the fact that translating code at run time adds to the overhead and therefore; causes the execution speed to be slower. This issue is overcome by exploiting; the *Just-In-Time* (`JIT; <https://en.wikipedia.org/wiki/Just-in-time_compilation>`_) compilation method,; which allows an efficient memory management (for example, by evaluating whether; a certain part of the source code is executed often, and then compile this part,; therefore reducing the overall execution time). With the JIT approach, the developer types the code in Cling's command; prompt. The input code is then lowered to Clang, where is compiled and; eventually transformed in order to attach specific behavior. Clang compiles then; the input into an AST representation, that is then lowered to LLVM IR, an; `intermediate language; <https://en.wikipedia.org/wiki/Common_Intermediate_L",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/implementation.rst:273,optimiz,optimizers,273,interpreter/cling/docs/chapters/implementation.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/docs/chapters/implementation.rst,2,['optimiz'],['optimizers']
Performance,"V2 to V3, generate; code that can be loaded and executed in a process; with SRAMECC enabled. If not specified for code object V4 or above, generate; code that can be loaded and executed in a process; with either setting of SRAMECC. tgsplit ``-m[no-]tgsplit`` Enable/disable generating code that assumes; work-groups are launched in threadgroup split mode.; When enabled the waves of a work-group may be; launched in different CUs. wavefrontsize64 - ``-m[no-]wavefrontsize64`` Control the wavefront size used when; generating code for kernels. When disabled; native wavefront size 32 is used, when enabled; wavefront size 64 is used. xnack - ``-mcpu`` If specified, generate code that can only be; - ``--offload-arch`` loaded and executed in a process that has a; matching setting for XNACK replay. If not specified for code object V2 to V3, generate; code that can be loaded and executed in a process; with XNACK replay enabled. If not specified for code object V4 or above, generate; code that can be loaded and executed in a process; with either setting of XNACK replay. XNACK replay can be used for demand paging and; page migration. If enabled in the device, then if; a page fault occurs the code may execute; incorrectly unless generated with XNACK replay; enabled, or generated for code object V4 or above without; specifying XNACK replay. Executing code that was; generated with XNACK replay enabled, or generated; for code object V4 or above without specifying XNACK replay,; on a device that does not have XNACK replay; enabled will execute correctly but may be less; performant than code generated for XNACK replay; disabled.; =============== ============================ ==================================================. .. _amdgpu-target-id:. Target ID; ---------. AMDGPU supports target IDs. See `Clang Offload Bundler; <https://clang.llvm.org/docs/ClangOffloadBundler.html>`_ for a general; description. The AMDGPU target specific information is:. **processor**; Is an AMDGPU processo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:18370,load,loaded,18370,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loaded']
Performance,"VG. Only when; relative changes too large, redraw complete canvas again.; 3. Use touch-punch.min.js to process touch events with jquery-ui; 4. Even when several TH1/TGraph/TF1 objects with fill attribute overlap each other,; one able to get tooltip for underlying objects; 5. Use jquery-ui menu for context menu; 6. From context menu one could select several options for drawing; 7. Provide user interface for executing TTree::Draw on THttpServer; 8. 3D graphic (three.js) works only with IE11. ## Changes in 3.1; 1. Correctly show tooltips in case of overlapped objects; 2. Implement JSROOT.create() method to create supported; in JavaScript ROOT classes like TH1 or TGraph; 3. Fix problem with JSROOT.draw in HTML element with zero width (display:none); 4. Provide possibility to load user scripts with JSROOT.BuildSimpleGUI; and JSROOT.AssertPrerequisites, also with main index.htm; 5. Support of TCutG drawing; 6. Implement hierarchy display (former dtree) with jQuery; 7. Fix several problems in drawing optimization; 8. Implement dragging objects from hierarchy browser into existing canvas; to superimpose several objects; 9. Implement col2 and col3 draw options, using html5 canvas; 10. Support 'p' and 'p0' draw options for TH1 class. ## Development of version 3.0. ### November 2014; 1. Better font size and position in pave stats; 2. Resize/move of element only inside correspondent pad; 3. Adjust of frame size when Y-axis exceed pad limits; 4. Correct values in tooltip for THStack; 5. Exclude drawing of markers from TGraph outside visible range; 6. Drawing of canvas without TFrame object; 7. Many other small bug fixes and improvements, thanks to Maximilian Dietrich. ### October 2014; 1. Add ""shortcut icon""; 2. Add demo of online THttpServer - shell script copies data from; running httpserver.C macro on Apache webserver; 3. Evaluate 'monitoring' parameter for online server like:; <http://localhost:8080/?monitoring=1000>; Parameter defines how often displayed objects should be up",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/changes.md:69745,optimiz,optimization,69745,js/changes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/changes.md,1,['optimiz'],['optimization']
Performance,"VM debug; information provides the following guarantees:. * LLVM debug information **always provides information to accurately read; the source-level state of the program**, regardless of which LLVM; optimizations have been run. :doc:`HowToUpdateDebugInfo` specifies how debug; info should be updated in various kinds of code transformations to avoid; breaking this guarantee, and how to preserve as much useful debug info as; possible. Note that some optimizations may impact the ability to modify the; current state of the program with a debugger, such as setting program; variables, or calling functions that have been deleted. * As desired, LLVM optimizations can be upgraded to be aware of debugging; information, allowing them to update the debugging information as they; perform aggressive optimizations. This means that, with effort, the LLVM; optimizers could optimize debug code just as well as non-debug code. * LLVM debug information does not prevent optimizations from; happening (for example inlining, basic block reordering/merging/cleanup,; tail duplication, etc). * LLVM debug information is automatically optimized along with the rest of; the program, using existing facilities. For example, duplicate; information is automatically merged by the linker, and unused information; is automatically removed. Basically, the debug information allows you to compile a program with; ""``-O0 -g``"" and get full debug information, allowing you to arbitrarily modify; the program as it executes from a debugger. Compiling a program with; ""``-O3 -g``"" gives you full debug information that is always available and; accurate for reading (e.g., you get accurate stack traces despite tail call; elimination and inlining), but you might lose the ability to modify the program; and call functions which were optimized out of the program, or inlined away; completely. The :doc:`LLVM test-suite <TestSuiteMakefileGuide>` provides a framework to; test the optimizer's handling of debugging information. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:4774,optimiz,optimizations,4774,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,1,['optimiz'],['optimizations']
Performance,"VafConf_LocalPodLocation` variable must be set before the; > `PoD_env.sh` script gets sourced, so set it either in; > `common.before`, `local.before` or `local.conf`. Since PoD is; > usually system-wide installed, its location is normally; > system-wide set in either the `local.conf` file by the system; > administrator. `$VafConf_RemotePodLocation`; : Full path to the PoD installation on the VAF master node. *Note: this variable should be set in the configuration files for; the local environment despite it refers to a software present on the; remote nodes.*. `$VafConf_PodRms` *(optional)*; : Name of the Resource Management System used for submitting PoD jobs.; Run `pod-submit -l` to see the possible values. If not set, defaults to `condor`. `$VafConf_PodQueue` *(optional)*; : Queue name where to submit PoD jobs. If no queue has been given, the default one configured on your RMS; will be used. ### Remote environment configuration. All the PoD commands sent to the VAF master will live in the environment; loaded via using the following scripts. Similarly to the local environment, configuration is split in different files; to allow for a system-wide configuration, which has precedence over; user's configuration in the home directory. If a script cannot be found,; it will be silently skipped. - `<output_of_payload>`. - `common.before`. - `remote.before`. - `remote.conf`. - `common.after`. - `remote.after`. For an explanation on how to pass extra data to the workers safely; through the payload, see below. ### Payload: sending local files to the remote nodes. In many cases it is necessary to send some local data to the remote; workers: it is very common, for instance, to distribute a local Grid; authentication proxy on the remote workers to let them authenticate to; access a data storage. The `payload` file must be an executable generating some output that; will be prepended to the remote environment preparation. Differently; than the other environment scripts, it is not e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/UsingVirtualAnalysisFacility.md:4519,load,loaded,4519,proof/doc/confman/UsingVirtualAnalysisFacility.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/confman/UsingVirtualAnalysisFacility.md,1,['load'],['loaded']
Performance,"W: llvm .bc -> .bc modular optimizer and analysis printer. USAGE: opt [subcommand] [options] <input bitcode file>. OPTIONS:; Optimizations available:; ...; -guard-widening - Widen guards; -gvn - Global Value Numbering; -gvn-hoist - Early GVN Hoisting of Expressions; -hello - Hello World Pass; -indvars - Induction Variable Simplification; -inferattrs - Infer set function attributes; ... The pass name gets added as the information string for your pass, giving some; documentation to users of :program:`opt`. Now that you have a working pass,; you would go ahead and make it do the cool transformations you want. Once you; get it all working and tested, it may become useful to find out how fast your; pass is. The :ref:`PassManager <writing-an-llvm-pass-passmanager>` provides a; nice command line option (:option:`-time-passes`) that allows you to get; information about the execution time of your pass along with the other passes; you queue up. For example:. .. code-block:: console. $ opt -load lib/LLVMHello.so -hello -time-passes < hello.bc > /dev/null; Hello: __main; Hello: puts; Hello: main; ===-------------------------------------------------------------------------===; ... Pass execution timing report ...; ===-------------------------------------------------------------------------===; Total Execution Time: 0.0007 seconds (0.0005 wall clock). ---User Time--- --User+System-- ---Wall Time--- --- Name ---; 0.0004 ( 55.3%) 0.0004 ( 55.3%) 0.0004 ( 75.7%) Bitcode Writer; 0.0003 ( 44.7%) 0.0003 ( 44.7%) 0.0001 ( 13.6%) Hello World Pass; 0.0000 ( 0.0%) 0.0000 ( 0.0%) 0.0001 ( 10.7%) Module Verifier; 0.0007 (100.0%) 0.0007 (100.0%) 0.0005 (100.0%) Total. As you can see, our implementation above is pretty fast. The additional; passes listed are automatically inserted by the :program:`opt` tool to verify; that the LLVM emitted by your pass is still valid and well formed LLVM, which; hasn't been broken somehow. Now that you have seen the basics of the mechanics behind passes, we can",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst:9935,load,load,9935,interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMPass.rst,1,['load'],['load']
Performance,"WGP.; * Each WGP has a single LDS memory shared by the wavefronts of the work-groups; executing on it.; * All LDS operations of a WGP are performed as wavefront wide operations in a; global order and involve no caching. Completion is reported to a wavefront in; execution order.; * The LDS memory has multiple request queues shared by the SIMDs of a; WGP. Therefore, the LDS operations performed by different wavefronts of a; work-group can be reordered relative to each other, which can result in; reordering the visibility of vector memory operations with respect to LDS; operations of other wavefronts in the same work-group. A ``s_waitcnt; lgkmcnt(0)`` is required to ensure synchronization between LDS operations and; vector memory operations between wavefronts of a work-group, but not between; operations performed by the same wavefront.; * The vector memory operations are performed as wavefront wide operations.; Completion of load/store/sample operations are reported to a wavefront in; execution order of other load/store/sample operations performed by that; wavefront.; * The vector memory operations access a vector L0 cache. There is a single L0; cache per CU. Each SIMD of a CU accesses the same L0 cache. Therefore, no; special action is required for coherence between the lanes of a single; wavefront. However, a ``buffer_gl0_inv`` is required for coherence between; wavefronts executing in the same work-group as they may be executing on SIMDs; of different CUs that access different L0s. A ``buffer_gl0_inv`` is also; required for coherence between wavefronts executing in different work-groups; as they may be executing on different WGPs.; * The scalar memory operations access a scalar L0 cache shared by all wavefronts; on a WGP. The scalar and vector L0 caches are not coherent. However, scalar; operations are used in a restricted way so do not impact the memory model. See; :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory L0 caches use an L1 cache shared ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:336599,load,load,336599,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,3,"['load', 'perform']","['load', 'performed']"
Performance,"We discuss proposed implementation techniques for the x86 architecture and the; LLVM compiler. These are primarily to serve as an example, as other; implementation techniques are very possible. ### x86 Implementation Details. On the x86 platform we break down the implementation into three core; components: accumulating the predicate state through the control flow graph,; checking the loads, and checking control transfers between procedures. #### Accumulating Predicate State. Consider baseline x86 instructions like the following, which test three; conditions and if all pass, loads data from memory and potentially leaks it; through some side channel:; ```; # %bb.0: # %entry; pushq %rax; testl %edi, %edi; jne .LBB0_4; # %bb.1: # %then1; testl %esi, %esi; jne .LBB0_4; # %bb.2: # %then2; testl %edx, %edx; je .LBB0_3; .LBB0_4: # %exit; popq %rax; retq; .LBB0_3: # %danger; movl (%rcx), %edi; callq leak; popq %rax; retq; ```. When we go to speculatively execute the load, we want to know whether any of; the dynamically executed predicates have been misspeculated. To track that,; along each conditional edge, we need to track the data which would allow that; edge to be taken. On x86, this data is stored in the flags register used by the; conditional jump instruction. Along both edges after this fork in control flow,; the flags register remains alive and contains data that we can use to build up; our accumulated predicate state. We accumulate it using the x86 conditional; move instruction which also reads the flag registers where the state resides.; These conditional move instructions are known to not be predicted on any x86; processors, making them immune to misprediction that could reintroduce the; vulnerability. When we insert the conditional moves, the code ends up looking; like the following:; ```; # %bb.0: # %entry; pushq %rax; xorl %eax, %eax # Zero out initial predicate state.; movq $-1, %r8 # Put all-ones mask into a register.; testl %edi, %edi; jne .LBB0_1; # %bb.2: # ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:15597,load,load,15597,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['load']
Performance,"Wed Jun 25 15:13:51 CDT 2003. First-level instrumentation; ---------------------------. We use opt to do Bytecode-to-bytecode instrumentation. Look at; back-edges and insert llvm_first_trigger() function call which takes; no arguments and no return value. This instrumentation is designed to; be easy to remove, for instance by writing a NOP over the function; call instruction. Keep count of every call to llvm_first_trigger(), and maintain; counters in a map indexed by return address. If the trigger count; exceeds a threshold, we identify a hot loop and perform second-level; instrumentation on the hot loop region (the instructions between the; target of the back-edge and the branch that causes the back-edge). We; do not move code across basic-block boundaries. Second-level instrumentation; ---------------------------. We remove the first-level instrumentation by overwriting the CALL to; llvm_first_trigger() with a NOP. The reoptimizer maintains a map between machine-code basic blocks and; LLVM BasicBlock*s. We only keep track of paths that start at the; first machine-code basic block of the hot loop region. How do we keep track of which edges to instrument, and which edges are; exits from the hot region? 3 step process. 1) Do a DFS from the first machine-code basic block of the hot loop; region and mark reachable edges. 2) Do a DFS from the last machine-code basic block of the hot loop; region IGNORING back edges, and mark the edges which are reachable in; 1) and also in 2) (i.e., must be reachable from both the start BB and; the end BB of the hot region). 3) Mark BBs which end in edges that exit the hot region; we need to; instrument these differently. Assume that there is 1 free register. On SPARC we use %g1, which LLC; has agreed not to use. Shift a 1 into it at the beginning. At every; edge which corresponds to a conditional branch, we shift 0 for not; taken and 1 for taken into a register. This uniquely numbers the paths; through the hot region. Silently fail if w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:558,perform,perform,558,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,1,['perform'],['perform']
Performance,"When coming from their container, the ownership is totally; unpredictable. Again, the ownership of overlapping regions highly; depends on the current track parameters. We must say that even the overlaps of type *A)* and *B)* are allowed in case; the corresponding nodes are created using; TGeoVolume::AddNodeOverlap() method. Navigation is performed in such; cases by giving priority to the non-overlapping nodes. The modeller has; to perform an additional search through the overlapping candidates.; These are detected automatically during the geometry closing procedure; in order to optimize the algorithm, but we will stress that extensive; usage of this feature leads to a drastic deterioration of performance.; In the following we will focus on the non-declared overlaps of type *A)*; and *B)* since this is the main source of errors during tracking. These; are generally non-intended overlaps due to coding mistakes or bad; geometry design. The checking package is loaded together with the; painter classes and contains an automated overlap checker. \image html geometry008.png ""Overlap checking"". This can be activated both at volume level (checking for illegal; overlaps only one level inside a given volume) and from the geometry; manager level (checking full geometry):. ~~~{.cpp}; myVolume->CheckOverlaps(precision, option);; gGeoManager->CheckOverlaps(precision);; myNode->CheckOverlaps(precision);; ~~~. Here precision represents the desired maximum accepted overlap value in; centimeters (default value is 0.1). This tool checks all possible; significant pairs of candidates inside a given volume (not declared as; overlapping or division volumes). The check is performed by verifying; the mesh representation of one candidate against the shape of the other.; This sort of check cannot identify all possible overlapping topologies,; but it works for more than 95% and is much faster than the usual; shape-to-shape comparison. For a 100% reliability, one can perform the; check at the lev",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:92759,load,loaded,92759,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['load'],['loaded']
Performance,"When coming from their container, the ownership is totally; unpredictable. Again, the ownership of overlapping regions highly; depends on the current track parameters. We must say that even the overlaps of type A) and B) are allowed in case; the corresponding nodes are created using; **`TGeoVolume`**`::AddNodeOverlap()` method. Navigation is performed in such; cases by giving priority to the non-overlapping nodes. The modeller has; to perform an additional search through the overlapping candidates.; These are detected automatically during the geometry closing procedure; in order to optimize the algorithm, but we will stress that extensive; usage of this feature leads to a drastic deterioration of performance.; In the following we will focus on the non-declared overlaps of type A); and B) since this is the main source of errors during tracking. These; are generally non-intended overlaps due to coding mistakes or bad; geometry design. The checking package is loaded together with the; painter classes and contains an automated overlap checker.**. ![Overlap checking](pictures/030001DF.png). This can be activated both at volume level (checking for illegal; overlaps only one level inside a given volume) and from the geometry; manager level (checking full geometry):. ``` {.cpp}; myVolume->CheckOverlaps(precision, option);; gGeoManager->CheckOverlaps(precision);; myNode->CheckOverlaps(precision);; ```. Here precision represents the desired maximum accepted overlap value in; centimeters (default value is 0.1). This tool checks all possible; significant pairs of candidates inside a given volume (not declared as; overlapping or division volumes). The check is performed by verifying; the mesh representation of one candidate against the shape of the other.; This sort of check cannot identify all possible overlapping topologies,; but it works for more than 95% and is much faster than the usual; shape-to-shape comparison. For a 100% reliability, one can perform the; check at the lev",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:132277,load,loaded,132277,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['load'],['loaded']
Performance,"While we could make an; alloca for it, it is actually easier to create a PHI node for it, so we; still just make the PHI. Here is the code after the mem2reg pass runs:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then:; br label %ifcont. else:; %subtmp = fsub double %x, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %subtmp5 = fsub double %x, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. This is a trivial case for mem2reg, since there are no redefinitions of; the variable. The point of showing this is to calm your tension about; inserting such blatant inefficiencies :). After the rest of the optimizers run, we get:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp ueq double %booltmp, 0.000000e+00; br i1 %ifcond, label %else, label %ifcont. else:; %subtmp = fsub double %x, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %subtmp5 = fsub double %x, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; ret double %addtmp. ifcont:; ret double 1.000000e+00; }. Here we see that the simplifycfg pass decided to clone the return; instruction into the end of the 'else' block. This allowed it to; eliminate some branches and the PHI node. Now that all symbol table references are updated to use stack variables,; we'll add the assignment operator. New Assignment Operator; =======================. With our current framework, adding a new assignment operator is really; simple. We will parse it just like any other binar",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:19243,optimiz,optimizers,19243,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['optimiz'],['optimizers']
Performance,"With the exception of retains done as part of initializing a ``__strong``; parameter variable or reading a ``__weak`` variable, whenever these semantics; call for retaining a value of block-pointer type, it has the effect of a; ``Block_copy``. The optimizer may remove such copies when it sees that the; result is used only as an argument to a call. When a block pointer type is converted to a non-block pointer type (such as; ``id``), ``Block_copy`` is called. This is necessary because a block allocated; on the stack won't get copied to the heap when the non-block pointer escapes.; A block pointer is implicitly converted to ``id`` when it is passed to a; function as a variadic argument. .. _arc.misc.exceptions:. Exceptions; ----------. By default in Objective C, ARC is not exception-safe for normal releases:. * It does not end the lifetime of ``__strong`` variables when their scopes are; abnormally terminated by an exception.; * It does not perform releases which would occur at the end of a; full-expression if that full-expression throws an exception. A program may be compiled with the option ``-fobjc-arc-exceptions`` in order to; enable these, or with the option ``-fno-objc-arc-exceptions`` to explicitly; disable them, with the last such argument ""winning"". .. admonition:: Rationale. The standard Cocoa convention is that exceptions signal programmer error and; are not intended to be recovered from. Making code exceptions-safe by; default would impose severe runtime and code size penalties on code that; typically does not actually care about exceptions safety. Therefore,; ARC-generated code leaks by default on exceptions, which is just fine if the; process is going to be immediately terminated anyway. Programs which do care; about recovering from exceptions should enable the option. In Objective-C++, ``-fobjc-arc-exceptions`` is enabled by default. .. admonition:: Rationale. C++ already introduces pervasive exceptions-cleanup code of the sort that ARC; introduces. C++ ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:97896,perform,perform,97896,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['perform']
Performance,"X axis. The return histogram will have as bin error an approximate error on the quantile assuming a normal distribution of the; bin contents in the other axis. - Update Projection methods of both TH2 and TH3 to not return a null; pointer when an histogram with the same name already existed and it; was not compatible. Now just set the new correct binning on the; previously existing histogram. ### TGraph. - `TGraph::Draw()` needed at least the option `AL` to draw the graph; axis even when there was no active canvas or when the active canvas; did not have any axis defined. This was counter-intuitive. Now if; `TGraph::Draw()` is invoked without parameter and if there is no; axis defined in the current canvas, the option `ALP` is automatically; set.; - Change `SavePrimtive()` to improve speed compilation on generated macros. ### TGraph2D. - When `GetX(YZ)axis` were called on a `TGraph2D`, the frame limit and; plotting options were changed.; - Modify the `Clear` function in order to be able to reuse a; `TGraph2D` after a `Clear` is performed.; - In `GetHistogram()` the lower and higher axis limits are always; different.; - Protection added to avoid a Seg Fault on `.q` when `SetHistogram()`; is called on a `TGraph2D`. ### TMultiGraph. - In `TMultiGraph::Add(TMultiGraph *multigraph, Option_t *chopt)`; If `chopt` is defined all the graphs in `multigraph` will be added; with the `chopt` option. If `chopt` is undefined each graph will; be added with the option it had in `multigraph`.; - The option ""A"" in the `Draw()` was not cleaning properly the; current pad.; - Implement this option `pads`. This option is equivalent to the one in; `THStack`. It allows to draw all the `TGraphs` in separated pads. ### THStack. - By default the background of the histograms is erased before drawing the; histograms. The new option `noclear` avoid this behaviour. This is useful; when drawing a `THStack` on top of an other plot. If the patterns used to; draw the histograms in the stack are transpar",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v600/index.md:11293,perform,performed,11293,hist/doc/v600/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/doc/v600/index.md,1,['perform'],['performed']
Performance,"X, X`` is represented as ``mul X, 2`` ⇒ ``shl X, 1``; #. Multiplies with a constant power-of-two argument are transformed into; shifts.; #. … etc. This pass can also simplify calls to specific well-known function calls (e.g.; runtime library functions). For example, a call ``exit(3)`` that occurs within; the ``main()`` function can be transformed into simply ``return 3``. Whether or; not library calls are simplified is controlled by the; :ref:`-function-attrs <passes-function-attrs>` pass and LLVM's knowledge of; library calls on different targets. .. _passes-aggressive-instcombine:. ``aggressive-instcombine``: Combine expression patterns; --------------------------------------------------------. Combine expression patterns to form expressions with fewer, simple instructions. For example, this pass reduce width of expressions post-dominated by TruncInst; into smaller width when applicable. It differs from instcombine pass in that it can modify CFG and contains pattern; optimization that requires higher complexity than the O(1), thus, it should run fewer; times than instcombine pass. ``internalize``: Internalize Global Symbols; -------------------------------------------. This pass loops over all of the functions in the input module, looking for a; main function. If a main function is found, all other functions and all global; variables with initializers are marked as internal. ``ipsccp``: Interprocedural Sparse Conditional Constant Propagation; -------------------------------------------------------------------. An interprocedural variant of :ref:`Sparse Conditional Constant Propagation; <passes-sccp>`. ``jump-threading``: Jump Threading; ----------------------------------. Jump threading tries to find distinct threads of control flow running through a; basic block. This pass looks at blocks that have multiple predecessors and; multiple successors. If one or more of the predecessors of the block can be; proven to always cause a jump to one of the successors, we forwa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:21236,optimiz,optimization,21236,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['optimiz'],['optimization']
Performance,"X1=3.000000 Y1=4.000000 X2=0.000000 Y2=0.000000; TLine X1=4.000000 Y1=5.000000 X2=0.000000 Y2=0.000000; root [] .q; ```. Here we note:. - A multi-line command starts with a { and ends with a }.; - Inside continuation, every line has to be correctly terminated with a ; (like in ""real''; C++).; - All objects are created in *global* scope.; - There is no way to back up; you are better off writing a script.; - Use `.q` to exit root. ## Feeding Sources Files To ROOT: C++ Scripts. ROOT script files (often called ""Macros"") contain pure C++ code. They can contain a simple; sequence of statements like in the multi command line example given; above, but also arbitrarily complex class and function definitions. The most frequent interaction with the ROOT prompt uses `.x` to ""run"" a file:. ```; root [] .x myScript.C; ```. This loads myScript.C into the interpreter and calls the function `myScript()`.; You can pass arguments using `.x myScript.C(12, ""A String"")`. Alternatively you can load the script and then run a function explicitly:. ```; root [] .L myScript.C; root [] myScript(); ```. The above is equivalent to `.x myScript.C`. In a named script, the objects created on the stack are deleted when; the function exits. In a common scenario you; create a histogram in a named script on the stack. You draw the; histogram, but when the function exits the canvas is empty and the; histogram has disappeared. To avoid the histogram from disappearing you can; create it on the heap (by using new). This will leave the histogram; object intact, but the pointer in the named script scope will be; ""gone"". Since histograms (and trees) are added to the list of objects; in the current directory, you can always retrieve them to delete them; if needed. ``` {.cpp}; root[] TH1F *h = (TH1F*)gDirectory->Get(""myHist""); // or; root[] TH1F *h = (TH1F*)gDirectory->GetList()->FindObject(""myHist"");; ```. In addition, histograms and trees are automatically deleted when the; current directory is closed. This wi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md:8041,load,load,8041,documentation/users-guide/Cling.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md,1,['load'],['load']
Performance,"X86). .. option:: --enable-no-infs-fp-math. Enable optimizations that assume no Inf values. .. option:: --enable-no-nans-fp-math. Enable optimizations that assume no NAN values. .. option:: --enable-no-signed-zeros-fp-math. Enable FP math optimizations that assume the sign of 0 is insignificant. .. option:: --enable-no-trapping-fp-math. Enable setting the FP exceptions build attribute not to use exceptions. .. option:: --enable-unsafe-fp-math. Enable optimizations that make unsafe assumptions about IEEE math (e.g. that; addition is associative) or may not work for all input ranges. These; optimizations allow the code generator to make use of some instructions which; would otherwise not be usable (such as ``fsin`` on X86). .. option:: --stats. Print statistics recorded by code-generation passes. .. option:: --time-passes. Record the amount of time needed for each pass and print a report to standard; error. .. option:: --load=<dso_path>. Dynamically load ``dso_path`` (a path to a dynamically shared object) that; implements an LLVM target. This will permit the target name to be used with; the :option:`-march` option so that code can be generated for that target. .. option:: -meabi=[default|gnu|4|5]. Specify which EABI version should conform to. Valid EABI versions are *gnu*,; *4* and *5*. Default value (*default*) depends on the triple. .. option:: -stack-size-section. Emit the .stack_sizes section which contains stack size metadata. The section; contains an array of pairs of function symbol values (pointer size) and stack; sizes (unsigned LEB128). The stack size values only include the space allocated; in the function prologue. Functions with dynamic stack allocations are not; included. .. option:: -remarks-section. Emit the __remarks (MachO) section which contains metadata about remark; diagnostics. Tuning/Configuration Options; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~. .. option:: --print-after-isel. Print generated machine code after instruction selection (useful for debugging)",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst:4347,load,load,4347,interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,1,['load'],['load']
Performance,"Y`` is defined in your Makefile, a shared (or dynamic); library will be built. Variables for Building Programs; -------------------------------. ``TOOLNAME``. This variable contains the name of the program that will be built. For; example, to build an executable named ``sample``, ``TOOLNAME`` should be set; to ``sample``. ``USEDLIBS``. This variable holds a space separated list of libraries that should be; linked into the program. These libraries must be libraries that come from; your **lib** directory. The libraries must be specified without their; ``lib`` prefix. For example, to link ``libsample.a``, you would set; ``USEDLIBS`` to ``sample.a``. Note that this works only for statically linked libraries. ``LLVMLIBS``. This variable holds a space separated list of libraries that should be; linked into the program. These libraries must be LLVM libraries. The; libraries must be specified without their ``lib`` prefix. For example, to; link with a driver that performs an IR transformation you might set; ``LLVMLIBS`` to this minimal set of libraries ``LLVMSupport.a LLVMCore.a; LLVMBitReader.a LLVMAsmParser.a LLVMAnalysis.a LLVMTransformUtils.a; LLVMScalarOpts.a LLVMTarget.a``. Note that this works only for statically linked libraries. LLVM is split; into a large number of static libraries, and the list of libraries you; require may be much longer than the list above. To see a full list of; libraries use: ``llvm-config --libs all``. Using ``LINK_COMPONENTS`` as; described below, obviates the need to set ``LLVMLIBS``. ``LINK_COMPONENTS``. This variable holds a space separated list of components that the LLVM; ``Makefiles`` pass to the ``llvm-config`` tool to generate a link line for; the program. For example, to link with all LLVM libraries use; ``LINK_COMPONENTS = all``. ``LIBS``. To link dynamic libraries, add ``-l<library base name>`` to the ``LIBS``; variable. The LLVM build system will look in the same places for dynamic; libraries as it does for static libraries. For e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Projects.rst:6702,perform,performs,6702,interpreter/llvm-project/llvm/docs/Projects.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Projects.rst,1,['perform'],['performs']
Performance,"[2:3], s[4:5], s[6:7]; s_cselect_b32 s1, s2, s3; s_andn2_b32 s2, s4, s6; s_lshr_b64 s[2:3], s[4:5], s6; s_ashr_i32 s2, s4, s6; s_bfm_b64 s[2:3], s4, s6; s_bfe_i64 s[2:3], s[4:5], s6; s_cbranch_g_fork s[4:5], s[6:7]. For full list of supported instructions, refer to ""SOP2 Instructions"" in ISA; Manual. SOPC; ++++. .. code-block:: nasm. s_cmp_eq_i32 s1, s2; s_bitcmp1_b32 s1, s2; s_bitcmp0_b64 s[2:3], s4; s_setvskip s3, s5. For full list of supported instructions, refer to ""SOPC Instructions"" in ISA; Manual. SOPP; ++++. .. code-block:: nasm. s_barrier; s_nop 2; s_endpgm; s_waitcnt 0 ; Wait for all counters to be 0; s_waitcnt vmcnt(0) & expcnt(0) & lgkmcnt(0) ; Equivalent to above; s_waitcnt vmcnt(1) ; Wait for vmcnt counter to be 1.; s_sethalt 9; s_sleep 10; s_sendmsg 0x1; s_sendmsg sendmsg(MSG_INTERRUPT); s_trap 1. For full list of supported instructions, refer to ""SOPP Instructions"" in ISA; Manual. Unless otherwise mentioned, little verification is performed on the operands; of SOPP Instructions, so it is up to the programmer to be familiar with the; range or acceptable values. VALU; ++++. For vector ALU instruction opcodes (VOP1, VOP2, VOP3, VOPC, VOP_DPP, VOP_SDWA),; the assembler will automatically use optimal encoding based on its operands. To; force specific encoding, one can add a suffix to the opcode of the instruction:. * _e32 for 32-bit VOP1/VOP2/VOPC; * _e64 for 64-bit VOP3; * _dpp for VOP_DPP; * _e64_dpp for VOP3 with DPP; * _sdwa for VOP_SDWA. VOP1/VOP2/VOP3/VOPC examples:. .. code-block:: nasm. v_mov_b32 v1, v2; v_mov_b32_e32 v1, v2; v_nop; v_cvt_f64_i32_e32 v[1:2], v2; v_floor_f32_e32 v1, v2; v_bfrev_b32_e32 v1, v2; v_add_f32_e32 v1, v2, v3; v_mul_i32_i24_e64 v1, v2, 3; v_mul_i32_i24_e32 v1, -3, v3; v_mul_i32_i24_e32 v1, -100, v3; v_addc_u32 v1, s[0:1], v2, v3, s[2:3]; v_max_f16_e32 v1, v2, v3. VOP_DPP examples:. .. code-block:: nasm. v_mov_b32 v0, v0 quad_perm:[0,2,1,1]; v_sin_f32 v0, v0 row_shl:1 row_mask:0xa bank_mask:0x1 bound_ctrl:0; v_mov_b32 v0, v0",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:428093,perform,performed,428093,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"[] AStruct<float> S0; // #1: implicit loading of libA. Full descriptor required.; root [] AStruct<float>* S1; // #2: implicit loading of libA. No full descriptor required.; root [] if (gFile) S1->doIt(); // #3: implicit loading of libA. Full descriptor required.; root [] gSystem->Load(""libA""); // #4: explicit loading of libA. No full descriptor required.; root [] do(); // #5: error: implicit loading of libA is currently unsupported. ```. This pattern is not only used in the ROOT prompt but in I/O hotspots such as; `ShowMembers` and `TClass::IsA`. A naive implementation of this feature would require inclusion of all reachable; library descriptors (aka header files) at ROOT startup time. Of course this is; not feasible and ROOT inserts a set of optimizations to fence itself from the; costly full header inclusion. Unfortunately, several of them are home-grown and; in a few cases inaccurate (eg line #5) causing a noticeable technical debt. Here we will briefly describe the three common layers of optimizations: ROOT PCH,; ROOTMAP and RDICT. The ROOT precompiled header (PCH) reduces the CPU and memory cost for ROOT's; most used libraries. The precompiled header technology is well-understood since; decades [[4]]. It is an efficient on-disk representation of the state of the; compiler after parsing a set of headers. It can be loaded before starting the; next instance to avoid doing redundant work. At build time, rootcling (ROOT's; dictionary generator) creates such PCH file which is attached at ROOT startup; time. Its major drawback is the fact that if third-party users want to include; their libraries, they have to recompile it every time there is a change. RDICT files store some useful information (in particular about class offsets) in; ROOT files to avoid the potentially expensive call to the interpreter if the; information is not the PCH. For example, ROOT's libGeom and other third-party; code. This is done to circumvent the costly call to `ShowMembers` which will; requ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:5823,optimiz,optimizations,5823,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,1,['optimiz'],['optimizations']
Performance,"\ s now keep two operands.; The first one, the defining access, is; always the previous ``MemoryDef`` or ``MemoryPhi`` in the same basic block, or; the last one in a dominating predecessor if the current block doesn't have any; other accesses writing to memory. This is needed for walking Def chains.; The second operand is the optimized access, if there was a previous call on the; walker's ``getClobberingMemoryAccess(MA)``. This API will cache information; as part of ``MA``.; Optimizing all ``MemoryDef``\ s has quadratic time complexity and is not done; by default. A walk of the uses for any MemoryDef can find the accesses that were optimized; to it.; A code snippet for such a walk looks like this:. .. code-block:: c++. MemoryDef *Def; // find who's optimized or defining for this MemoryDef; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *DefUser = cast_of_null<MemoryDef>MA); if (DefUser->isOptimized() && DefUser->getOptimized() == Def) {; // User who is optimized to Def; } else {; // User who's defining access is Def; optimized to something else or not optimized.; }; }. When ``MemoryUse``\ s are optimized, for a given store, you can find all loads; clobbered by that store by walking the immediate and transitive uses of; the store. .. code-block:: c++. checkUses(MemoryAccess *Def) { // Def can be a MemoryDef or a MemoryPhi.; for (auto& U : Def->uses()) {; MemoryAccess *MA = cast<MemoryAccess>(Use.getUser());; if (auto *MU = cast_of_null<MemoryUse>MA) {; // Process MemoryUse as needed.; }; else {; // Process MemoryDef or MemoryPhi as needed. // As a user can come up twice, as an optimized access and defining; // access, keep a visited list. // Check transitive uses as needed; checkUses (MA); // use a worklist for an iterative algorithm; }; }; }. An example of similar traversals can be found in the DeadStoreElimination pass. Invalidation and updating; -------------------------. Because ``MemorySSA`` keeps track of LLVM IR,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:13168,optimiz,optimized,13168,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,3,['optimiz'],['optimized']
Performance,"\\0"" |; +---------+------------+-----------------------------------------------------------------------------+. If debugging information is enabled, we will also create strings to indicate the; names and declarations of variables mapped in target regions. These have the; same format as the source location in the :ref:`identifier structure; <table-ident_t_structure>`, but the function name is replaced with the variable; name. .. _Device Compilation:. Offload Device Compilation; --------------------------. The input file is compiled for each active device toolchain. The device; compilation stage is performed differently from the host stage. Namely, we do; not generate any offloading entries. This is set by passing the; ``-fopenmp-is-target-device`` flag to the front-end. We use the host bitcode to; determine which symbols to export from the device. The bitcode file is passed in; from the previous stage using the ``-fopenmp-host-ir-file-path`` flag.; Compilation is otherwise performed as it would be for any other target triple. When compiling for the OpenMP device, we set the visibility of all device; symbols to be ``protected`` by default. This improves performance and prevents a; class of errors where a symbol in the target device could preempt a host; library. The OpenMP runtime library is linked in during compilation to provide the; implementations for standard OpenMP functionality. For GPU targets this is done; by linking in a special bitcode library during compilation, (e.g.; ``libomptarget-nvptx64-sm_70.bc``) using the ``-mlink-builtin-bitcode`` flag.; Other device libraries, such as CUDA's libdevice, are also linked this way. If; the target is a standard architecture with an existing ``libomp``; implementation, that will be linked instead. Finally, device tools are used to; create a relocatable device object file that can be embedded in the host. .. _Creating Fat Objects:. Creating Fat Objects; --------------------. A fat binary is a binary file that contains i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst:10657,perform,performed,10657,interpreter/llvm-project/clang/docs/OffloadingDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/OffloadingDesign.rst,1,['perform'],['performed']
Performance,"\addtogroup tutorial_dataframe. @{. [RDataFrame](classROOT_1_1RDataFrame.html) offers a high level interface for the analysis of data stored in [TTree](classTTree.html)s, [CSV files](classROOT_1_1RDF_1_1RCsvDS.html) and [other data formats](classROOT_1_1RDF_1_1RDataSource.html). In addition, multi-threading and other low-level optimisations allow users to exploit all the resources available on their machines transparently. In a nutshell:; ~~~{.cpp}; ROOT::EnableImplicitMT(); // Enable ROOT's implicit multi-threading; ROOT::RDataFrame d(""myTree"", ""file_*.root""); // Interface to TTree and TChain; auto histoA = d.Histo1D(""Branch_A""); // Book the filling of a histogram; auto histoB = d.Histo1D(""Branch_B""); // Book the filling of another histogram; // Data processing is triggered by the next line, which accesses a booked result for the first time; // All booked results are evaluated during the same parallel event loop.; histoA->Draw(); // <-- event loop runs here!; histoB->Draw(); // HistoB has already been filled, no event loop is run here; ~~~. Explore the examples below or go to [RDataFrame's user guide](classROOT_1_1RDataFrame.html). @}",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/index.md:293,multi-thread,multi-threading,293,tutorials/dataframe/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/dataframe/index.md,2,['multi-thread'],['multi-threading']
Performance,"\defgroup Geometry The Geometry Package. The %ROOT geometry package is a tool for building, browsing,; navigating and visualizing detector geometries. The code works; standalone with respect to any tracking Monte-Carlo engine; therefore,; it does not contain any constraints related to physics. However, the; navigation features provided by the package are designed to optimize; particle transport through complex geometries, working in correlation; with simulation packages such as GEANT3, GEANT4 and FLUKA. - [Quick Start: Creating the world](\ref GP00); - [Example 1: Creating the World](\ref GP00a); - [Example 2: A Geometrical Hierarchy Look and Feel](\ref GP00b); - [Selecting the System of Units in ROOT](\ref GPUNITS); - [Geometry Creation](\ref GP01); - [The Volume Hierarchy](\ref GP01a); - [Creating and Positioning Volumes](\ref GP01b); - [Making Volumes](\ref GP01ba); - [Example of Volume Creation](\ref GP01bb); - [Positioned Volumes (Nodes)](\ref GP01bc); - [Virtual Containers and Assemblies of Volumes](\ref GP01bd); - [Examples of Volume Positioning](\ref GP01be); - [Overlapping Volumes](\ref GP01bf); - [Replicating Volumes](\ref GP01bg); - [Volume Families](\ref GP01bh); - [Dividing Volumes](\ref GP01bi); - [Volume Assemblies](\ref GP01bj); - [Geometrical Transformations](\ref GP01c); - [Matrix Creation Example](\ref GP01ca); - [Rule for Creation of Transformations](\ref GP01cb); - [Available Geometrical Transformations](\ref GP01cc); - [Ownership of Geometry Objects](\ref GP01d); - [Navigation and Tracking](\ref GP02); - [TGeoNavigator Class](\ref GP02a); - [Initializing the Starting Point](\ref GP02b); - [Initializing the Direction](\ref GP02c); - [Initializing the State](\ref GP02d); - [Checking the Current State](\ref GP02e); - [Saving and Restoring the Current State](\ref GP02f); - [Navigation Queries](\ref GP02g); - [Finding If Current State Is Changed For a New Point](\ref GP02ga); - [Finding the Distance to the Next Boundary](\ref GP02gb); - [Computing th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:369,optimiz,optimize,369,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['optimiz'],['optimize']
Performance,"\defgroup Graphics3D 3D Graphics; \ingroup Graphics; \brief The 3D graphics related classes; \defgroup TEve Event Display; \ingroup Graphics3D; \brief The Event Display classes. Eve is a ROOT module based on experiment-independent part of the; ALICE event display developed in cooperation between ALICE offline; project and ROOT during the last two years. It has been used in; ALICE for more than a year to perform high-level event; visualization, debugging of simulation and reconstruction code as; well as for raw-data visualization. Papers describing Eve (older ones still using the old name - Reve):. - [EVE - Event Visualization Environment of the ROOT framework]; (http://pos.sissa.it//archive/conferences/070/103/ACAT08_103.pdf); presented at ACAT 2008. - [Event Visualization Environment of the ALICE experiment]; (http://indico.cern.ch/contributionDisplay.py?contribId=25&confId=13356); presented at ROOT Workshop 2007. - [Raw-data display and visual reconstruction validation in ALICE]; (http://indico.cern.ch/contributionDisplay.py?contribId=442&sessionId=23&confId=3580); presented at CHEP 2007. Eve is built on top of ROOT's GUI, GL and GED infrastructure and; delivers the following main features:. - Base-classes for representation of visual objects that can; be presented in list-tree views, object-editors and rendered; via OpenGL (TEveElement and sub-classes). - Application manager class TEveManager for top-level; management of elements, GUI components, geometries and events;. - Classes for presentation of full TGeo geometries; (TEveGeoNode and TEveGeoTopNode) as well as of; simplifed geometries via extraction of shape-data; (TEveGeoShape). \image html eve_cmsgeo.png ""CMS geometry"". - Classes for presentation of trajectories or tracks; (TEveTrack, TEveTrackPropagator) and hits or; clusters (TEvePointSet, TEvePointSetArray). \image html eve_alice3d.png ""A simulated ALICE pp@14TeV event in 3D"". - Base-classes for presentation of raw-data or digits; (TEveDigitSet, TEveQuadS",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf3d/eve/doc/index.md:407,perform,perform,407,graf3d/eve/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf3d/eve/doc/index.md,1,['perform'],['perform']
Performance,"\defgroup Matrix Matrix Linear Algebra; \ingroup Math; \brief The %ROOT Matrix Linear Algebra package. The %ROOT linear algebra package provides a complete environment in %ROOT to perform matrix; calculations such as matrix-vector and matrix-matrix multiplications and other linear; algebra calculations like equation solving and eigenvalue decompositions. The present package implements all the basic algorithms dealing; with vectors, matrices, matrix columns, rows, diagonals, etc.; In addition eigen-Vector analysis and several matrix decomposition; have been added (LU,QRH,Cholesky,Bunch-Kaufman and SVD) .; The decompositions are used in matrix inversion, equation solving. ### Matrix classes. %ROOT provides the following matrix classes, among others:. - `TMatrixDBase`. - `TMatrixF`. - `TMatrixFSym`. - `TVectorF`. - `TMatrixD`. - `TMatrixDSym`. - `TMatrixDSparse`. - `TDecompBase`. - `TDecompChol`. For a dense matrix, elements are arranged in memory in a ROW-wise; fashion . For (n x m) matrices where n*m <=kSizeMax (=25 currently); storage space is available on the stack, thus avoiding expensive; allocation/deallocation of heap space . However, this introduces of; course kSizeMax overhead for each matrix object . If this is an; issue recompile with a new appropriate value (>=0) for kSizeMax. Sparse matrices are also stored in row-wise fashion but additional; row/column information is stored, see TMatrixTSparse source for; additional details . Another way to assign and store matrix data is through Use; see for instance stressLinear.cxx file . Unless otherwise specified, matrix and vector indices always start; with 0, spanning up to the specified limit-1. However, there are; constructors to which one can specify arbitrary lower and upper; bounds, e.g. TMatrixD m(1,10,1,5) defines a matrix that ranges; from 1..10, 1..5 (a(1,1)..a(10,5)). #### Matrix properties. A matrix has five properties, which are all set in the constructor:. - `precision` <br>; If the `precision` is floa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/matrix/doc/index.md:180,perform,perform,180,math/matrix/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/matrix/doc/index.md,1,['perform'],['perform']
Performance,"\defgroup SMatrixGroup SMatrix Package; \ingroup Math. **SMatrix** is a C++ package for high performance vector and matrix computations. It can be; used only in problems when the size of the matrices is known at compile time, like in the; tracking reconstruction of HEP experiments. It is based on a C++ technique, called expression; templates, to achieve an high level optimization. The C++ templates can be used to implement; vector and matrix expressions such that these expressions can be transformed at compile time; to code which is equivalent to hand optimized code in a low-level language like FORTRAN or; C (see for example ref. 1). The SMatrix has been developed initially by T. Glebe of the Max-Planck-Institut, Heidelberg,; as part of the HeraB analysis framework. A subset of the original package has been now incorporated; in the %ROOT distribution, with the aim to provide to the LHC experiments a stand-alone and; high performant matrix package for reconstruction. The API of the current package differs; from the original one, in order to be compliant to the %ROOT coding conventions. SMatrix contains generic \ref SMatrixSVector to describe matrix and vector of arbitrary; dimensions and of arbitrary type. The classes are templated on the scalar type and on the; size of the matrix (number of rows and columns) or the vector. Therefore, the size has to; be known at compile time. Since the release 5.10, SMatrix supports symmetric matrices using; a storage class (ROOT::Math::MatRepSym) which contains only the N*(N+1)/2 independent element; of a NxN symmetric matrix.; It is not in the mandate of this package to provide a complete linear algebra functionality; for these classes. What is provided are basic \ref MatrixFunctions and \ref VectFunction,; such as the matrix-matrix, matrix-vector, vector-vector operations, plus some extra; functionality for square matrices, like inversion, which is based on the optimized Cramer; method for squared matrices of size up to 6x6, and d",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/smatrix/doc/index.md:93,perform,performance,93,math/smatrix/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/smatrix/doc/index.md,4,"['optimiz', 'perform']","['optimization', 'optimized', 'performance', 'performant']"
Performance,"\defgroup roofit_dev_docs_ad How to extend the use of Automatic Differentiation in RooFit; \ingroup roofit_dev_docs; \date October 2023; \brief Developer guide on how to add support for Automatic Differentiation via code generation. # How to extend the use of Automatic Differentiation in RooFit. ## What is RooFit?. [RooFit] is a statistical data analysis tool, widely used in scientific; research, especially in the high-energy physics (HEP) field. It is an; extension of the ROOT framework, a C++ based data analysis framework that; provides tools for data storage, analysis, and visualization. RooFit provides; a set of tools/classes to define and evaluate probability density functions; (PDFs), perform maximum likelihood fits, perform statistical tests, etc. ## Proof of Concept: Speeding up RooFit using Automatic Differentiation (AD). RooFit is used to reduce statistical models (functions) to find a set of; parameters that minimize the value of the function. This minimization happens; via one of several methods relying heavily on the computation of derivatives; of the function with respect to its free parameters. Currently, the; computation of Numerical Derivatives is the most time-consuming component of; RooFit [^1]. On the other hand, derivatives computed using the Automatic; Differentiation tool [Clad] have been shown to be far more efficient [^2]. \htmlonly; <div class=""pyrootbox"">; \endhtmlonly. Main Advantage of using AD with RooFit: efficient and more precise; derivatives. It computes derivatives with high precision, avoiding the errors; that may arise from approximating derivatives using finite differences. \htmlonly; </div>; \endhtmlonly. ### AD Support essentially requires Code Generation. As we'll discuss in upcoming sections, *AD support* can be added using *C++; Code generation*.; These two terms may be used interchangeably in this document, since the term; *Code Generation* better helps visualize the transformation that is enabling; AD support. ## Current S",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_ad.md:700,perform,perform,700,roofit/doc/developers/roofit_ad.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/roofit_ad.md,2,['perform'],['perform']
Performance,"\defgroup roofit_dev_docs_batchcompute RooBatchCompute library guide; \ingroup roofit_dev_docs; \date September 2021; \author Emmanouil Michalainas; \brief Overview of the RooBatchCompute library. ## RooBatchCompute Library; _Contains optimized computation functions for PDFs that enable significantly faster fittings._; #### Note: This library is still at an experimental stage. Tests are being conducted continuously to ensure correctness of the results, but the interfaces and the instructions on how to use might change. ### Purpose; While fitting, a significant amount of time and processing power is spent on computing the probability function for every event and PDF involved in the fitting model. To speed up this process, roofit can use the computation functions provided in this library. The functions provided here process whole data arrays (batches) instead of a single event at a time, as in the legacy evaluate() function in roofit. In addition, the code is written in a manner that allows for compiler optimizations, notably auto-vectorization. This library is compiled multiple times for different [vector instruction set architectures](https://en.wikipedia.org/wiki/SIMD) and the optimal code is executed during runtime, as a result of an automatic hardware detection mechanism that this library contains. **As a result, fits can benefit by a speedup of 3x-16x.**. As of ROOT v6.26, RooBatchComputes also provides multithread and [CUDA](https://en.wikipedia.org/wiki/CUDA) instances of the computation functions, resulting in even greater improvements for fitting times. ### How to use; This library is an internal component of RooFit, so users are not supposed to actively interact with it. Instead, they can benefit from significantly faster times for fitting by calling `fitTo()` and providing a `BatchMode(""cpu"")` or a `BatchMode(""cuda"")` option.; ``` {.cpp}; // fit using the most efficient library that the computer's CPU can support; RooMyPDF.fitTo(data, BatchMode(""cpu""));. //",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/batchcompute.md:235,optimiz,optimized,235,roofit/doc/developers/batchcompute.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/batchcompute.md,1,['optimiz'],['optimized']
Performance,"\defgroup roofit_dev_docs_test_statistics New RooFit TestStatistics usage notes; \ingroup roofit_dev_docs; \date December 2021; \author Patrick Bos; \brief Notes on the new `RooFit::TestStatistics` classes. # RooFit::TestStatistics usage notes. The `RooFit::TestStatistics` namespace contains a major refactoring of the `RooAbsTestStatistic`-`RooAbsOptTestStatistic`-`RooNLLVar` inheritance tree into:. 1. statistics-based classes on the one hand;; 2. calculation/evaluation/optimization based classes on the other hand. The motivation for this refactoring was also twofold:. 1. These test statistics classes make a cleaner separation of concerns than the existing `RooAbsTestStatistic` based tree and are hence more maintainable and future proof.; 2. They provided a place for us to try out new parallelized gradient calculation methods using the `RooFit::MultiProcess` module. See the usage example below on how to use this. ## Statistics; The likelihood is the central unit on the statistics side.; The `RooAbsL` class is implemented for four kinds of likelihoods: binned, unbinned, ""subsidiary"" (an optimization for numerical stability that gathers components like global observables) and ""sum"" (over multiple components of the other types), in the correspondingly named classes `RooBinnedL`, `RooUnbinnedL`, `RooSubsidiaryL` and `RooSumL`.; These classes provide a `evaluatePartition` function that allows for computing them in parallelizable chunks that can be used by the calculator classes as they see fit. On top of the likelihood classes, we also provide for convenience a likelihood builder `buildLikelihood`, as a free function in the namespace.; This function analyzes the `pdf` and automatically constructs the proper likelihood, built; up from the available `RooAbsL` subclasses. The new classes are not per se meant to be used outside of `RooMinimizer`, although they can be.; The main reason is that they do not behave as regular `RooAbsReal` objects, but have their own interface whi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/test_statistics.md:475,optimiz,optimization,475,roofit/doc/developers/test_statistics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/test_statistics.md,1,['optimiz'],['optimization']
Performance,"\defgroup webgui6 ROOT 6 Web Display; \ingroup webdisplay; \brief To display %ROOT 6 canvases in the web browser. This group contains TWebCanvas class which provides web-based TCanvasImp; and allows display of **%ROOT 6 TCanvas** in the web browser. This is fully reimplements TVirtualX and TVirtualPadPainter classes,; supporting majority of existing ROOT classes. Implementation does not; provide some interactive features - like custom mouse events handling.; Object changes performed in the browser (histogram color change); are not reflected in the C++ objects -; WebGui provides READ-ONLY display capability. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/webgui6/doc/index.md:478,perform,performed,478,gui/webgui6/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/webgui6/doc/index.md,1,['perform'],['performed']
Performance,"\n"";; errs() << ""CMP = "" << I << ""\n\n"";; }; }. //===---------------------------------------------------------------------===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits shrinks the ""and"" constant to 2 but instcombine misses the; icmp transform. //===---------------------------------------------------------------------===//. This code:. typedef struct {; int f1:1;; int f2:1;; int f3:1;; int f4:29;; } t1;. typedef struct {; int f1:1;; int f2:1;; int f3:30;; } t2;. t1 s1;; t2 s2;. void func1(void); {; s1.f1 = s2.f1;; s1.f2 = s2.f2;; }. Compiles into this IR (on x86-64 at least):. %struct.t1 = type { i8, [3 x i8] }; @s2 = global %struct.t1 zeroinitializer, align 4; @s1 = global %struct.t1 zeroinitializer, align 4; define void @func1() nounwind ssp noredzone {; entry:; %0 = load i32* bitcast (%struct.t1* @s2 to i32*), align 4; %bf.val.sext5 = and i32 %0, 1; %1 = load i32* bitcast (%struct.t1* @s1 to i32*), align 4; %2 = and i32 %1, -4; %3 = or i32 %2, %bf.val.sext5; %bf.val.sext26 = and i32 %0, 2; %4 = or i32 %3, %bf.val.sext26; store i32 %4, i32* bitcast (%struct.t1* @s1 to i32*), align 4; ret void; }. The two or/and's should be merged into one each. //===---------------------------------------------------------------------===//. Machine level code hoisting can be useful in some cases. For example, PR9408; is about:. typedef union {; void (*f1)(int);; void (*f2)(long);; } funcs;. void foo(funcs f, int which) {; int a = 5;; if (which) {; f.f1(a);; } else {; f.f2(a);; }; }. which we compile to:. foo: # @foo; # %bb.0: # %entry; pushq %rbp; movq %rsp, %rbp; testl %esi, %esi; movq %rdi, %rax; je .LBB0_2; # %bb.1: # %if.then; movl $5, %edi; callq *%rax; popq %rbp; ret; .LBB0_2: # %if.else; movl $5, %edi; callq *%rax; popq %rbp; r",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:66189,load,load,66189,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['load'],['load']
Performance,"\n"";; }. When debugging step-by-step, the value of `__int_32_0` seemingly does not; change, despite being frequently incremented, and instead is always `43`.; While this might be surprising, this is a result of the optimizer recognizing; that it can eliminate most of the load/store operations. The above code gets; optimized to the equivalent of:. .. code-block:: c++. static task coro_task(int v) {; store v to __int_32_0 in the frame; co_await await_counter{};; a = load __int_32_0; std::cout << a+1 << ""\n"";; std::cout << a+2 << ""\n"";; std::cout << a+3 << ""\n"";; co_await await_counter{};; a = load __int_32_0; std::cout << a+4 << ""\n"";; std::cout << a+5 << ""\n"";; }. It should now be obvious why the value of `__int_32_0` remains unchanged; throughout the function. It is important to recognize that `__int_32_0`; does not directly correspond to `a`, but is instead a variable generated; to assist the compiler in code generation. The variables in an optimized; coroutine frame should not be thought of as directly representing the; variables in the C++ source. Get the suspended points; ========================. An important requirement for debugging coroutines is to understand suspended; points, which are where the coroutine is currently suspended and awaiting. For simple cases like the above, inspecting the value of the `__coro_index`; variable in the coroutine frame works well. However, it is not quite so simple in really complex situations. In these; cases, it is necessary to use the coroutine libraries to insert the; line-number. For example:. .. code-block:: c++. // For all the promise_type we want:; class promise_type {; ...; + unsigned line_number = 0xffffffff;; };. #include <source_location>. // For all the awaiter types we need:; class awaiter {; ...; template <typename Promise>; void await_suspend(std::coroutine_handle<Promise> handle,; std::source_location sl = std::source_location::current()) {; ...; handle.promise().line_number = sl.line();; }; };. In this case, w",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst:10340,optimiz,optimized,10340,interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/DebuggingCoroutines.rst,1,['optimiz'],['optimized']
Performance,"\page Minuit2Page Minuit2. The **Minuit2** library is a new object-oriented implementation, written in C++,; of the popular MINUIT minimization package. These new version provides basically; all the functionality present in the old Fortran version, with almost equivalent; numerical accuracy and computational performances. Furthermore, it contains new; functionality, like the possibility to set single side parameter limits or the; FUMILI algorithm, which is an optimized method for least square and log likelihood; minimizations. The package has been originally developed by M. Winkler and F. James.; More information on the new C++ version can be found on the; [MINUIT Web Site](http://www.cern.ch/minuit). Minuit2, originally developed in the SEAL project, is now distributed within %ROOT.; The API has been then changed in this new version to follow the %ROOT coding convention; (function names starting with capital letters) and the classes have been moved inside; the namespace _ROOT::Minuit2_. In addition, the %ROOT distribution contains classes; needed to integrate Minuit2 in the %ROOT framework. A new class has been introduced, ROOT::Minuit2::Minuit2Minimizer, which implements; the interface ROOT::Math::Minimizer. Within %ROOT, it can be instantiates also using; the %ROOT plug-in manager. This class provides a convenient entry point for using Minuit2\.; An example of using this interface is the %ROOT tutorial _tutorials/fit/NumericalMinimization.C_; or the Minuit2 test program; [<tt>testMinimize.cxx</tt>](https://github.com/cxx-hep/root-cern/blob/master/math/minuit2/test/testMinimize.cxx). A standalone version of Minuit2 (independent of %ROOT) can be easily built and installed using `CMake`. See this [`README`](https://github.com/root-project/root/blob/master/math/minuit2/README.md) for the instructions on how to get the sources, building and installing a stand-alone Minuit2. The [Minuit2 User Guide](https://root.cern/root/htmldoc/guides/minuit2/Minuit2.html); provides a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/minuit2/doc/Minuit2.md:310,perform,performances,310,math/minuit2/doc/Minuit2.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/minuit2/doc/Minuit2.md,2,"['optimiz', 'perform']","['optimized', 'performances']"
Performance,"\ref GP04a); - [Visualization Settings and Attributes](\ref GP04b); - [Colors and Line Styles](\ref GP04ba); - [Visibility Settings](\ref GP04bb); - [Ray Tracing](\ref GP04c); - [Clipping Ray-traced Images](\ref GP04ca); - [Representing Misalignments of the Ideal Geometry](\ref GP05); - [Physical Nodes](\ref GP05a); - [Geometry I/O](\ref GP06); - [GDML](\ref GP06a); - [Navigation Algorithms](\ref GP07); - [Finding the State Corresponding to a Location (x,y,z)](\ref GP07a); - [Finding the Distance to Next Crossed Boundary](\ref GP07b); - [Output Values](\ref GP07c); - [Geometry Graphical User Interface](\ref GP08); - [Editing a Geometry](\ref GP08a); - [The Geometry Manager Editor](\ref GP08b); - [Editing Existing Objects](\ref GP08c); - [Creation of New Objects](\ref GP08d); - [Editing Volumes](\ref GP08e); - [How to Create a Valid Geometry with Geometry Editors](\ref GP08f). \anchor GP00; ## Quick Start: Creating the ""world"". This chapter will provide a detailed description on how to build valid; geometries as well as the ways to optimize them. There are several; components gluing together the geometrical model, but for the time being; let us get used with the most basic concepts. The basic bricks for building-up the model are called; ""volumes"". These represent the un-positioned pieces of the geometry; puzzle. The difference is just that the relationship between the pieces; is not defined by neighbors, but by ""containment"". In other words,; volumes are put one inside another making an in-depth hierarchy. From; outside, the whole thing looks like a big pack that you can open finding; out other smaller packs nicely arranged waiting to be opened at their; turn. The biggest one containing all others defines the ""world"" of the; model. We will often call this ""master reference system (MARS)"". Going; on and opening our packs, we will obviously find out some empty ones,; otherwise, something is very wrong... We will call these leaves (by; analogy with a tree structure). On ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md:3442,optimiz,optimize,3442,geom/geom/doc/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/index.md,1,['optimiz'],['optimize']
Performance,"](pictures/020000A6.jpg). If `pad1` is a pad then, it will divide the pad into 3 columns of 2; sub-pads. The generated sub-pads get names `pad1_i` where the index; `i=1` to `nxm` (in our case `pad1_1`, `pad1_2`...`pad1_6)`. The names; `pad1_1 `etc... correspond to new variables in Cling, so you may use them; as soon as the executed method was `pad->Divide()`. However, in a; compiled program, one has to access these objects. Remember that a pad; contains other objects and that these objects may themselves be pads. So; we can use the `GetPrimitive()` method:. ``` {.cpp}; TPad* pad1_1 = (TPad*)(pad1->GetPrimitive(""pad1_1"")); ```. One question remains. In case one does an automatic divide, how one can; set the default margins between pads? This is done by adding two; parameters to `Divide()`, which are the margins in `x` and `y`:. ``` {.cpp}; root[] pad1->Divide(3,2,0.1,0.1); ```. The margins are here set to 10% of the parent pad width. ### Updating the Pad. For performance reasons, a pad is not updated with every change. For; example, changing the coordinates of the pad does not automatically; redraw it. Instead, the pad has a ""bit-modified"" that triggers a redraw.; This bit is automatically set by:. - Touching the pad with the mouse - for example resizing it with the; mouse. - Finishing the execution of a script. - Adding a new primitive or modifying some primitives for example the; name and title of an object. - You can also set the ""bit-modified"" explicitly with the `Modified`; method:. ``` {.cpp}; // the pad has changed; root[] pad1->Modified(); // recursively update all modified pads:; root[] c1->Update(); ```. A subsequent call to `TCanvas::Update()` scans the list of sub-pads; and repaints the pads declared modified. In compiled code or in a long macro, you may want to access an object; created during the paint process. To do so, you can force the painting; with a `TCanvas::Update()`. For example, a **`TGraph`** creates a; histogram (**`TH1`**) to paint itself. I",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:26188,perform,performance,26188,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['perform'],['performance']
Performance,"]; %hdl = call noalias ptr @llvm.coro.begin(token %id, ptr %phi); br label %loop; loop:; %n.val = phi i32 [ %n, %coro.begin ], [ %inc, %loop ]; %inc = add nsw i32 %n.val, 1; store i32 %n.val, ptr %promise; %0 = call i8 @llvm.coro.suspend(token none, i1 false); switch i8 %0, label %suspend [i8 0, label %loop; i8 1, label %cleanup]; cleanup:; %mem = call ptr @llvm.coro.free(token %id, ptr %hdl); call void @free(ptr %mem); br label %suspend; suspend:; %unused = call i1 @llvm.coro.end(ptr %hdl, i1 false, token none); ret ptr %hdl; }. A coroutine consumer can rely on the `coro.promise`_ intrinsic to access the; coroutine promise. .. code-block:: llvm. define i32 @main() {; entry:; %hdl = call ptr @f(i32 4); %promise.addr = call ptr @llvm.coro.promise(ptr %hdl, i32 4, i1 false); %val0 = load i32, ptr %promise.addr; call void @print(i32 %val0); call void @llvm.coro.resume(ptr %hdl); %val1 = load i32, ptr %promise.addr; call void @print(i32 %val1); call void @llvm.coro.resume(ptr %hdl); %val2 = load i32, ptr %promise.addr; call void @print(i32 %val2); call void @llvm.coro.destroy(ptr %hdl); ret i32 0; }. After example in this section is compiled, result of the compilation will be:. .. code-block:: llvm. define i32 @main() {; entry:; tail call void @print(i32 4); tail call void @print(i32 5); tail call void @print(i32 6); ret i32 0; }. .. _final:; .. _final suspend:. Final Suspend; -------------. A coroutine author or a frontend may designate a particular suspend to be final,; by setting the second argument of the `coro.suspend`_ intrinsic to `true`.; Such a suspend point has two properties:. * it is possible to check whether a suspended coroutine is at the final suspend; point via `coro.done`_ intrinsic;. * a resumption of a coroutine stopped at the final suspend point leads to; undefined behavior. The only possible action for a coroutine at a final; suspend point is destroying it via `coro.destroy`_ intrinsic. From the user perspective, the final suspend point represents an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst:24100,load,load,24100,interpreter/llvm-project/llvm/docs/Coroutines.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Coroutines.rst,1,['load'],['load']
Performance,"^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimize",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:26527,optimiz,optimized,26527,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimized']
Performance,"^. A member of a struct or union may be declared to have ownership-qualified; type. If the type is qualified with ``__unsafe_unretained``, the semantics; of the containing aggregate are unchanged from the semantics of an unqualified type in a non-ARC mode. If the type is qualified with ``__autoreleasing``, the program is ill-formed. Otherwise, if the type is nontrivially ownership-qualified, additional rules apply. Both Objective-C and Objective-C++ support nontrivially ownership-qualified; fields. Due to formal differences between the standards, the formal; treatment is different; however, the basic language model is intended to; be the same for identical code. .. admonition:: Rationale. Permitting ``__strong`` and ``__weak`` references in aggregate types; allows programmers to take advantage of the normal language tools of; C and C++ while still automatically managing memory. While it is; usually simpler and more idiomatic to use Objective-C objects for; secondary data structures, doing so can introduce extra allocation; and message-send overhead, which can cause to unacceptable; performance. Using structs can resolve some of this tension. ``__autoreleasing`` is forbidden because it is treacherous to rely; on autoreleases as an ownership tool outside of a function-local; contexts. Earlier releases of Clang permitted ``__strong`` and ``__weak`` only; references in Objective-C++ classes, not in Objective-C. This; restriction was an undesirable short-term constraint arising from the; complexity of adding support for non-trivial struct types to C. In Objective-C++, nontrivially ownership-qualified types are treated; for nearly all purposes as if they were class types with non-trivial; default constructors, copy constructors, move constructors, copy assignment; operators, move assignment operators, and destructors. This includes the; determination of the triviality of special members of classes with a; non-static data member of such a type. In Objective-C, the definitio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:52406,perform,performance,52406,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['performance']
Performance,"^. The source manager block contains the serialized representation of Clang's; :ref:`SourceManager <SourceManager>` class, which handles the mapping from; source locations (as represented in Clang's abstract syntax tree) into actual; column/line positions within a source file or macro instantiation. The AST; file's representation of the source manager also includes information about all; of the headers that were (transitively) included when building the AST file. The bulk of the source manager block is dedicated to information about the; various files, buffers, and macro instantiations into which a source location; can refer. Each of these is referenced by a numeric ""file ID"", which is a; unique number (allocated starting at 1) stored in the source location. Clang; serializes the information for each kind of file ID, along with an index that; maps file IDs to the position within the AST file where the information about; that file ID is stored. The data associated with a file ID is loaded only when; required by the front end, e.g., to emit a diagnostic that includes a macro; instantiation history inside the header itself. The source manager block also contains information about all of the headers; that were included when building the AST file. This includes information about; the controlling macro for the header (e.g., when the preprocessor identified; that the contents of the header dependent on a macro like; ``LLVM_CLANG_SOURCEMANAGER_H``). .. _pchinternals-preprocessor:. Preprocessor Block; ^^^^^^^^^^^^^^^^^^. The preprocessor block contains the serialized representation of the; preprocessor. Specifically, it contains all of the macros that have been; defined by the end of the header used to build the AST file, along with the; token sequences that comprise each macro. The macro definitions are only read; from the AST file when the name of the macro first occurs in the program. This; lazy loading of macro definitions is triggered by lookups into the; :ref:`identifi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:10716,load,loaded,10716,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['load'],['loaded']
Performance,"^. There are two ways to map virtual registers to physical registers (or to memory; slots). The first way, that we will call *direct mapping*, is based on the use; of methods of the classes ``TargetRegisterInfo``, and ``MachineOperand``. The; second way, that we will call *indirect mapping*, relies on the ``VirtRegMap``; class in order to insert loads and stores sending and getting values to and from; memory. The direct mapping provides more flexibility to the developer of the register; allocator; however, it is more error prone, and demands more implementation; work. Basically, the programmer will have to specify where load and store; instructions should be inserted in the target function being compiled in order; to get and store values in memory. To assign a physical register to a virtual; register present in a given operand, use ``MachineOperand::setReg(p_reg)``. To; insert a store instruction, use ``TargetInstrInfo::storeRegToStackSlot(...)``,; and to insert a load instruction, use ``TargetInstrInfo::loadRegFromStackSlot``. The indirect mapping shields the application developer from the complexities of; inserting load and store instructions. In order to map a virtual register to a; physical one, use ``VirtRegMap::assignVirt2Phys(vreg, preg)``. In order to map; a certain virtual register to memory, use; ``VirtRegMap::assignVirt2StackSlot(vreg)``. This method will return the stack; slot where ``vreg``'s value will be located. If it is necessary to map another; virtual register to the same stack slot, use; ``VirtRegMap::assignVirt2StackSlot(vreg, stack_location)``. One important point; to consider when using the indirect mapping, is that even if a virtual register; is mapped to memory, it still needs to be mapped to a physical register. This; physical register is the location where the virtual register is supposed to be; found before being stored or after being reloaded. If the indirect strategy is used, after all the virtual registers have been; mapped to physical",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:64867,load,load,64867,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,2,['load'],"['load', 'loadRegFromStackSlot']"
Performance,"^^^. #. Mark functions as readnone/readonly/argmemonly or noreturn/nounwind when; known. The optimizer will try to infer these flags, but may not always be; able to. Manual annotations are particularly important for external; functions that the optimizer can not analyze. #. Use the lifetime.start/lifetime.end and invariant.start/invariant.end; intrinsics where possible. Common profitable uses are for stack like data; structures (thus allowing dead store elimination) and for describing; life times of allocas (thus allowing smaller stack sizes). #. Mark invariant locations using !invariant.load and TBAA's constant flags. Pass Ordering; ^^^^^^^^^^^^^. One of the most common mistakes made by new language frontend projects is to; use the existing -O2 or -O3 pass pipelines as is. These pass pipelines make a; good starting point for an optimizing compiler for any language, but they have; been carefully tuned for C and C++, not your target language. You will almost; certainly need to use a custom pass order to achieve optimal performance. A; couple specific suggestions:. #. For languages with numerous rarely executed guard conditions (e.g. null; checks, type checks, range checks) consider adding an extra execution or; two of LoopUnswitch and LICM to your pass order. The standard pass order,; which is tuned for C and C++ applications, may not be sufficient to remove; all dischargeable checks from loops. #. If your language uses range checks, consider using the IRCE pass. It is not; currently part of the standard pass order. #. A useful sanity check to run is to run your optimized IR back through the; -O2 pipeline again. If you see noticeable improvement in the resulting IR,; you likely need to adjust your pass order. I Still Can't Find What I'm Looking For; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. If you didn't find what you were looking for above, consider proposing a piece; of metadata which provides the optimization hint you need. Such extensions are; relatively common and",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:12742,perform,performance,12742,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['perform'],['performance']
Performance,"^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.select.v16i32 (<16 x i1> <condition>, <16 x i32> <on_true>, <16 x i32> <on_false>, i32 <evl>); declare <vscale x 4 x i64> @llvm.vp.select.nxv4i64 (<vscale x 4 x i1> <condition>, <vscale x 4 x i64> <on_true>, <vscale x 4 x i64> <on_false>, i32 <evl>). Overview:; """""""""""""""""". The '``llvm.vp.select``' intrinsic is used to choose one value based on a; condition vector, without IR-level branching. Arguments:; """""""""""""""""""". The first operand is a vector of ``i1`` and indicates the condition. The; second operand is the value that is selected where the condition vector is; true. The third operand is the value that is selected where the condition; vector is false. The vectors must be of the same size. The fourth operand is; the explicit vector length. #. The optional ``fast-math flags`` marker indicates that the select has one or; more :ref:`fast-math flags <fastmath>`. These are optimization hints to; enable otherwise unsafe floating-point optimizations. Fast-math flags are; only valid for selects that return a floating-point scalar or vector type,; or an array (nested to any depth) of floating-point scalar or vector types. Semantics:; """""""""""""""""""". The intrinsic selects lanes from the second and third operand depending on a; condition vector. All result lanes at positions greater or equal than ``%evl`` are undefined.; For all lanes below ``%evl`` where the condition vector is true the lane is; taken from the second operand. Otherwise, the lane is taken from the third; operand. Example:; """""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.select.v4i32(<4 x i1> %cond, <4 x i32> %on_true, <4 x i32> %on_false, i32 %evl). ;;; Expansion.; ;; Any result is legal on lanes at and above %evl.; %also.r = select <4 x i1> %cond, <4 x i32> %on_true, <4 x i32> %on_false. .. _int_vp_merge:. '``llvm.vp.merge.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:694152,optimiz,optimization,694152,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <2 x double> @llvm.experimental.vp.reverse.v2f64(<2 x double> %vec, <2 x i1> %mask, i32 %evl); declare <vscale x 4 x i32> @llvm.experimental.vp.reverse.nxv4i32(<vscale x 4 x i32> %vec, <vscale x 4 x i1> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.experimental.vp.reverse.*``' intrinsic is the vector length; predicated version of the '``llvm.experimental.vector.reverse.*``' intrinsic. Arguments:; """""""""""""""""""". The result and the first argument ``vec`` are vectors with the same type.; The second argument ``mask`` is a vector mask and has the same number of; elements as the result. The third argument is the explicit vector length of; the operation. Semantics:; """""""""""""""""""". This intrinsic reverses the order of the first ``evl`` elements in a vector.; The lanes in the result vector disabled by ``mask`` are ``poison``. The; elements past ``evl`` are poison. .. _int_vp_load:. '``llvm.vp.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <4 x float> @llvm.vp.load.v4f32.p0(ptr %ptr, <4 x i1> %mask, i32 %evl); declare <vscale x 2 x i16> @llvm.vp.load.nxv2i16.p0(ptr %ptr, <vscale x 2 x i1> %mask, i32 %evl); declare <8 x float> @llvm.vp.load.v8f32.p1(ptr addrspace(1) %ptr, <8 x i1> %mask, i32 %evl); declare <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p6(ptr addrspace(6) %ptr, <vscale x 1 x i1> %mask, i32 %evl). Overview:; """""""""""""""""". The '``llvm.vp.load.*``' intrinsic is the vector length predicated version of; the :ref:`llvm.masked.load <int_mload>` intrinsic. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is a; vector of boolean values with the same number of elements as the return type.; The third is the explicit vector length of the operation. The return type and; underlying type of the base pointer are the same vector types. The :ref:`align <attr_align>` parameter attribute can",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:782902,load,load,782902,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"^^^. ``exclude`` metadata may be attached to a global variable to signify that its; section should not be included in the final executable or shared library. This; option is only valid for global variables with an explicit section targeting ELF; or COFF. This is done using the ``SHF_EXCLUDE`` flag on ELF targets and the; ``IMAGE_SCN_LNK_REMOVE`` and ``IMAGE_SCN_MEM_DISCARDABLE`` flags for COFF; targets. Additionally, this metadata is only used as a flag, so the associated; node must be empty. The explicit section should not conflict with any other; sections that the user does not want removed after linking. .. code-block:: text. @object = private constant [1 x i8] c""\00"", section "".foo"" !exclude !0. ...; !0 = !{}. '``unpredictable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. ``unpredictable`` metadata may be attached to any branch or switch; instruction. It can be used to express the unpredictability of control; flow. Similar to the llvm.expect intrinsic, it may be used to alter; optimizations related to compare and branch instructions. The metadata; is treated as a boolean value; if it exists, it signals that the branch; or switch that it is attached to is completely unpredictable. .. _md_dereferenceable:. '``dereferenceable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The existence of the ``!dereferenceable`` metadata on the instruction; tells the optimizer that the value loaded is known to be dereferenceable,; otherwise the behavior is undefined.; The number of bytes known to be dereferenceable is specified by the integer; value in the metadata node. This is analogous to the ''dereferenceable''; attribute on parameters and return values. .. _md_dereferenceable_or_null:. '``dereferenceable_or_null``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The existence of the ``!dereferenceable_or_null`` metadata on the; instruction tells the optimizer that the value loaded is known to be either; dereferenceable or null, otherwise the behavior is undefined.; The number of",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:292369,optimiz,optimizations,292369,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizations']
Performance,"^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.fshl.v16i32 (<16 x i32> <left_op>, <16 x i32> <middle_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.fshl.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <middle_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.fshl.v256i64 (<256 x i64> <left_op>, <256 x i64> <middle_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated fshl of three vectors of integers. Arguments:; """""""""""""""""""". The first three operand and the result have the same vector of integer type. The; fourth operand is the vector mask and has the same number of elements as the; result vector type. The fifth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fshl``' intrinsic performs fshl (:ref:`fshl <int_fshl>`) of the first, second, and third; vector operand on each enabled lane. The result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.fshl.v4i32(<4 x i32> %a, <4 x i32> %b, <4 x i32> %c, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x i32> @llvm.fshl.v4i32(<4 x i32> %a, <4 x i32> %b, <4 x i32> %c); %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. '``llvm.vp.fshr.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.fshr.v16i32 (<16 x i32> <left_op>, <16 x i32> <middle_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.fshr.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <middle_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.fshr.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:839184,perform,performs,839184,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.fshr.v16i32 (<16 x i32> <left_op>, <16 x i32> <middle_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.fshr.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <middle_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.fshr.v256i64 (<256 x i64> <left_op>, <256 x i64> <middle_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated fshr of three vectors of integers. Arguments:; """""""""""""""""""". The first three operand and the result have the same vector of integer type. The; fourth operand is the vector mask and has the same number of elements as the; result vector type. The fifth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fshr``' intrinsic performs fshr (:ref:`fshr <int_fshr>`) of the first, second, and third; vector operand on each enabled lane. The result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.fshr.v4i32(<4 x i32> %a, <4 x i32> %b, <4 x i32> %c, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x i32> @llvm.fshr.v4i32(<4 x i32> %a, <4 x i32> %b, <4 x i32> %c); %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. '``llvm.vp.is.fpclass.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <vscale x 2 x i1> @llvm.vp.is.fpclass.nxv2f32(<vscale x 2 x float> <op>, i32 <test>, <vscale x 2 x i1> <mask>, i32 <vector_length>); declare <2 x i1> @llvm.vp.is.fpclass.v2f16(<2 x half> <op>, i32 <test>, <2 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated llvm.is.fpclass :ref:`llvm.is.fpclass <llvm.is.fpclass>`. Arguments:; """""""""""""""""""". ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:840747,perform,performs,840747,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use '``llvm.ptr.annotation``' on a; pointer to an integer of any width. *NOTE* you must specify an address space for; the pointer. The identifier for the default address space is the integer; '``0``'. ::. declare ptr @llvm.ptr.annotation.p0(ptr <val>, ptr <str>, ptr <str>, i32 <int>); declare ptr @llvm.ptr.annotation.p1(ptr addrspace(1) <val>, ptr <str>, ptr <str>, i32 <int>). Overview:; """""""""""""""""". The '``llvm.ptr.annotation``' intrinsic. Arguments:; """""""""""""""""""". The first argument is a pointer to an integer value of arbitrary bitwidth; (result of some expression), the second is a pointer to a global string, the; third is a pointer to a global string which is the source file name, and the; last argument is the line number. It returns the value of the first argument. Semantics:; """""""""""""""""""". This intrinsic allows annotation of a pointer to an integer with arbitrary; strings. This can be useful for special purpose optimizations that want to look; for these annotations. These have no other defined use; transformations preserve; annotations on a best-effort basis but are allowed to replace the intrinsic with; its first argument without breaking semantics and the intrinsic is completely; dropped during instruction selection. '``llvm.annotation.*``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use '``llvm.annotation``' on; any integer bit width. ::. declare i8 @llvm.annotation.i8(i8 <val>, ptr <str>, ptr <str>, i32 <int>); declare i16 @llvm.annotation.i16(i16 <val>, ptr <str>, ptr <str>, i32 <int>); declare i32 @llvm.annotation.i32(i32 <val>, ptr <str>, ptr <str>, i32 <int>); declare i64 @llvm.annotation.i64(i64 <val>, ptr <str>, ptr <str>, i32 <int>); declare i256 @llvm.annotation.i256(i256 <val>, ptr <str>, ptr <str>, i32 <int>). Overview:; """""""""""""""""". The '``llvm.annotation``' intrinsic. Arguments:; """""""""""""""""""". The first argument is an integer va",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:925914,optimiz,optimizations,925914,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizations']
Performance,"^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.maxnum.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.maxnum.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.maxnum.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point IEEE-754 maxNum of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.maxnum``' intrinsic performs floating-point maximum (:ref:`maxnum <i_maxnum>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.maxnum.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.maxnum.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_minimum:. '``llvm.vp.minimum.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.minimum.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.minimum.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>,",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:727913,perform,performs,727913,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.minnum.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.minnum.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.minnum.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point IEEE-754 minNum of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.minnum``' intrinsic performs floating-point minimum (:ref:`minnum <i_minnum>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.minnum.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.minnum.v4f32(<4 x float> %a, <4 x float> %b); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_maxnum:. '``llvm.vp.maxnum.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.maxnum.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.maxnum.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:726261,perform,performs,726261,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"^^^^^^^^^; PowerPC64/PowerPC64le supports the builtin function ``__builtin_setrnd`` to set; the floating point rounding mode. This function will use the least significant; two bits of integer argument to set the floating point rounding mode. .. code-block:: c++. double __builtin_setrnd(int mode);. The effective values for mode are:. - 0 - round to nearest; - 1 - round to zero; - 2 - round to +infinity; - 3 - round to -infinity. Note that the mode argument will modulo 4, so if the integer argument is greater; than 3, it will only use the least significant two bits of the mode.; Namely, ``__builtin_setrnd(102))`` is equal to ``__builtin_setrnd(2)``. PowerPC cache builtins; ^^^^^^^^^^^^^^^^^^^^^^. The PowerPC architecture specifies instructions implementing cache operations.; Clang provides builtins that give direct programmer access to these cache; instructions. Currently the following builtins are implemented in clang:. ``__builtin_dcbf`` copies the contents of a modified block from the data cache; to main memory and flushes the copy from the data cache. **Syntax**:. .. code-block:: c. void __dcbf(const void* addr); /* Data Cache Block Flush */. **Example of Use**:. .. code-block:: c. int a = 1;; __builtin_dcbf (&a);. Extensions for Static Analysis; ==============================. Clang supports additional attributes that are useful for documenting program; invariants and rules for static analysis tools, such as the `Clang Static; Analyzer <https://clang-analyzer.llvm.org/>`_. These attributes are documented; in the analyzer's `list of source-level annotations; <https://clang-analyzer.llvm.org/annotations.html>`_. Extensions for Dynamic Analysis; ===============================. Use ``__has_feature(address_sanitizer)`` to check if the code is being built; with :doc:`AddressSanitizer`. Use ``__has_feature(thread_sanitizer)`` to check if the code is being built; with :doc:`ThreadSanitizer`. Use ``__has_feature(memory_sanitizer)`` to check if the code is being built; wit",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:157229,cache,cache,157229,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,2,['cache'],['cache']
Performance,"^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.umul.fix.sat``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.umul.fix.sat.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.umul.fix.sat.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.umul.fix.sat.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.umul.fix.sat.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.umul.fix.sat``' family of intrinsic functions perform unsigned; fixed point saturating multiplication on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. ``%a`` and ``%b`` are the two; values that will undergo unsigned fixed point multiplication. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs fixed point multiplication on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. The maximum value this operation can clamp to is the largest unsigned value; representable by the bit width of the first 2 arguments. The minimum value is the; smallest unsigned value representable by this bit width (zero). Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.umul.fix.sat.i4(i4 3, i4 2, i32 0) ; %res = 6 (2 x 3 = 6); %res = call i4 @llvm.umul.fix.sat.i4(i4 3, i4 2, i32 1) ; %res = 3 (1.5 x 1 = 1.5). ; The result in the following could be rounded down to 2 or up to 2.5; %res = call i4 @llvm.umul.fix.sat.i4(i4 3, i4 3, i32 1) ; %res = 4 (or 5) (1.5 x 1.5 = 2.25). ; Saturation; %res = call i4 @llvm.umul.fix.sat.i4(i4 8, i4 2, i32 0) ; %res = 15 (8 x 2 ->",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:628547,perform,performs,628547,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.copysign.v16f32 (<16 x float> <mag_op>, <16 x float> <sign_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.copysign.nxv4f32 (<vscale x 4 x float> <mag_op>, <vscale x 4 x float> <sign_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.copysign.v256f64 (<256 x double> <mag_op>, <256 x double> <sign_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point copysign of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.copysign``' intrinsic performs floating-point copysign (:ref:`copysign <int_copysign>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.copysign.v4f32(<4 x float> %mag, <4 x float> %sign, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x float> @llvm.copysign.v4f32(<4 x float> %mag, <4 x float> %sign); %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_minnum:. '``llvm.vp.minnum.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.minnum.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.minnum.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:724588,perform,performs,724588,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.smul.fix.sat``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.smul.fix.sat.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.smul.fix.sat.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.smul.fix.sat.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.smul.fix.sat.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.smul.fix.sat``' family of intrinsic functions perform signed; fixed point saturating multiplication on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. ``%a`` and ``%b`` are the two; values that will undergo signed fixed point multiplication. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs fixed point multiplication on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. The maximum value this operation can clamp to is the largest signed value; representable by the bit width of the first 2 arguments. The minimum value is the; smallest signed value representable by this bit width. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.smul.fix.sat.i4(i4 3, i4 2, i32 0) ; %res = 6 (2 x 3 = 6); %res = call i4 @llvm.smul.fix.sat.i4(i4 3, i4 2, i32 1) ; %res = 3 (1.5 x 1 = 1.5); %res = call i4 @llvm.smul.fix.sat.i4(i4 3, i4 -2, i32 1) ; %res = -3 (1.5 x -1 = -1.5). ; The result in the following could be rounded up to -2 or down to -2.5; %res = call i4 @llvm.smul.fix.sat.i4(i4 3, i4 -3, i32 1) ; %res = -5 (or -4) (1.5 x -1.5 = -2.25). ; Satur",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:625985,perform,performs,625985,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"^^^^^^^^^^^^^^^^. Fixed size arrays are very simple and very fast. They are good if you know; exactly how many elements you have, or you have a (low) upper bound on how many; you have. .. _dss_heaparrays:. Heap Allocated Arrays; ^^^^^^^^^^^^^^^^^^^^^. Heap allocated arrays (``new[]`` + ``delete[]``) are also simple. They are good; if the number of elements is variable, if you know how many elements you will; need before the array is allocated, and if the array is usually large (if not,; consider a :ref:`SmallVector <dss_smallvector>`). The cost of a heap allocated; array is the cost of the new/delete (aka malloc/free). Also note that if you; are allocating an array of a type with a constructor, the constructor and; destructors will be run for every element in the array (re-sizable vectors only; construct those elements actually used). .. _dss_tinyptrvector:. llvm/ADT/TinyPtrVector.h; ^^^^^^^^^^^^^^^^^^^^^^^^. ``TinyPtrVector<Type>`` is a highly specialized collection class that is; optimized to avoid allocation in the case when a vector has zero or one; elements. It has two major restrictions: 1) it can only hold values of pointer; type, and 2) it cannot hold a null pointer. Since this container is highly specialized, it is rarely used. .. _dss_smallvector:. llvm/ADT/SmallVector.h; ^^^^^^^^^^^^^^^^^^^^^^. ``SmallVector<Type, N>`` is a simple class that looks and smells just like; ``vector<Type>``: it supports efficient iteration, lays out elements in memory; order (so you can do pointer arithmetic between elements), supports efficient; push_back/pop_back operations, supports efficient random access to its elements,; etc. The main advantage of SmallVector is that it allocates space for some number of; elements (N) **in the object itself**. Because of this, if the SmallVector is; dynamically smaller than N, no malloc is performed. This can be a big win in; cases where the malloc/free call is far more expensive than the code that; fiddles around with the elements. This ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:59090,optimiz,optimized,59090,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['optimiz'],['optimized']
Performance,"^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fmul.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fmul.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fmul.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point multiplication of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fmul``' intrinsic performs floating-point multiplication (:ref:`fmul <i_fmul>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.fmul.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = fmul <4 x float> %a, %b; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fdiv:. '``llvm.vp.fdiv.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fdiv.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fdiv.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fdi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:736420,perform,performs,736420,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"^^^^^^^^^^^^^^^^^^^^. An important transformation that happens during register allocation is called; the *SSA Deconstruction Phase*. The SSA form simplifies many analyses that are; performed on the control flow graph of programs. However, traditional; instruction sets do not implement PHI instructions. Thus, in order to generate; executable code, compilers must replace PHI instructions with other instructions; that preserve their semantics. There are many ways in which PHI instructions can safely be removed from the; target code. The most traditional PHI deconstruction algorithm replaces PHI; instructions with copy instructions. That is the strategy adopted by LLVM. The; SSA deconstruction algorithm is implemented in; ``lib/CodeGen/PHIElimination.cpp``. In order to invoke this pass, the identifier; ``PHIEliminationID`` must be marked as required in the code of the register; allocator. Instruction folding; ^^^^^^^^^^^^^^^^^^^. *Instruction folding* is an optimization performed during register allocation; that removes unnecessary copy instructions. For instance, a sequence of; instructions such as:. ::. %EBX = LOAD %mem_address; %EAX = COPY %EBX. can be safely substituted by the single instruction:. ::. %EAX = LOAD %mem_address. Instructions can be folded with the; ``TargetRegisterInfo::foldMemoryOperand(...)`` method. Care must be taken when; folding instructions; a folded instruction can be quite different from the; original instruction. See ``LiveIntervals::addIntervalsForSpills`` in; ``lib/CodeGen/LiveIntervalAnalysis.cpp`` for an example of its use. Built in register allocators; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The LLVM infrastructure provides the application developer with three different; register allocators:. * *Fast* --- This register allocator is the default for debug builds. It; allocates registers on a basic block level, attempting to keep values in; registers and reusing registers as appropriate. * *Basic* --- This is an incremental approach to register alloc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:68642,optimiz,optimization,68642,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,2,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fsub.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fsub.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fsub.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point subtraction of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fsub``' intrinsic performs floating-point subtraction (:ref:`fsub <i_fsub>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.fsub.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = fsub <4 x float> %a, %b; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fmul:. '``llvm.vp.fmul.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fmul.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fmul.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fmul",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:734827,perform,performs,734827,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.frem.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.frem.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.frem.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point remainder of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.frem``' intrinsic performs floating-point remainder (:ref:`frem <i_frem>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.frem.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = frem <4 x float> %a, %b; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fneg:. '``llvm.vp.fneg.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fneg.v16f32 (<16 x float> <op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fneg.nxv4f32 (<vscale x 4 x float> <op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fneg.v256f64 (<256 x double> <op>, <256 x i1> <mask>, i32 <vector_length>",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:739595,perform,performs,739595,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"^^^^^^^^^^^^^^^^^^^^^^^^; :doc:`This page <Statepoints>` contains detailed documentation for; ``gc.statepoint``. Using ``llvm.gcwrite``; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. .. code-block:: llvm. void @llvm.gcroot(i8** %ptrloc, i8* %metadata). The ``llvm.gcroot`` intrinsic is used to inform LLVM that a stack variable; references an object on the heap and is to be tracked for garbage collection.; The exact impact on generated code is specified by the Function's selected; :ref:`GC strategy <plugin>`. All calls to ``llvm.gcroot`` **must** reside; inside the first basic block. The first argument **must** be a value referring to an alloca instruction or a; bitcast of an alloca. The second contains a pointer to metadata that should be; associated with the pointer, and **must** be a constant or global value; address. If your target collector uses tags, use a null pointer for metadata. A compiler which performs manual SSA construction **must** ensure that SSA; values representing GC references are stored in to the alloca passed to the; respective ``gcroot`` before every call site and reloaded after every call.; A compiler which uses mem2reg to raise imperative code using ``alloca`` into; SSA form need only add a call to ``@llvm.gcroot`` for those variables which; are pointers into the GC heap. It is also important to mark intermediate values with ``llvm.gcroot``. For; example, consider ``h(f(), g())``. Beware leaking the result of ``f()`` in the; case that ``g()`` triggers a collection. Note, that stack variables must be; initialized and marked with ``llvm.gcroot`` in function's prologue. The ``%metadata`` argument can be used to avoid requiring heap objects to have; 'isa' pointers or tag bits. [Appel89_, Goldberg91_, Tolmach94_] If specified,; its value will be tracked along with the location of the pointer in the stack; frame. Consider the following fragment of Java code:. .. code-block:: java. {; Object X; // A null-initialized reference to an object; ...; }. This block (whic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:10818,perform,performs,10818,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['perform'],['performs']
Performance,"^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fadd.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fadd.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fadd.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point addition of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fadd``' intrinsic performs floating-point addition (:ref:`fadd <i_fadd>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.fadd.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = fadd <4 x float> %a, %b; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_fsub:. '``llvm.vp.fsub.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fsub.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fsub.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fsub.v",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:733240,perform,performs,733240,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.fdiv.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.fdiv.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.fdiv.v256f64 (<256 x double> <left_op>, <256 x double> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated floating-point division of two vectors of floating-point values. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of floating-point type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.fdiv``' intrinsic performs floating-point division (:ref:`fdiv <i_fdiv>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. The operation is; performed in the default floating-point environment. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x float> @llvm.vp.fdiv.v4f32(<4 x float> %a, <4 x float> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = fdiv <4 x float> %a, %b; %also.r = select <4 x i1> %mask, <4 x float> %t, <4 x float> poison. .. _int_vp_frem:. '``llvm.vp.frem.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x float> @llvm.vp.frem.v16f32 (<16 x float> <left_op>, <16 x float> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x float> @llvm.vp.frem.nxv4f32 (<vscale x 4 x float> <left_op>, <vscale x 4 x float> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x double> @llvm.vp.frem.v",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:738010,perform,performs,738010,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.udiv.fix.sat``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.udiv.fix.sat.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.udiv.fix.sat.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.udiv.fix.sat.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.udiv.fix.sat.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.udiv.fix.sat``' family of intrinsic functions perform unsigned; fixed point saturating division on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. ``%a`` and ``%b`` are the two; values that will undergo unsigned fixed point division. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs fixed point division on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. The maximum value this operation can clamp to is the largest unsigned value; representable by the bit width of the first 2 arguments. The minimum value is the; smallest unsigned value representable by this bit width (zero). It is undefined behavior if the second argument is zero. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.udiv.fix.sat.i4(i4 6, i4 2, i32 0) ; %res = 3 (6 / 2 = 3); %res = call i4 @llvm.udiv.fix.sat.i4(i4 6, i4 4, i32 1) ; %res = 3 (3 / 2 = 1.5). ; The result in the following could be rounded down to 0.5 or up to 1; %res = call i4 @llvm.udiv.fix.sat.i4(i4 3, i4 4, i32 1) ; %res = 1 (or 2) (1.5 / 2 = 0.75). ; Saturation; %res = call i4 @llvm.udiv.f",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:637239,perform,performs,637239,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The ``addEscapingUse`` method is used when the uses of a pointer value have; changed in ways that may invalidate precomputed analysis information.; Implementations may either use this callback to provide conservative responses; for points whose uses have change since analysis time, or may recompute some or; all of their internal state to continue providing accurate responses. In general, any new use of a pointer value is considered an escaping use, and; must be reported through this callback, *except* for the uses below:. * A ``bitcast`` or ``getelementptr`` of the pointer; * A ``store`` through the pointer (but not a ``store`` *of* the pointer); * A ``load`` through the pointer. Efficiency Issues; -----------------. From the LLVM perspective, the only thing you need to do to provide an efficient; alias analysis is to make sure that alias analysis **queries** are serviced; quickly. The actual calculation of the alias analysis results (the ""run""; method) is only performed once, but many (perhaps duplicate) queries may be; performed. Because of this, try to move as much computation to the run method; as possible (within reason). Limitations; -----------. The AliasAnalysis infrastructure has several limitations which make writing a; new ``AliasAnalysis`` implementation difficult. There is no way to override the default alias analysis. It would be very useful; to be able to do something like ""``opt -my-aa -O2``"" and have it use ``-my-aa``; for all passes which need AliasAnalysis, but there is currently no support for; that, short of changing the source code and recompiling. Similarly, there is; also no way of setting a chain of analyses as the default. There is no way for transform passes to declare that they preserve; ``AliasAnalysis`` implementations. The ``AliasAnalysis`` interface includes; ``deleteValue`` and ``copyValue`` methods which are intended to allow a pass to; keep an AliasAnalysis consistent, however there's no way for a pass ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst:16723,perform,performed,16723,interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AliasAnalysis.rst,2,['perform'],['performed']
Performance,"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.sdiv.fix.sat``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.sdiv.fix.sat.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.sdiv.fix.sat.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.sdiv.fix.sat.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.sdiv.fix.sat.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.sdiv.fix.sat``' family of intrinsic functions perform signed; fixed point saturating division on 2 arguments of the same scale. Arguments; """""""""""""""""""". The arguments (%a and %b) and the result may be of integer types of any bit; width, but they must have the same bit width. ``%a`` and ``%b`` are the two; values that will undergo signed fixed point division. The argument; ``%scale`` represents the scale of both operands, and must be a constant; integer. Semantics:; """""""""""""""""""". This operation performs fixed point division on the 2 arguments of a; specified scale. The result will also be returned in the same scale specified; in the third argument. If the result value cannot be precisely represented in the given scale, the; value is rounded up or down to the closest representable value. The rounding; direction is unspecified. The maximum value this operation can clamp to is the largest signed value; representable by the bit width of the first 2 arguments. The minimum value is the; smallest signed value representable by this bit width. It is undefined behavior if the second argument is zero. Examples; """""""""""""""""". .. code-block:: llvm. %res = call i4 @llvm.sdiv.fix.sat.i4(i4 6, i4 2, i32 0) ; %res = 3 (6 / 2 = 3); %res = call i4 @llvm.sdiv.fix.sat.i4(i4 6, i4 4, i32 1) ; %res = 3 (3 / 2 = 1.5); %res = call i4 @llvm.sdiv.fix.sat.i4(i4 3, i4 -2, i32 1) ; %res = -3 (1.5 / -1 = -1.5). ; The result in the following could be rounded up to 1 or down to 0.5; %res = call i4 @llvm.sdiv.fix.sat.i4(i4 3, i4 4, i32 1) ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:634867,perform,performs,634867,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata defines which loop attributes the remainder loop after; partial/runtime unrolling will have. See; :ref:`Transformation Metadata <transformation-metadata>` for details. '``llvm.loop.unroll_and_jam``'; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata is treated very similarly to the ``llvm.loop.unroll`` metadata; above, but affect the unroll and jam pass. In addition any loop with; ``llvm.loop.unroll`` metadata but no ``llvm.loop.unroll_and_jam`` metadata will; disable unroll and jam (so ``llvm.loop.unroll`` metadata will be left to the; unroller, plus ``llvm.loop.unroll.disable`` metadata will disable unroll and jam; too.). The metadata for unroll and jam otherwise is the same as for ``unroll``.; ``llvm.loop.unroll_and_jam.enable``, ``llvm.loop.unroll_and_jam.disable`` and; ``llvm.loop.unroll_and_jam.count`` do the same as for unroll.; ``llvm.loop.unroll_and_jam.full`` is not supported. Again these are only hints; and the normal safety checks will still be performed. '``llvm.loop.unroll_and_jam.count``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata suggests an unroll and jam factor to use, similarly to; ``llvm.loop.unroll.count``. The first operand is the string; ``llvm.loop.unroll_and_jam.count`` and the second operand is a positive integer; specifying the unroll factor. For example:. .. code-block:: llvm. !0 = !{!""llvm.loop.unroll_and_jam.count"", i32 4}. If the trip count of the loop is less than the unroll count the loop; will be partially unroll and jammed. '``llvm.loop.unroll_and_jam.disable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata disables loop unroll and jamming. The metadata has a single; operand which is the string ``llvm.loop.unroll_and_jam.disable``. For example:. .. code-block:: llvm. !0 = !{!""llvm.loop.unroll_and_jam.disable""}. '``llvm.loop.unroll_and_jam.enable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:304181,perform,performed,304181,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare <ty2>; @llvm.experimental.constrained.fcmp(<type> <op1>, <type> <op2>,; metadata <condition code>,; metadata <exception behavior>); declare <ty2>; @llvm.experimental.constrained.fcmps(<type> <op1>, <type> <op2>,; metadata <condition code>,; metadata <exception behavior>). Overview:; """""""""""""""""". The '``llvm.experimental.constrained.fcmp``' and; '``llvm.experimental.constrained.fcmps``' intrinsics return a boolean; value or vector of boolean values based on comparison of its operands. If the operands are floating-point scalars, then the result type is a; boolean (:ref:`i1 <t_integer>`). If the operands are floating-point vectors, then the result type is a; vector of boolean with the same number of elements as the operands being; compared. The '``llvm.experimental.constrained.fcmp``' intrinsic performs a quiet; comparison operation while the '``llvm.experimental.constrained.fcmps``'; intrinsic performs a signaling comparison operation. Arguments:; """""""""""""""""""". The first two arguments to the '``llvm.experimental.constrained.fcmp``'; and '``llvm.experimental.constrained.fcmps``' intrinsics must be; :ref:`floating-point <t_floating>` or :ref:`vector <t_vector>`; of floating-point values. Both arguments must have identical types. The third argument is the condition code indicating the kind of comparison; to perform. It must be a metadata string with one of the following values:. .. _fcmp_md_cc:. - ""``oeq``"": ordered and equal; - ""``ogt``"": ordered and greater than; - ""``oge``"": ordered and greater than or equal; - ""``olt``"": ordered and less than; - ""``ole``"": ordered and less than or equal; - ""``one``"": ordered and not equal; - ""``ord``"": ordered (no nans); - ""``ueq``"": unordered or equal; - ""``ugt``"": unordered or greater than; - ""``uge``"": unordered or greater than or equal; - ""``ult``"": unordered or less than; - ""``ule``"": unordered or less than or equal; - ""``une``"": unordered or not e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:884090,perform,performs,884090,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"_1 = { 0, sizeof(struct __block_literal_1) };. and where the ``Block`` literal itself appears:. .. code-block:: c. struct __block_literal_1 _block_literal = {; &_NSConcreteStackBlock,; (1<<29), <uninitialized>,; __block_invoke_1,; &__block_descriptor_1; };. A ``Block`` imports other ``Block`` references, ``const`` copies of other; variables, and variables marked ``__block``. In Objective-C, variables may; additionally be objects. When a ``Block`` literal expression is used as the initial value of a global; or ``static`` local variable, it is initialized as follows:. .. code-block:: c. struct __block_literal_1 __block_literal_1 = {; &_NSConcreteGlobalBlock,; (1<<28)|(1<<29), <uninitialized>,; __block_invoke_1,; &__block_descriptor_1; };. that is, a different address is provided as the first value and a particular; (1<<28) bit is set in the ``flags`` field, and otherwise it is the same as for; stack based ``Block`` literals. This is an optimization that can be used for; any ``Block`` literal that imports no ``const`` or ``__block`` storage; variables. Imported Variables; ==================. Variables of ``auto`` storage class are imported as ``const`` copies. Variables; of ``__block`` storage class are imported as a pointer to an enclosing data; structure. Global variables are simply referenced and not considered as; imported. Imported ``const`` copy variables; ---------------------------------. Automatic storage variables not marked with ``__block`` are imported as; ``const`` copies. The simplest example is that of importing a variable of type ``int``:. .. code-block:: c. int x = 10;; void (^vv)(void) = ^{ printf(""x is %d\n"", x); }; x = 11;; vv();. which would be compiled to:. .. code-block:: c. struct __block_literal_2 {; void *isa;; int flags;; int reserved;; void (*invoke)(struct __block_literal_2 *);; struct __block_descriptor_2 *descriptor;; const int x;; };. void __block_invoke_2(struct __block_literal_2 *_block) {; printf(""x is %d\n"", _block->x);; }. static str",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Block-ABI-Apple.rst:6497,optimiz,optimization,6497,interpreter/llvm-project/clang/docs/Block-ABI-Apple.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/Block-ABI-Apple.rst,1,['optimiz'],['optimization']
Performance,"_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate without leaking.; orl %eax, %edi; ```. While an addition happens to the loaded (potentially secret) value, that; doesn't leak any data and we then immediately harden it. ###### Hardening of loaded values deferred down the data-invariant expression graph. We can generalize the previous idea and sink the hardening down the expression; graph across as many data-invariant operations as desirable. This can use very; conservative rules for whether something is data-invariant. The primary goal; should be to handle multiple loads with a single hardening instruction:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate without leaking.; addl 4(%rsi), %edi # Continue without leaking.; addl 8(%rsi), %edi; orl %eax, %edi # Mask out bits from all three loads.; ```. ###### Preserving the flags while hardening loaded values on Haswell, Zen, and newer processors. Sadly, there are no useful instructions on x86 that apply a mask to all 64 bits; without touching the flag registers. However, we can harden loaded values that; are narrower than a word (fewer than 32-bits on 32-bit systems and fewer than; 64-bits on 64-bit systems) by zero-extending the value to the full word size; and then shifting right by at least the number of original bits using the BMI2; `shrx` instruction:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; addl (%rsi), %edi # Load and accumulate 32 bits of data.; shrxq %rax, %rdi, %rdi # Shift out all 32 bits loaded.; ```. Because on x86 the zero-extend is free, this can efficiently harden the loaded; value. ##### Hardening the address of the load. When hardening the loaded value is inapplicable, most often because the; instruction directly leaks information (like `cmp` or `jmpq`), we switch to; hardening the _address_ of the load instead of ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:26725,load,loaded,26725,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loaded']
Performance,"_BUILD_TYPE=RelWithDebInfo -DCMAKE_INSTALL_PREFIX=<path to environment python site-packages>; $ make -j <N> install. where the ``cmake`` command needs to be given the full path to; `site-packages/cppyy_backend` in the virtual environment or other; installation location.; Adjust other options (esp. ``CMAKE_CXX_STANDARD``) as needed.; For the build command, adjust the ``cmake`` command as appropriate for your; favorite, or platform-specific, build system and/or use ``cmake --build``; instead of ``make`` directly.; See the `cmake documentation`_ for details. Next up is ``cppyy-backend`` (cppyy-backend, subdirectory ""clingwrapper""; omit; the first step if you already cloned the repo for ``cppyy-cling``)::. $ git clone https://github.com/wlav/cppyy-backend.git; $ cd cppyy-backend/clingwrapper; $ python -m pip install . --upgrade --no-use-pep517 --no-deps. Note the use of ``--no-use-pep517``, which prevents ``pip`` from needlessly; going out to pypi.org and creating a local ""clean"" build environment from the; cached or remote wheels.; Instead, by skipping PEP 517, the local installation will be used.; This is imperative if there was a change in public headers or if the version; of ``cppyy-cling`` was locally updated and is thus not available on PyPI. Upgrading ``CPyCppyy`` (if on CPython; it's not needed for PyPy) and ``cppyy``; is very similar::. $ git clone https://github.com/wlav/CPyCppyy.git; $ cd CPyCppyy; $ python -m pip install . --upgrade --no-use-pep517 --no-deps. Just like ``cppyy-cling``, ``CPyCppyy`` has ``cmake`` scripts which are the; recommended way for development, as incremental builds are faster::. $ mkdir build; $ cmake ../CPyCppyy; $ make -j <N>. then simply point the ``PYTHONPATH`` envar to the `build` directory above to; pick up the local `cppyy.so` module. Finally, the top-level package ``cppyy``::. $ git clone https://github.com/wlav/cppyy.git; $ cd cppyy; $ python -m pip install . --upgrade --no-deps. Please see the `pip documentation`_ for more op",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/repositories.rst:5724,cache,cached,5724,bindings/pyroot/cppyy/cppyy/doc/source/repositories.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/repositories.rst,1,['cache'],['cached']
Performance,"_C_DYLIB ""Build LLVM-C.dll (Windows only)"" ON); # Set this variable to OFF here so it can't be set with a command-line; # argument.; set (LLVM_LINK_LLVM_DYLIB OFF); if (BUILD_SHARED_LIBS); message(FATAL_ERROR ""BUILD_SHARED_LIBS options is not supported on Windows.""); endif(); else(); option(LLVM_LINK_LLVM_DYLIB ""Link tools against the libllvm dynamic library"" OFF); option(LLVM_BUILD_LLVM_C_DYLIB ""Build libllvm-c re-export library (Darwin only)"" OFF); set(LLVM_BUILD_LLVM_DYLIB_default OFF); if(LLVM_LINK_LLVM_DYLIB OR LLVM_BUILD_LLVM_C_DYLIB); set(LLVM_BUILD_LLVM_DYLIB_default ON); endif(); option(LLVM_BUILD_LLVM_DYLIB ""Build libllvm dynamic library"" ${LLVM_BUILD_LLVM_DYLIB_default}); endif(). if (LLVM_LINK_LLVM_DYLIB AND BUILD_SHARED_LIBS); message(FATAL_ERROR ""Cannot enable BUILD_SHARED_LIBS with LLVM_LINK_LLVM_DYLIB. We recommend disabling BUILD_SHARED_LIBS.""); endif(). option(LLVM_OPTIMIZED_TABLEGEN ""Force TableGen to be built with optimization"" OFF); if(CMAKE_CROSSCOMPILING OR (LLVM_OPTIMIZED_TABLEGEN AND (LLVM_ENABLE_ASSERTIONS OR CMAKE_CONFIGURATION_TYPES))); set(LLVM_USE_HOST_TOOLS ON); endif(). option(LLVM_OMIT_DAGISEL_COMMENTS ""Do not add comments to DAG ISel"" ON); if (CMAKE_BUILD_TYPE AND uppercase_CMAKE_BUILD_TYPE MATCHES ""^(RELWITHDEBINFO|DEBUG)$""); set(LLVM_OMIT_DAGISEL_COMMENTS OFF); endif(). if (MSVC_IDE); option(LLVM_ADD_NATIVE_VISUALIZERS_TO_SOLUTION ""Configure project to use Visual Studio native visualizers"" TRUE); endif(). if(NOT LLVM_INDIVIDUAL_TEST_COVERAGE); if(LLVM_BUILD_INSTRUMENTED OR LLVM_BUILD_INSTRUMENTED_COVERAGE OR LLVM_ENABLE_IR_PGO); if(NOT LLVM_PROFILE_MERGE_POOL_SIZE); # A pool size of 1-2 is probably sufficient on an SSD. 3-4 should be fine; # for spinning disks. Anything higher may only help on slower mediums.; set(LLVM_PROFILE_MERGE_POOL_SIZE ""4""); endif(); if(NOT LLVM_PROFILE_FILE_PATTERN); if(NOT LLVM_PROFILE_DATA_DIR); file(TO_NATIVE_PATH ""${LLVM_BINARY_DIR}/profiles"" LLVM_PROFILE_DATA_DIR); endif(); file(TO_NATIVE_PATH ""${LLVM_",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:34560,optimiz,optimization,34560,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['optimiz'],['optimization']
Performance,"_HI`` Static, 2 ``word32`` (S + A) >> 32; Dynamic; ``R_AMDGPU_ABS64`` Static, 3 ``word64`` S + A; Dynamic; ``R_AMDGPU_REL32`` Static 4 ``word32`` S + A - P; ``R_AMDGPU_REL64`` Static 5 ``word64`` S + A - P; ``R_AMDGPU_ABS32`` Static, 6 ``word32`` S + A; Dynamic; ``R_AMDGPU_GOTPCREL`` Static 7 ``word32`` G + GOT + A - P; ``R_AMDGPU_GOTPCREL32_LO`` Static 8 ``word32`` (G + GOT + A - P) & 0xFFFFFFFF; ``R_AMDGPU_GOTPCREL32_HI`` Static 9 ``word32`` (G + GOT + A - P) >> 32; ``R_AMDGPU_REL32_LO`` Static 10 ``word32`` (S + A - P) & 0xFFFFFFFF; ``R_AMDGPU_REL32_HI`` Static 11 ``word32`` (S + A - P) >> 32; *reserved* 12; ``R_AMDGPU_RELATIVE64`` Dynamic 13 ``word64`` B + A; ``R_AMDGPU_REL16`` Static 14 ``word16`` ((S + A - P) - 4) / 4; ========================== ======= ===== ========== ==============================. ``R_AMDGPU_ABS32_LO`` and ``R_AMDGPU_ABS32_HI`` are only supported by; the ``mesa3d`` OS, which does not support ``R_AMDGPU_ABS64``. There is no current OS loader support for 32-bit programs and so; ``R_AMDGPU_ABS32`` is not used. .. _amdgpu-loaded-code-object-path-uniform-resource-identifier:. Loaded Code Object Path Uniform Resource Identifier (URI); ---------------------------------------------------------. The AMD GPU code object loader represents the path of the ELF shared object from; which the code object was loaded as a textual Uniform Resource Identifier (URI).; Note that the code object is the in memory loaded relocated form of the ELF; shared object. Multiple code objects may be loaded at different memory; addresses in the same process from the same ELF shared object. The loaded code object path URI syntax is defined by the following BNF syntax:. .. code::. code_object_uri ::== file_uri | memory_uri; file_uri ::== ""file://"" file_path [ range_specifier ]; memory_uri ::== ""memory://"" process_id range_specifier; range_specifier ::== [ ""#"" | ""?"" ] ""offset="" number ""&"" ""size="" number; file_path ::== URI_ENCODED_OS_FILE_PATH; process_id ::== DECIMAL_NUMBER; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:82574,load,loader,82574,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loader']
Performance,"_OP_call4, DW_OP_call_ref``. ``DW_OP_call2``, ``DW_OP_call4``, and ``DW_OP_call_ref`` perform DWARF; procedure calls during evaluation of a DWARF operation expression. ``DW_OP_call2`` and ``DW_OP_call4``, have one operand that is, respectively,; a 2-byte or 4-byte unsigned offset DR that represents the byte offset of a; debugging information entry D relative to the beginning of the current; compilation unit. ``DW_OP_call_ref`` has one operand that is a 4-byte unsigned value in the; 32-bit DWARF format, or an 8-byte unsigned value in the 64-bit DWARF format,; that represents the byte offset DR of a debugging information entry D; relative to the beginning of the ``.debug_info`` section that contains the; current compilation unit. D may not be in the current compilation unit. .. note::. DWARF Version 5 states that DR can be an offset in a ``.debug_info``; section other than the one that contains the current compilation unit. It; states that relocation of references from one executable or shared object; file to another must be performed by the consumer. But given that DR is; defined as an offset in a ``.debug_info`` section this seems impossible.; If DR was defined as an implementation defined value, then the consumer; could choose to interpret the value in an implementation defined manner to; reference a debug information in another executable or shared object. In ELF the ``.debug_info`` section is in a non-\ ``PT_LOAD`` segment so; standard dynamic relocations cannot be used. But even if they were loaded; segments and dynamic relocations were used, DR would need to be the; address of D, not an offset in a ``.debug_info`` section. That would also; need DR to be the size of a global address. So it would not be possible to; use the 32-bit DWARF format in a 64-bit global address space. In addition,; the consumer would need to determine what executable or shared object the; relocated address was in so it could determine the containing compilation; unit. GDB only interprets",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:73272,perform,performed,73272,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['performed']
Performance,"_STANDARD}); message(FATAL_ERROR ""Requested CMAKE_CXX_STANDARD=${CMAKE_CXX_STANDARD} which is less than the required ${LLVM_REQUIRED_CXX_STANDARD}.""); endif(). set(CMAKE_CXX_STANDARD ${LLVM_REQUIRED_CXX_STANDARD} CACHE STRING ""C++ standard to conform to""); set(CMAKE_CXX_STANDARD_REQUIRED YES). if (CYGWIN); # Cygwin is a bit stricter and lack things like 'strdup', 'stricmp', etc in; # c++xx mode.; set(CMAKE_CXX_EXTENSIONS YES); else(); set(CMAKE_CXX_EXTENSIONS NO); endif(). if (NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES); message(FATAL_ERROR ""; No build type selected. You need to pass -DCMAKE_BUILD_TYPE=<type> in order to configure LLVM.; Available options are:; * -DCMAKE_BUILD_TYPE=Release - For an optimized build with no assertions or debug info.; * -DCMAKE_BUILD_TYPE=Debug - For an unoptimized build with assertions and debug info.; * -DCMAKE_BUILD_TYPE=RelWithDebInfo - For an optimized build with no assertions but with debug info.; * -DCMAKE_BUILD_TYPE=MinSizeRel - For a build optimized for size instead of speed.; Learn more about these options in our documentation at https://llvm.org/docs/CMake.html#cmake-build-type; ""); endif(). # Set default build type for cmake's try_compile module.; # CMake 3.17 or newer sets CMAKE_DEFAULT_BUILD_TYPE to one of the; # items from CMAKE_CONFIGURATION_TYPES. Logic below can be further; # simplified once LLVM's minimum CMake version is updated to 3.17.; if(CMAKE_DEFAULT_BUILD_TYPE); set(CMAKE_TRY_COMPILE_CONFIGURATION ${CMAKE_DEFAULT_BUILD_TYPE}); else(); if(CMAKE_CONFIGURATION_TYPES); list(GET CMAKE_CONFIGURATION_TYPES 0 CMAKE_TRY_COMPILE_CONFIGURATION); elseif(CMAKE_BUILD_TYPE); set(CMAKE_TRY_COMPILE_CONFIGURATION ${CMAKE_BUILD_TYPE}); endif(); endif(). # Side-by-side subprojects layout: automatically set the; # LLVM_EXTERNAL_${project}_SOURCE_DIR using LLVM_ALL_PROJECTS; # This allows an easy way of setting up a build directory for llvm and another; # one for llvm+clang+... using the same sources.; set(LLVM_ALL_PROJ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:4307,optimiz,optimized,4307,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['optimiz'],['optimized']
Performance,"_indirect(void *callee)`` will be inserted on every indirect call. The functions `__sanitizer_cov_trace_pc_*` should be defined by the user. Example:. .. code-block:: c++. // trace-pc-guard-cb.cc; #include <stdint.h>; #include <stdio.h>; #include <sanitizer/coverage_interface.h>. // This callback is inserted by the compiler as a module constructor; // into every DSO. 'start' and 'stop' correspond to the; // beginning and end of the section with the guards for the entire; // binary (executable or DSO). The callback will be called at least; // once per DSO and may be called multiple times with the same parameters.; extern ""C"" void __sanitizer_cov_trace_pc_guard_init(uint32_t *start,; uint32_t *stop) {; static uint64_t N; // Counter for the guards.; if (start == stop || *start) return; // Initialize only once.; printf(""INIT: %p %p\n"", start, stop);; for (uint32_t *x = start; x < stop; x++); *x = ++N; // Guards should start from 1.; }. // This callback is inserted by the compiler on every edge in the; // control flow (some optimizations apply).; // Typically, the compiler will emit the code like this:; // if(*guard); // __sanitizer_cov_trace_pc_guard(guard);; // But for large functions it will emit a simple call:; // __sanitizer_cov_trace_pc_guard(guard);; extern ""C"" void __sanitizer_cov_trace_pc_guard(uint32_t *guard) {; if (!*guard) return; // Duplicate the guard check.; // If you set *guard to 0 this code will not be called again for this edge.; // Now you can get the PC and do whatever you want:; // store it somewhere or symbolize it and print right away.; // The values of `*guard` are as you set them in; // __sanitizer_cov_trace_pc_guard_init and so you can make them consecutive; // and use them to dereference an array or a bit vector.; void *PC = __builtin_return_address(0);; char PcDescr[1024];; // This function is a part of the sanitizer run-time.; // To use it, link with AddressSanitizer or other sanitizer.; __sanitizer_symbolize_pc(PC, ""%p %F %L"", PcDescr, size",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SanitizerCoverage.rst:2250,optimiz,optimizations,2250,interpreter/llvm-project/clang/docs/SanitizerCoverage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/SanitizerCoverage.rst,1,['optimiz'],['optimizations']
Performance,"_int_vp_add:. '``llvm.vp.add.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.add.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.add.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.add.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated integer addition of two vectors of integers. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of integer type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.add``' intrinsic performs integer addition (:ref:`add <i_add>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.add.v4i32(<4 x i32> %a, <4 x i32> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = add <4 x i32> %a, %b; %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_sub:. '``llvm.vp.sub.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.sub.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.sub.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.sub.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """"""",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:698245,perform,performs,698245,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. fence acq_rel - agent *none* 1. buffer_wbl2 sc1=1. - If OpenCL and; address space is; local, omit.; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_inv.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the cache. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. fence acq_rel - system *none* 1",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:327744,load,load,327744,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"_load sc0=1; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_inv. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. load atomic acquire - workgroup - generic 1. flat_load sc0=1; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - agent - global 1. buffer/global_load; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensure",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:296732,load,load,296732,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"_store; sc0=1 sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic sc0=1; load atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_load; load atomic monotonic - agent - global 1. buffer/global/flat_load; - generic sc1=1; load atomic monotonic - system - global 1. buffer/global/flat_load; - generic sc0=1 sc1=1; store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; store atomic monotonic - workgroup - global 1. buffer/global/flat_store; - generic sc0=1; store atomic monotonic - agent - global 1. buffer/global/flat_store; - generic sc1=1; store atomic monotonic - system - global 1. buffer/global/flat_store; - generic sc0=1 sc1=1; store atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_store; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; atomicrmw monotonic - system - global 1. buffer/global/flat_atomic; - generic sc1=1; atomicrmw monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - work",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:294396,load,load,294396,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"_strong`` local variables may be released as soon as the value in; the variable is no longer being used: either the variable stops; being used completely or a new value is stored in the variable. The intent of the third rule is to say that return values may be; released after they've been used. A computation history depends on a pointer value ``P`` if it:. * performs a pointer comparison with ``P``,; * loads from ``P``,; * stores to ``P``,; * depends on a pointer value ``Q`` derived via pointer arithmetic; from ``P`` (including an instance-variable or field access), or; * depends on a pointer value ``Q`` loaded from ``P``. Dependency applies only to values derived directly or indirectly from; a particular expression result and does not occur merely because a; separate pointer value dynamically aliases ``P``. Furthermore, this; dependency is not carried by values that are stored to objects. .. admonition:: Rationale. The restrictions on dependency are intended to make this analysis; feasible by an optimizer with only incomplete information about a; program. Essentially, dependence is carried to ""obvious"" uses of a; pointer. Merely passing a pointer argument to a function does not; itself cause dependence, but since generally the optimizer will not; be able to prove that the function doesn't depend on that parameter,; it will be forced to conservatively assume it does. Dependency propagates to values loaded from a pointer because those; values might be invalidated by deallocating the object. For; example, given the code ``__strong id x = p->ivar;``, ARC must not; move the release of ``p`` to between the load of ``p->ivar`` and the; retain of that value for storing into ``x``. Dependency does not propagate through stores of dependent pointer; values because doing so would allow dependency to outlive the; full-expression which produced the original value. For example, the; address of an instance variable could be written to some global; location and then freely accessed ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:80526,optimiz,optimizer,80526,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimizer']
Performance,"_t color = 1);; axis->SetLabelColor(Color_t color = 1);; axis->SetLabelFont(Style_t font = 62);; axis->SetLabelOffset(Float_t offset = 0.005);; axis->SetLabelSize(Float_t size = 0.04);; axis->SetNdivisions(Int_t n = 510, Bool_t optim = kTRUE);; axis->SetNoExponent(Bool_t noExponent = kTRUE);; axis->SetTickLength(Float_t length = 0.03);; axis->SetTitleOffset(Float_t offset = 1);; axis->SetTitleSize(Float_t size = 0.02);; ```. The getters corresponding to the described setters are also available.; The general options, not specific to axis, as for instance; `SetTitleTextColor()` are valid and do have an effect on axis; characteristics. ### Setting the Number of Divisions. Use `TAxis::SetNdivisions(ndiv,optim)` to set the number of divisions; for an axis. The `ndiv` and `optim` are as follows:. - `ndiv = N1 + 100*N2 + 10000*N3`. - `N1 =` number of first divisions. - `N2 =` number of secondary divisions. - `N3 =` number of tertiary divisions. - `optim = kTRUE ` (default), the divisions' number will be optimized; around the specified value. - `optim = kFALSE, ` or n \< 0, the axis will be forced to use exactly; n divisions. For example:. `ndiv = 0`: no tick marks. `ndiv = 2`: 2 divisions, one tick mark in the middle of the axis. `ndiv = 510`: 10 primary divisions, 5 secondary divisions. `ndiv = -10`: exactly 10 primary divisions. ### Zooming the Axis. You can use `TAxis::SetRange` or `TAxis::SetRangeUser` to zoom the axis. ``` {.cpp}; TAxis::SetRange(Int_t binfirst,Int_t binlast); ```. The `SetRange` method parameters are bin numbers. They are not axis. For; example if a histogram plots the values from 0 to 500 and has 100 bins,; `SetRange(0,10)` will cover the values 0 to 50. The parameters for; `SetRangeUser` are user coordinates. If the start or end is in the; middle of a bin the resulting range is approximation. It finds the low; edge bin for the start and the high edge bin for the high. ``` {.cpp}; TAxis::SetRangeUser(Axis_t ufirst,Axis_t ulast); ```. Both methods, `S",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:51767,optimiz,optimized,51767,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['optimiz'],['optimized']
Performance,"_trap`` instruction. For usage see:. - :ref:`amdgpu-trap-handler-for-amdhsa-os-v2-table`; - :ref:`amdgpu-trap-handler-for-amdhsa-os-v3-table`; - :ref:`amdgpu-trap-handler-for-amdhsa-os-v4-onwards-table`. .. table:: AMDGPU Trap Handler for AMDHSA OS Code Object V2; :name: amdgpu-trap-handler-for-amdhsa-os-v2-table. =================== =============== =============== =======================================; Usage Code Sequence Trap Handler Description; Inputs; =================== =============== =============== =======================================; reserved ``s_trap 0x00`` Reserved by hardware.; ``debugtrap(arg)`` ``s_trap 0x01`` ``SGPR0-1``: Reserved for Finalizer HSA ``debugtrap``; ``queue_ptr`` intrinsic (not implemented).; ``VGPR0``:; ``arg``; ``llvm.trap`` ``s_trap 0x02`` ``SGPR0-1``: Causes wave to be halted with the PC at; ``queue_ptr`` the trap instruction. The associated; queue is signalled to put it into the; error state. When the queue is put in; the error state, the waves executing; dispatches on the queue will be; terminated.; ``llvm.debugtrap`` ``s_trap 0x03`` *none* - If debugger not enabled then behaves; as a no-operation. The trap handler; is entered and immediately returns to; continue execution of the wavefront.; - If the debugger is enabled, causes; the debug trap to be reported by the; debugger and the wavefront is put in; the halt state with the PC at the; instruction. The debugger must; increment the PC and resume the wave.; reserved ``s_trap 0x04`` Reserved.; reserved ``s_trap 0x05`` Reserved.; reserved ``s_trap 0x06`` Reserved.; reserved ``s_trap 0x07`` Reserved.; reserved ``s_trap 0x08`` Reserved.; reserved ``s_trap 0xfe`` Reserved.; reserved ``s_trap 0xff`` Reserved.; =================== =============== =============== =======================================. .. .. table:: AMDGPU Trap Handler for AMDHSA OS Code Object V3; :name: amdgpu-trap-handler-for-amdhsa-os-v3-table. =================== =============== =============== ================",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:381387,queue,queue,381387,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['queue'],['queue']
Performance,"_udiv:. '``llvm.vp.udiv.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.udiv.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.udiv.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.udiv.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated, unsigned division of two vectors of integers. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of integer type. The third operand is the vector mask and has the same number of elements as the result vector type. The fourth operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.udiv``' intrinsic performs unsigned division; (:ref:`udiv <i_udiv>`) of the first and second vector operand on each enabled; lane. The result on disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.udiv.v4i32(<4 x i32> %a, <4 x i32> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = udiv <4 x i32> %a, %b; %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_srem:. '``llvm.vp.srem.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.srem.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.srem.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.srem.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overv",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:704022,perform,performs,704022,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; atomicrmw acquire - workgroup - local 1. ds/flat_atomic; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. atomicrmw acquire - agent - global 1. buffer/global_atomic; - system 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - agent - generic 1. flat_atomic; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - singlethread *none* *none*; - wavefront; fence acquire - workgroup *none* 1. s_waitcnt lgkmcnt(0). - If OpenCL and; address space is; not generic, omit.; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate. If; fence had an; address space then; set to address; space of OpenCL; fence flag, or to; generic if both; local and global; flags are; specified.; - Must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:216249,load,loads,216249,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; have; completed before; performing the; store that is being; released. 2. buffer/global/flat_store; store atomic release - workgroup - local 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_store; store atomic release - agent - global 1. s_waitcnt lgkmcnt(0) &; - system - generic vmcnt(0) & vscnt(0). - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt vscnt(0); and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any pr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:356161,load,load,356161,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"_wbinvl1_vol. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. fence acquire - system *none* 1. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL and; address space is; not generic, omit; lgkmcnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; fence-paired-atomic).; - Must happen before; the following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures that the; fence-paired atomic; has completed; before invalidating; the; cache. Therefore; any following; locations read must; be no older than; the value read by; the; fence-paired-atomic. 2. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before any; following global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes.; **Release Atomic**; ------------------------------------------------------------------------------------; store atomic release - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; store atomic release - singlethread - local *If TgSplit execution mode,; - wavefr",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:256018,load,load,256018,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"` --- Indexed store.; * ``setConvertAction`` --- Type conversion.; * ``setCondCodeAction`` --- Support for a given condition code. Note: on older releases, ``setLoadXAction`` is used instead of; ``setLoadExtAction``. Also, on older releases, ``setCondCodeAction`` may not; be supported. Examine your release to see what methods are specifically; supported. These callbacks are used to determine that an operation does or does not work; with a specified type (or types). And in all cases, the third parameter is a; ``LegalAction`` type enum value: ``Promote``, ``Expand``, ``Custom``, or; ``Legal``. ``SparcISelLowering.cpp`` contains examples of all four; ``LegalAction`` values. Promote; ^^^^^^^. For an operation without native support for a given type, the specified type; may be promoted to a larger type that is supported. For example, SPARC does; not support a sign-extending load for Boolean values (``i1`` type), so in; ``SparcISelLowering.cpp`` the third parameter below, ``Promote``, changes; ``i1`` type values to a large type before loading. .. code-block:: c++. setLoadExtAction(ISD::SEXTLOAD, MVT::i1, Promote);. Expand; ^^^^^^. For a type without native support, a value may need to be broken down further,; rather than promoted. For an operation without native support, a combination; of other operations may be used to similar effect. In SPARC, the; floating-point sine and cosine trig operations are supported by expansion to; other operations, as indicated by the third parameter, ``Expand``, to; ``setOperationAction``:. .. code-block:: c++. setOperationAction(ISD::FSIN, MVT::f32, Expand);; setOperationAction(ISD::FCOS, MVT::f32, Expand);. Custom; ^^^^^^. For some operations, simple type promotion or operation expansion may be; insufficient. In some cases, a special intrinsic function must be implemented. For example, a constant value may require special treatment, or an operation; may require spilling and restoring registers in the stack and working with; register allocat",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:58949,load,loading,58949,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['load'],['loading']
Performance,"` and placed in; ``XXXGenRegisterInfo.h.inc``. For example, the implementation of the; constructor for the SparcTargetLowering class (in ``SparcISelLowering.cpp``); starts with the following code:. .. code-block:: c++. addRegisterClass(MVT::i32, SP::IntRegsRegisterClass);; addRegisterClass(MVT::f32, SP::FPRegsRegisterClass);; addRegisterClass(MVT::f64, SP::DFPRegsRegisterClass);. You should examine the node types in the ``ISD`` namespace; (``include/llvm/CodeGen/SelectionDAGNodes.h``) and determine which operations; the target natively supports. For operations that do **not** have native; support, add a callback to the constructor for the ``XXXTargetLowering`` class,; so the instruction selection process knows what to do. The ``TargetLowering``; class callback methods (declared in ``llvm/Target/TargetLowering.h``) are:. * ``setOperationAction`` --- General operation.; * ``setLoadExtAction`` --- Load with extension.; * ``setTruncStoreAction`` --- Truncating store.; * ``setIndexedLoadAction`` --- Indexed load.; * ``setIndexedStoreAction`` --- Indexed store.; * ``setConvertAction`` --- Type conversion.; * ``setCondCodeAction`` --- Support for a given condition code. Note: on older releases, ``setLoadXAction`` is used instead of; ``setLoadExtAction``. Also, on older releases, ``setCondCodeAction`` may not; be supported. Examine your release to see what methods are specifically; supported. These callbacks are used to determine that an operation does or does not work; with a specified type (or types). And in all cases, the third parameter is a; ``LegalAction`` type enum value: ``Promote``, ``Expand``, ``Custom``, or; ``Legal``. ``SparcISelLowering.cpp`` contains examples of all four; ``LegalAction`` values. Promote; ^^^^^^^. For an operation without native support for a given type, the specified type; may be promoted to a larger type that is supported. For example, SPARC does; not support a sign-extending load for Boolean values (``i1`` type), so in; ``SparcISelLowering.c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:57871,load,load,57871,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['load'],['load']
Performance,"` attribute. the pointer name is formed by combining ``__imp_``; and the function or variable name. On XCOFF targets, ``dllexport`` indicates; that the symbol will be made visible to other modules using ""exported""; visibility and thus placed by the linker in the loader section symbol table.; Since this storage class exists for defining a dll interface, the compiler,; assembler and linker know it is externally referenced and must refrain from; deleting the symbol. A symbol with ``internal`` or ``private`` linkage cannot have a DLL storage; class. .. _tls_model:. Thread Local Storage Models; ---------------------------. A variable may be defined as ``thread_local``, which means that it will; not be shared by threads (each thread will have a separated copy of the; variable). Not all targets support thread-local variables. Optionally, a; TLS model may be specified:. ``localdynamic``; For variables that are only used within the current shared library.; ``initialexec``; For variables in modules that will not be loaded dynamically.; ``localexec``; For variables defined in the executable and only used within it. If no explicit model is given, the ""general dynamic"" model is used. The models correspond to the ELF TLS models; see `ELF Handling For; Thread-Local Storage <http://people.redhat.com/drepper/tls.pdf>`_ for; more information on under which circumstances the different models may; be used. The target may choose a different TLS model if the specified; model is not supported, or if a better choice of model can be made. A model can also be specified in an alias, but then it only governs how; the alias is accessed. It will not have any effect in the aliasee. For platforms without linker support of ELF TLS model, the -femulated-tls; flag can be used to generate GCC compatible emulated TLS code. .. _runtime_preemption_model:. Runtime Preemption Specifiers; -----------------------------. Global variables, functions and aliases may have an optional runtime preemption; specifie",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:26018,load,loaded,26018,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,"` class to represent; machine instructions supported by the target machine. * Describe the selection and conversion of the LLVM IR from a Directed Acyclic; Graph (DAG) representation of instructions to native target-specific; instructions. Use TableGen to generate code that matches patterns and; selects instructions based on additional information in a target-specific; version of ``TargetInstrInfo.td``. Write code for ``XXXISelDAGToDAG.cpp``,; where ``XXX`` identifies the specific target, to perform pattern matching and; DAG-to-DAG instruction selection. Also write code in ``XXXISelLowering.cpp``; to replace or remove operations and data types that are not supported; natively in a SelectionDAG. * Write code for an assembly printer that converts LLVM IR to a GAS format for; your target machine. You should add assembly strings to the instructions; defined in your target-specific version of ``TargetInstrInfo.td``. You; should also write code for a subclass of ``AsmPrinter`` that performs the; LLVM-to-assembly conversion and a trivial subclass of ``TargetAsmInfo``. * Optionally, add support for subtargets (i.e., variants with different; capabilities). You should also write code for a subclass of the; ``TargetSubtarget`` class, which allows you to use the ``-mcpu=`` and; ``-mattr=`` command-line options. * Optionally, add JIT support and create a machine code emitter (subclass of; ``TargetJITInfo``) that is used to emit binary code directly into memory. In the ``.cpp`` and ``.h``. files, initially stub up these methods and then; implement them later. Initially, you may not know which private members that; the class will need and which components will need to be subclassed. Preliminaries; -------------. To actually create your compiler backend, you need to create and modify a few; files. The absolute minimum is discussed here. But to actually use the LLVM; target-independent code generator, you must perform the steps described in the; :doc:`LLVM Target-Independent Code Gen",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:5156,perform,performs,5156,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['perform'],['performs']
Performance,"` directive is used to specify hints for optimizing the; subsequent for, while, do-while, or c++11 range-based for loop. The directive; provides options for vectorization, interleaving, predication, unrolling and; distribution. Loop hints can be specified before any loop and will be ignored if; the optimization is not safe to apply. There are loop hints that control transformations (e.g. vectorization, loop; unrolling) and there are loop hints that set transformation options (e.g.; ``vectorize_width``, ``unroll_count``). Pragmas setting transformation options; imply the transformation is enabled, as if it was enabled via the corresponding; transformation pragma (e.g. ``vectorize(enable)``). If the transformation is; disabled (e.g. ``vectorize(disable)``), that takes precedence over; transformations option pragmas implying that transformation. Vectorization, Interleaving, and Predication; --------------------------------------------. A vectorized loop performs multiple iterations of the original loop; in parallel using vector instructions. The instruction set of the target; processor determines which vector instructions are available and their vector; widths. This restricts the types of loops that can be vectorized. The vectorizer; automatically determines if the loop is safe and profitable to vectorize. A; vector instruction cost model is used to select the vector width. Interleaving multiple loop iterations allows modern processors to further; improve instruction-level parallelism (ILP) using advanced hardware features,; such as multiple execution units and out-of-order execution. The vectorizer uses; a cost model that depends on the register pressure and generated code size to; select the interleaving count. Vectorization is enabled by ``vectorize(enable)`` and interleaving is enabled; by ``interleave(enable)``. This is useful when compiling with ``-Os`` to; manually enable vectorization or interleaving. .. code-block:: c++. #pragma clang loop vectorize(enable); #",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:163284,perform,performs,163284,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['perform'],['performs']
Performance,"` directives fail,; FileCheck will recover by continuing to the next block, allowing multiple test; failures to be detected in a single invocation. There is no requirement that ``CHECK-LABEL:`` directives contain strings that; correspond to actual syntactic labels in a source or output language: they must; simply uniquely match a single line in the file being verified. ``CHECK-LABEL:`` directives cannot contain variable definitions or uses. Directive modifiers; ~~~~~~~~~~~~~~~~~~~. A directive modifier can be append to a directive by following the directive; with ``{<modifier>}`` where the only supported value for ``<modifier>`` is; ``LITERAL``. The ``LITERAL`` directive modifier can be used to perform a literal match. The; modifier results in the directive not recognizing any syntax to perform regex; matching, variable capture or any substitutions. This is useful when the text; to match would require excessive escaping otherwise. For example, the; following will perform literal matches rather than considering these as; regular expressions:. .. code-block:: text. Input: [[[10, 20]], [[30, 40]]]; Output %r10: [[10, 20]]; Output %r10: [[30, 40]]. ; CHECK{LITERAL}: [[[10, 20]], [[30, 40]]]; ; CHECK-DAG{LITERAL}: [[30, 40]]; ; CHECK-DAG{LITERAL}: [[10, 20]]. FileCheck Regex Matching Syntax; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. All FileCheck directives take a pattern to match.; For most uses of FileCheck, fixed string matching is perfectly sufficient. For; some things, a more flexible form of matching is desired. To support this,; FileCheck allows you to specify regular expressions in matching strings,; surrounded by double braces: ``{{yourregex}}``. FileCheck implements a POSIX; regular expression matcher; it supports Extended POSIX regular expressions; (ERE). Because we want to use fixed string matching for a majority of what we; do, FileCheck has been designed to support mixing and matching fixed string; matching with regular expressions. This allows you to write things li",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst:24250,perform,perform,24250,interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/FileCheck.rst,1,['perform'],['perform']
Performance,"` has to handle it though. It just means that; **something** must handle it. * operating on **vregs that can be loaded and stored** -- if necessary, the; target can select a ``G_LOAD``/``G_STORE`` of each gvreg operand. As opposed to SelectionDAG, there are no legalization phases. In particular,; 'type' and 'operation' legalization are not separate. Legalization is iterative, and all state is contained in GMIR. To maintain the; validity of the intermediate code, instructions are introduced:. * ``G_MERGE_VALUES`` --- concatenate multiple registers of the same; size into a single wider register. * ``G_UNMERGE_VALUES`` --- extract multiple registers of the same size; from a single wider register. * ``G_EXTRACT`` --- extract a simple register (as contiguous sequences of bits); from a single wider register. As they are expected to be temporary byproducts of the legalization process,; they are combined at the end of the :ref:`milegalizer` pass.; If any remain, they are expected to always be selectable, using loads and stores; if necessary. The legality of an instruction may only depend on the instruction itself and; must not depend on any context in which the instruction is used. However, after; deciding that an instruction is not legal, using the context of the instruction; to decide how to legalize the instruction is permitted. As an example, if we; have a ``G_FOO`` instruction of the form::. %1:_(s32) = G_CONSTANT i32 1; %2:_(s32) = G_FOO %0:_(s32), %1:_(s32). it's impossible to say that G_FOO is legal iff %1 is a ``G_CONSTANT`` with; value ``1``. However, the following::. %2:_(s32) = G_FOO %0:_(s32), i32 1. can say that it's legal iff operand 2 is an immediate with value ``1`` because; that information is entirely contained within the single instruction. .. _api-legalizerinfo:. API: LegalizerInfo; ^^^^^^^^^^^^^^^^^^. The recommended [#legalizer-legacy-footnote]_ API looks like this::. getActionDefinitionsBuilder({G_ADD, G_SUB, G_MUL, G_AND, G_OR, G_XOR, G_SHL}); .legal",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/Legalizer.rst:1346,load,loads,1346,interpreter/llvm-project/llvm/docs/GlobalISel/Legalizer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GlobalISel/Legalizer.rst,1,['load'],['loads']
Performance,"` operators for TF1 objects built with formula expressions; - `TF1(""model"", ""NSUM(gaus , expo)"", xmin, xmax)` will create a function composed of a normalized sum of a gaussian and an exponential.; - `TF1(""voigt"", ""CONV(breitwigner, gausn) , -15, 15)` will create a TF1 object made of a convolution between a Breit-Wigner and a Gaussian. ; - `TFormula` supports vectorization. All the `TF1` objected created with a formula expression can have a vectorized signature using `ROOT::Double_v`: `TF1::EvalPar( ROOT::Double_v * x,; double * p)`. The vectorization can then be used to speed-up fitting. It is not enabled by default, but it can be enabled by callig `TF1::SetVectorized(true)` or using the `""VEC""` option in the; constructor of TF1, when ROOT has been built with VecCore and one vectorization library such as Vc. ; - Added new auto-binning algorithm, referred to as `power-2`, which uses power of 2 bin widths to create bins; that are mergeable. The target use-case is support for auto-binning in multi-process or multi-thread execution,; e.g. `TDataFrame`, without the need of a synchronization point.; The new `power-2` algorithm is activated by setting the new `TH1::kAutoBinPTwo` status bit on the histogram.; The tutorial `tutorials/multicore/mt304_fillHistos.C` gives an example of how to use the functionality with; `TThreadedObject<TH1D>` . The `power-2` binning is currently available only for 1D histograms. ## Math Libraries; - The Fitting functions now support vectorization and parallelization.; - Added padding in the fit data classes for correct loading of SIMD arrays. ## RooFit Libraries. - Apply several fixes from the ATLAS Higgs combination branch of RooFit. These fixes include; - fix for computing the contraint normalization. This requires now the option GlobalObservables when creating the NLL.; - All the `RooAbsPdf::createNLL` used in The RooStats classes have been updated to include the `GlobalObservables` option.; - Remove the `Roo1DMomentMorphFunction` and repla",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md:17808,multi-thread,multi-thread,17808,README/ReleaseNotes/v612/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v612/index.md,1,['multi-thread'],['multi-thread']
Performance,` to only invalidate the volatile cache lines. The code sequences used to implement the memory model for GFX6-GFX9 are defined; in table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx6-gfx9-table`. .. table:: AMDHSA Memory Model Code Sequences GFX6-GFX9; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx6-gfx9-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX6-GFX9; Space; ============ ============ ============== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; glc=1 slc=1. - volatile. 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1. - volatile. 1. buffer/global/flat_store; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; --------------------------------------------------,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:212335,load,load,212335,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"` to use; inline code (e.g. the x86 ``sqrtsd`` instruction) without additional; checking to ensure that ``errno`` is set appropriately.; ``-fno-math-errno`` permits these transformations. On some targets, math library functions never set ``errno``, and so; ``-fno-math-errno`` is the default. This includes most BSD-derived; systems, including Darwin. .. option:: -f[no-]trapping-math. Control floating point exception behavior. ``-fno-trapping-math`` allows optimizations that assume that floating point operations cannot generate traps such as divide-by-zero, overflow and underflow. - The option ``-ftrapping-math`` behaves identically to ``-ffp-exception-behavior=strict``.; - The option ``-fno-trapping-math`` behaves identically to ``-ffp-exception-behavior=ignore``. This is the default. .. option:: -ffp-contract=<value>. Specify when the compiler is permitted to form fused floating-point; operations, such as fused multiply-add (FMA). Fused operations are; permitted to produce more precise results than performing the same; operations separately. The C standard permits intermediate floating-point results within an; expression to be computed with more precision than their type would; normally allow. This permits operation fusing, and Clang takes advantage; of this by default. This behavior can be controlled with the ``FP_CONTRACT``; and ``clang fp contract`` pragmas. Please refer to the pragma documentation; for a description of how the pragmas interact with this option. Valid values are:. * ``fast`` (fuse across statements disregarding pragmas, default for CUDA); * ``on`` (fuse in the same statement unless dictated by pragmas, default for languages other than CUDA/HIP); * ``off`` (never fuse); * ``fast-honor-pragmas`` (fuse across statements unless dictated by pragmas, default for HIP). .. option:: -f[no-]honor-infinities. Allow floating-point optimizations that assume arguments and results are; not +-Inf.; Defaults to ``-fhonor-infinities``. If both ``-fno-honor-infiniti",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:56913,perform,performing,56913,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['perform'],['performing']
Performance,"` with an ``int`` type; of; these two instantiations, ``twice`` will be optimized (because its definition; was outside the region) and ``thrice`` will not be optimized. Clang also implements MSVC's range-based pragma,; ``#pragma optimize(""[optimization-list]"", on | off)``. At the moment, Clang only; supports an empty optimization list, whereas MSVC supports the arguments, ``s``,; ``g``, ``t``, and ``y``. Currently, the implementation of ``pragma optimize`` behaves; the same as ``#pragma clang optimize``. All functions; between ``off`` and ``on`` will be decorated with the ``optnone`` attribute. .. code-block:: c++. #pragma optimize("""", off); // This function will be decorated with optnone.; void f1() {}. #pragma optimize("""", on); // This function will be optimized with whatever was specified on; // the commandline.; void f2() {}. // This will warn with Clang's current implementation.; #pragma optimize(""g"", on); void f3() {}. For MSVC, an empty optimization list and ``off`` parameter will turn off; all optimizations, ``s``, ``g``, ``t``, and ``y``. An empty optimization and; ``on`` parameter will reset the optimizations to the ones specified on the; commandline. .. list-table:: Parameters (unsupported by Clang). * - Parameter; - Type of optimization; * - g; - Deprecated; * - s or t; - Short or fast sequences of machine code; * - y; - Enable frame pointers. Extensions for loop hint optimizations; ======================================. The ``#pragma clang loop`` directive is used to specify hints for optimizing the; subsequent for, while, do-while, or c++11 range-based for loop. The directive; provides options for vectorization, interleaving, predication, unrolling and; distribution. Loop hints can be specified before any loop and will be ignored if; the optimization is not safe to apply. There are loop hints that control transformations (e.g. vectorization, loop; unrolling) and there are loop hints that set transformation options (e.g.; ``vectorize_width``, ``unroll_c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:161794,optimiz,optimization,161794,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"`$ROOTSYS/bin on Windows`) directory and a top Python; module, `ROOT.py`, will be copied into the same place. The final step is; to setup the shell environment, which is similar to what is described in; the chapter ‘Environment Settings'. Note that the `$ROOTSYS` entries are; probably already there if you followed the standard instructions, and; that the `PYTHONDIR` entries should be replaced as appropriate by your; choice at configuration time, or be left out if you had the; configuration script pick up them up from a default location. ### Using PyROOT. Since it is an extension module, the usage of `PyROOT` probably comes; naturally if you're used to Python. In general, `PyROOT` attempts to; allow working in both Python and ROOT style, and although it is; succeeding, it isn't perfect: there are edges. The following sections; explain in some detail what you can expect, and what you need to watch; out for. #### Access to ROOT Classes. Before a ROOT class can be used from Python, its dictionary needs to be; loaded into the current process. Starting with ROOT version 4.00/06,; this happens automatically for all classes that are declared to the; auto-loading mechanism through so-called `rootmap` files. Effectively,; this means that all classes in the ROOT distributions are directly; available for import. For example:. ``` {.cpp}; from ROOT import TCanvas # available at startup; c = TCanvas(). from ROOT import TLorentzVector # triggers auto-load of libPhysics; l = TLorentzVector(); ```. Although it is not recommended, a simple way of working with `PyROOT` is; doing a global import:. ``` {.cpp}; from ROOT import *. c = TCanvas(); l = TLorentzVector(); ```. Keeping the ROOT namespace (""`import ROOT`""), or only importing from; ROOT those classes that you will actually use (see above), however, will; always be cleaner and clearer:. ``` {.cpp}; import ROOT. c = ROOT.TCanvas(); l = ROOT.TLorentzVector(); ```. Since it is foreseen that most people will use the simple approach; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:9464,load,loaded,9464,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['load'],['loaded']
Performance,"`'.o'``); and ``null`` for not emitting anything (for performance testing). Note that not all targets support all options. .. option:: -mattr=a1,+a2,-a3,... Override or control specific attributes of the target, such as whether SIMD; operations are enabled or not. The default set of attributes is set by the; current CPU. For a list of available attributes, use:. .. code-block:: none. llvm-as < /dev/null | llc -march=xyz -mattr=help. .. option:: --frame-pointer. Specify effect of frame pointer elimination optimization (all,non-leaf,none). .. option:: --disable-excess-fp-precision. Disable optimizations that may produce excess precision for floating point.; Note that this option can dramatically slow down code on some systems; (e.g. X86). .. option:: --enable-no-infs-fp-math. Enable optimizations that assume no Inf values. .. option:: --enable-no-nans-fp-math. Enable optimizations that assume no NAN values. .. option:: --enable-no-signed-zeros-fp-math. Enable FP math optimizations that assume the sign of 0 is insignificant. .. option:: --enable-no-trapping-fp-math. Enable setting the FP exceptions build attribute not to use exceptions. .. option:: --enable-unsafe-fp-math. Enable optimizations that make unsafe assumptions about IEEE math (e.g. that; addition is associative) or may not work for all input ranges. These; optimizations allow the code generator to make use of some instructions which; would otherwise not be usable (such as ``fsin`` on X86). .. option:: --stats. Print statistics recorded by code-generation passes. .. option:: --time-passes. Record the amount of time needed for each pass and print a report to standard; error. .. option:: --load=<dso_path>. Dynamically load ``dso_path`` (a path to a dynamically shared object) that; implements an LLVM target. This will permit the target name to be used with; the :option:`-march` option so that code can be generated for that target. .. option:: -meabi=[default|gnu|4|5]. Specify which EABI version should conform to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst:3624,optimiz,optimizations,3624,interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,1,['optimiz'],['optimizations']
Performance,"`), this; option controls the passes that will be included in the final report. If this option is not used, all the passes are included in the optimization; record. .. _opt_fdiagnostics-show-hotness:. .. option:: -f[no-]diagnostics-show-hotness. Enable profile hotness information in diagnostic line. This option controls whether Clang prints the profile hotness associated; with diagnostics in the presence of profile-guided optimization information.; This is currently supported with optimization remarks (see; :ref:`Options to Emit Optimization Reports <rpass>`). The hotness information; allows users to focus on the hot optimization remarks that are likely to be; more relevant for run-time performance. For example, in this output, the block containing the callsite of `foo` was; executed 3000 times according to the profile data:. ::. s.c:7:10: remark: foo inlined into bar (hotness: 3000) [-Rpass-analysis=inline]; sum += foo(x, x - 2);; ^. This option is implied when; :ref:`-fsave-optimization-record <opt_fsave-optimization-record>` is used.; Otherwise, it defaults to off. .. option:: -fdiagnostics-hotness-threshold. Prevent optimization remarks from being output if they do not have at least; this hotness value. This option, which defaults to zero, controls the minimum hotness an; optimization remark would need in order to be output by Clang. This is; currently supported with optimization remarks (see :ref:`Options to Emit; Optimization Reports <rpass>`) when profile hotness information in; diagnostics is enabled (see; :ref:`-fdiagnostics-show-hotness <opt_fdiagnostics-show-hotness>`). .. _opt_fdiagnostics-fixit-info:. .. option:: -f[no-]diagnostics-fixit-info. Enable ""FixIt"" information in the diagnostics output. This option, which defaults to on, controls whether or not Clang; prints the information on how to fix a specific diagnostic; underneath it when it knows. For example, in this output:. ::. test.c:28:8: warning: extra tokens at end of #endif directive [-Wextra-t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:14557,optimiz,optimization-record,14557,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,2,['optimiz'],['optimization-record']
Performance,"`)``; This operator produces the base 2 log of *a* and produces the integer; result. The log of 0 or a negative number produces an error. This; is a flooring operation. ``!lt(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is less than *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!mul(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator multiplies *a*, *b*, etc., and produces the product. ``!ne(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is not equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``,; or record values. Use ``!cast<string>`` to compare other types of objects. ``!not(``\ *a*\ ``)``; This operator performs a logical NOT on *a*, which must be; an integer. The argument 0 results in 1 (true); any other; argument results in 0 (false). ``!or(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise OR on *a*, *b*, etc., and produces the; result. A logical OR can be performed if all the arguments are either; 0 or 1. ``!range([``\ *start*\ ``,]`` *end*\ ``[, ``\ *step*\ ``])``; This operator produces half-open range sequence ``[start : end : step)`` as; ``list<int>``. *start* is ``0`` and *step* is ``1`` by default. *step* can; be negative and cannot be 0. If *start* ``<`` *end* and *step* is negative,; or *start* ``>`` *end* and *step* is positive, the result is an empty list; ``[]<list<int>>``. For example:. * ``!range(4)`` is equivalent to ``!range(0, 4, 1)`` and the result is; `[0, 1, 2, 3]`.; * ``!range(1, 4)`` is equivalent to ``!range(1, 4, 1)`` and the result is; `[1, 2, 3]`.; * The result of ``!range(0, 4, 2)`` is `[0, 2]`.; * The results of ``!range(0, 4, -1)`` and ``!range(4, 0, 1)`` are empty. ``!range(``\ *list*\ ``)``; Equivalent to ``!range(0, !size(list))``. ``!repr(``\ *value*\ ``)``; Represents *value* as a string. String format for the value is not; guaranteed to be stable. Intended for debugging purposes only. ``!setdagarg(``\ *dag*\ ``,``\ *k",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:69535,perform,performed,69535,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,1,['perform'],['performed']
Performance,"`**. ``` {.cpp}; v->GetClipSet()->SetClipType(TGLClipSet::kClipPlane);; ```. Configure the clip object with **`TGLClipSet::SetClipState`**. ``` {.cpp}; Double_t planeEq[4] = {0.5,1.0,-1.0, 2.0};; v->GetClipSet()->SetClipState(TGLClipSet::kClipPlane, planeEq);; ```. As with cameras, any clip can be configured at any time, but you must; set the clip current to see the effect. #### Manipulators. *Manipulators* are GUI ‘widgets' or controls attached to a 3D object in; the viewer, allowing a direct manipulation of the object's geometry.; There are three manipulators for the three basic geometries; transformations. In each case, the *manipulator* consists of three; components, one for each local axis of the object, shown in standard; colors: red (X), green (Y) and blue (Z). ![GL Viewer object manipulators](pictures/030000DE.png). Activate the *manipulator* by moving the mouse over one of these; components (which turns yellow to indicate active state). Click with; left mouse and drag this active component to perform the manipulation.; Toggle between the *manipulator* types using the ‘x', ‘c', ‘v' keys; while the mouse cursor is above the manipulator. Note: Manipulators; cannot be controlled via the API at present. #### Guides. Guides are visual aids drawn into the viewer world. Controls for these; are under the ""Guides"" tab:. Viewer Controls Pane Guides Tab. Axes show the world (global) frame *coordinate*directions: X (red), Y; (green) and Z (blue). The negative portion of the *axis* line is shown; in dark color, the positive in bright. The *axis* name and minimum /; maximum values are labeled in the same color. There are three options; for *axes* drawing - selected by radio buttons:. - None - not drawn (default). - Edge - draw axes on the (minimum) edge of the scene extents box. - Origin - drawn axes through the origin. For *edge axes*, the zero value for each axis is marked on the axis line; with a colored sphere. For *origin axes,* a single white sphere is shown; at the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:116752,perform,perform,116752,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['perform'],['perform']
Performance,"`**.; Bypassing the `Streamer` improves the performance when writing/reading; the objects in the **`TClonesArray`**. However, the drawback is when a; **`TClonesArray`** is written with `split=0` bypassing the `Streamer`,; the `StreamerInfo `of the class in the array being optimized, one cannot; later use the **`TClonesArray`** with `split > 0`. For example, there is; a problem with the following scenario: a class `Foo` has a; **`TClonesArray`** of `Bar` objects the `Foo` object is written with; `split=0` to `Tree` `T1`. In this case the `StreamerInfo` for the class; `Bar` is created in optimized mode in such a way that data members of; the same type are written as an array improving the I/O performance. In; a new program, `T1` is read and a new `Tree` `T2` is created with the; object `Foo` in `split > 1`. When the `T2 `branch is created, the `StreamerInfo` for the class `Bar`; is created with no optimization (mandatory for the split mode). The; optimized Bar `StreamerInfo` is going to be used to read the; **`TClonesArray`** in `T1`. The result will be `Bar` objects with data; member values not in the right sequence. The solution to this problem is; to call `BypassStreamer(kFALSE)` for the **`TClonesArray`**. In this; case, the normal `Bar::Streamer` function will be called. The; `Bar::Streamer` function works OK independently if the `Bar`; `StreamerInfo `had been generated in optimized mode or not. ## Pointers and References in Persistency. An object pointer as a data member presents a challenge to the streaming; software. If the object pointed to is saved every time, it could create; circular dependencies and consume a large amount of disk space. The; network of references must be preserved on disk and recreated upon; reading the file. If you use independent I/O operations for pointers and their referenced; objects you can use the **`TRef`** class. Later in this section is an; example that compares disk space, memory usage, and I/O times of C++; pointers and **`TRef",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md:54945,optimiz,optimized,54945,documentation/users-guide/InputOutput.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md,1,['optimiz'],['optimized']
Performance,"`**`::Safety ()` is invoked. A safety value less than; **`TGeoShape`**`::Tolerance()` will set the flag IsOnBoundary to true.; On the other hand, a safety value bigger than the proposed step will; stop the computation of the distance to next boundary, returning the; current geometry location with the meaning that the proposed step is; safe. The next stage is to check if computation of the distance to a give; physical object specified by a path was required. If this is the case,; the modeller changes the state to point to the required object, converts; the current point and direction coordinates to the local frame of this; object and computes the distance to its shape. The node returned is the; one pointed by the input path in case the shape is crossed; otherwise; the returned value is NULL. In case the distance to next crossed; boundary is required, the current point has to be physically INSIDE the; shape pointed by the current volume. This is only insured in case a call; to `TGeoManager::FindNode()` was performed for the current point.; Therefore, the first step is to convert the global current point and; direction in the local reference frame of the current volume and to; compute the distance to exit its shape from inside. The returned value; is again compared to the maximum allowed step (the proposed one) and in; case the distance is safe no other action is performed and the proposed; step is approved. In case the boundary is closer, the computed distance; is taken as maximum allowed step. For optimization purposed, for; particles starting very close to the current volume boundary (less than; 0.01 microns) and exiting the algorithm stops here. After computing the distance to exit the current node, the distance to; the daughter of the current volume which is crossed next is computed by; **`TGeoManager`**`::FindNextDaughterBoundary().` This computes the; distance to all daughter candidates that can be possibly crossed by; using volume voxelization. The algorithm is ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:162438,perform,performed,162438,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['performed']
Performance,"`, the linker creates a; ``lto_module_t`` by using one of:. .. code-block:: c. lto_module_create(const char*); lto_module_create_from_memory(const void*, size_t). and when done, the handle is released via. .. code-block:: c. lto_module_dispose(lto_module_t). The linker can introspect the non-native object file by getting the number of; symbols and getting the name and attributes of each symbol via:. .. code-block:: c. lto_module_get_num_symbols(lto_module_t); lto_module_get_symbol_name(lto_module_t, unsigned int); lto_module_get_symbol_attribute(lto_module_t, unsigned int). The attributes of a symbol include the alignment, visibility, and kind. Tools working with object files on Darwin (e.g. lipo) may need to know properties like the CPU type:. .. code-block:: c. lto_module_get_macho_cputype(lto_module_t mod, unsigned int *out_cputype, unsigned int *out_cpusubtype). ``lto_code_gen_t``; ------------------. Once the linker has loaded each non-native object files into an; ``lto_module_t``, it can request ``libLTO`` to process them all and generate a; native object file. This is done in a couple of steps. First, a code generator; is created with:. .. code-block:: c. lto_codegen_create(). Then, each non-native object file is added to the code generator with:. .. code-block:: c. lto_codegen_add_module(lto_code_gen_t, lto_module_t). The linker then has the option of setting some codegen options. Whether or not; to generate DWARF debug info is set with:. .. code-block:: c. lto_codegen_set_debug_model(lto_code_gen_t). which kind of position independence is set with:. .. code-block:: c. lto_codegen_set_pic_model(lto_code_gen_t). And each symbol that is referenced by a native object file or otherwise must not; be optimized away is set with:. .. code-block:: c. lto_codegen_add_must_preserve_symbol(lto_code_gen_t, const char*). After all these settings are done, the linker requests that a native object file; be created from the modules with the settings using:. .. code-block:: c.",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:10029,load,loaded,10029,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,1,['load'],['loaded']
Performance,"`--print-supported-cpus`. .. option:: -march=<cpu>. Specify that Clang should generate code for a specific processor family; member and later. For example, if you specify -march=i486, the compiler is; allowed to generate instructions that are valid on i486 and later processors,; but which may not exist on earlier ones. Code Generation Options; ~~~~~~~~~~~~~~~~~~~~~~~. .. option:: -O0, -O1, -O2, -O3, -Ofast, -Os, -Oz, -Og, -O, -O4. Specify which optimization level to use:. :option:`-O0` Means ""no optimization"": this level compiles the fastest and; generates the most debuggable code. :option:`-O1` Somewhere between :option:`-O0` and :option:`-O2`. :option:`-O2` Moderate level of optimization which enables most; optimizations. :option:`-O3` Like :option:`-O2`, except that it enables optimizations that; take longer to perform or that may generate larger code (in an attempt to; make the program run faster). :option:`-Ofast` Enables all the optimizations from :option:`-O3` along; with other aggressive optimizations that may violate strict compliance with; language standards. :option:`-Os` Like :option:`-O2` with extra optimizations to reduce code; size. :option:`-Oz` Like :option:`-Os` (and thus :option:`-O2`), but reduces code; size further. :option:`-Og` Like :option:`-O1`. In future versions, this option might; disable different optimizations in order to improve debuggability. :option:`-O` Equivalent to :option:`-O1`. :option:`-O4` and higher. Currently equivalent to :option:`-O3`. .. option:: -g, -gline-tables-only, -gmodules. Control debug information output. Note that Clang debug information works; best at :option:`-O0`. When more than one option starting with `-g` is; specified, the last one wins:. :option:`-g` Generate debug information. :option:`-gline-tables-only` Generate only line table debug information. This; allows for symbolicated backtraces with inlining information, but does not; include any information about variables, their locations or types. :option:",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst:10982,optimiz,optimizations,10982,interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/CommandGuide/clang.rst,2,['optimiz'],['optimizations']
Performance,"`-help`` will become categorized if an option category is; declared. The output looks something like ::. OVERVIEW: This is a small program to demo the LLVM CommandLine API; USAGE: Sample [options]. OPTIONS:. General options:. -help - Display available options (-help-hidden for more); -help-list - Display list of available options (-help-list-hidden for more). Stage Selection Options:; These control which stages are run. -E - Run preprocessor stage.; -c - Run all stages except linking. In addition to the behaviour of ``-help`` changing when an option category is; declared, the command line option ``-help-list`` becomes visible which will; print the command line options as uncategorized list. Note that Options that are not explicitly categorized will be placed in the; ``cl::getGeneralCategory()`` category. .. _Reference Guide:. Reference Guide; ===============. Now that you know the basics of how to use the CommandLine library, this section; will give you the detailed information you need to tune how command line options; work, as well as information on more ""advanced"" command line option processing; capabilities. .. _positional:; .. _positional argument:; .. _Positional Arguments:; .. _Positional arguments section:; .. _positional options:. Positional Arguments; --------------------. Positional arguments are those arguments that are not named, and are not; specified with a hyphen. Positional arguments should be used when an option is; specified by its position alone. For example, the standard Unix ``grep`` tool; takes a regular expression argument, and an optional filename to search through; (which defaults to standard input if a filename is not specified). Using the; CommandLine library, this would be specified as:. .. code-block:: c++. cl::opt<string> Regex (cl::Positional, cl::desc(""<regular expression>""), cl::Required);; cl::opt<string> Filename(cl::Positional, cl::desc(""<input file>""), cl::init(""-""));. Given these two option declarations, the ``-help`` output fo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst:25948,tune,tune,25948,interpreter/llvm-project/llvm/docs/CommandLine.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst,1,['tune'],['tune']
Performance,"`. Care must be taken in the case of graphic objects: when drawn on the; current pad, a reference to the graphics is kept that `PyROOT` isn't; currently aware of, and it is up to the developer to keep at lease one; Python reference alive. See `$ROOTSYS/tutorials/pyroot/zdemo.py`; (available in the latest release) for an example. Alternatively, one can; tell python to give up ownership for individual instances:. ``` {.cpp}; o = ROOT.TObject(); ROOT.SetOwnership( o, False ) # True to own, False to release; ```. #### Memory Management by Hand. If needed, you can explicitly destroy a ROOT object that you own through; its associated **`TClass`**:. ``` {.cpp}; myobject.IsA().Destructor(myobject); ```. which will send out the deletion notification to the system (thus you do; not need to care anymore at this point about Python reference counting,; the object will go, even if it's reference count it non-zero), and free; the memory. ### Performance. The performance of `PyROOT` when programming with ROOT in Python is; similar to that of Cling. Differences occur mainly because of differences; in the respective languages: C++ is much harder to parse, but once; parsed, it is much easier to optimize. Consequently, individual calls to; ROOT are typically faster from `PyROOT`, whereas loops are typically; slower. When programming in Python, the modus operandi is to consider; performance generally ""good enough"" on the outset, and when it turns out; that, it is not good enough; the performance critical part is converted; into C/C++ in an extension module. The school of thought where; pre-mature optimization is the root of all evil should find this way of; working very satisfying. In addition, if you look at their history, you; will see that many of the standard Python modules have followed this; path. Your code should always make maximum use of ROOT facilities; such that; most of the time is spending in compiled code. This goes even for very; simple things: e.g. do not compute invarian",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:22331,perform,performance,22331,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['perform'],['performance']
Performance,"`. If you want to use; `gArray` in the same script as its declaration, you can do so.; However, if you want use the script in the interpreter you have to; bracket the usage of `gArray` between `#if's,` since the definition is; not visible. If you add the following preprocessor statements:. ``` {.cpp}; #if !defined(__CLING__); int gArray[] = { 2, 3, 4};; #elif defined(__ROOTCLING__); int gArray[];; #endif; ```. `gArray` will be visible to `rootcling` but still not visible to Cling.; If you use ACLiC, `gArray` will be available at the command line and; be initialized properly by the compiled code. We recommend you always write scripts with the needed include; statements. In most cases, the script will still run with the; interpreter. However, a few header files are not handled very well by; Cling. These types of headers can be included in interpreted and compiled; mode:. - The subset of standard C/C++ headers defined in; `$ROOTSYS/Cling/include.`. - Headers of classes defined in a previously loaded library; (including ROOT own). The defined class must have a name known to; ROOT (i.e. a class with a `ClassDef`). Hiding header files from `rootcling` that are necessary for the; compiler but optional for the interpreter can lead to a subtle but; fatal error. For example:. ``` {.cpp}; #ifndef __CLING__; #include ""TTree.h""; #else; class TTree;; #endif. class subTree : public TTree {; };; ```. In this case, `rootcling` does not have enough information about the; `TTree` class to produce the correct dictionary file. If you try; this, `rootcling` and compiling will be error free, however,; instantiating a `subTree` object from the Cling command line will cause; a fatal error. In general, it is recommended to let `rootcling` see as; many header files as possible. ## Classes Defined By Scripts. Lets create a small class `TMyClass` and a derived class; `TChild`. The virtual method `TMyClass::Print() `is overridden in; `TChild`. Save this in file called `script4.C`. ``` {.cpp}; #in",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md:23567,load,loaded,23567,documentation/users-guide/Cling.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md,1,['load'],['loaded']
Performance,"`; <path>/RooChebychev.cxx:66:34: warning: 'createIterator' is deprecated: There is a superior alternative: begin(), end() and range-based for loops. [-Wdeprecated-declarations]; TIterator* coefIter = coefList.createIterator() ;; ^; 1 warning generated.; ```. ## TMVA. This release provides a consolidation and several fixes of the new machine learning tools provided in TMVA such as the Deep Learning module.; The method `TMVA::Types::kDL` should be used now for building Deep Learning architecture in TMVA, while `TMVA::Types::kDNN` is now deprecated. `TMVA::Types::kDL` provides all the functionality of `TMVA::Types::kDNN`, i.e building fully connected dense layer, but in addition supports building convolutional and recurrent neural network architectures.; These release contains improvements in the `MethodDL` such as:; - fix droput support for dense layer; - add protection to avoid returning NaN in the cross-entropy loss function. In addition we have :. - New `TMVA::Executor` class to control the multi-thread running of TMVA. By default now MT running will be enabled only when `ROOT::EnabledImplicitMT()` is called. But we can take the control of the threads by using `TMVA::gConfig().EnableMT(...)` and `TMVA::gConfig().DisableMT()`. ### PyMVA; - add support when using the Tensorflow backend in Keras to control the number of threads; - add possibility to control options for configuring GPU running. FOr example we can now set the mode to allocate memory only as needed. This is required when using the new RTX gaming cards from NVIDIA. ## 2D Graphics Libraries. - In the statistics painting for 2D histograms, the central cell of; the underflow/overflow grid was not properly rendered for very large contents.; This problem was reported [here](https://root-forum.cern.ch/t/stat-box-for-th2/).; - The automatic placement of legend now ""sees"" TMultiGraph and THStack.; - Improve and simplify the drawing the 2D histogram's option ""ARR"".; - The option ARR can be combined with the optio",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v618/index.md:17918,multi-thread,multi-thread,17918,README/ReleaseNotes/v618/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v618/index.md,1,['multi-thread'],['multi-thread']
Performance,"`; are used, or for the reasons defined in ``SIFrameLowering``.; 2. Runtime stack alignment is supported. SGPR34 is used as a base pointer (BP); to access the incoming stack arguments in the function. The BP is needed; only when the function requires the runtime stack alignment. 3. Allocating SGPR arguments on the stack are not supported. 4. No CFI is currently generated. See; :ref:`amdgpu-dwarf-call-frame-information`. .. note::. CFI will be generated that defines the CFA as the unswizzled address; relative to the wave scratch base in the unswizzled private address space; of the lowest address stack allocated local variable. ``DW_AT_frame_base`` will be defined as the swizzled address in the; swizzled private address space by dividing the CFA by the wavefront size; (since CFA is always at least dword aligned which matches the scratch; swizzle element size). If no dynamic stack alignment was performed, the stack allocated arguments; are accessed as negative offsets relative to ``DW_AT_frame_base``, and the; local variables and register spill slots are accessed as positive offsets; relative to ``DW_AT_frame_base``. 5. Function argument passing is implemented by copying the input physical; registers to virtual registers on entry. The register allocator can spill if; necessary. These are copied back to physical registers at call sites. The; net effect is that each function call can have these values in entirely; distinct locations. The IPRA can help avoid shuffling argument registers.; 6. Call sites are implemented by setting up the arguments at positive offsets; from SP. Then SP is incremented to account for the known frame size before; the call and decremented after the call. .. note::. The CFI will reflect the changed calculation needed to compute the CFA; from SP. 7. 4 byte spill slots are used in the stack frame. One slot is allocated for an; emergency spill slot. Buffer instructions are used for stack accesses and; not the ``flat_scratch`` instruction. .. TODO::. ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:398970,perform,performed,398970,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performed']
Performance,"`; or an :ref:`absolute expression<amdgpu_synid_absolute_expression>`.; ============================= ====================================================================. Examples:. .. parsed-literal::. offset:-1; offset:0xfffff; offset:-x. VINTRP/VINTERP/LDSDIR Modifiers; -------------------------------. .. _amdgpu_synid_high:. high; ~~~~. Specifies which half of the LDS word to use. Low half of LDS word is used by default. ======================================== ================================; Syntax Description; ======================================== ================================; high Use the high half of LDS word.; ======================================== ================================. neg; ~~~. See a description :ref:`here<amdgpu_synid_neg>`. .. _amdgpu_synid_wait_exp:. wait_exp; ~~~~~~~~. Specifies a wait on the EXP counter before issuing the current instruction.; The counter must be less than or equal to this value before the instruction is issued.; If set to 7, no wait is performed. The default value is zero. This is a safe value, but it may be suboptimal. ================ ======================================================; Syntax Description; ================ ======================================================; wait_exp:{0..7} An additional wait on the EXP counter before; issuing this instruction.; ================ ======================================================. .. _amdgpu_synid_wait_vdst:. wait_vdst; ~~~~~~~~~. Specifies a wait on the VA_VDST counter before issuing the current instruction.; The counter must be less than or equal to this value before the instruction is issued.; If set to 15, no wait is performed. The default value is zero. This is a safe value, but it may be suboptimal. ================== ======================================================; Syntax Description; ================== ======================================================; wait_vdst:{0..15} An additional wait on the VA_VDST counter before; issuing t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst:35975,perform,performed,35975,interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUModifierSyntax.rst,1,['perform'],['performed']
Performance,"`C++; ROOT::EnableImplicitMT(nThreads);; ```. For more information, please have a look at this [contribution to the ACAT 2021 conference](https://indico.cern.ch/event/855454/contributions/4596763/) or consult the [RooBatchComupte README](https://github.com/root-project/root/tree/v6-26-00-patches/roofit/batchcompute).; The README also describes how to enable BatchMode support for your own PDFs. ### Parallel calculation of likelihood gradients during fitting; This release features two new optional RooFit libraries: `RooFit::MultiProcess` and `RooFit::TestStatistics`.; To activate both, build with `-Droofit_multiprocess=ON`. The `RooFit::TestStatistics` namespace contains a major refactoring of the `RooAbsTestStatistic`-`RooAbsOptTestStatistic`-`RooNLLVar` inheritance tree into:. 1. statistics-based classes on the one hand;; 2. calculation/evaluation/optimization based classes on the other hand. The main selling point of using `RooFit::TestStatistics` from a performance point of view is the implementation of the `RooFit::MultiProcess` based `LikelihoodGradientJob` calculator class.; To use it to perform a ""migrad"" fit (using Minuit2), one should create a `RooMinimizer` using a new constructor with a `RooAbsL` likelihood parameter as follows:. ```c++; using RooFit::TestStatistics::RooAbsL;; using RooFit::TestStatistics::buildLikelihood;. RooAbsPdf* pdf = ...; // build a pdf; RooAbsData* data = ...; // get some data. std::shared_ptr<RooAbsL> likelihood = buildLikelihood(pdf, data, [OPTIONAL ARGUMENTS]);. RooMinimizer m(likelihood);; m.migrad();; ```. The `RooMinimizer` object behaves as usual, except that behind the scenes it will now calculate each partial derivative on a separate process, ideally running on a separate CPU core.; This can be used to speed up fits with many parameters (at least as many as there are cores to parallelize over), since every parameter corresponds to a partial derivative.; The resulting fit parameters will be identical to those obtained with ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v626/index.md:17009,perform,performance,17009,README/ReleaseNotes/v626/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v626/index.md,1,['perform'],['performance']
Performance,"`Instruction::applyMergedLocation``. The purpose of this rule is to ensure that a) the single merged instruction; has a location with an accurate scope attached, and b) to prevent misleading; single-stepping (or breakpoint) behavior. Often, merged instructions are memory; accesses which can trap: having an accurate scope attached greatly assists in; crash triage by identifying the (possibly inlined) function where the bad; memory access occurred. This rule is also meant to assist SamplePGO by banning; scenarios in which a sample of a block containing a merged instruction is; misattributed to a block containing one of the instructions-to-be-merged. Examples of transformations that should follow this rule include:. * Merging identical loads/stores which occur on both sides of a CFG diamond; (see the ``MergedLoadStoreMotion`` pass). * Merging identical loop-invariant stores (see the LICM utility; ``llvm::promoteLoopAccessesToScalars``). * Peephole optimizations which combine multiple instructions together, like; ``(add (mul A B) C) => llvm.fma.f32(A, B, C)``. Note that the location of; the ``fma`` does not exactly correspond to the locations of either the; ``mul`` or the ``add`` instructions. Examples of transformations for which this rule *does not* apply include:. * Block-local peepholes which delete redundant instructions, like; ``(sext (zext i8 %x to i16) to i32) => (zext i8 %x to i32)``. The inner; ``zext`` is modified but remains in its block, so the rule for; :ref:`preserving locations<WhenToPreserveLocation>` should apply. * Converting an if-then-else CFG diamond into a ``select``. Preserving the; debug locations of speculated instructions can make it seem like a condition; is true when it's not (or vice versa), which leads to a confusing; single-stepping experience. The rule for; :ref:`dropping locations<WhenToDropLocation>` should apply here. * Hoisting identical instructions which appear in several successor blocks into; a predecessor block (see ``BranchFold",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst:4317,optimiz,optimizations,4317,interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToUpdateDebugInfo.rst,1,['optimiz'],['optimizations']
Performance,"`Mangle``, that will be used for symbol mangling; (more on that later); and finally an LLVMContext that clients will use when; building IR files for the JIT. Next up we have our class constructor, which takes a `JITTargetMachineBuilder``; that will be used by our IRCompiler, and a ``DataLayout`` that we will use to; initialize our DL member. The constructor begins by initializing our; ObjectLayer. The ObjectLayer requires a reference to the ExecutionSession, and; a function object that will build a JIT memory manager for each module that is; added (a JIT memory manager manages memory allocations, memory permissions, and; registration of exception handlers for JIT'd code). For this we use a lambda; that returns a SectionMemoryManager, an off-the-shelf utility that provides all; the basic memory management functionality required for this chapter. Next we; initialize our CompileLayer. The CompileLayer needs three things: (1) A; reference to the ExecutionSession, (2) A reference to our object layer, and (3); a compiler instance to use to perform the actual compilation from IR to object; files. We use the off-the-shelf ConcurrentIRCompiler utility as our compiler,; which we construct using this constructor's JITTargetMachineBuilder argument.; The ConcurrentIRCompiler utility will use the JITTargetMachineBuilder to build; llvm TargetMachines (which are not thread safe) as needed for compiles. After; this, we initialize our supporting members: ``DL``, ``Mangler`` and ``Ctx`` with; the input DataLayout, the ExecutionSession and DL member, and a new default; constructed LLVMContext respectively. Now that our members have been initialized,; so the one thing that remains to do is to tweak the configuration of the; *JITDylib* that we will store our code in. We want to modify this dylib to; contain not only the symbols that we add to it, but also the symbols from our; REPL process as well. We do this by attaching a; ``DynamicLibrarySearchGenerator`` instance using the; ``Dynamic",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:7599,perform,perform,7599,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,1,['perform'],['perform']
Performance,"`ROOT::Math::Minimizer` interface to perform the minimization of the objective function.; More information about the function interface and the multi-dimensional minimization in ROOT is given in the Mathematical Library chapter. Here we present a detailed description of the `ROOT::Fit` classes and how to use them.; Using these classes instead of the interface provided directly in the ROOT data objects, like `TH1::Fit` allow are more fine control; to configure and customise the fits. For example, using these classes a combined fit of several histograms can be performed. To understand how these class work, let's go through a simple example, such as fitting an histogram. When fitting an histogram, instead of using `TH1::Fit` we will show in the following hot wo use the `ROOT::Fit` classes.; We will show how to perform the following different type of fits with the histogram data:; * a least square fit using the observed errors (Neyman chi-squared);; * a least square fit using the expected errors from the function (Pearson chi-squared);; * a binned likelihood fit;; * an extended unbinned likelihood fits, if the histogram has been set to store in the buffer the original data used to fill it. Let's go through all the steps required for performing these fits using the `ROOT::Fit::Fitter` class.; These steps are:; 1. Create the input fit data object.; 2. Create the input model function.; 3. Configure the fit.; 4. Perform the data fitting.; 5. Examine the result. ### Creating the input fit data. We have two types of input data, binned data (class `ROOT::Fit::BinData`) used for least square (chi-square) fits of histograms or `TGraph` objects; or un-binned data (class `ROOT::Fit::UnBinData`) used for; fitting vectors of data points (e.g. from a `TTree`). #### Using Binned data. Let's suppose we have an histogram, represented as a **`TH1`** type object (it can be one or multi-dimensional). The following shows how to create and; fill a `ROOT:Fit::BinData` object. ``` {.cpp}; ROOT:",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md:28916,perform,perform,28916,documentation/users-guide/FittingHistograms.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/FittingHistograms.md,1,['perform'],['perform']
Performance,"`TGeoVolume::RandomRays()`. This; shoots rays from a given point in the local reference frame with random; directions. The intersections with displayed nodes appear as segments; having the color of the touched node. ## The Drawing Package. ![](pictures/030001E3.png)The modeller provides a powerful drawing; package, supporting several different options of visualization. A; library separated from the main one provides all functionality being; linked with the underlying ROOT visualization system. This library is; dynamically loaded by the plug-in manager only when drawing features are; requested. The geometrical structures that can be visualized are volumes; and volume hierarchies. The main component of the visualization system is volume primitive; painting in a ROOT pad. Starting from this one, several specific options; or subsystems are available, like: X3D viewing using hidden line and; surface removal algorithms, OpenGL viewing\* or ray tracing. The method `TGeoManager::GetGeomPainter()`loads the painting library in; memory. This is generally not needed since it is called automatically by; `TGeoVolume::Draw()` as well as by few other methods setting; visualization attributes. ### Drawing Volumes and Hierarchies of Volumes. The first thing one would like to do after building some geometry is to; visualize the volume tree. This provides the fastest validation check; for most common coding or design mistakes. As soon as the geometry is; successfully closed, one should draw it starting from the top-level; volume:. ``` {.cpp}; //... code for geometry building; root[] gGeoManager->CloseGeometry();; root[] gGeoManager->GetMasterVolume()->Draw();; ```. Doing this ensures that the original top-level volume of the geometry is; drawn, even if another volume is currently the geometry `root`. OK, I; suppose you already did that with your simple geometry and immediately; noticed a new ROOT canvas popping-up and having some more or less; strange picture inside. Here are few questi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:137140,load,loads,137140,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['load'],['loads']
Performance,"`TRFIOFile`**), the library to be loaded (in short; `libRFIO.so` to RFIO), and the constructor to be called (e.g.; ""`TRFIOFile()`""). This can be specified in the `.rootrc` which already; contains many plugin definitions, or by calls to; `gROOT->GetPluginManager()->AddHandler()`. #### Library AutoLoading. When using a class in Cling, e.g. in an interpreted source file, ROOT; will automatically load the library that defines this class. On; start-up, ROOT parses all files ending on `.rootmap` rootmap that are; in one of the `$LD_LIBRARY_PATH` (or `$DYLD_LIBRARY_PATH` for `MacOS`,; or `$PATH` for `Windows`). They contain class names and the library; names that the class depends on. After reading them, ROOT knows which; classes are available, and which libraries to load for them. When `TSystem::Load(""ALib"")` is called, ROOT uses this information to; determine which libraries `libALib.so` depends on. It will load these; libraries first. Otherwise, loading the requested library could cause; a system (dynamic loader) error due to unresolved symbols. ### \$ROOTSYS/tutorials. tutorials The tutorials directory contains many example example; scripts. They assume some basic knowledge of ROOT, and for the new; user we recommend reading the chapters: ""Histograms"" and; ""Input/Output"" before trying the examples. The more experienced user; can jump to chapter ""The Tutorials and Tests"" to find more explicit; and specific information about how to build and run the examples. The `$ROOTSYS/tutorials/` directory include the following; sub-directories:. `fft`: Fast Fourier Transform with the fftw package `fit`: Several; examples illustrating minimization/fitting `foam`: Random generator in; multidimensional space `geom`: Examples of use of the geometry package; (**`TGeo`** classes) `gl`: Visualisation with OpenGL `graphics`: Basic; graphics `graphs`: Use of **`TGraph`**, **`TGraphErrors`**, etc.; `gui`: Scripts to create Graphical User Interface `hist`: Histogramming; `image`: Image Process",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Introduction.md:20403,load,loading,20403,documentation/users-guide/Introduction.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Introduction.md,2,['load'],"['loader', 'loading']"
Performance,`XTHeadBb``; LLVM implements `the THeadBb (basic bit-manipulation) vendor-defined instructions specified in <https://github.com/T-head-Semi/thead-extension-spec/releases/download/2.2.2/xthead-2023-01-30-2.2.2.pdf>`_ by T-HEAD of Alibaba. Instructions are prefixed with `th.` as described in the specification. ``XTHeadBs``; LLVM implements `the THeadBs (single-bit operations) vendor-defined instructions specified in <https://github.com/T-head-Semi/thead-extension-spec/releases/download/2.2.2/xthead-2023-01-30-2.2.2.pdf>`_ by T-HEAD of Alibaba. Instructions are prefixed with `th.` as described in the specification. ``XTHeadCondMov``; LLVM implements `the THeadCondMov (conditional move) vendor-defined instructions specified in <https://github.com/T-head-Semi/thead-extension-spec/releases/download/2.2.2/xthead-2023-01-30-2.2.2.pdf>`_ by T-HEAD of Alibaba. Instructions are prefixed with `th.` as described in the specification. ``XTHeadCmo``; LLVM implements `the THeadCmo (cache management operations) vendor-defined instructions specified in <https://github.com/T-head-Semi/thead-extension-spec/releases/download/2.2.2/xthead-2023-01-30-2.2.2.pdf>`_ by T-HEAD of Alibaba. Instructions are prefixed with `th.` as described in the specification. ``XTHeadFMemIdx``; LLVM implements `the THeadFMemIdx (indexed memory operations for floating point) vendor-defined instructions specified in <https://github.com/T-head-Semi/thead-extension-spec/releases/download/2.2.2/xthead-2023-01-30-2.2.2.pdf>`_ by T-HEAD of Alibaba. Instructions are prefixed with `th.` as described in the specification. ``XTheadMac``; LLVM implements `the XTheadMac (multiply-accumulate instructions) vendor-defined instructions specified in <https://github.com/T-head-Semi/thead-extension-spec/releases/download/2.2.2/xthead-2023-01-30-2.2.2.pdf>`_ by T-HEAD of Alibaba. Instructions are prefixed with `th.` as described in the specification. ``XTHeadMemIdx``; LLVM implements `the THeadMemIdx (indexed memory operations) ve,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/RISCVUsage.rst:14616,cache,cache,14616,interpreter/llvm-project/llvm/docs/RISCVUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/RISCVUsage.rst,1,['cache'],['cache']
Performance,"`` becomes; meaningless once all pointers are opaque. While direct usage of pointer element types is immediately apparent in code,; there is a more subtle issue that opaque pointers need to contend with: A lot; of code assumes that pointer equality also implies that the used load/store; type or GEP source element type is the same. Consider the following examples; with typed and opaque pointers:. .. code-block:: llvm. define i32 @test(i32* %p) {; store i32 0, i32* %p; %bc = bitcast i32* %p to i64*; %v = load i64, i64* %bc; ret i64 %v; }. define i32 @test(ptr %p) {; store i32 0, ptr %p; %v = load i64, ptr %p; ret i64 %v; }. Without opaque pointers, a check that the pointer operand of the load and; store are the same also ensures that the accessed type is the same. Using a; different type requires a bitcast, which will result in distinct pointer; operands. With opaque pointers, the bitcast is not present, and this check is no longer; sufficient. In the above example, it could result in store to load forwarding; of an incorrect type. Code making such assumptions needs to be adjusted to; check the accessed type explicitly:; ``LI->getType() == SI->getValueOperand()->getType()``. Frontends; ---------. Frontends need to be adjusted to track pointee types independently of LLVM,; insofar as they are necessary for lowering. For example, clang now tracks the; pointee type in the ``Address`` structure. Frontends using the C API through an FFI interface should be aware that a; number of C API functions are deprecated and will be removed as part of the; opaque pointer transition::. LLVMBuildLoad -> LLVMBuildLoad2; LLVMBuildCall -> LLVMBuildCall2; LLVMBuildInvoke -> LLVMBuildInvoke2; LLVMBuildGEP -> LLVMBuildGEP2; LLVMBuildInBoundsGEP -> LLVMBuildInBoundsGEP2; LLVMBuildStructGEP -> LLVMBuildStructGEP2; LLVMBuildPtrDiff -> LLVMBuildPtrDiff2; LLVMConstGEP -> LLVMConstGEP2; LLVMConstInBoundsGEP -> LLVMConstInBoundsGEP2; LLVMAddAlias -> LLVMAddAlias2. Additionally, it will no longer be ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst:9507,load,load,9507,interpreter/llvm-project/llvm/docs/OpaquePointers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/OpaquePointers.rst,1,['load'],['load']
Performance,"`` debugging information entry, or if TS divided by; 8 (the byte size) and rounded up to a whole number is not equal to S. .. note::. This definition allows the base type to be a bit size since there seems no; reason to restrict it. It is an evaluation error if any bit of the value is retrieved from the; undefined location storage or the offset of any bit exceeds the size of the; location storage LS specified by any single location description SL of L. See :ref:`amdgpu-dwarf-implicit-location-description-operations` for special; rules concerning implicit location descriptions created by the; ``DW_OP_implicit_pointer`` and ``DW_OP_LLVM_aspace_implicit_pointer``; operations. 5. ``DW_OP_xderef`` *Deprecated*. ``DW_OP_xderef`` pops two stack entries. The first must be an integral type; value that represents an address A. The second must be an integral type; value that represents a target architecture specific address space; identifier AS. The operation is equivalent to performing ``DW_OP_swap;; DW_OP_LLVM_form_aspace_address; DW_OP_deref``. The value V retrieved is left; on the stack with the generic type. *This operation is deprecated as the* ``DW_OP_LLVM_form_aspace_address``; *operation can be used and provides greater expressiveness.*. 6. ``DW_OP_xderef_size`` *Deprecated*. ``DW_OP_xderef_size`` has a single 1-byte unsigned integral constant that; represents a byte result size S. It pops two stack entries. The first must be an integral type value that; represents an address A. The second must be an integral type value that; represents a target architecture specific address space identifier AS. The operation is equivalent to performing ``DW_OP_swap;; DW_OP_LLVM_form_aspace_address; DW_OP_deref_size S``. The zero-extended; value V retrieved is left on the stack with the generic type. *This operation is deprecated as the* ``DW_OP_LLVM_form_aspace_address``; *operation can be used and provides greater expressiveness.*. 7. ``DW_OP_xderef_type`` *Deprecated*. ``DW_OP_xder",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst:93476,perform,performing,93476,interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUDwarfExtensionsForHeterogeneousDebugging.rst,1,['perform'],['performing']
Performance,"`` flags for COFF; targets. Additionally, this metadata is only used as a flag, so the associated; node must be empty. The explicit section should not conflict with any other; sections that the user does not want removed after linking. .. code-block:: text. @object = private constant [1 x i8] c""\00"", section "".foo"" !exclude !0. ...; !0 = !{}. '``unpredictable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. ``unpredictable`` metadata may be attached to any branch or switch; instruction. It can be used to express the unpredictability of control; flow. Similar to the llvm.expect intrinsic, it may be used to alter; optimizations related to compare and branch instructions. The metadata; is treated as a boolean value; if it exists, it signals that the branch; or switch that it is attached to is completely unpredictable. .. _md_dereferenceable:. '``dereferenceable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The existence of the ``!dereferenceable`` metadata on the instruction; tells the optimizer that the value loaded is known to be dereferenceable,; otherwise the behavior is undefined.; The number of bytes known to be dereferenceable is specified by the integer; value in the metadata node. This is analogous to the ''dereferenceable''; attribute on parameters and return values. .. _md_dereferenceable_or_null:. '``dereferenceable_or_null``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The existence of the ``!dereferenceable_or_null`` metadata on the; instruction tells the optimizer that the value loaded is known to be either; dereferenceable or null, otherwise the behavior is undefined.; The number of bytes known to be dereferenceable is specified by the integer; value in the metadata node. This is analogous to the ''dereferenceable_or_null''; attribute on parameters and return values. .. _llvm.loop:. '``llvm.loop``'; ^^^^^^^^^^^^^^^. It is sometimes useful to attach information to loop constructs. Currently,; loop metadata is implemented as metadata attached to the branch ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:292746,optimiz,optimizer,292746,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,"['load', 'optimiz']","['loaded', 'optimizer']"
Performance,"`` if both operands are not a NAN and ``op1``; is less than ``op2``.; - ""``ole``"": yields ``true`` if both operands are not a NAN and ``op1``; is less than or equal to ``op2``.; - ""``one``"": yields ``true`` if both operands are not a NAN and ``op1``; is not equal to ``op2``.; - ""``ord``"": yields ``true`` if both operands are not a NAN.; - ""``ueq``"": yields ``true`` if either operand is a NAN or ``op1`` is; equal to ``op2``.; - ""``ugt``"": yields ``true`` if either operand is a NAN or ``op1`` is; greater than ``op2``.; - ""``uge``"": yields ``true`` if either operand is a NAN or ``op1`` is; greater than or equal to ``op2``.; - ""``ult``"": yields ``true`` if either operand is a NAN or ``op1`` is; less than ``op2``.; - ""``ule``"": yields ``true`` if either operand is a NAN or ``op1`` is; less than or equal to ``op2``.; - ""``une``"": yields ``true`` if either operand is a NAN or ``op1`` is; not equal to ``op2``.; - ""``uno``"": yields ``true`` if either operand is a NAN. The quiet comparison operation performed by; '``llvm.experimental.constrained.fcmp``' will only raise an exception; if either operand is a SNAN. The signaling comparison operation; performed by '``llvm.experimental.constrained.fcmps``' will raise an; exception if either operand is a NAN (QNAN or SNAN). Such an exception; does not preclude a result being produced (e.g. exception might only; set a flag), therefore the distinction between ordered and unordered; comparisons is also relevant for the; '``llvm.experimental.constrained.fcmps``' intrinsic. '``llvm.experimental.constrained.fmuladd``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare <type>; @llvm.experimental.constrained.fmuladd(<type> <op1>, <type> <op2>,; <type> <op3>,; metadata <rounding mode>,; metadata <exception behavior>). Overview:; """""""""""""""""". The '``llvm.experimental.constrained.fmuladd``' intrinsic represents; multiply-add expressions that can be fused if the code generator determines; that (a) the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:886966,perform,performed,886966,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"`` is compiled into native object code. .. code-block:: c++. --- a.h ---; extern int foo1(void);; extern void foo2(void);; extern void foo4(void);. --- a.c ---; #include ""a.h"". static signed int i = 0;. void foo2(void) {; i = -1;; }. static int foo3() {; foo4();; return 10;; }. int foo1(void) {; int data = 0;. if (i < 0); data = foo3();. data = data + 42;; return data;; }. --- main.c ---; #include <stdio.h>; #include ""a.h"". void foo4(void) {; printf(""Hi\n"");; }. int main() {; return foo1();; }. To compile, run:. .. code-block:: console. % clang -flto -c a.c -o a.o # <-- a.o is LLVM bitcode file; % clang -c main.c -o main.o # <-- main.o is native object file; % clang -flto a.o main.o -o main # <-- standard link command with -flto. * In this example, the linker recognizes that ``foo2()`` is an externally; visible symbol defined in LLVM bitcode file. The linker completes its usual; symbol resolution pass and finds that ``foo2()`` is not used; anywhere. This information is used by the LLVM optimizer and it; removes ``foo2()``. * As soon as ``foo2()`` is removed, the optimizer recognizes that condition ``i; < 0`` is always false, which means ``foo3()`` is never used. Hence, the; optimizer also removes ``foo3()``. * And this in turn, enables linker to remove ``foo4()``. This example illustrates the advantage of tight integration with the; linker. Here, the optimizer can not remove ``foo3()`` without the linker's; input. Alternative Approaches; ----------------------. **Compiler driver invokes link time optimizer separately.**; In this model the link time optimizer is not able to take advantage of; information collected during the linker's normal symbol resolution phase.; In the above example, the optimizer can not remove ``foo2()`` without the; linker's input because it is externally visible. This in turn prohibits the; optimizer from removing ``foo3()``. **Use separate tool to collect symbol information from all object files.**; In this model, a new, separate, tool or lib",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst:2761,optimiz,optimizer,2761,interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LinkTimeOptimization.rst,1,['optimiz'],['optimizer']
Performance,"`` is not a positive integer multiple of; ``element_size``, then the behaviour of the intrinsic is undefined. ``element_size`` must be a compile-time constant positive power of two no greater than; target-specific atomic access size limit. The ``dest`` input pointer must have the ``align`` parameter attribute specified. It; must be a power of two no less than the ``element_size``. Caller guarantees that; the destination pointer is aligned to that boundary. Semantics:; """""""""""""""""""". The '``llvm.memset.element.unordered.atomic.*``' intrinsic sets the ``len`` bytes of; memory starting at the destination location to the given ``value``. The memory is; set with a sequence of store operations where each access is guaranteed to be a; multiple of ``element_size`` bytes wide and aligned at an ``element_size`` boundary. The order of the assignment is unspecified. Only one write is issued to the; destination buffer per element. It is well defined to have concurrent reads and; writes to the destination provided those reads and writes are unordered atomic; when specified. This intrinsic does not provide any additional ordering guarantees over those; provided by a set of unordered stores to the destination. Lowering:; """""""""""""""""". In the most general case call to the '``llvm.memset.element.unordered.atomic.*``' is; lowered to a call to the symbol ``__llvm_memset_element_unordered_atomic_*``. Where '*'; is replaced with an actual element size. The optimizer is allowed to inline the memory assignment when it's profitable to do so. Objective-C ARC Runtime Intrinsics; ----------------------------------. LLVM provides intrinsics that lower to Objective-C ARC runtime entry points.; LLVM is aware of the semantics of these functions, and optimizes based on that; knowledge. You can read more about the details of Objective-C ARC `here; <https://clang.llvm.org/docs/AutomaticReferenceCounting.html>`_. '``llvm.objc.autorelease``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:965466,concurren,concurrent,965466,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['concurren'],['concurrent']
Performance,"`` object features it relies on have been; removed. Typing ``cppyy.gbl`` all the time gets old rather quickly, but the dynamic; nature of ``cppyy`` makes something like ``from cppyy.gbl import *``; impossible.; For example, classes can be defined dynamically after that statement and then; they would be missed by the import.; In scripts, it is easy enough to rebind names to achieve a good amount of; reduction in typing (and a modest performance improvement to boot, because of; fewer dictionary lookups), e.g.:. .. code-block:: python. import cppyy; std = cppyy.gbl.std; v = std.vector[int](range(10)). But even such rebinding becomes annoying for (brief) interactive sessions. For CPython only (and not with tools such as IPython or in IDEs that replace; the interactive prompt), there is a fix, using; ``from cppyy.interactive import *``.; This makes lookups in the global dictionary of the current frame also; consider everything under ``cppyy.gbl``.; This feature comes with a performance `penalty` and is not meant for; production code.; Example usage:. .. code-block:: python. >>> from cppyy.interactive import *; >>> v = std.vector[int](range(10)); >>> print(list(v)); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; >>>; >>> cppdef(""struct SomeStruct {};""); True; >>> s = SomeStruct() # <- dynamically made available; >>> s; <cppyy.gbl.SomeStruct object at 0x7fa9b8624320>; >>>. For PyPy, IPython, etc. ``cppyy.gbl`` is simply rebound as ``g`` and; ``cppyy.gbl.std`` is made available as ``std``.; Not as convenient as full lookup, and missing any other namespaces that may be; available, but still saves some typing in may cases. `Odds and ends`; ---------------. * **namespaces**: Are represented as python classes.; Namespaces are more open-ended than classes, so sometimes initial access may; result in updates as data and functions are looked up and constructed; lazily.; Thus the result of ``dir()`` on a namespace shows the classes and functions; available for binding, even if these may not have b",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/misc.rst:5074,perform,performance,5074,bindings/pyroot/cppyy/cppyy/doc/source/misc.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/misc.rst,1,['perform'],['performance']
Performance,"`` or ``addr2line``). It; doesn't contain any other data (e.g. description of local variables or; function parameters). .. option:: -fstandalone-debug. Clang supports a number of optimizations to reduce the size of debug; information in the binary. They work based on the assumption that; the debug type information can be spread out over multiple; compilation units. Specifically, the optimizations are:. - will not emit type definitions for types that are not needed by a; module and could be replaced with a forward declaration.; - will only emit type info for a dynamic C++ class in the module that; contains the vtable for the class.; - will only emit type info for a C++ class (non-trivial, non-aggregate); in the modules that contain a definition for one of its constructors.; - will only emit type definitions for types that are the subject of explicit; template instantiation declarations in the presence of an explicit; instantiation definition for the type. The **-fstandalone-debug** option turns off these optimizations.; This is useful when working with 3rd-party libraries that don't come; with debug information. Note that Clang will never emit type; information for types that are not referenced at all by the program. .. option:: -fno-standalone-debug. On Darwin **-fstandalone-debug** is enabled by default. The; **-fno-standalone-debug** option can be used to get to turn on the; vtable-based optimization described above. .. option:: -g. Generate complete debug info. .. option:: -feliminate-unused-debug-types. By default, Clang does not emit type information for types that are defined; but not used in a program. To retain the debug info for these unused types,; the negation **-fno-eliminate-unused-debug-types** can be used. Controlling Macro Debug Info Generation; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Debug info for C preprocessor macros increases the size of debug information in; the binary. Macro debug info generated by Clang can be controlled by the flags; listed ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:125817,optimiz,optimizations,125817,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimizations']
Performance,"`` will be passed only to the stage2 build.; This gives the opportunity to use Clang specific build flags.; For example, the following CMake call will enabled '-fno-addrsig' only during; the stage2 build for C and C++. .. code-block:: console. $ cmake [..] -DBOOTSTRAP_CMAKE_CXX_FLAGS='-fno-addrsig' -DBOOTSTRAP_CMAKE_C_FLAGS='-fno-addrsig' [..]. The clang build system refers to builds as stages. A stage1 build is a standard; build using the compiler installed on the host, and a stage2 build is built; using the stage1 compiler. This nomenclature holds up to more stages too. In; general a stage*n* build is built using the output from stage*n-1*. Apple Clang Builds (A More Complex Bootstrap); =============================================. Apple's Clang builds are a slightly more complicated example of the simple; bootstrapping scenario. Apple Clang is built using a 2-stage build. The stage1 compiler is a host-only compiler with some options set. The stage1; compiler is a balance of optimization vs build time because it is a throwaway.; The stage2 compiler is the fully optimized compiler intended to ship to users. Setting up these compilers requires a lot of options. To simplify the; configuration the Apple Clang build settings are contained in CMake Cache files.; You can build an Apple Clang compiler using the following commands:. .. code-block:: console. $ cmake -G Ninja -C <path to source>/clang/cmake/caches/Apple-stage1.cmake <path to source>/llvm; $ ninja stage2-distribution. This CMake invocation configures the stage1 host compiler, and sets; CLANG_BOOTSTRAP_CMAKE_ARGS to pass the Apple-stage2.cmake cache script to the; stage2 configuration step. When you build the stage2-distribution target it builds the minimal stage1; compiler and required tools, then configures and builds the stage2 compiler; based on the settings in Apple-stage2.cmake. This pattern of using cache scripts to set complex settings, and specifically to; make later stage builds include cache scripts",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:3641,optimiz,optimization,3641,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['optimiz'],['optimization']
Performance,"``, and ``%base``, ``%n`` are the two arguments to; ``llvm.get.active.lane.mask.*``, ``%icmp`` is an integer compare and ``ult``; the unsigned less-than comparison operator. Overflow cannot occur in; ``(%base + i)`` and its comparison against ``%n`` as it is performed in integer; numbers and not in machine numbers. If ``%n`` is ``0``, then the result is a; poison value. The above is equivalent to:. ::. %m = @llvm.get.active.lane.mask(%base, %n). This can, for example, be emitted by the loop vectorizer in which case; ``%base`` is the first element of the vector induction variable (VIV) and; ``%n`` is the loop tripcount. Thus, these intrinsics perform an element-wise; less than comparison of VIV with the loop tripcount, producing a mask of; true/false values representing active/inactive vector lanes, except if the VIV; overflows in which case they return false in the lanes where the VIV overflows.; The arguments are scalar types to accommodate scalable vector types, for which; it is unknown what the type of the step vector needs to be that enumerate its; lanes without overflow. This mask ``%m`` can e.g. be used in masked load/store instructions. These; intrinsics provide a hint to the backend. I.e., for a vector loop, the; back-edge taken count of the original scalar loop is explicit as the second; argument. Examples:; """""""""""""""""". .. code-block:: llvm. %active.lane.mask = call <4 x i1> @llvm.get.active.lane.mask.v4i1.i64(i64 %elem0, i64 429); %wide.masked.load = call <4 x i32> @llvm.masked.load.v4i32.p0v4i32(<4 x i32>* %3, i32 4, <4 x i1> %active.lane.mask, <4 x i32> poison). .. _int_experimental_vp_splice:. '``llvm.experimental.vp.splice``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <2 x double> @llvm.experimental.vp.splice.v2f64(<2 x double> %vec1, <2 x double> %vec2, i32 %imm, <2 x i1> %mask, i32 %evl1, i32 %evl2); declare <vscale x 4 x i32> @llvm.experimental.vp.splice.nxv4i32(<vscal",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:778613,scalab,scalable,778613,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"``-ffinite-math-only`` defines the; ``__FINITE_MATH_ONLY__`` preprocessor macro.; ``-ffinite-math-only`` implies:. * ``-fno-honor-infinities``; * ``-fno-honor-nans``. ``-ffno-inite-math-only`` implies:. * ``-fhonor-infinities``; * ``-fhonor-nans``. Defaults to ``-fno-finite-math-only``. .. option:: -f[no-]rounding-math. Force floating-point operations to honor the dynamically-set rounding mode by default. The result of a floating-point operation often cannot be exactly represented in the result type and therefore must be rounded. IEEE 754 describes different rounding modes that control how to perform this rounding, not all of which are supported by all implementations. C provides interfaces (``fesetround`` and ``fesetenv``) for dynamically controlling the rounding mode, and while it also recommends certain conventions for changing the rounding mode, these conventions are not typically enforced in the ABI. Since the rounding mode changes the numerical result of operations, the compiler must understand something about it in order to optimize floating point operations. Note that floating-point operations performed as part of constant initialization are formally performed prior to the start of the program and are therefore not subject to the current rounding mode. This includes the initialization of global variables and local ``static`` variables. Floating-point operations in these contexts will be rounded using ``FE_TONEAREST``. - The option ``-fno-rounding-math`` allows the compiler to assume that the rounding mode is set to ``FE_TONEAREST``. This is the default.; - The option ``-frounding-math`` forces the compiler to honor the dynamically-set rounding mode. This prevents optimizations which might affect results if the rounding mode changes or is different from the default; for example, it prevents floating-point operations from being reordered across most calls and prevents constant-folding when the result is not exactly representable. .. option:: -ffp-model=<value>",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:61151,optimiz,optimize,61151,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimize']
Performance,"``-fprofile-use`` compilation. .. code-block:: console. $ clang++ -O2 -fprofile-use=cs_code.profdata. The above command will read both profiles to the compiler at the identical; point of instrumentations. .. option:: -fprofile-use[=<pathname>]. Without any other arguments, ``-fprofile-use`` behaves identically to; ``-fprofile-instr-use``. Otherwise, if ``pathname`` is the full path to a; profile file, it reads from that file. If ``pathname`` is a directory name,; it reads from ``pathname/default.profdata``. .. option:: -fprofile-update[=<method>]. Unless ``-fsanitize=thread`` is specified, the default is ``single``, which; uses non-atomic increments. The counters can be inaccurate under thread; contention. ``atomic`` uses atomic increments which is accurate but has; overhead. ``prefer-atomic`` will be transformed to ``atomic`` when supported; by the target, or ``single`` otherwise. Fine Tuning Profile Collection; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The PGO infrastructure provides user program knobs to fine tune profile; collection. Specifically, the PGO runtime provides the following functions; that can be used to control the regions in the program where profiles should; be collected. * ``void __llvm_profile_set_filename(const char *Name)``: changes the name of; the profile file to ``Name``.; * ``void __llvm_profile_reset_counters(void)``: resets all counters to zero.; * ``int __llvm_profile_dump(void)``: write the profile data to disk.; * ``int __llvm_orderfile_dump(void)``: write the order file to disk. For example, the following pattern can be used to skip profiling program; initialization, profile two specific hot regions, and skip profiling program; cleanup:. .. code-block:: c. int main() {; initialize();. // Reset all profile counters to 0 to omit profile collected during; // initialize()'s execution.; __llvm_profile_reset_counters();; ... hot region 1; // Dump the profile for hot region 1.; __llvm_profile_set_filename(""region1.profraw"");; __llvm_profile_dump();. /",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:111110,tune,tune,111110,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['tune'],['tune']
Performance,"``. **Syntax**:. .. code-block:: c++. __builtin_rotateright32(x, y). **Examples**:. .. code-block:: c++. uint8_t rot_x = __builtin_rotateright8(x, y);; uint16_t rot_x = __builtin_rotateright16(x, y);; uint32_t rot_x = __builtin_rotateright32(x, y);; uint64_t rot_x = __builtin_rotateright64(x, y);. **Description**:. The '``__builtin_rotateright``' family of builtins is used to rotate; the bits in the first argument by the amount in the second argument.; For example, ``0b10000110`` rotated right by 3 becomes ``0b11010000``.; The shift value is treated as an unsigned amount modulo the size of; the arguments. Both arguments and the result have the bitwidth specified; by the name of the builtin. These builtins can be used within constant; expressions. ``__builtin_unreachable``; -------------------------. ``__builtin_unreachable`` is used to indicate that a specific point in the; program cannot be reached, even if the compiler might otherwise think it can.; This is useful to improve optimization and eliminates certain warnings. For; example, without the ``__builtin_unreachable`` in the example below, the; compiler assumes that the inline asm can fall through and prints a ""function; declared '``noreturn``' should not return"" warning. **Syntax**:. .. code-block:: c++. __builtin_unreachable(). **Example of use**:. .. code-block:: c++. void myabort(void) __attribute__((noreturn));; void myabort(void) {; asm(""int3"");; __builtin_unreachable();; }. **Description**:. The ``__builtin_unreachable()`` builtin has completely undefined behavior.; Since it has undefined behavior, it is a statement that it is never reached and; the optimizer can take advantage of this to produce better code. This builtin; takes no arguments and produces a void result. Query for this feature with ``__has_builtin(__builtin_unreachable)``. ``__builtin_unpredictable``; ---------------------------. ``__builtin_unpredictable`` is used to indicate that a branch condition is; unpredictable by hardware mechanisms",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:112409,optimiz,optimization,112409,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimization']
Performance,"``. However,; ``idx2`` computes the address of *the next* structure after ``@MyVar``, that is; ``MyVar+40``, because it indexes past the ten 4-byte integers in ``MyVar``.; Obviously, in such a situation, the pointers don't alias. Why do GEP x,1,0,0 and GEP x,1 alias?; -------------------------------------. Quick Answer: They compute the same address location. These two GEP instructions will compute the same address because indexing; through the 0th element does not change the address. Consider this example:. .. code-block:: llvm. @MyVar = global { [10 x i32] }; %idx1 = getelementptr { [10 x i32] }, ptr @MyVar, i64 1, i32 0, i64 0; %idx2 = getelementptr { [10 x i32] }, ptr @MyVar, i64 1. In this example, the value of ``%idx1`` is ``MyVar+40``, and the value of; ``%idx2`` is also ``MyVar+40``. Can GEP index into vector elements?; -----------------------------------. This hasn't always been forcefully disallowed, though it's not recommended. It; leads to awkward special cases in the optimizers, and fundamental inconsistency; in the IR. In the future, it will probably be outright disallowed. What effect do address spaces have on GEPs?; -------------------------------------------. None, except that the address space qualifier on the second operand pointer type; always matches the address space qualifier on the result type. How is GEP different from ``ptrtoint``, arithmetic, and ``inttoptr``?; ---------------------------------------------------------------------. It's very similar; there are only subtle differences. With ptrtoint, you have to pick an integer type. One approach is to pick i64;; this is safe on everything LLVM supports (LLVM internally assumes pointers are; never wider than 64 bits in many places), and the optimizer will actually narrow; the i64 arithmetic down to the actual pointer size on targets which don't; support 64-bit arithmetic in most cases. However, there are some cases where it; doesn't do this. With GEP you can avoid this problem. Also, GEP car",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst:9292,optimiz,optimizers,9292,interpreter/llvm-project/llvm/docs/GetElementPtr.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GetElementPtr.rst,1,['optimiz'],['optimizers']
Performance,"``. Subdirectory; names known to LLVM include:. * spec95. * speccpu2000. * speccpu2006. * povray31. Others are added from time to time, and can be determined from; ``configure``. Running Different Tests; =======================. In addition to the regular ""whole program"" tests, the ``test-suite``; module also provides a mechanism for compiling the programs in different; ways. If the variable TEST is defined on the ``gmake`` command line, the; test system will include a Makefile named; ``TEST.<value of TEST variable>.Makefile``. This Makefile can modify; build rules to yield different results. For example, the LLVM nightly tester uses ``TEST.nightly.Makefile`` to; create the nightly test reports. To run the nightly tests, run; ``gmake TEST=nightly``. There are several TEST Makefiles available in the tree. Some of them are; designed for internal LLVM research and will not work outside of the; LLVM research group. They may still be valuable, however, as a guide to; writing your own TEST Makefile for any optimization or analysis passes; that you develop with LLVM. Generating Test Output; ======================. There are a number of ways to run the tests and generate output. The; most simple one is simply running ``gmake`` with no arguments. This will; compile and run all programs in the tree using a number of different; methods and compare results. Any failures are reported in the output,; but are likely drowned in the other output. Passes are not reported; explicitly. Somewhat better is running ``gmake TEST=sometest test``, which runs the; specified test and usually adds per-program summaries to the output; (depending on which sometest you use). For example, the ``nightly`` test; explicitly outputs TEST-PASS or TEST-FAIL for every test after each; program. Though these lines are still drowned in the output, it's easy; to grep the output logs in the Output directories. Even better are the ``report`` and ``report.format`` targets (where; ``format`` is one of ``html``, `",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteMakefileGuide.rst:3587,optimiz,optimization,3587,interpreter/llvm-project/llvm/docs/TestSuiteMakefileGuide.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestSuiteMakefileGuide.rst,1,['optimiz'],['optimization']
Performance,"``; /opt/build/root_builds/rootcling.cmake/include/TSQLFile.h:225:19: error: declaration of 'GetStreamerInfoList' overrides a 'final' function; virtual TList *GetStreamerInfoList();; ^; /opt/build/root_builds/rootcling.cmake/include/TFile.h:231:24: note: overridden virtual function is here; virtual TList *GetStreamerInfoList() final; // Note: to override behavior, please override GetStreamerInfoListImpl; ^; ```. Instead you need to override the protected method:. ```; InfoListRet GetStreamerInfoListImpl(bool lookupSICache);; ```. which can be implemented as. ```; InfoListRet DerivedClass::GetStreamerInfoListImpl(bool /*lookupSICache*/) {; ROOT::Internal::RConcurrentHashColl::HashValue hash;; TList *infolist = nullptr;; //; // Body of the former Derived::GetStreamerInfoList with the; // return statement replaced with something like:. // The second element indicates success or failure of the load.; // (i.e. {nullptr, 0, hash} indicates the list has already been processed; // {nullptr, 1, hash} indicates the list failed to be loaded; return {infolist, 0, hash};; }; ```. See `TFile::GetStreamerInfoListImpl` implementation for an example on how to implement the caching. * ZLIB (with compression level 1) is now the default compression algorithm for new ROOT files (LZ4 was default compression algorithm in 6.14). Because of reported ""corner cases"" for LZ4, we are working on the fix to be landed in a next release and return back LZ4 as a default compression algorithm. * Introducing a possibility for ROOT to use generic compression algorithm/level/settings, by introducing new generic class RCompressionSetting together with new structs ELevel (compression level), EDefaults (default compression settings) and EAlgorithm (compression algorithm). These changes are the first step in generalization of setup of ROOT compression algorithm. It also provides correctness of resolution of compression level and compression algorithm from defined ROOT compression settings:. ```; Attaching fi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v616/index.md:5301,load,loaded,5301,README/ReleaseNotes/v616/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v616/index.md,1,['load'],['loaded']
Performance,"``ADD %EAX, %EBX``, in X86 is actually equivalent to ``%EAX = %EAX +; %EBX``. In order to produce correct code, LLVM must convert three address instructions; that represent two address instructions into true two address instructions. LLVM; provides the pass ``TwoAddressInstructionPass`` for this specific purpose. It; must be run before register allocation takes place. After its execution, the; resulting code may no longer be in SSA form. This happens, for instance, in; situations where an instruction such as ``%a = ADD %b %c`` is converted to two; instructions such as:. ::. %a = MOVE %b; %a = ADD %a %c. Notice that, internally, the second instruction is represented as ``ADD; %a[def/use] %c``. I.e., the register operand ``%a`` is both used and defined by; the instruction. The SSA deconstruction phase; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. An important transformation that happens during register allocation is called; the *SSA Deconstruction Phase*. The SSA form simplifies many analyses that are; performed on the control flow graph of programs. However, traditional; instruction sets do not implement PHI instructions. Thus, in order to generate; executable code, compilers must replace PHI instructions with other instructions; that preserve their semantics. There are many ways in which PHI instructions can safely be removed from the; target code. The most traditional PHI deconstruction algorithm replaces PHI; instructions with copy instructions. That is the strategy adopted by LLVM. The; SSA deconstruction algorithm is implemented in; ``lib/CodeGen/PHIElimination.cpp``. In order to invoke this pass, the identifier; ``PHIEliminationID`` must be marked as required in the code of the register; allocator. Instruction folding; ^^^^^^^^^^^^^^^^^^^. *Instruction folding* is an optimization performed during register allocation; that removes unnecessary copy instructions. For instance, a sequence of; instructions such as:. ::. %EBX = LOAD %mem_address; %EAX = COPY %EBX. can be safely subs",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:67855,perform,performed,67855,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['perform'],['performed']
Performance,"``LLVM-MCA-END`` directive. In the absence of overlapping regions,; an anonymous ``LLVM-MCA-END`` directive always ends the currently active user; defined region. Example of nesting regions:. .. code-block:: none. # LLVM-MCA-BEGIN foo; add %eax, %edx; # LLVM-MCA-BEGIN bar; sub %eax, %edx; # LLVM-MCA-END bar; # LLVM-MCA-END foo. Example of overlapping regions:. .. code-block:: none. # LLVM-MCA-BEGIN foo; add %eax, %edx; # LLVM-MCA-BEGIN bar; sub %eax, %edx; # LLVM-MCA-END foo; add %eax, %edx; # LLVM-MCA-END bar. Note that multiple anonymous regions cannot overlap. Also, overlapping regions; cannot have the same name. There is no support for marking regions from high-level source code, like C or; C++. As a workaround, inline assembly directives may be used:. .. code-block:: c++. int foo(int a, int b) {; __asm volatile(""# LLVM-MCA-BEGIN foo"":::""memory"");; a += 42;; __asm volatile(""# LLVM-MCA-END"":::""memory"");; a *= b;; return a;; }. However, this interferes with optimizations like loop vectorization and may have; an impact on the code generated. This is because the ``__asm`` statements are; seen as real code having important side effects, which limits how the code; around them can be transformed. If users want to make use of inline assembly; to emit markers, then the recommendation is to always verify that the output; assembly is equivalent to the assembly generated in the absence of markers.; The `Clang options to emit optimization reports <https://clang.llvm.org/docs/UsersManual.html#options-to-emit-optimization-reports>`_; can also help in detecting missed optimizations. INSTRUMENT REGIONS; ------------------. An InstrumentRegion describes a region of assembly code guarded by; special LLVM-MCA comment directives. .. code-block:: none. # LLVM-MCA-<INSTRUMENT_TYPE> <data>; ... ## asm. where `INSTRUMENT_TYPE` is a type defined by the target and expects; to use `data`. A comment starting with substring `LLVM-MCA-<INSTRUMENT_TYPE>`; brings data into scope for llvm-mca to ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:10123,optimiz,optimizations,10123,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['optimiz'],['optimizations']
Performance,"``MOVNT`` instruction on; x86. The optional ``!invariant.group`` metadata must reference a; single metadata name ``<empty_node>``. See ``invariant.group`` metadata. Semantics:; """""""""""""""""""". The contents of memory are updated to contain ``<value>`` at the; location specified by the ``<pointer>`` operand. If ``<value>`` is; of scalar type then the number of bytes written does not exceed the; minimum number of bytes needed to hold all bits of the type. For; example, storing an ``i24`` writes at most three bytes. When writing a; value of a type like ``i20`` with a size that is not an integral number; of bytes, it is unspecified what happens to the extra bits that do not; belong to the type, but they will typically be overwritten.; If ``<value>`` is of aggregate type, padding is filled with; :ref:`undef <undefvalues>`.; If ``<pointer>`` is not a well-defined value, the behavior is undefined. Example:; """""""""""""""". .. code-block:: llvm. %ptr = alloca i32 ; yields ptr; store i32 3, ptr %ptr ; yields void; %val = load i32, ptr %ptr ; yields i32:val = i32 3. .. _i_fence:. '``fence``' Instruction; ^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. fence [syncscope(""<target-scope>"")] <ordering> ; yields void. Overview:; """""""""""""""""". The '``fence``' instruction is used to introduce happens-before edges; between operations. Arguments:; """""""""""""""""""". '``fence``' instructions take an :ref:`ordering <ordering>` argument which; defines what *synchronizes-with* edges they add. They can only be given; ``acquire``, ``release``, ``acq_rel``, and ``seq_cst`` orderings. Semantics:; """""""""""""""""""". A fence A which has (at least) ``release`` ordering semantics; *synchronizes with* a fence B with (at least) ``acquire`` ordering; semantics if and only if there exist atomic operations X and Y, both; operating on some atomic object M, such that A is sequenced before X, X; modifies M (either directly or through some side effect of a sequence; headed by X), Y is sequenced before B, and Y observes M. This provides",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:423368,load,load,423368,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"```. To correctly cleanup JSROOT drawings from HTML element, one should call:. ```javascript; cleanup(""drawing"");; ```. ### File API. JSROOT defines the TFile class, which can be used to access binary ROOT files.; One should always remember that all I/O operations are asynchronous in JSROOT.; Therefore promises are used to retrieve results when the I/O operation is completed.; For example, reading an object from a file and displaying it will look like:. ```javascript; import { openFile, draw } from 'https://root.cern/js/latest/modules/main.mjs';; let filename = ""https://root.cern/js/files/hsimple.root"";; let file = await openFile(filename);; let obj = await file.readObject(""hpxpy;1"");; await draw(""drawing"", obj, ""colz"");; console.log('drawing completed');; ```. Here is [running example](https://root.cern/js/latest/api.htm#custom_html_read_root_file) and [source code](https://github.com/root-project/jsroot/blob/master/demo/read_file.htm). ### TTree API. Simple TTree::Draw operation can be performed with following code:. ```javascript; import { openFile } from 'https://root.cern/js/latest/modules/io.mjs';; import { draw } from 'https://root.cern/js/latest/modules/draw.mjs';; let file = await openFile(""https://root.cern/js/files/hsimple.root"");; let tree = await file.readObject(""ntuple;1"");; draw(""drawing"", tree, ""px:py::pz>5"");; ```. To get access to selected branches, one should use `TSelector` class:. ```javascript; import { openFile } from 'https://root.cern/js/latest/modules/io.mjs';; import { draw } from 'https://root.cern/js/latest/modules/draw.mjs';; import { TSelector, treeProcess } from 'https://root.cern/js/latest/modules/tree.mjs';. let file = await openFile(""https://root.cern/js/files/hsimple.root"");; let tree = await file.readObject(""ntuple;1"");; let selector = new TSelector();. selector.AddBranch(""px"");; selector.AddBranch(""py"");. let cnt = 0, sumpx = 0, sumpy = 0;. selector.Begin = function() {; // function called before reading of TTree starts; }. sele",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md:40587,perform,performed,40587,documentation/JSROOT/JSROOT.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/JSROOT/JSROOT.md,1,['perform'],['performed']
Performance,"```. You can append to the include path by typing:. ``` {.cpp}; root[] .include $HOME/mypackage/include; ```. In a script you can append to the include path:. ``` {.cpp}; gSystem->AddIncludePath("" -I$HOME/mypackage/include ""); ```. You can also overwrite the existing include path:. ``` {.cpp}; gSystem->SetIncludePath("" -I$HOME/mypackage/include ""); ```. The `$ROOTSYS/include` directory is automatically appended to the; include path, so you do not have to worry about including it. To add; library that should be used during linking of the shared library use; something like:. ``` {.cpp}; gSystem->AddLinkedLibs(""-L/my/path -lanylib"");; ```. This is especially useful for static libraries. For shared ones you; can also simply load them before trying to compile the script:. ``` {.cpp}; gSystem->Load(""mydir/mylib"");; ```. ACLiC uses the directive `fMakeSharedLibs` to create the shared; library. If loading the shared library fails, it tries to output a; list of missing symbols by creating an executable (on some platforms; like OSF, this does not HAVE to be an executable) containing the; script. It uses the directive `fMakeExe` to do so. For both; directives, before passing them to `TSystem::Exec()`, it expands the; variables `$SourceFiles`, `$SharedLib`, `$LibName`, `$IncludePath`,; `$LinkedLibs`, `$ExeName `and` $ObjectFiles`. See `SetMakeSharedLib()`; for more information on those variables. When the file being passed to; ACLiC is on a read only file system, ACLiC warns the user and creates; the library in a temporary directory:. ``` {.cpp}; root[] .L readonly/t.C++; Warning in <ACLiC>: /scratch/aclic/subs/./readonly is not writable!; Warning in <ACLiC>: Output will be written to /tmp; Info in <TUnixSystem::ACLiC>: creating shared library; /tmp//scratch/aclic/subs/./readonly/t_C.so; ```. To select the temporary directory ACLiC looks at `$TEMP`, `$TEMP_DIR`,; `$TEMPDIR`, `$TMP`, `$TMPDIR`, `$TMP_DIR `or uses `/tmp (`or `C:/)`.; Also, a new interface `TSystem::Get/SetBuildDir",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md:17384,load,loading,17384,documentation/users-guide/Cling.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Cling.md,1,['load'],['loading']
Performance,"``available_externally`` global is equivalent to; an external declaration. They exist to allow inlining and other; optimizations to take place given knowledge of the definition of the; global, which is known to be somewhere outside the module. Globals; with ``available_externally`` linkage are allowed to be discarded at; will, and allow inlining and other optimizations. This linkage type is; only allowed on definitions, not declarations.; ``linkonce``; Globals with ""``linkonce``"" linkage are merged with other globals of; the same name when linkage occurs. This can be used to implement; some forms of inline functions, templates, or other code which must; be generated in each translation unit that uses it, but where the; body may be overridden with a more definitive definition later.; Unreferenced ``linkonce`` globals are allowed to be discarded. Note; that ``linkonce`` linkage does not actually allow the optimizer to; inline the body of this function into callers because it doesn't; know if this definition of the function is the definitive definition; within the program or whether it will be overridden by a stronger; definition. To enable inlining and other optimizations, use; ""``linkonce_odr``"" linkage.; ``weak``; ""``weak``"" linkage has the same merging semantics as ``linkonce``; linkage, except that unreferenced globals with ``weak`` linkage may; not be discarded. This is used for globals that are declared ""weak""; in C source code.; ``common``; ""``common``"" linkage is most similar to ""``weak``"" linkage, but they; are used for tentative definitions in C, such as ""``int X;``"" at; global scope. Symbols with ""``common``"" linkage are merged in the; same way as ``weak symbols``, and they may not be deleted if; unreferenced. ``common`` symbols may not have an explicit section,; must have a zero initializer, and may not be marked; ':ref:`constant <globalvars>`'. Functions and aliases may not have; common linkage. .. _linkage_appending:. ``appending``; ""``appending``"" linkag",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:9061,optimiz,optimizer,9061,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizer']
Performance,"``cache type``; specifies whether the prefetch is performed on the data (1) or; instruction (0) cache. The ``rw``, ``locality`` and ``cache type``; arguments must be constant integers. Semantics:; """""""""""""""""""". This intrinsic does not modify the behavior of the program. In; particular, prefetches cannot trap and do not produce a value. On; targets that support this intrinsic, the prefetch can provide hints to; the processor cache for better performance. '``llvm.pcmarker``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare void @llvm.pcmarker(i32 <id>). Overview:; """""""""""""""""". The '``llvm.pcmarker``' intrinsic is a method to export a Program; Counter (PC) in a region of code to simulators and other tools. The; method is target specific, but it is expected that the marker will use; exported symbols to transmit the PC of the marker. The marker makes no; guarantees that it will remain with any specific instruction after; optimizations. It is possible that the presence of a marker will inhibit; optimizations. The intended use is to be inserted after optimizations to; allow correlations of simulation runs. Arguments:; """""""""""""""""""". ``id`` is a numerical id identifying the marker. Semantics:; """""""""""""""""""". This intrinsic does not modify the behavior of the program. Backends; that do not support this intrinsic may ignore it. '``llvm.readcyclecounter``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i64 @llvm.readcyclecounter(). Overview:; """""""""""""""""". The '``llvm.readcyclecounter``' intrinsic provides access to the cycle; counter register (or similar low latency, high accuracy clocks) on those; targets that support it. On X86, it should map to RDTSC. On Alpha, it; should map to RPCC. As the backing counters overflow quickly (on the; order of 9 seconds on alpha), this should only be used for small; timings. Semantics:; """""""""""""""""""". When directly supported, reading the cycle counter should not modify any; memory. Implementations are",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:524609,optimiz,optimizations,524609,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizations']
Performance,"``defvar`` or ``defset`` statements. * The iteration variable of a ``foreach``, such as the use of ``i`` in::. foreach i = 0...5 in; def Foo#i;. .. productionlist::; SimpleValue8: `ClassID` ""<"" `ArgValueList` "">"". This form creates a new anonymous record definition (as would be created by an; unnamed ``def`` inheriting from the given class with the given template; arguments; see `def`_) and the value is that record. A field of the record can be; obtained using a suffix; see `Suffixed Values`_. Invoking a class in this manner can provide a simple subroutine facility.; See `Using Classes as Subroutines`_ for more information. .. productionlist::; SimpleValue9: `BangOperator` [""<"" `Type` "">""] ""("" `ValueListNE` "")""; :| `CondOperator` ""("" `CondClause` ("","" `CondClause`)* "")""; CondClause: `Value` "":"" `Value`. The bang operators provide functions that are not available with the other; simple values. Except in the case of ``!cond``, a bang operator takes a list; of arguments enclosed in parentheses and performs some function on those; arguments, producing a value for that bang operator. The ``!cond`` operator; takes a list of pairs of arguments separated by colons. See `Appendix A:; Bang Operators`_ for a description of each bang operator. Suffixed values; ---------------. The :token:`SimpleValue` values described above can be specified with; certain suffixes. The purpose of a suffix is to obtain a subvalue of the; primary value. Here are the possible suffixes for some primary *value*. *value*\ ``{17}``; The final value is bit 17 of the integer *value* (note the braces). *value*\ ``{8...15}``; The final value is bits 8--15 of the integer *value*. The order of the; bits can be reversed by specifying ``{15...8}``. *value*\ ``[i]``; The final value is element `i` of the list *value* (note the brackets).; In other words, the brackets act as a subscripting operator on the list.; This is the case only when a single element is specified. *value*\ ``[i,]``; The final value is a lis",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:18407,perform,performs,18407,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,1,['perform'],['performs']
Performance,"``enable_sgpr_*`` bit fields, in which case only the first 16 are; actually initialized. These are then immediately followed by the System SGPRs; that are set up by ADC/SPI and can have different values for each wavefront of; the grid dispatch. SGPR register initial state is defined in; :ref:`amdgpu-amdhsa-sgpr-register-set-up-order-table`. .. table:: SGPR Register Set Up Order; :name: amdgpu-amdhsa-sgpr-register-set-up-order-table. ========== ========================== ====== ==============================; SGPR Order Name Number Description; (kernel descriptor enable of; field) SGPRs; ========== ========================== ====== ==============================; First Private Segment Buffer 4 See; (enable_sgpr_private :ref:`amdgpu-amdhsa-kernel-prolog-private-segment-buffer`.; _segment_buffer); then Dispatch Ptr 2 64-bit address of AQL dispatch; (enable_sgpr_dispatch_ptr) packet for kernel dispatch; actually executing.; then Queue Ptr 2 64-bit address of amd_queue_t; (enable_sgpr_queue_ptr) object for AQL queue on which; the dispatch packet was; queued.; then Kernarg Segment Ptr 2 64-bit address of Kernarg; (enable_sgpr_kernarg segment. This is directly; _segment_ptr) copied from the; kernarg_address in the kernel; dispatch packet. Having CP load it once avoids; loading it at the beginning of; every wavefront.; then Dispatch Id 2 64-bit Dispatch ID of the; (enable_sgpr_dispatch_id) dispatch packet being; executed.; then Flat Scratch Init 2 See; (enable_sgpr_flat_scratch :ref:`amdgpu-amdhsa-kernel-prolog-flat-scratch`.; _init); then Preloaded Kernargs N/A See; (kernarg_preload_spec :ref:`amdgpu-amdhsa-kernarg-preload`.; _length); then Private Segment Size 1 The 32-bit byte size of a; (enable_sgpr_private single work-item's memory; _segment_size) allocation. This is the; value from the kernel; dispatch packet Private; Segment Byte Size rounded up; by CP to a multiple of; DWORD. Having CP load it once avoids; loading it at the beginning of; every wavefront. This is not ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:184614,queue,queue,184614,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['queue'],"['queue', 'queued']"
Performance,"``llvm.usub.with.overflow``; on any integer bit width or vectors of integers. ::. declare {i16, i1} @llvm.usub.with.overflow.i16(i16 %a, i16 %b); declare {i32, i1} @llvm.usub.with.overflow.i32(i32 %a, i32 %b); declare {i64, i1} @llvm.usub.with.overflow.i64(i64 %a, i64 %b); declare {<4 x i32>, <4 x i1>} @llvm.usub.with.overflow.v4i32(<4 x i32> %a, <4 x i32> %b). Overview:; """""""""""""""""". The '``llvm.usub.with.overflow``' family of intrinsic functions perform; an unsigned subtraction of the two arguments, and indicate whether an; overflow occurred during the unsigned subtraction. Arguments:; """""""""""""""""""". The arguments (%a and %b) and the first element of the result structure; may be of integer types of any bit width, but they must have the same; bit width. The second element of the result structure must be of type; ``i1``. ``%a`` and ``%b`` are the two values that will undergo unsigned; subtraction. Semantics:; """""""""""""""""""". The '``llvm.usub.with.overflow``' family of intrinsic functions perform; an unsigned subtraction of the two arguments. They return a structure ---; the first element of which is the subtraction, and the second element of; which is a bit specifying if the unsigned subtraction resulted in an; overflow. Examples:; """""""""""""""""". .. code-block:: llvm. %res = call {i32, i1} @llvm.usub.with.overflow.i32(i32 %a, i32 %b); %sum = extractvalue {i32, i1} %res, 0; %obit = extractvalue {i32, i1} %res, 1; br i1 %obit, label %overflow, label %normal. '``llvm.smul.with.overflow.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.smul.with.overflow``; on any integer bit width or vectors of integers. ::. declare {i16, i1} @llvm.smul.with.overflow.i16(i16 %a, i16 %b); declare {i32, i1} @llvm.smul.with.overflow.i32(i32 %a, i32 %b); declare {i64, i1} @llvm.smul.with.overflow.i64(i64 %a, i64 %b); declare {<4 x i32>, <4 x i1>} @llvm.smul.with.overflow.v4i32(<4 x i32> %a, <4 x i32> %b). Overview:; """"""""""""",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:606831,perform,perform,606831,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['perform']
Performance,"``minimum_instruction_length`` (ubyte); For GFX9-GFX11 this is 4. ``maximum_operations_per_instruction`` (ubyte); For GFX9-GFX11 this is 1. Source text for online-compiled programs (for example, those compiled by the; OpenCL language runtime) may be embedded into the DWARF Version 5 line table.; See DWARF Version 5 section 6.2.4.1 which is updated by *DWARF Extensions For; Heterogeneous Debugging* section :ref:`DW_LNCT_LLVM_source; <amdgpu-dwarf-line-number-information-dw-lnct-llvm-source>`. The Clang option used to control source embedding in AMDGPU is defined in; :ref:`amdgpu-clang-debug-options-table`. .. table:: AMDGPU Clang Debug Options; :name: amdgpu-clang-debug-options-table. ==================== ==================================================; Debug Flag Description; ==================== ==================================================; -g[no-]embed-source Enable/disable embedding source text in DWARF; debug sections. Useful for environments where; source cannot be written to disk, such as; when performing online compilation.; ==================== ==================================================. For example:. ``-gembed-source``; Enable the embedded source. ``-gno-embed-source``; Disable the embedded source. 32-Bit and 64-Bit DWARF Formats; -------------------------------. See DWARF Version 5 section 7.4 and; :ref:`amdgpu-dwarf-32-bit-and-64-bit-dwarf-formats`. For AMDGPU:. * For the ``amdgcn`` target architecture only the 64-bit process address space; is supported. * The producer can generate either 32-bit or 64-bit DWARF format. LLVM generates; the 32-bit DWARF format. Unit Headers; ------------. For AMDGPU the following values apply for each of the unit headers described in; DWARF Version 5 sections 7.5.1.1, 7.5.1.2, and 7.5.1.3:. ``address_size`` (ubyte); Matches the address size for the ``Global`` address space defined in; :ref:`amdgpu-dwarf-address-space-identifier`. .. _amdgpu-code-conventions:. Code Conventions; ================. This section",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:115860,perform,performing,115860,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performing']
Performance,"``std::cout``, for example, the object would not necessarily be; automatically initialized before your use. To make ``std::cout`` and friends work correctly in these scenarios, the STL; that we use declares a static object that gets created in every translation; unit that includes ``<iostream>``. This object has a static constructor; and destructor that initializes and destroys the global iostream objects; before they could possibly be used in the file. The code that you see in the; ``.ll`` file corresponds to the constructor and destructor registration code. If you would like to make it easier to *understand* the LLVM code generated; by the compiler in the demo page, consider using ``printf()`` instead of; ``iostream``\s to print values. Where did all of my code go??; -----------------------------; If you are using the LLVM demo page, you may often wonder what happened to; all of the code that you typed in. Remember that the demo script is running; the code through the LLVM optimizers, so if your code doesn't actually do; anything useful, it might all be deleted. To prevent this, make sure that the code is actually needed. For example, if; you are computing some expression, return the value from the function instead; of leaving it in a local variable. If you really want to constrain the; optimizer, you can read from and assign to ``volatile`` global variables. What is this ""``undef``"" thing that shows up in my code?; --------------------------------------------------------; ``undef`` is the LLVM way of representing a value that is not defined. You; can get these if you do not initialize a variable before you use it. For; example, the C function:. .. code-block:: c. int X() { int i; return i; }. Is compiled to ""``ret i32 undef``"" because ""``i``"" never has a value specified; for it. Why does instcombine + simplifycfg turn a call to a function with a mismatched calling convention into ""unreachable""? Why not make the verifier reject it?; --------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FAQ.rst:8200,optimiz,optimizers,8200,interpreter/llvm-project/llvm/docs/FAQ.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/FAQ.rst,1,['optimiz'],['optimizers']
Performance,"`consumed`, signifying that the callee expects to take ownership; of a +1 retain count. This is done by adding the ``ns_consumed`` attribute to; the parameter declaration, like so:. .. code-block:: objc. void foo(__attribute((ns_consumed)) id x);; - (void) foo: (id) __attribute((ns_consumed)) x;. This attribute is part of the type of the function or method, not the type of; the parameter. It controls only how the argument is passed and received. When passing such an argument, ARC retains the argument prior to making the; call. When receiving such an argument, ARC releases the argument at the end of the; function, subject to the usual optimizations for local values. .. admonition:: Rationale. This formalizes direct transfers of ownership from a caller to a callee. The; most common scenario here is passing the ``self`` parameter to ``init``, but; it is useful to generalize. Typically, local optimization will remove any; extra retains and releases: on the caller side the retain will be merged with; a +1 source, and on the callee side the release will be rolled into the; initialization of the parameter. The implicit ``self`` parameter of a method may be marked as consumed by adding; ``__attribute__((ns_consumes_self))`` to the method declaration. Methods in; the ``init`` :ref:`family <arc.method-families>` are treated as if they were; implicitly marked with this attribute. It is undefined behavior if an Objective-C message send to a method with; ``ns_consumed`` parameters (other than self) is made with a null receiver. It; is undefined behavior if the method to which an Objective-C message send; statically resolves to has a different set of ``ns_consumed`` parameters than; the method it dynamically resolves to. It is undefined behavior if a block or; function call is made through a static type with a different set of; ``ns_consumed`` parameters than the implementation of the called block or; function. .. admonition:: Rationale. Consumed parameters with null receiver are",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:17596,optimiz,optimization,17596,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['optimiz'],['optimization']
Performance,"`diagAppertainsToDecl``, which checks if the attribute has been used on the; right kind of declaration and issues a diagnostic if not.; * ``diagLangOpts``, which checks if the attribute is permitted for the current; language mode and issues a diagnostic if not.; * ``existsInTarget``, which checks if the attribute is permitted for the given; target. To see a working example of an attribute plugin, see `the Attribute.cpp example; <https://github.com/llvm/llvm-project/blob/main/clang/examples/Attribute/Attribute.cpp>`_. Putting it all together; =======================. Let's look at an example plugin that prints top-level function names. This; example is checked into the clang repository; please take a look at; the `latest version of PrintFunctionNames.cpp; <https://github.com/llvm/llvm-project/blob/main/clang/examples/PrintFunctionNames/PrintFunctionNames.cpp>`_. Running the plugin; ==================. Using the compiler driver; --------------------------. The Clang driver accepts the `-fplugin` option to load a plugin.; Clang plugins can receive arguments from the compiler driver command; line via the `fplugin-arg-<plugin name>-<argument>` option. Using this; method, the plugin name cannot contain dashes itself, but the argument; passed to the plugin can. .. code-block:: console. $ export BD=/path/to/build/directory; $ make -C $BD CallSuperAttr; $ clang++ -fplugin=$BD/lib/CallSuperAttr.so \; -fplugin-arg-call_super_plugin-help \; test.cpp. If your plugin name contains dashes, either rename the plugin or used the; cc1 command line options listed below. Using the cc1 command line; --------------------------. To run a plugin, the dynamic library containing the plugin registry must be; loaded via the `-load` command line option. This will load all plugins; that are registered, and you can select the plugins to run by specifying the; `-plugin` option. Additional parameters for the plugins can be passed with; `-plugin-arg-<plugin-name>`. Note that those options must reach c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangPlugins.rst:4552,load,load,4552,interpreter/llvm-project/clang/docs/ClangPlugins.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangPlugins.rst,1,['load'],['load']
Performance,"`id objc_retainAutoreleasedReturnValue(id value);``; ----------------------------------------------------. *Precondition:* ``value`` is null or a pointer to a valid object. If ``value`` is null, this call has no effect. Otherwise, it attempts to; accept a hand off of a retain count from a call to; :ref:`objc_autoreleaseReturnValue <arc.runtime.objc_autoreleaseReturnValue>` on; ``value`` in a recently-called function or something it tail-calls. If that; fails, it performs a retain operation exactly like :ref:`objc_retain; <arc.runtime.objc_retain>`. Always returns ``value``. .. _arc.runtime.objc_retainBlock:. ``id objc_retainBlock(id value);``; ----------------------------------. *Precondition:* ``value`` is null or a pointer to a valid block object. If ``value`` is null, this call has no effect. Otherwise, if the block pointed; to by ``value`` is still on the stack, it is copied to the heap and the address; of the copy is returned. Otherwise a retain operation is performed on the; block exactly as if it had been sent the ``retain`` message. .. _arc.runtime.objc_storeStrong:. ``void objc_storeStrong(id *object, id value);``; ------------------------------------------------. *Precondition:* ``object`` is a valid pointer to a ``__strong`` object which is; adequately aligned for a pointer. ``value`` is null or a pointer to a valid; object. Performs the complete sequence for assigning to a ``__strong`` object of; non-block type [*]_. Equivalent to the following code:. .. code-block:: objc. void objc_storeStrong(id *object, id value) {; id oldValue = *object;; value = [value retain];; *object = value;; [oldValue release];; }. .. [*] This does not imply that a ``__strong`` object of block type is an; invalid argument to this function. Rather it implies that an ``objc_retain``; and not an ``objc_retainBlock`` operation will be emitted if the argument is; a block. .. _arc.runtime.objc_storeWeak:. ``id objc_storeWeak(id *object, id value);``; ----------------------------------",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:115881,perform,performed,115881,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['performed']
Performance,"`llvm.is.fpclass <llvm.is.fpclass>`). Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <2 x i1> @llvm.vp.is.fpclass.v2f16(<2 x half> %x, i32 3, <2 x i1> %m, i32 %evl); %t = call <vscale x 2 x i1> @llvm.vp.is.fpclass.nxv2f16(<vscale x 2 x half> %x, i32 3, <vscale x 2 x i1> %m, i32 %evl). .. _int_mload_mstore:. Masked Vector Load and Store Intrinsics; ---------------------------------------. LLVM provides intrinsics for predicated vector load and store operations. The predicate is specified by a mask operand, which holds one bit per vector element, switching the associated vector lane on or off. The memory addresses corresponding to the ""off"" lanes are not accessed. When all bits of the mask are on, the intrinsic is identical to a regular vector load or store. When all bits are off, no memory is accessed. .. _int_mload:. '``llvm.masked.load.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. The loaded data is a vector of any integer, floating-point or pointer data type. ::. declare <16 x float> @llvm.masked.load.v16f32.p0(ptr <ptr>, i32 <alignment>, <16 x i1> <mask>, <16 x float> <passthru>); declare <2 x double> @llvm.masked.load.v2f64.p0(ptr <ptr>, i32 <alignment>, <2 x i1> <mask>, <2 x double> <passthru>); ;; The data is a vector of pointers; declare <8 x ptr> @llvm.masked.load.v8p0.p0(ptr <ptr>, i32 <alignment>, <8 x i1> <mask>, <8 x ptr> <passthru>). Overview:; """""""""""""""""". Reads a vector from memory according to the provided mask. The mask holds a bit for each vector lane, and is used to prevent memory accesses to the masked-off lanes. The masked-off lanes in the result vector are taken from the corresponding lanes of the '``passthru``' operand. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is the alignment of the source location. It must be a power of two constant integer value. The third operand, mask, is a vector of boolean values with the same number of e",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:843259,load,loaded,843259,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,"`llvm.objc.copyWeak``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare void @llvm.objc.copyWeak(ptr, ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_copyWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#void-objc-copyweak-id-dest-id-src>`_. '``llvm.objc.destroyWeak``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare void @llvm.objc.destroyWeak(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_destroyWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#void-objc-destroyweak-id-object>`_. '``llvm.objc.initWeak``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.initWeak(ptr, ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_initWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#arc-runtime-objc-initweak>`_. '``llvm.objc.loadWeak``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.loadWeak(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_loadWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#arc-runtime-objc-loadweak>`_. '``llvm.objc.loadWeakRetained``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare ptr @llvm.objc.loadWeakRetained(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_loadWeakRetained <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#arc-runtime-objc-loadweakretained>`_. '``llvm.objc.moveWeak``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare void @llvm.objc.moveWeak(ptr, ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_moveWeak <https://clang.llvm.org/docs/AutomaticReferenceCounting.html#void-objc-moveweak-id-dest-id-src>`_. '``llvm.objc.release``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; ::. declare void @llvm.objc.release(ptr). Lowering:; """""""""""""""""". Lowers to a call to `objc_release <https://clang.llvm.org/docs/AutomaticRefere",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:968719,load,loadWeak,968719,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loadWeak']
Performance,"`llvm.vp.smax.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.smax.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.smax.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.smax.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated integer signed maximum of two vectors of integers. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of integer type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.smax``' intrinsic performs integer signed maximum (:ref:`smax <int_smax>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.smax.v4i32(<4 x i32> %a, <4 x i32> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x i32> @llvm.smax.v4i32(<4 x i32> %a, <4 x i32> %b); %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_smin:. '``llvm.vp.smin.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.smin.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.smin.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.smin.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:718528,perform,performs,718528,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"`llvm.vp.smin.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.smin.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.smin.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.smin.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated integer signed minimum of two vectors of integers. Arguments:; """""""""""""""""""". The first two operands and the result have the same vector of integer type. The; third operand is the vector mask and has the same number of elements as the; result vector type. The fourth operand is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.smin``' intrinsic performs integer signed minimum (:ref:`smin <int_smin>`); of the first and second vector operand on each enabled lane. The result on; disabled lanes is a :ref:`poison value <poisonvalues>`. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <4 x i32> @llvm.vp.smin.v4i32(<4 x i32> %a, <4 x i32> %b, <4 x i1> %mask, i32 %evl); ;; For all lanes below %evl, %r is lane-wise equivalent to %also.r. %t = call <4 x i32> @llvm.smin.v4i32(<4 x i32> %a, <4 x i32> %b); %also.r = select <4 x i1> %mask, <4 x i32> %t, <4 x i32> poison. .. _int_vp_umax:. '``llvm.vp.umax.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare <16 x i32> @llvm.vp.umax.v16i32 (<16 x i32> <left_op>, <16 x i32> <right_op>, <16 x i1> <mask>, i32 <vector_length>); declare <vscale x 4 x i32> @llvm.vp.umax.nxv4i32 (<vscale x 4 x i32> <left_op>, <vscale x 4 x i32> <right_op>, <vscale x 4 x i1> <mask>, i32 <vector_length>); declare <256 x i64> @llvm.vp.umax.v256i64 (<256 x i64> <left_op>, <256 x i64> <right_op>, <256",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:720026,perform,performs,720026,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performs']
Performance,"`nosync` function does ever synchronize with another thread,; the behavior is undefined.; ``nounwind``; This function attribute indicates that the function never raises an; exception. If the function does raise an exception, its runtime; behavior is undefined. However, functions marked nounwind may still; trap or generate asynchronous exceptions. Exception handling schemes; that are recognized by LLVM to handle asynchronous exceptions, such; as SEH, will still provide their implementation defined semantics.; ``nosanitize_bounds``; This attribute indicates that bounds checking sanitizer instrumentation; is disabled for this function.; ``nosanitize_coverage``; This attribute indicates that SanitizerCoverage instrumentation is disabled; for this function.; ``null_pointer_is_valid``; If ``null_pointer_is_valid`` is set, then the ``null`` address; in address-space 0 is considered to be a valid address for memory loads and; stores. Any analysis or optimization should not treat dereferencing a; pointer to ``null`` as undefined behavior in this function.; Note: Comparing address of a global variable to ``null`` may still; evaluate to false because of a limitation in querying this attribute inside; constant expressions.; ``optdebug``; This attribute suggests that optimization passes and code generator passes; should make choices that try to preserve debug info without significantly; degrading runtime performance.; This attribute is incompatible with the ``minsize``, ``optsize``, and; ``optnone`` attributes.; ``optforfuzzing``; This attribute indicates that this function should be optimized; for maximum fuzzing signal.; ``optnone``; This function attribute indicates that most optimization passes will skip; this function, with the exception of interprocedural optimization passes.; Code generation defaults to the ""fast"" instruction selector.; This attribute cannot be used together with the ``alwaysinline``; attribute; this attribute is also incompatible; with the ``minsize``, ``",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:95056,optimiz,optimization,95056,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimization']
Performance,"`s of single-precision floating point values has been greatly improved by employing; Kahan summations.; - The content of [execution logs](https://root.cern/doc/master/classROOT_1_1RDataFrame.html#rdf-logging) from RDataFrame; has been streamlined in order to make them more useful. ### Distributed RDataFrame. - Add support for systematic variations (e.g. `Vary` and `VariationsFor` operations) in distributed mode.; - If an instant action (e.g. `Snapshot`) is purposely made lazy by the user, distributed RDataFrame now respects this; and avoids triggering the computations right away.; - The algorithm for automatic splitting of the input dataset has been reworked, bringing the startup time cost of; distributed RDataFrame close to zero.; - A histogram model (name, title, binning) for the `Histo*D` actions is now required in distributed mode. See the; [relative PR](https://github.com/root-project/root/pull/10368) for more discussion.; - The performance of distributed RDataFrame for large computation graphs (>1000 operations) has been greatly improved.; - If the `npartitions` argument is not set by the user, the default number of tasks created by a distributed RDataFrame; is equal to the number of cores specified by the user when connecting to the cluster.; - C++ exceptions (i.e. instances of `std::exception` and derived) are now correctly propagated from the processes of the; computing nodes to the user side.; - The minimum `dask` version required to support distributed RDataFrame is 2022.8.1, since a series of critical bugs; present before that version were hindering the normal execution of the tool. Consequently, the minimum Python version; needed to include distributed RDataFrame in the ROOT build is Python 3.8. More information in the relative; [github issue](https://github.com/root-project/root/issues/11515).; - `Stats` and `StdDev` operations are now available in distributed mode.; - `GetColumnNames` operation is now available in distributed mode. ## Histogram Librar",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md:13663,perform,performance,13663,README/ReleaseNotes/v628/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v628/index.md,1,['perform'],['performance']
Performance,"a !{metadata !""Compiler V2""}. ;;;;; Inputs/ident.b.ll:. !llvm.ident = !{!0}; !0 = metadata !{metadata !""Compiler V3""}. For symmetry reasons, ``ident.ll`` is just a dummy file that doesn't; actually participate in the test besides holding the ``RUN:`` lines. .. note::. Some existing tests use ``RUN: true`` in extra files instead of just; putting the extra files in an ``Inputs/`` directory. This pattern is; deprecated. Fragile tests; -------------. It is easy to write a fragile test that would fail spuriously if the tool being; tested outputs a full path to the input file. For example, :program:`opt` by; default outputs a ``ModuleID``:. .. code-block:: console. $ cat example.ll; define i32 @main() nounwind {; ret i32 0; }. $ opt -S /path/to/example.ll; ; ModuleID = '/path/to/example.ll'. define i32 @main() nounwind {; ret i32 0; }. ``ModuleID`` can unexpectedly match against ``CHECK`` lines. For example:. .. code-block:: llvm. ; RUN: opt -S %s | FileCheck. define i32 @main() nounwind {; ; CHECK-NOT: load; ret i32 0; }. This test will fail if placed into a ``download`` directory. To make your tests robust, always use ``opt ... < %s`` in the RUN line.; :program:`opt` does not output a ``ModuleID`` when input comes from stdin. Platform-Specific Tests; -----------------------. Whenever adding tests that require the knowledge of a specific platform,; either related to code generated, specific output or back-end features,; you must make sure to isolate the features, so that buildbots that; run on different architectures (and don't even compile all back-ends),; don't fail. The first problem is to check for target-specific output, for example sizes; of structures, paths and architecture names, for example:. * Tests containing Windows paths will fail on Linux and vice-versa.; * Tests that check for ``x86_64`` somewhere in the text will fail anywhere else.; * Tests where the debug information calculates the size of types and structures. Also, if the test rely on any behaviour t",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestingGuide.rst:16999,load,load,16999,interpreter/llvm-project/llvm/docs/TestingGuide.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestingGuide.rst,1,['load'],['load']
Performance,"a 64-bit load - ``LDR q0, [foo]``. However if we try this in big endian mode, because of the byte swapping the lane indices end up being swapped! The zero'th item as laid out in memory becomes the n'th lane in the vector. .. figure:: ARM-BE-ld1.png; :align: right. Big endian vector load using ``LD1``. Note that the lanes retain the correct ordering. Because of this, the instruction ``LD1`` performs a vector load but performs byte swapping not on the entire 64 bits, but on the individual items within the vector. This means that the register content is the same as it would have been on a little endian system. It may seem that ``LD1`` should suffice to perform vector loads on a big endian machine. However there are pros and cons to the two approaches that make it less than simple which register format to pick. There are two options:. 1. The content of a vector register is the same *as if* it had been loaded with an ``LDR`` instruction.; 2. The content of a vector register is the same *as if* it had been loaded with an ``LD1`` instruction. Because ``LD1 == LDR + REV`` and similarly ``LDR == LD1 + REV`` (on a big endian system), we can simulate either type of load with the other type of load plus a ``REV`` instruction. So we're not deciding which instructions to use, but which format to use (which will then influence which instruction is best to use). .. The 'clearer' container is required to make the following section header come after the floated; images above.; .. container:: clearer. Note that throughout this section we only mention loads. Stores have exactly the same problems as their associated loads, so have been skipped for brevity. Considerations; ==============. LLVM IR Lane ordering; ---------------------. LLVM IR has first class vector types. In LLVM IR, the zero'th element of a vector resides at the lowest memory address. The optimizer relies on this property in certain areas, for example when concatenating vectors together. The intention is for arrays and ve",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:3919,load,loaded,3919,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,1,['load'],['loaded']
Performance,"a Clang library, and has; the capability to be reused in different contexts and by different clients.; Important Points to Consider; While we believe that the static analyzer is already very useful for finding; bugs, we ask you to bear in mind a few points when using it.; Work-in-Progress; The analyzer is a continuous work-in-progress. There are many planned; enhancements to improve both the precision and scope of its analysis algorithms; as well as the kinds of bugs it will find. While there are fundamental; limitations to what static analysis can do, we have a long way to go before; hitting that wall.; Slower than Compilation; Operationally, using static analysis to; automatically find deep program bugs is about trading CPU time for the hardening; of code. Because of the deep analysis performed by state-of-the-art static; analysis tools, static analysis can be much slower than compilation.; While the Clang Static Analyzer is being designed to be as fast and; light-weight as possible, please do not expect it to be as fast as compiling a; program (even with optimizations enabled). Some of the algorithms needed to find; bugs require in the worst case exponential time.; The Clang Static Analyzer runs in a reasonable amount of time by both; bounding the amount of checking work it will do as well as using clever; algorithms to reduce the amount of work it must do to find bugs.; False Positives; Static analysis is not perfect. It can falsely flag bugs in a program where; the code behaves correctly. Because some code checks require more analysis; precision than others, the frequency of false positives can vary widely between; different checks. Our long-term goal is to have the analyzer have a low false; positive rate for most code on all checks.; Please help us in this endeavor by reporting false; positives. False positives cannot be addressed unless we know about; them.; More Checks; Static analysis is not magic; a static analyzer can only find bugs that it; has been spec",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/index.html:3145,optimiz,optimizations,3145,interpreter/llvm-project/clang/www/analyzer/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/index.html,2,['optimiz'],['optimizations']
Performance,"a block ends with both a conditional branch and an ensuing unconditional; branch, then ``analyzeBranch`` (shown below) should return the conditional; branch destination (assuming it corresponds to a conditional evaluation of; ""``true``"") in the ``TBB`` parameter and the unconditional branch destination; in the ``FBB`` (corresponding to a conditional evaluation of ""``false``""). A; list of operands to evaluate the condition should be returned in the ``Cond``; parameter. .. code-block:: c++. unsigned SecondLastOpc = SecondLastInst->getOpcode();. if ((SecondLastOpc == ARM::Bcc && LastOpc == ARM::B) ||; (SecondLastOpc == ARM::tBcc && LastOpc == ARM::tB)) {; TBB = SecondLastInst->getOperand(0).getMBB();; Cond.push_back(SecondLastInst->getOperand(1));; Cond.push_back(SecondLastInst->getOperand(2));; FBB = LastInst->getOperand(0).getMBB();; return false;; }. For the last two cases (ending with a single conditional branch or ending with; one conditional and one unconditional branch), the operands returned in the; ``Cond`` parameter can be passed to methods of other instructions to create new; branches or perform other operations. An implementation of ``analyzeBranch``; requires the helper methods ``removeBranch`` and ``insertBranch`` to manage; subsequent operations. ``analyzeBranch`` should return false indicating success in most circumstances.; ``analyzeBranch`` should only return true when the method is stumped about what; to do, for example, if a block has three terminating branches.; ``analyzeBranch`` may return true if it encounters a terminator it cannot; handle, such as an indirect branch. .. _instruction-selector:. Instruction Selector; ====================. LLVM uses a ``SelectionDAG`` to represent LLVM IR instructions, and nodes of; the ``SelectionDAG`` ideally represent native target instructions. During code; generation, instruction selection passes are performed to convert non-native; DAG instructions into native target-specific instructions. The pass described",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst:51485,perform,perform,51485,interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/WritingAnLLVMBackend.rst,1,['perform'],['perform']
Performance,"a class name ``T``, the message; send expression has type ``T*``; otherwise; * if it is an instance method, and the receiver has type ``T``, the message; send expression has type ``T``; otherwise; * the message send expression has the normal result type of the method. This is a new rule of the Objective-C language and applies outside of ARC. .. admonition:: Rationale. ARC's automatic code emission is more prone than most code to signature; errors, i.e. errors where a call was emitted against one method signature,; but the implementing method has an incompatible signature. Having more; precise type information helps drastically lower this risk, as well as; catching a number of latent bugs. .. _arc.optimization:. Optimization; ============. Within this section, the word :arc-term:`function` will be used to; refer to any structured unit of code, be it a C function, an; Objective-C method, or a block. This specification describes ARC as performing specific ``retain`` and; ``release`` operations on retainable object pointers at specific; points during the execution of a program. These operations make up a; non-contiguous subsequence of the computation history of the program.; The portion of this sequence for a particular retainable object; pointer for which a specific function execution is directly; responsible is the :arc-term:`formal local retain history` of the; object pointer. The corresponding actual sequence executed is the; `dynamic local retain history`. However, under certain circumstances, ARC is permitted to re-order and; eliminate operations in a manner which may alter the overall; computation history beyond what is permitted by the general ""as if""; rule of C/C++ and the :ref:`restrictions <arc.objects.retains>` on; the implementation of ``retain`` and ``release``. .. admonition:: Rationale. Specifically, ARC is sometimes permitted to optimize ``release``; operations in ways which might cause an object to be deallocated; before it would otherwise be. Without ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:75715,perform,performing,75715,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['performing']
Performance,"a class that is; defined in a library. #### Linktime Library Dependencies. When building your own executable you will have to link against the; libraries that contain the classes you use. The ROOT reference guide; states the library a class is reference guide defined in. Almost all; relevant classes can be found in libraries returned by; `root-config -glibs`; the graphics libraries are retuned by; `root-config --libs`. These commands are commonly used in `Makefiles`.; Using `root-config` instead of enumerating the libraries by hand; allows you to link them in a platform independent way. Also, if ROOT; library names change you will not need to change your Makefile. A batch program that does not have a graphic display, which creates,; fills, and saves histograms and trees, only needs to link the core; libraries (`libCore`, `libRIO`), `libHist` and `libTree`.; If ROOT needs access to other libraries, it loads them dynamically.; For example, if the **`TreeViewer`** is used, `libTreePlayer` and all; libraries `libTreePlayer` depends on are loaded also. The dependent; libraries are shown in the ROOT reference guide's library dependency; graph. The difference between reference guide `libHist` and; `libHistPainter` is that the former needs to be explicitly linked and; the latter will be loaded automatically at runtime when ROOT needs it,; by means of the Plugin Manager. plugin manager. In the Figure 1-2, the libraries represented by green boxes outside of; the core are loaded via the plugin manager plugin manager or; equivalent techniques, while the white ones are not. Of course, if one; wants to access a plugin library directly, it has to be explicitly; linked. An example of a plugin library is `libMinuit`. To create and; fill histograms you need to link `libHist.so`. If the code has a call; to fit the histogram, the ""fitter"" will dynamically load libMinuit if; it is not yet loaded. #### Plugins: Runtime Library Dependencies for Linking. plugin manager The Plugin Manager *",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Introduction.md:18060,load,loaded,18060,documentation/users-guide/Introduction.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Introduction.md,1,['load'],['loaded']
Performance,"a compact encoding; that represents types, source locations, and variable locations. The short summary of this chapter is that we'll go through the; various things you have to add to a programming language to; support debug info, and how you translate that into DWARF. Caveat: For now we can't debug via the JIT, so we'll need to compile; our program down to something small and standalone. As part of this; we'll make a few modifications to the running of the language and; how programs are compiled. This means that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:1752,optimiz,optimized,1752,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,1,['optimiz'],['optimized']
Performance,"a concern. This flag is only compatible with :doc:`control flow integrity; <ControlFlowIntegrity>` schemes and :doc:`UndefinedBehaviorSanitizer`; checks other than ``vptr``. This flag is enabled by default for sanitizers in the ``cfi`` group. .. option:: -fsanitize-ignorelist=/path/to/ignorelist/file. Disable or modify sanitizer checks for objects (source files, functions,; variables, types) listed in the file. See; :doc:`SanitizerSpecialCaseList` for file format description. .. option:: -fno-sanitize-ignorelist. Don't use ignorelist file, if it was specified earlier in the command line. .. option:: -f[no-]sanitize-coverage=[type,features,...]. Enable simple code coverage in addition to certain sanitizers.; See :doc:`SanitizerCoverage` for more details. .. option:: -f[no-]sanitize-address-outline-instrumentation. Controls how address sanitizer code is generated. If enabled will always use; a function call instead of inlining the code. Turning this option on could; reduce the binary size, but might result in a worse run-time performance. See :doc: `AddressSanitizer` for more details. .. option:: -f[no-]sanitize-stats. Enable simple statistics gathering for the enabled sanitizers.; See :doc:`SanitizerStats` for more details. .. option:: -fsanitize-undefined-trap-on-error. Deprecated alias for ``-fsanitize-trap=undefined``. .. option:: -fsanitize-cfi-cross-dso. Enable cross-DSO control flow integrity checks. This flag modifies; the behavior of sanitizers in the ``cfi`` group to allow checking; of cross-DSO virtual and indirect calls. .. option:: -fsanitize-cfi-icall-generalize-pointers. Generalize pointers in return and argument types in function type signatures; checked by Control Flow Integrity indirect call checking. See; :doc:`ControlFlowIntegrity` for more details. .. option:: -fsanitize-cfi-icall-experimental-normalize-integers. Normalize integers in return and argument types in function type signatures; checked by Control Flow Integrity indirect call checking. S",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:79155,perform,performance,79155,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['perform'],['performance']
Performance,"a consistent ordering exists. Relevant standard; This corresponds to the C++/C ``memory_order_relaxed``; see those; standards for the exact definition. Notes for frontends; If you are writing a frontend which uses this directly, use with caution. The; guarantees in terms of synchronization are very weak, so make sure these are; only used in a pattern which you know is correct. Generally, these would; either be used for atomic operations which do not protect other memory (like; an atomic counter), or along with a ``fence``. Notes for optimizers; In terms of the optimizer, this can be treated as a read+write on the relevant; memory location (and alias analysis will take advantage of that). In addition,; it is legal to reorder non-atomic and Unordered loads around Monotonic; loads. CSE/DSE and a few other optimizations are allowed, but Monotonic; operations are unlikely to be used in ways which would make those; optimizations useful. Notes for code generation; Code generation is essentially the same as that for unordered for loads and; stores. No fences are required. ``cmpxchg`` and ``atomicrmw`` are required; to appear as a single operation. Acquire; -------. Acquire provides a barrier of the sort necessary to acquire a lock to access; other memory with normal loads and stores. Relevant standard; This corresponds to the C++/C ``memory_order_acquire``. It should also be; used for C++/C ``memory_order_consume``. Notes for frontends; If you are writing a frontend which uses this directly, use with caution.; Acquire only provides a semantic guarantee when paired with a Release; operation. Notes for optimizers; Optimizers not aware of atomics can treat this like a nothrow call. It is; also possible to move stores from before an Acquire load or read-modify-write; operation to after it, and move non-Acquire loads from before an Acquire; operation to after it. Notes for code generation; Architectures with weak memory ordering (essentially everything relevant today; except x86",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:11332,load,loads,11332,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['loads']
Performance,"a longer instruction encoding; which may put greater pressure on the frontend fetch and decode stages,; potentially reducing the rate that instructions are dispatched to the backend,; particularly on older hardware. Comparing baseline results with this mode; enabled can help determine the effects of the frontend and can be used to; improve latency and throughput estimates. .. option:: --repetition-mode=[duplicate|loop|min]. Specify the repetition mode. `duplicate` will create a large, straight line; basic block with `num-repetitions` instructions (repeating the snippet; `num-repetitions`/`snippet size` times). `loop` will, optionally, duplicate the; snippet until the loop body contains at least `loop-body-size` instructions,; and then wrap the result in a loop which will execute `num-repetitions`; instructions (thus, again, repeating the snippet; `num-repetitions`/`snippet size` times). The `loop` mode, especially with loop; unrolling tends to better hide the effects of the CPU frontend on architectures; that cache decoded instructions, but consumes a register for counting; iterations. If performing an analysis over many opcodes, it may be best to; instead use the `min` mode, which will run each other mode,; and produce the minimal measured result. .. option:: --num-repetitions=<Number of repetitions>. Specify the target number of executed instructions. Note that the actual; repetition count of the snippet will be `num-repetitions`/`snippet size`.; Higher values lead to more accurate measurements but lengthen the benchmark. .. option:: --loop-body-size=<Preferred loop body size>. Only effective for `-repetition-mode=[loop|min]`.; Instead of looping over the snippet directly, first duplicate it so that the; loop body contains at least this many instructions. This potentially results; in loop body being cached in the CPU Op Cache / Loop Cache, which allows to; which may have higher throughput than the CPU decoders. .. option:: --max-configs-per-opcode=<value>. Specify ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst:12917,cache,cache,12917,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,1,['cache'],['cache']
Performance,"a members. You gain; space on the file, and you do not loose functionality if you do not use; the `fBits` and `fUniqueID. `See ""The Role of TObject"" on the use of; `fBits` and `fUniqueID`. ### Streaming a TClonesArray. When writing a **`TClonesArray`** it bypasses by default the; `Streamer `of the member class and uses a more efficient internal; mechanism to write the members to the file. You can override the default; and specify that the member class `Streamer `is used by setting the; `TClonesArray::BypassStreamer` bit to false:. ``` {.cpp}; TClonesArray *fTracks;; fTracks->BypassStreamer(kFALSE); // use the member Streamer; ```. When the `kBypassStreamer` bit is set, the automatically generated; `Streamer `can call directly the method **`TClass::WriteBuffer`**.; Bypassing the `Streamer` improves the performance when writing/reading; the objects in the **`TClonesArray`**. However, the drawback is when a; **`TClonesArray`** is written with `split=0` bypassing the `Streamer`,; the `StreamerInfo `of the class in the array being optimized, one cannot; later use the **`TClonesArray`** with `split > 0`. For example, there is; a problem with the following scenario: a class `Foo` has a; **`TClonesArray`** of `Bar` objects the `Foo` object is written with; `split=0` to `Tree` `T1`. In this case the `StreamerInfo` for the class; `Bar` is created in optimized mode in such a way that data members of; the same type are written as an array improving the I/O performance. In; a new program, `T1` is read and a new `Tree` `T2` is created with the; object `Foo` in `split > 1`. When the `T2 `branch is created, the `StreamerInfo` for the class `Bar`; is created with no optimization (mandatory for the split mode). The; optimized Bar `StreamerInfo` is going to be used to read the; **`TClonesArray`** in `T1`. The result will be `Bar` objects with data; member values not in the right sequence. The solution to this problem is; to call `BypassStreamer(kFALSE)` for the **`TClonesArray`**. In ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md:54259,optimiz,optimized,54259,documentation/users-guide/InputOutput.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md,1,['optimiz'],['optimized']
Performance,"a problem on PIC16 (and is also probably wrong on alpha and other 64-bit; targets). //===----------------------------------------------------------------------===//; // The proposal; //===----------------------------------------------------------------------===//. I suggest that we have the front-end fully lower out the ABI issues here to; LLVM IR. This makes it 100% explicit what is going on and means that there is; no cause for confusion. For example, the cases above should compile into:. define i32 @z() nounwind {; entry:; %0 = tail call i32 (...)* @y() nounwind; 	%1 = trunc i32 %0 to i16; %2 = sext i16 %1 to i32; ret i32 %2; }; define i32 @b() nounwind {; entry:; 	%0 = tail call i32 (...)* @a() nounwind; 	%retval12 = trunc i32 %0 to i16; 	%tmp = sext i16 %retval12 to i32; 	ret i32 %tmp; }. In this model, no functions will return an i1/i8/i16 (and on a x86-64 target; that extends results to i64, no i32). This solves the ambiguity issue, allows us ; to fully describe all possible ABIs, and now allows the optimizers to reason; about and eliminate these extensions. The one thing that is missing is the ability for the front-end and optimizer to; specify/infer the guarantees provided by the ABI to allow other optimizations.; For example, in the y/z case, since y is known to return a sign extended value,; the trunc/sext in z should be eliminable. This can be done by introducing new sext/zext attributes which mean ""I know; that the result of the function is sign extended at least N bits. Given this,; and given that it is stuck on the y function, the mid-level optimizer could; easily eliminate the extensions etc with existing functionality. The major disadvantage of doing this sort of thing is that it makes the ABI; lowering stuff even more explicit in the front-end, and that we would like to; eventually move to having the code generator do more of this work. However,; the sad truth of the matter is that this is a) unlikely to happen anytime in; the near future, and b) th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt:3875,optimiz,optimizers,3875,interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ExtendedIntegerResults.txt,1,['optimiz'],['optimizers']
Performance,"a single thread. A **`thread`** is a sequence of instructions being executed in a; program. A thread has a program counter and a private stack to keep; track of local variables and return addresses. A multithreaded process; is associated with one or more threads. Threads execute independently.; All threads in a given process share the private address space of that; process. **`Concurrency`** exists when at least two threads are in progress at; the same time. A system with only a single processor can support; concurrency by switching execution contexts among multiple threads. **`Parallelism`** arises when at least two threads are executing; simultaneously. This requires a system with multiple processors.; Parallelism implies concurrency, but not vice-versa. A function is **`reentrant`** if it will behave correctly even if a; thread of execution enters the function while one or more threads are; already executing within the function. These could be the same thread,; in the case of recursion, or different threads, in the case of; concurrency. **`Thread-specific data`** (**`TSD`**) is also known as thread-local; storage (TLS). Normally, any data that has lifetime beyond the local; variables on the thread's private stack are shared among all threads; within the process. Thread-specific data is a form of static or global; data that is maintained on a per-thread basis. That is, each thread gets; its own private copy of the data. Left to their own devices, threads execute independently.; **`Synchronization`** is the work that must be done when there are, in; fact, interdependencies that require some form of communication among; threads. Synchronization tools include mutexes, semaphores, condition; variables, and other variations on locking. A **`critical section`** is a section of code that accesses a; non-sharable resource. To ensure correct code, only one thread at a time; may execute in a critical section. In other words, the section is not; reentrant. A **`mutex`**, or ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md:20833,concurren,concurrency,20833,documentation/users-guide/Threads.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Threads.md,1,['concurren'],['concurrency']
Performance,"a subsequent LoadLibrary() call.; Source: ; MSDN: LoadLibrary function, Security Remarks. #include <windows.h>. HINSTANCE test() {; char filePath[100];; SearchPath(NULL, ""file.dll"", NULL, 100, filePath, NULL);; return LoadLibrary(filePath); // warn; }. WinAPI.WideCharToMultiByte; (C); Buffer overrun while calling WideCharToMultiByte(). The size of; the input buffer equals the number of characters in the Unicode string, while; the size of the output buffer equals the number of bytes.; Source: ; MSDN: WideCharToMultiByte function. #include <windows.h>. void test() {; wchar_t ws[] = L""abc"";; char s[3];; WideCharToMultiByte(CP_UTF8, 0, ws, -1, s,; 3, NULL, NULL); // warn; }. optimization. Name, DescriptionExampleProgress. optimization.PassConstObjByValue; (C, C++); Optimization: It is more effective to pass constant parameter by reference to; avoid unnecessary object copying. struct A {};. void f(const struct A a); // warn. optimization.PostfixIncIter; (C++); Optimization: It is more effective to use prefix increment operator with; iterator.; Source: Scott Meyers ""More Effective C++"", item 6:; Distinguish between prefix and postfix forms of increment and decrement; operators. #include <vector>. void test() {; std::vector<int> v;; std::vector<int>::const_iterator it;; for(it = v.begin();; it != v.end(); it++) {}; // warn; }. optimization.MultipleCallsStrlen; (C); Optimization: multiple calls to strlen() for a string in an; expression. It is more effective to hold a value returned; from strlen() in a temporary variable. #include <string.h>. void test(const char* s) {; if (strlen(s) > 0 &&; strlen(s) < 7) {}; // warn; }. optimization.StrLengthCalculation; (C++); Optimization: it is more efficient to use string::length() to; calculate the length of an std::string. #include <string>; #include <string.h>. void test() {; std::string s;; if (strlen(s.c_str()) != 0) {}; // warn; }. optimization.EmptyContainerDetect; (C++); Optimization: It is more efficient to use containers emp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html:27506,Optimiz,Optimization,27506,interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/potential_checkers.html,1,['Optimiz'],['Optimization']
Performance,"a summary of the major modifications since 5.34 . ### New developments/functionality. - Several improvements in the merging phase; in particular:; - Modification of output sending protocol to control memory usage, significantly reducing the memory footprint on the master, in particular when merging; large numbers of histograms.; - Use an hash table for the output list to significantly speed up names lookups during merging.; - Add support for dynamic addition of workers to a currently running process (currently supported by the unit packetizer).; - Automatization of the usage of file-based technology to handle outputs.; - [Improved dataset management model](https://root.cern/doc/v628/classTDataSetManagerAliEn.html); where the PROOF (ROOT) dataset manager is a light frontend to the experiment file catalogs; TDataSetManagerFile is still; used as local cache of the experiment information or to store the work-in-progress status of the dataset manager daemon. This model addresses the scalability issues observed at ALICE AFs.; - Improvements in [TProofBench](https://root.cern.ch/doc/master/classTProofBench.html):; - Recording and display of the maximum rate during query, CPU efficiency calculation for PROOF-Lite runs, better measurement of wall time.; - Support for dynamic startup mode. - Test program xpdtest to test the status of xproofd (see also man page under $ROOTSYS/man/man1):. ``` {.sh}; $ xpdtest [options]; --help, -h; Gives a short list of options avaliable, and exit; -t <test>; type of test to be run:; 0 ping the daemon (includes process existence check if pid specified; see below); 1 ping the daemon and check connection for default user; 2 ping the daemon and check connection for the default user and all recent users; ...; ```; - Interface with **igprof** for fast statistic profiling. Like valgrind, it can be specified as option to TProof::Open and the output is available via the log viewer technology:. ``` {.cpp}; root[] p = TProof::Open(""master"", ""igprof-pp""); ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v600/index.md:1175,scalab,scalability,1175,proof/doc/v600/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/doc/v600/index.md,1,['scalab'],['scalability']
Performance,"a testcase:. ::. extern foo(a); # ok, defines foo.; def foo(b) b; # Error: Unknown variable name. (decl using 'a' takes precedence). Driver Changes and Closing Thoughts; ===================================. For now, code generation to LLVM doesn't really get us much, except that; we can look at the pretty IR calls. The sample code inserts calls to; codegen into the ""``HandleDefinition``"", ""``HandleExtern``"" etc; functions, and then dumps out the LLVM IR. This gives a nice way to look; at the LLVM IR for simple functions. For example:. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Note how the parser turns the top-level expression into anonymous; functions for us. This will be handy when we add `JIT; support <LangImpl04.html#adding-a-jit-compiler>`_ in the next chapter. Also note that the; code is very literally transcribed, no optimizations are being performed; except simple constant folding done by IRBuilder. We will `add; optimizations <LangImpl04.html#trivial-constant-folding>`_ explicitly in the next; chapter. ::. ready> def foo(a b) a*a + 2*a*b + b*b;; Read function definition:; define double @foo(double %a, double %b) {; entry:; %multmp = fmul double %a, %a; %multmp1 = fmul double 2.000000e+00, %a; %multmp2 = fmul double %multmp1, %b; %addtmp = fadd double %multmp, %multmp2; %multmp3 = fmul double %b, %b; %addtmp4 = fadd double %addtmp, %multmp3; ret double %addtmp4; }. This shows some simple arithmetic. Notice the striking similarity to the; LLVM builder calls that we use to create the instructions. ::. ready> def bar(a) foo(a, 4.0) + bar(31337);; Read function definition:; define double @bar(double %a) {; entry:; %calltmp = call double @foo(double %a, double 4.000000e+00); %calltmp1 = call double @bar(double 3.133700e+04); %addtmp = fadd double %calltmp, %calltmp1; ret double %addtmp; }. This shows some function calls. Note that this function will take a long; time to execute if you call it. In the futu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:18321,optimiz,optimizations,18321,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,1,['optimiz'],['optimizations']
Performance,"a to i8; %b2 = sext i4 %b to i8; %mul = mul nsw nuw i8 %a2, %b2; %scale2 = trunc i32 %scale to i8; %r = ashr i8 %mul, i8 %scale2 ; this is for a target rounding down towards negative infinity; %result = trunc i8 %r to i4. The ``llvm.*div.fix`` family of intrinsic functions represents a division of; fixed point numbers through scaled integers. Fixed point division can be; represented as:. .. code-block:: llvm. %result call i4 @llvm.sdiv.fix.i4(i4 %a, i4 %b, i32 %scale). ; Expands to; %a2 = sext i4 %a to i8; %b2 = sext i4 %b to i8; %scale2 = trunc i32 %scale to i8; %a3 = shl i8 %a2, %scale2; %r = sdiv i8 %a3, %b2 ; this is for a target rounding towards zero; %result = trunc i8 %r to i4. For each of these functions, if the result cannot be represented exactly with; the provided scale, the result is rounded. Rounding is unspecified since; preferred rounding may vary for different targets. Rounding is specified; through a target hook. Different pipelines should legalize or optimize this; using the rounding specified by this hook if it is provided. Operations like; constant folding, instruction combining, KnownBits, and ValueTracking should; also use this hook, if provided, and not assume the direction of rounding. A; rounded result must always be within one unit of precision from the true; result. That is, the error between the returned result and the true result must; be less than 1/2^(scale). '``llvm.smul.fix.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax; """""""""""""". This is an overloaded intrinsic. You can use ``llvm.smul.fix``; on any integer bit width or vectors of integers. ::. declare i16 @llvm.smul.fix.i16(i16 %a, i16 %b, i32 %scale); declare i32 @llvm.smul.fix.i32(i32 %a, i32 %b, i32 %scale); declare i64 @llvm.smul.fix.i64(i64 %a, i64 %b, i32 %scale); declare <4 x i32> @llvm.smul.fix.v4i32(<4 x i32> %a, <4 x i32> %b, i32 %scale). Overview; """""""""""""""""". The '``llvm.smul.fix``' family of intrinsic functions perform signed; fixed point multiplication on 2 arg",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:620678,optimiz,optimize,620678,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimize']
Performance,"a tool that optimizes binaries; post-link by profiling them at runtime and then using that information to; optimize the layout of the final binary among other optimizations performed; at the binary level. There are also CMake caches available to build; LLVM/Clang with BOLT. To configure a single-stage build that builds LLVM/Clang and then optimizes; it with BOLT, use the following CMake configuration:. .. code-block:: console. $ cmake <path to source>/llvm -C <path to source>/clang/cmake/caches/BOLT.cmake. Then, build the BOLT-optimized binary by running the following ninja command:. .. code-block:: console. $ ninja clang-bolt. If you're seeing errors in the build process, try building with a recent; version of Clang/LLVM by setting the CMAKE_C_COMPILER and; CMAKE_CXX_COMPILER flags to the appropriate values. It is also possible to use BOLT on top of PGO and (Thin)LTO for an even more; significant runtime speedup. To configure a three stage PGO build with ThinLTO; that optimizes the resulting binary with BOLT, use the following CMake; configuration command:. .. code-block:: console. $ cmake -G Ninja <path to source>/llvm \; -C <path to source>/clang/cmake/caches/BOLT-PGO.cmake \; -DBOOTSTRAP_LLVM_ENABLE_LLD=ON \; -DBOOTSTRAP_BOOTSTRAP_LLVM_ENABLE_LLD=ON \; -DPGO_INSTRUMENT_LTO=Thin. Then, to build the final optimized binary, build the stage2-clang-bolt target:. .. code-block:: console. $ ninja stage2-clang-bolt. 3-Stage Non-Determinism; =======================. In the ancient lore of compilers non-determinism is like the multi-headed hydra.; Whenever its head pops up, terror and chaos ensue. Historically one of the tests to verify that a compiler was deterministic would; be a three stage build. The idea of a three stage build is you take your sources; and build a compiler (stage1), then use that compiler to rebuild the sources; (stage2), then you use that compiler to rebuild the sources a third time; (stage3) with an identical configuration to the stage2 build. At th",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst:10921,optimiz,optimizes,10921,interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AdvancedBuilds.rst,1,['optimiz'],['optimizes']
Performance,"a wavefront in; execution order of other load/store/sample operations performed by that; wavefront.; * The vector memory operations access a vector L0 cache. There is a single L0; cache per CU. Each SIMD of a CU accesses the same L0 cache. Therefore, no; special action is required for coherence between the lanes of a single; wavefront. However, a ``buffer_gl0_inv`` is required for coherence between; wavefronts executing in the same work-group as they may be executing on SIMDs; of different CUs that access different L0s. A ``buffer_gl0_inv`` is also; required for coherence between wavefronts executing in different work-groups; as they may be executing on different WGPs.; * The scalar memory operations access a scalar L0 cache shared by all wavefronts; on a WGP. The scalar and vector L0 caches are not coherent. However, scalar; operations are used in a restricted way so do not impact the memory model. See; :ref:`amdgpu-amdhsa-memory-spaces`.; * The vector and scalar memory L0 caches use an L1 cache shared by all WGPs on; the same SA. Therefore, no special action is required for coherence between; the wavefronts of a single work-group. However, a ``buffer_gl1_inv`` is; required for coherence between wavefronts executing in different work-groups; as they may be executing on different SAs that access different L1s.; * The L1 caches have independent quadrants to service disjoint ranges of virtual; addresses.; * Each L0 cache has a separate request queue per L1 quadrant. Therefore, the; vector and scalar memory operations performed by different wavefronts, whether; executing in the same or different work-groups (which may be executing on; different CUs accessing different L0s), can be reordered relative to each; other. A ``s_waitcnt vmcnt(0) & vscnt(0)`` is required to ensure; synchronization between vector memory operations of different wavefronts. It; ensures a previous vector memory operation has completed before executing a; subsequent vector memory or LDS operation an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:337633,cache,caches,337633,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['cache'],"['cache', 'caches']"
Performance,"a) + thrice(b);; }; #pragma clang optimize on. In this example, the definition of the template function ``twice`` is outside; the pragma region, whereas the definition of ``thrice`` is inside the region.; The ``container`` function is also in the region and will not be optimized, but; it causes the instantiation of ``twice`` and ``thrice`` with an ``int`` type; of; these two instantiations, ``twice`` will be optimized (because its definition; was outside the region) and ``thrice`` will not be optimized. Clang also implements MSVC's range-based pragma,; ``#pragma optimize(""[optimization-list]"", on | off)``. At the moment, Clang only; supports an empty optimization list, whereas MSVC supports the arguments, ``s``,; ``g``, ``t``, and ``y``. Currently, the implementation of ``pragma optimize`` behaves; the same as ``#pragma clang optimize``. All functions; between ``off`` and ``on`` will be decorated with the ``optnone`` attribute. .. code-block:: c++. #pragma optimize("""", off); // This function will be decorated with optnone.; void f1() {}. #pragma optimize("""", on); // This function will be optimized with whatever was specified on; // the commandline.; void f2() {}. // This will warn with Clang's current implementation.; #pragma optimize(""g"", on); void f3() {}. For MSVC, an empty optimization list and ``off`` parameter will turn off; all optimizations, ``s``, ``g``, ``t``, and ``y``. An empty optimization and; ``on`` parameter will reset the optimizations to the ones specified on the; commandline. .. list-table:: Parameters (unsupported by Clang). * - Parameter; - Type of optimization; * - g; - Deprecated; * - s or t; - Short or fast sequences of machine code; * - y; - Enable frame pointers. Extensions for loop hint optimizations; ======================================. The ``#pragma clang loop`` directive is used to specify hints for optimizing the; subsequent for, while, do-while, or c++11 range-based for loop. The directive; provides options for vectorization, inter",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:161467,optimiz,optimize,161467,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimize']
Performance,"a);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the function. The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once again:. .. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass());; ... It is interesting to see what the code looks like before and after the; mem2reg optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. Here there is only one variable (x, the input argument) but you can; still see the extremely simple-minded code generation strategy we are; using. In the entry block, an alloca is",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:17027,optimiz,optimization,17027,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,1,['optimiz'],['optimization']
Performance,"a);; root[] gGeoManager->SetTopVisible(); // the TOP is invisible; root[] top->Draw();; ```. ### Example 2: A Geometrical Hierarchy Look and Feel. Before going further, let us get a look and feel of interacting with the; modeller. For this, we will use one of the examples illustrating the; geometry package. To get an idea on the geometry structure created in; this example, just look at the link:; <http://root.cern.ch/root/html/tutorials/geom/rootgeom.C.html>. You will; notice that this is a bit more complex that just creating the ""world""; since several other volumes are created and put together in a hierarchy.; The purpose here is just to learn how to interact with a geometry that; is already built, but just few hints on the building steps in this; example might be useful. The geometry here represents the word ROOT that; is replicated in some symmetric manner. You might for instance ask some; questions after having a first look:. ***`Q:`*** ""OK, I understand the first lines that load the libGeom library and create; a geometry manager object. I also recognize from the previous example the following; lines creating some materials and media, but what about the geometrical transformations below?"". ***`A:`*** As explained before, the model that we are trying to create; is a hierarchy of volumes based on ***`containment`***. This is; accomplished by ***`positioning`*** some volumes ***`inside`*** others.; Any volume is an un-positioned object in the sense that it defines only; a ***`local frame`*** (matching the one of its ***`shape`***). In order; to fully define the mother-daughter relationship between two volumes one; has to specify how the daughter will be positioned inside. This is; accomplished by defining a ***`local geometrical transformation`*** of; the daughter with respect to the mother coordinate system. These; transformations will be subsequently used in the example. ***`Q:`*** ""I see the lines defining the top level volume as in the previous example,; but wh",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:5120,load,load,5120,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['load'],['load']
Performance,"a, 0		; <i1> [#uses=1]; 	br i1 %tmp.7, label %then.1, label %return. then.1:		; preds = %else.0; 	%tmp.11 = add i32 %a, -2		; <i32> [#uses=1]; 	%tmp.9 = call i32 @t4( i32 %tmp.11 )		; <i32> [#uses=1]; 	br label %return. return:		; preds = %then.1, %else.0, %then.0; 	%result.0 = phi i32 [ 0, %else.0 ], [ %tmp.3, %then.0 ],; [ %tmp.9, %then.1 ]; 	ret i32 %result.0; }. //===---------------------------------------------------------------------===//. Tail recursion elimination should handle:. int pow2m1(int n) {; if (n == 0); return 0;; return 2 * pow2m1 (n - 1) + 1;; }. Also, multiplies can be turned into SHL's, so they should be handled as if; they were associative. ""return foo() << 1"" can be tail recursion eliminated. //===---------------------------------------------------------------------===//. Argument promotion should promote arguments for recursive functions, like ; this:. ; RUN: llvm-as < %s | opt -argpromotion | llvm-dis | grep x.val. define internal i32 @foo(i32* %x) {; entry:; 	%tmp = load i32* %x		; <i32> [#uses=0]; 	%tmp.foo = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp.foo; }. define i32 @bar(i32* %x) {; entry:; 	%tmp3 = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp3; }. //===---------------------------------------------------------------------===//. We should investigate an instruction sinking pass. Consider this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is l",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:11669,load,load,11669,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['load'],['load']
Performance,"a, especially tbaa metadata, to communicate; otherwise-non-deducible pointer aliasing facts. #. Use inbounds on geps. This can help to disambiguate some aliasing queries. Undefined Values; ^^^^^^^^^^^^^^^^. #. Use poison values instead of undef values whenever possible. #. Tag function parameters with the noundef attribute whenever possible. Modeling Memory Effects; ^^^^^^^^^^^^^^^^^^^^^^^^. #. Mark functions as readnone/readonly/argmemonly or noreturn/nounwind when; known. The optimizer will try to infer these flags, but may not always be; able to. Manual annotations are particularly important for external; functions that the optimizer can not analyze. #. Use the lifetime.start/lifetime.end and invariant.start/invariant.end; intrinsics where possible. Common profitable uses are for stack like data; structures (thus allowing dead store elimination) and for describing; life times of allocas (thus allowing smaller stack sizes). #. Mark invariant locations using !invariant.load and TBAA's constant flags. Pass Ordering; ^^^^^^^^^^^^^. One of the most common mistakes made by new language frontend projects is to; use the existing -O2 or -O3 pass pipelines as is. These pass pipelines make a; good starting point for an optimizing compiler for any language, but they have; been carefully tuned for C and C++, not your target language. You will almost; certainly need to use a custom pass order to achieve optimal performance. A; couple specific suggestions:. #. For languages with numerous rarely executed guard conditions (e.g. null; checks, type checks, range checks) consider adding an extra execution or; two of LoopUnswitch and LICM to your pass order. The standard pass order,; which is tuned for C and C++ applications, may not be sufficient to remove; all dischargeable checks from loops. #. If your language uses range checks, consider using the IRCE pass. It is not; currently part of the standard pass order. #. A useful sanity check to run is to run your optimized IR back throu",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:12303,load,load,12303,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['load'],['load']
Performance,a/clang-tidy/cert/NonTrivialTypesLibcMemoryCallsCheck.h; clang-tools-extra/clang-tidy/cert/PostfixOperatorCheck.cpp; clang-tools-extra/clang-tidy/cert/PostfixOperatorCheck.h; clang-tools-extra/clang-tidy/cert/ProperlySeededRandomGeneratorCheck.cpp; clang-tools-extra/clang-tidy/cert/ProperlySeededRandomGeneratorCheck.h; clang-tools-extra/clang-tidy/cert/SetLongJmpCheck.cpp; clang-tools-extra/clang-tidy/cert/SetLongJmpCheck.h; clang-tools-extra/clang-tidy/cert/StaticObjectExceptionCheck.cpp; clang-tools-extra/clang-tidy/cert/StaticObjectExceptionCheck.h; clang-tools-extra/clang-tidy/cert/StrToNumCheck.cpp; clang-tools-extra/clang-tidy/cert/StrToNumCheck.h; clang-tools-extra/clang-tidy/cert/ThrownExceptionTypeCheck.cpp; clang-tools-extra/clang-tidy/cert/ThrownExceptionTypeCheck.h; clang-tools-extra/clang-tidy/cert/VariadicFunctionDefCheck.cpp; clang-tools-extra/clang-tidy/cert/VariadicFunctionDefCheck.h; clang-tools-extra/clang-tidy/concurrency/MtUnsafeCheck.cpp; clang-tools-extra/clang-tidy/concurrency/MtUnsafeCheck.h; clang-tools-extra/clang-tidy/concurrency/ThreadCanceltypeAsynchronousCheck.cpp; clang-tools-extra/clang-tidy/concurrency/ThreadCanceltypeAsynchronousCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidGotoCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidGotoCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidNonConstGlobalVariablesCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/AvoidNonConstGlobalVariablesCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/CppCoreGuidelinesTidyModule.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/InitVariablesCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/InitVariablesCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/InterfacesGlobalInitCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/InterfacesGlobalInitCheck.h; clang-tools-extra/clang-tidy/cppcoreguidelines/MacroUsageCheck.cpp; clang-tools-extra/clang-tidy/cppcoreguidelines/MacroUsageCheck.h;,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:51265,concurren,concurrency,51265,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['concurren'],['concurrency']
Performance,"a; <range-metadata>` and LLVM can do the sext to zext conversion for you. Zext GEP indices to machine register width; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Internally, LLVM often promotes the width of GEP indices to machine register; width. When it does so, it will default to using sign extension (sext); operations for safety. If your source language provides information about; the range of the index, you may wish to manually extend indices to machine; register width using a zext instruction. When to specify alignment; ^^^^^^^^^^^^^^^^^^^^^^^^^^; LLVM will always generate correct code if you don’t specify alignment, but may; generate inefficient code. For example, if you are targeting MIPS (or older; ARM ISAs) then the hardware does not handle unaligned loads and stores, and; so you will enter a trap-and-emulate path if you do a load or store with; lower-than-natural alignment. To avoid this, LLVM will emit a slower; sequence of loads, shifts and masks (or load-right + load-left on MIPS) for; all cases where the load / store does not have a sufficiently high alignment; in the IR. The alignment is used to guarantee the alignment on allocas and globals,; though in most cases this is unnecessary (most targets have a sufficiently; high default alignment that they’ll be fine). It is also used to provide a; contract to the back end saying ‘either this load/store has this alignment, or; it is undefined behavior’. This means that the back end is free to emit; instructions that rely on that alignment (and mid-level optimizers are free to; perform transforms that require that alignment). For x86, it doesn’t make; much difference, as almost all instructions are alignment-independent. For; MIPS, it can make a big difference. Note that if your loads and stores are atomic, the backend will be unable to; lower an under aligned access into a sequence of natively aligned accesses.; As a result, alignment is mandatory for atomic loads and stores. Other Things to Consider; ^^^^",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:4956,load,loads,4956,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,4,['load'],"['load', 'load-left', 'load-right', 'loads']"
Performance,"a; SREM or UREM operation. The `legalize types`_ and `legalize operations`_ phases; are responsible for turning an illegal DAG into a legal DAG. .. _SelectionDAG-Process:. SelectionDAG Instruction Selection Process; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. SelectionDAG-based instruction selection consists of the following steps:. #. `Build initial DAG`_ --- This stage performs a simple translation from the; input LLVM code to an illegal SelectionDAG. #. `Optimize SelectionDAG`_ --- This stage performs simple optimizations on the; SelectionDAG to simplify it, and recognize meta instructions (like rotates; and ``div``/``rem`` pairs) for targets that support these meta operations.; This makes the resultant code more efficient and the `select instructions; from DAG`_ phase (below) simpler. #. `Legalize SelectionDAG Types`_ --- This stage transforms SelectionDAG nodes; to eliminate any types that are unsupported on the target. #. `Optimize SelectionDAG`_ --- The SelectionDAG optimizer is run to clean up; redundancies exposed by type legalization. #. `Legalize SelectionDAG Ops`_ --- This stage transforms SelectionDAG nodes to; eliminate any operations that are unsupported on the target. #. `Optimize SelectionDAG`_ --- The SelectionDAG optimizer is run to eliminate; inefficiencies introduced by operation legalization. #. `Select instructions from DAG`_ --- Finally, the target instruction selector; matches the DAG operations to target instructions. This process translates; the target-independent input DAG into another DAG of target instructions. #. `SelectionDAG Scheduling and Formation`_ --- The last phase assigns a linear; order to the instructions in the target-instruction DAG and emits them into; the MachineFunction being compiled. This step uses traditional prepass; scheduling techniques. After all of these steps are complete, the SelectionDAG is destroyed and the; rest of the code generation passes are run. One of the most common ways to debug these steps is using `",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:37588,optimiz,optimizer,37588,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['optimiz'],['optimizer']
Performance,"a; compilation. This document will provide a basic walkthrough of how to write and; run a Clang Plugin. Introduction; ============. Clang Plugins run FrontendActions over code. See the :doc:`FrontendAction; tutorial <RAVFrontendAction>` on how to write a ``FrontendAction`` using the; ``RecursiveASTVisitor``. In this tutorial, we'll demonstrate how to write a; simple clang plugin. Writing a ``PluginASTAction``; =============================. The main difference from writing normal ``FrontendActions`` is that you can; handle plugin command line options. The ``PluginASTAction`` base class declares; a ``ParseArgs`` method which you have to implement in your plugin. .. code-block:: c++. bool ParseArgs(const CompilerInstance &CI,; const std::vector<std::string>& args) {; for (unsigned i = 0, e = args.size(); i != e; ++i) {; if (args[i] == ""-some-arg"") {; // Handle the command line argument.; }; }; return true;; }. Registering a plugin; ====================. A plugin is loaded from a dynamic library at runtime by the compiler. To; register a plugin in a library, use ``FrontendPluginRegistry::Add<>``:. .. code-block:: c++. static FrontendPluginRegistry::Add<MyPlugin> X(""my-plugin-name"", ""my plugin description"");. Defining pragmas; ================. Plugins can also define pragmas by declaring a ``PragmaHandler`` and; registering it using ``PragmaHandlerRegistry::Add<>``:. .. code-block:: c++. // Define a pragma handler for #pragma example_pragma; class ExamplePragmaHandler : public PragmaHandler {; public:; ExamplePragmaHandler() : PragmaHandler(""example_pragma"") { }; void HandlePragma(Preprocessor &PP, PragmaIntroducer Introducer,; Token &PragmaTok) {; // Handle the pragma; }; };. static PragmaHandlerRegistry::Add<ExamplePragmaHandler> Y(""example_pragma"",""example pragma description"");. Defining attributes; ===================. Plugins can define attributes by declaring a ``ParsedAttrInfo`` and registering; it using ``ParsedAttrInfoRegister::Add<>``:. .. code-block:: c++. c",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangPlugins.rst:1095,load,loaded,1095,interpreter/llvm-project/clang/docs/ClangPlugins.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangPlugins.rst,1,['load'],['loaded']
Performance,"a; metadata object representing a :doc:`type identifier <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.test`` intrinsic tests whether the given pointer is associated; with the given type identifier. .. _type.checked.load:. '``llvm.type.checked.load``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare {ptr, i1} @llvm.type.checked.load(ptr %ptr, i32 %offset, metadata %type) nounwind memory(argmem: read). Arguments:; """""""""""""""""""". The first argument is a pointer from which to load a function pointer. The; second argument is the byte offset from which to load the function pointer. The; third argument is a metadata object representing a :doc:`type identifier; <TypeMetadata>`. Overview:; """""""""""""""""". The ``llvm.type.checked.load`` intrinsic safely loads a function pointer from a; virtual table pointer using type metadata. This intrinsic is used to implement; control flow integrity in conjunction with virtual call optimization. The; virtual call optimization pass will optimize away ``llvm.type.checked.load``; intrinsics associated with devirtualized calls, thereby removing the type; check in cases where it is not needed to enforce the control flow integrity; constraint. If the given pointer is associated with a type metadata identifier, this; function returns true as the second element of its return value. (Note that; the function may also return true if the given pointer is not associated; with a type metadata identifier.) If the function's return value's second; element is true, the following rules apply to the first element:. - If the given pointer is associated with the given type metadata identifier,; it is the function pointer loaded from the given byte offset from the given; pointer. - If the given pointer is not associated with the given type metadata; identifier, it is one of the following (the choice of which is unspecified):. 1. The function pointer that would have been loaded from an arbitrarily chosen; (through an unspecified ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:938443,optimiz,optimization,938443,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],"['optimization', 'optimize']"
Performance,"a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execution environment. For these reasons, LLVM has been designed from the beginning as a long-term ; solution to these problems. Its design allows the large body of platform ; independent, static, program optimizations currently in compilers to be ; reused unchanged in their current form. It also provides important static ; type information to enable powerful dynamic and link time optimizations ; to be performed quickly and efficiently. This combination enables an ; increase in effective system performance for real world environments.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:2703,optimiz,optimizations,2703,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,4,"['optimiz', 'perform']","['optimizations', 'performance', 'performed']"
Performance,"a>` for details. '``llvm.loop.distribute.followup_sequential``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata defines which attributes the isolated loops with unsafe; memory dependencies will have. See; :ref:`Transformation Metadata <transformation-metadata>` for details. '``llvm.loop.distribute.followup_fallback``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. If loop versioning is necessary, this metadata defined the attributes; the non-distributed fallback version will have. See; :ref:`Transformation Metadata <transformation-metadata>` for details. '``llvm.loop.distribute.followup_all``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The attributes in this metadata is added to all followup loops of the; loop distribution pass. See; :ref:`Transformation Metadata <transformation-metadata>` for details. '``llvm.licm.disable``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. This metadata indicates that loop-invariant code motion (LICM) should not be; performed on this loop. The metadata has a single operand which is the string; ``llvm.licm.disable``. For example:. .. code-block:: llvm. !0 = !{!""llvm.licm.disable""}. Note that although it operates per loop it isn't given the llvm.loop prefix; as it is not affected by the ``llvm.loop.disable_nonforced`` metadata. '``llvm.access.group``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. ``llvm.access.group`` metadata can be attached to any instruction that; potentially accesses memory. It can point to a single distinct metadata; node, which we call access group. This node represents all memory access; instructions referring to it via ``llvm.access.group``. When an; instruction belongs to multiple access groups, it can also point to a; list of accesses groups, illustrated by the following example. .. code-block:: llvm. %val = load i32, ptr %arrayidx, !llvm.access.group !0; ...; !0 = !{!1, !2}; !1 = distinct !{}; !2 = distinct !{}. It is illegal for the list node to",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:309646,perform,performed,309646,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,"able code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via add",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:1958,optimiz,optimized,1958,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimized']
Performance,"able in bitcode form); it simply lowers; every atomic intrinsic. ``lowerinvoke``: Lower invokes to calls, for unwindless code generators; -----------------------------------------------------------------------. This transformation is designed for use by code generators which do not yet; support stack unwinding. This pass converts ``invoke`` instructions to; ``call`` instructions, so that any exception-handling ``landingpad`` blocks; become dead code (which can be removed by running the ``-simplifycfg`` pass; afterwards). ``lowerswitch``: Lower ``SwitchInst``\ s to branches; ----------------------------------------------------. Rewrites switch instructions with a sequence of branches, which allows targets; to get away with not implementing the switch instruction until it is; convenient. .. _passes-mem2reg:. ``mem2reg``: Promote Memory to Register; ---------------------------------------. This file promotes memory references to be register references. It promotes; alloca instructions which only have loads and stores as uses. An ``alloca`` is; transformed by using dominator frontiers to place phi nodes, then traversing; the function in depth-first order to rewrite loads and stores as appropriate.; This is just the standard SSA construction algorithm to construct ""pruned"" SSA; form. ``memcpyopt``: MemCpy Optimization; ----------------------------------. This pass performs various transformations related to eliminating ``memcpy``; calls, or transforming sets of stores into ``memset``\ s. ``mergefunc``: Merge Functions; ------------------------------. This pass looks for equivalent functions that are mergeable and folds them. Total-ordering is introduced among the functions set: we define comparison; that answers for every two functions which of them is greater. It allows to; arrange functions into the binary tree. For every new function we check for equivalent in tree. If equivalent exists we fold such functions. If both functions are overridable,; we move the functionali",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:30584,load,loads,30584,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['load'],['loads']
Performance,"able plotting of component p.d.f.s using same scheme as RooAddPdf, i.e. just use the Components() specified in plotOn().; RooExpensiveObjectCache - New cache manager for sharing and storing of expensive components cached by operator p.d.f.s ; RooMCStudy - Add Silence() argument to constructor to request minimal verbosity during running; RooMinuit - Improve contour() method to return RooPlots rather than drawing TGraphs straight on a canvas; RooWorkspace - Add private expensive object cache to workspace; RooBinningCategory - New real-to-category function that maps values of input RooRealVar to categories with labels that correspond to bins of input RooRealVar. See rf405_realtocatfuncs.C for details . RooStats; This is a new package introduced in this version for statistical tools built on top of RooFit. It is a joint effort between the LHC experiments and the ROOT team (see the RooStats Wiki page).; ; This version contains the interfaces for performing the statistical calculations and dealing with the obtained results and concrete classes implementing the statistical methods.; ; All the classes and functions in RooStats are provided in the namespace RooStats.; ; RooStats interfaces. ConfInterval: interface for describing a confidence interval. ; IntervalCalculator: interface for a statistical tool producing confidence intervals (class ConfInterval).; HypoTestResult: interface for representing results of a hypothesis test; HypoTestCalculator: interface for a statistical tool performing an hypothesis test. ; CombinedCalculator: interface for a statistical tool which can produce both hypothesis test results and confidence intervals. ; RooStats concrete classes. The concrete classes describing statistical tools implementing the above interfaces are:; ; ProfileLikelihoodCalculator: it is an implementation of a CombinedCalculator using the profile likelihood ratio as a test statistics. After configuring the calculator, one only needs to ask GetHypoTest() (which will return ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v522/index.html:9581,perform,performing,9581,roofit/doc/v522/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v522/index.html,2,['perform'],['performing']
Performance,"able, in some cases, to emit such; instructions into the header even if they are not used along a rarely; executed path that exits the loop. This guidance specifically does not; apply if the condition which terminates the loop header is itself invariant,; or can be easily discharged by inspecting the loop index variables. #. In hot loops, consider duplicating instructions from small basic blocks; which end in highly predictable terminators into their successor blocks.; If a hot successor block contains instructions which can be vectorized; with the duplicated ones, this can provide a noticeable throughput; improvement. Note that this is not always profitable and does involve a; potentially large increase in code size. #. When checking a value against a constant, emit the check using a consistent; comparison type. The GVN pass *will* optimize redundant equalities even if; the type of comparison is inverted, but GVN only runs late in the pipeline.; As a result, you may miss the opportunity to run other important; optimizations. #. Avoid using arithmetic intrinsics unless you are *required* by your source; language specification to emit a particular code sequence. The optimizer; is quite good at reasoning about general control flow and arithmetic, it is; not anywhere near as strong at reasoning about the various intrinsics. If; profitable for code generation purposes, the optimizer will likely form the; intrinsics itself late in the optimization pipeline. It is *very* rarely; profitable to emit these directly in the language frontend. This item; explicitly includes the use of the :ref:`overflow intrinsics <int_overflow>`. #. Avoid using the :ref:`assume intrinsic <int_assume>` until you've; established that a) there's no other way to express the given fact and b); that fact is critical for optimization purposes. Assumes are a great; prototyping mechanism, but they can have negative effects on both compile; time and optimization effectiveness. The former is fixable with",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:8744,optimiz,optimizations,8744,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['optimiz'],['optimizations']
Performance,"able-general; .. _type metadata: https://llvm.org/docs/TypeMetadata.html; .. _ByteArrayBuilder: https://llvm.org/docs/doxygen/html/structllvm_1_1ByteArrayBuilder.html. Optimizations; -------------. The scheme as described above is the fully general variant of the scheme.; Most of the time we are able to apply one or more of the following; optimizations to improve binary size or performance. In fact, if you try the above example with the current version of the; compiler, you will probably find that it will not use the described virtual; table layout or machine instructions. Some of the optimizations we are about; to introduce cause the compiler to use a different layout or a different; sequence of machine instructions. Stripping Leading/Trailing Zeros in Bit Vectors; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. If a bit vector contains leading or trailing zeros, we can strip them from; the vector. The compiler will emit code to check if the pointer is in range; of the region covered by ones, and perform the bit vector check using a; truncated version of the bit vector. For example, the bit vectors for our; example class hierarchy will be emitted like this:. .. csv-table:: Bit Vectors for A, B, C; :header: Class, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14. A, , , 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, ,; B, , , , , , , , 1, , , , , , ,; C, , , , , , , , , , , , , 1, ,. Short Inline Bit Vectors; ~~~~~~~~~~~~~~~~~~~~~~~~. If the vector is sufficiently short, we can represent it as an inline constant; on x86. This saves us a few instructions when reading the correct element; of the bit vector. If the bit vector fits in 32 bits, the code looks like this:. .. code-block:: none. dc2: 48 8b 03 mov (%rbx),%rax; dc5: 48 8d 15 14 1e 00 00 lea 0x1e14(%rip),%rdx; dcc: 48 89 c1 mov %rax,%rcx; dcf: 48 29 d1 sub %rdx,%rcx; dd2: 48 c1 c1 3d rol $0x3d,%rcx; dd6: 48 83 f9 03 cmp $0x3,%rcx; dda: 77 2f ja e0b <main+0x9b>; ddc: ba 09 00 00 00 mov $0x9,%edx; de1: 0f a3 ca bt %ecx,%edx; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst:4297,perform,perform,4297,interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrityDesign.rst,1,['perform'],['perform']
Performance,"able[] = {; { ""Costa"", 0x2, 0x1 }, // 0; { ""Carol"", 0x2, 0x6 }, // 1; { ""Ted"", 0x4, 0x4 }, // 2; { ""Alice"", 0x4, 0x5 }, // 3; { ""Bob"", 0x5, 0x3 }, // 4; /* { ""Dale"", 0x2, 0x1 }, // 5 */ // We don't generate this line as `IsNeeded` is 0.; };. const AEntry *lookupATableByValues(uint8_t Val1, uint16_t Val2) {; struct KeyType {; uint8_t Val1;; uint16_t Val2;; };; KeyType Key = { Val1, Val2 };; auto Table = ArrayRef(ATable);; auto Idx = std::lower_bound(Table.begin(), Table.end(), Key,; [](const AEntry &LHS, const KeyType &RHS) {; if (LHS.Val1 < RHS.Val1); return true;; if (LHS.Val1 > RHS.Val1); return false;; if (LHS.Val2 < RHS.Val2); return true;; if (LHS.Val2 > RHS.Val2); return false;; return false;; });. if (Idx == Table.end() ||; Key.Val1 != Idx->Val1 ||; Key.Val2 != Idx->Val2); return nullptr;; return &*Idx;; }; #endif. The table entries in ``ATable`` are sorted in order by ``Val1``, and within; each of those values, by ``Val2``. This allows a binary search of the table,; which is performed in the lookup function by ``std::lower_bound``. The; lookup function returns a reference to the found table entry, or the null; pointer if no entry is found. This example includes a field whose type TableGen cannot deduce. The ``Kind``; field uses the enumerated type ``CEnum`` defined above. To inform TableGen; of the type, the record derived from ``GenericTable`` must include a string field; named ``TypeOf_``\ *field*, where *field* is the name of the field whose type; is required. .. code-block:: text. def CTable : GenericTable {; let FilterClass = ""CEntry"";; let Fields = [""Name"", ""Kind"", ""Encoding""];; string TypeOf_Kind = ""CEnum"";; let PrimaryKey = [""Encoding""];; let PrimaryKeyName = ""lookupCEntryByEncoding"";; }. class CEntry<string name, CEnum kind, int enc> {; string Name = name;; CEnum Kind = kind;; bits<16> Encoding = enc;; }. def : CEntry<""Apple"", CFoo, 10>;; def : CEntry<""Pear"", CBaz, 15>;; def : CEntry<""Apple"", CBar, 13>;. Here is the generated C++ code. .. code-block",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:27397,perform,performed,27397,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,1,['perform'],['performed']
Performance,"ables; When linking a ROOT executable, the setup functions from the sanitiser config library might get ignored, because they are not used in any of our executables.; In `cmake/modules/SetUp{Linux|MacOS}.cmake`, the functions are therefore marked as ""undefined"" for the linker, so it starts copying; them into all ROOT executables.; This way, root.exe, cling, ... can start up with a sane default config. ### Use your own address/leak sanitizer configuration; The default configurations can be overridden using the environment variables `ASAN_OPTIONS` and `LSAN_OPTIONS`. Refer to the; [address sanitizer documentation](https://github.com/google/sanitizers/wiki/AddressSanitizer) or use `ASAN_OPTIONS=help=1` when starting; up a sanitised executable (e.g. `root.exe`). A template for a leak suppression file can be found in `$ROOTSYS/etc/lsan-root.supp`. ## Create your own sanitised executable; ROOT exports a library target called `ROOT::ROOTStaticSanitizerConfig` that can be used to create sanitised executables with ROOT's default; address sanitizer config. Linking against this target will add the above setup functions and also add the address sanitizer flags that; ROOT was built with. It should be sufficient to; - Link all executables against this target; - And have `-fsanitize=address` in the `CXXFLAGS`. ## Use sanitised ROOT libraries from a non-sanitised executable (e.g. `python`); When ROOT libraries are built with sanitizers, the address sanitizer runtime needs to be loaded at startup. However, when calling into ROOT; functions from python, that won't happen, since python is not sanitised. Therefore, the address sanitizer runtime has to be preloaded with; `LD_PRELOAD=<pathToRuntime>:libROOTSanitizerConfig.<so|dylib> pythonX ROOTScript.py`. Preloading the shared sanitizer config as above is optional, but recommended, because it adds leak sanitizer suppressions. On Mac, preloading is theoretically possible, but code signing and many other barriers might make it difficult.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/sanitizer/README.md:2726,load,loaded,2726,core/sanitizer/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/sanitizer/README.md,1,['load'],['loaded']
Performance,"accepts executable lines.; Example::. >>> cppyy.cppexec(r""""""std::string hello = ""Hello, World!"";""""""); True; >>> cppyy.cppexec(""std::cout << hello << std::endl;""); Hello, World!; True; >>> . * ``include``: load declarations into the interpreter.; This function accepts C++ declarations from a file, typically a header.; Files are located through include paths given to the Cling.; Example::. >>> cppyy.include(""vector"") # equivalent to ""#include <vector>""; True; >>> . * ``c_include``: load declarations into the interpreter.; This function accepts C++ declarations from a file, typically a header.; Name mangling is an important difference between C and C++ code.; The use of ``c_include`` instead of ``include`` prevents mangling. * ``load_library``: load compiled C++ into the interpreter.; This function takes the name of a shared library and loads it into current; process, exposing all external symbols to Cling.; Libraries are located through load paths given to Cling, either through the; ""-L"" compiler flag or the dynamic search path environment variable (system; dependent).; Any method that brings symbols into the process (including normal linking,; e.g. when embedding Python in a C++ application) is suitable to expose; symbols.; An alternative for ``load_library`` is for example ``ctypes.CDLL``, but; that function does not respect dynamic load paths on all platforms. If a compilation error occurs during JITing of C++ code in any of the above; helpers, a Python ``SyntaxError`` exception is raised.; If a compilation warning occurs, a Python warning is issued. `Configuring Cling`; -------------------. It is often convenient to add additional search paths for Cling to find; headers and libraries when loading a module (Python does not have standard; locations to place headers and libraries, but their locations can usually; be inferred from the location of the module, i.e. it's ``__file__``; attribute).; cppyy provides the following two helpers:. * ``add_include_path``: add add",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst:2785,load,load,2785,bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/toplevel.rst,1,['load'],['load']
Performance,"accessed as MTYPE CC (cache coherent) and; so the L2 cache will be coherent with the CPU and other agents. Scratch backing memory (which is used for the private address space) is accessed; with MTYPE NC (non-coherent). Since the private address space is only accessed; by a single thread, and is always write-before-read, there is never a need to; invalidate these entries from the L0 or L1 caches. Wavefronts are executed in native mode with in-order reporting of loads and; sample instructions. In this mode vmcnt reports completion of load, atomic with; return and sample instructions in order, and the vscnt reports the completion of; store and atomic without return in order. See ``MEM_ORDERED`` field in; :ref:`amdgpu-amdhsa-compute_pgm_rsrc1-gfx6-gfx12-table`. Wavefronts can be executed in WGP or CU wavefront execution mode:. * In WGP wavefront execution mode the wavefronts of a work-group are executed; on the SIMDs of both CUs of the WGP. Therefore, explicit management of the per; CU L0 caches is required for work-group synchronization. Also accesses to L1; at work-group scope need to be explicitly ordered as the accesses from; different CUs are not ordered.; * In CU wavefront execution mode the wavefronts of a work-group are executed on; the SIMDs of a single CU of the WGP. Therefore, all global memory access by; the work-group access the same L0 which in turn ensures L1 accesses are; ordered and so do not require explicit management of the caches for; work-group synchronization. See ``WGP_MODE`` field in; :ref:`amdgpu-amdhsa-compute_pgm_rsrc1-gfx6-gfx12-table` and; :ref:`amdgpu-target-features`. The code sequences used to implement the memory model for GFX10-GFX11 are defined in; table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx10-gfx11-table`. .. table:: AMDHSA Memory Model Code Sequences GFX10-GFX11; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx10-gfx11-table. ============ ============ ============== ========== ================================; LLVM",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:342419,cache,caches,342419,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"accessed as MTYPE CC (cache coherent) and; so the L2 cache will be coherent with the CPU and other agents. Scratch backing memory (which is used for the private address space) is accessed; with MTYPE NC_NV (non-coherent non-volatile). Since the private address space is; only accessed by a single thread, and is always write-before-read, there is; never a need to invalidate these entries from the L1 cache. Hence all cache; invalidates are done as ``*_vol`` to only invalidate the volatile cache lines. The code sequences used to implement the memory model for GFX90A are defined; in table :ref:`amdgpu-amdhsa-memory-model-code-sequences-gfx90a-table`. .. table:: AMDHSA Memory Model Code Sequences GFX90A; :name: amdgpu-amdhsa-memory-model-code-sequences-gfx90a-table. ============ ============ ============== ========== ================================; LLVM Instr LLVM Memory LLVM Memory AMDGPU AMDGPU Machine Code; Ordering Sync Scope Address GFX90A; Space; ============ ============ ============== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; glc=1 slc=1. - volatile. 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1. - volatile. 1. buffer/global/flat_store; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; ad",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:242506,load,load,242506,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"accesses to the masked-off lanes. The masked-off lanes in the result vector are taken from the corresponding lanes of the '``passthru``' operand. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is the alignment of the source location. It must be a power of two constant integer value. The third operand, mask, is a vector of boolean values with the same number of elements as the return type. The fourth is a pass-through value that is used to fill the masked-off lanes of the result. The return type, underlying type of the base pointer and the type of the '``passthru``' operand are the same vector types. Semantics:; """""""""""""""""""". The '``llvm.masked.load``' intrinsic is designed for conditional reading of selected vector elements in a single IR operation. It is useful for targets that support vector masked loads and allows vectorizing predicated basic blocks on these targets. Other targets may support this intrinsic differently, for example by lowering it into a sequence of branches that guard scalar load operations.; The result of this operation is equivalent to a regular vector load instruction followed by a 'select' between the loaded and the passthru values, predicated on the same mask. However, using this intrinsic prevents exceptions on memory access to masked-off lanes. ::. %res = call <16 x float> @llvm.masked.load.v16f32.p0(ptr %ptr, i32 4, <16 x i1>%mask, <16 x float> %passthru). ;; The result of the two following instructions is identical aside from potential memory access exception; %loadlal = load <16 x float>, ptr %ptr, align 4; %res = select <16 x i1> %mask, <16 x float> %loadlal, <16 x float> %passthru. .. _int_mstore:. '``llvm.masked.store.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. The data stored in memory is a vector of any integer, floating-point or pointer data type. ::. declare void @llvm.masked.store.v8i32.p0 (<8 x i32> <value>, ptr <ptr>, i3",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:844939,load,load,844939,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"according to the; following rules.; - s_waitcnt vmcnt(0); must happen after any; preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after any; preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_store; sc0=1 sc1=1; atomicrmw release - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic sc0=1; atomicrmw release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - agent - global 1. buffer_wbl2 sc1=1; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at agent scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:310283,load,load,310283,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ace the standard `ClassImp` macro by `ClassImpQ`. Signals are currently implemented for all ROOT GUI classes and the [TTimer](https://root.cern/doc/master/classTTimer.html) and [TCanvas](https://root.cern/doc/master/classTCanvas.html) classes (to find quickly all defined signals do for example: `grep '*SIGNAL*' $ROOTSYS/include/*.h`). ## Examples. ### A First Time Example ([rqfirst.C](http://root.cern.ch/root/rqex/rqfirst.C)). This example shows:. * How to create interpreted class with signals with different types/number of arguments.; * How to connect signals to slots.; * How to activate signals. ### Histogram Filling with Dynamic State Reported via Signals ([rqsimple.C](http://root.cern.ch/root/rqex/rqsimple.C)). Based on hsimple this example demonstrates:. * All features of the hsimple example.; * How to create an interpreted class with signals which will report about dynamic state of the histogram processing.; * How to use the [TTimer](https://root.cern/doc/master/classTTimer.html) class for emulation of ""multithreading"".; * How to use signals for the concurrent update of pad, file, benchmark facility, etc. ### An Example on How to Use Canvas Event Signals ([rqfiller.C](http://root.cern.ch/root/rqex/rqfiller.C)). This example shows:. * How the object communication mechanism can be used for handling the [TCanvas](https://root.cern/doc/master/classTCanvas.html)'s mouse/key events in an interpreted class. With this demo you can fill histograms by hand:. * Click the left button or move mouse with button pressed to fill histograms.; * Use the right button of the mouse to reset the histograms. ### Complex GUI Using Signals and Slots ([guitest.C](https://root.cern.ch/doc/master/guitest_8C.html)). Based on `$ROOTSYS/test/guitest.cxx` this example demonstrates:. * All features of the original compiled guitest.cxx program.; * Sophisticated use of signals and slots to build a complete user interface that can be executed either in the interpreter or as a compiled program.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/SignalSlot.md:8215,concurren,concurrent,8215,documentation/users-guide/SignalSlot.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/SignalSlot.md,1,['concurren'],['concurrent']
Performance,"ach Recipe for generating its instructions at; VPlan execution time, will instead take part in the planning process by modeling; them as VPInstructions. Finally, only logic that applies to instructions as a; group will remain in Recipes, such as interleave groups and potentially other; idiom groups having synergistic cost. Related LLVM components; -----------------------; 1. SLP Vectorizer: one can compare the VPlan model with LLVM's existing SLP; tree, where TSLP [3]_ adds Plan Step 2.b. 2. RegionInfo: one can compare VPlan's H-CFG with the Region Analysis as used by; Polly [7]_. 3. Loop Vectorizer: the Vectorization Plan aims to upgrade the infrastructure of; the Loop Vectorizer and extend it to handle outer loops [8]_, [9]_. References; ----------; .. [1] ""Outer-loop vectorization: revisited for short SIMD architectures"", Dorit; Nuzman and Ayal Zaks, PACT 2008. .. [2] ""Proposal for function vectorization and loop vectorization with function; calls"", Xinmin Tian, [`cfe-dev; <http://lists.llvm.org/pipermail/cfe-dev/2016-March/047732.html>`_].,; March 2, 2016.; See also `review <https://reviews.llvm.org/D22792>`_. .. [3] ""Throttling Automatic Vectorization: When Less is More"", Vasileios; Porpodas and Tim Jones, PACT 2015 and LLVM Developers' Meeting 2015. .. [4] ""Exploiting mixed SIMD parallelism by reducing data reorganization; overhead"", Hao Zhou and Jingling Xue, CGO 2016. .. [5] ""Register Allocation via Hierarchical Graph Coloring"", David Callahan and; Brian Koblenz, PLDI 1991. .. [6] ""Structural analysis: A new approach to flow analysis in optimizing; compilers"", M. Sharir, Journal of Computer Languages, Jan. 1980. .. [7] ""Enabling Polyhedral Optimizations in LLVM"", Tobias Grosser, Diploma; thesis, 2011. .. [8] ""Introducing VPlan to the Loop Vectorizer"", Gil Rapaport and Ayal Zaks,; European LLVM Developers' Meeting 2017. .. [9] ""Extending LoopVectorizer: OpenMP4.5 SIMD and Outer Loop; Auto-Vectorization"", Intel Vectorizer Team, LLVM Developers' Meeting 2016.; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/VectorizationPlan.rst:11470,optimiz,optimizing,11470,interpreter/llvm-project/llvm/docs/VectorizationPlan.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/VectorizationPlan.rst,1,['optimiz'],['optimizing']
Performance,"ach location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion rout",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:4160,optimiz,optimized,4160,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,1,['optimiz'],['optimized']
Performance,"achine Code; Ordering Sync Scope Address GFX90A; Space; ============ ============ ============== ========== ================================; **Non-Atomic**; ------------------------------------------------------------------------------------; load *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_load; - constant; - !volatile & nontemporal. 1. buffer/global/flat_load; glc=1 slc=1. - volatile. 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. load *none* *none* - local 1. ds_load; store *none* *none* - global - !volatile & !nontemporal; - generic; - private 1. buffer/global/flat_store; - constant; - !volatile & nontemporal. 1. buffer/global/flat_store; glc=1 slc=1. - volatile. 1. buffer/global/flat_store; 2. s_waitcnt vmcnt(0). - Must happen before; any following volatile; global/generic; load/store.; - Ensures that; volatile; operations to; different; addresses will not; be reordered by; hardware. store *none* *none* - local 1. ds_store; **Unordered Atomic**; ------------------------------------------------------------------------------------; load atomic unordered *any* *any* *Same as non-atomic*.; store atomic unordered *any* *any* *Same as non-atomic*.; atomicrmw unordered *any* *any* *Same as monotonic atomic*.; **Monotonic Atomic**; ------------------------------------------------------------------------------------; load atomic monotonic - singlethread - global 1. buffer/global/flat_load; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic glc=1. - If not TgSplit execution; mode, omit glc=1. load atomic monotonic - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; - workgroup be used.*. 1. ds_load; load atomic monotonic - agent - global 1. buffer/",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:243288,load,load,243288,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"actually; needed. That is, it is a pruned SSA form, like LLVM's SSA form. For; example, consider:. .. code-block:: llvm. define void @foo() {; entry:; %p1 = alloca i8; %p2 = alloca i8; %p3 = alloca i8; ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %p3; br label %while.cond. while.cond:; ; 3 = MemoryPhi({%0,1},{if.end,2}); br i1 undef, label %if.then, label %if.else. if.then:; br label %if.end. if.else:; br label %if.end. if.end:; ; MemoryUse(1); %1 = load i8, ptr %p1; ; 2 = MemoryDef(3); store i8 2, ptr %p2; ; MemoryUse(1); %2 = load i8, ptr %p3; br label %while.cond; }. Because we removed the stores from ``if.then`` and ``if.else``, a ``MemoryPhi``; for ``if.end`` would be pointless, so we don't place one. So, if you need to; place a ``MemoryDef`` in ``if.then`` or ``if.else``, you'll need to also create; a ``MemoryPhi`` for ``if.end``. If it turns out that this is a large burden, we can just place ``MemoryPhi``\ s; everywhere. Because we have Walkers that are capable of optimizing above said; phis, doing so shouldn't prohibit optimizations. Non-Goals; ---------. ``MemorySSA`` is meant to reason about the relation between memory; operations, and enable quicker querying.; It isn't meant to be the single source of truth for all potential memory-related; optimizations. Specifically, care must be taken when trying to use ``MemorySSA``; to reason about atomic or volatile operations, as in:. .. code-block:: llvm. define i8 @foo(ptr %a) {; entry:; br i1 undef, label %if.then, label %if.end. if.then:; ; 1 = MemoryDef(liveOnEntry); %0 = load volatile i8, ptr %a; br label %if.end. if.end:; %av = phi i8 [0, %entry], [%0, %if.then]; ret i8 %av; }. Going solely by ``MemorySSA``'s analysis, hoisting the ``load`` to ``entry`` may; seem legal. Because it's a volatile load, though, it's not. Design tradeoffs; ----------------. Precision; ^^^^^^^^^. ``MemorySSA`` in LLVM deliberately trades off precision for speed.; Let us think about memory variables as if they were disjoint partit",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:15746,optimiz,optimizing,15746,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,2,['optimiz'],"['optimizations', 'optimizing']"
Performance,"ad atomic acquire - system - generic 1. flat_load sc0=1 sc1=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; the following; buffer_inv.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; following buffer_inv.; - Ensures the atomicrmw; has completed; before invalidating; the cache. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data r",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:299348,cache,cache,299348,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['cache']
Performance,"ad can safely read @glb; store ptr null, ptr @glb; ret void; }. 3. The call's behavior depends on any bit of the pointer carrying information. .. code-block:: llvm. @glb = global i8 0. define void @f(ptr %a) {; %c = icmp eq ptr %a, @glb; br i1 %c, label %BB_EXIT, label %BB_CONTINUE ; escapes %a; BB_EXIT:; call void @exit(); unreachable; BB_CONTINUE:; ret void; }. 4. The pointer is used in a volatile access as its address. .. _volatile:. Volatile Memory Accesses; ------------------------. Certain memory accesses, such as :ref:`load <i_load>`'s,; :ref:`store <i_store>`'s, and :ref:`llvm.memcpy <int_memcpy>`'s may be; marked ``volatile``. The optimizers must not change the number of; volatile operations or change their order of execution relative to other; volatile operations. The optimizers *may* change the order of volatile; operations relative to non-volatile operations. This is not Java's; ""volatile"" and has no cross-thread synchronization behavior. A volatile load or store may have additional target-specific semantics.; Any volatile operation can have side effects, and any volatile operation; can read and/or modify state which is not accessible via a regular load; or store in this module. Volatile operations may use addresses which do; not point to memory (like MMIO registers). This means the compiler may; not use a volatile operation to prove a non-volatile access to that; address has defined behavior. The allowed side-effects for volatile accesses are limited. If a; non-volatile store to a given address would be legal, a volatile; operation may modify the memory at that address. A volatile operation; may not modify any other memory accessible by the module being compiled.; A volatile operation may not call any code in the current module. In general (without target specific context), the address space of a; volatile operation may not be changed. Different address spaces may; have different trapping behavior when dereferencing an invalid; pointer. The compiler may",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:146313,load,load,146313,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"ad casts are not in themselves control flow integrity violations, but they; can also create security vulnerabilities, and the implementation uses many; of the same mechanisms. There are two types of bad cast that may be forbidden: bad casts; from a base class to a derived class (which can be checked with; ``-fsanitize=cfi-derived-cast``), and bad casts from a pointer of; type ``void*`` or another unrelated type (which can be checked with; ``-fsanitize=cfi-unrelated-cast``). The difference between these two types of casts is that the first is defined; by the C++ standard to produce an undefined value, while the second is not; in itself undefined behavior (it is well defined to cast the pointer back; to its original type) unless the object is uninitialized and the cast is a; ``static_cast`` (see C++14 [basic.life]p5). If a program as a matter of policy forbids the second type of cast, that; restriction can normally be enforced. However it may in some cases be necessary; for a function to perform a forbidden cast to conform with an external API; (e.g. the ``allocate`` member function of a standard library allocator). Such; functions may be :ref:`ignored <cfi-ignorelist>`. For this scheme to work, all translation units containing the definition; of a virtual member function (whether inline or not), other than members; of :ref:`ignored <cfi-ignorelist>` types or types with public :doc:`LTO; visibility <LTOVisibility>`, must be compiled with ``-flto`` or ``-flto=thin``; enabled and be statically linked into the program. Non-Virtual Member Function Call Checking; =========================================. This scheme checks that non-virtual calls take place using an object of; the correct dynamic type; that is, the dynamic type of the called object; must be a derived class of the static type of the object used to make the; call. The checks are currently only introduced where the object is of a; polymorphic class type. This CFI scheme can be enabled on its own using; ``-fsan",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst:6507,perform,perform,6507,interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ControlFlowIntegrity.rst,1,['perform'],['perform']
Performance,"ad"" state. We could read that value from memory; as part of the conditional move instruction, however, this creates more; micro-ops and requires the load-store unit to be involved. Currently, we place; the value into a virtual register and allow the register allocator to decide; when the register pressure is sufficient to make it worth spilling to memory; and reloading. #### Hardening Loads. Once we have the predicate accumulated into a special value for correct vs.; misspeculated, we need to apply this to loads in a way that ensures they do not; leak secret data. There are two primary techniques for this: we can either; harden the loaded value to prevent observation, or we can harden the address; itself to prevent the load from occurring. These have significantly different; performance tradeoffs. ##### Hardening loaded values. The most appealing way to harden loads is to mask out all of the bits loaded.; The key requirement is that for each bit loaded, along the misspeculated path; that bit is always fixed at either 0 or 1 regardless of the value of the bit; loaded. The most obvious implementation uses either an `and` instruction with; an all-zero mask along misspeculated paths and an all-one mask along correct; paths, or an `or` instruction with an all-one mask along misspeculated paths; and an all-zero mask along correct paths. Other options become less appealing; such as multiplying by zero, or multiple shift instructions. For reasons we; elaborate on below, we end up suggesting you use `or` with an all-ones mask,; making the x86 instruction sequence look like the following:; ```; ... .LBB0_4: # %danger; cmovneq %r8, %rax # Conditionally update predicate state.; movl (%rsi), %edi # Load potentially secret data from %rsi.; orl %eax, %edi; ```. Other useful patterns may be to fold the load into the `or` instruction itself; at the cost of a register-to-register copy. There are some challenges with deploying this approach:; 1. Many loads on x86 are folded into other",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:22861,load,loaded,22861,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,2,['load'],['loaded']
Performance,"ad/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_inv and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_inv sc0=1. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - global 1. buffer/global_atomic; sc1=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_inv sc0=1 sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale MTYPE NC global data.; MTYPE RW and CC memory will; never be stale due to the; memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:300471,load,loads,300471,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['loads']
Performance,"ad/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - system - global 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. load atomic acquire - agent - generic 1. flat_load glc=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. load atomic acquire - system - generic 1. flat_load glc=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:248061,load,load,248061,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ad/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local; atomicrmw value; being acquired. atomicrmw acquire - workgroup - generic 1. flat_atomic; 2. s_waitcnt lgkm/vmcnt(0). - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit lgkmcnt(0).; - Must happen before; the following; buffer_wbinvl1_vol and; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local; atomicrmw value; being acquired. 3. buffer_wbinvl1_vol. - If not TgSplit execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acquire - agent - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - system - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acquire - agent - generic 1. flat_atomic; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 3. buffer_wbinvl1_vol. - Must happen before; any followin",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:250719,load,load,250719,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ad/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after any; preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; store.; - Ensures that all; memory operations; to memory and the L2; writeback have; completed before; performing the; store that is being; released. 3. buffer/global/flat_store; atomicrmw release - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw release - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - workgroup - global 1. s_waitcnt lgkm/vmcnt(0); - generic; - Use lgkmcnt(0) if not; TgSplit execution mode; and vmcnt(0) if TgSplit; execution mode.; - If OpenCL, omit; lgkmcnt(0).; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic; atomicrmw release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:260279,load,load,260279,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"ad; - wavefront - generic; load atomic monotonic - workgroup - global 1. buffer/global/flat_load; - generic glc=1. - If CU wavefront execution; mode, omit glc=1. load atomic monotonic - singlethread - local 1. ds_load; - wavefront; - workgroup; load atomic monotonic - agent - global 1. buffer/global/flat_load; - system - generic glc=1 dlc=1. - If GFX11, omit dlc=1. store atomic monotonic - singlethread - global 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load glc=1. - If CU wavefront execution; mode, omit glc=1. 2. s_waitcnt vmcnt(0). - If CU wavefront execution; mode, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - local 1. ds_load; 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following buffer_gl0_inv; and before any following; global/generic load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 3. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Ensures that; following; loads will not see; stale data. load atomic acquire - workgroup - generic 1. flat_load glc=",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:346211,load,load,346211,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"adata format is very simple. ``!tbaa.struct`` metadata; nodes are a list of operands which are in conceptual groups of three.; For each group of three, the first operand gives the byte offset of a; field in bytes, the second gives its size in bytes, and the third gives; its tbaa tag. e.g.:. .. code-block:: llvm. !4 = !{ i64 0, i64 4, !1, i64 8, i64 4, !2 }. This describes a struct with two fields. The first is at offset 0 bytes; with size 4 bytes, and has tbaa tag !1. The second is at offset 8 bytes; and has size 4 bytes and has tbaa tag !2. Note that the fields need not be contiguous. In this example, there is a; 4 byte gap between the two fields. This gap represents padding which; does not carry useful data and need not be preserved. '``noalias``' and '``alias.scope``' Metadata; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. ``noalias`` and ``alias.scope`` metadata provide the ability to specify generic; noalias memory-access sets. This means that some collection of memory access; instructions (loads, stores, memory-accessing calls, etc.) that carry; ``noalias`` metadata can specifically be specified not to alias with some other; collection of memory access instructions that carry ``alias.scope`` metadata.; Each type of metadata specifies a list of scopes where each scope has an id and; a domain. When evaluating an aliasing query, if for some domain, the set; of scopes with that domain in one instruction's ``alias.scope`` list is a; subset of (or equal to) the set of scopes for that domain in another; instruction's ``noalias`` list, then the two memory accesses are assumed not to; alias. Because scopes in one domain don't affect scopes in other domains, separate; domains can be used to compose multiple independent noalias sets. This is; used for example during inlining. As the noalias function parameters are; turned into noalias scope metadata, a new domain is used every time the; function is inlined. The metadata identifying each domain is itself a list containing",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:281161,load,loads,281161,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,add_llvm_component_library(LLVMGlobalISel; CSEInfo.cpp; GISelKnownBits.cpp; CSEMIRBuilder.cpp; CallLowering.cpp; GlobalISel.cpp; Combiner.cpp; CombinerHelper.cpp; GIMatchTableExecutor.cpp; GISelChangeObserver.cpp; IRTranslator.cpp; InlineAsmLowering.cpp; InstructionSelect.cpp; InstructionSelector.cpp; LegalityPredicates.cpp; LegalizeMutations.cpp; Legalizer.cpp; LegalizerHelper.cpp; LegalizerInfo.cpp; LegacyLegalizerInfo.cpp; LoadStoreOpt.cpp; Localizer.cpp; LostDebugLocObserver.cpp; MachineIRBuilder.cpp; RegBankSelect.cpp; Utils.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/CodeGen/GlobalISel. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; MC; SelectionDAG; Support; Target; TransformUtils; ); ,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CMakeLists.txt:430,Load,LoadStoreOpt,430,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CMakeLists.txt,1,['Load'],['LoadStoreOpt']
Performance,add_llvm_component_library(LLVMVectorize; LoadStoreVectorizer.cpp; LoopVectorizationLegality.cpp; LoopVectorize.cpp; SLPVectorizer.cpp; Vectorize.cpp; VectorCombine.cpp; VPlan.cpp; VPlanAnalysis.cpp; VPlanHCFGBuilder.cpp; VPlanRecipes.cpp; VPlanSLP.cpp; VPlanTransforms.cpp; VPlanVerifier.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/Vectorize. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; Support; TransformUtils; ); ,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/CMakeLists.txt:42,Load,LoadStoreVectorizer,42,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/CMakeLists.txt,1,['Load'],['LoadStoreVectorizer']
Performance,"additional; :ref:`optimization rules <arc.optimization>` which permit the removal or; optimization of operations based on local knowledge of data flow. The; semantics describe the high-level behaviors that the compiler implements, not; an exact sequence of operations that a program will be compiled into. .. _arc.objects.operands:. Retainable object pointers as operands and arguments; ----------------------------------------------------. In general, ARC does not perform retain or release operations when simply using; a retainable object pointer as an operand within an expression. This includes:. * loading a retainable pointer from an object with non-weak :ref:`ownership; <arc.ownership>`,; * passing a retainable pointer as an argument to a function or method, and; * receiving a retainable pointer as the result of a function or method call. .. admonition:: Rationale. While this might seem uncontroversial, it is actually unsafe when multiple; expressions are evaluated in ""parallel"", as with binary operators and calls,; because (for example) one expression might load from an object while another; writes to it. However, C and C++ already call this undefined behavior; because the evaluations are unsequenced, and ARC simply exploits that here to; avoid needing to retain arguments across a large number of calls. The remainder of this section describes exceptions to these rules, how those; exceptions are detected, and what those exceptions imply semantically. .. _arc.objects.operands.consumed:. Consumed parameters; ^^^^^^^^^^^^^^^^^^^. A function or method parameter of retainable object pointer type may be marked; as :arc-term:`consumed`, signifying that the callee expects to take ownership; of a +1 retain count. This is done by adding the ``ns_consumed`` attribute to; the parameter declaration, like so:. .. code-block:: objc. void foo(__attribute((ns_consumed)) id x);; - (void) foo: (id) __attribute((ns_consumed)) x;. This attribute is part of the type of the function or me",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:16123,load,load,16123,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['load'],['load']
Performance,"additionally provides GCC-compatible ``__atomic_*``; builtins and OpenCL 2.0 ``__opencl_atomic_*`` builtins. The OpenCL 2.0; atomic builtins are an explicit form of the corresponding OpenCL 2.0; builtin function, and are named with a ``__opencl_`` prefix. The macros; ``__OPENCL_MEMORY_SCOPE_WORK_ITEM``, ``__OPENCL_MEMORY_SCOPE_WORK_GROUP``,; ``__OPENCL_MEMORY_SCOPE_DEVICE``, ``__OPENCL_MEMORY_SCOPE_ALL_SVM_DEVICES``,; and ``__OPENCL_MEMORY_SCOPE_SUB_GROUP`` are provided, with values; corresponding to the enumerators of OpenCL's ``memory_scope`` enumeration.). __scoped_atomic builtins; ------------------------. Clang provides a set of atomics taking a memory scope argument. These atomics; are identical to the standard GNU / GCC atomic builtins but taking an extra; memory scope argument. These are designed to be a generic alternative to the; ``__opencl_atomic_*`` builtin functions for targets that support atomic memory; scopes. Atomic memory scopes are designed to assist optimizations for systems with; several levels of memory hierarchy like GPUs. The following memory scopes are; currently supported:. * ``__MEMORY_SCOPE_SYSTEM``; * ``__MEMORY_SCOPE_DEVICE``; * ``__MEMORY_SCOPE_WRKGRP``; * ``__MEMORY_SCOPE_WVFRNT``; * ``__MEMORY_SCOPE_SINGLE``. This controls whether or not the atomic operation is ordered with respect to the; whole system, the current device, an OpenCL workgroup, wavefront, or just a; single thread. If these are used on a target that does not support atomic; scopes, then they will behave exactly as the standard GNU atomic builtins. Low-level ARM exclusive memory builtins; ---------------------------------------. Clang provides overloaded builtins giving direct access to the three key ARM; instructions for implementing atomic operations. .. code-block:: c. T __builtin_arm_ldrex(const volatile T *addr);; T __builtin_arm_ldaex(const volatile T *addr);; int __builtin_arm_strex(T val, volatile T *addr);; int __builtin_arm_stlex(T val, volatile T *addr);; voi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:142824,optimiz,optimizations,142824,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimizations']
Performance,"address space. Access of the high bytes can interfere with debugging tools, so; should not be accessed if the function has the ``sanitize_thread`` or; ``sanitize_address`` attributes. The alignment is only optional when parsing textual IR; for in-memory IR, it is; always present. An omitted ``align`` argument means that the operation has the; ABI alignment for the target. The optional ``!nontemporal`` metadata must reference a single; metadata name ``<nontemp_node>`` corresponding to a metadata node with one; ``i32`` entry of value 1. The existence of the ``!nontemporal``; metadata on the instruction tells the optimizer and code generator; that this load is not expected to be reused in the cache. The code; generator may select special instructions to save cache bandwidth, such; as the ``MOVNT`` instruction on x86. The optional ``!invariant.load`` metadata must reference a single; metadata name ``<empty_node>`` corresponding to a metadata node with no; entries. If a load instruction tagged with the ``!invariant.load``; metadata is executed, the memory location referenced by the load has; to contain the same value at all points in the program where the; memory location is dereferenceable; otherwise, the behavior is; undefined. The optional ``!invariant.group`` metadata must reference a single metadata name; ``<empty_node>`` corresponding to a metadata node with no entries.; See ``invariant.group`` metadata :ref:`invariant.group <md_invariant.group>`. The optional ``!nonnull`` metadata must reference a single; metadata name ``<empty_node>`` corresponding to a metadata node with no; entries. The existence of the ``!nonnull`` metadata on the; instruction tells the optimizer that the value loaded is known to; never be null. If the value is null at runtime, a poison value is returned; instead. This is analogous to the ``nonnull`` attribute on parameters and; return values. This metadata can only be applied to loads of a pointer type. The optional ``!dereferenceable`` metad",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:415450,load,load,415450,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"aded with an ``LDR`` instruction.; 2. The content of a vector register is the same *as if* it had been loaded with an ``LD1`` instruction. Because ``LD1 == LDR + REV`` and similarly ``LDR == LD1 + REV`` (on a big endian system), we can simulate either type of load with the other type of load plus a ``REV`` instruction. So we're not deciding which instructions to use, but which format to use (which will then influence which instruction is best to use). .. The 'clearer' container is required to make the following section header come after the floated; images above.; .. container:: clearer. Note that throughout this section we only mention loads. Stores have exactly the same problems as their associated loads, so have been skipped for brevity. Considerations; ==============. LLVM IR Lane ordering; ---------------------. LLVM IR has first class vector types. In LLVM IR, the zero'th element of a vector resides at the lowest memory address. The optimizer relies on this property in certain areas, for example when concatenating vectors together. The intention is for arrays and vectors to have identical memory layouts - ``[4 x i8]`` and ``<4 x i8>`` should be represented the same in memory. Without this property there would be many special cases that the optimizer would have to cleverly handle. Use of ``LDR`` would break this lane ordering property. This doesn't preclude the use of ``LDR``, but we would have to do one of two things:. 1. Insert a ``REV`` instruction to reverse the lane order after every ``LDR``.; 2. Disable all optimizations that rely on lane layout, and for every access to an individual lane (``insertelement``/``extractelement``/``shufflevector``) reverse the lane index. AAPCS; -----. The ARM procedure call standard (AAPCS) defines the ABI for passing vectors between functions in registers. It states:. When a short vector is transferred between registers and memory it is treated as an opaque object. That is a short vector is stored in memory as if it were sto",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst:4769,optimiz,optimizer,4769,interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/BigEndianNEON.rst,1,['optimiz'],['optimizer']
Performance,"aded with the :option:`-load` option. Use; the :option:`-help` option to determine what optimizations you can use. If ``filename`` is omitted from the command line or is ""``-``"", :program:`opt`; reads its input from standard input. Inputs can be in either the LLVM assembly; language format (``.ll``) or the LLVM bitcode format (``.bc``). If an output filename is not specified with the :option:`-o` option,; :program:`opt` writes its output to the standard output. OPTIONS; -------. .. option:: -f. Enable binary output on terminals. Normally, :program:`opt` will refuse to; write raw bitcode output if the output stream is a terminal. With this option,; :program:`opt` will write raw bitcode regardless of the output device. .. option:: -help. Print a summary of command line options. .. option:: -o <filename>. Specify the output filename. .. option:: -S. Write output in LLVM intermediate language (instead of bitcode). .. option:: -{passname}. :program:`opt` provides the ability to run any of LLVM's optimization or; analysis passes in any order. The :option:`-help` option lists all the passes; available. The order in which the options occur on the command line are the; order in which they are executed (within pass constraints). .. option:: -strip-debug. This option causes opt to strip debug information from the module before; applying other optimizations. It is essentially the same as `-strip`; but it ensures that stripping of debug information is done first. .. option:: -verify-each. This option causes opt to add a verify pass after every pass otherwise; specified on the command line (including `-verify`). This is useful; for cases where it is suspected that a pass is creating an invalid module but; it is not clear which pass is doing it. .. option:: -stats. Print statistics. .. option:: -time-passes. Record the amount of time needed for each pass and print it to standard; error. .. option:: -debug. If this is a debug build, this option will enable debug printouts from pass",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/opt.rst:1503,optimiz,optimization,1503,interpreter/llvm-project/llvm/docs/CommandGuide/opt.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/opt.rst,1,['optimiz'],['optimization']
Performance,"ader, and packaging; dependencies in so-called ""dictionaries."". A `dictionary` is a generated C++ source file containing references to the; header locations used when building (and any additional locations provided),; a set of forward declarations to reduce the need of loading header files, and; a few I/O helper functions.; The name ""dictionary"" is historic: before ``cling`` was used, it contained; the complete generated C++ reflection information, whereas now that is; derived at run-time from the header files.; It is still possible to fully embed header files rather than only storing; their names and search locations, to make the dictionary more self-contained. After generating the dictionary, it should be compiled into a shared library.; This provides additional dependency control: by linking it directly with any; further libraries needed, you can use standard mechanisms such as ``rpath``; to locate those library dependencies.; Alternatively, you can add the additional libraries to load to the mapping; files of the class loader (see below). .. note::. The JIT needs to resolve linker symbols in order to call them through; generated wrappers.; Thus, any classes, functions, and data that will be used in Python need; to be exported.; This is the default behavior on Mac and Linux, but not on Windows.; On that platform, use ``__declspec(dllexport)`` to explicitly export the; classes and function you expect to call.; CMake has simple `support for exporting all`_ C++ symbols. In tandem with any dictionary, a pre-compiled module (.pcm) file will be; generated.; C++ modules are still on track for inclusion in the C++20 standard and most; modern C++ compilers, ``clang`` among them, already have implementations.; The benefits for cppyy include faster bindings generation, lower memory; footprint, and isolation from preprocessor macros and compiler flags.; The use of modules is transparent, other than the requirement that they; need to be co-located with the compiled dictionary",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst:1957,load,load,1957,bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/utilities.rst,2,['load'],"['load', 'loader']"
Performance,"aders are meant to improve overall compile times for projects, so; the design of precompiled headers is entirely driven by performance concerns.; The use case for precompiled headers is relatively simple: when there is a; common set of headers that is included in nearly every source file in the; project, we *precompile* that bundle of headers into a single precompiled; header (PCH file). Then, when compiling the source files in the project, we; load the PCH file first (as a prefix header), which acts as a stand-in for that; bundle of headers. A precompiled header implementation improves performance when:. * Loading the PCH file is significantly faster than re-parsing the bundle of; headers stored within the PCH file. Thus, a precompiled header design; attempts to minimize the cost of reading the PCH file. Ideally, this cost; should not vary with the size of the precompiled header file. * The cost of generating the PCH file initially is not so large that it; counters the per-source-file performance improvement due to eliminating the; need to parse the bundled headers in the first place. This is particularly; important on multi-core systems, because PCH file generation serializes the; build when all compilations require the PCH file to be up-to-date. Modules, as implemented in Clang, use the same mechanisms as precompiled; headers to save a serialized AST file (one per module) and use those AST; modules. From an implementation standpoint, modules are a generalization of; precompiled headers, lifting a number of restrictions placed on precompiled; headers. In particular, there can only be one precompiled header and it must; be included at the beginning of the translation unit. The extensions to the; AST file format required for modules are discussed in the section on; :ref:`modules <pchinternals-modules>`. Clang's AST files are designed with a compact on-disk representation, which; minimizes both creation time and the time required to initially load the AST; file. The ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst:2149,perform,performance,2149,interpreter/llvm-project/clang/docs/PCHInternals.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/PCHInternals.rst,1,['perform'],['performance']
Performance,"adient calculation using:; ``` {.cpp}; std::shared_ptr<RooAbsL> likelihood = /* see examples above */;; RooMinimizer m(likelihood);; ```. By default, `RooFit::MultiProcess` spins up as many workers as there are cores in the system (as detected by `std::thread::hardware_concurrency()`).; To change the number of workers, call `RooFit::MultiProcess::Config::setDefaultNWorkers(desired_N_workers)` **before** creating the `RooMinimizer`. As noted above, offsetting is purely a function of the `RooMinimizer` when using `TestStatistics` classes.; Whereas with `fitTo` we can pass in a `RooFit::Offset(true)` optional `RooCmdArg` argument to activate offsetting, here we must do it on the minimizer as follows:; ``` {.cpp}; m.setOffsetting(true);; ```. All existing functionality of the `RooMinimizer` can be used on `TestStatistics` likelihoods as well.; For instance, running a `migrad` fit:; ``` {.cpp}; m.migrad(); ```. ## Constant term optimization; The `RooAbsTestStatistic` based classes not only combine statistics and calculation, but also constant term optimization routines.; These can be run on PDFs and datasets before starting a fit.; They search the calculation graph for parts that are independent of the fit parameters, precalculates them, and adds them to (a clone of) the dataset so that these values can be used during calculation. In `RooFit::TestStatistics`, we separated this functionality out into the `ConstantTermsOptimizer` class.; In fact, it is not so much a class, as it is a collection of static functions that can be applied to any combination of pdf and dataset.; This class does essentially the same as `constOptimizeTestStatistic` did on a `RooNLLVar`, except that it has been factored out into a separate class. ### Usage example: apply constant term optimization on pdf and dataset inside a likelihood; Applying the default `ConstantTermsOptimizer` optimization routines on the pdf and dataset inside a `RooAbsL` likelihood is as simple as:. ``` {.cpp}; likelihood.co",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/test_statistics.md:9474,optimiz,optimization,9474,roofit/doc/developers/test_statistics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/developers/test_statistics.md,2,['optimiz'],['optimization']
Performance,"ads if possible (for example with; [RDataFrame](https://root.cern/doc/master/classROOT_1_1RDataFrame.html); and [EnableImplicitMT](https://root.cern/doc/master/namespaceROOT.html#a06f2b8b216b615e5abbc872c9feff40f)); or switch to a machine with a more performant CPU.; - The 'Real Time' is significantly higher than 'CPU Time / number of threads'*.; If the real time is higher than the CPU time per core it implies the reading of data is the; bottleneck, as the CPU cores are wasting time waiting for data to arrive from your disk/drive; or network connection in order to decompress it.; The best way to decrease your runtime would be transferring the data you need onto a faster; storage medium (ie. a faster disk/drive such as an SSD, or connecting to a faster network; for remote file access), or to use a compression algorithm with a higher compression ratio,; possibly at the cost of the decompression rate.; Changing the number of threads is unlikely to help, and in fact using too many threads may; degrade performance if they make requests to different regions of your local storage. ; * If no '--threads' argument was provided this is 1, otherwise it is the minimum of the value; provided and the number of threads your CPU can run in parallel. It is worth noting that -; on shared systems or if running other heavy applications - the number of your own threads; running at any time may be lower than the limit due to demand on the CPU.; - The 'Real Time' is similar to 'CPU Time / number of threads' AND 'Compressed Throughput' is lower than expected; for your storage medium: this would imply that your CPU threads aren't decompressing data as fast as your storage; medium can provide it, and so decompression is the bottleneck.; The best way to decrease your runtime would be to utilise a system with a faster CPU, or make use; use of more threads when running, or use a compression algorithm with a higher decompression rate such as LZ4,; possibly at the cost of some extra file size. ### ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/readspeed/README.md:2408,perform,performance,2408,tree/readspeed/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/readspeed/README.md,1,['perform'],['performance']
Performance,"adura, Yuka Takahashi and Raphael Isemann*. ## Overview. ROOT has several features which interact with libraries and require implicit; header inclusion. This can be triggered by reading or writing data on disk,; or user actions at the prompt. Often, the headers are immutable and reparsing is; redundant. C++ Modules are designed to minimize the reparsing of the same; header content by providing an efficient on-disk representation of C++ Code. The ROOT v6.16 release came with a preview of the module technology;; dedicated binaries have been built and can be reproduced by passing; `-Druntime_cxxmodules=On` as configure flag. The goals of this technology are:; * Gain feedback from early adoption -- the technology is being long anticipated; by some of the users of ROOT. It improves correctness of ROOT and improves; performance when carefully adopted.; * Study performance bottlenecks -- the feature is designed with performance; considerations in mind. In this document we describe the current performance; bottlenecks and trade-offs.; * Understand if the gradual migration policy is sufficient -- C++ Modules in; ROOT support gradual migration. In particular, ROOT can enable C++ Modules for; itself and still run in legacy mode for the third-party code (generating; rootmap files and other scaffolding). C++ Modules are here and we would like to give a brief introduction of how the; feature works, what are its pros and cons, what's the current state of the; implementation and how third-party code can use it. Read more [[1]]. C++ Modules in ROOT are default since v6.20 (Unix) and v6.22 (OSX). ## Design Goals. * Coherence with standard C++ -- C++ Modules TS is advancing and will be; likely part the upcoming C++20 standard;; * Performance -- provide performance that is competitive to ROOT with PCH and; advance further the implementation of the C++ Modules in clang to optimize; memory footprint and execution time;; * Incremental adoption -- provide third-party code with an increment",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md:1072,perform,performance,1072,README/README.CXXMODULES.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/README.CXXMODULES.md,2,"['bottleneck', 'perform']","['bottlenecks', 'performance']"
Performance,"afe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The ocaml frametable structure supports liveness information. It would be good; to support it. //===---------------------------------------------------------------------===//. The FIXME in ComputeCommonTailLength in BranchFolding.cpp needs to be; revisited. The check is there to work around a misuse of directives in inline; assembly. //===---------------------------------------------------------------------===//. It would be good to detect collector/target compatibility instead of silently; doing the wrong thing. //===---------------------------------------------------------------------===//. It would be really nice to be able to write patterns in .td files for copies,; which would eliminate a bunch of explicit predicates on them (e.g. no side; effects). Once this is in place, it would be even better to have tblgen; synthesize the various copy insertion/inspection meth",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:3773,optimiz,optimizations,3773,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt,1,['optimiz'],['optimizations']
Performance,"afe""; languages would normally split a 64-bit store on ARM into two 32-bit unordered; stores.). Notes for optimizers; In terms of the optimizer, this prohibits any transformation that transforms a; single load into multiple loads, transforms a store into multiple stores,; narrows a store, or stores a value which would not be stored otherwise. Some; examples of unsafe optimizations are narrowing an assignment into a bitfield,; rematerializing a load, and turning loads and stores into a memcpy; call. Reordering unordered operations is safe, though, and optimizers should; take advantage of that because unordered operations are common in languages; that need them. Notes for code generation; These operations are required to be atomic in the sense that if you use; unordered loads and unordered stores, a load cannot see a value which was; never stored. A normal load or store instruction is usually sufficient, but; note that an unordered load or store cannot be split into multiple; instructions (or an instruction which does multiple memory operations, like; ``LDRD`` on ARM without LPAE, or not naturally-aligned ``LDRD`` on LPAE ARM). Monotonic; ---------. Monotonic is the weakest level of atomicity that can be used in synchronization; primitives, although it does not provide any general synchronization. It; essentially guarantees that if you take all the operations affecting a specific; address, a consistent ordering exists. Relevant standard; This corresponds to the C++/C ``memory_order_relaxed``; see those; standards for the exact definition. Notes for frontends; If you are writing a frontend which uses this directly, use with caution. The; guarantees in terms of synchronization are very weak, so make sure these are; only used in a pattern which you know is correct. Generally, these would; either be used for atomic operations which do not protect other memory (like; an atomic counter), or along with a ``fence``. Notes for optimizers; In terms of the optimizer, this can be",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:9750,load,load,9750,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,2,['load'],['load']
Performance,"after re-staging; of the dataset files, which typically changes the end-point data servers.; Entry-lists created with the full path should still be matched correctly. Miscellaneous. Repaired the behavior of TTreeCache when the TTree has a dramatic dynamic range with a lots of very small entriesat the beginning and very large entries at the end, the size in bytes of the cluster for the later entries will be very large (because of the cluster size in entries is large!). TTreeCache::FillBuffer was always attempting to load complete clusters not matter the; size (even with the size was larger than 2GB!). This patch resolves the issue by limiting the amount of memory used to:. The requested size if more than one cluster fits in the cache.; Twice the requested size if at least one basket per branch fits in the cache.; Four time the requested size in the case where the cache can not even hold one basket per branch. The filling will restart at the next cluster boundary in the case a) and will; restart at the maximum of entry number read in the cache in the case b) and c).; Baskets that are below this boundary and did not fit in the cache will be read; individually.; Repaired the basket flushing frequency when the TTree has already more than one cluster size.; Repaired binning of string histogram generated by TTree::Draw.; Many bug fixes and fix for issues discovery by Coverity, see change log for more details.; In TTree::MakeProxy add proper support for top level stl collection of objects and for stl collection of objects that are 'empty' in the file (and thus we know nothing about its content).; Avoid deficiency in hadd when the resulting TTree is longer than the AutoSave length *and* the TFileMerger needs to handle the input files in more than one pass for example when there is more than 1000 input files or the -n option is passed to hadd.; Fix support for emulated class that derived from an abstract class.; This can happen when reading a file containing an ancient; class ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v534/index.html:3570,cache,cache,3570,tree/doc/v534/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v534/index.html,2,['cache'],['cache']
Performance,"again, they are still visible in the ``-help-hidden output``). Now the application code can simply use:. .. code-block:: c++. ...; if (!Quiet) printInformationalMessage(...);; ... ... which is much nicer! The ""`cl::alias`_"" can be used to specify an; alternative name for any variable type, and has many uses. .. _unnamed alternatives using the generic parser:. Selecting an alternative from a set of possibilities; ----------------------------------------------------. So far we have seen how the CommandLine library handles builtin types like; ``std::string``, ``bool`` and ``int``, but how does it handle things it doesn't; know about, like enums or '``int*``'s?. The answer is that it uses a table-driven generic parser (unless you specify; your own parser, as described in the `Extension Guide`_). This parser maps; literal strings to whatever type is required, and requires you to tell it what; this mapping should be. Let's say that we would like to add four optimization levels to our optimizer,; using the standard flags ""``-g``"", ""``-O0``"", ""``-O1``"", and ""``-O2``"". We; could easily implement this with boolean options like above, but there are; several problems with this strategy:. #. A user could specify more than one of the options at a time, for example,; ""``compiler -O3 -O2``"". The CommandLine library would not be able to catch; this erroneous input for us. #. We would have to test 4 different variables to see which ones are set. #. This doesn't map to the numeric levels that we want... so we cannot easily; see if some level >= ""``-O1``"" is enabled. To cope with these problems, we can use an enum value, and have the CommandLine; library fill it in with the appropriate level directly, which is used like this:. .. code-block:: c++. enum OptLevel {; g, O1, O2, O3; };. cl::opt<OptLevel> OptimizationLevel(cl::desc(""Choose optimization level:""),; cl::values(; clEnumVal(g , ""No optimizations, enable debugging""),; clEnumVal(O1, ""Enable trivial optimizations""),; clEnumVal(O2, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst:13847,optimiz,optimization,13847,interpreter/llvm-project/llvm/docs/CommandLine.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst,2,['optimiz'],"['optimization', 'optimizer']"
Performance,"age(${value}); endforeach(); endforeach(). You'll notice that the inner foreach loop's list is doubly dereferenced. This is; because the first dereference turns ``list_name`` into the name of the sub-list; (a, b, or c in the example), then the second dereference is to get the value of; the list. This pattern is used throughout CMake, the most common example is the compiler; flags options, which CMake refers to using the following variable expansions:; CMAKE_${LANGUAGE}_FLAGS and CMAKE_${LANGUAGE}_FLAGS_${CMAKE_BUILD_TYPE}. Other Types; -----------. Variables that are cached or specified on the command line can have types; associated with them. The variable's type is used by CMake's UI tool to display; the right input field. A variable's type generally doesn't impact evaluation,; however CMake does have special handling for some variables such as PATH.; You can read more about the special handling in `CMake's set documentation; <https://cmake.org/cmake/help/v3.5/command/set.html#set-cache-entry>`_. Scope; -----. CMake inherently has a directory-based scoping. Setting a variable in a; CMakeLists file, will set the variable for that file, and all subdirectories.; Variables set in a CMake module that is included in a CMakeLists file will be; set in the scope they are included from, and all subdirectories. When a variable that is already set is set again in a subdirectory it overrides; the value in that scope and any deeper subdirectories. The CMake set command provides two scope-related options. PARENT_SCOPE sets a; variable into the parent scope, and not the current scope. The CACHE option sets; the variable in the CMakeCache, which results in it being set in all scopes. The; CACHE option will not set a variable that already exists in the CACHE unless the; FORCE option is specified. In addition to directory-based scope, CMake functions also have their own scope.; This means variables set inside functions do not bleed into the parent scope.; This is not true of macros, ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMakePrimer.rst:6154,cache,cache-entry,6154,interpreter/llvm-project/llvm/docs/CMakePrimer.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CMakePrimer.rst,1,['cache'],['cache-entry']
Performance,"age; ===============. The stack map support described in this document can be used to; precisely determine the location of values at a specific position in; the code. LLVM does not maintain any mapping between those values and; any higher-level entity. The runtime must be able to interpret the; stack map record given only the ID, offset, and the order of the; locations, records, and functions, which LLVM preserves. Note that this is quite different from the goal of debug information,; which is a best-effort attempt to track the location of named; variables at every instruction. An important motivation for this design is to allow a runtime to; commandeer a stack frame when execution reaches an instruction address; associated with a stack map. The runtime must be able to rebuild a; stack frame and resume program execution using the information; provided by the stack map. For example, execution may resume in an; interpreter or a recompiled version of the same function. This usage restricts LLVM optimization. Clearly, LLVM must not move; stores across a stack map. However, loads must also be handled; conservatively. If the load may trigger an exception, hoisting it; above a stack map could be invalid. For example, the runtime may; determine that a load is safe to execute without a type check given; the current state of the type system. If the type system changes while; some activation of the load's function exists on the stack, the load; becomes unsafe. The runtime can prevent subsequent execution of that; load by immediately patching any stack map location that lies between; the current call site and the load (typically, the runtime would; simply patch all stack map locations to invalidate the function). If; the compiler had hoisted the load above the stack map, then the; program could crash before the runtime could take back control. To enforce these semantics, stackmap and patchpoint intrinsics are; considered to potentially read and write all memory. This may limit;",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst:17893,optimiz,optimization,17893,interpreter/llvm-project/llvm/docs/StackMaps.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/StackMaps.rst,1,['optimiz'],['optimization']
Performance,"ager uses a; round-robin selector to guarantee that resource usage is uniformly distributed; between all units of a group. :program:`llvm-mca`'s scheduler internally groups instructions into three sets:. * WaitSet: a set of instructions whose operands are not ready.; * ReadySet: a set of instructions ready to execute.; * IssuedSet: a set of instructions executing. Depending on the operands availability, instructions that are dispatched to the; scheduler are either placed into the WaitSet or into the ReadySet. Every cycle, the scheduler checks if instructions can be moved from the WaitSet; to the ReadySet, and if instructions from the ReadySet can be issued to the; underlying pipelines. The algorithm prioritizes older instructions over younger; instructions. Write-Back and Retire Stage; """"""""""""""""""""""""""""""""""""""""""""""""""""""; Issued instructions are moved from the ReadySet to the IssuedSet. There,; instructions wait until they reach the write-back stage. At that point, they; get removed from the queue and the retire control unit is notified. When instructions are executed, the retire control unit flags the instruction as; ""ready to retire."". Instructions are retired in program order. The register file is notified of the; retirement so that it can free the physical registers that were allocated for; the instruction during the register renaming stage. Load/Store Unit and Memory Consistency Model; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; To simulate an out-of-order execution of memory operations, :program:`llvm-mca`; utilizes a simulated load/store unit (LSUnit) to simulate the speculative; execution of loads and stores. Each load (or store) consumes an entry in the load (or store) queue. Users can; specify flags ``-lqueue`` and ``-squeue`` to limit the number of entries in the; load and store queues respectively. The queues are unbounded by default. The LSUnit implements a relaxed consistency model for memory loads and stores.; The rules are:. 1. A younger load is allowed to ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:38824,queue,queue,38824,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['queue'],['queue']
Performance,"agnostics for compiler crashes (default)); * ``all`` (Generate diagnostics for all tools which support it). .. option:: -fno-crash-diagnostics. Disable auto-generation of preprocessed source files during a clang crash. The -fno-crash-diagnostics flag can be helpful for speeding the process; of generating a delta reduced test case. .. option:: -fcrash-diagnostics-dir=<dir>. Specify where to write the crash diagnostics files; defaults to the; usual location for temporary files. .. envvar:: CLANG_CRASH_DIAGNOSTICS_DIR=<dir>. Like ``-fcrash-diagnostics-dir=<dir>``, specifies where to write the; crash diagnostics files, but with lower precedence than the option. Clang is also capable of generating preprocessed source file(s) and associated; run script(s) even without a crash. This is specially useful when trying to; generate a reproducer for warnings or errors while using modules. .. option:: -gen-reproducer. Generates preprocessed source files, a reproducer script and if relevant, a; cache containing: built module pcm's and all headers needed to rebuild the; same modules. .. _rpass:. Options to Emit Optimization Reports; ------------------------------------. Optimization reports trace, at a high-level, all the major decisions; done by compiler transformations. For instance, when the inliner; decides to inline function ``foo()`` into ``bar()``, or the loop unroller; decides to unroll a loop N times, or the vectorizer decides to; vectorize a loop body. Clang offers a family of flags which the optimizers can use to emit; a diagnostic in three cases:. 1. When the pass makes a transformation (`-Rpass`). 2. When the pass fails to make a transformation (`-Rpass-missed`). 3. When the pass determines whether or not to make a transformation; (`-Rpass-analysis`). NOTE: Although the discussion below focuses on `-Rpass`, the exact; same options apply to `-Rpass-missed` and `-Rpass-analysis`. Since there are dozens of passes inside the compiler, each of these flags; take a regular exp",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:23393,cache,cache,23393,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['cache'],['cache']
Performance,"ail for unsupported atomic; operations; if you need such an operation, use explicit locking. Relevant standard; This is intended to match the Java memory model for shared variables. Notes for frontends; This cannot be used for synchronization, but is useful for Java and other; ""safe"" languages which need to guarantee that the generated code never; exhibits undefined behavior. Note that this guarantee is cheap on common; platforms for loads of a native width, but can be expensive or unavailable for; wider loads, like a 64-bit store on ARM. (A frontend for Java or other ""safe""; languages would normally split a 64-bit store on ARM into two 32-bit unordered; stores.). Notes for optimizers; In terms of the optimizer, this prohibits any transformation that transforms a; single load into multiple loads, transforms a store into multiple stores,; narrows a store, or stores a value which would not be stored otherwise. Some; examples of unsafe optimizations are narrowing an assignment into a bitfield,; rematerializing a load, and turning loads and stores into a memcpy; call. Reordering unordered operations is safe, though, and optimizers should; take advantage of that because unordered operations are common in languages; that need them. Notes for code generation; These operations are required to be atomic in the sense that if you use; unordered loads and unordered stores, a load cannot see a value which was; never stored. A normal load or store instruction is usually sufficient, but; note that an unordered load or store cannot be split into multiple; instructions (or an instruction which does multiple memory operations, like; ``LDRD`` on ARM without LPAE, or not naturally-aligned ``LDRD`` on LPAE ARM). Monotonic; ---------. Monotonic is the weakest level of atomicity that can be used in synchronization; primitives, although it does not provide any general synchronization. It; essentially guarantees that if you take all the operations affecting a specific; address, a consistent ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:9253,optimiz,optimizations,9253,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,3,"['load', 'optimiz']","['load', 'loads', 'optimizations']"
Performance,"ailcallee(i32 inreg %a1, i32 inreg %a2, i32 %a3, i32 %a4). define fastcc i32 @tailcaller(i32 %in1, i32 %in2) {; %l1 = add i32 %in1, %in2; %tmp = tail call fastcc i32 @tailcallee(i32 inreg %in1, i32 inreg %in2, i32 %in1, i32 %l1); ret i32 %tmp; }. Implications of ``-tailcallopt``:. To support tail call optimization in situations where the callee has more; arguments than the caller a 'callee pops arguments' convention is used. This; currently causes each ``fastcc`` call that is not tail call optimized (because; one or more of above constraints are not met) to be followed by a readjustment; of the stack. So performance might be worse in such cases. Sibling call optimization; -------------------------. Sibling call optimization is a restricted form of tail call optimization.; Unlike tail call optimization described in the previous section, it can be; performed automatically on any tail calls when ``-tailcallopt`` option is not; specified. Sibling call optimization is currently performed on x86/x86-64 when the; following constraints are met:. * Caller and callee have the same calling convention. It can be either ``c`` or; ``fastcc``. * The call is a tail call - in tail position (ret immediately follows call and; ret uses value of call or is void). * Caller and callee have matching return type or the callee result is not used. * If any of the callee arguments are being passed in stack, they must be; available in caller's own incoming argument stack and the frame offsets must; be the same. Example:. .. code-block:: llvm. declare i32 @bar(i32, i32). define i32 @foo(i32 %a, i32 %b, i32 %c) {; entry:; %0 = tail call i32 @bar(i32 %a, i32 %b); ret i32 %0; }. The X86 backend; ---------------. The X86 code generator lives in the ``lib/Target/X86`` directory. This code; generator is capable of targeting a variety of x86-32 and x86-64 processors, and; includes support for ISA extensions such as MMX and SSE. X86 Target Triples supported; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The following ar",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:88540,optimiz,optimization,88540,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,2,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"aining module. The; following session shows that in detail:. ``` {.cpp}; >>> from ROOT import *; >>> print(gDebug); 0; >>> gROOT.ProcessLine( 'gDebug = 7;' ); >>> print(gDebug); 0 # local gDebug is unchanged; >>> gDebug = 5 # changes _local_ reference only; >>> print(gDebug); 5 # locally correct, but ...; >>> gROOT.ProcessLine( 'cout << gDebug << endl;' ); 7 # ... ROOT global unchanged; >>> import ROOT; >>> print(ROOT.gDebug); 7 # still the old value (not '5'); >>> ROOT.gDebug = 3 # changes ROOT module reference; >>> gROOT.ProcessLine( 'cout << gDebug << endl;' ); 3 # ROOT global properly changed; >>>; ```. The above is another good reason to prefer 'import ROOT' over 'from ROOT; import \*'. #### Access to Python. The access to Python from Cling goes through the **`TPython`** class, or; directly if a Python object or class has crossed the border. The; **`TPython`** class, which looks approximately like this:. ``` {.cpp}; class TPython {. public:; // load a Python script as if it were a macro; static void LoadMacro(const char* name);. // execute a Python statement (e.g. ""import ROOT""); static void Exec(const char* cmd);. // evaluate a Python expression (e.g. ""1+1""); static const TPyReturn& Eval(const char* expr);. // bind a ROOT object with, at the Python side, the name ""label""; static bool Bind(TObject* obj,const char* label);. // enter an interactive Python session (exit with ^D); static void Prompt();; };; ```. `LoadMacro(const char* name)` - the argument is a name of a Python file; that is to be executed (`'execfile'`), after which any new classes are; automatically made available to Cling. Since it is non-selective, use; with care. `ExecScript(const char* name,int argc=0,const char** argv=0)` - the; argument is a name of a python file that is to be executed ('execfile'); in a private namespace to minimize side-effects. Optionally, you can add; CLI-style arguments which are handed to the script through 'sys.argv' in; the normal way. `Exec(const char* cmd) `- the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md:15212,load,load,15212,documentation/users-guide/PythonRuby.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/PythonRuby.md,1,['load'],['load']
Performance,"ains decomposable records, e.g. CFI records; in an eh-frame section). * Graph utility operations. * ``getName`` returns the name of this graph, which is usually based on the; name of the input object file. * ``getTargetTriple`` returns an `llvm::Triple` for the executor process. * ``getPointerSize`` returns the size of a pointer (in bytes) in the executor; process. * ``getEndinaness`` returns the endianness of the executor process. * ``allocateString`` copies data from a given ``llvm::Twine`` into the; link graph's internal allocator. This can be used to ensure that content; created inside a pass outlives that pass's execution. .. _generic_link_algorithm:. Generic Link Algorithm; ======================. JITLink provides a generic link algorithm which can be extended / modified at; certain points by the introduction of JITLink :ref:`passes`. At the end of each phase the linker packages its state into a *continuation*; and calls the ``JITLinkContext`` object to perform a (potentially high-latency); asynchronous operation: allocating memory, resolving external symbols, and; finally transferring linked memory to the executing process. #. Phase 1. This phase is called immediately by the ``link`` function as soon as the; initial configuration (including the pass pipeline setup) is complete. #. Run pre-prune passes. These passes are called on the graph before it is pruned. At this stage; ``LinkGraph`` nodes still have their original vmaddrs. A mark-live pass; (supplied by the ``JITLinkContext``) will be run at the end of this; sequence to mark the initial set of live symbols. Notable use cases: marking nodes live, accessing/copying graph data that; will be pruned (e.g. metadata that's important for the JIT, but not needed; for the link process). #. Prune (dead-strip) the ``LinkGraph``. Removes all symbols and blocks not reachable from the initial set of live; symbols. This allows JITLink to remove unreachable symbols / content, including; overridden weak and redundant ODR ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst:17664,perform,perform,17664,interpreter/llvm-project/llvm/docs/JITLink.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/JITLink.rst,2,"['latency', 'perform']","['latency', 'perform']"
Performance,"aitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global have; completed before; performing the; atomicrmw that is; being released. 3. flat_atomic; 4. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Must happen before; following; buffer_inv.; - Ensures the; atomicrmw has; completed before; invalidating the; cache. 5. buffer_inv sc1=1. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acq_rel - system - generic 1. buffer_wbl2 sc0=1 sc1=1. - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and L2 writeback; have completed before; performing the; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:323351,load,load,323351,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"aitcnt; lgkmcnt(0) and so do; not need to be; considered.); - s_waitcnt vmcnt(0); must happen after; preceding; global/generic load; atomic/; atomicrmw-with-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vmcnt(0) and so do; not need to be; considered.); - s_waitcnt vscnt(0); Must happen after; preceding; global/generic store; atomic/; atomicrmw-no-return-value; with memory; ordering of seq_cst; and with equal or; wider sync scope.; (Note that seq_cst; fences have their; own s_waitcnt; vscnt(0) and so do; not need to be; considered.); - Ensures any; preceding; sequential; consistent global; memory instructions; have completed; before executing; this sequentially; consistent; instruction. This; prevents reordering; a seq_cst store; followed by a; seq_cst load. (Note; that seq_cst is; stronger than; acquire/release as; the reordering of; load acquire; followed by a store; release is; prevented by the; s_waitcnt of; the release, but; there is nothing; preventing a store; release followed by; load acquire from; completing out of; order. The s_waitcnt; could be placed after; seq_store or before; the seq_load. We; choose the load to; make the s_waitcnt be; as late as possible; so that the store; may have already; completed.). 2. *Following; instructions same as; corresponding load; atomic acquire,; except must generate; all instructions even; for OpenCL.*; store atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local store atomic release,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; atomicrmw seq_cst - singlethread - global *Same as corresponding; - wavefront - local atomicrmw acq_rel,; - workgroup - generic except must generate; - agent all instructions even; - system for OpenCL.*; fence seq_cst - singlethread *none* *Same as corresponding; - wavefront fence acq_rel,; - workgroup except must generate; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:378990,load,load,378990,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"aive mathematical formulas for complex; multiplication and enables application of Smith's algorithm for complex; division. See SMITH, R. L. Algorithm 116: Complex division. Commun.; ACM 5, 8 (1962). The default is ``-fno-cx-fortran-rules``. .. _floating-point-environment:. Accessing the floating point environment; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; Many targets allow floating point operations to be configured to control things; such as how inexact results should be rounded and how exceptional conditions; should be handled. This configuration is called the floating point environment.; C and C++ restrict access to the floating point environment by default, and the; compiler is allowed to assume that all operations are performed in the default; environment. When code is compiled in this default mode, operations that depend; on the environment (such as floating-point arithmetic and `FLT_ROUNDS`) may have; undefined behavior if the dynamic environment is not the default environment; for; example, `FLT_ROUNDS` may or may not simply return its default value for the target; instead of reading the dynamic environment, and floating-point operations may be; optimized as if the dynamic environment were the default. Similarly, it is undefined; behavior to change the floating point environment in this default mode, for example; by calling the `fesetround` function.; C provides two pragmas to allow code to dynamically modify the floating point environment:. - ``#pragma STDC FENV_ACCESS ON`` allows dynamic changes to the entire floating; point environment. - ``#pragma STDC FENV_ROUND FE_DYNAMIC`` allows dynamic changes to just the floating; point rounding mode. This may be more optimizable than ``FENV_ACCESS ON`` because; the compiler can still ignore the possibility of floating-point exceptions by default. Both of these can be used either at the start of a block scope, in which case; they cover all code in that scope (unless they're turned off in a child scope),; or at the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst:69857,optimiz,optimized,69857,interpreter/llvm-project/clang/docs/UsersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/UsersManual.rst,1,['optimiz'],['optimized']
Performance,"ake EnableImplicitMT no-op if IMT is already on; - Decompress `TTreeCache` in parallel if IMT is on (upgrade of the `TTreeCacheUnzip` class).; - In `TTreeProcessorMT` delete friend chains after the main chain to avoid double deletes.; - If IMT is enabled, the multithreaded execution of the fit respects the number of threads IMT has been initialized with. ## Language Bindings. ### Notebook integration; - In the ROOT kernel, avoid import of unnecessary components.; - In the ROOT kernel, optimise regexes involved in tab-completion which could take up to minutes to be executed. ## JavaScript ROOT; ; Upgrade JSROOT to v5.4.1. Following new features implemented:. * New supported classes:; - TDiamond; - TArc; - TCurlyLine; - TCurlyArc; - TCrown; * New draw options:; - ""RX"" and ""RY"" for TGraph to reverse axis; - ""noopt"" for TGraph to disable drawing optimization; - ""CPN"" for TCanvas to create color palette from N last colors; - ""line"" for TGraph2D; * New features:; - support LZ4 compression; - tooltips and zooming in TGraphPolar drawings; - TPavesText with multiple underlying paves; - implement all fill styles; - draw borders for TWbox; - draw all objects from TList/TObjArray as they appear in list of primitives; - let enable/disable highlight of extra objects in geometry viewer; - draw axis labels on both sides when pad.fTick[x/y] > 1; - make drawing of TCanvas with many primitives smoother; - add fOptTitle, fOptLogx/y/z fields in JSROOT.gStyle; * Behavior changes:; - disable automatic frame adjustment, can be enabled with ""&adjframe"" parameter in URL; - when drawing TH2/TH3 scatter plots, always generate same ""random"" pattern; - use barwidth/baroffset parameters in lego plots; * Bug fixes:; - use same number of points to draw lines and markers on the TGraph; - correctly draw filled TArrow endings; - let combine ""L"" or ""C"" TGraph draw option with others; - correct positioning of custom axis labels; - correctly toggle lin/log axes in lego plot; - let correctly change marker",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md:15722,optimiz,optimization,15722,README/ReleaseNotes/v614/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md,1,['optimiz'],['optimization']
Performance,"ake incorrect assumptions (most notably, that X == X; is always true, since it does not hold for NaN).; (Difficulty: Medium). Improved loop execution modeling.; The analyzer simply unrolls each loop N times before; dropping the path, for a fixed constant N.; However, that results in lost coverage in cases where the loop always; executes more than N times.; A Google Summer Of Code; project; was completed to make the loop bound parameterizable,; but the widening; problem still remains open. (Difficulty: Hard). Basic function summarization support; The analyzer performs inter-procedural analysis using; either inlining or ""conservative evaluation"" (invalidating all data; passed to the function).; Often, a very simple summary; (e.g. ""this function is pure"") would be; enough to be a large improvement over conservative evaluation.; Such summaries could be obtained either syntactically,; or using a dataflow framework.; (Difficulty: Hard). Implement a dataflow flamework.; The analyzer core; implements a symbolic execution; engine, which performs checks; (use-after-free, uninitialized value read, etc.); over a single program path.; However, many useful properties; (dead code, check-after-use, etc.) require; reasoning over all possible in a program.; Such reasoning requires a; dataflow analysis framework.; Clang already implements; a few dataflow analyses (most notably, liveness),; but they implemented in an ad-hoc fashion.; A proper framework would enable us writing many more useful checkers.; (Difficulty: Hard) . Track type information through casts more precisely.; The DynamicTypePropagation; checker is in charge of inferring a region's; dynamic type based on what operations the code is performing.; Casts are a rich source of type information that the analyzer currently ignores.; (Difficulty: Medium). Fixing miscellaneous bugs; Apart from the open projects listed above,; contributors are welcome to fix any of the outstanding; bugs; in the Bugzilla.; (Difficulty: Anything). ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/open_projects.html:7834,perform,performs,7834,interpreter/llvm-project/clang/www/analyzer/open_projects.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/analyzer/open_projects.html,4,['perform'],"['performing', 'performs']"
Performance,"al 1. buffer/global/flat_load; glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. load atomic acquire - agent - generic 1. flat_load glc=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. load atomic acquire - system - generic 1. flat_load glc=1; 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_invl2 and; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the caches. 3. buffer_invl2;; buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale L1 global data,; nor see stale L2 MTYPE; NC global data.; MTYPE RW and CC memory will; never be stale in L2 due to; the memory probes. atomicrmw acquire - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; atomicrmw acquire - singlethread - local *If TgSplit execution mode,; - wavefront local address space cannot; be used.*. 1. ds_atomic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; 2. s_waitcnt vmcnt(0). - If not TgSplit execution; mode, omit.; - Must happen before the; followi",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:248157,load,load,248157,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"al 1. buffer/global/flat_store; - wavefront - generic; - workgroup; - agent; - system; store atomic monotonic - singlethread - local 1. ds_store; - wavefront; - workgroup; atomicrmw monotonic - singlethread - global 1. buffer/global/flat_atomic; - wavefront - generic; - workgroup; - agent; - system; atomicrmw monotonic - singlethread - local 1. ds_atomic; - wavefront; - workgroup; **Acquire Atomic**; ------------------------------------------------------------------------------------; load atomic acquire - singlethread - global 1. buffer/global/ds/flat_load; - wavefront - local; - generic; load atomic acquire - workgroup - global 1. buffer/global_load; load atomic acquire - workgroup - local 1. ds/flat_load; - generic 2. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures any; following global; data read is no; older than a local load; atomic value being; acquired. load atomic acquire - agent - global 1. buffer/global_load; - system glc=1; 2. s_waitcnt vmcnt(0). - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following; loads will not see; stale global data. load atomic acquire - agent - generic 1. flat_load glc=1; - system 2. s_waitcnt vmcnt(0) &; lgkmcnt(0). - If OpenCL omit; lgkmcnt(0).; - Must happen before; following; buffer_wbinvl1_vol.; - Ensures the flat_load; has completed; before invalidating; the cache. 3. buffer_wbinvl1_vol. - Must happen before; any following; global/generic; load/load; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. atomicrmw acquire - singlethread - global 1. buffer/global/ds/flat_atomic; - wavefront - local; - generic; atomicrmw acquire - workgroup - global 1. buffer/global_atomic; atomicrmw acquire - wor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:214584,load,load,214584,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"al destination which contains an unreachable; instruction. This form conveys to the optimizer that the call returns; abnormally. For an invoke which neither returns normally or requires unwind; code in the current function, you can use a noreturn call instruction if; desired. This is generally not required because the optimizer will convert; an invoke with an unreachable unwind destination to a call instruction. #. Use profile metadata to indicate statically known cold paths, even if; dynamic profiling information is not available. This can make a large; difference in code placement and thus the performance of tight loops. #. When generating code for loops, try to avoid terminating the header block of; the loop earlier than necessary. If the terminator of the loop header; block is a loop exiting conditional branch, the effectiveness of LICM will; be limited for loads not in the header. (This is due to the fact that LLVM; may not know such a load is safe to speculatively execute and thus can't; lift an otherwise loop invariant load unless it can prove the exiting; condition is not taken.) It can be profitable, in some cases, to emit such; instructions into the header even if they are not used along a rarely; executed path that exits the loop. This guidance specifically does not; apply if the condition which terminates the loop header is itself invariant,; or can be easily discharged by inspecting the loop index variables. #. In hot loops, consider duplicating instructions from small basic blocks; which end in highly predictable terminators into their successor blocks.; If a hot successor block contains instructions which can be vectorized; with the duplicated ones, this can provide a noticeable throughput; improvement. Note that this is not always profitable and does involve a; potentially large increase in code size. #. When checking a value against a constant, emit the check using a consistent; comparison type. The GVN pass *will* optimize redundant equalities even ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:7551,load,load,7551,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,2,['load'],['load']
Performance,"al mitigation: by; forming these invalid stack pointer addresses we can prevent speculative; returns from successfully reading speculatively written values to the actual; stack. This works first by forming a data-dependency between computing the; address of the return address on the stack and our predicate state. And even; when satisfied, if a misprediction causes the state to be poisoned the; resulting stack pointer will be invalid. ##### Rewrite API of internal functions to directly propagate predicate state. (Not yet implemented.). We have the option with internal functions to directly adjust their API to; accept the predicate as an argument and return it. This is likely to be; marginally cheaper than embedding into `%rsp` for entering functions. ##### Use `lfence` to guard function transitions. An `lfence` instruction can be used to prevent subsequent loads from; speculatively executing until all prior mispredicted predicates have resolved.; We can use this broader barrier to speculative loads executing between; functions. We emit it in the entry block to handle calls, and prior to each; return. This approach also has the advantage of providing the strongest degree; of mitigation when mixed with unmitigated code by halting all misspeculation; entering a function which is mitigated, regardless of what occurred in the; caller. However, such a mixture is inherently more risky. Whether this kind of; mixture is a sufficient mitigation requires careful analysis. Unfortunately, experimental results indicate that the performance overhead of; this approach is very high for certain patterns of code. A classic example is; any form of recursive evaluation engine. The hot, rapid call and return; sequences exhibit dramatic performance loss when mitigated with `lfence`. This; component alone can regress performance by 2x or more, making it an unpleasant; tradeoff even when only used in a mixture of code. ##### Use an internal TLS location to pass predicate state. We can define",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md:41472,load,loads,41472,interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SpeculativeLoadHardening.md,1,['load'],['loads']
Performance,"al.stepvector.nxv4i32(); declare <vscale x 8 x i16> @llvm.experimental.stepvector.nxv8i16(). The '``llvm.experimental.stepvector``' intrinsics are used to create vectors; of integers whose elements contain a linear sequence of values starting from 0; with a step of 1. This experimental intrinsic can only be used for vectors; with integer elements that are at least 8 bits in size. If the sequence value; exceeds the allowed limit for the element type then the result for that lane is; undefined. These intrinsics work for both fixed and scalable vectors. While this intrinsic; is marked as experimental, the recommended way to express this operation for; fixed-width vectors is still to generate a constant vector instead. Arguments:; """""""""""""""""""". None. '``llvm.experimental.get.vector.length``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.experimental.get.vector.length.i32(i32 %cnt, i32 immarg %vf, i1 immarg %scalable); declare i32 @llvm.experimental.get.vector.length.i64(i64 %cnt, i32 immarg %vf, i1 immarg %scalable). Overview:; """""""""""""""""". The '``llvm.experimental.get.vector.length.*``' intrinsics take a number of; elements to process and returns how many of the elements can be processed; with the requested vectorization factor. Arguments:; """""""""""""""""""". The first argument is an unsigned value of any scalar integer type and specifies; the total number of elements to be processed. The second argument is an i32; immediate for the vectorization factor. The third argument indicates if the; vectorization factor should be multiplied by vscale. Semantics:; """""""""""""""""""". Returns a positive i32 value (explicit vector length) that is unknown at compile; time and depends on the hardware specification.; If the result value does not fit in the result type, then the result is; a :ref:`poison value <poisonvalues>`. This intrinsic is intended to be used by loop vectorization with VP intrinsics; in order",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:673692,scalab,scalable,673692,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"al/generic load/store/; load atomic/store atomic/; atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; have; completed before; performing the; atomicrmw that is; being released. 2. buffer/global/flat_atomic; atomicrmw release - workgroup - local *If TgSplit execution mode,; local address space cannot; be used.*. 1. ds_atomic; atomicrmw release - agent - global 1. s_waitcnt lgkmcnt(0) &; - generic vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory operations; to global and local; have completed; before performing; the atomicrmw that; is being released. 2. buffer/global/flat_atomic; atomicrmw release - system - global 1. buffer_wbl2; - generic; - Must happen before; following s_waitcnt.; - Performs L2 writeback to; ensure previous; global/generic; store/atomicrmw are; visible at system scope. 2. s_waitcnt lgkmcnt(0) &; vmcnt(0). - If TgSplit execution mode,; omit lgkmcnt(0).; - If OpenCL, omit; lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0) and; s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/store/load; atomic/store; atomic/atomicrmw.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/at",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:261157,load,load,261157,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],['load']
Performance,"alN(Debug, ""g"", ""No optimizations, enable debugging""),; clEnumVal(O1 , ""Enable trivial optimizations""),; clEnumVal(O2 , ""Enable default optimizations""),; clEnumVal(O3 , ""Enable expensive optimizations"")));. ...; if (OptimizationLevel == Debug) outputDebugInfo(...);; ... By using the ""``clEnumValN``"" macro instead of ""``clEnumVal``"", we can directly; specify the name that the flag should get. In general a direct mapping is nice,; but sometimes you can't or don't want to preserve the mapping, which is when you; would use it. Named Alternatives; ------------------. Another useful argument form is a named alternative style. We shall use this; style in our compiler to specify different debug levels that can be used.; Instead of each debug level being its own switch, we want to support the; following options, of which only one can be specified at a time:; ""``--debug-level=none``"", ""``--debug-level=quick``"",; ""``--debug-level=detailed``"". To do this, we use the exact same format as our; optimization level flags, but we also specify an option name. For this case,; the code looks like this:. .. code-block:: c++. enum DebugLev {; nodebuginfo, quick, detailed; };. // Enable Debug Options to be specified on the command line; cl::opt<DebugLev> DebugLevel(""debug_level"", cl::desc(""Set the debugging level:""),; cl::values(; clEnumValN(nodebuginfo, ""none"", ""disable debug information""),; clEnumVal(quick, ""enable quick debug information""),; clEnumVal(detailed, ""enable detailed debug information"")));. This definition defines an enumerated command line variable of type ""``enum; DebugLev``"", which works exactly the same way as before. The difference here is; just the interface exposed to the user of your program and the help output by; the ""``-help``"" option:. ::. USAGE: compiler [options] <input file>. OPTIONS:; Choose optimization level:; -g - No optimizations, enable debugging; -O1 - Enable trivial optimizations; -O2 - Enable default optimizations; -O3 - Enable expensive optimizations; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst:17319,optimiz,optimization,17319,interpreter/llvm-project/llvm/docs/CommandLine.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandLine.rst,1,['optimiz'],['optimization']
Performance,"alculations:. **A**; Represents the addend used to compute the value of the relocatable field. If; the addend field is smaller than 64 bits then it is zero-extended to 64 bits; for use in the calculations below. (In practice this only affects ``_HI``; relocation types on Mesa/AMDPAL, where the addend comes from the 32-bit field; but the result of the calculation depends on the high part of the full 64-bit; address.). **G**; Represents the offset into the global offset table at which the relocation; entry's symbol will reside during execution. **GOT**; Represents the address of the global offset table. **P**; Represents the place (section offset for ``et_rel`` or address for ``et_dyn``); of the storage unit being relocated (computed using ``r_offset``). **S**; Represents the value of the symbol whose index resides in the relocation; entry. Relocations not using this must specify a symbol index of; ``STN_UNDEF``. **B**; Represents the base address of a loaded executable or shared object which is; the difference between the ELF address and the actual load address.; Relocations using this are only valid in executable or shared objects. The following relocation types are supported:. .. table:: AMDGPU ELF Relocation Records; :name: amdgpu-elf-relocation-records-table. ========================== ======= ===== ========== ==============================; Relocation Type Kind Value Field Calculation; ========================== ======= ===== ========== ==============================; ``R_AMDGPU_NONE`` 0 *none* *none*; ``R_AMDGPU_ABS32_LO`` Static, 1 ``word32`` (S + A) & 0xFFFFFFFF; Dynamic; ``R_AMDGPU_ABS32_HI`` Static, 2 ``word32`` (S + A) >> 32; Dynamic; ``R_AMDGPU_ABS64`` Static, 3 ``word64`` S + A; Dynamic; ``R_AMDGPU_REL32`` Static 4 ``word32`` S + A - P; ``R_AMDGPU_REL64`` Static 5 ``word64`` S + A - P; ``R_AMDGPU_ABS32`` Static, 6 ``word32`` S + A; Dynamic; ``R_AMDGPU_GOTPCREL`` Static 7 ``word32`` G + GOT + A - P; ``R_AMDGPU_GOTPCREL32_LO`` Static 8 ``word32`` (G + GOT +",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:80942,load,loaded,80942,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,['load'],"['load', 'loaded']"
Performance,"alf by refactoring the code reading and writing the TBasket data;; A single transient buffer holding the compressed data is now managed by TTree (and could be made thread local); rather than having one per TBranch. Updated TBranchElement::Unroll to no longer split a base class; that can not be split (i.e. respect the information returned; by TStreamerElement::CannotSplit (and thus TClass::CanSplit). This disabling is currently _not_ done automatically for backward compatibility reasons and because; ; Without TClass::SetCanSplit there was no way to; force the splitting (short of setting the split level lower); Some classes still requires a custom streamer solely to; read older data files (for example for file written before; the advent of StreamerInfo) and are such not necessary to; be used when writting (and schema evolution rules can not; yet be used in this case). Allowed removing branches when cloning a TNtuple. Added an option value (""cachedbranches"") to the Print() function of TTreeCache to be able to print the list of cached branches. Made the ownership of the TBranch by fBranch clearer (and thus allow the 'reuse' of TTree object without memory leak). Introduced GetLeaf(branchname,leafname) used in TTreeFormula to avoid ambiguity in the syntax introduced by too many slashes. Improved performance of TTree::GetEntry. With this changes the 'overhead'; compare to protobuf goes from 48% to 24%. (This does not include the; cost of the file opening which can be comparatively large for small; files. For the example used in the comparison the cost TFile::Open is 8% of the cost; of 100000 calls to TTree::GetEntry). Prevented the use of non-existent memory when reading in an object that is part of an STL collection and which used; to contains an embedded object (and this data member has been removed). Now properly recognize a TClonesArray data member even if the requested type was a typedef (to TClonesArray) that is in a namespace (for example edm::Event::ContaierType). ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v532/index.html:3645,perform,performance,3645,tree/doc/v532/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v532/index.html,2,['perform'],['performance']
Performance,"algorithm. This means that, for; example, a binary operator will stackify with its user before its operands.; However, if moving the binary operator to its user moves it to a place where; its operands can't be moved to, it would be better to leave it in place, or; perhaps move it up, so that it can stackify its operands. A binary operator; has two operands and one result, so in such cases there could be a net win by; preferring the operands. //===---------------------------------------------------------------------===//. Instruction ordering has a significant influence on register stackification and; coloring. Consider experimenting with the MachineScheduler (enable via; enableMachineScheduler) and determine if it can be configured to schedule; instructions advantageously for this purpose. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify currently assumes that the stack must be empty after; an instruction with no return values, however wasm doesn't actually require; this. WebAssemblyRegStackify could be extended, or possibly rewritten, to take; full advantage of what WebAssembly permits. //===---------------------------------------------------------------------===//. Add support for mergeable sections in the Wasm writer, such as for strings and; floating-point constants. //===---------------------------------------------------------------------===//. The function @dynamic_alloca_redzone in test/CodeGen/WebAssembly/userstack.ll; ends up with a local.tee in its prolog which has an unused result, requiring; an extra drop:. global.get $push8=, 0; local.tee $push9=, 1, $pop8; drop $pop9; [...]. The prologue code initially thinks it needs an FP register, but later it; turns out to be unneeded, so one could either approach this by being more; clever about not inserting code for an FP in the first place, or optimizing; away the copy later. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:7233,optimiz,optimizing,7233,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,2,['optimiz'],['optimizing']
Performance,"alid; pointer. The compiler may assume execution will continue after a volatile operation,; so operations which modify memory or may have undefined behavior can be; hoisted past a volatile operation. As an exception to the preceding rule, the compiler may not assume execution; will continue after a volatile store operation. This restriction is necessary; to support the somewhat common pattern in C of intentionally storing to an; invalid pointer to crash the program. In the future, it might make sense to; allow frontends to control this behavior. IR-level volatile loads and stores cannot safely be optimized into llvm.memcpy; or llvm.memmove intrinsics even when those intrinsics are flagged volatile.; Likewise, the backend should never split or merge target-legal volatile; load/store instructions. Similarly, IR-level volatile loads and stores cannot; change from integer to floating-point or vice versa. .. admonition:: Rationale. Platforms may rely on volatile loads and stores of natively supported; data width to be executed as single instruction. For example, in C; this holds for an l-value of volatile primitive type with native; hardware support, but not necessarily for aggregate types. The; frontend upholds these expectations, which are intentionally; unspecified in the IR. The rules above ensure that IR transformations; do not violate the frontend's contract with the language. .. _memmodel:. Memory Model for Concurrent Operations; --------------------------------------. The LLVM IR does not define any way to start parallel threads of; execution or to register signal handlers. Nonetheless, there are; platform-specific ways to create them, and we define LLVM IR's behavior; in their presence. This model is inspired by the C++ memory model. For a more informal introduction to this model, see the :doc:`Atomics`. We define a *happens-before* partial order as the least partial order; that. - Is a superset of single-thread program order, and; - When ``a`` *synchronizes-wit",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:148278,load,loads,148278,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loads']
Performance,"aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't expo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12438,load,load,12438,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,6,['load'],['load']
Performance,"alignment, the following convention applies:. - 1 = bottom. - 2 = centered. - 3 = top. For example, align: 11 = left adjusted and bottom adjusted; 32 = right; adjusted and vertically centered. #### Setting Text Angle. Use `TAttText::SetTextAngle` to set the text angle. The `angle` is the; degrees of the horizontal. ``` {.cpp}; root[] la->SetTextAngle(angle); ```. #### Setting Text Color. Use `TAttText::SetTextColor` to set the text color. The `color` is the; color index. The colors are described in ""Color and Color Palettes"". ``` {.cpp}; root[] la->SetTextColor(color); ```. #### Setting Text Font. Use `TAttText::SetTextFont` to set the font. The parameter font is the; font code, combining the font and precision:; `font = 10 * fontID + precision`. ``` {.cpp}; root[] la->SetTextFont(font); ```. The table below lists the available fonts. The font IDs must be between; 1 and 14. The precision can be:. - Precision = 0 fast hardware fonts (steps in the size). - Precision = 1 scalable and rotate-able hardware fonts (see below). - Precision = 2 scalable and rotate-able hardware fonts. When precision 0 is used, only the original non-scaled system fonts are; used. The fonts have a minimum (4) and maximum (37) size in pixels.; These fonts are fast and are of good quality. Their size varies with; large steps and they cannot be rotated. Precision 1 and 2 fonts have a; different behavior depending if True Type Fonts (TTF) are used or not.; If TTF are used, you always get very good quality scalable and; rotate-able fonts. However, TTF are slow. Precision 1 and 2 fonts have a; different behavior for PostScript in case of **`TLatex`** objects:. - With precision 1, the PostScript text uses the old convention (see; **`TPostScript`**) for some special characters to draw sub and; superscripts or Greek text. - With precision 2, the ""PostScript"" special characters are drawn as; such. To draw sub and superscripts it is highly recommended to use; **`TLatex`** objects instead. For example: `fo",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md:70674,scalab,scalable,70674,documentation/users-guide/Graphics.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Graphics.md,1,['scalab'],['scalable']
Performance,"aling bit, and; LLVM does not correctly represent this. See `issue #60796; <https://github.com/llvm/llvm-project/issues/60796>`_. .. _fastmath:. Fast-Math Flags; ---------------. LLVM IR floating-point operations (:ref:`fneg <i_fneg>`, :ref:`fadd <i_fadd>`,; :ref:`fsub <i_fsub>`, :ref:`fmul <i_fmul>`, :ref:`fdiv <i_fdiv>`,; :ref:`frem <i_frem>`, :ref:`fcmp <i_fcmp>`), :ref:`phi <i_phi>`,; :ref:`select <i_select>` and :ref:`call <i_call>`; may use the following flags to enable otherwise unsafe; floating-point transformations. ``nnan``; No NaNs - Allow optimizations to assume the arguments and result are not; NaN. If an argument is a nan, or the result would be a nan, it produces; a :ref:`poison value <poisonvalues>` instead. ``ninf``; No Infs - Allow optimizations to assume the arguments and result are not; +/-Inf. If an argument is +/-Inf, or the result would be +/-Inf, it; produces a :ref:`poison value <poisonvalues>` instead. ``nsz``; No Signed Zeros - Allow optimizations to treat the sign of a zero; argument or zero result as insignificant. This does not imply that -0.0; is poison and/or guaranteed to not exist in the operation. ``arcp``; Allow Reciprocal - Allow optimizations to use the reciprocal of an; argument rather than perform division. ``contract``; Allow floating-point contraction (e.g. fusing a multiply followed by an; addition into a fused multiply-and-add). This does not enable reassociating; to form arbitrary contractions. For example, ``(a*b) + (c*d) + e`` can not; be transformed into ``(a*b) + ((c*d) + e)`` to create two fma operations. .. _fastmath_afn:. ``afn``; Approximate functions - Allow substitution of approximate calculations for; functions (sin, log, sqrt, etc). See floating-point intrinsic definitions; for places where this can apply to LLVM's intrinsic math functions. ``reassoc``; Allow reassociation transformations for floating-point instructions.; This may dramatically change results in floating-point. ``fast``; This flag implies all o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:162266,optimiz,optimizations,162266,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizations']
Performance,"alizationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModule the OptimizeLayer will call our optimizeModule function before passing; the transformed module on to the CompileLayer below. Of course, we could have; called optimizeModule directly in our addModule function and not gone to the; bother of using the IRTransformLayer, but doing so gives us another opportunity; to see how layers compose. It also provides a neat entry point to the *layer*; concept itself, because IRTransformLayer is one of the simplest layers that; can be implemented. .. code-block:: c++. // From IRTransformLayer.h:; class IRTransformLayer : public IRLayer {; public:; using TransformFunction = std::function<Expected<ThreadSafeModule>(; ThreadSafeModule, const MaterializationResponsibility &R)>;. IRTransformLayer(ExecutionSession &ES, IRLayer &BaseLayer,; TransformFunction Transform = identityTransform);. void setTransform(TransformFunction Transform) {; this->Transform = std::move(Transform);; }. static ThreadSafeModule; identityTransform(ThreadSafeModule TSM,; const MaterializationResponsibility &R) {; return TSM;; }. void emit(MaterializationResponsibility R, ThreadSafeModule TSM) override;. private:; IRLayer &BaseLayer;; TransformFunction Transform;; };. // From IRTransformLayer.cpp:. IRTransfor",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:6126,optimiz,optimizeModule,6126,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,1,['optimiz'],['optimizeModule']
Performance,"alizedString(@""LocalizedString"", @"" ""); // warn; NSString *string3 = NSLocalizedStringWithDefaultValue(; @""LocalizedString"", nil, [[NSBundle alloc] init], nil,@""""); // warn; }. .. _optin-osx-cocoa-localizability-NonLocalizedStringChecker:. optin.osx.cocoa.localizability.NonLocalizedStringChecker (ObjC); """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Warns about uses of non-localized NSStrings passed to UI methods expecting localized NSStrings. .. code-block:: objc. NSString *alarmText =; NSLocalizedString(@""Enabled"", @""Indicates alarm is turned on"");; if (!isEnabled) {; alarmText = @""Disabled"";; }; UILabel *alarmStateLabel = [[UILabel alloc] init];. // Warning: User-facing text should use localized string macro; [alarmStateLabel setText:alarmText];. .. _optin-performance-GCDAntipattern:. optin.performance.GCDAntipattern; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; Check for performance anti-patterns when using Grand Central Dispatch. .. _optin-performance-Padding:. optin.performance.Padding; """"""""""""""""""""""""""""""""""""""""""""""""""; Check for excessively padded structs. .. _optin-portability-UnixAPI:. optin.portability.UnixAPI; """"""""""""""""""""""""""""""""""""""""""""""""""; Finds implementation-defined behavior in UNIX/Posix functions. .. _security-checkers:. security; ^^^^^^^^. Security related checkers. .. _security-cert-env-InvalidPtr:. security.cert.env.InvalidPtr; """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""". Corresponds to SEI CERT Rules `ENV31-C <https://wiki.sei.cmu.edu/confluence/display/c/ENV31-C.+Do+not+rely+on+an+environment+pointer+following+an+operation+that+may+invalidate+it>`_ and `ENV34-C <https://wiki.sei.cmu.edu/confluence/display/c/ENV34-C.+Do+not+store+pointers+returned+by+certain+functions>`_. * **ENV31-C**:; Rule is about the possible problem with ``main`` function's third argument, environment pointer,; ""envp"". When environment array is modified using some modification function; such as ``putenv``, ``setenv`` or others, It may happen that memory is reallocated,; however ""envp"" i",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst:19512,perform,performance,19512,interpreter/llvm-project/clang/docs/analyzer/checkers.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers.rst,1,['perform'],['performance']
Performance,"all optimization,; and omission of register saves and restores in some cases; it is not; checked or enforced when generating the callee. The parameter and the; function return type must be valid operands for the; :ref:`bitcast instruction <i_bitcast>`. This is not a valid attribute for; return values and can only be applied to one parameter. ``nonnull``; This indicates that the parameter or return pointer is not null. This; attribute may only be applied to pointer typed parameters. This is not; checked or enforced by LLVM; if the parameter or return pointer is null,; :ref:`poison value <poisonvalues>` is returned or passed instead.; The ``nonnull`` attribute should be combined with the ``noundef`` attribute; to ensure a pointer is not null or otherwise the behavior is undefined. ``dereferenceable(<n>)``; This indicates that the parameter or return pointer is dereferenceable. This; attribute may only be applied to pointer typed parameters. A pointer that; is dereferenceable can be loaded from speculatively without a risk of; trapping. The number of bytes known to be dereferenceable must be provided; in parentheses. It is legal for the number of bytes to be less than the; size of the pointee type. The ``nonnull`` attribute does not imply; dereferenceability (consider a pointer to one element past the end of an; array), however ``dereferenceable(<n>)`` does imply ``nonnull`` in; ``addrspace(0)`` (which is the default address space), except if the; ``null_pointer_is_valid`` function attribute is present.; ``n`` should be a positive number. The pointer should be well defined,; otherwise it is undefined behavior. This means ``dereferenceable(<n>)``; implies ``noundef``. ``dereferenceable_or_null(<n>)``; This indicates that the parameter or return value isn't both; non-null and non-dereferenceable (up to ``<n>`` bytes) at the same; time. All non-null pointers tagged with; ``dereferenceable_or_null(<n>)`` are ``dereferenceable(<n>)``.; For address space 0 ``dereferenceable_o",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:61308,load,loaded,61308,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['loaded']
Performance,"all sites, polling for a; safepoint, and emitting load and store barriers. You can also extend LLVM -; possibly through a loadable :ref:`code generation plugins <plugin>` - to; generate code and data structures which conforms to the *binary interface*; specified by the *runtime library*. This is similar to the relationship between; LLVM and DWARF debugging info, for example. The difference primarily lies in; the lack of an established standard in the domain of garbage collection --- thus; the need for a flexible extension mechanism. The aspects of the binary interface with which LLVM's GC support is; concerned are:. * Creation of GC safepoints within code where collection is allowed to execute; safely. * Computation of the stack map. For each safe point in the code, object; references within the stack frame must be identified so that the collector may; traverse and perhaps update them. * Write barriers when storing object references to the heap. These are commonly; used to optimize incremental scans in generational collectors. * Emission of read barriers when loading object references. These are useful; for interoperating with concurrent collectors. There are additional areas that LLVM does not directly address:. * Registration of global roots with the runtime. * Registration of stack map entries with the runtime. * The functions used by the program to allocate memory, trigger a collection,; etc. * Computation or compilation of type maps, or registration of them with the; runtime. These are used to crawl the heap for object references. In general, LLVM's support for GC does not include features which can be; adequately addressed with other features of the IR and does not specify a; particular binary interface. On the plus side, this means that you should be; able to integrate LLVM with an existing runtime. On the other hand, it can; have the effect of leaving a lot of work for the developer of a novel; language. We try to mitigate this by providing built in collector",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst:6894,optimiz,optimize,6894,interpreter/llvm-project/llvm/docs/GarbageCollection.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GarbageCollection.rst,1,['optimiz'],['optimize']
Performance,"all the required executables should be present in the Windows; **PATH** environment variable.; * In case of MSYS Git, choose the option ""Run Git from Windows; Command Prompt"" during installation. A good way to check if everything is detected properly by the script is to; run the following command:; ```sh; cd tools/packaging/; ./cpt.py --check-requirements; ```. #### Red Hat Linux (Fedora/Scientific Linux CERN); This section applies to all distros based on Red Hat Linux like Fedora, and; Scientific Linux CERN (SLC). Apparently, you can build RPM packages in any; distro regardless of the package manager it uses. This has been tested on; Fedora, SLC, Ubuntu, and CrunchBang. If you are interested, you can test it; on your favourite platform and email me the results. Depending on the package manager of your distro, you can install the; packages required by CPT to build RPM bundles. For a Red Hat based distro; (which uses ```yum``` package manager), you can use the following command; (also performed automatically by CPT):; ```sh; sudo yum update; sudo yum install git gcc gcc-c++ rpm-build python; ```. #### Mac OS X; Mac OS X provides a sane environement for CPT to build Apple Disk Images; (DMG Installers). On older versions of Mac OS, you need to update XCode to; get the latest version of Clang supporting c++11 features. A great package; manager for Mac OS X is [Macports]. It is recommended that you use the; packages provided by Macports for running CPT (or any other tool if that; is the case) rather than the ones which come pre-installed with Mac OS.; Assuming that you have Macports installed on your Mac, you can use the; following command to install the requisite packages (also done automatically; by CPT):; [Macports]:http://www.macports.org/; ```sh; sudo port -v selfupdate; sudo port install git g++ python; ```. ### Usage; ```sh; cd tools/packaging/; ```. ```; usage: cpt.py [-h] [-c] [--current-dev CURRENT_DEV]; [--last-stable LAST_STABLE] [--tarball-tag TARBALL_TAG]; [",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/packaging/README.md:5410,perform,performed,5410,interpreter/cling/tools/packaging/README.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/tools/packaging/README.md,1,['perform'],['performed']
Performance,"all. This more or less means; that the pointer is only dereferenced, and not returned from the function or; stored in a global. This pass is implemented as a bottom-up traversal of the; call-graph. ``globaldce``: Dead Global Elimination; --------------------------------------. This transform is designed to eliminate unreachable internal globals from the; program. It uses an aggressive algorithm, searching out globals that are known; to be alive. After it finds all of the globals which are needed, it deletes; whatever is left over. This allows it to delete recursive chunks of the; program which are unreachable. ``globalopt``: Global Variable Optimizer; ----------------------------------------. This pass transforms simple global variables that never have their address; taken. If obviously true, it marks read/write globals as constant, deletes; variables only stored to, etc. ``gvn``: Global Value Numbering; -------------------------------. This pass performs global value numbering to eliminate fully and partially; redundant instructions. It also performs redundant load elimination. .. _passes-indvars:. ``indvars``: Canonicalize Induction Variables; ---------------------------------------------. This transformation analyzes and transforms the induction variables (and; computations derived from them) into simpler forms suitable for subsequent; analysis and transformation. This transformation makes the following changes to each loop with an; identifiable induction variable:. * All loops are transformed to have a *single* canonical induction variable; which starts at zero and steps by one.; * The canonical induction variable is guaranteed to be the first PHI node in; the loop header block.; * Any pointer arithmetic recurrences are raised to use array subscripts. If the trip count of a loop is computable, this pass also makes the following; changes:. * The exit condition for the loop is canonicalized to compare the induction; value against the exit value. This turns loops l",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst:17381,perform,performs,17381,interpreter/llvm-project/llvm/docs/Passes.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Passes.rst,1,['perform'],['performs']
Performance,"alloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19633; We could eliminate the branch condition here, loading from null is undefined:. struct S { int w, x, y, z; };; struct T { int r; struct S s; };; void bar (struct S, int);; void foo (int a, struct T b); {; struct S *c = 0;; if (a); c = &b.s;; bar (*c, a);; }. //===---------------------------------------------------------------------===//. simplifylibcalls should do several optimizations for strspn/strcspn:. strcspn(x, ""a"") -> inlined loop for up to 3 letters (similarly for strspn):. size_t __strcspn_c3 (__const char *__s, int __reject1, int __reject2,; int __reject3) {; register size_t __result = 0;; while (__s[__result] != '\0' && __s[__result] != __reject1 &&; __s[__result] != __reject2 && __s[__result] != __reject3); ++__result;; return __result;; }. This should turn into a switch on the character. See PR3253 for some notes on; codegen. 456.hmmer apparently uses strcspn and strspn a lot. 471.omnetpp uses strspn. //===---------------------------------------------------------------------===//. simplifylibcalls should turn these snprintf idioms into memcpy (GCC PR47917). char buf1[6], buf2[6], buf3[4], buf4[4];; int i;. int foo (void) {; int ret = snprintf (buf1, sizeof buf1, ""abcde"");; ret += snprintf (buf2, sizeof buf2, ""abcdef"") * 16;; ret += snprintf (buf3, sizeof buf3, ""%s"", i++ < 6 ? ""abc"" : ""def"") * 256;; ret += snprintf (buf4, sizeof buf4, ""%s"",",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:36814,optimiz,optimizations,36814,interpreter/llvm-project/llvm/lib/Target/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt,2,['optimiz'],['optimizations']
Performance,"allocated for; the instruction during the register renaming stage. Load/Store Unit and Memory Consistency Model; """"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""; To simulate an out-of-order execution of memory operations, :program:`llvm-mca`; utilizes a simulated load/store unit (LSUnit) to simulate the speculative; execution of loads and stores. Each load (or store) consumes an entry in the load (or store) queue. Users can; specify flags ``-lqueue`` and ``-squeue`` to limit the number of entries in the; load and store queues respectively. The queues are unbounded by default. The LSUnit implements a relaxed consistency model for memory loads and stores.; The rules are:. 1. A younger load is allowed to pass an older load only if there are no; intervening stores or barriers between the two loads.; 2. A younger load is allowed to pass an older store provided that the load does; not alias with the store.; 3. A younger store is not allowed to pass an older store.; 4. A younger store is not allowed to pass an older load. By default, the LSUnit optimistically assumes that loads do not alias; (`-noalias=true`) store operations. Under this assumption, younger loads are; always allowed to pass older stores. Essentially, the LSUnit does not attempt; to run any alias analysis to predict when loads and stores do not alias with; each other. Note that, in the case of write-combining memory, rule 3 could be relaxed to; allow reordering of non-aliasing store operations. That being said, at the; moment, there is no way to further relax the memory model (``-noalias`` is the; only option). Essentially, there is no option to specify a different memory; type (e.g., write-back, write-combining, write-through; etc.) and consequently; to weaken, or strengthen, the memory model. Other limitations are:. * The LSUnit does not know when store-to-load forwarding may occur.; * The LSUnit does not know anything about cache hierarchy and memory types.; * The LSUnit does not know how to identify serial",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst:40140,load,load,40140,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-mca.rst,1,['load'],['load']
Performance,"allows the (re)use of temporary objects to store different data in the same buffer.; - Reuse branch proxies internally used by TTreeReader{Value,Array} therewith increasing performance when having multiple readers pointing to the same branch.; - Implement reading of objects data from JSON; - Provide TBufferJSON::ToJSON() and TBufferJSON::FromJSON() methods; - Provide TBufferXML::ToXML() and TBufferXML::FromXML() methods; - Converts NaN and Infinity values into null in JSON, there are no other direct equivalent. ## TTree Libraries; - Enable the TTreeCache by default of `TTree::Draw`, `TTreeReader` and `RDataFrame`; - Significant enhancement in the `TTreeCache` filling algorithm to increase robustness in case of oddly clustered `TTree` and under provisioned cache size. See the [merge request](https://github.com/root-project/root/pull/1960) for more details.; - Proxies are now properly re-used when multiple TTreeReader{Value,Array}s are associated to a single branch. Deserialisation is therefore performed once. This is an advantage for complex TDataFrame graphs.; - Add TBranch::BackFill to allow the addition of new branches to an existing tree and keep the new basket clustered in the same way as the rest of the TTree. Use with the following pattern,; make sure to to call BackFill for the same entry for all the branches consecutively:; ```; for(auto e = 0; e < tree->GetEntries(); ++e) { // loop over entries.; for(auto branch : branchCollection) {; ... Make change to the data associated with the branch ...; branch->BackFill();; }; }; ```; Since we loop over all the branches for each new entry all the baskets for a cluster are consecutive in the file. ### RDataFrame (formerly TDataFrame); #### Behaviour, interface and naming changes; - `TDataFrame` and `TDataSource` together with their federation of classes have been renamed according to the coding conventions for new interfaces and extracted from the `Experimental` namespace: they can now be found in the ROOT namespace an",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md:4942,perform,performed,4942,README/ReleaseNotes/v614/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v614/index.md,1,['perform'],['performed']
Performance,"ally computed to; fit the legend box. If `gStyle->GetLegendTextSize()` is non equal to 0 and if; text size directly set on the `TLegend` object is 0, then the `gStyle` value is; used to draw the legend text. If the text size directly set on the `TLegend`; object is not null, then it is used to draw the legend text. ### TTexDump. - The hollow fill style was not rendered correctly.; (see https://sft.its.cern.ch/jira/browse/ROOT-6841); - Better line width matching with screen and pdf output.; - Text color was ignored. It was always black.; - Text color was ignored. It was always black.; - The underscore `_` produced an error outside the TeX math context.; - Fix an issue with transparent pads.; - Implement transparent colors using TiKZ ""opacity"".; - Implement `TStyle::SetLineScalePS()` to control le global basic line width.; - Offer 0 as line width option. Useful to make a line invisible. ### TPostScript. - Small fix for fill patterns 1, 2 and 3.; - With `TMathtext`, only the fonts really used are now loaded in the PostScript; file. Typically it reduces the file size by a factor 10 (compare to the previous; implementation) for normal plots with math formulae and greek characters.; - Offer 0 as line width option. Useful to make a line invisible. ### TPDF. - When a text size was equal or smaller than 0 the PDF file was corrupted.; - Small fix for fill patterns 1, 2 and 3.; - When printing a coloured 2D histograms (with option COLZ) into a PDF or PostScript; file, the preview on screen using many standard PDF previewer tools showed very; thin white lines between the bins as well as in the color palette.; This made very ugly the final output.; This problem is due to bad implementation of anti-aliasing in these previewers.; A way to bypass this issue was to turn off the anti-aliasing in the previewer; but then the rest of the document does not look nice. This problem is now bypassed; with a fix in both PDF and PostScript output.; - Offer 0 as line width option. Useful to mak",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md:21831,load,loaded,21831,README/ReleaseNotes/v604/index.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/README/ReleaseNotes/v604/index.md,1,['load'],['loaded']
Performance,"ally invasive changes guarded under the HLSL language options. We will; seek to make HLSL language support as minimal a maintenance burden as possible. DXC Driver; ----------. A DXC driver mode will provide command-line compatibility with DXC, supporting; DXC's options and flags. The DXC driver is HLSL-specific and will create an; HLSLToolchain which will provide the basis to support targeting both DirectX and; Vulkan. Parser; ------. Following the examples of other parser extensions HLSL will add a ParseHLSL.cpp; file to contain the implementations of HLSL-specific extensions to the Clang; parser. The HLSL grammar shares most of its structure with C and C++, so we will; use the existing C/C++ parsing code paths. Sema; ----. HLSL's Sema implementation will also provide an ``ExternalSemaSource``. In DXC,; an ``ExternalSemaSource`` is used to provide definitions for HLSL built-in data; types and built-in templates. Clang is already designed to allow an attached; ``ExternalSemaSource`` to lazily complete data types, which is a **huge**; performance win for HLSL. If precompiled headers are used when compiling HLSL, the ``ExternalSemaSource``; will be a ``MultiplexExternalSemaSource`` which includes both the ``ASTReader``; and ``HLSLExternalSemaSource``. For Built-in declarations that are already; completed in the serialized AST, the ``HLSLExternalSemaSource`` will reuse the; existing declarations and not introduce new declarations. If the built-in types; are not completed in the serialized AST, the ``HLSLExternalSemaSource`` will; create new declarations and connect the de-serialized decls as the previous; declaration. CodeGen; -------. Like OpenCL, HLSL relies on capturing a lot of information into IR metadata.; *hand wave* *hand wave* *hand wave* As a design principle here we want our IR to; be idiomatic Clang IR as much as possible. We will use IR attributes wherever we; can, and use metadata as sparingly as possible. One example of a difference from; DXC already impl",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/HLSLSupport.rst:3624,perform,performance,3624,interpreter/llvm-project/clang/docs/HLSL/HLSLSupport.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/HLSLSupport.rst,1,['perform'],['performance']
Performance,"allyConsistent fences behave as both an Acquire and a; Release fence, and additionally provide a total ordering with some; complicated guarantees, see the C++ standard for details. Frontends generating atomic instructions generally need to be aware of the; target to some degree; atomic instructions are guaranteed to be lock-free, and; therefore an instruction which is wider than the target natively supports can be; impossible to generate. .. _Atomic orderings:. Atomic orderings; ================. In order to achieve a balance between performance and necessary guarantees,; there are six levels of atomicity. They are listed in order of strength; each; level includes all the guarantees of the previous level except for; Acquire/Release. (See also `LangRef Ordering <LangRef.html#ordering>`_.). .. _NotAtomic:. NotAtomic; ---------. NotAtomic is the obvious, a load or store which is not atomic. (This isn't; really a level of atomicity, but is listed here for comparison.) This is; essentially a regular load or store. If there is a race on a given memory; location, loads from that location return undef. Relevant standard; This is intended to match shared variables in C/C++, and to be used in any; other context where memory access is necessary, and a race is impossible. (The; precise definition is in `LangRef Memory Model <LangRef.html#memmodel>`_.). Notes for frontends; The rule is essentially that all memory accessed with basic loads and stores; by multiple threads should be protected by a lock or other synchronization;; otherwise, you are likely to run into undefined behavior. If your frontend is; for a ""safe"" language like Java, use Unordered to load and store any shared; variable. Note that NotAtomic volatile loads and stores are not properly; atomic; do not try to use them as a substitute. (Per the C/C++ standards,; volatile does provide some limited guarantees around asynchronous signals, but; atomics are generally a better solution.). Notes for optimizers; Introducing ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst:6150,load,load,6150,interpreter/llvm-project/llvm/docs/Atomics.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Atomics.rst,1,['load'],['load']
Performance,"als::addIntervalsForSpills`` in; ``lib/CodeGen/LiveIntervalAnalysis.cpp`` for an example of its use. Built in register allocators; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^. The LLVM infrastructure provides the application developer with three different; register allocators:. * *Fast* --- This register allocator is the default for debug builds. It; allocates registers on a basic block level, attempting to keep values in; registers and reusing registers as appropriate. * *Basic* --- This is an incremental approach to register allocation. Live; ranges are assigned to registers one at a time in an order that is driven by; heuristics. Since code can be rewritten on-the-fly during allocation, this; framework allows interesting allocators to be developed as extensions. It is; not itself a production register allocator but is a potentially useful; stand-alone mode for triaging bugs and as a performance baseline. * *Greedy* --- *The default allocator*. This is a highly tuned implementation of; the *Basic* allocator that incorporates global live range splitting. This; allocator works hard to minimize the cost of spill code. * *PBQP* --- A Partitioned Boolean Quadratic Programming (PBQP) based register; allocator. This allocator works by constructing a PBQP problem representing; the register allocation problem under consideration, solving this using a PBQP; solver, and mapping the solution back to a register assignment. The type of register allocator used in ``llc`` can be chosen with the command; line option ``-regalloc=...``:. .. code-block:: bash. $ llc -regalloc=linearscan file.bc -o ln.s; $ llc -regalloc=fast file.bc -o fa.s; $ llc -regalloc=pbqp file.bc -o pbqp.s. .. _Prolog/Epilog Code Insertion:. Prolog/Epilog Code Insertion; ----------------------------. .. note::. To Be Written. Compact Unwind; --------------. Throwing an exception requires *unwinding* out of a function. The information on; how to unwind a given function is traditionally expressed in DWARF unwind; (a.k.a. frame) ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst:70115,tune,tuned,70115,interpreter/llvm-project/llvm/docs/CodeGenerator.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CodeGenerator.rst,1,['tune'],['tuned']
Performance,"also be used on purely fixed; types. Scalable vectors can only be inserted into other scalable vectors. Arguments:; """""""""""""""""""". The ``vec`` is the vector which ``subvec`` will be inserted into.; The ``subvec`` is the vector that will be inserted. ``idx`` represents the starting element number at which ``subvec`` will be; inserted. ``idx`` must be a constant multiple of ``subvec``'s known minimum; vector length. If ``subvec`` is a scalable vector, ``idx`` is first scaled by; the runtime scaling factor of ``subvec``. The elements of ``vec`` starting at; ``idx`` are overwritten with ``subvec``. Elements ``idx`` through (``idx`` +; num_elements(``subvec``) - 1) must be valid ``vec`` indices. If this condition; cannot be determined statically but is false at runtime, then the result vector; is a :ref:`poison value <poisonvalues>`. '``llvm.vector.extract``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. ; Extract fixed type from scalable type; declare <4 x float> @llvm.vector.extract.v4f32.nxv4f32(<vscale x 4 x float> %vec, i64 <idx>); declare <2 x double> @llvm.vector.extract.v2f64.nxv2f64(<vscale x 2 x double> %vec, i64 <idx>). ; Extract scalable type from scalable type; declare <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv4f32(<vscale x 4 x float> %vec, i64 <idx>). ; Extract fixed type from fixed type; declare <2 x double> @llvm.vector.extract.v2f64.v4f64(<4 x double> %vec, i64 <idx>). Overview:; """""""""""""""""". The '``llvm.vector.extract.*``' intrinsics extract a vector from within another; vector starting from a given index. The return type must be explicitly; specified. Conceptually, this can be used to decompose a scalable vector into; non-scalable parts, however this intrinsic can also be used on purely fixed; types. Scalable vectors can only be extracted from other scalable vectors. Arguments:; """""""""""""""""""". The ``vec`` is the vector from which we will extract a subvector. The ``idx`` specifies the starting",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:664289,scalab,scalable,664289,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['scalab'],['scalable']
Performance,"alue is then passed by-address. For; output-only parameters, the temporary is uninitialized when passed (if the; parameter is not explicitly initialized inside the function an undefined value; is stored back to the argument expression). For parameters that are both input; and output, the temporary is initialized from the lvalue argument expression; through implicit or explicit casting from the lvalue argument type to the; parameter type. On return of the function, the values of any parameter temporaries are written; back to the argument expression through an inverted conversion sequence (if an; ``out`` parameter was not initialized in the function, the uninitialized value; may be written back). Parameters of constant-sized array type are also passed with value semantics.; This requires input parameters of arrays to construct temporaries and the; temporaries go through array-to-pointer decay when initializing parameters. Implementations are allowed to avoid unnecessary temporaries, and HLSL's strict; no-alias rules can enable some trivial optimizations. Array Temporaries; -----------------. Given the following example:. .. code-block:: c++. void fn(float a[4]) {; a[0] = a[1] + a[2] + a[3];; }. float4 main() : SV_Target {; float arr[4] = {1, 1, 1, 1};; fn(arr);; return float4(arr[0], arr[1], arr[2], arr[3]);; }. In C or C++, the array parameter decays to a pointer, so after the call to; ``fn``, the value of ``arr[0]`` is ``3``. In HLSL, the array is passed by value,; so modifications inside ``fn`` do not propagate out. .. note::. DXC may pass unsized arrays directly as decayed pointers, which is an; unfortunate behavior divergence. Out Parameter Temporaries; -------------------------. .. code-block:: c++. void Init(inout int X, inout int Y) {; Y = 2;; X = 1;; }. void main() {; int V;; Init(V, V); // MSVC (or clang-cl) V == 2, Clang V == 1; }. In the above example the ``Init`` function's behavior depends on the C++; implementation. C++ does not define the order in which",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/FunctionCalls.rst:2375,optimiz,optimizations,2375,interpreter/llvm-project/clang/docs/HLSL/FunctionCalls.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/HLSL/FunctionCalls.rst,1,['optimiz'],['optimizations']
Performance,"alue() << ""\n"";. Note that ``enumerate`` has ``zip_equal`` semantics and provides elements; through a 'reference wrapper' proxy, which makes them modifiable when accessed; through structured bindings or the ``value()`` member function. When two or more; ranges are passed, ``enumerate`` requires them to have equal lengths (checked; with an ``assert``). .. _debugging:. Debugging; =========. A handful of `GDB pretty printers; <https://sourceware.org/gdb/onlinedocs/gdb/Pretty-Printing.html>`__ are; provided for some of the core LLVM libraries. To use them, execute the; following (or add it to your ``~/.gdbinit``)::. source /path/to/llvm/src/utils/gdb-scripts/prettyprinters.py. It also might be handy to enable the `print pretty; <http://ftp.gnu.org/old-gnu/Manuals/gdb/html_node/gdb_57.html>`__ option to; avoid data structures being printed as a big block of text. .. _common:. Helpful Hints for Common Operations; ===================================. This section describes how to perform some very simple transformations of LLVM; code. This is meant to give examples of common idioms used, showing the; practical side of LLVM transformations. Because this is a ""how-to"" section, you should also read about the main classes; that you will be working with. The :ref:`Core LLVM Class Hierarchy Reference; <coreclasses>` contains details and descriptions of the main classes that you; should know about. .. _inspection:. Basic Inspection and Traversal Routines; ---------------------------------------. The LLVM compiler infrastructure have many different data structures that may be; traversed. Following the example of the C++ standard template library, the; techniques used to traverse these various data structures are all basically the; same. For an enumerable sequence of values, the ``XXXbegin()`` function (or; method) returns an iterator to the start of the sequence, the ``XXXend()``; function returns an iterator pointing to one past the last valid element of the; sequence, and there ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:103620,perform,perform,103620,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,1,['perform'],['perform']
Performance,"alue; being acquired. 4. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - local 1. s_waitcnt vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit.; - If OpenCL, omit.; - Could be split into; separate s_waitcnt; vmcnt(0) and s_waitcnt; vscnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - Must happen before; the following; store.; - Ensures that all; global memory; operations have; completed before; performing the; store that is being; released. 2. ds_atomic; 3. s_waitcnt lgkmcnt(0). - If OpenCL, omit.; - Must happen before; the following; buffer_gl0_inv.; - Ensures any; following global; data read is no; older than the local load; atomic value being; acquired. 4. buffer_gl0_inv. - If CU wavefront execution; mode, omit.; - If OpenCL omit.; - Ensures that; following; loads will not see; stale data. atomicrmw acq_rel - workgroup - generic 1. s_waitcnt lgkmcnt(0) &; vmcnt(0) & vscnt(0). - If CU wavefront execution; mode, omit vmcnt(0) and; vscnt(0).; - If OpenCL, omit lgkmcnt(0).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store; atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; atomicrmw.; - Ensures that all; memory op",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:365348,load,load,365348,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['load'],['load']
Performance,"always available and; accurate for reading (e.g., you get accurate stack traces despite tail call; elimination and inlining), but you might lose the ability to modify the program; and call functions which were optimized out of the program, or inlined away; completely. The :doc:`LLVM test-suite <TestSuiteMakefileGuide>` provides a framework to; test the optimizer's handling of debugging information. It can be run like; this:. .. code-block:: bash. % cd llvm/projects/test-suite/MultiSource/Benchmarks # or some other level; % make TEST=dbgopt. This will test impact of debugging information on optimization passes. If; debugging information influences optimization passes then it will be reported; as a failure. See :doc:`TestingGuide` for more information on LLVM test; infrastructure and how to run various tests. .. _format:. Debugging information format; ============================. LLVM debugging information has been carefully designed to make it possible for; the optimizer to optimize the program and debugging information without; necessarily having to know anything about debugging information. In; particular, the use of metadata avoids duplicated debugging information from; the beginning, and the global dead code elimination pass automatically deletes; debugging information for a function if it decides to delete the function. To do this, most of the debugging information (descriptors for types,; variables, functions, source files, etc) is inserted by the language front-end; in the form of LLVM metadata. Debug information is designed to be agnostic about the target debugger and; debugging information representation (e.g. DWARF/Stabs/etc). It uses a generic; pass to decode the information that represents variables, types, functions,; namespaces, etc: this allows for arbitrary source-language semantics and; type-systems to be used, as long as there is a module written for the target; debugger to interpret the information. To provide basic functionality, the LLVM debugge",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst:6385,optimiz,optimizer,6385,interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/SourceLevelDebugging.rst,2,['optimiz'],"['optimize', 'optimizer']"
Performance,"always get it right the first time, but we fix any problems; when we realize we made a mistake. Integration with IDEs. We believe that Integrated Development Environments (IDE's) are a great way; to pull together various pieces of the development puzzle, and aim to make clang; work well in such an environment. The chief advantage of an IDE is that they; typically have visibility across your entire project and are long-lived; processes, whereas stand-alone compiler tools are typically invoked on each; individual file in the project, and thus have limited scope.; There are many implications of this difference, but a significant one has to; do with efficiency and caching: sharing an address space across different files; in a project, means that you can use intelligent caching and other techniques to; dramatically reduce analysis/compilation time.; A further difference between IDEs and batch compiler is that they often; impose very different requirements on the front-end: they depend on high; performance in order to provide a ""snappy"" experience, and thus really want; techniques like ""incremental compilation"", ""fuzzy parsing"", etc. Finally, IDEs; often have very different requirements than code generation, often requiring; information that a codegen-only frontend can throw away. Clang is; specifically designed and built to capture this information. Use the LLVM 'Apache 2' License. We actively intend for clang (and LLVM as a whole) to be used for; commercial projects, not only as a stand-alone compiler but also as a library; embedded inside a proprietary application. We feel that the license encourages; contributors to pick up the source and work with it, and believe that those; individuals and organizations will contribute back their work if they do not; want to have to maintain a fork forever (which is time consuming and expensive; when merges are involved). Further, nobody makes money on compilers these days,; but many people need them to get bigger goals accomplished",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/features.html:10026,perform,performance,10026,interpreter/llvm-project/clang/www/features.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/www/features.html,2,['perform'],['performance']
Performance,"always inline attribute. ``Missed``. Remarks that describe an attempt to an optimization by the compiler that; could not be performed. :Example:. ::. foo not inlined into bar because it should never be inlined; (cost=never): noinline function attribute. ``Analysis``. Remarks that describe the result of an analysis, that can bring more; information to the user regarding the generated code. :Example:. ::. 16 stack bytes in function. ::. 10 instructions in function. Enabling optimization remarks; =============================. There are two modes that are supported for enabling optimization remarks in; LLVM: through remark diagnostics, or through serialized remarks. Remark diagnostics; ------------------. Optimization remarks can be emitted as diagnostics. These diagnostics will be; propagated to front-ends if desired, or emitted by tools like :doc:`llc; <CommandGuide/llc>` or :doc:`opt <CommandGuide/opt>`. .. option:: -pass-remarks=<regex>. Enables optimization remarks from passes whose name match the given (POSIX); regular expression. .. option:: -pass-remarks-missed=<regex>. Enables missed optimization remarks from passes whose name match the given; (POSIX) regular expression. .. option:: -pass-remarks-analysis=<regex>. Enables optimization analysis remarks from passes whose name match the given; (POSIX) regular expression. Serialized remarks; ------------------. While diagnostics are useful during development, it is often more useful to; refer to optimization remarks post-compilation, typically during performance; analysis. For that, LLVM can serialize the remarks produced for each compilation unit to; a file that can be consumed later. By default, the format of the serialized remarks is :ref:`YAML; <yamlremarks>`, and it can be accompanied by a :ref:`section <remarkssection>`; in the object files to easily retrieve it. :doc:`llc <CommandGuide/llc>` and :doc:`opt <CommandGuide/opt>` support the; following options:. ``Basic options``. .. option:: -pass-remarks-output",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst:1518,optimiz,optimization,1518,interpreter/llvm-project/llvm/docs/Remarks.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Remarks.rst,1,['optimiz'],['optimization']
Performance,"am may bitcast the; function pointer to a pointer to the constant's type and dereference; index -1. This implies that the IR symbol points just past the end of; the prefix data. For instance, take the example of a function annotated; with a single ``i32``,. .. code-block:: llvm. define void @f() prefix i32 123 { ... }. The prefix data can be referenced as,. .. code-block:: llvm. %a = getelementptr inbounds i32, ptr @f, i32 -1; %b = load i32, ptr %a. Prefix data is laid out as if it were an initializer for a global variable; of the prefix data's type. The function will be placed such that the; beginning of the prefix data is aligned. This means that if the size; of the prefix data is not a multiple of the alignment size, the; function's entrypoint will not be aligned. If alignment of the; function's entrypoint is desired, padding must be added to the prefix; data. A function may have prefix data but no body. This has similar semantics; to the ``available_externally`` linkage in that the data may be used by the; optimizers but will not be emitted in the object file. .. _prologuedata:. Prologue Data; -------------. The ``prologue`` attribute allows arbitrary code (encoded as bytes) to; be inserted prior to the function body. This can be used for enabling; function hot-patching and instrumentation. To maintain the semantics of ordinary function calls, the prologue data must; have a particular format. Specifically, it must begin with a sequence of; bytes which decode to a sequence of machine instructions, valid for the; module's target, which transfer control to the point immediately succeeding; the prologue data, without performing any other visible action. This allows; the inliner and other passes to reason about the semantics of the function; definition without needing to reason about the prologue data. Obviously this; makes the format of the prologue data highly target dependent. A trivial example of valid prologue data for the x86 architecture is ``i8 144``,; which ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:73880,optimiz,optimizers,73880,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['optimiz'],['optimizers']
Performance,"amdgcn_sched_group_barrier(2, 1, 0)``; | ``// 5 MFMA``; | ``__builtin_amdgcn_sched_group_barrier(8, 5, 0)``. llvm.amdgcn.iglp_opt An **experimental** intrinsic for instruction group level parallelism. The intrinsic; implements predefined intruction scheduling orderings. The intrinsic applies to the; surrounding scheduling region. The intrinsic takes a value that specifies the; strategy. The compiler implements two strategies. 0. Interleave DS and MFMA instructions for small GEMM kernels.; 1. Interleave DS and MFMA instructions for single wave small GEMM kernels. Only one iglp_opt intrinsic may be used in a scheduling region. The iglp_opt intrinsic; cannot be combined with sched_barrier or sched_group_barrier. The iglp_opt strategy implementations are subject to change. llvm.amdgcn.atomic.cond.sub.u32 Provides direct access to flat_atomic_cond_sub_u32, global_atomic_cond_sub_u32; and ds_cond_sub_u32 based on address space on gfx12 targets. This; performs subtraction only if the memory value is greater than or; equal to the data value. llvm.amdgcn.s.getpc Provides access to the s_getpc_b64 instruction, but with the return value; sign-extended from the width of the underlying PC hardware register even on; processors where the s_getpc_b64 instruction returns a zero-extended value. ============================================== ==========================================================. .. TODO::. List AMDGPU intrinsics. LLVM IR Attributes; ------------------. The AMDGPU backend supports the following LLVM IR attributes. .. table:: AMDGPU LLVM IR Attributes; :name: amdgpu-llvm-ir-attributes-table. ======================================= ==========================================================; LLVM Attribute Description; ======================================= ==========================================================; ""amdgpu-flat-work-group-size""=""min,max"" Specify the minimum and maximum flat work group sizes that; will be specified when the kernel is dispatched. Gene",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:45409,perform,performs,45409,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['perform'],['performs']
Performance,"ame>,<VersionNumber>); ```. The version number identifies this particular version of the class. When; a class has version 0 it is not stored in a root file but its base; class(es) is(are). The reason can be that this class has no data members; worth saving or all real info is in the base classes. The version number; is written to the file in the `Streamer` by the call; `TBuffer::WriteVersion`. You, as the designer of the class, do not need; to do any manual modification in the `Streamer`. ROOT schema evolution; mechanism is automatic and handled by the `StreamerInfo`. ### Manual Data Model Evolution Capabilities. The automatic data model schema evolution implemented in ROOT makes it possible; to read back the serialized data object in the situation when the definition of; the classes those objects represent changed slightly (some of the data members were; removed or some new ones added). It is also possible to manually specify the rules; for more sophisticated data transformations done while reading to load the serialized; objects into data structures that changed quite significantly. ROOT provides two interface enabling users to specify the conversion rules. The; first way is to define a rule in the dictionary file and the second way is to insert; it to the TClass object using the C++ API. There are two types of conversion rules. The first of them, the normal rules, are; the ones that should be used in the most of the cases. They provide a buffered input; data and an address of the in-memory target object and allow user to specify the; conversion function mapping the data being read to the output format. The second type; of the rules, the raw rules, also provide the pointer to the target object but the; input is a raw TBuffer object containing the input data member declared as an input; to the rule. This type of a rule is provided mainly to handle the file format changes; that couldn't have been handled otherwise and in general should not be used unless there; is no",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md:77010,load,load,77010,documentation/users-guide/InputOutput.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/InputOutput.md,1,['load'],['load']
Performance,"ame`` is ""``-``"" or omitted, :program:`llc` reads from standard input.; Otherwise, it will read from ``filename``. Inputs can be in either the LLVM; assembly language format (``.ll``) or the LLVM bitcode format (``.bc``). If the :option:`-o` option is omitted, then :program:`llc` will send its output; to standard output if the input is from standard input. If the :option:`-o`; option specifies ""``-``"", then the output will also be sent to standard output. If no :option:`-o` option is specified and an input file other than ""``-``"" is; specified, then :program:`llc` creates the output filename by taking the input; filename, removing any existing ``.bc`` extension, and adding a ``.s`` suffix. Other :program:`llc` options are described below. End-user Options; ~~~~~~~~~~~~~~~~. .. option:: -help. Print a summary of command line options. .. option:: -o <filename>. Use ``<filename>`` as the output filename. See the summary above for more; details. .. option:: -O=uint. Generate code at different optimization levels. These correspond to the; ``-O0``, ``-O1``, ``-O2``, and ``-O3`` optimization levels used by; :program:`clang`. .. option:: -mtriple=<target triple>. Override the target triple specified in the input file with the specified; string. .. option:: -march=<arch>. Specify the architecture for which to generate assembly, overriding the target; encoded in the input file. See the output of ``llc -help`` for a list of; valid architectures. By default this is inferred from the target triple or; autodetected to the current architecture. .. option:: -mcpu=<cpuname>. Specify a specific chip in the current architecture to generate code for.; By default this is inferred from the target triple and autodetected to; the current architecture. For a list of available CPUs, use:. .. code-block:: none. llvm-as < /dev/null | llc -march=xyz -mcpu=help. .. option:: -filetype=<output file type>. Specify what kind of output ``llc`` should generated. Options are: ``asm``; for textual assem",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst:1597,optimiz,optimization,1597,interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llc.rst,1,['optimiz'],['optimization']
Performance,"ameter and :ref:`return a; retained object <arc.object.operands.retained-return-values>`. Neither of; these properties can be altered through attributes. A call to an ``init`` method with a receiver that is either ``self`` (possibly; parenthesized or casted) or ``super`` is called a :arc-term:`delegate init; call`. It is an error for a delegate init call to be made except from an; ``init`` method, and excluding blocks within such methods. As an exception to the :ref:`usual rule <arc.misc.self>`, the variable ``self``; is mutable in an ``init`` method and has the usual semantics for a ``__strong``; variable. However, it is undefined behavior and the program is ill-formed, no; diagnostic required, if an ``init`` method attempts to use the previous value; of ``self`` after the completion of a delegate init call. It is conventional,; but not required, for an ``init`` method to return ``self``. It is undefined behavior for a program to cause two or more calls to ``init``; methods on the same object, except that each ``init`` method invocation may; perform at most one delegate init call. .. _arc.family.semantics.result_type:. Related result types; ^^^^^^^^^^^^^^^^^^^^. Certain methods are candidates to have :arc-term:`related result types`:. * class methods in the ``alloc`` and ``new`` method families; * instance methods in the ``init`` family; * the instance method ``self``; * outside of ARC, the instance methods ``retain`` and ``autorelease``. If the formal result type of such a method is ``id`` or protocol-qualified; ``id``, or a type equal to the declaring class or a superclass, then it is said; to have a related result type. In this case, when invoked in an explicit; message send, it is assumed to return a type related to the type of the; receiver:. * if it is a class method, and the receiver is a class name ``T``, the message; send expression has type ``T*``; otherwise; * if it is an instance method, and the receiver has type ``T``, the message; send expression has ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:74001,perform,perform,74001,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['perform']
Performance,"ample creates an icon object. First we create an object of type; **`TGPicture`**. The **`TGPicture`** objects are never created directly; by the application code. We call **`TGClient`** telling it the pixmap's; file name to create a **`TGPicture`** object and, in turn, it will; return a pointer to the created object. If the pixmap file cannot be; found the returned pointer will be `NULL`. As usual, the first parameter; of a **`TGIcon`** constructor is the parent frame. The second one is the; **`TGPicture`** object holding the pixmap we want to show. Last two; parameters define the width and height of pixmap in pixels. In the end; we add the created icon object to its parent. ``` {.cpp}; // icon widget; const TGPicture *ipic=(TGPicture *)gClient->GetPicture(""leaf.xpm"");; TGIcon *icon = new TGIcon(parent,ipic,40,40);; parent->AddFrame(icon,new TGLayoutHints(kLHintsLeft|kLHintsBottom,; 1, 15, 1, 1));; ```. The **`TGPicture`** objects are cached by **`TGClient`** in order to; keep the resource usage low and to improve the efficiency of the; client-server windowing systems. **`TGClient`** will check whether a; pixmap with the same name was already loaded before to register a new; picture object. If it finds it, it will return a pointer to the existing; object. Also, it will increase the usage counter for the object. All **`TGPicture`** objects are managed by the class; **`TGPicturePool`**. **`TGClient`** creates an object of this type upon; initialization. Normally your application program does not deal directly; with this class because all manipulations go through **`TGClient`**; class. Once you have finished with using of the **`TGPicture`** object, you; should call the method **`TGClient::FreePicture(const TGPicture *pic)`**; to free it. The usage counter of the picture object will be decreased; and when it reaches zero - the **`TGPicture`** object will be deleted. ### Status Bar. The status bar widget is used to display some information about the; current application ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md:90980,cache,cached,90980,documentation/users-guide/WritingGUI.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/WritingGUI.md,1,['cache'],['cached']
Performance,"ample_combined_GaussExample_model.root . Near term goals for HistFactory. Utilities for dealing with Monte Carlo statistical uncertainty in the template histograms; Support for N-D histograms; A new style of histogram variations without a constraint term attached (for shapes determined from control samples); XML steering for interpolation types. RooStats; General Improvements. This release brings several speed improvements to the RooStats tools and improved stability and performance with PROOF. This comes mainly through changes to the ToyMCSampler. In addition the HypoTestInverter tool has been rewritten, leading to some changes in the HypoTestResult. Finally, a new hypothesis test new called FrequentistCalculator was written, which plays the same role as the HybridCalculator but eliminates nuisance parameters in a frequentist way. ToyMCSampler. The primary interface for this class is to return a SamplingDistribution of a given TestStatistic.; The ToyMCSampler had a number of internal changes for improved performance with PROOF. These should be transparent. In addition, a new method was added RooAbsData* GenerateToyData(RooArgSet& paramPoint) that gives public access to the generation of toy data with all the same options for the treatment of nuisance parameters, binned or unbinned data, treatment of the global observables, importance sampling, etc. This is new method particularly useful for producing the expected limit bands where one needs to generate background-only pseudo-experiments in the same way that was used for the primary limit calculation. HypoTestResult. In the process of writing the new HypoTestInverter the conventions for p-values, CLb, CLs+b, and CLs were revisited. The situation is complicated by the fact that when performing a hypothesis test for discovery the null is background-only, but when performing an inverted hypothesis test the null is a signal+background model. The new convention is that the p-value for both the null and the alternate are ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v530/index.html:2031,perform,performance,2031,roofit/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/doc/v530/index.html,2,['perform'],['performance']
Performance,"an ``LLVMContext``. Entities in different contexts; *cannot* interact with each other: ``Module``\ s in different contexts cannot be; linked together, ``Function``\ s cannot be added to ``Module``\ s in different; contexts, etc. What this means is that is safe to compile on multiple; threads simultaneously, as long as no two threads operate on entities within the; same context. In practice, very few places in the API require the explicit specification of a; ``LLVMContext``, other than the ``Type`` creation/lookup APIs. Because every; ``Type`` carries a reference to its owning context, most other entities can; determine what context they belong to by looking at their own ``Type``. If you; are adding new entities to LLVM IR, please try to maintain this interface; design. .. _jitthreading:. Threads and the JIT; -------------------. LLVM's ""eager"" JIT compiler is safe to use in threaded programs. Multiple; threads can call ``ExecutionEngine::getPointerToFunction()`` or; ``ExecutionEngine::runFunction()`` concurrently, and multiple threads can run; code output by the JIT concurrently. The user must still ensure that only one; thread accesses IR in a given ``LLVMContext`` while another thread might be; modifying it. One way to do that is to always hold the JIT lock while accessing; IR outside the JIT (the JIT *modifies* the IR by adding ``CallbackVH``\ s).; Another way is to only call ``getPointerToFunction()`` from the; ``LLVMContext``'s thread. When the JIT is configured to compile lazily (using; ``ExecutionEngine::DisableLazyCompilation(false)``), there is currently a `race; condition <https://bugs.llvm.org/show_bug.cgi?id=5184>`_ in updating call sites; after a function is lazily-jitted. It's still possible to use the lazy JIT in a; threaded program if you ensure that only one thread at a time can call any; particular lazy stub and that the JIT lock guards any IR access, but we suggest; using only the eager JIT in threaded programs. .. _advanced:. Advanced Topics; ====",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst:124284,concurren,concurrently,124284,interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/ProgrammersManual.rst,2,['concurren'],['concurrently']
Performance,"an be asked for at a given moment, but; rather represented by the combination: current node/current global; matrix. However, physical nodes have unique ID's that can be retrieved; for a given modeller state. These can be fed back to the modeller in; order to force a physical node to become current. The advantage of this; comes from the fact that all navigation queries check first the current; node; therefore the location of a point in the geometry can be saved as; a starting state for later use. Nodes can be declared as `overlapping` in case they do overlap with; other nodes inside the same container or extrude this container (see; also ‘Checking the Geometry'). Non-overlapping nodes can be created; with:. ``` {.cpp}; TGeoVolume::AddNode(TGeoVolume *daughter,Int_t copy_No,; TGeoMatrix *matr);; ```. The creation of overlapping nodes can be done with a similar prototype:. ``` {.cpp}; TGeoVolume::AddNodeOverlap(/*same arguments*/);; ```. When closing the geometry, overlapping nodes perform a check of possible; overlaps with their neighbors. These are stored and checked all the time; during navigation; therefore, navigation is slower when embedding such; nodes into geometry. Nodes have visualization attributes as the volume; has. When undefined by users, painting a node on a pad will take the; corresponding volume attributes. ### Creating and Positioning Volumes. #### Making Volumes. As mentioned before, volumes are the basic objects used in building the; geometrical hierarchy. They represent objects that are not positioned,; but store all information about the placement of the other volumes they; may contain. Therefore a volume can be replicated several times in the; geometry. As it was explained, in order to create a volume, one has to; put together a shape and a medium, which are already defined. Volumes have to be named by users at creation time. Every different name; may represent a unique volume object, but may also represent more; general a family (class) of volum",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:62922,perform,perform,62922,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['perform']
Performance,"an be created; stand-alone:. ~~~ {.cpp}; TGeoBBox *box = new TGeoBBox(""s_box"",halfX,halfY,halfZ); // named; TGeoTube *tub = new TGeoTube(rmin,rmax,halfZ); // no name; //... (See all specific shape constructors); ~~~. Sometimes it is much easier to create a volume having a given shape in; one step, since shapes are not directly linked in the geometrical tree; but volumes are:. ~~~ {.cpp}; TGeoVolume *vol_box = gGeoManager->MakeBox(""BOX_VOL"",pmed,halfX,; halfY,halfZ);; TGeoVolume *vol_tub = gGeoManager->MakeTube(""TUB_VOL"",pmed,rmin,; rmax,halfZ);; // ...(See MakeXXX() utilities in TGeoManager class); ~~~. \anchor SHAPES04; ### Dividing Shapes. Shapes can generally be divided along a given axis. Supported axes are:; `X`, `Y`, `Z`, `Rxy`, `Phi`, `Rxyz`. A given shape cannot be divided; however on any axis. The general rule is that that divisions are; possible on whatever axis that produces still known shapes as slices.; The division of shapes are performed by the call `TGeoShape::Divide()`,; but this operation can be done only via `TGeoVolume::Divide()` method.; In other words, the algorithm for dividing a specific shape is known by; the shape object, but is always invoked in a generic way from the volume; level. Details on how to do that can be found in the paragraph ‘Dividing; volumes'. One can see how all division options are interpreted and which; their result inside specific shape classes is. \anchor SHAPES05; ### Parametric Shapes. Shapes generally have a set of parameters that is well defined at build; time. In fact, when the final geometrical hierarchy is assembled and the; geometry is closed, all constituent shapes `MUST`**have well defined and; valid parameters. In order to ease-up geometry creation, some; parameterizations are however allowed. For instance let's suppose that we need to define several volumes having; exactly the same properties but different sizes. A way to do this would; be to create as many different volumes and shapes. The modeller allows; h",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/shapes.md:7702,perform,performed,7702,geom/geom/doc/shapes.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/doc/shapes.md,1,['perform'],['performed']
Performance,"an be easily discharged by inspecting the loop index variables. #. In hot loops, consider duplicating instructions from small basic blocks; which end in highly predictable terminators into their successor blocks.; If a hot successor block contains instructions which can be vectorized; with the duplicated ones, this can provide a noticeable throughput; improvement. Note that this is not always profitable and does involve a; potentially large increase in code size. #. When checking a value against a constant, emit the check using a consistent; comparison type. The GVN pass *will* optimize redundant equalities even if; the type of comparison is inverted, but GVN only runs late in the pipeline.; As a result, you may miss the opportunity to run other important; optimizations. #. Avoid using arithmetic intrinsics unless you are *required* by your source; language specification to emit a particular code sequence. The optimizer; is quite good at reasoning about general control flow and arithmetic, it is; not anywhere near as strong at reasoning about the various intrinsics. If; profitable for code generation purposes, the optimizer will likely form the; intrinsics itself late in the optimization pipeline. It is *very* rarely; profitable to emit these directly in the language frontend. This item; explicitly includes the use of the :ref:`overflow intrinsics <int_overflow>`. #. Avoid using the :ref:`assume intrinsic <int_assume>` until you've; established that a) there's no other way to express the given fact and b); that fact is critical for optimization purposes. Assumes are a great; prototyping mechanism, but they can have negative effects on both compile; time and optimization effectiveness. The former is fixable with enough; effort, but the later is fairly fundamental to their designed purpose. Describing Language Specific Properties; =======================================. When translating a source language to LLVM, finding ways to express concepts; and guarantees availa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst:8901,optimiz,optimizer,8901,interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Frontend/PerformanceTips.rst,1,['optimiz'],['optimizer']
Performance,"an be reordered; relative to each other.; * A ``s_waitcnt vmcnt(0)`` is required to ensure synchronization between; vector memory operations of different CUs. It ensures a previous vector; memory operation has completed before executing a subsequent vector memory; or LDS operation and so can be used to meet the requirements of acquire and; release.; * An L2 cache can be kept coherent with other L2 caches by using the MTYPE RW; (read-write) for memory local to the L2, and MTYPE NC (non-coherent) with; the PTE C-bit set for memory not local to the L2. * Any local memory cache lines will be automatically invalidated by writes; from CUs associated with other L2 caches, or writes from the CPU, due to; the cache probe caused by the PTE C-bit.; * XGMI accesses from the CPU to local memory may be cached on the CPU.; Subsequent access from the GPU will automatically invalidate or writeback; the CPU cache due to the L2 probe filter.; * To ensure coherence of local memory writes of CUs with different L1 caches; in the same agent a ``buffer_wbl2`` is required. It does nothing if the; agent is configured to have a single L2, or will writeback dirty L2 cache; lines if configured to have multiple L2 caches.; * To ensure coherence of local memory writes of CUs in different agents a; ``buffer_wbl2 sc1`` is required. It will writeback dirty L2 cache lines.; * To ensure coherence of local memory reads of CUs with different L1 caches; in the same agent a ``buffer_inv sc1`` is required. It does nothing if the; agent is configured to have a single L2, or will invalidate non-local L2; cache lines if configured to have multiple L2 caches.; * To ensure coherence of local memory reads of CUs in different agents a; ``buffer_inv sc0 sc1`` is required. It will invalidate non-local L2 cache; lines if configured to have multiple L2 caches. * PCIe access from the GPU to the CPU can be kept coherent by using the MTYPE; UC (uncached) which bypasses the L2. Scalar memory operations are only used to a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:288843,cache,caches,288843,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,1,['cache'],['caches']
Performance,"an then query ``MemorySSA`` for things; like the dominance relation between ``MemoryAccess``\ es, and get the; ``MemoryAccess`` for any given ``Instruction`` . When ``MemorySSA`` is done building, it also hands you a ``MemorySSAWalker``; that you can use (see below). The walker; ----------. A structure that helps ``MemorySSA`` do its job is the ``MemorySSAWalker``, or; the walker, for short. The goal of the walker is to provide answers to clobber; queries beyond what's represented directly by ``MemoryAccess``\ es. For example,; given:. .. code-block:: llvm. define void @foo() {; %a = alloca i8; %b = alloca i8. ; 1 = MemoryDef(liveOnEntry); store i8 0, ptr %a; ; 2 = MemoryDef(1); store i8 0, ptr %b; }. The store to ``%a`` is clearly not a clobber for the store to ``%b``. It would; be the walker's goal to figure this out, and return ``liveOnEntry`` when queried; for the clobber of ``MemoryAccess`` ``2``. By default, ``MemorySSA`` provides a walker that can optimize ``MemoryDef``\ s; and ``MemoryUse``\ s by consulting whatever alias analysis stack you happen to; be using. Walkers were built to be flexible, though, so it's entirely reasonable; (and expected) to create more specialized walkers (e.g. one that specifically; queries ``GlobalsAA``, one that always stops at ``MemoryPhi`` nodes, etc). Default walker APIs; ^^^^^^^^^^^^^^^^^^^. There are two main APIs used to retrieve the clobbering access using the walker:. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA);`` return the; clobbering memory access for ``MA``, caching all intermediate results; computed along the way as part of each access queried. - ``MemoryAccess *getClobberingMemoryAccess(MemoryAccess *MA, const MemoryLocation &Loc);``; returns the access clobbering memory location ``Loc``, starting at ``MA``.; Because this API does not request the clobbering access of a specific memory; access, there are no results that can be cached. Locating clobbers yourself; ^^^^^^^^^^^^^^^^^^^^^^^^^^. If you ch",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst:9439,optimiz,optimize,9439,interpreter/llvm-project/llvm/docs/MemorySSA.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/MemorySSA.rst,1,['optimiz'],['optimize']
Performance,"ana Micro Systems. All instructions are prefixed with `vt.` as described in the specification, and the riscv-toolchain-convention document linked above. These instructions are only available for riscv64 at this time. ``XSfvcp``; LLVM implements `version 1.0.0 of the SiFive Vector Coprocessor Interface (VCIX) Software Specification <https://sifive.cdn.prismic.io/sifive/c3829e36-8552-41f0-a841-79945784241b_vcix-spec-software.pdf>`_ by SiFive. All instructions are prefixed with `sf.vc.` as described in the specification, and the riscv-toolchain-convention document linked above. ``XCVbitmanip``; LLVM implements `version 1.0.0 of the CORE-V Bit Manipulation custom instructions specification <https://github.com/openhwgroup/cv32e40p/blob/62bec66b36182215e18c9cf10f723567e23878e9/docs/source/instruction_set_extensions.rst>`_ by OpenHW Group. All instructions are prefixed with `cv.` as described in the specification. ``XCVelw``; LLVM implements `version 1.0.0 of the CORE-V Event load custom instructions specification <https://github.com/openhwgroup/cv32e40p/blob/master/docs/source/instruction_set_extensions.rst>`_ by OpenHW Group. All instructions are prefixed with `cv.` as described in the specification. These instructions are only available for riscv32 at this time. ``XCVmac``; LLVM implements `version 1.0.0 of the CORE-V Multiply-Accumulate (MAC) custom instructions specification <https://github.com/openhwgroup/cv32e40p/blob/4f024fe4b15a68b76615b0630c07a6745c620da7/docs/source/instruction_set_extensions.rst>`_ by OpenHW Group. All instructions are prefixed with `cv.mac` as described in the specification. These instructions are only available for riscv32 at this time. ``XCVmem``; LLVM implements `version 1.0.0 of the CORE-V Post-Increment load and stores custom instructions specification <https://github.com/openhwgroup/cv32e40p/blob/master/docs/source/instruction_set_extensions.rst>`_ by OpenHW Group. All instructions are prefixed with `cv.` as described in the specification",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/RISCVUsage.rst:18118,load,load,18118,interpreter/llvm-project/llvm/docs/RISCVUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/RISCVUsage.rst,1,['load'],['load']
Performance,"analogous rule applies to ``T&`` and ``U&`` in Objective-C++. .. admonition:: Rationale. These rules provide a reasonable level of type-safety for indirect pointers,; as long as the underlying memory is not deallocated. The conversion to; ``const __unsafe_unretained`` is permitted because the semantics of reads are; equivalent across all these ownership semantics, and that's a very useful and; common pattern. The interconversion with ``void*`` is useful for allocating; memory or otherwise escaping the type system, but use it carefully.; ``reinterpret_cast`` is considered to be an obvious enough sign of taking; responsibility for any problems. It is undefined behavior to access an ownership-qualified object through an; lvalue of a differently-qualified type, except that any non-``__weak`` object; may be read through an ``__unsafe_unretained`` lvalue. It is undefined behavior if the storage of a ``__strong`` or ``__weak``; object is not properly initialized before the first managed operation; is performed on the object, or if the storage of such an object is freed; or reused before the object has been properly deinitialized. Storage for; a ``__strong`` or ``__weak`` object may be properly initialized by filling; it with the representation of a null pointer, e.g. by acquiring the memory; with ``calloc`` or using ``bzero`` to zero it out. A ``__strong`` or; ``__weak`` object may be properly deinitialized by assigning a null pointer; into it. A ``__strong`` object may also be properly initialized; by copying into it (e.g. with ``memcpy``) the representation of a; different ``__strong`` object whose storage has been properly initialized;; doing this properly deinitializes the source object and causes its storage; to no longer be properly initialized. A ``__weak`` object may not be; representation-copied in this way. These requirements are followed automatically for objects whose; initialization and deinitialization are under the control of ARC:. * objects of static, automa",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst:45158,perform,performed,45158,interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/AutomaticReferenceCounting.rst,1,['perform'],['performed']
Performance,"analyzer.llvm.org/annotations.html>`_. Extensions for Dynamic Analysis; ===============================. Use ``__has_feature(address_sanitizer)`` to check if the code is being built; with :doc:`AddressSanitizer`. Use ``__has_feature(thread_sanitizer)`` to check if the code is being built; with :doc:`ThreadSanitizer`. Use ``__has_feature(memory_sanitizer)`` to check if the code is being built; with :doc:`MemorySanitizer`. Use ``__has_feature(dataflow_sanitizer)`` to check if the code is being built; with :doc:`DataFlowSanitizer`. Use ``__has_feature(safe_stack)`` to check if the code is being built; with :doc:`SafeStack`. Extensions for selectively disabling optimization; =================================================. Clang provides a mechanism for selectively disabling optimizations in functions; and methods. To disable optimizations in a single function definition, the GNU-style or C++11; non-standard attribute ``optnone`` can be used. .. code-block:: c++. // The following functions will not be optimized.; // GNU-style attribute; __attribute__((optnone)) int foo() {; // ... code; }; // C++11 attribute; [[clang::optnone]] int bar() {; // ... code; }. To facilitate disabling optimization for a range of function definitions, a; range-based pragma is provided. Its syntax is ``#pragma clang optimize``; followed by ``off`` or ``on``. All function definitions in the region between an ``off`` and the following; ``on`` will be decorated with the ``optnone`` attribute unless doing so would; conflict with explicit attributes already present on the function (e.g. the; ones that control inlining). .. code-block:: c++. #pragma clang optimize off; // This function will be decorated with optnone.; int foo() {; // ... code; }. // optnone conflicts with always_inline, so bar() will not be decorated.; __attribute__((always_inline)) int bar() {; // ... code; }; #pragma clang optimize on. If no ``on`` is found to close an ``off`` region, the end of the region is the; end of the com",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst:158840,optimiz,optimized,158840,interpreter/llvm-project/clang/docs/LanguageExtensions.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/LanguageExtensions.rst,1,['optimiz'],['optimized']
Performance,"ance to next boundary is:. ``` {.cpp}; TGeoNode *TGeoManager::FindNextBoundary(stepmax, path); ```. The output node returned by the method is the object which shape; boundary will be crossed first. The distance to the next crossing can be; retrieved after the call:. ``` {.cpp}; Double_t TGeoManager::GetStep(); ```. - The main input parameter is `stepmax,` which act as a trigger for; different features. The absolute value of this parameter represents; the step value proposed by the user. The algorithm will never try o; search for boundaries further than this distance. In case no; boundary is found the returned node will be the current one and the; computed step to boundary will be equal to abs (`stepmax`) having; the meaning *""step approved""*. The default value for `stepmax` is; `TGeoShape::Big `with the meaning that boundaries are looked for; without limitation. ![Finding the distance to the next crossed boundary](pictures/080001E8.png). According the values of the input parameters the method will perform; additional optional tasks:. **`|stepmax| < `** ***`TGeoShape::Big()`*** **` `**. The safe distance in the current volume is also computed. Moving the; particle from its current location with this distance in any direction; is safe in the sense that will never change the current state. **`stepmax < 0`**. The global matrix for the object that will have the next crossed; boundary is also computed. This can be retrieved for masterlocal point; or vector conversions: **`TGeoManager`**::`GetNextMatrix`(). In case the computation of the normal vector to the next crossed surface; is required, using a negative stepmax value is recommended. In this case; one can subsequently call a method for fast normal computation:. ``` {.cpp}; Double_t *TGeoManager::FindNormalFast(); ```. **`path `** **` 0`**. In case a path to a given physical object is specified, the distance to; its boundary is computed ignoring the rest of the geometry. #### Output Values. `TGeoManager::GetStep()`: di",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md:159973,perform,perform,159973,documentation/users-guide/Geometry.md,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/documentation/users-guide/Geometry.md,1,['perform'],['perform']
Performance,"ance. What does affect performance is the overhead to cross the language barrier.; This consists of unboxing Python objects to extract or convert the underlying; objects or data to something that matches what C++ expects; overload; resolution based on the unboxed arguments; offset calculations; and finally; the actual dispatch.; As a practical matter, overload resolution is the most costly part, followed; by the unboxing and conversion.; Best performance is achieved by specialization of the paths through the; run-time: recognize early the case at hand and select an optimized path.; For that reason, `PyPy`_ is so fast: JIT-ed traces operate on unboxed objects; and resolved overloads are baked into the trace, incurring no further cost.; Similarly, this is why pybind11 is so slow: its code generation is the C++; compiler's template engine, so complex path selection and specialization is; very hard to do in a performance-portable way. In cppyy, a great deal of attention has gone into built-in specialization; paths, which drives its performance.; For example, basic inheritance sequentially lines up classes, whereas; multiple (virtual) inheritance usually requires thunks.; Thus, when calling base class methods on a derived instance, the latter; requires offset calculations that depend on that instance, whereas the former; has fixed offsets fully determined by the class definitions themselves.; By labeling classes appropriately, single inheritance classes (by far the; most common case) do not incur the overhead in PyPy's JIT-ed traces that is; otherwise unavoidable for multiple virtual inheritance.; As another example, consider that the C++ standard does not allow modifying; a ``std::vector`` while looping over it, whereas Python has no such; restriction, complicating loops.; Thus, cppyy has specialized ``std::vector`` iteration for both PyPy and; CPython, easily outperforming looping over an equivalent numpy array. In CPython, the performance of `non-overloaded` function ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst:2775,perform,performance,2775,bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/philosophy.rst,1,['perform'],['performance']
Performance,"anch); and - twice the requeste basket size (TBranch::GetBasketSize).; the size of the buffer is reduced to the max of; 'the data in the current basket' and 'the average' and the requested; buffer size and aligned to next highest multiple of 512.; In TBranchRef distinguish between the entry we need (now called RequestedEntry) and the; entry we have read (fReadEntry) so that we can avoid re-reading the same entry too many; times when executing TRef::GetObject.; Reduce by 40% the time taken GetEntry for a branch created using a leaflist (exclusive of the decompression time).; Introduce TVirtualPerfStats::FileUnzipEvent to be able to keep track of the cost of unzipping and use this in TTreePerfStats and TBasket ... This give a good picture of where the time in unzip or in unstreaming; Add more clusters to the TTreeCache buffer until fBufferMinSize is hit to avoid severely underfilled buffer when; a low number of branches is selected/used.; When reading backwards, make sure to load a full (new) cluster and several other fixes to TTreeCache.; Reduce the memory used by a TTree in half. Refactor the code reading and writing the TBasket data.; A single transient buffer holding the compressed data is now managed by TTree (and could be made thread local); rather than having one per TBranch.; In TTree::Fill, call FlushBasket before calling OptimizeBaskets so that we have a correct; and accurate value of fTotBytes to use as the requested memory.; In TTree::OptimizeBasket enforces hard minimun for the basket size (no lower than the; estimate size of one entry in the branch and no lower than 8 bytes). TTree::Process. Add support for the flag TSelector::kAbortFile. TTree::Draw. The line width setting was missing in a few places.; Namely support the option 'a' for TGraphs in TTree::Draw (delegate the axis management to the TGraph object). TTreeSQL. Allow TTreeSQL to see temporary tables.; Avoid creating the unnecessary array fEntryOffset ... which when its content is always set to z",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v530/index.html:2002,load,load,2002,tree/doc/v530/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v530/index.html,2,['load'],['load']
Performance,anch; kInternalError (-4) : Internal error (could not find the type corresponding to a data type number.; kMissingCompiledCollectionProxy (-3) : Missing compiled collection proxy for a compiled collection.; kMismatch (-2) : Non-Class Pointer type given does not match the type expected by the branch.; kClassMismatch (-1) : Class Pointer type given does not match the type expected by the branch.; kMatch (0) : perfect match.; kMatchConversion (1) : match with (I/O) conversion.; kMatchConversionCollection (2) : match with (I/O) conversion of the content of a collection.; kMakeClass (3) : MakeClass mode so we can not check.; kVoidPtr (4) : void* passed so no check was made.; kNoCheck (5) : Underlying TBranch not yet available so no check was made. Insure that the TTreeCloner (fast merging) is able to also copy 'uninitialized' TStreamerInfo describing abstract classes.; Repair several use case of splitting collection of pointers (especially when their split level is 1).; Several run-time performance improvements.; In TTree::Fill use fZipBytes instead of fTotBytes for deciding when to flush or autosave.; Properly handle TTree aliases containing array indices.; Fix the default sorting order of baskets when the TTree is an older in-memory TTree.; Enhance the sort order to use the 'entry number' when the seek position are equal.; Consequently the default sort order for an older in-memory TTree is now; essentially kSortBasketsByEntry rather than kSortBasketsByBranch (old 'correct' sort; order) or 'random' (the 'broken' sort order prior to this release). IMPORTANT enhancement in TTree::Fill:; Slides from a recent seminar describing the main features of ROOT IO and Trees and the recent; improvements described below are available at; http://root.cern/files/brun_lcgapp09.pptx ; or; http://root.cern/files/brun_lcgapp09.pdf .; The baskets are flushed and the Tree header saved at regular intervals (See AutoFlush and OptimizeBaskets); When the amount of data written so far (fTotBytes),MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html:2569,perform,performance,2569,tree/doc/v526/index.html,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/doc/v526/index.html,2,['perform'],['performance']
Performance,"and LLVM with Profile-Guided Optimizations; =============================================================. Introduction; ============. PGO (Profile-Guided Optimization) allows your compiler to better optimize code; for how it actually runs. Users report that applying this to Clang and LLVM can; decrease overall compile time by 20%. This guide walks you through how to build Clang with PGO, though it also applies; to other subprojects, such as LLD. If you want to build other software with PGO, see the `end-user documentation; for PGO <https://clang.llvm.org/docs/UsersManual.html#profile-guided-optimization>`_. Using preconfigured CMake caches; ================================. See https://llvm.org/docs/AdvancedBuilds.html#multi-stage-pgo. Using the script; ================. We have a script at ``utils/collect_and_build_with_pgo.py``. This script is; tested on a few Linux flavors, and requires a checkout of LLVM, Clang, and; compiler-rt. Despite the name, it performs four clean builds of Clang, so it; can take a while to run to completion. Please see the script's ``--help`` for; more information on how to run it, and the different options available to you.; If you want to get the most out of PGO for a particular use-case (e.g. compiling; a specific large piece of software), please do read the section below on; 'benchmark' selection. Please note that this script is only tested on a few Linux distros. Patches to; add support for other platforms, as always, are highly appreciated. :). This script also supports a ``--dry-run`` option, which causes it to print; important commands instead of running them. Selecting 'benchmarks'; ======================. PGO does best when the profiles gathered represent how the user plans to use the; compiler. Notably, highly accurate profiles of llc building x86_64 code aren't; incredibly helpful if you're going to be targeting ARM. By default, the script above does two things to get solid coverage. It:. - runs all of Clang and LLVM's lit te",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst:1052,perform,performs,1052,interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HowToBuildWithPGO.rst,1,['perform'],['performs']
Performance,"and bundles <assume_opbundles>`. Arguments:; """""""""""""""""""". The argument of the call is the condition which the optimizer may assume is; always true. Semantics:; """""""""""""""""""". The intrinsic allows the optimizer to assume that the provided condition is; always true whenever the control flow reaches the intrinsic call. No code is; generated for this intrinsic, and instructions that contribute only to the; provided condition are not used for code generation. If the condition is; violated during execution, the behavior is undefined. Note that the optimizer might limit the transformations performed on values; used by the ``llvm.assume`` intrinsic in order to preserve the instructions; only used to form the intrinsic's input argument. This might prove undesirable; if the extra information provided by the ``llvm.assume`` intrinsic does not cause; sufficient overall improvement in code quality. For this reason,; ``llvm.assume`` should not be used to document basic mathematical invariants; that the optimizer can otherwise deduce or facts that are of little use to the; optimizer. .. _int_ssa_copy:. '``llvm.ssa.copy``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare type @llvm.ssa.copy(type returned %operand) memory(none). Arguments:; """""""""""""""""""". The first argument is an operand which is used as the returned value. Overview:; """""""""""""""""""". The ``llvm.ssa.copy`` intrinsic can be used to attach information to; operations by copying them and giving them new names. For example,; the PredicateInfo utility uses it to build Extended SSA form, and; attach various forms of information to operands that dominate specific; uses. It is not meant for general use, only for building temporary; renaming forms that require value splits at certain points. .. _type.test:. '``llvm.type.test``' Intrinsic; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """""""""""""". ::. declare i1 @llvm.type.test(ptr %ptr, metadata %type) nounwind memory(none). Arguments:; """""""""""""""""""". The first argument",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:936403,optimiz,optimizer,936403,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,2,['optimiz'],['optimizer']
Performance,"and is the explicit vector length of the; operation. Semantics:; """""""""""""""""""". The '``llvm.vp.is.fpclass``' intrinsic performs llvm.is.fpclass (:ref:`llvm.is.fpclass <llvm.is.fpclass>`). Examples:; """""""""""""""""". .. code-block:: llvm. %r = call <2 x i1> @llvm.vp.is.fpclass.v2f16(<2 x half> %x, i32 3, <2 x i1> %m, i32 %evl); %t = call <vscale x 2 x i1> @llvm.vp.is.fpclass.nxv2f16(<vscale x 2 x half> %x, i32 3, <vscale x 2 x i1> %m, i32 %evl). .. _int_mload_mstore:. Masked Vector Load and Store Intrinsics; ---------------------------------------. LLVM provides intrinsics for predicated vector load and store operations. The predicate is specified by a mask operand, which holds one bit per vector element, switching the associated vector lane on or off. The memory addresses corresponding to the ""off"" lanes are not accessed. When all bits of the mask are on, the intrinsic is identical to a regular vector load or store. When all bits are off, no memory is accessed. .. _int_mload:. '``llvm.masked.load.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. The loaded data is a vector of any integer, floating-point or pointer data type. ::. declare <16 x float> @llvm.masked.load.v16f32.p0(ptr <ptr>, i32 <alignment>, <16 x i1> <mask>, <16 x float> <passthru>); declare <2 x double> @llvm.masked.load.v2f64.p0(ptr <ptr>, i32 <alignment>, <2 x i1> <mask>, <2 x double> <passthru>); ;; The data is a vector of pointers; declare <8 x ptr> @llvm.masked.load.v8p0.p0(ptr <ptr>, i32 <alignment>, <8 x i1> <mask>, <8 x ptr> <passthru>). Overview:; """""""""""""""""". Reads a vector from memory according to the provided mask. The mask holds a bit for each vector lane, and is used to prevent memory accesses to the masked-off lanes. The masked-off lanes in the result vector are taken from the corresponding lanes of the '``passthru``' operand. Arguments:; """""""""""""""""""". The first operand is the base pointer for the load. The second operand is the alignment of the ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:843145,load,load,843145,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['load'],['load']
Performance,"and matches the script name. Sometimes a test absolutely depends on hand-written assertions and should not; have assertions automatically generated. In that case, add the text ``NOTE: Do; not autogenerate`` to the first line, and the scripts will skip that test. It; is a good idea to explain why generated assertions will not work for the test; so future developers will understand what is going on. These are the most common scripts and their purposes/applications in generating; assertions:. .. code-block:: none. update_analyze_test_checks.py; opt -passes='print<cost-model>'. update_cc_test_checks.py; C/C++, or clang/clang++ (IR checks). update_llc_test_checks.py; llc (assembly checks). update_mca_test_checks.py; llvm-mca. update_mir_test_checks.py; llc (MIR checks). update_test_checks.py; opt. Precommit workflow for tests; ----------------------------. If the test does not crash, assert, or infinite loop, commit the test with; baseline check-lines first. That is, the test will show a miscompile or; missing optimization. Add a ""TODO"" or ""FIXME"" comment to indicate that; something is expected to change in a test. A follow-up patch with code changes to the compiler will then show check-line; differences to the tests, so it is easier to see the effect of the patch.; Remove TODO/FIXME comments added in the previous step if a problem is solved. Baseline tests (no-functional-change or NFC patch) may be pushed to main; without pre-commit review if you have commit access. Best practices for regression tests; -----------------------------------. - Use auto-generated check lines (produced by the scripts mentioned above); whenever feasible.; - Include comments about what is tested/expected in a particular test. If there; are relevant issues in the bug tracker, add references to those bug reports; (for example, ""See PR999 for more details"").; - Avoid undefined behavior and poison/undef values unless necessary. For; example, do not use patterns like ``br i1 undef``, which are likel",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestingGuide.rst:12922,optimiz,optimization,12922,interpreter/llvm-project/llvm/docs/TestingGuide.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TestingGuide.rst,1,['optimiz'],['optimization']
Performance,"and out of memory errors. To use the 64-bit ""; ""host compiler, pass -Thost=x64 on the CMake command line.""); endif(). if (CMAKE_GENERATOR STREQUAL ""Xcode"" AND NOT CMAKE_OSX_ARCHITECTURES); # Some CMake features like object libraries get confused if you don't; # explicitly specify an architecture setting with the Xcode generator.; set(CMAKE_OSX_ARCHITECTURES ""x86_64""); endif(). project(LLVM; VERSION ${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH}; LANGUAGES C CXX ASM). if (NOT DEFINED CMAKE_INSTALL_LIBDIR AND DEFINED LLVM_LIBDIR_SUFFIX); # Must go before `include(GNUInstallDirs)`.; set(CMAKE_INSTALL_LIBDIR ""lib${LLVM_LIBDIR_SUFFIX}""); endif(). # Must go after `DEFINED LLVM_LIBDIR_SUFFIX` check.; set(LLVM_LIBDIR_SUFFIX """" CACHE STRING ""Define suffix of library directory name (32/64)"" ). # Must go after `project(..)`.; include(GNUInstallDirs). # This C++ standard is required to build LLVM.; set(LLVM_REQUIRED_CXX_STANDARD 17). # If we find that the cache contains CMAKE_CXX_STANDARD it means that it's a old CMakeCache.txt; # and we can just inform the user and then reset it.; if($CACHE{CMAKE_CXX_STANDARD} AND $CACHE{CMAKE_CXX_STANDARD} LESS ${LLVM_REQUIRED_CXX_STANDARD}); message(WARNING ""Resetting cache value for CMAKE_CXX_STANDARD to ${LLVM_REQUIRED_CXX_STANDARD}""); unset(CMAKE_CXX_STANDARD CACHE); endif(). # if CMAKE_CXX_STANDARD is still set after the cache unset above it means that the user requested it; # and we allow it to be set to something newer than the required standard but otherwise we fail.; if(DEFINED CMAKE_CXX_STANDARD AND CMAKE_CXX_STANDARD LESS ${LLVM_REQUIRED_CXX_STANDARD}); message(FATAL_ERROR ""Requested CMAKE_CXX_STANDARD=${CMAKE_CXX_STANDARD} which is less than the required ${LLVM_REQUIRED_CXX_STANDARD}.""); endif(). set(CMAKE_CXX_STANDARD ${LLVM_REQUIRED_CXX_STANDARD} CACHE STRING ""C++ standard to conform to""); set(CMAKE_CXX_STANDARD_REQUIRED YES). if (CYGWIN); # Cygwin is a bit stricter and lack things like 'strdup', 'stricmp', etc",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt:2657,cache,cache,2657,interpreter/llvm-project/llvm/CMakeLists.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/CMakeLists.txt,1,['cache'],['cache']
Performance,"and regression tests and other sanity checks on LLVM infrastructure. These; are intended to run quickly and cover a lot of territory without being exhaustive. ``test-suite``; --------------. A comprehensive correctness, performance, and benchmarking test suite; for LLVM. This comes in a ``separate git repository; <https://github.com/llvm/llvm-test-suite>``, because it contains a; large amount of third-party code under a variety of licenses. For; details see the :doc:`Testing Guide <TestingGuide>` document. .. _tools:. ``llvm/tools``; --------------. Executables built out of the libraries; above, which form the main part of the user interface. You can always get help; for a tool by typing ``tool_name -help``. The following is a brief introduction; to the most important tools. More detailed information is in; the `Command Guide <CommandGuide/index.html>`_. ``bugpoint``. ``bugpoint`` is used to debug optimization passes or code generation backends; by narrowing down the given test case to the minimum number of passes and/or; instructions that still cause a problem, whether it is a crash or; miscompilation. See `<HowToSubmitABug.html>`_ for more information on using; ``bugpoint``. ``llvm-ar``. The archiver produces an archive containing the given LLVM bitcode files,; optionally with an index for faster lookup. ``llvm-as``. The assembler transforms the human readable LLVM assembly to LLVM bitcode. ``llvm-dis``. The disassembler transforms the LLVM bitcode to human readable LLVM assembly. ``llvm-link``. ``llvm-link``, not surprisingly, links multiple LLVM modules into a single; program. ``lli``. ``lli`` is the LLVM interpreter, which can directly execute LLVM bitcode; (although very slowly...). For architectures that support it (currently x86,; Sparc, and PowerPC), by default, ``lli`` will function as a Just-In-Time; compiler (if the functionality was compiled in), and will execute the code; *much* faster than the interpreter. ``llc``. ``llc`` is the LLVM backend compiler",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst:38906,optimiz,optimization,38906,interpreter/llvm-project/llvm/docs/GettingStarted.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/GettingStarted.rst,1,['optimiz'],['optimization']
Performance,"and vice versa.; It can also reduce or remove temporaries, especially for template; expressions.; Thus, there can be significant speedups for mixed code, beyond the Numba; compilation of Python code itself.; The current implementation integrates compiled C++ through function pointers,; object pointers, and pointer offsets, into the intermediate representation; (IR) as generated by Numba.; A future version may integrate Cling-generated IR directly into Numba IR (or; vice versa), e.g. if the C++ code is exposed from (precompiled) headers.; This would allow inlining of C++ code into Numba traces, for further; expected speedups. Why Numba?; ----------. The advertised premise of Numba is that it ""makes Python code fast.""; However, there is a much more compelling reason: Numba allows developers to; stay in their chosen ecosystem, be it Python or C++, in mixed environments,; without paying for their choice in lost performance.; For example, a Python developer using Numba does not need to rewrite a kernel; into C++ just to run performantly in a C++ framework.; Similarly, a C++ developer can use Numba to compile and create function; pointers to Python code for easy, performant, access.; This becomes even more compelling if the deployment target is a GPU, which; would otherwise certainly require a rewrite of the Python code.; Add that Numba, as a JIT-compiler, is fully run-time just like ``cppyy``,; and the use case for integration is clear.; (Numba does not currently provide support for C++.). Usage; -------. ``cppyy`` does not use Numba extension hooks to minimize accidental; dependencies.; Instead, it requires that the extensions are loaded explicitly by any code; that uses it::. import cppyy.numba_ext. After that, Numba is able to trace ``cppyy`` bound code when applying the; usual ``numba.njit`` decorator. Numba type declarations are done lazily, with the ``numba_ext`` module only; initially registering hooks on proxy base classes, to keep overheads in; Numba's type-reso",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/numba.rst:1759,perform,performantly,1759,bindings/pyroot/cppyy/cppyy/doc/source/numba.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy/doc/source/numba.rst,1,['perform'],['performantly']
Performance,"and-assemble-snippet|assemble-measured-code|measure]. By default, when `-mode=` is specified, the generated snippet will be executed; and measured, and that requires that we are running on the hardware for which; the snippet was generated, and that supports performance measurements.; However, it is possible to stop at some stage before measuring. Choices are:; * ``prepare-snippet``: Only generate the minimal instruction sequence.; * ``prepare-and-assemble-snippet``: Same as ``prepare-snippet``, but also dumps an excerpt of the sequence (hex encoded).; * ``assemble-measured-code``: Same as ``prepare-and-assemble-snippet``. but also creates the full sequence that can be dumped to a file using ``--dump-object-to-disk``.; * ``measure``: Same as ``assemble-measured-code``, but also runs the measurement. .. option:: --x86-lbr-sample-period=<nBranches/sample>. Specify the LBR sampling period - how many branches before we take a sample.; When a positive value is specified for this option and when the mode is `latency`,; we will use LBRs for measuring.; On choosing the ""right"" sampling period, a small value is preferred, but throttling; could occur if the sampling is too frequent. A prime number should be used to; avoid consistently skipping certain blocks. .. option:: --x86-disable-upper-sse-registers. Using the upper xmm registers (xmm8-xmm15) forces a longer instruction encoding; which may put greater pressure on the frontend fetch and decode stages,; potentially reducing the rate that instructions are dispatched to the backend,; particularly on older hardware. Comparing baseline results with this mode; enabled can help determine the effects of the frontend and can be used to; improve latency and throughput estimates. .. option:: --repetition-mode=[duplicate|loop|min]. Specify the repetition mode. `duplicate` will create a large, straight line; basic block with `num-repetitions` instructions (repeating the snippet; `num-repetitions`/`snippet size` times). `loop` will, opti",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst:11543,latency,latency,11543,interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/CommandGuide/llvm-exegesis.rst,1,['latency'],['latency']
Performance,"and; Sequencing of Instructions"".; 4. Scheduling for reduced register pressure. E.g. ""Minimum Register; Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------===//. Should we promote i16 to i32 to avoid partial register update stalls?. //===---------------------------------------------------------------------===//. Leave any_extend as pseudo instruction and hint to register; allocator. Delay codegen until post register allocation.; Note. any_extend is now turned into an INSERT_SUBREG. We still need to teach; the coalescer how to deal with it though. //===---------------------------------------------------------------------===//. It appears icc use push for parameter passing. Need to investigate. //===---------------------------------------------------------------------===//. The instruction selector sometimes misses folding a load into a compare. The; pattern is written as (cmp reg, (load p)). Because the compare isn't; commutative, it is not matched with the load on both sides. The dag combiner; should be made smart enough to canonicalize the load into the RHS of a compare; when it can invert the result of the compare for free. //===---------------------------------------------------------------------===//. In many cases, LLVM generates code like this:. _test:; movl 8(%esp), %eax; cmpl %eax, 4(%esp); setl %al; movzbl %al, %eax; ret. on some processors (which ones?), it is more efficient to do this:. _test:; movl 8(%esp), %ebx; xor %eax, %eax; cmpl %ebx, 4(%esp); setl %al; ret. Doing this correctly is tricky though, as the xor clobbers the flags. //===---------------------------------------------------------------------===//. We should generate bts/btr/etc instructions on targets where they are cheap or; when codesize is important. e.g., for:. void setbit(int *target, int bit) {; *target |= ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:2680,load,load,2680,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt,2,['load'],['load']
Performance,"and; address space is; not generic, omit; lgkmcnt(0).; - If OpenCL and; address space is; local, omit; vmcnt(0) and vscnt(0).; - However, since LLVM; currently has no; address space on; the fence need to; conservatively; always generate; (see comment for; previous fence).; - Could be split into; separate s_waitcnt; vmcnt(0), s_waitcnt; vscnt(0) and s_waitcnt; lgkmcnt(0) to allow; them to be; independently moved; according to the; following rules.; - s_waitcnt vmcnt(0); must happen after; any preceding; global/generic; load/load; atomic/; atomicrmw-with-return-value.; - s_waitcnt vscnt(0); must happen after; any preceding; global/generic; store/store atomic/; atomicrmw-no-return-value.; - s_waitcnt lgkmcnt(0); must happen after; any preceding; local/generic; load/store/load; atomic/store; atomic/atomicrmw.; - Must happen before; the following; buffer_gl*_inv.; - Ensures that the; preceding; global/local/generic; load; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; acquire-fence-paired-atomic); has completed; before invalidating; the caches. This; satisfies the; requirements of; acquire.; - Ensures that all; previous memory; operations have; completed before a; following; global/local/generic; store; atomic/atomicrmw; with an equal or; wider sync scope; and memory ordering; stronger than; unordered (this is; termed the; release-fence-paired-atomic).; This satisfies the; requirements of; release. 2. buffer_gl0_inv;; buffer_gl1_inv. - Must happen before; any following; global/generic; load/load; atomic/store/store; atomic/atomicrmw.; - Ensures that; following loads; will not see stale; global data. This; satisfies the; requirements of; acquire. **Sequential Consistent Atomic**; ------------------------------------------------------------------------------------; load atomic seq_cst - singlethread - global *Same as corresponding; - wavefront - local load atomic acquire,; - generic except must gene",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst:372719,load,load,372719,interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/AMDGPUUsage.rst,2,"['cache', 'load']","['caches', 'load']"
Performance,"andler:; d->~D(); // destruct D as it goes out of scope when entering catch clauses; goto TryHandler. In general, TryHandler is not the same as bazHandler, because multiple ; function calls could be made from the try block. In this case, trivial ; optimization could merge the two basic blocks. TryHandler is the code ; that actually determines the type of exception, based on the Exception object; itself. For this discussion, assume that the exception object contains *at; least*:. 1. A pointer to the RTTI info for the contained object; 2. A pointer to the dtor for the contained object; 3. The contained object itself. Note that it is necessary to maintain #1 & #2 in the exception object itself; because objects without virtual function tables may be thrown (as in this ; example). Assuming this, TryHandler would look something like this:. TryHandler: ; Exception *E = getThreadLocalException();; switch (E->RTTIType) {; case IntRTTIInfo:; ...int Stuff... // The action to perform from the catch block; break;; case DoubleRTTIInfo:; ...double Stuff... // The action to perform from the catch block; goto TryCleanup // This catch block rethrows the exception; break; // Redundant, eliminated by the optimizer; default:; goto TryCleanup // Exception not caught, rethrow; }. // Exception was consumed; if (E->dtor); E->dtor(E->object) // Invoke the dtor on the object if it exists; goto EndTry // Continue mainline code... And that is all there is to it. The throw(E) function would then be implemented like this (which may be ; inlined into the caller through standard optimization):. function throw(Exception *E) {; // Get the start of the stack trace...; %frame %f = call getStackCurrentFrame(). // Get the label information that corresponds to it; label * %L = call getFrameLabel(%f); while (%L == 0 && !isFirstFrame(%f)) {; // Loop until a cleanup handler is found; %f = call getNextFrame(%f); %L = call getFrameLabel(%f); }. if (%L != 0) {; call setThreadLocalException(E) // Allow handlers a",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:5913,perform,perform,5913,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,1,['perform'],['perform']
Performance,"anes. %masked.a = select <4 x i1> %mask, <4 x i8> %a, <4 x i8> <i8 -128, i8 -128, i8 -128, i8 -128>; %reduction = call i8 @llvm.vector.reduce.smax.v4i8(<4 x i8> %masked.a); %also.r = call i8 @llvm.smax.i8(i8 %reduction, i8 %start). .. _int_vp_reduce_smin:. '``llvm.vp.reduce.smin.*``' Intrinsics; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^. Syntax:; """"""""""""""; This is an overloaded intrinsic. ::. declare i32 @llvm.vp.reduce.smin.v4i32(i32 <start_value>, <4 x i32> <val>, <4 x i1> <mask>, i32 <vector_length>); declare i16 @llvm.vp.reduce.smin.nxv8i16(i16 <start_value>, <vscale x 8 x i16> <val>, <vscale x 8 x i1> <mask>, i32 <vector_length>). Overview:; """""""""""""""""". Predicated signed-integer ``MIN`` reduction of a vector and a scalar starting; value, returning the result as a scalar. Arguments:; """""""""""""""""""". The first operand is the start value of the reduction, which must be a scalar; integer type equal to the result type. The second operand is the vector on; which the reduction is performed and must be a vector of integer values whose; element type is the result/start type. The third operand is the vector mask and; is a vector of boolean values with the same number of elements as the vector; operand. The fourth operand is the explicit vector length of the operation. Semantics:; """""""""""""""""""". The '``llvm.vp.reduce.smin``' intrinsic performs the signed-integer ``MIN``; reduction (:ref:`llvm.vector.reduce.smin <int_vector_reduce_smin>`) of the; vector operand ``val`` on each enabled lane, and taking the minimum of that and; the scalar ``start_value``. Disabled lanes are treated as containing the; neutral value ``INT_MAX`` (i.e. having no effect on the reduction operation).; If the vector length is zero, the result is the start value. To ignore the start value, the neutral value can be used. Examples:; """""""""""""""""". .. code-block:: llvm. %r = call i8 @llvm.vp.reduce.smin.v4i8(i8 %start, <4 x i8> %a, <4 x i1> %mask, i32 %evl); ; %r is equivalent to %also.r, where lanes greater than or equ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst:765521,perform,performed,765521,interpreter/llvm-project/llvm/docs/LangRef.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/LangRef.rst,1,['perform'],['performed']
Performance,ang-tidy/objc/ForbiddenSubclassingCheck.h; clang-tools-extra/clang-tidy/objc/MissingHashCheck.cpp; clang-tools-extra/clang-tidy/objc/MissingHashCheck.h; clang-tools-extra/clang-tidy/objc/NSInvocationArgumentLifetimeCheck.cpp; clang-tools-extra/clang-tidy/objc/NSInvocationArgumentLifetimeCheck.h; clang-tools-extra/clang-tidy/objc/PropertyDeclarationCheck.h; clang-tools-extra/clang-tidy/objc/SuperSelfCheck.cpp; clang-tools-extra/clang-tidy/objc/SuperSelfCheck.h; clang-tools-extra/clang-tidy/openmp/ExceptionEscapeCheck.cpp; clang-tools-extra/clang-tidy/openmp/ExceptionEscapeCheck.h; clang-tools-extra/clang-tidy/openmp/OpenMPTidyModule.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.h; clang-tools-extra/clang-tidy/performance/FasterStringFindCheck.cpp; clang-tools-extra/clang-tidy/performance/ForRangeCopyCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.h; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleC,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:64909,perform,performance,64909,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,ang-tools-extra/clang-tidy/objc/DeallocInCategoryCheck.cpp; clang-tools-extra/clang-tidy/objc/DeallocInCategoryCheck.h; clang-tools-extra/clang-tidy/objc/ForbiddenSubclassingCheck.h; clang-tools-extra/clang-tidy/objc/MissingHashCheck.cpp; clang-tools-extra/clang-tidy/objc/MissingHashCheck.h; clang-tools-extra/clang-tidy/objc/NSInvocationArgumentLifetimeCheck.cpp; clang-tools-extra/clang-tidy/objc/NSInvocationArgumentLifetimeCheck.h; clang-tools-extra/clang-tidy/objc/PropertyDeclarationCheck.h; clang-tools-extra/clang-tidy/objc/SuperSelfCheck.cpp; clang-tools-extra/clang-tidy/objc/SuperSelfCheck.h; clang-tools-extra/clang-tidy/openmp/ExceptionEscapeCheck.cpp; clang-tools-extra/clang-tidy/openmp/ExceptionEscapeCheck.h; clang-tools-extra/clang-tidy/openmp/OpenMPTidyModule.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.cpp; clang-tools-extra/clang-tidy/openmp/UseDefaultNoneCheck.h; clang-tools-extra/clang-tidy/performance/FasterStringFindCheck.cpp; clang-tools-extra/clang-tidy/performance/ForRangeCopyCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.h; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtr,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:64773,perform,performance,64773,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,ang-tools-extra/clang-tidy/performance/ForRangeCopyCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.h; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.h; clang-tools-extra/clang-tidy/performance/UnnecessaryCopyInitialization.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.h; clang-tools-extra/clang-tidy/plugin/ClangTidyPlugin.cpp; clang-tools-extra/clang-tidy/portability/PortabilityTidyModule.cpp; clang-tools-extra/clang-tidy/portability/RestrictSystemIncludesCheck.cpp; clang-tools-extra/clang-tidy/portability/SIMDIntrinsicsCheck.cpp; clang-tools-extra/clang-tidy/readability/AvoidConstParamsInDecls.h; clang-tools-extra/clang-tidy/readability/BracesAroundStat,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:65749,perform,performance,65749,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,ang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientAlgorithmCheck.h; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.h; clang-tools-extra/clang-tidy/performance/UnnecessaryCopyInitialization.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.h; clang-tools-extra/clang-tidy/plugin/ClangTidyPlugin.cpp; clang-tools-extra/clang-tidy/portability/PortabilityTidyModule.cpp; clang-tools-extra/clang-tidy/portability/RestrictSystemIncludesCheck.cpp; clang-tools-extra/clang-tidy/portability/SIMDIntrinsicsCheck.cpp; clang-tools-extra/clang-tidy/readability/AvoidConstParamsInDecls.h; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.cpp; clang-tools-extra/clang-tidy/readability/Braces,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:65809,perform,performance,65809,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,ang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.cpp; clang-tools-extra/clang-tidy/performance/InefficientStringConcatenationCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstArgCheck.h; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.cpp; clang-tools-extra/clang-tidy/performance/MoveConstructorInitCheck.h; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.cpp; clang-tools-extra/clang-tidy/performance/NoAutomaticMoveCheck.h; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.cpp; clang-tools-extra/clang-tidy/performance/NoexceptMoveConstructorCheck.h; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.cpp; clang-tools-extra/clang-tidy/performance/NoIntToPtrCheck.h; clang-tools-extra/clang-tidy/performance/PerformanceTidyModule.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.cpp; clang-tools-extra/clang-tidy/performance/TriviallyDestructibleCheck.h; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.cpp; clang-tools-extra/clang-tidy/performance/TypePromotionInMathFnCheck.h; clang-tools-extra/clang-tidy/performance/UnnecessaryCopyInitialization.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.cpp; clang-tools-extra/clang-tidy/performance/UnnecessaryValueParamCheck.h; clang-tools-extra/clang-tidy/plugin/ClangTidyPlugin.cpp; clang-tools-extra/clang-tidy/portability/PortabilityTidyModule.cpp; clang-tools-extra/clang-tidy/portability/RestrictSystemIncludesCheck.cpp; clang-tools-extra/clang-tidy/portability/SIMDIntrinsicsCheck.cpp; clang-tools-extra/clang-tidy/readability/AvoidConstParamsInDecls.h; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.cpp; clang-tools-extra/clang-tidy/readability/BracesAroundStatementsCheck.h; clang-tools-extra/clang-tidy/readability/ConstReturnTypeCheck.cpp; clang-tools-extra/clang-tidy/readability/ContainerC,MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt:65950,perform,performance,65950,interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/tools/clang-formatted-files.txt,1,['perform'],['performance']
Performance,"ang/examples/PrintFunctionNames/PrintFunctionNames.cpp>`_. Running the plugin; ==================. Using the compiler driver; --------------------------. The Clang driver accepts the `-fplugin` option to load a plugin.; Clang plugins can receive arguments from the compiler driver command; line via the `fplugin-arg-<plugin name>-<argument>` option. Using this; method, the plugin name cannot contain dashes itself, but the argument; passed to the plugin can. .. code-block:: console. $ export BD=/path/to/build/directory; $ make -C $BD CallSuperAttr; $ clang++ -fplugin=$BD/lib/CallSuperAttr.so \; -fplugin-arg-call_super_plugin-help \; test.cpp. If your plugin name contains dashes, either rename the plugin or used the; cc1 command line options listed below. Using the cc1 command line; --------------------------. To run a plugin, the dynamic library containing the plugin registry must be; loaded via the `-load` command line option. This will load all plugins; that are registered, and you can select the plugins to run by specifying the; `-plugin` option. Additional parameters for the plugins can be passed with; `-plugin-arg-<plugin-name>`. Note that those options must reach clang's cc1 process. There are two; ways to do so:. * Directly call the parsing process by using the `-cc1` option; this; has the downside of not configuring the default header search paths, so; you'll need to specify the full system path configuration on the command; line.; * Use clang as usual, but prefix all arguments to the cc1 process with; `-Xclang`. For example, to run the ``print-function-names`` plugin over a source file in; clang, first build the plugin, and then call clang with the plugin from the; source tree:. .. code-block:: console. $ export BD=/path/to/build/directory; $ (cd $BD && make PrintFunctionNames ); $ clang++ -D_GNU_SOURCE -D_DEBUG -D__STDC_CONSTANT_MACROS \; -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -D_GNU_SOURCE \; -I$BD/tools/clang/include -Itools/clang/include -I$BD/includ",MatchSource.DOCS,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangPlugins.rst:5297,load,load,5297,interpreter/llvm-project/clang/docs/ClangPlugins.rst,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/ClangPlugins.rst,1,['load'],['load']
