quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Deployability,"@antonkulaga I will update Readme to point right configuration, however on `what should be given when running java -jar Cromwell.jar` it doesn't need any additional runtime attributes besides uncommenting Spark backend configuration from `reference.conf` and on `what should be put to wdl` is referred here : [WDL](https://github.com/broadinstitute/cromwell#sample-wdl)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134:20,update,update,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134,3,"['configurat', 'update']","['configuration', 'update']"
Deployability,"@antonkulaga Internally we're not allowed to use docker on our unix machines for exactly this reason, so handling this problem was never prioritized by our product ownership. We had to deal with it for our work on the upcoming AWS backend, as @mcovarr pointed out. The 25 release should be out in a matter of days",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283050869:272,release,release,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283050869,1,['release'],['release']
Deployability,"@antonkulaga It appears that changing the `name` only changes the filename of the output produced by your task, but not the actual processing. So I bet if you look at the files produced by the diamond blast task for all 3 workflows you'll see that even though their names differ they have the same content.; Cromwell by default only cares about the content of a file with respect to call caching and its name is ignored. In this case it likely md5ed the files and found they had the same hash so the copy task was cached.; I'll wait for you to confirm that the output files have indeed the same hash before closing this:. ```; md5 /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/ab101af5-26ba-45c8-b592-fb37e06a523d/call-diamond_blast/execution/graywhale_in_human_blastp.m8; md5 /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/3d75657d-7dc9-4b6f-bbc8-ae579a3fa773/call-diamond_blast/execution/graywhale_in_cow_blastp.m8; md5 /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/276d6f9e-15b1-4dc3-a8a7-889414406511/call-diamond_blast/execution/graywhale_in_cow_blastp.m8; ```. should all produce the same hash",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351125450:632,pipeline,pipelines,632,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351125450,3,['pipeline'],['pipelines']
Deployability,@antonkulaga It'll probably be > 1 week. We can't predict the frequency of the dot releases as they're almost always in response to some fire that erupts in production.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367445955:83,release,releases,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367445955,1,['release'],['releases']
Deployability,"@antonkulaga What @cjllanwarne means is that if you build off of the `develop` branch it should work for you. If you're ok with waiting until the next release (likely 31, potentially 30.3), it'll also be fixed for you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367443014:151,release,release,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367443014,1,['release'],['release']
Deployability,"@antonkulaga commented on [Thu Feb 16 2017](https://github.com/broadinstitute/wdl4s/issues/86). If there will be ScalaJS support, then it will be possible to do things like validation of wdl4s directly in the browser. ---. @geoffjentry commented on [Thu Feb 16 2017](https://github.com/broadinstitute/wdl4s/issues/86#issuecomment-280335418). that's an interesting point. My take is that I have nothing against doing it, for me personally it'd likely involve flipping the switch and if it Just Works great and if not (IIRC scala.js isn't 100% source compatible?) I'm not going to go much further. . I mean this in a non-snarky way but this is really going to be in a ""patches welcome"" territory as I doubt it'll be officially prioritized and while I just added it to my ""it'd be a good thing to do"" mental todo list it's not the top item and I don't pop things off that list as frequently as I'd prefer.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2693:667,patch,patches,667,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2693,1,['patch'],['patches']
Deployability,"@aofarrel Thanks a lot! This actually worked just fine till I hit the memory wall of 200Gb of RAM. In fact, my Docker launches a tool invoking a _Snakemake_ pipeline for genome inference, the fourth step of which requires 200Gb of memory. Now, prior to your suggestion my `Docker Engine` was running with 20Gb of RAM, I then pushed it to 120. This helped to go through the 3rd step of the _Snakemake_ pipeline, requiring 100Gb of memory and where previously my WDL run was terminated, still the entire script cannot complete its execution due to memory requirement. With that said, I might keep this issue open for a bit longer maybe someone can relate to this and, most importantly, someone without the high memory demand for the tool I'm using might actually get the job done with this simple workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943:157,pipeline,pipeline,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943,4,['pipeline'],['pipeline']
Deployability,"@breilly2 oops you're right, sorry 😬 I'll recycle your other non-Cromwell PRs for the rest of the hotfix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6482#issuecomment-912506219:98,hotfix,hotfix,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6482#issuecomment-912506219,1,['hotfix'],['hotfix']
Deployability,"@byoo fixed, will be in soon-to-be released 31.1 and next major version 32",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3421#issuecomment-373764453:35,release,released,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3421#issuecomment-373764453,1,['release'],['released']
Deployability,@cahrens as this is already merged I will add this as a TODO to the release ticket BT-509,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1012168472:68,release,release,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1012168472,1,['release'],['release']
Deployability,"@carolynlawrence Just to double check, are you all using Docker in your workflow tasks? The only reason we're fiddling with permissions is due to Docker, and I don't know of anyone using Docker w/ Cromwell in an HPC environment - so my thought was that we could simply disable that permission activity for tasks which are not using docker. . To tie it into what @danbills suggested, perhaps **that** should be what the configuration flag is doing, just to be sure it's not breaking anyone's reliance on current behavior either way.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374722237:419,configurat,configuration,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374722237,2,['configurat'],['configuration']
Deployability,"@chapmanb Also can you explain what the different systems are? It looks like everyone is using the same configuration here, right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607#issuecomment-387853025:104,configurat,configuration,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607#issuecomment-387853025,1,['configurat'],['configuration']
Deployability,"@chapmanb Somatic completed successfully by bumping the memory (I doubled it to 8GB) :); I have another question about the rnaseq pipeline if you don't mind.; I'm hitting this error on the `pipeline_summary` task:. ```; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/cyvcf2/__init__.py:1: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from .cyvcf2 import (VCF, Variant, Writer, r_ as r_unphased, par_relatedness,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/_libs/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (hashtable as _hashtable,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import algos, lib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import hashing, tslib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/indexes/base.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (lib, index as libindex, tslib as libts,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/tseries/offsets.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; import pandas._libs.tslibs.offsets as liboffsets; /usr/loca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:130,pipeline,pipeline,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['pipeline'],['pipeline']
Deployability,@chapmanb things lined up nicely to make this a pretty easy fix; it will be in the next release of Cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4023#issuecomment-415427329:88,release,release,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023#issuecomment-415427329,1,['release'],['release']
Deployability,"@cjllanwarne , @curoli ; I am writing a UI to deal with cromwell. There I just make Ajax calls to cromwell from ScalaJS without bothering about redirecting everything to the server. I had to configure nginx to provide allow-origin, however,it will be way better if there will be allow-origin option in cromwell config, so people will be able to use my UI without messing with nginx configurations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-344307228:382,configurat,configurations,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-344307228,1,['configurat'],['configurations']
Deployability,"@cjllanwarne -- great suggestion, but I want to make the least amount of possible changes pre-release. I can ticket it; @mcovarr other than re-starting the JES build, is there anything else require to get a thumb?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1464#issuecomment-248677461:94,release,release,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1464#issuecomment-248677461,1,['release'],['release']
Deployability,"@cjllanwarne @aednichols I have updated this to reflect Adam's changes, plus naming convention changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6466#issuecomment-902916922:32,update,updated,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6466#issuecomment-902916922,1,['update'],['updated']
Deployability,"@cjllanwarne @kshakir Thinking more about this, I'm wondering if the ""Backend Return Code"" is something we want to consider abstract/general enough that every backend should have the option to return one (in which case an `Option[BackendRc]` should be wire from the backend back to the CA, like for the script rc), or if it's something really only specific to JES in which case we should treat it the same way we do the JES status and Run ID (which are updated directly from JES Backend, which is not great but will probably be fixed with intel changes when Backends become actors)..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184747668:453,update,updated,453,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184747668,1,['update'],['updated']
Deployability,@cjllanwarne @mcovarr code has been updated,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/362#issuecomment-170566228:36,update,updated,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/362#issuecomment-170566228,1,['update'],['updated']
Deployability,@cjllanwarne @salonishah11 this is really fixed now and ready for re-review. The Google errors provided by customers have `\n` in them which our regexes did not match. Fixed the regex and updated test cases to match the actual error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961:188,update,updated,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961,1,['update'],['updated']
Deployability,"@cjllanwarne Although perhaps the solution there is to have a release assembly which bundles everything together and a lighter weight one as well, which we can use for the situation you describe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/681#issuecomment-208424103:62,release,release,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/681#issuecomment-208424103,1,['release'],['release']
Deployability,@cjllanwarne Good point on the rc file location. The command script should be updated to redirect to an rc file qualified by the path of the call directory.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/200#issuecomment-143254145:78,update,updated,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/200#issuecomment-143254145,1,['update'],['updated']
Deployability,"@cjllanwarne I certainly don't think this needs a unit test for a hotfix. And as there was no unit test added for the introduction of the queue, and many unit and virtually all integration tests exercise this indirectly I'm not sure I see the necessity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371:66,hotfix,hotfix,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371,2,"['hotfix', 'integrat']","['hotfix', 'integration']"
Deployability,"@cjllanwarne I have seen these errors before. It happens when you use a java version that is higher than 8. (My OS has 11 installed by default, and I use a conda environment to use OpenJDK 8 on intellij). So it may be an update of travis CI's default image. . EDIT: Hmm I checked the `.travis.yml` and the openjdk8 is explicitly specified. Really weird that a higher version of java is used. EDIT2: And the compilation works again. Sometimes it is best to let a restart do the work :wink:. The errors that occur now is because quay.io is down, and related tests fail. https://status.quay.io/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015:122,install,installed,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015,2,"['install', 'update']","['installed', 'update']"
Deployability,"@cjllanwarne I have updated the documentation and the only test on Travis that fails is related to relative imports on CWL, so not something in this PR. I have tested the strategies in real life and found no problems. I see no ""ready for review"" label, and the ""on-deck for review label"" prioritizes the PR (which I feel I am not in a position to do). What is the usual process for declaring the PR ready?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601606406:20,update,updated,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601606406,1,['update'],['updated']
Deployability,"@cjllanwarne I know you just made an update to the IntelliJ plugin, did you fix this too?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982393:37,update,update,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982393,1,['update'],['update']
Deployability,"@cjllanwarne I run it on cromwell 30.2 release, the latest release at the moment",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367442607:39,release,release,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367442607,2,['release'],['release']
Deployability,@cjllanwarne I'll update that too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/594#issuecomment-199399532:18,update,update,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/594#issuecomment-199399532,1,['update'],['update']
Deployability,@cjllanwarne Just need to update the piece around private dockerhub support --otherwise approved 👍,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4633#issuecomment-486211090:26,update,update,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4633#issuecomment-486211090,1,['update'],['update']
Deployability,"@cjllanwarne My plan was following the next release I'd make the change in develop, so 2 releases from now",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/402#issuecomment-174552313:44,release,release,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/402#issuecomment-174552313,2,['release'],"['release', 'releases']"
Deployability,"@cjllanwarne Something went wrong with the latest release of the womtool.; The cromwell.jar does properly display the CLI usage when run.; ```; $ java -jar womtool-53.jar ; Exception in thread ""main"" java.lang.NoClassDefFoundError: scala/concurrent/duration/Duration; 	at scopt.Read$.liftedTree1$1(options.scala:67); 	at scopt.Read$.<init>(options.scala:66); 	at scopt.Read$.<clinit>(options.scala); 	at womtool.cmdline.WomtoolCommandLineParser.<init>(WomtoolCommandLineParser.scala:30); 	at womtool.cmdline.WomtoolCommandLineParser$.instance$lzycompute(WomtoolCommandLineParser.scala:13); 	at womtool.cmdline.WomtoolCommandLineParser$.instance(WomtoolCommandLineParser.scala:13); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:158); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:166); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:27); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:27); 	at womtool.WomtoolMain.main(WomtoolMain.scala); Caused by: java.lang.ClassNotFoundException: scala.concurrent.duration.Duration; 	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581); 	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178); 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522); 	... 18 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5808:50,release,release,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5808,1,['release'],['release']
Deployability,"@cjllanwarne Thanks for fixing this so fast. For validation I just skip version 32 ;) When is 33 planned?. We want to upgrade to wdl 1.0 anyway but first need to complete our testing framework around wdl, see also https://github.com/biopet/biowdl-test-utils (library) and https://github.com/biowdl/QC (real pipeline with testing)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-398279541:118,upgrade,upgrade,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-398279541,2,"['pipeline', 'upgrade']","['pipeline', 'upgrade']"
Deployability,"@cjllanwarne Thanks for notifying! We always use the `docker` attribute in BioWDL, so BioWDL pipelines can run without any extra configuration. On our cluster we have configured this so that we run the docker images using singularity. . I think it is a good thing that custom runtime attributes can be cached now. We recently added a `time_minutes` attribute to our pipelines in order to work better with SLURM. I hope this code ties in nicely when we switch to `hints` in WDL 2.0.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996:93,pipeline,pipelines,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996,3,"['configurat', 'pipeline']","['configuration', 'pipelines']"
Deployability,"@cjllanwarne Thanks for the clarification. I was already wondering why you would negate your own well-written namespace code with a single line... Anyway I created a pull request on the spec here: https://github.com/openwdl/wdl/pull/347, your feedback would be much appreciated. Here's to hoping that it gets unanimously approved :crossed_fingers: . @geoffjentry yes, the Cromwell team has a lot of influence on the spec by implementing or not implementing things. I can understand the temptation to use this for ""the greater good"" :wink: . But I am quite happy that the Cromwell developers chose to be in touch with the community and aggressively implement the development spec in the development version of Cromwell. This allows us to see how certain spec changes turn out *before* they get implemented in production. In this case I came across this when I was testing the code for #5312 and found that I could not set my resource requirements for BWA anymore (in BioWDL all tasks default to the least number of cores needed, and sometimes you want to override this for more power). Since BWA was nested in a subworkflow this turned out not to be possible. So now we can fix the spec and Cromwell before this ever gets into a release. I think it is great work by the Cromwell team. It can't always be easy to follow the spec that closely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176:1228,release,release,1228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176,2,['release'],['release']
Deployability,@cjllanwarne Thanks for the quick response!. I agreed that it seems reasonable to have built-in support for FUSE mounts in Cromwell. Nevertheless this PR can be a neat addition to the existing Google Cloud integration. I've updated the docs with the FUSE filesystem usage limitations as you asked. Looking forward for the review. Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572980186:206,integrat,integration,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572980186,2,"['integrat', 'update']","['integration', 'updated']"
Deployability,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:900,Pipeline,Pipeline,900,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064,2,['Pipeline'],['Pipeline']
Deployability,"@cjllanwarne Yes, my money is that you have pegged this exactly. I would love this update. I prefer the ``Int? mem=4`` for specifying default values. Otherwise, we have to pepper our command and runtime blocks with ``${default=4 mem}`` or, even worse, something with ``select_first``. We often have inputs that are derived (e.g. ``e``) and we do not want these exposed in ``wdltool inputs ...``. I do not have a good idea for how to handle ``f``. I'm assuming you do not have access to the raw expression when rendering ``wdltool inputs ...``, so can you just say that it has a complex default expression?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2532#issuecomment-321288565:83,update,update,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2532#issuecomment-321288565,1,['update'],['update']
Deployability,"@cjllanwarne any update on this issue? Is it still to-do?; Also, what is it for?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2316#issuecomment-332235465:17,update,update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2316#issuecomment-332235465,1,['update'],['update']
Deployability,@cjllanwarne any update on this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1316#issuecomment-313485510:17,update,update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316#issuecomment-313485510,1,['update'],['update']
Deployability,"@cjllanwarne any update on whether this is still an issue? I know aborts is a tangled mess, is this still part of that?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-329662040:17,update,update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-329662040,1,['update'],['update']
Deployability,"@cjllanwarne having the docker root be a runtime option as then it's hardcoded to the WDL. . At least for this use case it's not and should not be a WDL concept - the reason it comes up is they are using Singularity for their docker containers, and that's a Cromwell-wide configuration thing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420768761:272,configurat,configuration,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420768761,1,['configurat'],['configuration']
Deployability,"@cjllanwarne helped me with this issue but its led to new ones:; https://github.com/broadinstitute/cromwell/issues/5793. Cromwell tries to chmod the mounted sra directory which is not allowed.; code:; https://github.com/broadinstitute/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:315,pipeline,pipelines,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,2,['pipeline'],['pipelines']
Deployability,"@cjllanwarne new lines adjusted, so are my intellij settings thanks to Jose. I'm thinking the integrationTestCases should run weekly instead of nightly even. @jsotobroad as the original creator of the integrationTestCases -- does that sound okay?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352044523:94,integrat,integrationTestCases,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352044523,2,['integrat'],['integrationTestCases']
Deployability,"@cjllanwarne sorry, I probably missed that part when I was reviewing the Google doc.; I'm not sure I agree with that approach. Reference bucket names can be configurable via Cromwell configuration file.; I'm not sure there will be more than one official bucket with references in future, but if there will be, we may create different disk images and manifests for different buckets (and each bucket contains it's own manifest file) rather than mixing them all together. This approach would also give us enough flexibility for ""user's own reference buckets/disks use-case"". I envisioned a configuration like this:; ```; backend {; PAPIv2beta {; reference-data-conf {; reference-buckets: [; { broad_official_bucket_name: broad_official_disk_image_locator },; { broad_official_bucket_another_one_name: broad_official_disk_image_another_one_locator },; { users_private_bucket_name: users_private_disk_image_locator }; ]; }; }; }; ```. In this example Cromwell then may check the input file GCS path against the configured list of reference buckets and figure out which image to mount based on that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801:183,configurat,configuration,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801,2,['configurat'],['configuration']
Deployability,@cjllanwarne thanks for merging! Can the changelog bits be retroactively added to the releases page so it is out there for everyone to see? Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4878#issuecomment-488564412:86,release,releases,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4878#issuecomment-488564412,1,['release'],['releases']
Deployability,"@cjllanwarne this wasn't fixed in your PR, right? What would be involved in fixing this for the next release? I don't want to hold it up but improving our magic release would be nice :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2225#issuecomment-300302414:101,release,release,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2225#issuecomment-300302414,2,['release'],['release']
Deployability,@cjllanwarne updated,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170668783:13,update,updated,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170668783,1,['update'],['updated']
Deployability,"@cjllanwarne updated the container image. I'm not inclined to add a glob test as I don't think it's really adding any value. As I said previously, if you'd like to add it knock yourself out",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484174726:13,update,updated,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484174726,1,['update'],['updated']
Deployability,"@cjllanwarne we could, but it's really just for our internal deployments with devops so imo it's not necessary and would just add to the visual noise of our docs. It'd only take a couple of minutes to create said visual noise if you really think it'd be helpful",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2541#issuecomment-321910634:61,deploy,deployments,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2541#issuecomment-321910634,1,['deploy'],['deployments']
Deployability,"@cjllanwarne, here is the PR. This is only for workflow definitions, and only for line numbers. I found that it is, as you were saying, hard to extract reliable information from Hermes for column numbers. I *would* like to get the entire extent in the source file covered by an AST. It was slow slog to updates the tests to correctly check line numbers. Let's start with this change, and see how it goes. . Thank you,; Ohad.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4938#issuecomment-489182117:303,update,updates,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4938#issuecomment-489182117,1,['update'],['updates']
Deployability,"@cjllanwarne: I also encountered this issue. Until the mentioned upgrade script is released, is information available that highlights the changes necessary to migrate from draft 2 (or 3/1) to WDL 1.0? My files are in draft-2 format. Any sort of guidance about what's different between the versions would be helpful. Doing a visual diff of the `SPEC.md` files isn't ideal... Somewhat related: Is there an estimate of when womtool will have `-imports` exposed as a parameter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111:65,upgrade,upgrade,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111,4,"['release', 'upgrade']","['released', 'upgrade']"
Deployability,"@coreone Merge at will. If the database isn't updated, the new build will crash with an error containing the SQL that needs to be run. The paths to the scripts have also changed, so even if an old jenkins job tries to run the scripts manually, I suspect it would fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-171434974:46,update,updated,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-171434974,1,['update'],['updated']
Deployability,"@cpavanrun If I understood you correctly I think this change will give you what you want, albeit you'll need to opt in to that behavior via configuration",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424755522:140,configurat,configuration,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424755522,1,['configurat'],['configuration']
Deployability,"@curoli commented on [Wed Oct 04 2017](https://github.com/broadinstitute/wdl4s/issues/248). Currently, subclasses of WOM GraphNodes are case classes which auto-generate equals and hashCode based on components. This doesn't work well for two reasons.; First, WOM Graph assumes object identity and will fail if a node is replaced by one that is equal based on components. The code is already in several places using eq instead of == to account for this.; Second, since graph nodes contain all upstream nodes, component-based hashCode or equals are unworkable for real-life pipelines, because hashCode/equals will be called for every upstream node through every possible upstream path, which grows exponentially with graph size for workflows that contain branches that rejoin (and most do). . ---. @danbills commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334536133). I think this comment is more appropriate here:. Equals is supposed to be bitwise/value-based equality, whereas `eq` indicates reference equality. I don't recall the exact motivation behind using `eq` in the test, maybe @cjllanwarne can comment when he returns on Tuesday. ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334555964). If you don't like this fix, what do you suggest we do instead? How about we accept this fix now and you can rework the whole thing later, if you want?. This is a show-stopper once you want to create larger (aka real-life) workflows. . For example: if node a has two outputs, which are inputs of b, then b.hashCode calls a.hasCode twice. If b produces two outputs which are inputsof c, then c.hashCode calls b.hashCode twice, and a.hashCode is called four times. If c produces two outputs which are inputs of d, d.hashCode calls c.hashCode twice, b.hashCode four times and a.hashCode eight times, etc. . ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2694:571,pipeline,pipelines,571,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2694,2,['pipeline'],['pipelines']
Deployability,@cy-bao is this still the case on the latest version of Cromwell in FireCloud? . All future updates to this issue will be made in JIRA: ; https://broadworkbench.atlassian.net/browse/BA-2791; Sorry for the inconvenience.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-506470150:92,update,updates,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-506470150,1,['update'],['updates']
Deployability,@danbills I ended up using the global conf in the interest of time.; I'm sure there's a better way to do it (@cjllanwarne suggested through the language factory configuration which seems a good idea). This is what the original ticket actually intended: https://github.com/broadinstitute/cromwell/issues/2611,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3669:161,configurat,configuration,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3669,1,['configurat'],['configuration']
Deployability,@danbills I'd be happy to help debug any funnel issues you run into. Were you testing the v0.2 release or the latest on our master branch?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332604270:95,release,release,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332604270,1,['release'],['release']
Deployability,"@danbills The Orchestrator pattern as described above is what we discussed. . Per your other questions, the answer is that AWS Batch does not take a array of arbitrary scripts as an option, nor can you override a Docker container's `ENTRY_POINT` to supply your own script if the entry point of the container has been changed from the default shell. You can only specify an array to pass into Docker daemon's `CMD`. Speaking of default shells, the other arguments against a set of shell scripts is that it limits the set of containers that can be called from a WDL. For example, the current Cromwell scripts that are injected into the container assume Bash support, but by default Alpine Linux (and many containers that build off of it) do not have Bash installed. . Most of the time the above two items are safe assumptions, but not always, hence the current plan to implement data staging via a sibling container approach similar to how CI systems are deployed today. For inspiration, I refer to [Dave Hein's excellent article on running sibling containers in lieu of docker-in-docker](https://www.develves.net/blogs/asd/2016-05-27-alternative-to-docker-in-docker/)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-400025987:753,install,installed,753,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-400025987,2,"['deploy', 'install']","['deployed', 'installed']"
Deployability,@danbills any update on this ticket?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2719#issuecomment-335898405:14,update,update,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2719#issuecomment-335898405,1,['update'],['update']
Deployability,"@danbills commented on [Fri Sep 15 2017](https://github.com/broadinstitute/wdl4s/issues/216). We have existing tests (in ExportCwlSamplesSpec) that are testing output as previously implemented, yet library collisions forced us to remove that capability. If and when [circe-yaml](https://github.com/circe/circe-yaml) updates their cats dependency we could use that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2725:316,update,updates,316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2725,1,['update'],['updates']
Deployability,"@danbills does the following sound accurate to you?. As a **user running workflows on Pipelines API**, I want **Cromwell to notify me when it hits the limit for retries**, so that **I know why my workflow failed**.; * Effort: Small; * Risk: Small; * Business value: Small to Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2426#issuecomment-332648095:86,Pipeline,Pipelines,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2426#issuecomment-332648095,1,['Pipeline'],['Pipelines']
Deployability,"@davidangb I think that's fair. How about us saying that we'll continue to run the rawls/agora upgrades, but only if changes are made to the WDL draft 2 libraries. Otherwise we'll leave them at their current version?. In effect that probably means we won't be changing anything in the draft-2 libraries for the next few weeks... but it's good to have a fall back in case an urgent fix is needed (we didn't have anything planned in any case, which is why we didn't mind this freeze in the first place).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-490096475:95,upgrade,upgrades,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-490096475,1,['upgrade'],['upgrades']
Deployability,@davidbernick @hjfbynara would you please confirm update script has been run so that I can rule out pingdom/firewall issues?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-452850659:50,update,update,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-452850659,1,['update'],['update']
Deployability,"@delagoya I can't seem to find the thread, but the pathing reversal from @kshakir's notes above was purposeful. By using /test1/... rather than .../test1, we get the advantage of a more useful host path when traversing manually for debugging purposes for example, or by being able to segment what we know are large tasks to different filesystems. I'm comfortable changing the disks configuration name but this should probably be tracked in a separate issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-405687348:382,configurat,configuration,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-405687348,1,['configurat'],['configuration']
Deployability,@delagoya The plan is to bake this into an AMI similar to the way ecs agent is installed. It will be a container with a always-on restart policy. https://docs.docker.com/config/containers/start-containers-automatically/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-410858494:79,install,installed,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-410858494,1,['install'],['installed']
Deployability,"@delagoya your dependencies update might conflict with a round I just did in our `develop` branch. Namely we are using cats 1.0.1, and I'm not 100% sure 1.1 will work. So if you pull/rebase you will get most of what you posted minus the sttp update",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-382077268:28,update,update,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-382077268,2,['update'],['update']
Deployability,"@delocalizer @kcibul we talked about this internally. As background we went down this path as our integration tests were frequently failing in travis - hte output files would be empty or incomplete. . It was noted that our tests use a lot of `echo` and `cat` and are quite short, so the theory is we're running into [this](https://www.turnkeylinux.org/blog/unix-buffering). **if** that turns out to be the culprit (and it does make a lot of sense) one could either take the stance that tools need to be well formed and have properly flushed, or we could try to bake something into our controller bash script (which IMO adding so much stuff to that bash script is a bomb waiting to happen, but ....), some [ideas](http://serverfault.com/questions/294218/is-there-a-way-to-redirect-output-to-a-file-without-buffering-on-unix-linux) are in that link. @kcibul what's your reaction to the above? does it ring true or still seem fishy?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175:98,integrat,integration,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175,1,['integrat'],['integration']
Deployability,@delocalizer Hey sorry I forgot to give you an update on that and I don't know if you've seen it. You were right and the bug should be fixed now. It was due to a bug in the `better files` library we use and I filed a ticket on their github https://github.com/pathikrit/better-files/issues/115.; In the meantime I added a workaround.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950#issuecomment-281994706:47,update,update,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950#issuecomment-281994706,1,['update'],['update']
Deployability,"@delocalizer We're starting to consider that the issue is in tooling, specifically in the tools we use for our integration tests. Since you are as far as I know the largest user of the shared file system backend(s), to what degree do you trust that tools are flushing when they're complete?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482:111,integrat,integration,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482,1,['integrat'],['integration']
Deployability,"@delocalizer ah, thanks. I didn't realize the new backend system was this flexible (the `README` is a little opaque). Thank you for sharing your PBS configuration file! I confirm it works for me, too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254206921:149,configurat,configuration,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254206921,1,['configurat'],['configuration']
Deployability,"@dformoso I checked in with the team and this issue is scheduled to be fixed by the time Cromwell releases support for WDL 2.0. At that time, `version development` will be promoted to an officially supported version; before then, `development` should be used with caution & probably not in production.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671439046:98,release,releases,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671439046,1,['release'],['releases']
Deployability,"@dheiman this is actually more of a feature request as this feature has not yet been implemented. I will update the issue accordingly, thanks for your feedback!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-287458911:105,update,update,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-287458911,2,['update'],['update']
Deployability,@dmohs I can't find it at the moment but this is effectively a dupe of another ticket. It's been a known problem for years now that the logs Cromwell emits (console or otherwise) are trying to cover multiple user personas at once (and IMO not doing a great job for any of them) and that a more holistic solution needs to be put into place. . When I find the ticket I redirect all of this stuff towards I'll update,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062#issuecomment-417414448:407,update,update,407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062#issuecomment-417414448,1,['update'],['update']
Deployability,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... 🤷. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:957,pipeline,pipelines,957,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972,5,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipelines']"
Deployability,"@droazen not that i'm aware of. If you're referring to what I think you're referring to, @leetl1220 is experiencing these errors as part of the Pipelines API process which isn't code we control.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318397531:144,Pipeline,Pipelines,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318397531,1,['Pipeline'],['Pipelines']
Deployability,"@dspeck1 I updated this branch to trigger another CI run, let's see what happens",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1589327404:11,update,updated,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1589327404,1,['update'],['updated']
Deployability,"@dtenenba , @vortexing - The [docs](https://docs.opendata.aws/genomics-workflows) for creating the genomics workflow environment (i.e. AWS Batch and related resources) have been updated. Use of custom AMIs has been deprecated in favor of using EC2 Launch Templates. There's also additional parameter validation under the hood around setting up an environment for Cromwell to avoid these configuration errors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885:178,update,updated,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"@dtenenba - the space on the scratch mount point (for cromwell it is `/cormwell_root`) is managed by a monitoring tool `ebs-autoscale` that is installed when creating a custom AMI configured for Cromwell, and then referencing that AMI when creating Batch compute environments. Running out of space points to one or more of the following:. * the monitor is not installed; * the monitor is looking at the wrong location in the filesystem. If you've created a custom AMI, I suggest launching an instance with it and checking that the monitor is watching the correct location. Do this by checking the log: `/var/log/ebs-autoscale.log`. If it's not, you'll need to recreate both the AMI and the Batch Compute Environment, and associate the new CE with your Job Queue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942:143,install,installed,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942,2,['install'],['installed']
Deployability,"@dvoet the reason for not having been done yet was that the primary motivating use case (or problem turner upper as it were) was GOTC who were able to work around it by not globbing in the first place. I believe this will be part of the upcoming 23 release. re S3, good news there for you, we have to do our own localization so don't need to rely on others to tell us what's going on ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1395#issuecomment-256901209:249,release,release,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1395#issuecomment-256901209,1,['release'],['release']
Deployability,@dvoet wouldn't adding the changeset at the beginning of the log cause checksum/validation error for Cromwells that are already deployed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880:128,deploy,deployed,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880,1,['deploy'],['deployed']
Deployability,"@eddiebroad commented on [Wed Aug 10 2016](https://github.com/broadinstitute/wdltool/issues/12). I try to use wdltool validate on a WDL and it seems to _not_ catch; an error in the WDL. In the example ""bad"" WDL attached the call to VCF_to_MAF_task; has the line ""inputVCF=inputVCF"" but the ""inputVCF"" exists only; in the task but not at the workflow level ; but invoking wdltool 0.4 on it seems to _NOT_; cause an error. Shouldn't it be saying the WDL has an error; because inputVCF does _not_ exist at the workflow level?. I downloaded the wdltool from the latest release; https://github.com/broadinstitute/wdltool/releases/download/0.4/wdltool-0.4.jar. the two WDLs are attached. [wdl_files.zip](https://github.com/broadinstitute/wdltool/files/412067/wdl_files.zip). ```; wm8b1-75c:red_bug esalinas$ find *.wdl -exec java -jar wdltool-0.4.jar validate {} \;. wm8b1-75c:red_bug esalinas$ diff good.wdl bad.wdl ; 188c188; < inputVCF=inVCF,. ---; > inputVCF=inputVCF,; wm8b1-75c:red_bug esalinas$ ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2876:565,release,release,565,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2876,2,['release'],"['release', 'releases']"
Deployability,"@elerch Careful with that `always-on` restart policy from docker. In my experience, it did not re-read `env-files` (in my case those env vars are sitting on the host's `/etc/defaults/ecs`). I expected SIGHUP-like behavior when changing ecs-agent attributes like `ECS_CLUSTER`, i.e:. https://github.com/umccr/umccrise/blob/master/deploy/roles/brainstorm.umccrise-docker/files/bootstrap_instance.sh#L39. Instead, I had to resort to a systemd service that re-runs the ecs-agent docker container on boot:. https://github.com/umccr/umccrise/blob/master/deploy/roles/brainstorm.ecs-agent/tasks/main.yml#L75",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-413092230:329,deploy,deploy,329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-413092230,2,['deploy'],['deploy']
Deployability,"@elerch I previously submitted a PR for this #4109 but it seems the change hasn't been propagated to the image actually used by the AWS resources, so globbing doesn't work in the deployment as described here http://aws-genomics-workflows.s3-website-us-east-1.amazonaws.com/aws-batch/create-custom-ami/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4335:179,deploy,deployment,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4335,1,['deploy'],['deployment']
Deployability,"@ernoc Can you provide some more info on your setup? What backend you're using, what DB configuration, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423360617:88,configurat,configuration,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423360617,1,['configurat'],['configuration']
Deployability,"@ernoc So this sounds like there's a disconnect between the status changing in memory and getting updated in the db (as the REST endpoints report status from the db). . 1. Do you see anything in your logs that indicate db errors?; 2. What does your db config look like? ; 3. When you report the REST endpoint shows the workflow as 'Running', what about the `executionStatus` key in the metadata? Are some jobs marked as 'Running' as well?; 4. Do you see this behavior only with large scatters (10K) or do you see it with smaller scatters as well? Or any other type of workflow shape?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445245400:98,update,updated,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445245400,1,['update'],['updated']
Deployability,@ernoc We hope to have this feature released in the next few months that involves being able to cleanup workflow outputs!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-417331227:36,release,released,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-417331227,1,['release'],['released']
Deployability,"@ffinfo Hi Peter - apologies for taking so long, the release I mentioned ended up taking a while longer than we thought. I talked to our PO this morning about this pull request and his take was that if this could be hooked up in a way which keeps the tests green (as much as they ever are) and doesn't add noticeable latency in the system for other users (and/or the behavior change is put behind a config option) that he'd be good with this concept. . It's been a month now so it's entirely possible you've already moved on with life or perhaps you have no interest for other reasons so I'll leave it up to you on how to proceed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442:53,release,release,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442,1,['release'],['release']
Deployability,@ffinfo the fix will be in the next release of Cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051#issuecomment-420759722:36,release,release,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051#issuecomment-420759722,1,['release'],['release']
Deployability,"@francares @geoffjentry I'm going to close this version of the PR. The comments are all heading to the other PR and Master will be updated in about a week anyway when we publish 0.17. . At that point, the Master PR will essentially be equivalent to this one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/399#issuecomment-174653940:131,update,updated,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/399#issuecomment-174653940,1,['update'],['updated']
Deployability,@francares Can you update the story on waffle (#884) and move it to in-review?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1100#issuecomment-229752230:19,update,update,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1100#issuecomment-229752230,1,['update'],['update']
Deployability,@francares I believe the only attribute strictly required is docker as all of the other ones should have defaults set by the google genomics pipeline API (e.g. disk defaults to 500GB). Anything provided should be valid however.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212482498:141,pipeline,pipeline,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212482498,1,['pipeline'],['pipeline']
Deployability,"@francares sorry for the slow responses this week. We've got people out this week so we're falling a bit behind on code reviews (also this week was a release week, which has taken up a good amount of my time). I will definitely review again by EOD tomorrow, but I'm not avoiding this! I need to review @kshakir's PR first because I'm way overdue on that one, but this one is next.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-191947625:150,release,release,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-191947625,1,['release'],['release']
Deployability,"@freeseek if I send you a patched JAR do you think you'd be able to verify the fix?. Which is to say, do you get 504s frequently enough that you can test updated handling?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760326826:26,patch,patched,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760326826,2,"['patch', 'update']","['patched', 'updated']"
Deployability,@gauravs90 @francares Please note I've rebased and therefore had to update the ValidateActor to no longer require a backend on construction. I've also modified ValidateActorSpec to feed in the mock backend to the static CromwellBackend pool during testing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/586#issuecomment-199336078:68,update,update,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/586#issuecomment-199336078,1,['update'],['update']
Deployability,@gauravs90 I'm happy to handle the tidy-up of this PR. Could you though review it (including my integration of your changes) and thumbs-up if you're happy? Cheers!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/707#issuecomment-211403226:96,integrat,integration,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/707#issuecomment-211403226,1,['integrat'],['integration']
Deployability,"@gauravs90 a couple of global comments:; - The symbol store and execution store in the old WorkflowActor were not necessarily database-backed. They just stored which calls were completed and which were in flight.; - The graph is a nice idea but currently isn't as eager as it could be. Consider:. ```; A -> B -> C; X -> Y -> Z; ```; - I believe this will run this in pairs, `(A,X)`, `(B,Y)`, `(C,Z)`. But what if A and B are really quick but X and Y are really slow - we're slowed down from executing C because the unrelated tasks X and Y haven't finished yet.; - I think I would prefer the existing method of determining (after every job completes) the set of jobs which have now become runnable. There's already an implicit DAG there. ; - A major reason is, it has already been shown to work with the scatter/gather and other features which update the graph at run-time and I can't see how that would work with this static graph approach?. _NB Sorry all for the repeated almost-identical edits to the above comment. Markdown is hard... :(_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-215527919:843,update,update,843,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-215527919,1,['update'],['update']
Deployability,"@gemmalam Just wanted to check up on this since it's been a month - will it be reviewed as part of the current sprint, and hopefully included in release 43?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-499651256:145,release,release,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-499651256,1,['release'],['release']
Deployability,"@geoffjentry & @Horneth for review please, including the lenthall patch needed to get this cromwell build repaired",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213251040:66,patch,patch,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213251040,1,['patch'],['patch']
Deployability,"@geoffjentry - we are facing something similar. Our SGE was recently updated and job submissions randomly fail due `unable to contact qmaster`. Our HPC team is looking into it. In the meantime, I am looking for a way to configure Cromwell to retry failed job submissions using the SGE backend. I have tried adding `maxRetries` to the runtime attributes to retry failed job submissions, but seems like this does not retry job submission errors. Only retries task errors. Is that correct? Any advice would be appreciated. Is this a feature that is currently supported? Thanks in advance. I also have seen various different configs on the WDL/Cromwell forum, but not sure if any are still supported. For example:. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/10475/cromewell-28-root-configuration-not-working); ```; system {; max-retries = 10; }; backend {; max-job-retries = 4; }; ```. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure); ```; system {; max-retries = 50; job-rate-control {; jobs = 5; per = 1 second; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897:69,update,updated,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897,2,"['configurat', 'update']","['configuration-not-working', 'updated']"
Deployability,"@geoffjentry -- as of Cromwell 35, `backend` key in the workflow options is honored above the default backend. In case of the workflow option asking for a backend that doesn't exit, Cromwell explicitly fails with:. `Backend for call <call-name> ('<backend-name') not registered in configuration file...`. I believe this issue has been resolved with these changes. Feel free to re-open if something was missed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-424937274:281,configurat,configuration,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-424937274,1,['configurat'],['configuration']
Deployability,"@geoffjentry -- to put this in your head. Let's not jump to it being an engine feature. But I think the point here is a good one. . I think this, and other use cases, might be best met by a set of ""built-in"" tasks that cromwell could come with. For example, through the use of imports and having cromwell released with a equivalent of ""genomics-stdlib"" we could provide genomics specific manipulations as tasks. This solves the problem of the user having to write them themselves. Then through the use of smart multi-backend support we could also have some of these stdlib tasks run on the same machine as cromwell. This requires a bunch of advances to the engine, but I think it's where we can provide a lot of value to the users. The first step in this could be having Kate & Crew (along with our help) publish that ""gatk-stdlib.wdl"" that performs these functions and people could import. Then if that is successful we could see how we would best provide that sort of support in a batteries included fashion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-255404918:305,release,released,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-255404918,1,['release'],['released']
Deployability,"@geoffjentry ; I trying to write some kind of integration test for my fix of this task and me with @TimurKustov came to idea of executing two workflows sequential in order to get outputs, results and call logs copied after execution of the first workflow and assure that they are exist and correctly placed by running second workflow, which would check these files locations and existence.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-520024075:46,integrat,integration,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-520024075,1,['integrat'],['integration']
Deployability,@geoffjentry @Horneth this could use a re-review since I updated it a bunch,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/992#issuecomment-225997411:57,update,updated,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/992#issuecomment-225997411,1,['update'],['updated']
Deployability,"@geoffjentry @cjllanwarne do you think your swagger is in good enough shape now for codegen to work well? Green was hoping to use your client in our next project instead of rolling yet-another-of-our-own, but some of the endpoints we need (top-level query, and labels patch) haven't been implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1855#issuecomment-400669179:173,rolling,rolling,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1855#issuecomment-400669179,2,"['patch', 'rolling']","['patch', 'rolling']"
Deployability,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:161,pipeline,pipelines,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956,6,"['Continuous', 'Pipeline', 'integrat', 'pipeline']","['Continuous', 'Pipelines', 'integration', 'pipelined', 'pipelines']"
Deployability,"@geoffjentry Ah, thanks for the clarification -- by ""Pipelines API process"", do you mean JES? Are you in touch with the people who maintain that (or are we able to submit PRs to that project)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318399842:53,Pipeline,Pipelines,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318399842,1,['Pipeline'],['Pipelines']
Deployability,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:74,configurat,configuration,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849,5,"['configurat', 'update']","['configuration', 'update']"
Deployability,"@geoffjentry Current state of Reliance Point backend, that is being implemented here, requires during initialization to know the order of calls execution. This is because the component behind this backend needs to know basic execution order since it still does not understand WDL therefore it can't read a complete DAG. ; I thought this was a bug due to the loose of oder when a fold or groupBy is applied in map of call -> backend.; We have a possible release date (given by the team who is developing it) of the RP component for beginning next year which will support WDL spec but for now we need to support this.; Please let me know if we can keep this change in the engine since it does not hurt. Later when RP WDL version is in place we revert this change.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235952476:453,release,release,453,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235952476,1,['release'],['release']
Deployability,@geoffjentry I am currently debugging a workflow on SLURM and can provide a beta backend configuration. Should I submit this as a merge request to `core/src/main/resources/reference.conf`?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-291606740:89,configurat,configuration,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-291606740,1,['configurat'],['configuration']
Deployability,"@geoffjentry I believe recently released a change with ""null"", is that related here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-330607992:32,release,released,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-330607992,1,['release'],['released']
Deployability,"@geoffjentry I removed retry in two spots which hopefully could be added back per this scheme as part of #808. Note that neither of those spots corresponds to GCS auth upload, which should be happening in the initialization actor and is the subject of #806. I actually thought this ticket was meant for the hotfix branch to deal with problems the Greenies had seen, but there doesn't seem to be any more detail or labeling to confirm or refute that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/834#issuecomment-219795863:307,hotfix,hotfix,307,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/834#issuecomment-219795863,1,['hotfix'],['hotfix']
Deployability,@geoffjentry I totally agree with @rtitle that this is something that should be added to next FC release as the performance improvement is significant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317803184:97,release,release,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317803184,1,['release'],['release']
Deployability,"@geoffjentry I want to resort to authority and say ""Zen of Akka""... . Reasons for my gut feeling: A mutable val makes it look and act more like a state machine, and reduces the risk of accidentally leaking the variable pointer to other threads which may update it out of band. Obviously not likely in this case, but as a muscle memory thing a-la `Some(constant)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1044#issuecomment-227570413:254,update,update,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1044#issuecomment-227570413,1,['update'],['update']
Deployability,@geoffjentry Is the use case that a Cromwell dev updated liquidbase and a user needs to migrate? How much effort would this take?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2429#issuecomment-333185907:49,update,updated,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2429#issuecomment-333185907,1,['update'],['updated']
Deployability,"@geoffjentry Not really solved. The pipeline could be terminated by the same error, i just extract the samples that are not processed and run it again. It would be better with local MySQL database.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-462649294:36,pipeline,pipeline,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-462649294,1,['pipeline'],['pipeline']
Deployability,"@geoffjentry Thanks for the quick response and the suggestion. I wrote a quick wdl script, that runs on cromwell + local backend, but when deployed on the AWS batch backend it fails in the same way..; The script `exampleWorkflows/bbmap.wdl` is: . ```; task bbmaptask {; File f1; File f2; command {; reformat.sh maxcalledquality=40 in=${f1} in2=${f2} out=${f1}.ref.fq.gz out2=${f2}.ref.fq.gz; }; output {; Array[File] response = glob(""*R?.ref.fq.gz""); }; runtime {; docker: ""*********.dkr.ecr.us-east-1.amazonaws.com/ngs/bbmap:v37.64""; }. }. workflow bbmapwf{; 	call bbmaptask; }; ```. The input file `exampleInput/fastq.s3.wdl.json`; ```; {; ""bbmapwf.bbmaptask.f1"": ""s3://bucket/fastq.20180820-150001/DA0000317_WSU-DLCL_R_02_01_02_S36_R1_001.fastq.gz"",; ""bbmapwf.bbmaptask.f2"": ""s3://bucket/fastq.20180820-150001/DA0000317_WSU-DLCL_R_02_01_02_S36_R2_001.fastq.gz""; }. ```. I run cromwell as:. ```; java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run -i exampleInput/fastq.s3.wdl.json exampleWorkflows/bbmap.wdl. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542#issuecomment-453807479:139,deploy,deployed,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542#issuecomment-453807479,1,['deploy'],['deployed']
Deployability,"@geoffjentry That makes sense, thanks. Given the current code structure it's not at all clear to me how Docker-dependent branching would fit in - maybe this would be easier as a boolean configuration option adjacent to `workflow-log-dir`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693:186,configurat,configuration,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693,2,['configurat'],['configuration']
Deployability,"@geoffjentry The main advantage of running these test cases daily vs weekly seems to be that it’s easier to narrow down which change couldve caused this test suite to fail. However, it seems unlikely to me that these tests could find breaking changes everyday that the centaur standard test cases wouldn’t already uncover. I see these tests as a release requirement for Cromwell more than anything else. However, it’s totally upto the team on whatever makes them most comfortable, I don’t have a strong opinion on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352073368:346,release,release,346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352073368,1,['release'],['release']
Deployability,"@geoffjentry This is my github ID. :). @katevoss We've been using parameters to tasks to override runtime attributes. However, I am not sure how this affects call caching, it is very clunky, and the standard names my group uses may different from another. This lattermost is a headache for pipeline engineers. Anyway, it would be nice to have a mechanism for overriding runtime attributes, mostly for users, not developers. This would cover times where WDL writers have hardcoded runtime attributes that do not fit a user's need. For me, this is no longer high priority... . Though is another issue needed for these parameters that I use adversely affecting call caching (e.g. my `preemptible_attempts` parameter is only used in the runtime block and should not cause a cache miss if it is changed)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325718645:290,pipeline,pipeline,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325718645,1,['pipeline'],['pipeline']
Deployability,"@geoffjentry Very nice, thanks for the link! Wish I did know this earlier... :+1: ; Could this file then be provided to `cromwell` when running integration test via `centaur`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433:144,integrat,integration,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433,1,['integrat'],['integration']
Deployability,"@geoffjentry We are not using Docker, but you are right that it might be better to make it a configuration flag like @danbills initially suggested, rather than automatic, in case it would break someone's workflow... like maybe if they were using Docker and non-Docker in the same workflow?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374724001:93,configurat,configuration,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374724001,1,['configurat'],['configuration']
Deployability,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:741,deploy,deployment,741,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235,1,['deploy'],['deployment']
Deployability,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:873,pipeline,pipeline,873,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901,2,['pipeline'],['pipeline']
Deployability,"@geoffjentry can you update the ticket with more info? Where is this parameter used, and what is the effect of this being ignored?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1097#issuecomment-229629709:21,update,update,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1097#issuecomment-229629709,1,['update'],['update']
Deployability,"@geoffjentry commented on [Wed May 04 2016](https://github.com/broadinstitute/centaur/issues/38). Now that the centaur code base is starting to accumulate code that's not just tests, we should start having actual centaur unit tests as well. That'll be a little odd simply because sbt test will run both those and the integration tests but i'm sure we'll manage",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2886:317,integrat,integration,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2886,1,['integrat'],['integration']
Deployability,"@geoffjentry do you mean from a default configuration perspective? We could have a pluggable ""metadata service"" in Cromwell (which I think we already have) with two implementations (direct DB write vs JMS emitter) could go into mainstream cromwell. Of course the listener for that would be a separate service (and easy to scale). Maybe that's what you mean though?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320245697:40,configurat,configuration,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320245697,1,['configurat'],['configuration']
Deployability,"@geoffjentry in such case I have not idea how I can do anything other than super-simple linear pipelines with wdl: tsv-s with headers cannot be read, read_json does not work, loops do not work, I cannot make even simpliest preprocessing of input arrays or maps with wdl!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367423915:95,pipeline,pipelines,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367423915,2,['pipeline'],['pipelines']
Deployability,@geoffjentry is there any update on udocker support or is that already works with some tricks ?. I got same question for singularity as well.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-385569620:26,update,update,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-385569620,1,['update'],['update']
Deployability,@geoffjentry it was partial thought :) My mind can be scattered. Ive update the comment,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-333223653:69,update,update,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-333223653,1,['update'],['update']
Deployability,"@geoffjentry sounds like this will be necessary in a CaaS world, do you agree?. As a **FC/GOTC developer**, I want **Cromwell to test with Cloud SQL after every release**, so that I can **avoid critical (? @helgridly a bug in Cloud SQL would be critical, right?) regressions and issues in production**.; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302:161,release,release,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302,1,['release'],['release']
Deployability,"@geoffjentry thanks for the update, I'll keep an eye on the pull request.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511141850:28,update,update,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511141850,1,['update'],['update']
Deployability,"@geoffjentry yes I'm fine with merging this after release (or the release branch is cut), barring any substantive problems turned up by eagle-eyed reviewers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203034975:50,release,release,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203034975,2,['release'],['release']
Deployability,"@geoffjentry yes. My current deployment is v42. If you have access to the GATK forums, i put more details in my post there: https://gatkforums.broadinstitute.org/wdl/discussion/24268/aws-batch-randomly-fails-when-running-multiple-workflows/p1?new=1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-514937738:29,deploy,deployment,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-514937738,1,['deploy'],['deployment']
Deployability,"@geoffjentry, is the MetadataSummarizer config toggled with the `metadata-summary-refresh-interval` in the reference configuration?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-347328631:47,toggle,toggled,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-347328631,2,"['configurat', 'toggle']","['configuration', 'toggled']"
Deployability,"@geoffjentry, thanks for clearing that up. I guess I know more if the documentation gets updated. @ffinfo, thanks for picking this up!. We do have a very well trained HPC admin team that will bash (hehe) anyone abusing the SGE queque master too much.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424758349:89,update,updated,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424758349,2,['update'],['updated']
Deployability,"@geoffjentry, thanks for the answer :); Actually, we tried to do something similar to how it was done in GCP (or TES) but it didn't work out. We added logging to the `mapCommandLineWomFile` method so that we can see what `womFiles` Cromwell passes to this method. And it turns out Cromwell never passes ""ad hoc"" files to this method, therefore `asAdHocFile`, for example, always returns `None`. In particular, in our integration test (PR #5057) it passes only two womFiles with values something like ; `s3://bucket-name/cromwell-execution/cwl_temp_file_some-numbers.cwl/some-numbers/call-test`; and; `s3://bucket-name/cromwell-execution/cwl_temp_file_some-numbers.cwl/some-numbers/call-test/tmp.59740063`; I'm not sure, but it looks like the first `womValue` somehow related to the `runtimeEnvironment` field in the `StandardAsyncExecutionActor`. The second value is something else too, since ""ad hoc"" files are placed in `call-test` directory.; It is possible that we misunderstood something, but for now, it looks like a dead-end.; By the way, we also tried to override `localizeAdHocValues` method `AwsBatchAsyncBackendJobExecutionActor` so that it would copy ""ad hoc"" files to the ""/cromwell_root"" directory. It fails with an AccessDeniedException.; I hope this gives you an understanding of why we came to the proposed solutions. As I said, perhaps we misunderstood something, so we will be happy if you can give us some hint.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509765587:417,integrat,integration,417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509765587,1,['integrat'],['integration']
Deployability,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:125,update,update,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948,1,['update'],['update']
Deployability,@grsterin I have updated the key to replace 'Connected Components' with 'In-memory Storage'.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554533453:17,update,updated,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554533453,1,['update'],['updated']
Deployability,@grsterin LGTM. Reminder to also make a hotfix edition of this change,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5994#issuecomment-719027140:40,hotfix,hotfix,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5994#issuecomment-719027140,1,['hotfix'],['hotfix']
Deployability,"@grsterin added support for the updated backend and tested with the same workflow/inputs as above, but using the new backend and all seems to be well. Thanks for taking a look at this!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-582120445:32,update,updated,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-582120445,1,['update'],['updated']
Deployability,@grsterin remember to update the PR base before merging.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5255#issuecomment-550390960:22,update,update,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5255#issuecomment-550390960,1,['update'],['update']
Deployability,@guma44 we have moved to Jira and this ticket is in review at the moment on our new board. Please check out this ticket https://broadworkbench.atlassian.net/browse/BA-5933 or this Pull Request for updates https://github.com/broadinstitute/cromwell/pull/5180,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-535943929:197,update,updates,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-535943929,1,['update'],['updates']
Deployability,"@helgridly @davidangb for context, our understanding is that the absolute worst case here is, ""Cromwell might make a change, meaning that Rawls/Agora won't validate something that Cromwell could run - if only it were to be submitted"". Or alternatively, ""Cromwell will regress and no longer succeed a workflow that Rawls thinks is fine"". Given the slow rate of change in the WDL draft-2 libraries recently, and the (very significant) pain in updating Rawls and Agora every release, that feels like a reasonable position for a few months while we await the switchover to womtool-as-a-service. Are we missing something?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-489665477:472,release,release,472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-489665477,1,['release'],['release']
Deployability,"@hmkim ; I continue the break point to run it again, it works now.; What part of process takes long idle time in your instance? what makes the long idle time?; In fact, the pipeline always consists of multiple processes and works on hundreds of samples. ; In case of time, what should i config to avoid this errors not run it again?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-439905197:173,pipeline,pipeline,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-439905197,1,['pipeline'],['pipeline']
Deployability,"@horneth - so I see 2 Futures in here. One the little bit changing the state (around the Props) and the other was resolveAndEvaluate. As background for my statement I'll say the following:; - We've already seen firsthand the havoc which can erupt from having Futures rolling around inside an Actor. They break the Actor Model's abstraction that the internals of an actor are single threaded, meaning you now have to reason about shared mutable state, etc. We _can_ do that, but there are easier paths than actors to deal with that. We've been better about this recently but my concern is that it's too easy for stuff like that to sneak into what were previously pure Futures. Mixing Futures & Actors is not really a great idea.; - There are two async operations in the actor, which means that it is certainly doing two different things (I'll admit that the creation of an actor is a fairly lame 'thing'), disrupting Akka's mantra that actors should do one thing only. What I was suggesting was that the work being performed by these Futures be themselves pushed to their own actors. When they complete they can message back to this one, and those messages could be use to manage state transitions and such. (and to be clear, this is _not_ our little 'tol' code phrase - it's something I think we need to be much better about as we refactor cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678:267,rolling,rolling,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678,2,['rolling'],['rolling']
Deployability,@illusional I have also provided jars here: https://github.com/rhpvorderman/cromwell/releases/tag/50-dev-lumctest,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-600018995:85,release,releases,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-600018995,1,['release'],['releases']
Deployability,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:141,configurat,configuration,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399,6,['configurat'],['configuration']
Deployability,@illusional I'm sure it's true that squashfs is a build requirement. The admins installed it on our cluster worker nodes for that reason. So yes we should probably mention that in the Singularity installation section. Is that what you're asking?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468090428:80,install,installed,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468090428,2,['install'],"['installation', 'installed']"
Deployability,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:251,configurat,configuration,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330,1,['configurat'],['configuration']
Deployability,"@illusional, I've added Singularity installation docs, and udocker cache docs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464537286:36,install,installation,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464537286,1,['install'],['installation']
Deployability,@ilovezfs Ok. Will update the PR in a few.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067822:19,update,update,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067822,1,['update'],['update']
Deployability,@iyanuobidele can you please pull it and try one more round of integration testing with the Spark cluster ? ; @geoffjentry : I rebased it with the develop branch let me know how does it look ? I will merge it then. Thank you.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512:63,integrat,integration,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512,1,['integrat'],['integration']
Deployability,"@jacarey as per what @mcovarr said, can you update the PR to be against develop?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/179#issuecomment-139891013:44,update,update,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/179#issuecomment-139891013,1,['update'],['update']
Deployability,@jainh Can you update / help clarify the readme docs for spark?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283210838:15,update,update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283210838,1,['update'],['update']
Deployability,"@jainh Yes, there are multiple `--conf` attributes.; If there is no space in the value of `--conf` attribute, single quote is not needed; otherwise, I think it's needed. However the [gatk-launch](https://github.com/broadinstitute/gatk/blob/70edbb6e4caa2b7cf1b8678450443c0c590a2b76/gatk-launch) in GATK beta 4 does not produce the single quote for such case; but if I run the following without single quote, it leads to error:; >Error: Unrecognized option: -Dsamjdk.use_async_io_read_samtools=false. command:; ```; /opt/spark-latest/bin/spark-submit --master spark://localhost:6066 --deploy-mode cluster \; --driver-cores 4 --driver-memory 8g --executor-memory 4g --total-executor-cores 10 \; --conf 'spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false' \; --conf 'spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true' \; ...; ```; Here is a related [post](https://stackoverflow.com/questions/28166667/how-to-pass-d-parameter-or-environment-variable-to-spark-job) on stackoverflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-330666862:583,deploy,deploy-mode,583,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-330666862,1,['deploy'],['deploy-mode']
Deployability,"@jbakerpmc GCP Batch reference disks are broken in Cromwell 87, you'll need to run from the `develop` branch at least until Cromwell 88 is released.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2377234954:139,release,released,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2377234954,1,['release'],['released']
Deployability,"@jbakerpmc there have been no changes to reference disk localization configuration between GCP Batch and PAPI v2 beta that I'm aware of. You can `include` if you prefer to keep reference disk config in a separate file, or just inline to your main config if you prefer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2375382005:69,configurat,configuration,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2375382005,1,['configurat'],['configuration']
Deployability,@jdidion I talked to a site admin and this setting should be updated. Please let me know if you run into this error again and I will add you manually and continue to communicate with our site admin about this issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-499936542:61,update,updated,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-499936542,1,['update'],['updated']
Deployability,"@jgainerdewar Latest updates [here](https://github.com/broadinstitute/cromwell/pull/7000). Note that the error in the CI build seems to be a test error related to call caching, and way above that in the build spew there were notifications about not having access to `ubuntu:latest`. Not sure if those two observations are related. I also don't know if the build/test failures are related to the fact we haven't merged the latest changes from `develop` into this PR yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995:21,update,updates,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995,1,['update'],['updates']
Deployability,"@jgainerdewar those Java files are auto-generated, so we would need to update the parser-generator in order to maintainably address the warnings. https://github.com/broadinstitute/cromwell/blob/622c8e6b79b4ce123912ace0ed37b28f1c461324/wdl/transforms/draft3/src/main/java/wdl/draft3/parser/WdlParser.java#L2-L14",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279624277:71,update,update,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279624277,1,['update'],['update']
Deployability,@jishuxu @rexwangcc -- have you updated your Cromwell version recently?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-413966538:32,update,updated,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-413966538,1,['update'],['updated']
Deployability,"@jsotobroad I believe the integration tests you set up for Green are covered by this ticket, do you agree?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247:26,integrat,integration,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247,1,['integrat'],['integration']
Deployability,@katevoss ; I would like to add our use-case for the ability to access an ID of a workflow/subworkflow. Our users want the output of the Cromwell to be copied to their local locations and they complain that they cannot read the directory structure - I agree with them as they are data scientists and they want something useful after the pipeline finishes. As we provide the service we are responsible to provide them with something. An idea was to use [croo](https://github.com/ENCODE-DCC/croo) to achieve this. It is really useful solution but it requires manual intervention and knowledge of the pipeline IDs etc. Thus I though I could split the workflow into root and two sub-workflows: `do-the-job` and `copy-files`. However to achieve this the `copy-files` would need to have an access to the `do-the-job` sub-workflow ID or at least the root workflow ID to query for the metadata. I agree it is not deterministic and it should not be. Such a task cannot be cached too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825:337,pipeline,pipeline,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825,2,['pipeline'],['pipeline']
Deployability,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:496,release,release,496,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,"@katevoss I'm one of the developers of Singularity and I would like to +1 this request! I don't know scala, but if it comes down to making an equivalent folder [like this one for Docker](https://github.com/broadinstitute/cromwell/tree/9aff9f2957d303a4789801d6a482777faf47d48f/dockerHashing/src/main/scala/cromwell/docker) I can give a first stab at it. Or if it's more helpful I can give complete examples for all the steps to working with singularity images. We have both a registry ([Singularity Hub](https://singularity-hub.org) that is hooked up to the singularity command line client to work with images. So - to integrate into cromwell you could either just run the container via a singularity command, or implement your own connection to our API to download the image. Please let me know how I might be helpful, and I'd gladly help. If you want me to give a go at scala I would just ask for your general workflow to compile and test functionality.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968:618,integrat,integrate,618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968,1,['integrat'],['integrate']
Deployability,"@katevoss I've re-tested with release 29 and this works as expected now — and may have been working for some time, I haven't revisited this for a while. Thanks to all for the fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1702#issuecomment-331019025:30,release,release,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702#issuecomment-331019025,1,['release'],['release']
Deployability,"@katevoss This one is important. Although I am open to alternatives, I believe that we need a way to set any parameter from the json file -- no matter how deeply buried the parameter is within subworkflows, etc. Having to explicitly expose parameters has become too big a hardship on developers and has now led to a bugfix GATK4 release with another one forthcoming.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2912#issuecomment-363466390:329,release,release,329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2912#issuecomment-363466390,1,['release'],['release']
Deployability,"@katevoss commented on [Thu Dec 08 2016](https://github.com/broadinstitute/dsde-docs/issues/1515). - [ ] Link to [Dev Docs](https://software.broadinstitute.org/wdl/devzone), latest version and release notes, blog, contact us (Slack and Forum), WDL Spec, [WDL website](https://software.broadinstitute.org/wdl/), [GATK website](https://software.broadinstitute.org/gatk/); - [ ] Link and describe the following files: [Authors](https://github.com/broadinstitute/cromwell/blob/develop/AUTHORS), [Changelog](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md), [Apache License](https://github.com/broadinstitute/cromwell/blob/develop/LICENSE-ASL-2.0), [Broad License](https://github.com/broadinstitute/cromwell/blob/develop/LICENSE.txt), [Migration](https://github.com/broadinstitute/cromwell/blob/develop/MIGRATION.md), [Making a Backend](https://github.com/broadinstitute/cromwell/blob/develop/MakingABackend.MD), [Notice](https://github.com/broadinstitute/cromwell/blob/develop/NOTICE), [Security](https://github.com/broadinstitute/cromwell/blob/develop/SecurityRecommendations.md).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1768:193,release,release,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1768,1,['release'],['release']
Deployability,@kbergin Having this feature will help us remove all of our private docker images from pipeline-tools and will make it much easier to write adapter workflows in the future,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4236#issuecomment-429117857:87,pipeline,pipeline-tools,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4236#issuecomment-429117857,1,['pipeline'],['pipeline-tools']
Deployability,"@kbergin `womtool validate` now (technically, next time Cromwell is released) has an optional flag to supply an `inputs.json` to validate against - is that what you meant?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-386009040:68,release,released,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-386009040,1,['release'],['released']
Deployability,"@kcibul ; I am missing few things in the implementation like the way to construct Spark command inside Spark backend for this PR, Let me add that to make it Spark'ified. It should be same like Htcondor but entertain Runtime attributes like executor-memory, cores and configuration based master to execute Spark job both in Dockerized or non-Dockerized mode. ; So let me close this pull request (**I will not merge it** ) and create a new one after changes. My goal is to create Spark backend to run Spark jobs in standalone cluster mode for this PR, then later to add other Resource manager related support. ; Does it make sense ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231182019:267,configurat,configuration,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231182019,1,['configurat'],['configuration']
Deployability,"@kcibul A few questions about this:; - Should it support in-place DB migration, or DB A to DB B, or both ?; - Is it a separate program that runs independently ? If yes does it still live in the Cromwell repo ? Or a new flag in cromwell (like server, run, migration), ? Is there any ""automagic"" detection that my DB needs an update when I run cromwell 0.20 on a pre-0.20 DB ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-226247983:324,update,update,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-226247983,1,['update'],['update']
Deployability,@kcibul I strike to close this - no one has answered your last question for multiple months now and upgrades to 0.2x are underway,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-253877756:100,upgrade,upgrades,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-253877756,1,['upgrade'],['upgrades']
Deployability,@kcibul This is important. I was testing to see if the new release fixed the hanging issue previously reported #1649,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265341832:59,release,release,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265341832,1,['release'],['release']
Deployability,@kcibul next time I won't update tests or docs to keep the file count down ;),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669:26,update,update,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669,1,['update'],['update']
Deployability,"@kcibul np, we just need to remember to cherry pick it into master and update staging when merged",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/153#issuecomment-134213848:71,update,update,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/153#issuecomment-134213848,1,['update'],['update']
Deployability,@knoblett is this still blocking Cromwell's call caching because calls aren't updated as succeeded? I haven't heard this come up in a while.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-327935802:78,update,updated,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-327935802,1,['update'],['updated']
Deployability,"@kshakir @mcovarr re the Either, after dusting off our set theory books and exploring the options it looks like the two of us chiming in from the peanut gallery have agreed that the right thing here would be to set up a little sealed trait/case class ADT to capture the disjunction here. . \/ has a built in right bias. Either has an implicit right bias in the way it's used. The UnionTypes @mcovarr mentioned are, well, interesting. Then consider that this signature is being used in two places and it seemed like rolling our own sum type would be the way to go here. Agree? Disagree?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-142310052:515,rolling,rolling,515,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-142310052,1,['rolling'],['rolling']
Deployability,"@kshakir Beyond unit tests, how was this tested? It'd be good to try to throw this at some live mysql installations of various configurations to make sure it doesn't blow up",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324406500:102,install,installations,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324406500,2,"['configurat', 'install']","['configurations', 'installations']"
Deployability,"@kshakir I updated the swagger description of the input files in the submit endpoint to say ""JSON or YAML"". It doesn't get automatically updated in the docs but @geoffjentry said we'll figure that out when @katevoss is back.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2832#issuecomment-343156709:11,update,updated,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2832#issuecomment-343156709,2,['update'],['updated']
Deployability,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:481,update,updated,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627,4,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,@kshakir Updated the release doc with instruction for generating a commit list and adding the output to the release ticket.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6160#issuecomment-764008813:9,Update,Updated,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6160#issuecomment-764008813,3,"['Update', 'release']","['Updated', 'release']"
Deployability,"@kshakir commented on [Mon Feb 27 2017](https://github.com/broadinstitute/wdl4s/issues/92). The following wdl currently parses in wdl4s without error, later causing an error within cromwell. ```; workflow undeclared_scatter_variable {; scatter (i in undeclared) {}; }; ```. See https://github.com/broadinstitute/cromwell/issues/2020 for more info of cromwell patches, including a centaur test, that may be updated or removed if this is fixed in wdl4s.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2699:359,patch,patches,359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2699,2,"['patch', 'update']","['patches', 'updated']"
Deployability,"@kshakir commented on [Mon Feb 27 2017](https://github.com/broadinstitute/wdl4s/issues/93). The following wdl currently parses in wdl4s without error, later causing an error within cromwell. ```; task x {; Int i; command { echo $i }; runtime { docker: ""ubuntu"" }; output {; Int out_but_intentionally_misnamed = i; Boolean validOutput = i % 2 == 0; }; }. workflow missing_optional_output {; Array[Int] arr = [0,1,2,3]; scatter (i in arr) {; call x { input: i = i }; if (x.validOutput) {; Int x_out = x.out_except_undeclared; }; }. Array[Int?] x_out_maybes = x_out; }; ```. See https://github.com/broadinstitute/cromwell/issues/2020 for more info of cromwell patches, including a centaur test, that may be updated or removed if this is fixed in wdl4s.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2700:657,patch,patches,657,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2700,2,"['patch', 'update']","['patches', 'updated']"
Deployability,"@kshakir good point thanks, I'll update local too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389289995:33,update,update,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389289995,1,['update'],['update']
Deployability,@kshakir is this the library that we downgraded the last time we tried to upgrade all of our libraries?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4701#issuecomment-469719251:74,upgrade,upgrade,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4701#issuecomment-469719251,1,['upgrade'],['upgrade']
Deployability,@kshakir updated the config value,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-274852497:9,update,updated,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-274852497,1,['update'],['updated']
Deployability,"@ktibbett I finally have time to follow up on this, can you explain more about why you want the WDL pipeline to copy workflow outputs into two locations?; How often do you face this problem? Is it a weekly/daily/monthly thing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1642#issuecomment-325751623:100,pipeline,pipeline,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1642#issuecomment-325751623,1,['pipeline'],['pipeline']
Deployability,@ktibbett can you comment if this is something you need patched into hotfix? Or something you can tolerate until you're on 0.20+,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-230537661:56,patch,patched,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-230537661,2,"['hotfix', 'patch']","['hotfix', 'patched']"
Deployability,"@ktibbett is this still a feature that would help the production pipeline? ; @geoffjentry aside from the risk of duplicate naming for output and logs, are there any other risks involved? What would be the effort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959:65,pipeline,pipeline,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959,1,['pipeline'],['pipeline']
Deployability,"@ldgauthier has the actual use case. We've already upgraded the methods cromwell. Is there a way to scatter over an iterator, so the whole list does not have to be read into RAM?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338348644:51,upgrade,upgraded,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338348644,1,['upgrade'],['upgraded']
Deployability,@leipzig can you please target the `develop` branch rather than `master`? We only merge to `master` on releases. 🙏,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6555#issuecomment-1024498874:103,release,releases,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6555#issuecomment-1024498874,1,['release'],['releases']
Deployability,"@likeanowl - Took a look at the PR. Overall, looks good, but had a couple questions. Do the new integration tests you mention cover the points I brought up - i.e. mostly around default credentials use and default region config?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-523568967:96,integrat,integration,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-523568967,1,['integrat'],['integration']
Deployability,@likeanowl any update on the test case?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-547451533:15,update,update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-547451533,1,['update'],['update']
Deployability,"@markjschreiber . I tested your theory, and while the job was able to complete successfully the second time around (after changing the job definition), it didn't update the status in the Cromwell database. Do you reckon it should be possible for me to manually change a record in the database in order to get cromwell to continue where it left off, or will I need to resubmit the entire workflow, and hope that CallCaching is working?. In this particular workflow I'm running, I've observed that CallCaching works.... sometimes(?).... but I was surprised by the amount of Cache misses I observed, which I'm not really sure how to troubleshoot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775:162,update,update,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775,1,['update'],['update']
Deployability,@mcovarr -- I think this is part of what you've done addressing #1065. Can you update here?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1015#issuecomment-229632978:79,update,update,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1015#issuecomment-229632978,1,['update'],['update']
Deployability,"@mcovarr ; because docker does not run without root on Linux. Although, there is a workaround:; ```; sudo usermod -aG docker $USER; ```; But after doing this it did not solve a problem. I am using the last release because I have dependency problems when building from master.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283005522:206,release,release,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283005522,1,['release'],['release']
Deployability,@mcovarr @Horneth - the code is slightly different than the hotfix to account for slightly different realities (plus a moving-stuff-around refactor) but more or less the same.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/864#issuecomment-220720400:60,hotfix,hotfix,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/864#issuecomment-220720400,1,['hotfix'],['hotfix']
Deployability,@mcovarr @Horneth . I haven't looked but I'm guessing the non-hotfix will be nearly (if not fully) identical so I'll wait until this is done before proceeding on the real one.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/860:62,hotfix,hotfix,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/860,1,['hotfix'],['hotfix']
Deployability,"@mcovarr @cjllanwarne I made substantial changes to allow for automatic release number calculation and added the few things we talked about (pin centaur branch, add hotfix branch). It still has command injection though...; I tested it on a fork and as far as I can tell everything looked good.; If you don't mind re-giving it a look, otherwise I'll probably merge it as is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325:72,release,release,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325,2,"['hotfix', 'release']","['hotfix', 'release']"
Deployability,@mcovarr @cjllanwarne updated again,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/362#issuecomment-170628682:22,update,updated,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/362#issuecomment-170628682,1,['update'],['updated']
Deployability,@mcovarr @kcibul I updated the examples above to reflect the SQL queries generated post PR feedback. Main changes:; - No `EXISTS` for the types of query that Job Manager 0.5.9 produces (labelsAnd and labelsOr).; - Because we now use `JOIN`s for both `LabelAND` and `LabelOR` queries.; - Using `1` instead of `*` for those sub-selects. Would appreciate any other thoughts,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469889241:19,update,updated,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469889241,2,['update'],['updated']
Deployability,@mcovarr @wdesouza I will try to update this pr asap,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949583536:33,update,update,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949583536,1,['update'],['update']
Deployability,@mcovarr I forgot this before… you should update the changelog to include that statsdproxy was removed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1012098813:42,update,update,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1012098813,1,['update'],['update']
Deployability,@mcovarr I investigated as part of the ticket. I wasn't able to recreate the problem but I couldn't see why the option to update the abort function should be disallowed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164849222:122,update,update,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164849222,1,['update'],['update']
Deployability,"@mcovarr I should have made that on the hotfix one, it's more in how that's communicated (and if they even care)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292596812:40,hotfix,hotfix,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292596812,1,['hotfix'],['hotfix']
Deployability,"@mcovarr I think the intention is to start moving people ASAP to WDL 1.0 once it's released next month. We'll need to support workflows which predate that, but it doesn't mean that we need to give those users new functionality and thus reasons to not upgrade",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3388#issuecomment-372489504:83,release,released,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3388#issuecomment-372489504,2,"['release', 'upgrade']","['released', 'upgrade']"
Deployability,"@mcovarr I was running the GotC pipeline x 20. It was scattered quite wide at the time, I believe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1007#issuecomment-226490409:32,pipeline,pipeline,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1007#issuecomment-226490409,1,['pipeline'],['pipeline']
Deployability,@mcovarr I've left the update enabled but added a new warning. So hopefully it's at least as vocal and warn-y as before but won't outright ignore the updated abort function.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164886510:23,update,update,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164886510,2,['update'],"['update', 'updated']"
Deployability,@mcovarr I've updated to report earlier,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/362#issuecomment-170090256:14,update,updated,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/362#issuecomment-170090256,1,['update'],['updated']
Deployability,"@mcovarr Indeed, these tests were checking that updateBackendExecutionStatus was updating the execution table which is no longer its responsibility. Looking over the tests, this seems to be checked for the setStatus call elsewhere.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145657965:48,update,updateBackendExecutionStatus,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145657965,1,['update'],['updateBackendExecutionStatus']
Deployability,"@mcovarr It's more concise but it makes several implicit assumptions to fit exactly our 2 current JES use cases (GotC and FireCloud).; IMO it should be flexible enough so that GotC and Firecloud are just combinations of configuration entries among all the possible ones. > // The JES backend is assumed to use the GCS filesystem with user authentication, dropping back to Cromwell; > // authentication if user authentication is not defined. In GotC there is no user authentication, so; > // Cromwell authentication it is. This is tailor made to accommodate GotC and FireCloud with as few configuration changes as possible, but I think we should move towards something more generic, even if the confs look a bit more different between the two.; For example what if you want to use refresh token auth mode for creating the pipeline as well ?; Or refresh token only for pipeline and service account for gcs ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203453031:220,configurat,configuration,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203453031,4,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"@mcovarr Need to update the readme and changelog, thanks for the reminder :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1652#issuecomment-259157871:17,update,update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1652#issuecomment-259157871,1,['update'],['update']
Deployability,"@mcovarr README updated, wording okay?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/504#issuecomment-191889151:16,update,updated,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/504#issuecomment-191889151,1,['update'],['updated']
Deployability,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, I’m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:83,update,updated,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124,1,['update'],['updated']
Deployability,@mcovarr any chance of getting it re-released as 28.1 or 29? Unfortunately users will just get a checksum mismatch error if the jar is already in their cache since cromwell is `bottle :unneeded`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288:37,release,released,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288,1,['release'],['released']
Deployability,"@mcovarr as first reviewer. Travis already seems configured to ignore integration tests, so no other changes appeared necessary for this temporary fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169440511:70,integrat,integration,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169440511,1,['integrat'],['integration']
Deployability,"@mcovarr correct, the more general problem exists throughout cromwell though and I didn't want to get into a rabbit hole with this but after digging a bit more it's actually not that bad. I'll update the PR with some better crash handling instead",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229134405:193,update,update,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229134405,1,['update'],['update']
Deployability,"@mcovarr fixed this in hotfix, so please check that out when fixing this in develop",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757#issuecomment-217263652:23,hotfix,hotfix,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757#issuecomment-217263652,1,['hotfix'],['hotfix']
Deployability,@mcovarr https://github.com/broadinstitute/wdl4s/pull/6. Also I updated the code in this PR,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/378#issuecomment-171758540:64,update,updated,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/378#issuecomment-171758540,1,['update'],['updated']
Deployability,@mcovarr is there a reason why your test WDL is installing `jq` in the command rather than picking a docker image which already has it installed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-461103349:48,install,installing,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-461103349,2,['install'],"['installed', 'installing']"
Deployability,@mcovarr would you be content if I made a mock `PipelinesApiRequestWorker` that always crashes and check that the manager handles it?. I'm also thinking about introducing error types at this interface in the stack so that explosions in Google code don't percolate into Cromwell; ```; at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-492863137:48,Pipeline,PipelinesApiRequestWorker,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-492863137,4,"['Pipeline', 'pipeline']","['PipelinesApiRequestWorker', 'pipelines']"
Deployability,"@mcovarr, @cjllanwarne agree on your comments, I implemented a PoC on graph topology creation using topological sort algorithm based on DFS calculation at the beginning of this year. The idea was to propose the integration of that concept to Cromwell after PBE.; Coming back to this issue, If we need to recreate a DAG in the backend side it means there is something wrong with that backend (we know it). I explained why we try to do so in the Waffle.io ticket but I think we can explain better in the next meeting we may have. Anyways @jainh will not proceed with this change.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236381218:211,integrat,integration,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236381218,1,['integrat'],['integration']
Deployability,"@mcovarr, looks like I got past the initial issue but now getting the following error:; ```; [2021-08-25 01:11:31,83] [info] WorkflowManagerActor: Workflow 2a7b8039-a555-4f58-86b0-dc4a6fa21dff failed (during ExecutingWorkflowState): java.lang.Exception: Task cumulus.cluster:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. generic::failed_precondition: Constraint constraints/compute.trustedImageProjects violated for project gred-cumulus-sb-01-991a49c4. Use of images from project cloud-lifesciences is prohibited.; ```; Looks like our GCP accounts don't allow non standard images. Which image is this workflow trying to use? Is there a way to provide our own image to this pipeline instead? . Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905094601:708,pipeline,pipeline,708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905094601,1,['pipeline'],['pipeline']
Deployability,@meganshand I've confirmed this bug is resolved in release 0.21! I'm closing this issue for that reason.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/781#issuecomment-252242676:51,release,release,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/781#issuecomment-252242676,1,['release'],['release']
Deployability,"@meganshand if this is still happening on a modern cromwell, please open a new ticket w/ updated info",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-414067237:89,update,updated,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-414067237,1,['update'],['updated']
Deployability,@meganshand thanks for the update! I'll lower this in priority.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-289777291:27,update,update,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-289777291,1,['update'],['update']
Deployability,@mwalker174 thanks for circling back to this and giving us this update!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4873#issuecomment-488065955:64,update,update,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873#issuecomment-488065955,1,['update'],['update']
Deployability,@natechols we are currently releasing every 3 weeks and next week is when we are aiming to release 41. We'll make sure to circle back to this PR and work with you on what to test when we have more time. The goal would be to get this released in 42 if all goes well!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488378799:91,release,release,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488378799,2,['release'],"['release', 'released']"
Deployability,"@noblem The primary difference is that the path forward to have the spec change is to present a PR and not to have an issue open. My point in my last reply is that someone might see it at some point and make a PR for you but there's no guarantee of that happening in a timely fashion whereas opening a PR against the spec yourself does guarantee that happening :). FWIW I realize that the switcharoo in process can be frustrating. As an example of how we feel the pain as well, there are cases where we (Cromwell team) forgot to update the spec document when we made changes which now means that those changes aren't actually part of the spec. So for instance [this PR](https://github.com/openwdl/wdl/pull/148) is just trying to specify behavior Cromwell already does but is likely to wind up different than what we did & thus making Cromwell non-conforming. That's what we get for being lazy 6 months ago :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-342223670:529,update,update,529,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-342223670,1,['update'],['update']
Deployability,"@nrockweiler hello, please refer to https://github.com/broadinstitute/cromwell/issues/2916 for requester pays updates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3449#issuecomment-387113756:110,update,updates,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3449#issuecomment-387113756,1,['update'],['updates']
Deployability,@nrockweiler requester pays buckets can be used in Cromwell when the Cromwell 32 is released. It is scheduled for this month,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916#issuecomment-390666394:84,release,released,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916#issuecomment-390666394,1,['release'],['released']
Deployability,"@orodeh I don't think we'll do a hotfix for this back into 36, but the better news is that 37 isn't too far away, so it shouldn't be too long before this gets released.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4559#issuecomment-455640456:33,hotfix,hotfix,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4559#issuecomment-455640456,2,"['hotfix', 'release']","['hotfix', 'released']"
Deployability,@orodeh I would probably start with something in `WdlFileToWomSpec`. If you wanted to do that you'll probably need to find a way to customize the configuration for just that test case,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-510842794:146,configurat,configuration,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-510842794,1,['configurat'],['configuration']
Deployability,"@orodeh thanks for reporting, the fix will be included in the next release of Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3811#issuecomment-422889488:67,release,release,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3811#issuecomment-422889488,1,['release'],['release']
Deployability,@osha52101 - please run this with the latest release (87). I believe this issue was resolved.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7408#issuecomment-2110532858:45,release,release,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7408#issuecomment-2110532858,1,['release'],['release']
Deployability,"@patmagee not to my knowledge but I can start the ball rolling. ; @geoffjentry & @mcovarr, any opinions on supporting enums?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325807471:55,rolling,rolling,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325807471,1,['rolling'],['rolling']
Deployability,@pshapiro4broad ; Is there any update on this or planned inclusion? I am also running into the same issue. It would be nice if Cromwell supported the latest specification(s) of WDL.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-870046753:31,update,update,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-870046753,1,['update'],['update']
Deployability,"@raylim At the moment, no. Also note that if you're using the google backend that the pipelines API currently has the same limitation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2593#issuecomment-327320882:86,pipeline,pipelines,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2593#issuecomment-327320882,1,['pipeline'],['pipelines']
Deployability,"@rhpvorderman I usually solve it by:; ```; ln -s ~{bam} ~{basename(bam)}; ```; However, even though I have workarounds I think it is a big flow of Cromwell that it allows the tools to mutate the Input folder instead of writing to executions. I believe Inputs should be immutable by design. Many new Cromwell users can waste hours (like I did) before untile they realize that some tools produce output but write in Inputs instead of execution. P.S. Also, I am a big fun of BioWDL and I get inspiration from it in my pipelines. However, as the workflows of our lab are different I do not git-clone them but prefer to implement my own versions. I am also open to contribute the tasks for the tools that are not in BioWDL but which I wrote in my pipelines and which may be interesting for many users",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618714864:515,pipeline,pipelines,515,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618714864,2,['pipeline'],['pipelines']
Deployability,"@rhpvorderman to give you an update - I agree that there's something wrong here, but I can't identify which component requires updating - the WDL spec, the WDL grammar in spec repo, or Cromwell. I will consult my teammate @cjllanwarne when he comes back from vacation in about 1.5 weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-415466354:29,update,update,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-415466354,1,['update'],['update']
Deployability,"@rsasch @cjllanwarne I have updated the diagram and added a key. Regarding (Chris's) Comment 3, we can surely go 5000ft higher and create a 10000ft diagram. I will take a look. Somehow 10000ft higher is harder to visualize! :) ; Update: Ticket for The 10000ft view of actor system has been created. JIRA ticket [WA-67](https://broadworkbench.atlassian.net/browse/WA-67)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551306283:28,update,updated,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551306283,2,"['Update', 'update']","['Update', 'updated']"
Deployability,"@rsasch I believe you're next on the release rotation. This might be a nice thing to do prior to the next release. Once an account has been generated, its likely that the release checklist needs a first step of ""generate a token for this user"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-424932935:37,release,release,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-424932935,3,['release'],['release']
Deployability,"@rtitle ; The real meat of this ticket though is less the ""omg the DB barfed"" but rather ""omg we don't do anything smart when omg the DB barfed"" :). That said, the bulk of our data storage *is* separated out, just not practically in our default ""Jane User"" configuration. Currently the ""Jane User"" configuration is the only one which exists. So e.g. for CaaS that's not going to be the case, and we'll probably need to support horizontal scaling scenarios that don't take adantage of GCP tooling for external customers as well. . The lion's share of our DB activity consists of writes coming from the write side of our cqrs to the read side's event store and reads on that event store coming from the API. It's logically all separated out but in stock Cromwell they're sharing the same DB/connection/etc. So e.g. one possibility (which has come up before for other reasons) would be to make it easier for a user to bifurcate those to using separate DBs (or at least separate connections), although you'd still possibly have this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999:257,configurat,configuration,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999,2,['configurat'],['configuration']
Deployability,@ruchim -- can you update this ticket with the current status (e.g. how far we scaled initially)?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/794#issuecomment-248658974:19,update,update,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/794#issuecomment-248658974,1,['update'],['update']
Deployability,"@ruchim . I think it's fine to update the README even though it's not usable - the develop branch is known to be unstable at the moment, and isn't the default branch on github. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/823/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/823#issuecomment-218770232:31,update,update,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/823#issuecomment-218770232,2,['update'],['update']
Deployability,@ruchim @danbills did this get updated in the intellij repository?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-303595288:31,update,updated,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-303595288,1,['update'],['updated']
Deployability,"@ruchim @kshakir Yes, @Horneth could you please add to hotfix branch? They are getting 32 ready for FC, would like to get this in there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3715#issuecomment-393910783:55,hotfix,hotfix,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3715#issuecomment-393910783,1,['hotfix'],['hotfix']
Deployability,"@ruchim Any updates on this issue? We are running this issue with CaaS/Cromwell again, unfortunately :( The current workaround is, for a query of 1000 workflows, we send 1000 individual requests to the Cromwell, which could also put pressure to Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3115#issuecomment-453156671:12,update,updates,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3115#issuecomment-453156671,1,['update'],['updates']
Deployability,@ruchim I believe this is now supported as of Cromwell 27 (released yesterday).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2106#issuecomment-305232962:59,release,released,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2106#issuecomment-305232962,1,['release'],['released']
Deployability,"@ruchim I should clarify. There are a lot of people at this workshop (if not all of them) that cannot use the cloud, due to regulations, clinical requirements, etc. Or they need to be able to run WDL on their local on-prem compute cluster for testing on small cohorts, etc. This is a common configuration that prohibits docker. We really do not want these users to be forced to change the WDL that we (DSP methods) write and test. In order to stay backend-agnostic, can we implement a null option for docker as described in this issue (and #1804 )?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605:291,configurat,configuration,291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605,1,['configurat'],['configuration']
Deployability,"@ruchim I think @geoffjentry is spot on - adding a scope by itself won't change existing behavior. It's only when the user sets `monitoring_image` to `quay.io/broadinstitute/cromwell-monitor-bigquery`, _then_ it will fail if the SA for the task doesn't have `bigquery.tables.updateData` permission on the monitoring dataset. So existing users on Terra won't be affected, unless we start routinely adding that option to all workflows and don't adjust the IAM permissions on pets.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502242348:275,update,updateData,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502242348,1,['update'],['updateData']
Deployability,"@ruchim I updated the description and title, this is not nearly as bad as the previous title made it sound. It's a very weird case found in the centaur test failures that I'm looking at.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4202#issuecomment-427022611:10,update,updated,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4202#issuecomment-427022611,1,['update'],['updated']
Deployability,"@ruchim I'm not sure about the details, we have a monitor script (https://github.com/HumanCellAtlas/pipeline-tools/blob/c6c11a20c91aa360fcd7ca7c28de14b281cabd7b/adapter_pipelines/ss2_single_sample/options.json#L2) running as workflow options besides the actual RSEM tool, which is monitoring the disk space. it outputs:; ```; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 19: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 13: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; ``` ; but not exit codes. Do you think it's possible to add some error handling to that bash script to let cromwell know the out of space error during the runtime? Even if it's practical to do that, it may still not as safe as the exit code throw by the actual tool. so wait for @jishuxu's response.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517:100,pipeline,pipeline-tools,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517,1,['pipeline'],['pipeline-tools']
Deployability,"@ruchim Just fyi, the workflows ran fine with preemptions occuring in 0.21 release",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1690#issuecomment-261690558:75,release,release,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1690#issuecomment-261690558,1,['release'],['release']
Deployability,"@ruchim Thanks for the update. I can find a workaround, e.g. reducing the number of exposed outputs; it's just that this will affect others using the same workflow on e.g. DNAnexus . If I try to make a PR myself, based on what's done for AWS_CROMWELL_INPUTS_GZ, do you think you'd be able to look at it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-445982069:23,update,update,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-445982069,1,['update'],['update']
Deployability,"@ruchim That's fine, this isn't blocking me right now, as long as I have something to tell my boss. However if someone has time to suggest what tests I should add for Postgresql, I can try to have those added by the time you're ready to review the entire mess. (Or should it just be everything you're testing for MySQL? This seems easy enough to add but I'm nervous about bloating your travis-ci runtimes.). Just out of curiosity, what is the timeline for Cromwell releases this year? I am fine using my fork for now but eventually we want to be able to use the official jarfile.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488330109:465,release,releases,465,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488330109,1,['release'],['releases']
Deployability,@ruchim and @kshakir: Any update on this? This is for our piplines a real blocker to bring our pipelines to production. See also https://github.com/biowdl,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4050#issuecomment-418056753:26,update,update,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050#issuecomment-418056753,2,"['pipeline', 'update']","['pipelines', 'update']"
Deployability,"@ruchim here are the two use cases:; As a user, I want to update an existing label after submission.; As a user, I want to remove an existing label after submission. This is outside of JES, no increase or decrease of ""JESification"" for now.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2332:58,update,update,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2332,1,['update'],['update']
Deployability,"@ruchim updated the issue, I'm thinking we could add them to centaur too eventually",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4163#issuecomment-425088460:8,update,updated,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4163#issuecomment-425088460,1,['update'],['updated']
Deployability,@salonishah11 I believe we now have upgrade tests in our CI which would catch that. @mcovarr @kshakir does that sound right?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-490140739:36,upgrade,upgrade,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-490140739,1,['upgrade'],['upgrade']
Deployability,"@salonishah11 it looks like we are already doing an archive status check immediately after the existence check:. https://github.com/broadinstitute/cromwell/blob/6e212299af22c9a3d5cf38d6d518afdcb61ce524/engine/src/main/scala/cromwell/webservice/routes/MetadataRouteSupport.scala#L240-L249. Today on Prod when I open the page for an archived workflow like [this one](https://app.terra.bio/#workspaces/broad-firecloud-dsde/CanaryTest/job_history/61157341-8d2f-4a15-bc6e-e67104c8eab8/63ac4bc1-388c-430d-86f5-d123a7073e3c) (canary workspace) Cromwell responds with the following JSON:. ```; {; ""id"": ""63ac4bc1-388c-430d-86f5-d123a7073e3c"",; ""message"": ""Cromwell has archived this workflow's metadata according to the lifecycle policy. The workflow completed at 2023-08-30T16:39:09.168Z, which was 36384533045 milliseconds ago. It is available in the archive bucket, or via a support request in the case of a managed instance."",; ""metadataArchiveStatus"": ""ArchivedAndDeleted""; }; ```. It comes from `checkIfMetadataDeletedAndRespond` so it seems like the workflow is somehow passing the `validateWorkflowIdInMetadata` existence check despite being archived.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053:420,Canary,CanaryTest,420,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053,2,"['Canary', 'canary']","['CanaryTest', 'canary']"
Deployability,@scala-steward - there may be two small bugs happening in this PR:. 1. I didn't expect scala-steward to continue posting updates to this dependency given that there's a `scala-steward:off` line in the file.; 2. This PR is not actually updating the version of sbt - it's just whitespace tidying?. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5727#issuecomment-670565924:121,update,updates,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5727#issuecomment-670565924,1,['update'],['updates']
Deployability,@scala-steward this string matching has found an incorrect string comment to update.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5632#issuecomment-672904524:77,update,update,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5632#issuecomment-672904524,1,['update'],['update']
Deployability,"@scottfrazer @geoffjentry Should ""getting started"" be updated to point users to our website's documentation, or is this a separate issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/705#issuecomment-212617929:54,update,updated,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/705#issuecomment-212617929,1,['update'],['updated']
Deployability,@scottfrazer @mcovarr this PR and Tyburn (please also check broadinstitute/tyburn#16) have been updated. There are now just two things to tidy up in WDL4S and a change in the WDL spec...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/353#issuecomment-170925365:96,update,updated,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/353#issuecomment-170925365,1,['update'],['updated']
Deployability,"@scottfrazer FWIW my vision of the world was a lot closer to the diagram you drew up but I don't have strong feelings on that. In terms of distributed jars what I'd like to see distributed to the world would be:; - cromwell.jar: full fat jar like we have today which also includes all of the supported backends built in; - cromwell-backend.jar: a jar providing the interface stuff which someone can use to build their own backend jar. I'd be totally okay with (and could see value in):; - cromwell-lite.jar (or something like that): a stripped down fat jar w/o any supported backends or maybe local; - foo-backend.jar for each supported backend. My main concern though is that we always make the most obvious download for a naive user of cromwell to be the one with all of the supported (or perhaps a 'very common supported' subset) backends built in so as to minimize the work someone needs to do to get rolling. The other jars are really an artifact of the multi-project model and can be ignored. The side discussion about `core` is exactly why I was picturing the hierarchy stemming from `backend` all along. Despite the name of `supportedBackends` being my request I'll admit I was just looking at the name when I said that, not thinking the whole thing through critically :). IMO `core` should be code which is shared between all components, I'd call the filesystem concept a component, not something in core itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209656235:905,rolling,rolling,905,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209656235,1,['rolling'],['rolling']
Deployability,"@scottfrazer So the reason I'm asking about the required functionality and JES (and asked if the main issue was the eventual annoying rebase if this isn't merged) is that my concern is that this is a hefty change mid-sprint when we're already concerned w/ the hairiness of our actual sprint goals. For instance what if this causes some unforeseen issue which causes the s/g to not be complete this sprint. We can handwave all we want about what is truly important or not but the only official metric of importance is what's in our sprint and if this disrupts that's no bueno - and regardless of our confidence level there _is_ a risk here. I suppose we could back it out but that'd still likely end up having been a big time disruption at that point. I would feel a lot more comfortable if a large body of WDL was run against JES backend (and Local too, really - though that's less worrisome) - it'd have been nice if someone decided the integration test battery was important enough to work on the side ;) If people have actually been listening to my requests to paste their interesting WDLs on that ticket that'd be a good start, but double check with @cjllanwarne as he wrote a WDL to exercise all the various functionality we supported at the time. . Actually what'd be really awesome is if you could run the WDL they're using for the demo as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661:938,integrat,integration,938,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661,1,['integrat'],['integration']
Deployability,"@scottfrazer Updated w/ your suggestions, appears to work fine in docker",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/42#issuecomment-111221496:13,Update,Updated,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/42#issuecomment-111221496,1,['Update'],['Updated']
Deployability,@scottfrazer updated to use WdlValues which are properly marshalled as JSON,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/205#issuecomment-144162941:13,update,updated,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/205#issuecomment-144162941,1,['update'],['updated']
Deployability,"@scottfrazer yes, and I need to update the sample response in the YAML too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/205#issuecomment-143854440:32,update,update,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/205#issuecomment-143854440,1,['update'],['update']
Deployability,"@seandavi I know that GCS != S3, but when I had a brief look at the [source](https://github.com/broadinstitute/cromwell/blob/3b29af0d8f116d63e1fcb85f5b4903fd615a5386/engine/src/main/scala/cromwell/server/CromwellRootActor.scala#L89) where that configuration `io` block is being used, `number-of-requests` is used to set a throttle on a fairly low-level Actor that at least at might be used by the AWS batch backend... I haven't looked at the implementation in detail. I'll do that tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436944065:244,configurat,configuration,244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436944065,1,['configurat'],['configuration']
Deployability,"@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71"" failed: exit status 1: sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: Pulling from broadinstitute/gatk; cromwell_1 | ae79f2514705: Pulling fs layer; cromwell_1 | 5ad56d5fc149: Pulling fs layer; cromwell_1 | 170e558760e8: Pulling fs layer; cromwell_1 | 395460e233f5: Pulling fs layer; cromwell_1 | 6f01dc62e444: Pulling fs layer; cromwell_1 | 98db058f41f6: Pulling fs layer; [...]; cromwell_1 | failed to register layer: Error processing tar file(exit status 1): write /root/.cache/pip/http/5/1/d/8/2/51d82969228464b761a16257d5eefe8e2b3dde3c1ad733721353e785: no space left on device; cromwell_1 |; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); cromwell_1 | at akka.dispatch.BatchingExec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:2052,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2052,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"@sharmaashish that's a great point. I _suspect_ (but haven't checked this) that if we don't specify any driver version then the VM we receive will spin up with a GPU attached, but no driver installed. So it's not ""required"" in that sense, but if you want to use GPUs then perhaps it is... I think it would be appropriate to open an issue to ""Update the default Nvidia GPU version in PAPIv2"" with this suggestion (or maybe re-open this issue one with an updated title and description - I'll leave that up to you). I'll also say that since you've already identified the line in the code and the change you want to make, we always appreciate PRs from motivated contributors!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4942#issuecomment-492702334:190,install,installed,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4942#issuecomment-492702334,3,"['Update', 'install', 'update']","['Update', 'installed', 'updated']"
Deployability,@shengqh when you use Google Custom Pipelines (genomics) Cromwell database is not persistent by default. I use a Cromwell server instance configured with an external mysql database. Details can be found in https://cromwell.readthedocs.io/en/latest/Configuring/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-450830409:36,Pipeline,Pipelines,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-450830409,1,['Pipeline'],['Pipelines']
Deployability,"@slnovak Hi - thanks for this! I just had a couple of quick questions on the Homebrew & maintenance front. ; - What's the plan going forward in terms of keeping it up to date, is this something that you plan on doing? If so will you be tracking the official releases (e.g. now 0.16) or updating on some semi-regular (or even not-so-semi) interval? ; - Is there anything we could do in terms of helping out w/ the homebrew angle?; - I'm not at all familiar w/ the homebrew formula stuff. Where is it calling `sbt assemble`? Is that done elsewhere? If so, how does that work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166030670:258,release,releases,258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166030670,1,['release'],['releases']
Deployability,"@slnovak Sounds good, thanks. We need to solidify our release procedure to be less ad hoc now that others are actually using this. We'll try to get that squared away early in the new year - I think what would make sense would be for us to lump the PRs in as part of that procedure but we might come calling for advice/help at some point in the next few weeks :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166332671:54,release,release,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166332671,1,['release'],['release']
Deployability,"@sndrtj To follow up a bit on what @kshakir says here, for any users who do not need call caching I'd highly recommend working with the develop branch. We expect to release Any Day Now (he's been saying for a few weeks) and while it'll have its own problems with life it's already demonstrated itself to be superior in just about every way to 0.19",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-246820302:165,release,release,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-246820302,1,['release'],['release']
Deployability,"@sooheelee commented on [Mon Mar 13 2017](https://github.com/broadinstitute/dsde-docs/issues/1815). Investigate whether Cromwell v25 may be misinterpreting our public PESS pipeline script. The ""unmerged"" label is not present when user runs the workflow. . ```; output_bam_basename = sub(sub(unmapped_bam, sub_strip_path, """"), sub_strip_unmapped, """") + "".unmerged"",; ```. ---; At the bottom of the stack, after Cromwell calls docker, which executes a shell script, the following shell command fails:. java -Xmx3000m -jar /usr/gitc/picard.jar \; SamToFastq \; INPUT=/cromwell-executions/PairedEndSingleSampleWorkflow/610155cd-e94f-4db2-bd9e-9b2322fc412f/call-SamToFastqAndBwaMem/shard-0/inputs/home/orodeh/genomics-public-data/test-data/dna/wgs/hiseq2500/NA12878/H06HDADXX130110.1.ATCACGAT.20k_reads.bam \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true | \; /usr/gitc/bwa mem -K 100000000 -p -v 3 -t 16 $bash_ref_fasta /dev/stdin - 2> >(tee genomics-public-data/test-data/dna/wgs/hiseq2500/NA12878/H06HDADXX130110.1.ATCACGAT.20k_reads.unmerged.bwa.stderr.log >&2) | \; samtools view -1 - > genomics-public-data/test-data/dna/wgs/hiseq2500/NA12878/H06HDADXX130110.1.ATCACGAT.20k_reads.unmerged.bam && \; grep -m1 ""read .* ALT contigs"" genomics-public-data/test-data/dna/wgs/hiseq2500/NA12878/H06HDADXX130110.1.ATCACGAT.20k_reads.unmerged.bwa.stderr.log | \; grep -v ""read 0 ALT contigs"". The input file exists, but something goes wrong, and the outputs are not generated. This caused the rest of the pipeline to fail. Perhaps that provides more debugging information. . This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/comment/37003#Comment_37003. ---. @sooheelee commented on [Mon Mar 13 2017](https://github.com/broadinstitute/dsde-docs/issues/1815#issuecomment-286223385). Since Kate said she hasn't answered any questions, perhaps she would like to take ownership of this one. I've also gone ahead and asked two devs. ---. @sooheelee ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2064:172,pipeline,pipeline,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2064,1,['pipeline'],['pipeline']
Deployability,"@tcibinan Thanks for the contribution! To expectation set, we'll aim to review your PR within a reasonable timeframe but we unfortunately can't make any firm guarantees of the timeline since we're always working to internal deadlines of our own!. My initial thought is:; 1. Since the filesystem is something that Cromwell cannot have any control over, a number of its assumptions might be invalidated (depending on how the workflow author uses the filesystem). That's probably fine, as long as both the author and the workflow runner are aware of it and can work around it.; 2. We probably want to have our own version of ""pre-computed inputs mounted by fuse"" with some more controls around the problems mentioned in (1.) to help keep our assumptions correct.; 3. None of those points are any worse than passing in `gs://` paths as strings and localizing them manually, so we shouldn't disallow people opting-in to fuse support like this if they really want to. Could you add some additional warning comments along these lines?; 1. Any inputs brought in via a Fuse filesystem will not be considered for call caching.; 1. Any outputs stored via a Fuse filesystem will not be recreated if a task is replayed from a call-cache hit.; 1. If the filesystem is writable, your job is potentially no longer idempotent - Cromwell may decide to retry your job for you, and you might get unforeseen file collisions or even incorrect results if that happens.; 1. This is a community contribution and not officially supported (at least not yet!) by the Cromwell team?. @ruchim I know you were looking at Fuse integration too. Do you have any thoughts on this?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597:1595,integrat,integration,1595,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597,1,['integrat'],['integration']
Deployability,"@tom-dyar Can you try again with latest? I fixed a ton of stuff in the last 24 hours, including a whole new processing pipeline for stdout/stderr and rc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395499661:119,pipeline,pipeline,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395499661,1,['pipeline'],['pipeline']
Deployability,@tomkinsc What we're going to do is time allowing try to do it by hand for this upcoming release and either way make it part of our automated process for the following release. I'm assuming you're willing to provide a helpful hand if we need some guidance on the specifics?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815:89,release,release,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815,4,['release'],['release']
Deployability,@vanajasmy - you need to create a custom AMI for your compute environment that includes a patched ecs-agent that is specific for cromwell. See the following documentation on how to do this:. https://docs.opendata.aws/genomics-workflows/aws-batch/create-custom-ami/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4604#issuecomment-464240535:90,patch,patched,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4604#issuecomment-464240535,1,['patch'],['patched']
Deployability,@vdauwera Can you explain the situation? I'm not clear what the exact feature request is. It won't make it into this release but we can see about next (Cromwell 28).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301213292:117,release,release,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301213292,2,['release'],['release']
Deployability,"@vdauwera I spotted the issue but it was @kshakir who ended up resolving it. I believe this had to do with the auth that was used to perform the read_* function, and not having access to the proper google credentials. I checked the [config](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/cromwell_launcher/jes_template.conf) wdl_runner uses and I believe it's missing the goolge.auths key and the engine.filesystem.gcs.auth key in the config, which is probably what Cromwell requires to parse gcs files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165:275,pipeline,pipelines-api-examples,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165,1,['pipeline'],['pipelines-api-examples']
Deployability,"@vdauwera I think it's basically ready to go now, we didn't want to go live until after the 28 release last week",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312980238:95,release,release,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312980238,1,['release'],['release']
Deployability,@vdauwera My thought was that if you're wiring up something like that that there's probably a good reason to have done so. I could easily see people wanting to munge those values when debugging and such but then would it be an attractive nuisance once released into the wild?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323822585:252,release,released,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323822585,1,['release'],['released']
Deployability,"@vdauwera We deliberately kept this out of the 28 release pending discussion. We'd certainly like to include this in 29, but that release won't go out for a while. I'd be happy to meet with you this week to discuss.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312980064:50,release,release,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312980064,2,['release'],['release']
Deployability,"@vdauwera commented on [Sun Jan 08 2017](https://github.com/broadinstitute/wdltool/issues/20). I had a few issues that were fixed by building the latest from source. There's a good chance that will also be the case for the other issues that have been reported. Would be good to cut a release to make it available without requiring people to build from source. . ---. @knoblett commented on [Mon Feb 20 2017](https://github.com/broadinstitute/wdltool/issues/20#issuecomment-281030248). Are we responsible for building a new release now that we own this repo? Or am I misremembering?. ---. @ruchim commented on [Mon Feb 20 2017](https://github.com/broadinstitute/wdltool/issues/20#issuecomment-281054777). The wdltool release is a part of the Cromwell release process . ---. @knoblett commented on [Mon Feb 20 2017](https://github.com/broadinstitute/wdltool/issues/20#issuecomment-281096658). Ah, thank you, that makes sense. @vdauwera do we still need a release? The most recent release was done 3 days prior to the creation of this ticket. ---. @vdauwera commented on [Mon Feb 20 2017](https://github.com/broadinstitute/wdltool/issues/20#issuecomment-281097178). Does the release process include making a packaged jar? I think my problem was that there wasn't a jar that corresponded to the latest releases version available for download, come to think of it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2872:284,release,release,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2872,8,['release'],"['release', 'releases']"
Deployability,"@vdauwera commented on [Wed Apr 12 2017](https://github.com/broadinstitute/dsde-docs/issues/1963). This needs to be discussed further with both workbench and cromwell peeps/POs. ----. This may have been asked before, but I can't find the relevant thread...; It looks like any change to a method configuration automatically triggers a version increase. It would be very useful to be able to control this. For example, changing a memory parameter should be possible without changing the version. In Firehose this was/is possible with the 'Outdate task' option. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/firecloud/discussion/9388/versioning-of-method-configuration/p1. ---. @vdauwera commented on [Tue Apr 18 2017](https://github.com/broadinstitute/dsde-docs/issues/1963#issuecomment-295047979). @knoblett @katevoss This is a total can of worms that should probably be discussed with POs across teams. ---. @knoblett commented on [Wed Apr 19 2017](https://github.com/broadinstitute/dsde-docs/issues/1963#issuecomment-295445174). Is this a Red Team request or a FireCloud request? Seems more like the latter, but just want to clarify. ---. @vdauwera commented on [Wed Apr 19 2017](https://github.com/broadinstitute/dsde-docs/issues/1963#issuecomment-295453435). I think it cuts across both. There are Cromwell-level implications in terms of call caching.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2268:295,configurat,configuration,295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2268,2,['configurat'],['configuration']
Deployability,@vivster7 is this issue meant for 0.19 hotfix or PBE Cromwell (or both)?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/947#issuecomment-231108479:39,hotfix,hotfix,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/947#issuecomment-231108479,1,['hotfix'],['hotfix']
Deployability,"@vortexing - task input and output data staging is handled by the `ecs-proxy` container that is installed when you create a custom AMI with ""cromwell"" settings. If you are not seeing data move in/out a good place to check for errors is the Cloudwatch log for a task that didn't have it's data staged correctly. Append `-proxy` to the job's cloudwatch log url to get the logging generated by the `ecs-proxy`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-467676845:96,install,installed,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-467676845,1,['install'],['installed']
Deployability,"@vsoch @geoffjentry I just wanted to come back to this since singularity 3.0.1 was released a few weeks ago. The backend configuration can now be made a lot more simplistic:; ```; submit-docker = """"""; echo ' \; singularity exec --bind /run,/exports,${cwd}:${docker_cwd} docker://${docker} bash ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; ```; The bind to `/run` was neccessary on our SGE cluster to make python multiprocessing work, as in [this issue](https://github.com/sylabs/singularity/issues/455). The bind to `/exports` is also specific to our cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053:83,release,released,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053,4,"['configurat', 'release']","['configuration', 'released']"
Deployability,"@vsoch @geoffjentry We did manage to get it working, with some caveats. We also haven't really tested it very extensively yet.; These are the relevant lines from the backend configuration:; ```; submit-docker = """"""; echo ' \; CROMWELLROOT=$(echo ${cwd} | sed ""s/cromwell-executions\\/.*/cromwell-executions/"") && \; sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script && \; chmod 775 ${cwd}/execution/script && \; singularity exec --bind /exports:/data/,$CROMWELLROOT:/config docker://${docker} ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; dockerRoot = ""/config""; ```; > This only works if your container has both a /data and /config mount point. I tested this (very shallowly) using biocontainers. Line by line:; 1. `CROMWELLROOT=$(echo ${cwd} | sed ""s/cromwell-executions\\/.*/cromwell-executions/"")` ; 1. If dockerRoot is `/cromwell-executions`; 2. The script will contains paths like: `/cromwell-executions/test/<hash>/call-task/execution/rc`; 3. Therefore we need to have the entire structure under the root of the execution folder mounted, as such, we need to bind the entire execution folder.; 4. This gets the path to the root of the execution folder.; - I also tried setting dockerRoot to be the same as `cwd`: `dockerRoot = ""${cwd}""`, but this resulted in `${cwd}` being placed literally in the execution script. If this had been an option we wouldn't have to bind the execution directory separately (I think), but since it isn't we do have to do so.; 1. `sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script` ; - This a bit of a nasty workaround to convert absolute paths used in the commands to what their path would be in the container. This is necessary if you have (eg.) a String type output directory in a command. There are other ways of dealing with this, you could make a /data directory which links to /expo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799:174,configurat,configuration,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799,1,['configurat'],['configuration']
Deployability,@vsoch Hi - I think what you're looking for is [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf) which is where we put examples like this. So if there's a configuration for a backend which works we'd put it in there so we could point people at it. Does that make sense?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413676397:210,configurat,configuration,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413676397,1,['configurat'],['configuration']
Deployability,"@vsoch Just a heads up, @katevoss did point out that her fix will be on the `/develop` version of RTD shortly but won't be on `/stable` until the next Cromwell release",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3862#issuecomment-402788985:160,release,release,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3862#issuecomment-402788985,1,['release'],['release']
Deployability,"@vsoch Sorry, our devops team has asked us to be especially thorough with PRs affecting our CI environments/dependencies. I've been bouncing between reading up on the [CircleCI docs](https://circleci.com/docs/2.0/configuration-reference/) and checking on several 🔥events this week. If you have time, perhaps we can setup a remote session next week where you can give me a tour of yml and everything that's going on? If you propose three times that work for you I'll pick one. Feel free to msg me here or email if that's easier. Otherwise I'll continue looking through the docs and get back to you once the flames die down. 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415767742:213,configurat,configuration-reference,213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415767742,1,['configurat'],['configuration-reference']
Deployability,"@vsoch That's the theory. Let me know if that doesn't seem to be working for you and we can go from there. The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people **never** want to use actual Docker. However, there are a few buts to the above .... - A Cromwell server can have multiple backends, and workflows can be directed to specific backends, for the case where one sometimes wants it on and sometimes not . - None of this will cover the case where a user **really* wants Singularity (as in an actual Singularity container) instead of the ""use singularity to run a docker container"" model. We'll need to address this separately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413023778:154,configurat,configuration,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413023778,1,['configurat'],['configuration']
Deployability,"@vsoch if you've not already done that, we can always reopen this PR. You all are right, I'm wrong. The reason why I was leaning towards avoiding `cromwell.examples.conf` blurbs is that (as @TMiguelT points out) it's a bit of a mess right now due to having too much stuff in there. TBH work needs to be done to start making that more organized. I sometimes can lean towards throwing the baby out with the bathwater in circumstances like that. I think it's fine to put a number of configurations into `cromwell.examples.conf` as long as the full block is fairly well self contained, and well documented. IOW something which would be easy to peel out into a separate file if/when we get there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468965389:480,configurat,configurations,480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468965389,1,['configurat'],['configurations']
Deployability,@wdesouza I am seeing this as well. This fork was created just before #6194 that upgraded Cromwell's Java version from 8 to 11. I think these compilation errors may represent some (hopefully minor) incompatibilities.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949582463:81,upgrade,upgraded,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949582463,1,['upgrade'],['upgraded']
Deployability,@wleepang @geoffjentry ; Any updates about PR #5110 ? :smile:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-522934849:29,update,updates,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-522934849,1,['update'],['updates']
Deployability,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:537,install,installed,537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341,2,['install'],"['install', 'installed']"
Deployability,@ysp0606 FYI we've had to disable our Alibaba tests for Cromwell while we wait for our OSS access to be restored. More info in https://broadworkbench.atlassian.net/browse/BA-6345. On our last update we gave Alibaba support a temporary access key from our account so they can try and debug the error code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666:192,update,update,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666,1,['update'],['update']
Deployability,A batch endpoint to update labels for workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3755:20,update,update,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3755,1,['update'],['update']
Deployability,A fix for this just landed on `develop` and will be in the next release of Cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318#issuecomment-434312154:64,release,release,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318#issuecomment-434312154,1,['release'],['release']
Deployability,"A flexible way of capturing the data involved in a performance run for later analysis:. Capture the following into files and copy them into GCS somewhere, perhaps [perf bucket]/[perf name]/[cromwell version]. where perf name is a small enumerated list of things we want to perf test, e.g. (5_genome, TJeandet_Call_Cache). * reported metrics; * all metadata; * configuration. Once this is done, the user may delete the instance running cromwell, the DB. It may also choose not to report metrics to Perf Grafana",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4106:360,configurat,configuration,360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4106,1,['configurat'],['configuration']
Deployability,"A quick explanation of the strategy - I did this with a database update at (actually just after) evaluation time. This has a few benefits (as I see it):; - The database is as easy to read and interpret as the metadata.; - We don't need to rehydrate the various data structures required by the expression evaluation logic just to perform a metadata query.; - We know that the evaluated expression in the database is the **exact** expression used in the call. There's no possibility of it being re-calculated incorrectly at a later time (maybe expression logic changes, etc); - I got to re-learn slick database update operations :-D",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/341:65,update,update,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/341,2,['update'],['update']
Deployability,"A similar patch was tested by @jsotobroad. Once this PR against develop is reviewed & merged, we can submit another PR against master, and _that one_ should make #695 good to go.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213246179:10,patch,patch,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213246179,1,['patch'],['patch']
Deployability,"A simple grep through the source code reveals several hits with Log4j:. ```; CromwellRefdiskManifestCreator/pom.xml: <groupId>org.apache.logging.log4j</groupId>; CromwellRefdiskManifestCreator/pom.xml: <artifactId>log4j-core</artifactId>; CromwellRefdiskManifestCreator/pom.xml: <groupId>org.apache.logging.log4j</groupId>; CromwellRefdiskManifestCreator/pom.xml: <artifactId>log4j-api</artifactId>; CromwellRefdiskManifestCreator/src/main/java/org/broadinstitute/manifestcreator/CromwellRefdiskManifestCreatorApp.java:import org.apache.logging.log4j.Level;; CromwellRefdiskManifestCreator/src/main/java/org/broadinstitute/manifestcreator/CromwellRefdiskManifestCreatorApp.java:import org.apache.logging.log4j.LogManager;; CromwellRefdiskManifestCreator/src/main/java/org/broadinstitute/manifestcreator/CromwellRefdiskManifestCreatorApp.java:import org.apache.logging.log4j.Logger;; CromwellRefdiskManifestCreator/src/main/java/org/broadinstitute/manifestcreator/CromwellRefdiskManifestCreatorApp.java:import org.apache.logging.log4j.core.config.Configurator;; project/Dependencies.scala: // Replace all log4j usage with slf4j; project/Dependencies.scala: // https://www.slf4j.org/legacy.html#log4j-over-slf4j; project/Dependencies.scala: ""org.slf4j"" % ""log4j-over-slf4j"" % slf4jV; ```. I wasn't able to expose a vulnerability by using malicious code but my test is probably not extensive.; It looks like this lib is used in a packaging tool of Cromwell so probably not executed during production.; On the other hand, slj4j seems to be used everywere. Is that abstraction layer vulnerable ?. Could you please let us know if you believe Cromwell is affected by Log4shell ?. Thanks,",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6588:1046,Configurat,Configurator,1046,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588,1,['Configurat'],['Configurator']
Deployability,"A, non-polyadenylated RNA, pre-processed RNA, tRNA, and may also contain regulatory RNA molecules such as microRNA (miRNA) and short interfering RNA (siRNA), snRNA, and other RNA transcripts of yet unknown function. Ambion RiboMinus rRNA depletion was performed as described in the manufacturer’s protocol (Pub. Part no.: 100004590, Rev. date 2 December 2011) following the standard protocol.\nTruSeq RNA Sample Preparation was performed on the RiboMinus™ RNA fraction as described in the manufacturer’s protocol (Pub. Part no.: 15026495 Rev. F March 2014) following the low sample protocol.\nThe libraries were sequenced on Illumina’s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check using fastQC version 0.11.2.\nAlignment of unpaired unstranded reads using STAR version 2.4.0.\nQuantification of transcripts and isoforms using RSEM version 1.2.21 using rsem-calculate-expression, both alignment and quantification was done using the STAR_RSEM.sh pipeline (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\nThe programe featurecounts version 1.4.6-p2 from the SourceForge Subread package was used to produce a summary file of counts from all the alignement .bam files.\nThe summary file of counts (RNAseq.counts) was used to plot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_format_and_content: .osc files are simple tab delimited files. They were generated by combining the isoform.results files outputed by RSEM with the gencode v19 .gtf file. It contains abundance measurements and transcript isoforms. It also contains metadata that is inputed into ZENBU.\nSupplementary_files_format_and_content: RNAseq.counts is a simple tab delimited file containing the counts for all the RNA-seq libraries for each gen",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4519:2261,pipeline,pipeline,2261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519,1,['pipeline'],['pipeline']
Deployability,AC: Confirm that the Pipelines API Request Worker can handle a 502 -- it should be retried.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-490175339:21,Pipeline,Pipelines,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-490175339,1,['Pipeline'],['Pipelines']
Deployability,"AC: Multiple cromwells are used to execute a performance test. ## Desired Deployment. Utilize single instances for summarizer & reader. Bring up a instance ""group"" (a.k.a. cluster) for workers. ### Worker Cromwells [Instance Group](https://cloud.google.com/compute/docs/instance-groups/). Instantiate a GCP instance group to bring up a cluster of 3 nodes. ### TODO. - [ ] Write an instance template to represent a cromwell worker.; - [ ] Edit Jenkins script to bring up instances as described above.; - [ ] Edit instance startup script to remove shutdown part, move that work to end of Jenkins script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4842:74,Deploy,Deployment,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4842,1,['Deploy'],['Deployment']
Deployability,AC: Release is running in both FireCloud and CaaS Prod,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4843:4,Release,Release,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4843,1,['Release'],['Release']
Deployability,AN-126 Update Cromwell code ownership,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7570:7,Update,Update,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7570,1,['Update'],['Update']
Deployability,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2412,configurat,configuration,2412,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,1,['configurat'],['configuration']
Deployability,AWS Batch Job Retry configuration with Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6114:20,configurat,configuration,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6114,1,['configurat'],['configuration']
Deployability,AWS Batch works in a different way than current platforms and we are taking that into account for something we think will be great on release. Stay tuned to #3744 for more details and progress on this item,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395590745:134,release,release,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395590745,1,['release'],['release']
Deployability,AWS profileName configuration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5452:16,configurat,configuration,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452,1,['configurat'],['configuration']
Deployability,"A_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 2019-01-31 20:10:51,492 INFO - changesets/failure_metadata.xml::causedByLists::cjllanwarne: Successfully released change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1504,update,update,1504,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['update'],['update']
Deployability,"Aberrant data sometimes requires our pipelines to run with much more memory than is required in the normal case. It would be helpful to have some mechanism to support automatically re-running a task with more memory (or other resources) if it fails for certain reasons -- e.g. if we knew that the task failed for OOM, we would want to try again with double the memory, perhaps more than once (up to some set threshold of attempts or total memory).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1729:37,pipeline,pipelines,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1729,1,['pipeline'],['pipelines']
Deployability,"Acceptance Criteria:. DevOps has deployed an instance ready to be configured using `summarizer` for `INSTANCE_TYPE` environment variable, [seen in this firecloud-develop PR](https://github.com/broadinstitute/firecloud-develop/pull/1590). In detail this consists of a terraform `instance` stanza with `count=1` or omitted.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4799:33,deploy,deployed,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4799,1,['deploy'],['deployed']
Deployability,"According to [the docs](https://dev.mysql.com/downloads/connector/j/) it is ""highly recommended"" that we upgrade to the latest version, even if our MySQL is behind - `5.6.36-google-log` in our case. Reason I started looking into this was https://github.com/broadinstitute/cromwell/issues/4689",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4690:105,upgrade,upgrade,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4690,1,['upgrade'],['upgrade']
Deployability,"According to https://cromwell.readthedocs.io/en/stable/backends/Google/; The format for the json file should be; ```; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. I followed this format but got this error; ```; [2022-11-20 18:17:16,88] [warn] Failed to build PipelinesApiConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$$anon$1: Google Pipelines API configuration is not valid: Errors:; Attempt to decode value on failed cursor: DownField(manifestFormatVersion); at cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$.apply(PipelinesApiConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:498,Pipeline,PipelinesApiConfigurationAttributes,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,10,"['Pipeline', 'configurat', 'pipeline']","['Pipelines', 'PipelinesApiBackendLifecycleActorFactory', 'PipelinesApiConfigurationAttributes', 'configuration', 'pipelines']"
Deployability,"According to the PR description, the WDL 1.0 spec did not have an opinion here, so we let our 1.0 implementation change. The `development` version of WDL says; >In the event that there is no protocol the import is resolved **relative** to the location of the current document. If a protocol-less import starts with `/` it will be interpreted as starting from the root of the host in the resolved URL. so I think it would be most pragmatic to close this issue with a documentation-update PR. @tlangs does this sound fair to you?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4515#issuecomment-451608883:480,update,update,480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4515#issuecomment-451608883,1,['update'],['update']
Deployability,"Actor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transfor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:1469,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"Actor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transfor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1747,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1747,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505180523:61,patch,patch,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505180523,1,['patch'],['patch']
Deployability,"Actually I'd like to add a Centaur integration test for this, I can hopefully get this done today.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518583719:35,integrat,integration,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518583719,1,['integrat'],['integration']
Deployability,Actually one minor request: could you please rebase on `develop` and add an entry to the version 70 release notes for this added functionality with a credit to yourself. 🙂,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6489#issuecomment-936307629:100,release,release,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6489#issuecomment-936307629,1,['release'],['release']
Deployability,"Actually the more I'm digging into this I take it all back. For now. The `zones` field in the Cromwell code doesn't seem to be actually used anywhere except for tests. . Thining about it now I have a recollection that this was part of the cloud formation setup for the batch configuration. I'll need to dig into this unless @wleepang swoops in with some wizardly knowledge. BTW, it could be (and would make sense) that `~/.aws/conf` file is getting picked up via one of the Amazon libraries Cromwell is using. But I see no evidence that it's being directly used by Cromwell itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493269281:275,configurat,configuration,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493269281,1,['configurat'],['configuration']
Deployability,"Actually what is bugging me I think is precisely the distinction between ""cromwell"" and ""user"" auth I think :smile: ; I think we called them like this because it makes sense in the current way we use them. We use ""cromwell"" auth pretty much always with service-account and ""user"" auth with a ""refreh token mode"" (falling back to the ""cromwell"" auth if there's no ""user"" auth). But really what we need is an auth for pipelines and another one for gcs (they might be the same or not, use service account or refresh tokens, but we shouldn't care from a cromwell perspective).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203478872:416,pipeline,pipelines,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203478872,1,['pipeline'],['pipelines']
Deployability,Adapt engine upgrade Centaur test for horicromtal,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4786:13,upgrade,upgrade,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4786,1,['upgrade'],['upgrade']
Deployability,Add 2 perf test cases to benchmark the performance of running an integration test case (https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/integrationTestCases/germline/single-sample-production-workflow/PairedEndSingleSampleWf.wdl) using an inputs file referencing all DOS data. 1. A test case that runs the single sample workflow (above) against a dos input files at scale (200 times); 1. A test case that tries to call cache the single sample workflow (above) against a dos input files at scale (200 times),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4300:65,integrat,integration,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4300,2,['integrat'],"['integration', 'integrationTestCases']"
Deployability,Add AWS version of Mutect2 integration test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4972:27,integrat,integration,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4972,1,['integrat'],['integration']
Deployability,"Add a configuration option to deal with nested scatters in WDL. Currently, the inner scatter is converted into a subworkflow. This behavior is now controlled by the `inner-outer-scatter` option defined in `wom/src/main/resources/reference.conf` configuration file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5061:6,configurat,configuration,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061,2,['configurat'],['configuration']
Deployability,Add a new API endpoint to CromIAM which will allow a user to update the collection name for one or more workflows in a one to many fashion - i.e. one collection name will be applied to 1+ workflows. If Sam says that the user does not have access to this collection name it will be an error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2839:61,update,update,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2839,1,['update'],['update']
Deployability,"Add ability to cripple local via the configuration file, for hot-fix 25.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2073:37,configurat,configuration,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2073,1,['configurat'],['configuration']
Deployability,"Add an **Engine** JobExecutionActor which can act as the representative of a single Job within the engine. When asked to restart a job, it should first reference the JobStore (#1115) to see whether it has already been completed. If a job is new or not-yet-completed, it should be forwarded to the BackendJobExecutionActor, as before. See actor diagram in https://docs.google.com/a/broadinstitute.com/document/d/1U3SbbRMyNUCs2uHHhG-NvC7P6sPBJXN2WvP0SJ4vMco/edit?usp=sharing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1116:379,a/b,a/broadinstitute,379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1116,1,['a/b'],['a/broadinstitute']
Deployability,Add call caching changes to release notes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/685:28,release,release,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/685,1,['release'],['release']
Deployability,Add commented out configuration for increasing the number of threads …,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/442:18,configurat,configuration,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/442,1,['configurat'],['configuration']
Deployability,"Add configuration for volcano, a batch system on k8s. [BA-6011]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5184:4,configurat,configuration,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5184,1,['configurat'],['configuration']
Deployability,Add cron run integrations each day of the week,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7089:13,integrat,integrations,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7089,1,['integrat'],['integrations']
Deployability,Add integration tests for AWS authentication,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3747:4,integrat,integration,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3747,1,['integrat'],['integration']
Deployability,Add nvidia driver install. Remove unused debugging printlns that should have been removed earlier.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7235:18,install,install,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7235,1,['install'],['install']
Deployability,Add quickfix for gsutil /metadata issues to a hotfix,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4641:46,hotfix,hotfix,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4641,1,['hotfix'],['hotfix']
Deployability,Add task to release wdl to make a github release,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2275:12,release,release,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2275,2,['release'],['release']
Deployability,"Add the `logsGroup` and `resourceTags` config option to the `default-runtime-attributes` section of the AWS Batch configuration. This enables you to send the logs to a custom log group name and tag the jobs that Cromwell submits. Sample usage:; ```; default-runtime-attributes {; queueArn: ""arn:aws:batch:us-east-1:111222333444:job-queue/job-queue""; logsGroup: ""/Cromwell/job/""; resourceTags {; projectid: ""project1""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7219:114,configurat,configuration,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7219,1,['configurat'],['configuration']
Deployability,Add update and remove label functionality,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2332:4,update,update,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2332,1,['update'],['update']
Deployability,"Added Docker specification content types to the manifest unmarshaller.; Updated spec for testing GCR w/o authentication, as cromwell's Google Credentials utility now automatically detects the application default credentials on a system.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/715:72,Update,Updated,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/715,1,['Update'],['Updated']
Deployability,"Added a new Shared File System (SFS) Backend package as a base for any/most backends wanting to execute cromwell calls on an SFS.; Added SGE and LSF configs (commented out) to the main `application.conf`, plus other small tweaks.; Non-main `application.conf` files only set the keys that they need to add or override from the main.; More of the config is defaulted across the board.; A number of classes moved/refactored from other backends for use by the SFS backend.; New trait `RuntimeAttributesValidation[ValidatedType]` for proccessing runtime attributes.; First pass at a `WorkflowFileSystemProvider` that combines a config and workflow options to create a filesystem. Only implemented for local and GCS files.; Added a Config backend that implements the SFS backend by reading wdl-lite command strings and `job-id-regex`.; Currently extending SFS backend to create the SGE backend until one can specify runtime attributes via the Config backend.; Added a ""shadow"" local backend, extending the SFS backend, that will be evaluated for a while before hopefully usurping the original.; Fixed integration tests that were attempting to mock the gcs file system.; Gave `MetadataDatabaseAccessSpec` a little bit more time the non-split up ""create and query a workflow"".; Fixed the `SprayDockerRegistryApiClientSpec` for v1 registries by switching from the `ubuntu` repo to the `registry` repo (we query the registry for a repo hosting images of the docker registry-- inception).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1202:1095,integrat,integration,1095,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1202,1,['integrat'],['integration']
Deployability,"Added a test to our CI. It will build docker images for `cromwell`, `womtool`, `cromiam`, and `cromwell-drs-localizer` in a way that is very similar to how we do it in our release process (`chart_update_on_merge.yml`). Build errors should now be caught earlier in the development process.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7151:172,release,release,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7151,1,['release'],['release']
Deployability,Added engine upgrade test for call caching.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4178:13,upgrade,upgrade,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4178,1,['upgrade'],['upgrade']
Deployability,"Added methods for retrieving cwl File/Directory parts like basename.; Added a directory listing stub close to the existing glob listing code.; Updated CWL types to reuse more Polys and to parse ecmascripts with embedded newlines and extra whitespace.; Minor cleanup around throw/ErrorOr/Try conversions especially around Javascript processing.; JsUtil encoding now encodes WomMap/WomArray into instances of JsObject instead of java.util.Map/java.lang.Array.; Hacked JsUtil to support reading in ""structs"" of mixed types.; Replaced all usages of .stripMargin.replaceAll with .stripMargin.replace so that the replacements aren't processed like regexs.; Removed docs/.Rhistory and ./Running empty files.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3124:143,Update,Updated,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3124,1,['Update'],['Updated']
Deployability,Added some cleanup after the apt-get install to clean up that layer due to suggestions from @hjfbynara,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/804#issuecomment-218265245:37,install,install,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/804#issuecomment-218265245,1,['install'],['install']
Deployability,Adding additional process detail and a link to the deployment guide for CAAS. Link to the new documentation in situ: https://github.com/broadinstitute/cromwell/tree/cjl_caas_process/processes/release_processes#how-to-deploy-cromwell-in-caas-prod,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5002:51,deploy,deployment,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5002,2,['deploy'],"['deploy-cromwell-in-caas-prod', 'deployment']"
Deployability,Adding five dollar genome and Gary's Canary scatter test to perf [BA-5954],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5196:37,Canary,Canary,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5196,1,['Canary'],['Canary']
Deployability,Adding query parameters for metadata endpoint. Closes #719 as hotfix,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/767:62,hotfix,hotfix,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/767,1,['hotfix'],['hotfix']
Deployability,Adding the symlink fix to the hotfix branch,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1074:30,hotfix,hotfix,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1074,1,['hotfix'],['hotfix']
Deployability,Adding updated reference to reference.conf for Spark Backend. Referring to an issue [Issue](https://github.com/broadinstitute/cromwell/issues/2033),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2043:7,update,updated,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2043,1,['update'],['updated']
Deployability,"Additionally:; Ignoring `PostMVP` specs to allow `sbt alltests` to run.; Shutting down many more actor systems by creating a `TestKitSuite` mixing Akka's `TestKit` and ScalaTest's `Suite`.; DRYed out some backend specs with a `BackendSpec`.; Moved more classes/files from `engine` to `core`, including `SampleWdl`, `application.conf`, etc.; Gave more time to the integration test `SprayDockerRegistryApiClientSpec`.; `DockerTest` and `IntegrationTest` tags now in `crowell.core`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1091:363,integrat,integration,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1091,2,"['Integrat', 'integrat']","['IntegrationTest', 'integration']"
Deployability,"Addresses [WX-1210](https://broadworkbench.atlassian.net/browse/WX-1210). PR adds the JIRA ticket ID to the auto-commit message we make on the `Cromwhelm` repo on merge. That repo performs checks for a JIRA ID as a step on the `Update and publish new cromwell-helm chart` action. Without the id, the action will always fail. [WX-1210]: https://broadworkbench.atlassian.net/browse/WX-1210?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7180:228,Update,Update,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7180,1,['Update'],['Update']
Deployability,"Addresses [WX-1306](https://broadworkbench.atlassian.net/browse/WX-1306), [WX-1307](https://broadworkbench.atlassian.net/browse/WX-1307), [WX-1308](https://broadworkbench.atlassian.net/browse/WX-1308), [WX-983](https://broadworkbench.atlassian.net/browse/WX-983). PR creates a GHA (currently runs on dispatch, can be updated to run on schedule of choice) that creates a billing project and BEE, attaches the BEE to a static landing zone, creates a workspace and provisions an app within the BEE, submits a workflow (basic hello world) to Cromwell, and performs app/workspace/billing project cleanup afterwards. BEE template is flagged by Janitor for post workflow cleanup to ensure no lingering resources. Workspace deletion and billing project deletion are finicky due to invariable timing of the deletion itself (can be either extremely short or longer than 12 minutes), so those two steps are handled by either an exception block (workspace deletion) or `continue-on-error` (billing project) to ensure that failures there do not reflect a failure on the test against Cromwell. Workspace provisioning and app creation are necessary for running tests against Cromwell, so a failure there will be reported as a failure on the Cromwell test. (As an aside, this could be rectified by having a static testing app that's always running in a dedicated testing environment. Test could be updated to run submissions against it so as long as that app is kept up to date. [WX-1306]: https://broadworkbench.atlassian.net/browse/WX-1306?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-1307]: https://broadworkbench.atlassian.net/browse/WX-1307?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-1308]: https://broadworkbench.atlassian.net/browse/WX-1308?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-983]: https://broadworkbench.atlassian.net/browse/WX-983?atlOri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7236:317,update,updated,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7236,1,['update'],['updated']
Deployability,"Addresses [WX-1306](https://broadworkbench.atlassian.net/browse/WX-1306). PR adds a Python script that automates the submission of a basic workflow to a CoA instance. PR is an offshoot of [WX-983](https://broadworkbench.atlassian.net/browse/WX-983). As such the PR is carrying the GHA infrastructure as well as commented out code that pertains to the App provisioning steps via Rawls and Leo, both of which are not needed for the script review. In order to test the script locally you'll need to set env variables for the CoA base url and a Bearer Token. Both can be obtained by reviewing the network data when visiting your Workspace on Terra. Once that's done, navigate to `server/src/test/python/cromwell-az-e2e-test`. Run `poetry install` to install dependencies and then run `poetry run start` to run the script. [WX-1306]: https://broadworkbench.atlassian.net/browse/WX-1306?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-983]: https://broadworkbench.atlassian.net/browse/WX-983?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7230:734,install,install,734,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7230,2,['install'],['install']
Deployability,Addresses issue 5004 and possibly several others with the AWS Batch backend. The changes allow for re-use of job definitions to prevent eventual consistency collisions. Other changes are largely simplifications or improvements to the backend that simplify the integration with AWS. 1. Added additional documentation; 1. Added additional server logging which is useful for diagnostics and has been requested by users; 1. Updated `cromwell-aws-s3filesystem` package to use AWS SDKv2 removing the need for two versions of the SDK; 1. Removed the requirement for a proxy container is the AWS worker; 1. Removed the need for a custom AMI for the EC2 workers; 1. Removed the need for a custom ECS agent; 1. Set up `/var/lib/docker/docker` to auto-expand as inputs are now read directly into the container,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5468:260,integrat,integration,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468,2,"['Update', 'integrat']","['Updated', 'integration']"
Deployability,Adds a cron trigger for the centaur integration tests that cause the integration tests to run at midnight each weekday and post failures to slack at #cromwell-integration-action,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7089:36,integrat,integration,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7089,3,['integrat'],"['integration', 'integration-action']"
Deployability,Adds in supported backend for Google cloud batch. This is an initial commit to start code review and perform integration tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7125:109,integrat,integration,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7125,1,['integrat'],['integration']
Deployability,"Adds in supported backend for Google cloud batch. This is an initial commit to start code review. Two of the PAPIv2 integration tests failed on DRS, but can't see bucket output. Likely Java GCS library issue since that was upgraded.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7129:116,integrat,integration,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7129,2,"['integrat', 'upgrade']","['integration', 'upgraded']"
Deployability,"Adds support for automatically publishing Docker images that are less formal than numbered releases but are suitable for production due to their non-designation as development snapshots `-SNAP`. - `85-443a6fc-SNAP` for testing images of code not merged to develop; - Explicit: `sbt -Dproject.isSnapshot=true -Dproject.isRelease=<ANY> dockerBuildAndPush`; - [Accepting defaults: `sbt dockerBuildAndPush`](https://github.com/broadinstitute/cromwell/blob/b432e640e6978030c8a111119a267bdfc564c36c/src/ci/bin/test.inc.sh#L1216); - **New** `85-443a6fc` for images of commits merged to develop and ready for Terra; - Explicit: `sbt -Dproject.isSnapshot=false -Dproject.isRelease=false dockerBuildAndPush`; - `85` for full shrink-wrapped releases; - Explicit: `sbt -Dproject.isSnapshot=false -Dproject.isRelease=true dockerBuildAndPush`; - [Accepting defaults: `sbt -Dproject.isSnapshot=false dockerBuildAndPush`](https://github.com/broadinstitute/cromwell/blob/e28dcb12257f82f340d5db4edb4e7d1ca6e6dbc9/publish/publish_workflow.wdl#L55). I went out of my way to make sure existing invocations keep working the same via defaults. Updating the whole codebase, Travis, Jenkins, and who knows what else is not something I want to sign up for at the moment.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6923:91,release,releases,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6923,2,['release'],['releases']
Deployability,"After deep-diving into the QA perf environment, I think we could get some continuous-testing value very quickly by doing the following:. - Copy their `project-deploy` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/job/project-deploy/) job over to deploy our `develop` image to our own perf environment every night; - Or: do something more custom since we don't need all the general-purpose non-Cromwell stuff; - Copy their `perf-run-single-test-until-it-fails` [(link)](https://fc-jenkins.dsp-techops.broadinstitute.org/view/Perf/job/perf-run-single-test-until-it-fails/) job and make it run against our own environment every night. Even something this simple would (maybe after they check our working a few times?) allow us to remove the ""wait for QA"" portion of the release process since we'd be able to look-up the results rather than regenerating them. We'd also be able to track performance over time and have a better idea of how much signal-to-noise there is in the tests (eg how much variation is within normal bounds?).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4980:74,continuous,continuous-testing,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4980,5,"['continuous', 'deploy', 'release']","['continuous-testing', 'deploy', 'release']"
Deployability,"After discussing internally, we unfortunately can't add this code or support the general direction of proxy compatibility. Docker lookups and call caching are pretty sensitive and involved and we're looking to keep the configuration surface area to an absolute minimum.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1545803504:219,configurat,configuration,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1545803504,1,['configurat'],['configuration']
Deployability,"After discussing with @abaumann and @geoffjentry, we are going to plan this for Cromwell 27, our first release of Q4 (April-or-so). Once this is complete, the A-Team will be able to use the Docker Hash library for their own features.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2062#issuecomment-285791090:103,release,release,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2062#issuecomment-285791090,1,['release'],['release']
Deployability,"After running a job that completed (or so it seems by the files generated ... Cromwell labels it as failed), I tried to retrieve the metadata but I got this error message instead:; ```; {; ""status"": ""error"",; ""message"": ""Metadata for workflow xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx exists in database but cannot be served because row count of 1249471 exceeds configured limit of 1000000.""; }; ```; I have tried to understand what configuration variable holds this 1000000 row limits, but I could not figure it out. :-( I think it would save users valuable time if they were directed to what to do when these type of very specific errors are recognized. I know this is a little bit more work on the developer side but I would certainly be very grateful ... and I would also stop asking questions. :-)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6236:428,configurat,configuration,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6236,1,['configurat'],['configuration']
Deployability,After tech talk discussion this is in standby until after we release,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-371150276:61,release,release,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-371150276,1,['release'],['release']
Deployability,"After the changes above, I believe the various Specs should run as expected:; - `sbt alltests:test`; - `sbt notests:test`; - `sbt nodocker:test`; - `sbt dbms:test`; - `sbt integration:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/558#issuecomment-196466681:172,integrat,integration,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/558#issuecomment-196466681,1,['integrat'],['integration']
Deployability,"After updating docker container to the latest cromwell:dev from cromwell:37 I started noticing multiple file not found exception for my https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/bs_extract_run.wdl pipeline ( sample input is https://github.com/antonkulaga/rna-seq/blob/master/pipelines/bs-seq/inputs/extract_run/bs_extract_SRR948855.json ).I think it has something to do with the fact that my cromwell configuration in docker-swarm uses /data/cromwell-executions as root instead of default /cromwell-executions (my config is here https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/app-config/application.conf ), however, there may be other reasons.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4698:187,pipeline,pipelines,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4698,4,"['configurat', 'pipeline']","['configuration', 'pipeline', 'pipelines']"
Deployability,Against hotfix instead https://github.com/broadinstitute/cromwell/pull/847,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/846#issuecomment-219846749:8,hotfix,hotfix,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/846#issuecomment-219846749,1,['hotfix'],['hotfix']
Deployability,"Ah gotcha! To summarize:. - no changes are being made to the (scala) cromwell code to integrate a backend; - the specification of the backend still happens on the level of the pipeline, via the backend.conf; - of which we can provide an example from the `cromwell.examples.conf`. So I just need to write that example :) Did I get that right this time?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413692055:86,integrat,integrate,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413692055,2,"['integrat', 'pipeline']","['integrate', 'pipeline']"
Deployability,"Ah ha, I believe I've worked it out. The problem is, my docker image has an entrypoint of `/app/fastqc_docker.py`. What this means is that Cromwell generates the full docker command as follows:; ```bash; docker run ; -v /tmp/ggp-556542479:/tmp/ggp-556542479 ; -v /mnt/local-disk:/cromwell_root ; [various -e flags] ; asia.gcr.io/ringed-griffin-220204/dx_fastqc; /tmp/ggp-556542479; ```; So ultimately what my python script sees is this, because the entrypoint and CMD are combined:. ```bash; /app/fastqc_docker.py /tmp/ggp-556542479; ```. So this is why the right command was being run, but it wasn't finding any of the required arguments, like `--read`. Unfortunately this seems to be a longstanding issue in the Google Cloud PAPI v1, (#2461, #2256), and it looks like we're waiting for PAPI v2 to be released before this can be easily fixed. I can't find any obvious workaround for this, and I can't change my entrypoint because that's required elsewhere in my system.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381#issuecomment-438109764:802,release,released,802,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381#issuecomment-438109764,1,['release'],['released']
Deployability,"Ah ok, I'm not familiar with how to add configuration options. Depending on how involved a process it is / if there is some example from an earlier pull request I could follow, I would be happy to give it a shot. If not, I'm also ok with shelving this for now and just using an ad-hoc build in the meantime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-547681039:40,configurat,configuration,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-547681039,1,['configurat'],['configuration']
Deployability,"Ah that assumption is indeed faulty -- in a full pipeline we can have upward of 20 inputs that may be used at various points, some only in the last few tasks to run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293588966:49,pipeline,pipeline,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293588966,1,['pipeline'],['pipeline']
Deployability,"Ah, I see. Just a guess, but are you sure your S3 URLs (or more likely your S3 bucket in the configuration file) are in the right format? `s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt` doesn't look valid to me. It should be more like `s3://concr-genomics-results`. Alternatively, maybe the AWS Batch role doesn't have read access to the S3 bucket? The Cromwell server and the Batch instances are different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016934:93,configurat,configuration,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016934,1,['configurat'],['configuration']
Deployability,"Ah, oh well, thanks for the second set of eyes. https://github.com/broadinstitute/cromwell/commit/2682c001d99823098e655acd1dd7a3062a68f495 has your change implemented with some ~nasty~ reflection that hopefully the rest of the DSP-Batcher's will accept, and the test re-enabled. CI running here to see if it breaks anything:; https://travis-ci.com/broadinstitute/cromwell/builds/116462548. Assuming this works, I think the changes will pass all of our existing unit and integration tests!. We can get two others to review. I should abstain due to our collaboration on the code. 😉",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504500160:470,integrat,integration,470,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504500160,1,['integrat'],['integration']
Deployability,"Ah, okay! Thank you for the update.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/838#issuecomment-260672380:28,update,update,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/838#issuecomment-260672380,1,['update'],['update']
Deployability,"Aha. So maybe we can just default in our own Noop DSN to silence the error. ToL: The lack-of-a-DSN-message is also [Logback adjacent](https://docs.sentry.io/clients/java/modules/logback/#usage). Someday I'll figure out how the hell to use logback/Joran. On first glance it looks a lot like HOCON's embedded default `application.conf` that can be overriden via `-Dconfig.file=…` except one is supposed to use [`-Dlogback.configurationFile=…`](https://logback.qos.ch/manual/configuration.html#configFileProperty). But while I ""get"" HOCON's mechanics I do not yet ""get"" best practices for logback [overrides/includes](https://stackoverflow.com/a/23737143/3320205).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389034690:420,configurat,configurationFile,420,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389034690,2,['configurat'],"['configuration', 'configurationFile']"
Deployability,Ahh -- disregard. I did not install Akka.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334#issuecomment-165839405:28,install,install,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334#issuecomment-165839405,1,['install'],['install']
Deployability,"Ahh okay, so you did need root permission for what I expected. If you needed root for those 2, arguably another user (or cluster admin) doing the deployment would consider the additional root as trivial. And the benefits to this would be - having the registry deployed on a cluster (still needing root) but without Docker?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537248811:146,deploy,deployment,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537248811,2,['deploy'],"['deployed', 'deployment']"
Deployability,"Akka HTTP is effectively the next version of Spray. Spray is for all intents and purposes a dead technology. Considering that's been the case for over a year now, we should update at some point just because eventually we'll run into a problem that won't be resolved by developers. Also, Akka HTTP is now hitting a point in its development cycle where not only is it deemed ""Better"" :tm: but they've switched to ease of use and performance and it's eclipsing Spray in those categories now as well. I've labeled this as developers choice as it certainly doesn't _need_ to happen any time soon, although if a someone wearing a PO hat thought it was otherwise important it could certainly be pushed in elsewhere (since it likely _would_ include performance enhancements and such)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1243:173,update,update,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1243,1,['update'],['update']
Deployability,Akka update to 2.6.8 (fixup for scala-steward PR 5755) [BA-6566],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5776:5,update,update,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5776,1,['update'],['update']
Deployability,"All fixed -- see https://github.com/broadinstitute/mock-jes/pull/1. Since this only works when deployed to App Engine, I've deployed it to our ""mock production"" there and ran 50 workflows... only the /batch endpoint is invoked and everything seems to be working properly",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257118745:95,deploy,deployed,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257118745,2,['deploy'],['deployed']
Deployability,"Allow Cromwell system administrators to restrict WDL HTTP imports to specific, trusted hosts/domains. This prevents the import mechanism from being used to inappropriately access resources that the Cromwell instance has access to on its internal LAN, but which are not exposed to the end users. Terra configuration here: https://github.com/broadinstitute/firecloud-develop/pull/3138",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6938:301,configurat,configuration,301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6938,1,['configurat'],['configuration']
Deployability,Allow GCP global pipeline timeout to be configurable,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5273:17,pipeline,pipeline,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273,1,['pipeline'],['pipeline']
Deployability,Allow GCP global pipeline timeout to be configurable [CI clone],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5315:17,pipeline,pipeline,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5315,1,['pipeline'],['pipeline']
Deployability,Allow overriding system-level `hog-factor` setting in individual backend configurations [BA-6584],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5911:73,configurat,configurations,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5911,1,['configurat'],['configurations']
Deployability,Allowed the CallActor to receive an updated abort function,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/326:36,update,updated,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/326,1,['update'],['updated']
Deployability,"Alright, @cjllanwarne and @kshakir I updated the code again... it's changed quite significantly to address all comments so probably needs another look!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-223352694:37,update,updated,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-223352694,1,['update'],['updated']
Deployability,Also I have not updated the README whatsoever,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203057361:16,update,updated,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203057361,1,['update'],['updated']
Deployability,"Also I've done some more research on `flock`, and although people claim that `flock` doesn't work on certain filesystems (NFS, for example), this has been fixed for a very long time in the Linux kernel (since Linux 2.6.12, released in 2005). In addition, it seems to be widely available in Linux distros, and is installed by default, unlike other locking tools like `lockfile`. So I still think `flock` is the best option for this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510760560:223,release,released,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510760560,2,"['install', 'release']","['installed', 'released']"
Deployability,"Also have this error, using Cromwell 52, installed using this manual : . https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. logs say : fetch_and_run.is is a directory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747587937:41,install,installed,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747587937,2,"['Install', 'install']","['Installing', 'installed']"
Deployability,Also updated jenkins with file that appears in the dev /etc/cromwell.conf.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/85:5,update,updated,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/85,1,['update'],['updated']
Deployability,Also updated the sbt assembly plugin.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7123:5,update,updated,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7123,1,['update'],['updated']
Deployability,"Also, if possible, run something in JES beforehand, then run the liquibase update over the existing data and check the /metadata and /timing can still be accessed for the preexisting tasks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-181437112:75,update,update,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-181437112,1,['update'],['update']
Deployability,"Also, regardless of where things stand, this will *not* be part of the upcoming release. I want to give downstream users a chance to more thoroughly vet this in case there are subtle changes not picked up by our testing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731:80,release,release,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731,1,['release'],['release']
Deployability,"Also, that `hermes` command now needs to be updated for the context of wdl4s. I can do that though if there's a PR for it on wdl4s",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/367#issuecomment-170639638:44,update,updated,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/367#issuecomment-170639638,1,['update'],['updated']
Deployability,"Also, the shrink-wrapped releases never had any additional testing done compared to the daily snapshots; the daily system works for us because we have a TON of tests on every PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057:25,release,releases,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057,1,['release'],['releases']
Deployability,"Also. File locks are not that good. But locking via the database would be ideal for horicromtal. If I get some pointers I can implement a database lock. This will require an extra table or something, so I need some pointers on how this should be ideally integrated in cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498644488:254,integrat,integrated,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498644488,1,['integrat'],['integrated']
Deployability,"An attempt to document my observation of our general purpose debugging process - will hopefully help the next generation of Cromwell fire troubleshooters. * Moves the release processes under a new ""processes"" banner instead of awkwardly sitting in ""scripts""; * Adds a general-purpose recover process . See the process rendered and [in situ](https://github.com/broadinstitute/cromwell/tree/cjl_all_purpose_mess_remover/processes/troubleshooting). NB: If this gets approval, I'll update our playbook to link to this as our ""general purpose fallback process""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4991:167,release,release,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4991,2,"['release', 'update']","['release', 'update']"
Deployability,"An early look at the hog factor:. Adds:; - A hog factor in the configuration file representing ""how many greedy users would it take to use up all our resources""; - Higher values protect Cromwell's resources to keep them available for small-scale users; - Lower values let power users get their stuff done as fast as possible; - 1 is equivalent to ""no hog factor"" (and is the default); - Idea: it would be awesome to be able to dynamically scale this up and down based on ""we need to run a workflow"" or ""person X really needs to run their stuff at *full* speed for the next 4 hours""; - The ability to identify a workflow option as indicating hog group ; - A hog group is assigned to every `BackendWorkflowDescriptor` (using the specified workflow option if possible, or 'root workflow ID' if the specified option is not provided); - An update to the `TokenPool` to make it hog-group aware. TODOs:; - [x] Enhance `RoundRobinQueueIteratorSpec` with hog-limit tests; - [x] Enhance `TokenQueueSpec` with hog-limit tests; - [ ] Add full-system tests demonstrating the hog limit in action",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4013:63,configurat,configuration,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013,2,"['configurat', 'update']","['configuration', 'update']"
Deployability,"An example from a couple of errors that failed two PapiV2 CRON workflows:. ```; 2018-06-07 08:24:03,050 cromwell-system-akka.dispatchers.backend-dispatcher-666 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$trans",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:167,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"An example from a failed CRON test:. ```; 2018-07-04 07:18:56,909 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:128,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,3,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipelines']"
Deployability,"An update...; It looks like the performance of `sync` — run on command line entirely outside the context of cromwell — that we see on our box that happens to be running cromwell is atypical. Other machines in our compute cluster with same OS and similar amounts of memory execute `sync` in milliseconds, the cromwell server box takes up to a second. we're still trying to identify the cause; there are some configuration differences in the machines, specifically the number, type and state of the mounted filesystems, as well as differences in the applications running in background, but as it stands I'm happy to call this as not a cromwell performance issue, so much as a potential performance issue running cromwell in certain environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989:3,update,update,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989,2,"['configurat', 'update']","['configuration', 'update']"
Deployability,"Another point that Eric and I came across is that a ""Docker in Docker"" solution - i.e. installing Docker inside the Docker container where he's running Cromwell - is not good either because it necessitates pushing and re-pulling the Docker image he's iterating on, which makes for annoyingly long cycle times and can't work with bad or no Internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469427900:87,install,installing,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469427900,1,['install'],['installing']
Deployability,Any update about WDL 1.1 support ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1036412500:4,update,update,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1036412500,1,['update'],['update']
Deployability,"Any updates on this front? We’ve been having issues with occasional spikes in memory usage that don’t abide by a linear model for memory allocation. Currently this requires a lot of “babysitting” for our pipelines (or overprovisioning of memory), reducing their reliability and increasing their cost (on GCP, though the main cost factor is still developers’ time..). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4346#issuecomment-485594157:4,update,updates,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4346#issuecomment-485594157,2,"['pipeline', 'update']","['pipelines', 'updates']"
Deployability,Any updates on this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-392741040:4,update,updates,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-392741040,2,['update'],['updates']
Deployability,Any updates on this? I have run into the latter example here.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-850005555:4,update,updates,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-850005555,1,['update'],['updates']
Deployability,Anyone trying to set up cromwell in a non-default configuration (i.e. one that requires a custom `application.conf`) has to find all relevant stanzas within the documentation (and these are not all together). We should provide a default template that works with the latest release. Then a user can simply modify that default template and get up-and-running.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1590:50,configurat,configuration,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,"ApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.sca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:1155,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"Apologies if this has been addressed elsewhere but I'm not entirely sure where my problem is originating so no amount of Googling i've done today has given me any clue how to fix this. I'm running the pipeline ""processing-for-variant-discovery-gatk4.wdl"" on a local machine. I've left everything in the wdl file as it came from github, and the only thing i've changed in the input file is references to a local unmapped bam, and local reference files. edit: I'm using cromwell-39.jar. The problem is (i think) Cromwell keeps nesting the execution folder and copying over all inputs and references over and over again until the drive ultimately gets filled and the process crashes. An example is here in a screenshot:; ![repeated_cromwell_nesting](https://user-images.githubusercontent.com/1724546/66525112-4afd5600-eaa9-11e9-8995-f8b8005710f1.png). I'm really at a loss to what's happening and it's driving me insane -- any help is greatly appreciated.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5221:201,pipeline,pipeline,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5221,1,['pipeline'],['pipeline']
Deployability,"Apologies, you're totally right. I checked the wrong pipeline for this. This issue came from the WDL in the `gatk3-data-processing` workflow: https://github.com/gatk-workflows/gatk3-data-processing/blob/88fb7e2ba99b251ff917965640a5fe89f02c4dfd/processing-for-variant-discovery-gatk3.wdl#L279. I'll file an issue there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4393#issuecomment-440161270:53,pipeline,pipeline,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4393#issuecomment-440161270,1,['pipeline'],['pipeline']
Deployability,Appears to work in local testing: I can rename my backend to any goofy name I want and call caching still works as long as `name-for-call-caching-purposes` is set to whatever the backend name used to be. Still working on the whole PAPI upgrade Centaur test thing and needs docs obvs.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4460:236,upgrade,upgrade,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4460,1,['upgrade'],['upgrade']
Deployability,Archiving trigger updates [BW-564],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6230:18,update,updates,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6230,1,['update'],['updates']
Deployability,Are there any other changes which will have to be wrapped up in the RELEASE 0.17 umbrella?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/405#issuecomment-175232646:68,RELEASE,RELEASE,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/405#issuecomment-175232646,1,['RELEASE'],['RELEASE']
Deployability,"Are there updated acceptance criteria for this ticket, or can this be marked as complete? This currently works on the caas-dev server:. ```; $ curl https://cromwell.caas-dev.broadinstitute.org/engine/v1/status 2>/dev/null | jq .; {; ""ok"": true,; ""systems"": {; ""Cromwell"": {; ""ok"": true; },; ""Sam"": {; ""ok"": true; }; }; }; $ ; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4594#issuecomment-458194462:10,update,updated,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4594#issuecomment-458194462,1,['update'],['updated']
Deployability,"Are you guys going to do a proper release? Or label an existing tag as; newest release?. On Tue, Jan 10, 2017 at 4:35 PM, Jeff Gentry <notifications@github.com>; wrote:. > @Horneth <https://github.com/Horneth> @LeeTL1220; > <https://github.com/LeeTL1220> That's a good point. The latest version of; > wdltool should always work against the latest wdl4s/Cromwell; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271705111>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk07G1Ukk27IO1RNKJa46Dg9Vs14uks5rQ_mWgaJpZM4Lf57n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271878243:34,release,release,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271878243,2,['release'],['release']
Deployability,"Are you running 0.24?. On Sun, Jan 22, 2017 at 1:52 PM, Jeff Gentry <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> In the file you sent me the; > lines look like this /local/cga-fh/cga/Lee_Normal_; > Analysis/Pair/CESC-HSCX1005-TP-NB--/jobs/capture/mut/; > oncotate/job.83173721/CESC-HSCX1005-TP-NB--.snp.capture.maf.annotated. In; > the ""fixed"" one you sent me the lines look like this; > /dsde/data/test_dl_oxoq/CESC-HSCX1005-TP-NB--.snp.capture.maf.annotated.; > Further, even running it through dos2unix doesn't change this fact.; >; > Was full_m1_oncotated_list_pc.txt the actual file which caused the; > problem? If so could either this have been a GIGO situation or something; > else in the WDL run putting the wrong paths in your file? I find it hard to; > believe that those paths are what you meant.; >; > Whatever is going on here I don't believe it has to do with DOS-style; > newline chars as that doesn't seem to matter, even when I forcibly insert; > them.; >; > I'm closing this issue as one way or the other it appears to be a; > misnomer. However let's continue to followup either here or in person and; > potentially open a new issue w/ updated info.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-274350415>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk29mFygmQZGpLlqpvgZcjpAsa6BCks5rU6VtgaJpZM4LoLVh>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-274498966:1189,update,updated,1189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-274498966,1,['update'],['updated']
Deployability,Are you running the cromwell 31 release jar or did you build Cromwell from the sources ?; We changed something related to permissions in the develop branch ~1 week ago but it's not in the released 31 jar so I want to (maybe) rule that out.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3500#issuecomment-380213827:32,release,release,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3500#issuecomment-380213827,2,['release'],"['release', 'released']"
Deployability,"Are you using Pipelines API v1?. We are tracking a possible PAPI GPU issue that is limited to v1. If you are able to use the v2 backend instead, I would recommend that as a workaround (v2 is better in other ways too!).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4935#issuecomment-489150608:14,Pipeline,Pipelines,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935#issuecomment-489150608,1,['Pipeline'],['Pipelines']
Deployability,Aribtrary WDL in runtime attributes of provider configuration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4700:48,configurat,configuration,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4700,1,['configurat'],['configuration']
Deployability,"As a **Cromwell dev**, I want **to be able to release Cromwell with the same Github account**, so that **I don't have to use my personal github token.**. - effort: small; - risk: small; - business value: small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-344712648:46,release,release,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-344712648,1,['release'],['release']
Deployability,"As a **Cromwell dev**, I want **wdltool to be released automatically**, so that **when I release Cromwell, wdltool is released and up to date**.; - Effort: Small?; - Risk: Small; - Business Value: Small?; - @Horneth how much time/effort does it take to manually release wdltool? how much risk of human error is there?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089:46,release,released,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089,4,['release'],"['release', 'released']"
Deployability,"As a **Redteam member**, I want **to improve the Cromwell release process**, so that **it gets easier every time we release**.; * Effort: depends on the issue; * Risk: Small, depends on the issue; * Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666:58,release,release,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666,2,['release'],['release']
Deployability,"As a **production pipeline runner**, I want **to write all output files in one directory (rather than hierarchical)**, so that I can **(@ktibbett why is this helpful?)**.; - Effort: **Small**; - Risk: **Medium**; - if files have the same name they could be overwritten; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-326068415:18,pipeline,pipeline,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-326068415,1,['pipeline'],['pipeline']
Deployability,"As a **user running workflows on PAPI**, I want **my workflow to be upgraded from preemptible to regular compute if it is killed after 24 hours rather than retrying**, so that I can **avoid retrying on a job that will need more than 24 hours to run**.; - Effort: **Small**; - Risk: **Small to Medium**; - We'd need a way of being certain that the job was killed due to timeout, rather than another reason, to prevent from upgrading jobs that the user doesn't want upgraded.; - The information about a preemptible VM timing out should come from Google.; - This should be an ""opt-in"" feature, so users do not have the default behavior change from under them.; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096:68,upgrade,upgraded,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096,2,['upgrade'],['upgraded']
Deployability,"As a **user running workflows**, I want to **see my stderr output even when Cromwell gets a ""FileNotFound"" response** so that I can **debug my workflow**.; - Effort: Small to Medium; - We're not sure of the exact way to fix it, so for now we have been patching the issue.; - Risk: Small; - Business value: Small; - There is a workaround, to manually look up the stderr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336149092:252,patch,patching,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336149092,1,['patch'],['patching']
Deployability,"As a **user setting up Cromwell**, I want **only want to see references to Google Genomics Pipelines API**, so that **I know how to set up the Google backend, not some JES thing.**; - effort: small; - risk: small to medium; - business value: small; - may grow if it becomes confusing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-344685888:91,Pipeline,Pipelines,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-344685888,1,['Pipeline'],['Pipelines']
Deployability,"As a **workbench QA**, I want **to know what is being tested**, so that I **don't under- or over-test for each release**. ; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660:111,release,release,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660,1,['release'],['release']
Deployability,"As a Site Reliability Engineer (SRE) , I would like to have Cromwell support Sentry (https://sentry.io) to capture and report exceptions. This will allow me to better support our runtime operations and know when the system is functioning properly. This could be as simple as a document detailing how to deploy cromwell with the appropriate configuration, or it may involve code changes. @ansingh7115 is working on this in workbench at the moment and should be able to provide more information. @davidbernick can also provide configuration details.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2120:303,deploy,deploy,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2120,3,"['configurat', 'deploy']","['configuration', 'deploy']"
Deployability,"As a follow up I did some extra testing. As cromwell evaluates imports from `$PWD`. This means that ; ```; java -jar /some/absolute/path/cromwell-<version>.jar /another/absolute/path/workflow.wdl; ```; yields different results depending on the current working directory. In my opinion this is not desirable behavior. The small patch code that I wrote does not solve this issue. If cromwell is run from the same directory as the workflow.wdl it works, but in other cases it does not. . In an ideal case ; ```; java -jar /some/absolute/path/cromwell-<version>.jar /another/absolute/path/workflow.wdl; ```; will always lead to the same result no matter what $PWD is. This makes workflows reproducible and easy to be reused. ; This means that cromwell should use the absolute parent path of `workflow.wdl` to evaluate its imports from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-367252775:327,patch,patch,327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-367252775,1,['patch'],['patch']
Deployability,"As a high volume production user, I'm glad that Cromwell now has a decoupled mechanism for submitting workflows versus running them. I often will submit more workflows than my server or google quotas can handle and so I keep the maximum running workflows set to a smaller number of workflows than I submit. Previously I would have to keep my own queue and dribble workflows into Cromwell to get the same effect. However, one thing I have lost is the ability to prioritize those workflows! I need to be able to prioritize those submitted workflows that have not started running. This feature has several components:; - ability to specify a priority (integer) when submitting a workflow; - ability to set a priority for a submitted workflow (ok to not check it's status) [new REST endpoint to PATCH a workflow]; - change the query for the polling mechanism that picks up workflows to sort by descending priority (e.g. 1 is highest priority) when selecting workflows. While users can change the priority of a running workflow, this has no effect. It is the priority to start a workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1566:791,PATCH,PATCH,791,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566,1,['PATCH'],['PATCH']
Deployability,"As a pipeline author, I don't enjoy having to spin up a VM, with docker in order to do string substitutions on my parameters. In the GOTC pipeline, we do this in order to strip off the extension of the input file in order to get a base-name, which happens ~40 times in a 20-plex workflow. . This causes a real problem because by requiring so many VMs to spun up, we spend more money (although that cost is quite small) but also eat into our quotas and QPS limits, which actually does hurt our scalability. The proposal is to add a new expression language function which allows for regex substitutions:. sub(string, pattern, replacement). For example,. to strip off an extension from a file you could use `sub(filename, "".bam$"","""")`; to swap an extension, you could use `sub(filename, "".bam$"", "".metrics"")`. By being constrained to a regex (unlike an arbitrary code block) we don't have concerns about security or evaluating these in the cromwell engine. This does not eliminate the need for a generally more expressive expression language or user defined functions, but does solve a large class of common usages that impact ease of use and performance",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/597:5,pipeline,pipeline,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/597,2,['pipeline'],['pipeline']
Deployability,"As a pipeline author, I would like the ability to add assert-like statements to my tasks. For example, after the task has run check the output log for the presence of some message. This is important because we often find problems in the pipelines that we could have detected with some basic checks that could be part of the pipeline. They are different than unit tests because these problems often only arise under unusual data conditions that are hard to predict. The current approach on this is for @yfarjoun to test run these ideas just within the command block of WDL as a way to figure out what this feature really looks like. Then from that set of we can figure out what features need to be added in order to support this in a natural, DRY way",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1146:5,pipeline,pipeline,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146,3,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"As a pipeline author, sometimes I have pipelines which require lots of input files. For example, the joint calling pipeline at one point has a step where I need to combine genotypes from all the samples. This can be in the 10,000s range and growing. Currently, this causes lots of problems. In Cromwell having that many inputs causes memory and database problems because parameters are first class citizens and there are so many of them. In addition they are file paths which can be very long. This can lead to GBs of footprint. Similarly this causes problem for the underlying backend (e.g. JES) because of the volume. Recently our requests to JES were truncated because a load-balancer in front of the service had a maximum request size. I would like to be able to instead specify a file, which is full of file names. My task will know what to do with this. I need a way to indicate this in wdl (perhaps a new type, like FOFN instead of File?). With this information, the Cromwell backend can do the correct thing during localization. For example in JES, we would tell the JES Api this is a FOFN. . Each backend would then need to handle this type. When receiving a FOFN input type the backend would first localize the FOFN and then iterate through the contents to localize each file. A new FOFN would then be rewritten to reference the local paths, and that FOFN would be used in place of the original FOFN as parameters to the tasks. . First, we should conduct a feasibility effort on this with a thought experiment on the joint calling workflow to see if FOFNs would solve the parameter space problem ( #1059 )",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1058:5,pipeline,pipeline,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058,3,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"As a short-term workaround you might define different config backends for each Docker configuration you want to support, but that wouldn't scale well if you have a lot of different configurations. Also that requires changes to your conf file to match your WDL which is kind of gross. Do you have any specific suggestions how you'd want this to work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265:86,configurat,configuration,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265,2,['configurat'],"['configuration', 'configurations']"
Deployability,"As a user who runs large scale pipelines in Google, I may run into quota limitations on external IP addresses as they are consumed by my JES compute VMs. In many cases (using GCR and using GCS only) I would like to have JES create a node without an external IP address. JES has added this feature (see https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines) through the ""noAddress"" flag. noAddress must be set in both ephermeralPipeline and pipelineArgs. NOTE: Your project must be whitelisted in ""Google Access for Private IPs Early Access Program"". If your project is not whitelisted and you use noAddress, the operation will hang. This should be specified in the WDL as a runtime attribute, similar to the way cpus, memory or preemptible VMs are requested",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1325:31,pipeline,pipelines,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1325,3,['pipeline'],"['pipelineArgs', 'pipelines']"
Deployability,"As discussed in https://github.com/broadinstitute/cromwell/issues/6235, developers of workflows for GCP who store their images in Google Container Repositories can be exposed to large Google GCS egress charges when users attempt to run workflows in different continental regions, resulting in many trans-continental container pulls. There currently does not seem to be a satisfactory way to guard against this:. - We can't make our image repositories private because we want to make the workflows available to the public via Terra.; - We can't make the repositories requester-pays because the pipelines API does not support pulling images from requester-pays repositories.; - We can mirror our repositories to different regions, but we are still dependent on our users to configure their workflows to point to the right region and take good-faith extra steps to help us avoid these charges. Some possible ideas were suggested by @freeseek in https://github.com/broadinstitute/cromwell/issues/6235:. - Convince Google to support requester-pays buckets for container pulls in PAPI.; - Modify some combination of Cromwell/PAPI to cache images rather than pulling them for each task that is run.; - Develop infrastructure within Cromwell to know what region the workflow is running in and automatically select the right GCR mirror to pull from.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442:593,pipeline,pipelines,593,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442,1,['pipeline'],['pipelines']
Deployability,"As explained [here](https://cromwell.readthedocs.io/en/stable/filesystems/FileTransferProtocol/#instance-configuration), you will need an ftp stanza inside your backend or engine filesystem stanza such as this (look at the example as well):; ```; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```; I suppose you might leave the fto stanza empty without the optional parts, but maybe you still need it nevertheless. Remember also that the engine filesystem stanza is for Cromwell to be able to access the files with functions such as `read_lines()/read_map()/read_tsv()/read_json()/write_lines()/write_map()/write_tsv()/write_json()/etc.`, while the backend filesystem stanza is for the tasks to be able to access and localize files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392:105,configurat,configuration,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392,1,['configurat'],['configuration']
Deployability,"As explained in issue [#4304](https://github.com/broadinstitute/cromwell/issues/4304) now, it seems like the following three roles are required to run Cromwell with the PAPIv2 backend:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (`lifesciences.workflowsRunner`); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (`iam.serviceAccountUser`); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (`storage.objectAdmin`). Rather than the roles `storage.objectCreator` `storage.objectViewer` `genomics.pipelinesRunner` `genomics.admin` `iam.serviceAccountUser` `storage.objects.create` as explained in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-680282969:647,pipeline,pipelinesRunner,647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-680282969,1,['pipeline'],['pipelinesRunner']
Deployability,"As far as I remember, in the Past it was possible to reference global workflow variables inside a task. But now I get wdl validation errors like this:; ```; ERROR: Variable genome does not reference any declaration in the task (line 36, col 27):. curl -z ${folder}""/""${genome} --max-time 10 --retry 3 --retry-delay 1 ${genomeURL}; ^. Task defined here (line 26, col 6):. task download_genome {; ```; Here is the wdl; ```wdl; workflow indexes {. File genomesFolder; String version #release version; String species #species and also name of the index/. String releaseURL #path to releseas. String transcriptome #relative file name (.fa.gz); String genome #relative file name (.fa.gz); String annotation #relative annotation file name (.gtf). call download_genome {; input:; genomeURL = releaseURL + ""/"" + genome,; transcriptomeURL = releaseURL + ""/"" + transcriptome,; annotationURL = releaseURL + ""/"" + annotation,; folder = genomesFolder + ""/"" + species + ""/"" + version; }. }. task download_genome {. String genomeURL; String transcriptomeURL; String annotationURL; String folder. command {; mkdir -p ${folder}; curl -z ${folder}""/""${genome} --max-time 10 --retry 3 --retry-delay 1 ${genomeURL}; curl -z ${folder}""/""${transcriptome} --max-time 10 --retry 3 --retry-delay 1 ${transcriptomeURL}; curl -z ${folder}""/""${annotation} --max-time 10 --retry 3 --retry-delay 1 ${annotationURL}; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2504:481,release,release,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2504,5,['release'],"['release', 'releaseURL']"
Deployability,"As in, a nice easy-to-use client API for the Cromwell REST endpoint. So if anything RESTy changes in the future (like the shift to multipart forms...) a user must simply update their client API version, rather than modifying their own code.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1855:170,update,update,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1855,1,['update'],['update']
Deployability,"As most of biological containers (like http://biocontainers.pro) are hosted at quay.io it will be useful to get docker hash from there for call caching. Right now with the latest release I get the following message:; """"""; Cromwell attempted to retrieve the current hash for this docker image but failed.\nThis is not necessarily a cause for concern as Cromwell is currently only able to retrieve hashes for Dockerhub and GCR images.\nThe job will be dispatched to the appropriate backend that will attempt to run it.; """"""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2252:179,release,release,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2252,1,['release'],['release']
Deployability,As of recently-- some of the nightly integration tests have been failing with issues evaluating the contents of the RC file. . This is a transient failure that should already be retried -- and a part of the problem here is the inability to confirm if the operation was retried as expected. So there were two discussed solutions:; 1. Log the number of attempts to read a file as a part of the failure message for a job.; 2. There is a Cromwell configuration for the number of times an IO operation should be retried -- raise that number as a way to retry cloud hiccups.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4069:37,integrat,integration,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4069,2,"['configurat', 'integrat']","['configuration', 'integration']"
Deployability,"As per TechTalk this morning we're a bit on the fence about whether break develop (so that we're forced to fix it and don't have to hotfix 3 branches) or keep a separate branch and leave develop as is (so that people using it can keep doing that and we have a non-hotfix working branch).; I personally don't have a very strong opinion either way, I may slightly lean towards keeping a separate branch as long as we don't make significant changes to develop. If we do merge to develop I think we should change the default github branch to point to master though so that someone cloning Cromwell doesn't accidentally ends up with the broken version.; @geoffjentry ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2613#issuecomment-328900320:132,hotfix,hotfix,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2613#issuecomment-328900320,2,['hotfix'],['hotfix']
Deployability,"As per standup today I'm opening up a PR for the work in progress and will transition to more targeted PRs after this. There's a lot in this PR which is just renaming and/or moving stuff around, but feel free to comment fully on everything since I disallowed it the last round. Be aware that my response might wind up being ""that's already one of the things I want to do"" and thus get pushed off to a followup PR. Specific changes I made:. - Converted the actor based model for a future based model; - Updated the URL structure to reflect the [current nomenclature](https://github.com/ga4gh/workflow-execution-service-schemas/pull/33), closes #3877 ; - Where possible I named things after the corresponding concept in the WES spec, which is most of the renaming here; - A few other small changes which were either new-to-WES or not-yet-implemented",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3932:502,Update,Updated,502,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3932,1,['Update'],['Updated']
Deployability,"As per the comment @Horneth made I'm going to close this. @rgobbel if you still see this behavior in the newest versions (`develop` or a `hotfix` releas), please reopen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3325#issuecomment-371209125:138,hotfix,hotfix,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325#issuecomment-371209125,1,['hotfix'],['hotfix']
Deployability,As pipelines often deal with very large files (including intermediate files) and writing a good pipeline implies many runs it will be useful to have a clean rest API call to clean up previous executions to save some hard disk space.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2223:3,pipeline,pipelines,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2223,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"As pointed out by @davidbernick, there are some vulnerabilities in Cromwell's docker image. Beyond that, it's a good idea to periodically update the underlying image. This is not deemed to be a critical issue (yet) from a security perspective, but we should make sure to clear this up when we get a chance. $ docker run -it --rm -e CLAIR_ADDR=http://clair.bits-infosec.broadinstitute.org:6060 -e CLAIR_OUTPUT=High -e CLAIR_THRESHOLD=10 -e DOCKER_USER=davidbernick -e DOCKER_PASSWORD='xxxxx' broadinstitute/klar broadinstitute/cromwell:dev; clair timeout 1m0s; docker timeout: 1m0s; no whitelist file; Analysing 10 layers; Got results from Clair API v1; Found 139 vulnerabilities; Unknown: 3; Negligible: 47; Low: 38; Medium: 44; High: 7. CVE-2017-12424: [High] ; Found in: shadow [1:4.4-4.1]; Fixed By: ; In shadow before 4.5, the newusers tool could be made to manipulate internal data structures in ways unintended by the authors. Malformed input may lead to crashes (with a buffer overflow or other memory corruption) or other unspecified behaviors. This crosses a privilege boundary in, for example, certain web-hosting environments in which a Control Panel allows an unprivileged user account to create subaccounts.; https://security-tracker.debian.org/tracker/CVE-2017-12424; -----------------------------------------; CVE-2018-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; htt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4979:138,update,update,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979,1,['update'],['update']
Deployability,"As someone who is having a somewhat similar issue, what did you do to fix the resource management problem? I'm seeing a lot of processes spawned, and memory usage growing out of control. I have a system with a 6-core CPU and 32 GB of RAM, running Linux Mint, and I'm trying to run the processing-for-variant-discovery-gatk4.wdl pipeline (with some interest in running other GATK pipelines as well)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-508219639:328,pipeline,pipeline,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-508219639,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Assigning myself since the MySQL upgrade is the underlying cause. Here is the command sequence I use to create a new local DB for use by a fresh, totally default Cromwell checkout.; ```; docker run --name=mysql1 -p 3306:3306 -d mysql/mysql-server:latest; docker logs mysql1 # copy the auto generated password; docker exec -it mysql1 mysql -uroot -p # paste in the password from the previous stpe; ALTER USER 'root'@'localhost' IDENTIFIED BY '';; CREATE database cromwell_test;; CREATE USER 'root'@'%' IDENTIFIED BY '';; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' WITH GRANT OPTION;; FLUSH PRIVILEGES;; ```; I just validated it still works with recent versions, so it seems my theory about Docker using UTC is correct.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4803#issuecomment-480417841:33,upgrade,upgrade,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4803#issuecomment-480417841,1,['upgrade'],['upgrade']
Deployability,"Assuming I'm not too far from the reality, my opinion on this is that ; - I like that the actors don't have to know about the actual reference of the Metadata service. Meaning I'm ok with the push to the event stream approach.; - I like less that metadata is pushed only when actors change state. Their data can be changed (and is changed) when they receive a message but don't necessarily transition. And if I'm understanding this correctly this is not enough to capture that.; For example, the `WorkflowExecutionActor` state are basically `NotRunning`, `Running`, `Done`, we can only update metadata between those states. So there's no update of call status in real-time, outputs etc... Now from your comment I understand you want to do this by having the backend actors publishing metadata, but ; 1) nothing guarantees that they will, or with which format; 2) there are states that could only be known by then engine and that we might want to publish (`JobInitializing`, `Finalizing` etc...). Even more generally, I think the engine is the ultimate decider of what the state of a call is in the workflow, and relying on the backend to get this information seems a bit wrong.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219042534:586,update,update,586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219042534,2,['update'],['update']
Deployability,"Assuming `PostMVP` tagged tests are filtered out of `sbt test`, then they should never be _added_ to the ignored count. This patch therefore only adds to the ignored count for `sbt alltests:test`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1093#issuecomment-229391214:125,patch,patch,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1093#issuecomment-229391214,1,['patch'],['patch']
Deployability,"Assuming that putting that in the runtime {cpuPlatform: ""Intel Cascade Lake""} setting in the Cromwell conf file counts as ""in the configuration"", then yes. If we have to hand edit every single one of our WDL task files to put that in a runtime block, not I haven't tried that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2254634598:130,configurat,configuration,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2254634598,1,['configurat'],['configuration']
Deployability,At Fred Hutch we're using Github container registries and guiding people who are new to WDL and Cromwell to use them. They do work on our deployment of Cromwell but I can confirm that no tasks are ever call caching hits with ghcr.io containers. https://github.com/getwilds/wilds-docker-library,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-2153074225:138,deploy,deployment,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-2153074225,2,['deploy'],['deployment']
Deployability,At least a dozen Cromwell configuration links broken (not a copy of #6329),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6797:26,configurat,configuration,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6797,1,['configurat'],['configuration']
Deployability,"At some point we will have to update our liquibase library, either for vulnerability patches or other bug fixes. . However there is a report that the current liquibase version (3.6.3) plus the way our changelog differentiates databases (ex: using strings matching for things like `mysql`) causes an issue with MariaDB. This ticket is not about updating liquibase. Before that can occur we need a CI regression test to ensure that MariaDB is supported. A/C:; - At least centaur-local running against mariadb 10.3+. Links:; - https://github.com/broadinstitute/cromwell/issues/4605; - https://www.sourceclear.com/vulnerability-database/security/cross-site-scripting-xss-/java/sid-6098; - https://liquibase.jira.com/browse/CORE-3203; - https://liquibase.jira.com/browse/CORE-3263 (may not be related) ; - https://docs.travis-ci.com/user/database-setup/#mariadb; - https://docs.travis-ci.com/user/build-matrix/#explicitly-including-jobs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4618:30,update,update,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4618,2,"['patch', 'update']","['patches', 'update']"
Deployability,Attempt to decode value on failed cursor: DownField(manifestFormatVersion); at cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$.apply(PipelinesApiConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at cromwell.engine.backend.BackendConfigurationEntry.$anonfun$asBackendLifecycleActorFactory$1(BackendConfiguration.scala:13); at scala.util.Try$.apply(Try.scala:210); at cromwell.engine.backend.BackendConfigurationEntry.asBackendLifecycleActorFactory(BackendConfiguration.scala:14); at cromwell.engine.backend.CromwellBackends.$anonfun$backendLif,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:1703,pipeline,pipelines,1703,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,1,['pipeline'],['pipelines']
Deployability,"Authentication configuration has been coded but not properly tested for the ability to assume roles, etc. Integration tests should exist for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3747:15,configurat,configuration,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3747,2,"['Integrat', 'configurat']","['Integration', 'configuration']"
Deployability,Auto release wdltool on github,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2400:5,release,release,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400,1,['release'],['release']
Deployability,Automate brew release,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2574:14,release,release,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2574,1,['release'],['release']
Deployability,Automatically apply cromwell workflow id as a Pipelines API label,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1813:46,Pipeline,Pipelines,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1813,1,['Pipeline'],['Pipelines']
Deployability,"Automatically build Cromwell, update Cromwhelm on merge to develop [BW-1211]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6739:30,update,update,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739,1,['update'],['update']
Deployability,Available system variables accessible from Cromwell configuration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6005:52,configurat,configuration,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6005,1,['configurat'],['configuration']
Deployability,"Aye aye! I don't know scala, but I found the [developer docs](http://cromwell.readthedocs.io/en/develop/Building/) and I know how to use GIthub, so I'm ready to go, lol. I likely won't start this weekend (I have a few projects I'm working on!) but next week for sure. I'll put updates, troubles, and other musings here - thanks in advance for your help :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412309341:277,update,updates,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412309341,1,['update'],['updates']
Deployability,Azure blob read/write integration test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7024:22,integrat,integration,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7024,1,['integrat'],['integration']
Deployability,BA-5680 Test Jira integration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5011:18,integrat,integration,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5011,1,['integrat'],['integration']
Deployability,"BCS appears to have already wired through a `timeout` runtime attribute. This would be valuable as an option in PAPIv2 as well, especially as we are encountering problems with non-terminating actions. See https://gatkforums.broadinstitute.org/gatk/discussion/comment/58454/#Comment_58454 for a motivating use case. The code to amend with a custom timeout is where we build the `Pipeline` for the request: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/GenomicsFactory.scala#L135",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4946:378,Pipeline,Pipeline,378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4946,3,"['Pipeline', 'pipeline']","['Pipeline', 'pipelines']"
Deployability,BT-431 Update DRS Localizer for multiple access token strategies,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6553:7,Update,Update,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6553,1,['Update'],['Update']
Deployability,BT-637 Upgrade GoogleCloudStorage to 2.3.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6794:7,Upgrade,Upgrade,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6794,1,['Upgrade'],['Upgrade']
Deployability,BT-681 Add TES tags to release notes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6783:23,release,release,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6783,1,['release'],['release']
Deployability,BT-689 Restore and update cromwell.examples.conf,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6800:19,update,update,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6800,1,['update'],['update']
Deployability,BT-690 Add release notes for Cromwell 82,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6806:11,release,release,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6806,1,['release'],['release']
Deployability,BT-690 Add release notes to release process,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6809:11,release,release,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6809,2,['release'],['release']
Deployability,"BT-707 Update release process doc: Terra release notes, homebrew",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6812:7,Update,Update,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6812,3,"['Update', 'release']","['Update', 'release']"
Deployability,BT-745 Batch 2 of scala steward updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6906:32,update,updates,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6906,1,['update'],['updates']
Deployability,BT-745_b3 Update google api dependancies,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6911:10,Update,Update,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6911,1,['Update'],['Update']
Deployability,BW-1224 Security upgrade for jackson-databind,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6747:17,upgrade,upgrade,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6747,1,['upgrade'],['upgrade']
Deployability,"BW-1227 Upgrade jackson-databind, nimbus-jose-jwt [due 7/22]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6776:8,Upgrade,Upgrade,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6776,1,['Upgrade'],['Upgrade']
Deployability,BW-1227 Upgrade org.bouncycastle:bcprov-jdk15on:1.67 [due 6/17],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6775:8,Upgrade,Upgrade,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6775,1,['Upgrade'],['Upgrade']
Deployability,BW-1228 security upgrades,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6793:17,upgrade,upgrades,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6793,1,['upgrade'],['upgrades']
Deployability,"BW-1231 CHANGELOG, README updates for CWL, Alibaba",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6758:26,update,updates,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6758,1,['update'],['updates']
Deployability,BW-1258: Update awsSDKv from 2.17.152 to 2.17.194,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6814:9,Update,Update,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6814,1,['Update'],['Update']
Deployability,BW-1305 Swagger Update,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6818:16,Update,Update,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6818,1,['Update'],['Update']
Deployability,BW-1317 Upgrade `googleCloudNioV` to latest,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804:8,Upgrade,Upgrade,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804,1,['Upgrade'],['Upgrade']
Deployability,BW-1393 Traditional post-release doc updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6839:25,release,release,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6839,2,"['release', 'update']","['release', 'updates']"
Deployability,Backend configuration for SLURM added in beta state. #1750,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2158:8,configurat,configuration,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2158,1,['configurat'],['configuration']
Deployability,Backend secondary file and directory updates.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3153:37,update,updates,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3153,1,['update'],['updates']
Deployability,"Backend: AWS Batch; Cromwell version: 45.1; ----; I am building a WDL pipeline using the CloudFormation set up provided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162:70,pipeline,pipeline,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162,1,['pipeline'],['pipeline']
Deployability,Based on changes in https://github.com/cjllanwarne/wdl/pull/5; Updates String literal elements to maybe contain expression placeholders,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3312:63,Update,Updates,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3312,1,['Update'],['Updates']
Deployability,"Based on configuration (`shadowExecutionEnabled`), initialize the appropriate WorkflowManagerActor for both the CromwellServer mode and for command line execution.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/730:9,configurat,configuration,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/730,1,['configurat'],['configuration']
Deployability,"Based on the example you shared, would you mind listing what is actually produced inside of the call directory? ; ```s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-496508131:145,Pipeline,Pipeline,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-496508131,1,['Pipeline'],['Pipeline']
Deployability,"Based on the need for this use case, I strongly support merging this (or some) basic configuration, and then having documentation with a writeup about these different use cases. @TMiguelT you are spot on about creating the image before hand, and don't forget that in addition to build (which most users might not be able to do on their cluster) you can also just pull from docker:. ```bash; singularity pull docker://<container>; ```; and then create a binary (singularity image) that can be given directly to the run/exec command.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461444105:85,configurat,configuration,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461444105,1,['configurat'],['configuration']
Deployability,"Basic Expectations: ; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade → ; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; Log names (detritus files) don’t change (before and after), and if yes, then print a warning; Streaming logs (before and after), else error; Caching (before and after), else error; Job success (before and after), else error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4101:98,Upgrade,Upgrade,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4101,3,"['Upgrade', 'upgrade']","['Upgrade', 'upgrade']"
Deployability,"Basic Expectations:; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade →; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; - ~Log names (detritus files) don’t change (before and after), and if yes, then print a warning~ Split into Issue: #4188; - ~Streaming logs (before and after), else error~ Split into Issue: #4187; - ~Caching (before and after), else error~ PR merged: #4178; - ~Job success (before and after), else error~ PR merged: #4132",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4099:97,Upgrade,Upgrade,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4099,3,"['Upgrade', 'upgrade']","['Upgrade', 'upgrade']"
Deployability,Batch 1 of scala steward updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6903:25,update,updates,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6903,1,['update'],['updates']
Deployability,Batch submit as a hotfix. Closes #634,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/769:18,hotfix,hotfix,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/769,1,['hotfix'],['hotfix']
Deployability,"Before:; ```; select ; `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_DEFINITION`, ; `WORKFLOW_URL`, ; `WORKFLOW_ROOT`, ; `WORKFLOW_TYPE`, ; `WORKFLOW_TYPE_VERSION`, ; `WORKFLOW_INPUTS`, ; `WORKFLOW_OPTIONS`, ; `WORKFLOW_STATE`, ; `SUBMISSION_TIME`, ; `IMPORTS_ZIP`, ; `CUSTOM_LABELS`, ; `CROMWELL_ID`, ; `HEARTBEAT_TIMESTAMP`, ; `HOG_GROUP`, ; `WORKFLOW_STORE_ENTRY_ID` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (; `HEARTBEAT_TIMESTAMP` < '2022-02-01 21:13:48.287'; ); ) ; and (; not (`WORKFLOW_STATE` = 'On Hold'); ) ; order by ; `SUBMISSION_TIME` ; limit ; 1 for ; update; ```; After, with no groups excluded: `and (not false)`; ```; select ; `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_DEFINITION`, ; `WORKFLOW_URL`, ; `WORKFLOW_ROOT`, ; `WORKFLOW_TYPE`, ; `WORKFLOW_TYPE_VERSION`, ; `WORKFLOW_INPUTS`, ; `WORKFLOW_OPTIONS`, ; `WORKFLOW_STATE`, ; `SUBMISSION_TIME`, ; `IMPORTS_ZIP`, ; `CUSTOM_LABELS`, ; `CROMWELL_ID`, ; `HEARTBEAT_TIMESTAMP`, ; `HOG_GROUP`, ; `WORKFLOW_STORE_ENTRY_ID` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (; `HEARTBEAT_TIMESTAMP` < '2022-03-01 20:08:12.447'; ); ) ; and (; not (`WORKFLOW_STATE` = 'On Hold'); ); ) ; and (not false) ; order by ; `SUBMISSION_TIME` ; limit ; 1 for ; update; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6668#issuecomment-1055833845:600,update,update,600,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6668#issuecomment-1055833845,2,['update'],['update']
Deployability,"Besides concurring with the updated-names request, LGTM 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2080/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2080#issuecomment-289107247:28,update,updated-names,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2080#issuecomment-289107247,1,['update'],['updated-names']
Deployability,"Better error message for ""Upgrade Config from C26""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2186:26,Upgrade,Upgrade,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2186,1,['Upgrade'],['Upgrade']
Deployability,"Better still, we'd like to either:. 1) add `cloud-platform` scope, which would allow calling _any_ Google APIs. I understand this may not bode well with the current security model used in Firecloud; however, Google itself recommends migrating away from scopes in favor of IAM, as scopes were introduced before IAM existed [1]. 2) make scopes configurable, ideally at the workflow level, or at least at Cromwell config level. There may be a set of obligatory scopes that is hard-coded in Cromwell (e.g. `genomics` or `compute`), and then `additionalScopes` specified via configuration. This way, we satisfy both the need to restrict the scopes by default, and address other use cases when needed. Our ideal picture for this is that we'd be able to call any Google APIs (e.g. Pub/Sub, Firestore, or BigQuery) from workflows running on CaaS. We don't want to ""wait"" for a new scope to be added upstream each time we have to call a new API. Thanks!. [1] https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4115#issuecomment-424583308:570,configurat,configuration,570,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4115#issuecomment-424583308,1,['configurat'],['configuration']
Deployability,"Better theory is https://github.com/broadinstitute/firecloud-develop/pull/1556. IMO the risk/reward for the upgrade no longer checks out, especially since we are in a bit of a crunch mode and relatively ill-equipped to deal with unexpected problems.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4701:108,upgrade,upgrade,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4701,1,['upgrade'],['upgrade']
Deployability,"Bloom filter is a test of ""have we seen this before?"" on a huge set. We get one for free from Google Guava [here](https://google.github.io/guava/releases/22.0/api/docs/com/google/common/hash/BloomFilter.html)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332235515:145,release,releases,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332235515,1,['release'],['releases']
Deployability,Bonus awesomeness – removing this backend nerfs the vulnerable JDOM dependency that we would [otherwise have to upgrade](https://broadworkbench.atlassian.net/browse/BW-1228). ```; root(develop)> | 81> whatDependsOn org.jdom jdom2 2.0.6; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] ; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell-bcs-backend_2.13:81-5f48ded-SNAP [S]; [info] ; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell-engine_2.13:81-5f48ded-SNAP [S]; [info] ; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell-bcs-backend_2.13:81-5f48ded-SNAP [S]; [info] | +-org.broadinstitute:cromwell_2.13:81-5f48ded-SNAP [S]; [info] | ; [info] +-org.broadinstitute:cromwell-engine_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell_2.13:81-5f48ded-SNAP [S]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6785#issuecomment-1175286890:112,upgrade,upgrade,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6785#issuecomment-1175286890,1,['upgrade'],['upgrade']
Deployability,"Both test failures appear to be bogus, once @kshakir's work to tag the breaking tests as integration is complete the Travis builds should go green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/354#issuecomment-169402572:89,integrat,integration,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/354#issuecomment-169402572,1,['integrat'],['integration']
Deployability,"Brilliant, thanks for that workaround. That reverts back to the old approach which lets me update to 35 and make use of that temporarily. I'd still love to move to a Java-only version and help with runtimes if there's anything I can do to help debug that, but appreciate the workaround until we have more time to look at that. Thanks again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4183#issuecomment-426280070:91,update,update,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4183#issuecomment-426280070,1,['update'],['update']
Deployability,Bring Lenthall back + wdl4s update Closes #1691 Closes #589,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1769:28,update,update,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1769,1,['update'],['update']
Deployability,Bs integratedtest,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3212:3,integrat,integratedtest,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3212,1,['integrat'],['integratedtest']
Deployability,Bug Fix: Response error codes in releaseHold endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3918:33,release,releaseHold,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3918,1,['release'],['releaseHold']
Deployability,Build & doc updates.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2964:12,update,updates,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2964,1,['update'],['updates']
Deployability,Build WOMTool Jar with Cromwell release and update docs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3031:32,release,release,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3031,2,"['release', 'update']","['release', 'update']"
Deployability,Build updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4930:6,update,updates,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4930,1,['update'],['updates']
Deployability,Build updates incl. scala 2.12.12 and adoptopenjdk 11-hs BT-125 BT-126,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6194:6,update,updates,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6194,1,['update'],['updates']
Deployability,Build updates including checking for already published artifacts BT-250,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6370:6,update,updates,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6370,1,['update'],['updates']
Deployability,"Bump to [~3.2.1~ 3.2.2 patch version](http://slick.lightbend.com/news/2017/07/20/slick-3.2.1-released.html) that may help with this problem:. ```; Exception in thread ""db-10602"" java.lang.IllegalArgumentException: requirement failed: count cannot be decreased; at scala.Predef$.require(Predef.scala:277); at slick.util.ManagedArrayBlockingQueue.$anonfun$decreaseInUseCount$1(ManagedArrayBlockingQueue.scala:53); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at slick.util.ManagedArrayBlockingQueue.slick$util$ManagedArrayBlockingQueue$$locked(ManagedArrayBlockingQueue.scala:217); at slick.util.ManagedArrayBlockingQueue.decreaseInUseCount(ManagedArrayBlockingQueue.scala:52); at slick.util.AsyncExecutor$$anon$2$$anon$1.afterExecute(AsyncExecutor.scala:87); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```. Most of the changes in this PR are for the default length of strings in Slick going from 255 to 16777216 in this patch release (!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3414:23,patch,patch,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3414,4,"['patch', 'release']","['patch', 'release', 'released']"
Deployability,"Bumps [jackson-databind](https://github.com/FasterXML/jackson) from 2.11.1 to 2.12.6.1.; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/FasterXML/jackson/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.fasterxml.jackson.core:jackson-databind&package-manager=maven&previous-version=2.11.1&new-version=2.12.6.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6743:585,update,updates,585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6743,1,['update'],['updates']
Deployability,"Bumps [jackson-databind](https://github.com/FasterXML/jackson) from 2.13.2.2 to 2.13.4.1.; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/FasterXML/jackson/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.fasterxml.jackson.core:jackson-databind&package-manager=maven&previous-version=2.13.2.2&new-version=2.13.4.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6935:589,update,updates,589,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6935,1,['update'],['updates']
Deployability,"Bumps [jackson-databind](https://github.com/FasterXML/jackson) from 2.13.4.1 to 2.13.4.2.; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/FasterXML/jackson/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.fasterxml.jackson.core:jackson-databind&package-manager=maven&previous-version=2.13.4.1&new-version=2.13.4.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). You can trigger a rebase of this PR by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7110:589,update,updates,589,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7110,1,['update'],['updates']
Deployability,"Bumps [junit](https://github.com/junit-team/junit4) from 4.13 to 4.13.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/junit-team/junit4/releases"">junit's releases</a>.</em></p>; <blockquote>; <h2>JUnit 4.13.1</h2>; <p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.1.md"">release notes</a> for details.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/junit-team/junit4/blob/main/doc/ReleaseNotes4.13.1.md"">junit's changelog</a>.</em></p>; <blockquote>; <h2>Summary of changes in version 4.13.1</h2>; <h1>Rules</h1>; <h3>Security fix: <code>TemporaryFolder</code> now limits access to temporary folders on Java 1.7 or later</h3>; <p>A local information disclosure vulnerability in <code>TemporaryFolder</code> has been fixed. See the published <a href=""https://github.com/junit-team/junit4/security/advisories/GHSA-269g-pwp5-87pp"">security advisory</a> for details.</p>; <h1>Test Runners</h1>; <h3>[Pull request <a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1669"">#1669</a>:](<a href=""https://github-redirect.dependabot.com/junit-team/junit/pull/1669"">junit-team/junit#1669</a>) Make <code>FrameworkField</code> constructor public</h3>; <p>Prior to this change, custom runners could make <code>FrameworkMethod</code> instances, but not <code>FrameworkField</code> instances. This small change allows for both now, because <code>FrameworkField</code>'s constructor has been promoted from package-private to public.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/junit-team/junit4/commit/1b683f4ec07bcfa40149f086d32240f805487e66""><code>1b683f4</code></a> [maven-release-plugin] prepare release r4.13.1</li>; <li><a href=""https://github.com/junit-team/junit4/commit/ce6ce3aadc070db2902698fe0d3dc6729cd631f2""><code>ce6ce3a</code></a> Draft 4.13.1 ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5941:94,Release,Release,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5941,4,"['Release', 'release']","['Release', 'release', 'releases']"
Deployability,"Bumps log4j-api from 2.13.3 to 2.15.0. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-api&package-manager=maven&previous-version=2.13.3&new-version=2.15.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6586:353,update,updates,353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6586,1,['update'],['updates']
Deployability,"Bumps log4j-api from 2.13.3 to 2.16.0. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-api&package-manager=maven&previous-version=2.13.3&new-version=2.16.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6591:353,update,updates,353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6591,1,['update'],['updates']
Deployability,"Bumps log4j-api from 2.16.0 to 2.17.0. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-api&package-manager=maven&previous-version=2.16.0&new-version=2.17.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6594:353,update,updates,353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6594,1,['update'],['updates']
Deployability,"Bumps log4j-api from 2.17.0 to 2.17.1. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-api&package-manager=maven&previous-version=2.17.0&new-version=2.17.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6639:353,update,updates,353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6639,1,['update'],['updates']
Deployability,"Bumps log4j-core from 2.13.3 to 2.15.0. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-core&package-manager=maven&previous-version=2.13.3&new-version=2.15.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6587:355,update,updates,355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6587,1,['update'],['updates']
Deployability,"Bumps log4j-core from 2.13.3 to 2.16.0. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-core&package-manager=maven&previous-version=2.13.3&new-version=2.16.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6592:355,update,updates,355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6592,1,['update'],['updates']
Deployability,"Bumps log4j-core from 2.16.0 to 2.17.0. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-core&package-manager=maven&previous-version=2.16.0&new-version=2.17.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6595:355,update,updates,355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6595,1,['update'],['updates']
Deployability,"Bumps log4j-core from 2.17.0 to 2.17.1. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.logging.log4j:log4j-core&package-manager=maven&previous-version=2.17.0&new-version=2.17.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6640:355,update,updates,355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6640,1,['update'],['updates']
Deployability,"But the job ran successfully. Here is the full logs:. [ec2-user@ip-10-80-199-174 ~]$ java -Dconfig.file=aws.callcache.conf -jar ~/cromwell-35.jar run -i hello_inputs.json hello.wdl; [2018-11-21 15:08:54,14] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,32] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,62] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2018-11-21 15:09:03,91] [info] Slf4jLogger started; [2018-11-21 15:09:04,16] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-23ba05a"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-11-21 15:09:04,43] [info] Metadata summary refreshing every 2 seconds.; [2018-11-21 15:09:04,51] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,53] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,60] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-11-21 15:09:05,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Version 35; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-11-21 15:09:05,51] [info] Unspecified type (Unspecified version) workflow 02306258-436a-4372-ab54-2dcd83c42b47 submitted; [2018-11-21 15:09:05,52] [info] SingleWorkflowRunnerActor: Workflow submitted 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,53] [info] 1 new workflows fetched; [2018-11-21 15:09:05,53] [info] WorkflowManagerActor Starting workflow 02306258-436a-43",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:569,configurat,configuration,569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,2,['configurat'],['configuration']
Deployability,By default all cromwell task containers submitted by Google Cloud backend doesn't allow user to mount any filesystems within a container. It happens because containers are launched without specific linux capabilities being enabled. . Nevertheless filesystem mounts can be of help in some workflows because it doesn't require all the task resources to be localized or to be embedded in docker images. Google pipelines api allows to set `ENABLE_FUSE` flag for all submitted action. Once specified it forces Google pipelines engine to launch action containers with additional linux capabilities such as `CAP_SYS_ADMIN` being enabled. The pull request adds support for launching cromwell task containers with an enabled support for fuses in Google Cloud. Fuses support can be enabled via a workflow option `enable_fuse` or via a Google Cloud backend configuration attribute `backend.providers.Papiv2.config.genomics.enable-fuse`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5343:407,pipeline,pipelines,407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343,3,"['configurat', 'pipeline']","['configuration', 'pipelines']"
Deployability,"By making the draft release before building Cromwell, we will find out early if there's a problem with the auth token",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4198:20,release,release,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4198,1,['release'],['release']
Deployability,"By the book, the customized branch names have to merge so that tests pass, and then can be updated in a follow-on PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1553462820:91,update,updated,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1553462820,1,['update'],['updated']
Deployability,CAAS deployment process redux,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5002:5,deploy,deployment,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5002,1,['deploy'],['deployment']
Deployability,CI test Cromwell upgrade,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4099:17,upgrade,upgrade,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4099,1,['upgrade'],['upgrade']
Deployability,CI updates for Travis/Jenkins/Sentry.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4021:3,update,updates,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4021,1,['update'],['updates']
Deployability,CI updates including Travis re-containment BA-6026,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5202:3,update,updates,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5202,1,['update'],['updates']
Deployability,CI updates.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4287:3,update,updates,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4287,1,['update'],['updates']
Deployability,CI updates:,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4169:3,update,updates,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4169,1,['update'],['updates']
Deployability,"CROM-6865 Fix changelog parsing, update release doc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6799:33,update,update,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6799,2,"['release', 'update']","['release', 'update']"
Deployability,CROM-6894 Logging updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813:18,update,updates,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813,1,['update'],['updates']
Deployability,"CWL --type-version v1.0 --workflow-root main; [2018-10-23 17:48:48,28] [info] Running with database db.url = jdbc:hsqldb:mem:3bd78058-b880-451a-b3ef-71a48a2a17ce;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,34] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-23 17:48:55,36] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-23 17:48:55,49] [info] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:2553,configurat,configuration,2553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,2,['configurat'],['configuration']
Deployability,CWL support [was removed](https://github.com/broadinstitute/cromwell/releases/tag/79). References to it in code and documentation are being cleaned up as opportunity arises.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6973#issuecomment-1367087595:69,release,releases,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6973#issuecomment-1367087595,1,['release'],['releases']
Deployability,CWL updates to add directories and additional files.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3124:4,update,updates,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3124,1,['update'],['updates']
Deployability,CWL: large increase in pre-processing/salading runtimes in release 35,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4183:59,release,release,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4183,1,['release'],['release']
Deployability,Calculating pipeline billing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4394:12,pipeline,pipeline,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4394,1,['pipeline'],['pipeline']
Deployability,Call Caching Configuration (Cromwell + Workflow),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/292:13,Configurat,Configuration,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/292,1,['Configurat'],['Configuration']
Deployability,"Can be reproduced using the following workflow. ```wdl; version 1.0. task crash {; command <<<; kill -9 $$; >>>; }. workflow crash {; call crash ; }. ```; We use a configuration with the following values:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 120; default-runtime-attributes {; maxRetries: 2; }; }; }; }; }; workflow-options {; workflow-failure-mode = ""ContinueWhilePossible""; }; ```; On Cromwell 37 the workflow will be run. Jobs will be killed and retried.; On Cromwell 39, the retries will not happen any more.; This is very annoying, as our cluster kills jobs that exceed the memory limit, and some java based jobs seem to have random memory spikes. Having only 1 try means basically that a workflow with 50-100 jobs will usually fail, unless we give some jobs an insane memory parameter. This is probably caused by the refactoring in:; https://github.com/broadinstitute/cromwell/pull/4654/files; EDIT: This statement was not meant to put a blame on someone. I understand that code needs to be refactored at times and that bugs can creep in. I will look if I can fix the issue myself but maybe @cjllanwarne can also have a quick look? That would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4998:164,configurat,configuration,164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998,1,['configurat'],['configuration']
Deployability,Can we update the test cases which now work? I suspect `custom_mount_point` at least could be re-enabled?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4863#issuecomment-485560062:7,update,update,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4863#issuecomment-485560062,1,['update'],['update']
Deployability,"Can you add a REST API endpoint to return the value of a given configuration key, from the configuration of this Cromwell server?; I'm writing a script to run a workflow on a specified backend, automatically putting the inputs into the right filesystem; but there is no way to get the filesystem configured for given backend.; Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4317:63,configurat,configuration,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4317,2,['configurat'],['configuration']
Deployability,Can you also patch this in `wdltool` and any other dependencies that are as yet un-cromwell-repo-ified?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2755#issuecomment-337259904:13,patch,patch,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2755#issuecomment-337259904,1,['patch'],['patch']
Deployability,"Can you update the issue with an example of what you want to do, using the regex?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255241948:8,update,update,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255241948,1,['update'],['update']
Deployability,Candidate branch with updated versions and release notes.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1565:22,update,updated,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1565,2,"['release', 'update']","['release', 'updated']"
Deployability,Carbonite reading: configurable bucket read limit [BA-6489][51 hotfix],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5557:63,hotfix,hotfix,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5557,1,['hotfix'],['hotfix']
Deployability,"Carrying forwards only the ""womtool graph"" updates from #4522, allowing (in theory) WDL 1.0 and CWL graphs. Thorough review would be extremely welcome - this was originally part of a hackathon project so the quality of the code might be... somewhat wanting.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5318:43,update,updates,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5318,1,['update'],['updates']
Deployability,"Caused by: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; new issues",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3346#issuecomment-373590817:99,UPDATE,UPDATE,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3346#issuecomment-373590817,1,['UPDATE'],['UPDATE']
Deployability,Centaur should also be updated run the two tests in #1383 before closing the ticket.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1392#issuecomment-246056672:23,update,updated,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1392#issuecomment-246056672,1,['update'],['updated']
Deployability,Centaur test horizontal PAPI v2 upgrade,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4801:32,upgrade,upgrade,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4801,1,['upgrade'],['upgrade']
Deployability,"Centaur testing appears to fail when too many tests run concurrently due to:; ```; java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); [...]; Caused by: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: Batch, Status Code: 429, Request ID: 932e695f-5a4b-11e9-abf3-d5638efd51d6); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); [...]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4816:765,pipeline,pipeline,765,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4816,2,['pipeline'],['pipeline']
Deployability,Centaur upgrade,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3827:8,upgrade,upgrade,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3827,1,['upgrade'],['upgrade']
Deployability,Centaur/Sentry updates ahead of BigQuery.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4055:15,update,updates,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4055,1,['update'],['updates']
Deployability,"Certain error messages that Cromwell receives are longer than the default limit, which is a big pain when debugging. Going from 64 to 1024 characters (1kb) doesn't seem unreasonable and solves this issue. For context, the error message below is 364 characters. . [Relevant Akka Doc](https://doc.akka.io/docs/akka-http/10.0/configuration.html). . Before:; ```; 2024-04-12 14:58:18 cromwell-system-akka.actor.default-dispatcher-26 ERROR - Error in stage [akka.http.impl.engine.client.OutgoingConnectionBlueprint$PrepareResponse@71a2a20e]: Response reason phrase exceeds the configured limit of 64 characters; akka.http.scaladsl.model.IllegalResponseException: Response reason phrase exceeds the configured limit of 64 characters; 	at akka.http.impl.engine.client.OutgoingConnectionBlueprint$PrepareResponse$$anon$3.onPush(OutgoingConnectionBlueprint.scala:191); 	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:523); 	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:409); 	at akka.stream.impl.fusing.GraphInterpreterShell.runBatch(ActorGraphInterpreter.scala:606); 	at akka.stream.impl.fusing.ActorGraphInterpreter$SimpleBoundaryEvent.execute(ActorGraphInterpreter.scala:47); 	at akka.stream.impl.fusing.ActorGraphInterpreter$SimpleBoundaryEvent.execute$(ActorGraphInterpreter.scala:43); 	at akka.stream.impl.fusing.ActorGraphInterpreter$BatchingActorInputBoundary$OnNext.execute(ActorGraphInterpreter.scala:85); 	at akka.stream.impl.fusing.GraphInterpreterShell.processEvent(ActorGraphInterpreter.scala:581); 	at ; ...; ```. After: ; ```; <!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN"">; <html><head>; <title>401 Unauthorized</title>; </head><body>; <h1>Unauthorized</h1>; <p>This server could not verify that you; are authorized to access the document; requested. Either you supplied the wrong; credentials (e.g., bad password), or your; browser doesn't understand how to supply; the credentials required.</p>; </body></html>; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7406:323,configurat,configuration,323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7406,1,['configurat'],['configuration']
Deployability,Changelog updated and this PR is available for re-review. The lab deployed a `-SNAP` version. Updated the changelog list this change as part of `59` and filed BT-187 regarding hotfix PRs breaking in Circle CI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676:10,update,updated,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676,4,"['Update', 'deploy', 'hotfix', 'update']","['Updated', 'deployed', 'hotfix', 'updated']"
Deployability,Check out https://github.com/broadinstitute/wdltool/releases/tag/0.8,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271909372:52,release,releases,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271909372,1,['release'],['releases']
Deployability,Checkpoint update to wes2cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3932:11,update,update,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3932,1,['update'],['update']
Deployability,Chris and Adam;; Thanks so much for this fix. I really appreciate having an updated release with this working plus all the goodness since release 36.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4654#issuecomment-470983287:76,update,updated,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4654#issuecomment-470983287,3,"['release', 'update']","['release', 'updated']"
Deployability,"Chris;; Thanks for working on this and for the test case to iterate with. This example does work for me in the sense that it generates an md5sum, but also demonstrates the underlying issue I'm having with https inputs. I also get them downloaded and staged into my pipeline, but the file names get mangled into random download number. md5sum is cool with this, but many of my real tasks fail because the expected file extensions and associated secondary file extensions get lost with the random file names. Here's the example output I get from running this that demonstrates the file naming issue:; ```; /usr/bin/md5sum '/home/chapmanb/tmp/cromwell/cromwell_work/cromwell-executions/main-http_inputs.cwl/093e2835-e4cc-4731-9248-88d74dec0977/call-sum/inputs/1515144/1710814112361209342' | cut -c1-32; ```; This input should be called `jamie_the_cromwell_pig.png` but instead gets a long number attached to it. Is it possible to preserve initial file names with https like happens with other filesystem types?. In terms of the test cases, it would be great if it also checked that the file extension and name get preserved. Thanks again for looking at this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-439428999:265,pipeline,pipeline,265,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-439428999,1,['pipeline'],['pipeline']
Deployability,Clean up unzipped directories [36 hotfix],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4634:34,hotfix,hotfix,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4634,1,['hotfix'],['hotfix']
Deployability,Clean up unzipped directories [37 hotfix],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4637:34,hotfix,hotfix,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4637,1,['hotfix'],['hotfix']
Deployability,Cleanup documentation. Remove references to lifesciences and update docker authentication.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7196:61,update,update,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7196,1,['update'],['update']
Deployability,Clone pipeline resources and strip out mount point for runtime Closes #1501,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1507:6,pipeline,pipeline,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507,1,['pipeline'],['pipeline']
Deployability,"Closed the previous Pull Request (#841) since I ended up moving things to a new branch, but I'll address previous comments here:. Q: ""Is the system section [of application.conf] mandatory? It looks like this would throw if it's missing"" ; A: I don't think the section is mandatory in 0.19_hotfix, but looking back, it is mandatory in develop, something that needs a patch surely. Q: ""How can this [val serverMode = CromwellServer.isServerMode] be false ?"" ; A: You're right, it wasn't wired to be false ever in my previous PR. I've since changed it, please review it!. Documentation for this config change has also been updated and ready for review. Question for the reviewers: I arbitrarily moved the config to the backend stanza of the config file, since the system stanza doesn't exist anymore and the ""abortJobsOnTerminate"" config is also within the backends stanza. Is there a better place for it?. **MainSpec tests failing--it must be my changes, checking that out now, but hopefully the remaining changes can be examined in parallel**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/889:366,patch,patch,366,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/889,2,"['patch', 'update']","['patch', 'updated']"
Deployability,Closes #4473 . Potentially hotfix worthy,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4479:27,hotfix,hotfix,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4479,1,['hotfix'],['hotfix']
Deployability,"Closes #4917 . The ""should be confirmed"" part of the story is handled by `PipelinesApiRequestManagerSpec` where we have a mock `PipelinesApiRequestWorker` explode in various ways and check that the manager retries.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4976:74,Pipeline,PipelinesApiRequestManagerSpec,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4976,2,['Pipeline'],"['PipelinesApiRequestManagerSpec', 'PipelinesApiRequestWorker']"
Deployability,Closing because the update to 2.6.8 is about to be reverted,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5927#issuecomment-706336430:20,update,update,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5927#issuecomment-706336430,1,['update'],['update']
Deployability,"Closing on the basis that https://github.com/broadinstitute/cromwell/pull/3996 is on dev and no one seems to think this is important enough to hotfix - in fact, we forgot all about it for a while",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3997#issuecomment-416696267:143,hotfix,hotfix,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3997#issuecomment-416696267,1,['hotfix'],['hotfix']
Deployability,Closing this and including the update to WdlParser in the call element PR that I'm working on.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3322#issuecomment-369031380:31,update,update,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3322#issuecomment-369031380,1,['update'],['update']
Deployability,Closing this as sublime text and vim-wdl were updated. There remains an issue w/ intellij captured [here](https://github.com/broadinstitute/winstanley/issues/10) so that remains open.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-330277323:46,update,updated,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-330277323,1,['update'],['updated']
Deployability,Closing this given the fact that the latest QA testing framework in the form of a Jenkins build is now green: https://fc-jenkins.dsp-techops.broadinstitute.org/view/CromIAM-Testing/job/Taurus-Gatling-Test-Pipeline/237/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4268#issuecomment-450773870:205,Pipeline,Pipeline,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4268#issuecomment-450773870,1,['Pipeline'],['Pipeline']
Deployability,Closing this hotfix PR due to BT-187. Sibling develop PR is here: https://github.com/broadinstitute/cromwell/pull/6218,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6219#issuecomment-801443507:13,hotfix,hotfix,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6219#issuecomment-801443507,1,['hotfix'],['hotfix']
Deployability,"Closing this, we've released at least once since Nov '17",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2872#issuecomment-388163007:20,release,released,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2872#issuecomment-388163007,1,['release'],['released']
Deployability,"Closing this; I [added a cromwell recipe to bioconda](https://github.com/bioconda/bioconda-recipes/pull/2348), so now it is possible to `conda install cromwell` (if you have the bioconda channel in your config). See here for more info:; https://bioconda.github.io/recipes/cromwell/README.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1446#issuecomment-248438503:143,install,install,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1446#issuecomment-248438503,1,['install'],['install']
Deployability,"Code looks good, it just needs to be updated with the latest state for `develop`. I think Adam's updates should solve the issues Christian had regarding testing his changes in CI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1683875151:37,update,updated,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1683875151,2,['update'],"['updated', 'updates']"
Deployability,"Codecov can merge multiple reports from our various travis runs. For this to work, three steps are required:. - [x] Remove cromwell-core dependency from cloud-support #2938 ; - [x] Run jes centaur on travis #2948; - [x] Generate coverage for integration tests #2955",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2943:242,integrat,integration,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2943,1,['integrat'],['integration']
Deployability,Codify our release processes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4884:11,release,release,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4884,1,['release'],['release']
Deployability,Coerce evaluated declaration to their expected type + Bonus wdl4s update to fix https://github.com/broadinstitute/centaur/pull/176,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2119:66,update,update,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2119,1,['update'],['update']
Deployability,Command line configuration documentation.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5560:13,configurat,configuration,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5560,1,['configurat'],['configuration']
Deployability,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run hello_world/hello_world_0.wdl; ```; Output:; ```; [2018-08-30 17:53:11,17] [info] Running with database db.url = jdbc:hsqldb:mem:4fbaa426-09e6-4c70-9a1a-15469c4d77a0;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:19,24] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:53:19,26] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:53:19,39] [info] Running with database db.url = jdbc:hsqldb:mem:146c8707-d56e-4f58-a2de-df327f328109;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:20,13] [info] Slf4jLogger started; [2018-08-30 17:53:20,68] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-232861f"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-30 17:53:20,92] [info] Metadata summary refreshing every 2 seconds.; [2018-08-30 17:53:21,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-30 17:53:21,03] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,03] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,89] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-30 17:53:21,95] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-30 17:53:21,97] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-30 17:53:22,05] [info] Unspecified type (Unspecified version) workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc submitted; [2018-08-30 17:53:22,16] [info] SingleWorkflowRunnerActor: Workflow submitted 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,16] [info] 1 new workflows fetched; [2018-08-30 17:53:22,16] [info] WorkflowManagerActor Starting workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:725,configurat,configuration,725,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,1,['configurat'],['configuration']
Deployability,Completed https://github.com/broadinstitute/cromwell/blob/develop/release/release_workflow.wdl#L182-L291,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2574#issuecomment-424937576:66,release,release,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2574#issuecomment-424937576,1,['release'],['release']
Deployability,Config Backend configuration: job-id-regex vs multiline output,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4436:15,configurat,configuration,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4436,1,['configurat'],['configuration']
Deployability,"ConfigException$IO: application.conf: java.io.IOException: resource not found on classpath: application.conf; at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:190); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:152); at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:185); ... 48 more; Caused by: java.io.IOException: resource not found on classpath: application.conf; at com.typesafe.config.impl.Parseable$ParseableResources.rawParseValue(Parseable.java:726); at com.typesafe.config.impl.Parseable$ParseableResources.rawParseValue(Parseable.java:701); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); ... 51 more; ```. <!-- Which backend are you running? -->. The backend I'm running on is slurm and local . <!-- Paste/Attach your workflow if possible: -->. Workflow link. <!-- Def not an joke about best practices. Also thanks for publishing the gatk best practices and the warp pipelines -->. https://github.com/mmterpstra/Bestie. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Below are the first few lines of the config shown. ```; #see also https://cromwell.readthedocs.io/en/stable/backends/SLURM/; # include the application.conf at the top; include required(classpath(""application"")); workflow-options {; workflow-failure-mode = ContinueWhilePossible; delete_intermediate_output_files = true; final_workflow_outputs_dir = ""cromwell-results"",; use_relative_output_paths = true,; final_workflow_log_dir = ""cromwell-logs"",; final_call_logs_dir = ""cromwell-call_logs""; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=50000;; hsqldb.large_data=true;; hsqldb.applog=1;; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:5645,pipeline,pipelines,5645,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['pipeline'],['pipelines']
Deployability,Configurable value for service account Google Pipelines API uses for GCE nodes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1656:46,Pipeline,Pipelines,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1656,1,['Pipeline'],['Pipelines']
Deployability,"Configuring as you suggested (following the instructions on the provided URL) does not even start the process. Apart of some typos in the instructions (e.g., `MYPASSWORD` instead of `MYSQL_PASSWORD`), it looks that there is a conectivity problem with the docker container running mysql. Steps to reproduce:. ```bash; # start mysql-server container; docker run -p 3306:3306 --name cromwell_db -e MYSQL_ROOT_PASSWORD=`cat my_sql.root.pwd` -e MYSQL_DATABASE=cromwell -e MYSQL_USER=cromwell -e MYSQL_PASSWORD=cromwell -d mysql/mysql-server:5.7; ```. The docker server is working and I can access the database using `docker exec -it cromwell_db mysql -u cromwell -p`. Adding to my configuration file:. ```; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?useSSL=false""; user = ""cromwell""; password = ""cromwell""; connectionTimeout = 5000; }; }; ```. And running locally:. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Produces the following log, which is the same even increasing the timeout:. ```; [2018-03-12 11:25:38,45] [info] Running with database db.url = jdbc:mysql://localhost/cromwell?useSSL=false; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:24); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:63); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:47); 	at cromwell.CommandLineParser$.runCromwell(CommandLineParser.scala:95); 	at cromwell.CommandLineParser$.delayedEndpoint$cromwell$CommandLineParser$1(CommandLineParser.scala:105); 	at cromwell.CommandLineParser$delayedInit$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:676,configurat,configuration,676,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['configurat'],['configuration']
Deployability,"Confirmed that AWS SDK has a dependency on netty version 4.1.22.Final from [line 93 of AWS SDK pom.xml](https://github.com/aws/aws-sdk-java-v2/blob/2.0.0-preview-9/pom.xml#L93). `<netty.version>4.1.22.Final</netty.version>`; ; This is likely coming from very old version of Scala HTTP client `val sttpV = ""0.0.16""` current version is `v.1.1.12` https://github.com/softwaremill/sttp/releases",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381408850:382,release,releases,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381408850,1,['release'],['releases']
Deployability,Confirmed through manual testing with Cromwell and GCP Batch backend that providing just the `network-name` in the configuration works and is ok to leave off the `subnetwork-name`. I believe this can be a replacement in GCP Batch for the use of the `*` character for the region in the `subnetwork-name` that was used with PAPI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7505#issuecomment-2346612179:115,configurat,configuration,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7505#issuecomment-2346612179,1,['configurat'],['configuration']
Deployability,Confirmed working with an ad-hoc alpha deployment test:. ![image](https://user-images.githubusercontent.com/13006282/119009188-debf2580-b960-11eb-90c5-03c397abe1c4.png),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6358:39,deploy,deployment,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6358,1,['deploy'],['deployment']
Deployability,Consider refactoring the Demo DOS configuration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3858:34,configurat,configuration,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3858,1,['configurat'],['configuration']
Deployability,Considering the release and thus the release notes have already happened I'm going to close this even if it wasn't fully satisfied. @jsotobroad You likely already know this but FWIW the caching is not forward compatible from 0.19 to 0.21 but we expect to be forward compatible from here on out,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/685#issuecomment-253877212:16,release,release,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/685#issuecomment-253877212,2,['release'],['release']
Deployability,"Content seems good 👍 . ToL: If you don't want to in this PR, I might reorder this so that it goes from least-scary to most-scary, eg:. 1. How do I create my own configuration file for Cromwell; 1. A ""hello world"" example; 1. Description of all the options I can put in my configuration file; 1. What is reference.conf for and where is it?; 1. What is application.conf for and where is it?. Maybe with 2/3 swapped?. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2520/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2520#issuecomment-320297837:161,configurat,configuration,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2520#issuecomment-320297837,2,['configurat'],['configuration']
Deployability,"Conveniently, I was able to adopt the new GCS request code without updating any libraries, which seems appropriate for a hotfix.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5572:121,hotfix,hotfix,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5572,1,['hotfix'],['hotfix']
Deployability,"Cool thanks! So just to verify - I don't actually need to touch any scala, this is just a custom backend.conf for singularity (most of which I've already got a good start on?) This would simplify things quite a bit! Is this then provided in the workflow / pipeline or with cromwell here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412897542:256,pipeline,pipeline,256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412897542,2,['pipeline'],['pipeline']
Deployability,Cool! I've been thinking about how to approach this too. I looked at [shapeless' Typeable](https://github.com/milessabin/shapeless/blob/master/core/src/main/scala/shapeless/typeable.scala#L28) (and [docs](https://github.com/milessabin/shapeless/wiki/Feature-overview:-shapeless-2.0.0#type-safe-cast)) which co-exists with [ValueTypeable](https://github.com/milessabin/shapeless/blob/master/core/src/main/scala/shapeless/typeable.scala#L53) (declare from -> to types). Dunno if it has any advantages over rolling your own but it's worth a look to see if there are any.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373570762:504,rolling,rolling,504,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373570762,1,['rolling'],['rolling']
Deployability,"Cool. 59 is scheduled to go out before March 26. I'll edit the changelog once I figure out if the lab wants to:; - wait for us to finish fixing our CI and patch 58, or; - take a 59-SNAP and this fix may not be hotfixed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800644653:155,patch,patch,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800644653,2,"['hotfix', 'patch']","['hotfixed', 'patch']"
Deployability,"Copy of #5441. This adds a mechanism of gzipping the list of output files in the AWS backend to avoid the container override size limit. This mechanism was already in place for the inputs, this will simply utilize it for the outputs as well. I tried testing it, but the new `proxy` file doesn't seem to have been used. I guess the docker image needs to be updated for that. Does anyone have any ideas on how to do that? I couldn't find where this docker image gets used in cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5447:356,update,updated,356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447,1,['update'],['updated']
Deployability,Correct the path to the integration tests directory.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2822:24,integrat,integration,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2822,1,['integrat'],['integration']
Deployability,Could you hold off a bit? There's an update in flight to the plugin.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302112778:37,update,update,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302112778,1,['update'],['update']
Deployability,"Could you post the Cromwell configuration you're using ? Are the samples you're using relatively large?; I'm just wild guessing but one option would be that Cromwell is spending a lot of time trying to calculate md5 hashes for input files.; There's a configuration option to use file paths instead of file content to determine file equivalence for call caching purposes.; If you can make the assumptions that files are immutable and you don't go back and change their content manually this can be something to try.; To try that, you can update your Cromwell configuration: in the `backend.providers.SLURM.config` section (or whatever your backend is named instead of `SLURM`), and add this:; `filesystems.local.caching.hashing-strategy=""path""`; and this; `filesystems.local.caching.duplication-strategy=[""soft-link""]`. If possible I'd also recommend using a MySql DB instead of file based HSQL.; There are instructions here on how to do that in a few lines: http://cromwell.readthedocs.io/en/develop/tutorials/PersistentServer/#lets-get-started. Let me know if that makes any difference :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386406902:28,configurat,configuration,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386406902,4,"['configurat', 'update']","['configuration', 'update']"
Deployability,"Create Release candidate branch (Please keep in mind the various issues marked ""Blocks Release"" as the plan is to include those changes in the next release.). Verifying Release Branch. https://broadinstitute.atlassian.net/wiki/display/DSDEEPB/Cromwell+Release",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/609:7,Release,Release,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/609,5,"['Release', 'release']","['Release', 'release']"
Deployability,Create a configuration file suitable for WAAS. Outcomes:; * A PR in firecloud-develop with the new configuration; * Documentation on how to configure for WAAS; * [Optional] An example `.conf` file?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4738:9,configurat,configuration,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4738,2,['configurat'],['configuration']
Deployability,Create a github account with push access that can be used for the release,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2402:66,release,release,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402,1,['release'],['release']
Deployability,"Create a new ""final call"" execution to manage the copying of workflow outputs. Needs to be integrated with the changes in #351.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/354:91,integrat,integrated,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/354,1,['integrat'],['integrated']
Deployability,"Create a standalone application which will:. - Create/update schema as per #3242 ; - Subscribe to a PubSub topic; - Consume events in the format emitted by the metadata implementation at #3098 ; - For each metadatum, performs any necessary upserts into CloudSQL. This system should pull events off of the message queue exactly as fast as it is writing them to the database, i.e. it shouldn't be backing up and causing Slick* errors but it should also not be dawdling either. * I'm not at all opposed to using something other than Slick. If you want to go this route, let's talk.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3244:54,update,update,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3244,1,['update'],['update']
Deployability,Create folder with configuration (backends) examples,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4696:19,configurat,configuration,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4696,1,['configurat'],['configuration']
Deployability,Create integration tests for AWS,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3286:7,integrat,integration,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3286,1,['integrat'],['integration']
Deployability,Create new JAR / release notes; Update Cromwell Version in Homebrew. More details: https://broadinstitute.atlassian.net/wiki/display/DSDEEPB/Cromwell+Release,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/610:17,release,release,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/610,3,"['Release', 'Update', 'release']","['Release', 'Update', 'release']"
Deployability,Creating this PR for some insight into our CI pipeline. Not expecting integration tests to pass.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7023:46,pipeline,pipeline,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7023,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,CromIAM Swagger update BA-5829,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5068:16,update,update,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5068,1,['update'],['update']
Deployability,CromIAM deployment configuration in fc-develop repo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4545:8,deploy,deployment,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4545,2,"['configurat', 'deploy']","['configuration', 'deployment']"
Deployability,"Cromwell (38, in this case) is saturating the available connections to our managed MySQL database. Our DBAs increased the limit and Cromwell proceeded to fill up these slots. How can we limit the number of concurrent connections to a MySQL database? There doesn't seem to be any configuration option for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4777:279,configurat,configuration,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4777,1,['configurat'],['configuration']
Deployability,Cromwell 0.16 is available now on Macs via `brew install cromwell`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/372#issuecomment-170980870:49,install,install,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/372#issuecomment-170980870,1,['install'],['install']
Deployability,Cromwell 29 release notes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2530:12,release,release,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2530,2,['release'],['release']
Deployability,Cromwell 34 Hotfix -- stdout path mismatches metadata expectations,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4166:12,Hotfix,Hotfix,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4166,1,['Hotfix'],['Hotfix']
Deployability,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:43,configurat,configuration,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['configurat'],['configuration']
Deployability,Cromwell 55 expected to handle 504 error in GCS but instead WDL pipeline failed,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154:64,pipeline,pipeline,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154,1,['pipeline'],['pipeline']
Deployability,Cromwell Martha integration handles timezoneless dates [QA-1244],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5819:16,integrat,integration,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5819,1,['integrat'],['integration']
Deployability,Cromwell Martha integration handles timezoneless dates [QA-1244][53 hotfix],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5821:16,integrat,integration,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5821,2,"['hotfix', 'integrat']","['hotfix', 'integration']"
Deployability,Cromwell Status sometimes doesn't update correctly,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/800:34,update,update,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/800,1,['update'],['update']
Deployability,Cromwell WOM update,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2680:13,update,update,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2680,1,['update'],['update']
Deployability,"Cromwell appears to lock virtual machines to the nvidia driver 390.46 . https://github.com/broadinstitute/cromwell/blob/db19bada612dbbec3c3fea5e12fab83666bffea8/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L23. This becomes problematic because this driver is incompatible with CUDA 10, which is required for the stable release of TensorFlow. GCP recommends using 410.79 or higher. ; https://cloud.google.com/compute/docs/gpus/add-gpus#install-driver-script. Could the upcoming release move over to the higher version? The upgrade to the newer driver version should not break driver compatibility (famous last words); https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4942:186,pipeline,pipelines,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4942,8,"['Pipeline', 'deploy', 'install', 'pipeline', 'release', 'upgrade']","['PipelinesApiRuntimeAttributes', 'deploy', 'install-driver-script', 'pipelines', 'release', 'upgrade']"
Deployability,"Cromwell cannot handle any output that is struct base. I enclose a workflow where it is very clear to see that even though both latest development versions (as well as latest releases) of Cromwell and Womtool validated and executed the workflow for some strange reason at runtime Cromwell consider that it is a Map and not a struct and crashes in the very end of execution; ```; QuantifiedRun quantified_run = {""run"": srr, ""folder"": quant_folder, ""quant"": quant, ""lib"": quant_lib}; ```; ![screenshot_2019-02-15 screenshot](https://user-images.githubusercontent.com/842436/52889800-4dade280-318a-11e9-87b9-8b364e3408dd.png); [crashes_at_runtime.zip](https://github.com/broadinstitute/cromwell/files/2871310/crashes_at_runtime.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4663:175,release,releases,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663,1,['release'],['releases']
Deployability,"Cromwell communicates with various gcloud endpoints. Instead of having N creds for N endpoints, sometimes Cromwell reuses creds for something else. For example: using the GCS creds for docker hash lookups [here](https://github.com/broadinstitute/cromwell/blob/78/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiBackendLifecycleActorFactory.scala#L70-L82). This PR adds an optional config for reference disk validation auth falling back to the `genomics` auth that was used previously. One can then use USA authentication for individual genomics calls and a separate system SA for verifying which of the reference disks are valid (at startup).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933:288,pipeline,pipelines,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933,3,"['Pipeline', 'pipeline']","['PipelinesApiBackendLifecycleActorFactory', 'pipelines']"
Deployability,Cromwell configuration to use reference disk for localization [BA-6390],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5596:9,configurat,configuration,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5596,1,['configurat'],['configuration']
Deployability,"Cromwell doesn't control how the worker VM responds to 403s, that is internal to PAPI. At most, it can retry the whole task (pipeline) if it classifies `PAPI error code 7` as a retryable failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545949117:125,pipeline,pipeline,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545949117,1,['pipeline'],['pipeline']
Deployability,"Cromwell doesn't support images from ghcr.io. Not sure if there's a workaround besides personally rehosting the image somewhere else. Seeing as a lot of bioinformatics images are hosted there I'd like to see it be supported. ## error returned; [2022-08-11 12:40:55,22] [warn] BackendPreparationActor_for_e48a0b67:Minos.minos_adjudicate:-1:1 [e48a0b67]: Docker lookup failed; java.lang.Exception: Registry ghcr.io is not supported. ## backend; Running Cromwell on a local machine, eventually will be running in Terra. My local machine has Docker installed and already has the required Docker image pulled. ## relevant workflow task; ```; task minos_adjudicate {; 	input {; 		File ref; 		File reads; 		File vcf1; 		File vcf2. 		# runtime attributes; 		Int addldisk = 1; 		Int cpu = 4; 		Int retries = 1; 		Int memory = 8; 		Int preempt = 2; 	}; 	# Estimate disk size required; 	Int ref_size = ceil(size(ref, ""GB"")); 	Int finalDiskSize = 2*ref_size + addldisk. 	String ref_basename = basename(ref). 	command <<<; 		# softlinks don't seem to cut it here; 		set -eux -o pipefail; 		cp ~{ref} .; 		minos adjudicate --reads ~{reads} outdir ~{ref} ~{vcf1} ~{vcf2}; 	>>>; 	; 	runtime {; 		cpu: cpu; 		docker: ""ghcr.io/iqbal-lab-org/minos""; 		disks: ""local-disk "" + finalDiskSize + "" HDD""; 		maxRetries: ""${retries}""; 		memory: ""${memory} GB""; 		preemptibles: ""${preempt}""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6827:545,install,installed,545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6827,1,['install'],['installed']
Deployability,"Cromwell itself does not use Log4j. This can be verified by executing `sbt dependencyTree` and noting that all instances of ""log4j"" occur in `org.slf4j:log4j-over-slf4j` which is a Log4j [compatibility bridge from a different project](http://www.slf4j.org/legacy.html#log4j-over-slf4j). The utility tool `CromwellRefdiskManifestCreator` is written in Java and does use Log4j. It is not included in the Cromwell JAR. It is [being updated](https://github.com/broadinstitute/cromwell/pull/6593) presently.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-994127087:429,update,updated,429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-994127087,1,['update'],['updated']
Deployability,"Cromwell localizes every input file in the call directory when the task gets run. You can specify how it gets localized (soft-link, hard-link, copy) in the [configuration file](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/application.conf#L148). Does this make your workflow / call fail ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1140#issuecomment-231414741:157,configurat,configuration,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1140#issuecomment-231414741,1,['configurat'],['configuration']
Deployability,Cromwell may be vulnerable in certain configurations. This is being looked into. We recommend the immediate remedy of disabling the vulerable feature of Log4j:; ```; ‐Dlog4j2.formatMsgNoLookups=True; ```; [Source.](https://logging.apache.org/log4j/2.x/),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-992699211:38,configurat,configurations,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-992699211,1,['configurat'],['configurations']
Deployability,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:37,Pipeline,Pipelines,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898,2,['Pipeline'],['Pipelines']
Deployability,"Cromwell perf ad-hoc deploy, test, and destroy scripts BA-4980",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5012:21,deploy,deploy,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5012,1,['deploy'],['deploy']
Deployability,"Cromwell publishes no metadata regarding which Cromwell instances have processed a workflow. This has been quite reasonable up to now since it has only been possible to have one worker Cromwell per deployment, but that is now changing with horizontal Cromwell. It would useful for operations and provenance to know which instances have been involved in processing a workflow. Things to think about:. - This data can be multi-valued; - Time indexes are important; - Stable Cromwell identifiers are important. Conversely, ephemeral Cromwell IDs that evaporate on restarts aren't useful.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4723:198,deploy,deployment,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4723,1,['deploy'],['deployment']
Deployability,"Cromwell release 36.1 has a docker image available for the release, but nowhere in the docs is the cromwell docker container documented (as far as I can tell).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4682:9,release,release,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4682,2,['release'],['release']
Deployability,Cromwell release doesn't bump WDL4S versions,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2225:9,release,release,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2225,1,['release'],['release']
Deployability,Cromwell requires Java 11 [0][1] and I see 17 there. [0] https://cromwell.readthedocs.io/en/stable/Releases/; [1] https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#60-release-notes,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1241312157:99,Release,Releases,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1241312157,2,"['Release', 'release']","['Releases', 'release-notes']"
Deployability,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3109:55,configurat,configuration,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109,2,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"Cromwell tests have an application.conf file... and the main source code has a default application.conf file. The test application.conf overwrites values in the main one, but this has caused confusion in the past. It'd be nicer if only one configuration file were used during tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/768:240,configurat,configuration,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/768,1,['configurat'],['configuration']
Deployability,Cromwell's failure is now complete (hotfix edition),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2205:36,hotfix,hotfix,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2205,1,['hotfix'],['hotfix']
Deployability,"Cromwell: 36; Backend: PAPI/Google Cloud. If an input file to a WDL pipeline contains a space, the script Cromwell generates to run on the pipelines API will fail. For instance, I have an input file like this (truncated):; ```json; {; ""best_practise.ref_fasta"": ""gs://genovic-test-data/hg19/Reference Genome/default_unzipped/ucsc.hg19.fasta"",; }; ```; This make Cromwell generate a PAPI script containing this line:; ```bash; bash_ref_fasta=/cromwell_root/genovic-test-data/hg19/Reference Genome/default_unzipped/ucsc.hg19.fasta; ```. This then causes the following error in the script:; ```; /cromwell_root/script: line 26: Genome/default_unzipped/ucsc.hg19.fasta: No such file or directory; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4393:68,pipeline,pipeline,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4393,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Cromwell: 36; Mode: Server; Backend: Google Cloud; ***; I've noticed a strange error that only happens on the Google Cloud backend. Cromwell runs the WDL tasks with the correct command line, but somehow the arguments after the initial command aren't being picked up by the binary inside the docker container. It seems like only the first argument is actually being used. This isn't an issue with my python script, because I can run it directly and everything works fine. Cromwell showing the command line:; ```; cromwell_1 | 2018-11-12 06:57:56,451 cromwell-system-akka.dispatchers.backend-dispatcher-40 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(5d4c4459)germline_variant_calling.fastqc:0:1]: `/app/fastqc_docker.py --output-dir . --read ""/cromwell_root/genovic-test-data/cardiom/NA12878_CARDIACM_MUTATED_L001_R1.fastq.gz"" --format fastq`; ```. Cromwell failing with an error because the `--read` argument is missing (even though you can see it's not, in the above log):; ```; cromwell_1 | java.lang.Exception: Task germline_variant_calling.fastqc:0:1 failed. The job was stopped before the command finished. PAPI error code 10. 11: Docker run failed: command failed: usage: fastqc_docker.py [-h] -r READ -o OUTPUT_DIR [-c CONTAMINANTS]; cromwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:611,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,611,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"Cromwell: Development (`37-3b2affa`); Backend: HPC (`ConfigBackendLifecycleActorFactory`). I wanted access to a recently merged pull request (#4437), so I built a development version of Cromwell. However, when I run it with the same configuration file as I used for Cromwell 36, I get this error:; ```; [ERROR] [01/24/2019 11:10:24.126] [cromwell-system-akka.actor.default-dispatcher-4] [akka://cromwell-system/user/SingleWorkflowRunnerActor] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:233,configurat,configuration,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,2,['configurat'],['configuration']
Deployability,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3554:376,integrat,integration,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554,1,['integrat'],['integration']
Deployability,"Current proposal is to support the [`disks` runtime attribute](http://cromwell.readthedocs.io/en/develop/RuntimeAttributes/#disks) using the following rules:. 1. For all tasks, provide a predictable bind mount for `local-disk`. Specifications for disk size and disk type will be ignored, as they are not needed or configurable at runtime for AWS Batch. ; 2. Other mount points that are defined (e.g. `disks: ""/mnt/my_mnt 3 SSD, /mnt/my_mnt2 500 HDD""`) will result in additional bind mounts from the host container to the running docker container for the task. The disk size and disk type are ignored. The AWS Batch reference deployment for Cromwell will provide a mount point for each task which the `disks` will be structured under. As an example, assume the following runtime attribute definition:. ```; runtime {; disks: ""local-disk 100 SSD, /mnt/my_mnt 3 SSD, /mnt/my_mnt2 500 HDD""; }; ```. Will result in a filesystem tree structure on the host:. ```; /mnt/cromwell_io_mountpoint/; ├── $CROMWELL_TASK_ID; ├── /cromwell_root; └── /mnt/; ├── /my_mnt; └── /my_mnt2; ```. And the running container will see the `/cromwell_root`, `/mnt/my_mnt` and `/mnt/my_mnt2` directories.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-398854923:625,deploy,deployment,625,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-398854923,1,['deploy'],['deployment']
Deployability,Currently all output files will be kept. In most cases not all output files need to be kept and only wasting a lot of disk space. When having to not a lot of diskspace available might block a full scale pipeline run. To do this each jobs and/or (sub)workflow should have a runtime tag `intermediate: Boolean` or `intermediate_files: Array[File]`. When no downstream dependency need those files anymore they can be removed during the pipeline run. The same functionality was already implemented in GATK/Queue.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3881:203,pipeline,pipeline,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881,2,['pipeline'],['pipeline']
Deployability,"Currently each of cromwell, wdl4s, wdltool etc. each handle their versioning independently. . It would be easier to script releases if they used a standardized approach, be it version.sbt or project/Version.scala.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2544:123,release,releases,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2544,1,['release'],['releases']
Deployability,Currently our release to the `brew` package management system is manual. It should be scripted and included in the release process.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2574:14,release,release,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2574,2,['release'],['release']
Deployability,"Currently when Cromwell starts it updates all `Running` workflows to `RestartableRunning` and `Aborting` to `RestartableAborting` so that it knows which workflows were already running and need to be restarted.; If multiple Cromwells are started against the same DB, they can't all keep changing all workflow statuses as they start.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3347:34,update,updates,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3347,1,['update'],['updates']
Deployability,"Currently when I ask for metadata or for logs, I get something like:; ```; {; ""calls"": {; ""annotate_de_novo.transdecoder"": [; {; ""stderr"": ""/pipelines/cromwell_1/cromwell-executions/annotate_de_novo/e64a866e-d5c1-4779-8f9d-75457f22f43e/call-transdecoder/execution/stderr"",; ""stdout"": ""/pipelines/cromwell_1/cromwell-executions/annotate_de_novo/e64a866e-d5c1-4779-8f9d-75457f22f43e/call-transdecoder/execution/stdout"",; ""attempt"": 1,; ""shardIndex"": -1; }; ]; },; ""id"": ""e64a866e-d5c1-4779-8f9d-75457f22f43e""; }; ```; All the details about what really happened (stderr and stdout of the tools that failed) are not accessible via REST API and I have to ssh to the server to read those files. What can be useful is to have a way to get stdout/stderr content for each call with Cromwell REST API, that can save a lot of time on ssh-ing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2928:141,pipeline,pipelines,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2928,2,['pipeline'],['pipelines']
Deployability,"Currently when a user issues a request to abort a workflow, the process is purely in-memory. This means that there's no way to manually trigger an abort other than the web endpoint. Instead, do something similar to the workflow store. Record the request in a table, and have cromwell monitor that table looking for workflows it is running, and when it sees such a thing use that to trigger the abort. . It is **not** a bad state if there's a workflow in the table which Cromwell doesn't know about. Make sure any updates to this table are [locked](https://github.com/broadinstitute/cromwell/issues/3342)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3344:513,update,updates,513,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3344,1,['update'],['updates']
Deployability,"Currently, monitoring only functions on google VMs and not on local cromwell runs (the documentation is not clear on that). . The mint team is developing pipelines where we would like to use a monitoring script to judge ram and disk usage to better estimate runtime specifications. It would be helpful if it could be enabled for local use as well as VM usage (I hear this isn't hard). . Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2642:154,pipeline,pipelines,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2642,1,['pipeline'],['pipelines']
Deployability,"Currently, the configuration file should be passed as `java -Dconfig.file=/path/to/your.conf cromwell.jar`; neverhteless, if installed with the formula from homebrew-core, the wrapper script does not allow that configuration:. ```; #!/bin/bash; exec java -jar /usr/local/Cellar/cromwell/30.2/libexec/cromwell-30.2.jar ""$@""; ```. Could it be possible to add a way to provide the config file in this case? Something like an environmental variable can be useful (e.g., `export CROMWELL_CONFIG_FILE=/path/to/your.conf` will pick up directly this in every run).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3262:15,configurat,configuration,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3262,3,"['configurat', 'install']","['configuration', 'installed']"
Deployability,"Currently, the formula in hombrew-core (https://github.com/Homebrew/homebrew-core/blob/master/Formula/cromwell.rb) does install only cromwell, but not womtool. It will be nice to install both at the same time, or have an option to do so. Another possibility is to add a new formula for it informing that it exists...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3254:120,install,install,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3254,2,['install'],['install']
Deployability,"Currently, the local backend will spawn the maximum number of processes to run a workflow. . Why is this a problem? ; - This can cripple a machine with fewer CPUs than number of tasks that can be executed.; - This can cause all of the RAM to be used running a workflow. Again, crippling a machine.; - If either of the two above conditions are met, total wall clock time will increase and/or jobs will be killed and/or the cromwell process will be killed.; - This hinders the development of pipelines. Proposed solution:; - Allow users to specify the maximum number of simultaneous jobs to run for a workflow. Similar to how it is done in Queue. Workaround:; - Use JES, if available; - Use SGE, if available. Who?; - Pipeline engineers; - Method developers; - External researchers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1354:490,pipeline,pipelines,490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1354,2,"['Pipeline', 'pipeline']","['Pipeline', 'pipelines']"
Deployability,"Currently, we have a number of ignored tests that are part of the default 'sbt test' configuration which we are not planning on re-enabling for MVP. By excluding these we (a) don't lose them and (b) should be able to drive the ignored tests to 0 (yay!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/996:85,configurat,configuration,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/996,1,['configurat'],['configuration']
Deployability,Cut loose PapiV1 cron and minor CI updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3681:35,update,updates,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3681,1,['update'],['updates']
Deployability,"D: cffe6e45-d66c-11e8-a1df-05402551b0ba)`. The specific case where this happens is in the `gatk3-data-processing` workflow, when running the `ApplyBQSR` task, which is run in parallel over some calculated intervals. The full error trace I get is:. ```; 2018-10-23 02:39:07,631 cromwell-system-akka.dispatchers.backend-dispatcher-53345 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(6d97fef4)GPPW.ApplyBQSR:15:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: cfc6e34e-d66c-11e8-be0b-dd778498cf15); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(R",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:1262,pipeline,pipeline,1262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,DBMS updates BA-5795 BA-5692,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5058:5,update,updates,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5058,1,['update'],['updates']
Deployability,DNS & batch request minimal fixes [47(!!) hotfix edition] [BA-6534],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5600:42,hotfix,hotfix,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5600,1,['hotfix'],['hotfix']
Deployability,DNS & batch request minimal fixes [C51 hotfix 6],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5572:39,hotfix,hotfix,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5572,1,['hotfix'],['hotfix']
Deployability,"DO NOT MERGE - the dependent wdl4s version is not on develop or even cromwomification yet. A serious contender for the most boring PR ever, this is just the updates to point to a subprojectified, cromwommy version of wdl4s.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2448:157,update,updates,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2448,1,['update'],['updates']
Deployability,"D_GCS=https://storage.googleapis.com/cos-tools; + COS_KERNEL_SRC_GIT=https://chromium.googlesource.com/chromiumos/third_party/kernel; + COS_KERNEL_SRC_ARCHIVE=kernel-src.tar.gz; + TOOLCHAIN_URL_FILENAME=toolchain_url; + TOOLCHAIN_ARCHIVE=toolchain.tar.xz; + TOOLCHAIN_ENV_FILENAME=toolchain_env; + CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chromiumos-sdk; + ROOT_OS_RELEASE=/root/etc/os-release; + KERNEL_SRC_DIR=/build/usr/src/linux; + NVIDIA_DRIVER_VERSION=418.40.04; + NVIDIA_DRIVER_MD5SUM=; + NVIDIA_INSTALL_DIR_HOST=/var/lib/nvidia; + NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia; + ROOT_MOUNT_DIR=/root; + CACHE_FILE=/usr/local/nvidia/.cache; + LOCK_FILE=/root/tmp/cos_gpu_installer_lock; + LOCK_FILE_FD=20; + set +x; [INFO 2020-08-04 23:40:07 UTC] Checking if this is the only cos-gpu-installer that is running.; [INFO 2020-08-04 23:40:07 UTC] Running on COS build id 12871.1174.0; [INFO 2020-08-04 23:40:07 UTC] Checking if third party kernel modules can be installed; [INFO 2020-08-04 23:40:07 UTC] Checking cached version; [INFO 2020-08-04 23:40:07 UTC] Cache file /usr/local/nvidia/.cache not found.; [INFO 2020-08-04 23:40:07 UTC] Did not find cached version, building the drivers...; [INFO 2020-08-04 23:40:07 UTC] Downloading GPU installer ...; [INFO 2020-08-04 23:40:09 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/tesla/418.40.04/NVIDIA-Linux-x86_64-418.40.04.run; ls: cannot access '/build/usr/src/linux': No such file or directory; [INFO 2020-08-04 23:40:11 UTC] Kernel sources not found locally, downloading; [INFO 2020-08-04 23:40:11 UTC] Kernel source archive download URL: https://storage.googleapis.com/cos-tools/12871.1174.0/kernel-src.tar.gz. real	0m2.220s; user	0m0.183s; sys	0m0.338s; [INFO 2020-08-04 23:40:18 UTC] Setting up compilation environment; [INFO 2020-08-04 23:40:18 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain_env. real	0m0.126s; user	0m0.014s; sys	0m0.001s; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:3416,install,installed,3416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['install'],['installed']
Deployability,Database patches:,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6345:9,patch,patches,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6345,1,['patch'],['patches']
Deployability,Databases are hard (hotfix edition),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/740:20,hotfix,hotfix,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/740,1,['hotfix'],['hotfix']
Deployability,Db perf scripts tj update,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4034:19,update,update,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4034,1,['update'],['update']
Deployability,"Dear Cromwell Team,; I am trying to run a workflow written in WDL using Cromwell v.65. The workflow reports the following error in the stdout:; ```[2023-08-11 14:21:11,58] [error] SingleWorkflowRunnerActor received Failure message: Metadata for workflow <UUID> exists in database but cannot be served because row count of 3138431 exceeds configured limit of 1000000.; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow <UUID> exists in database but cannot be served because row count of 3138431 exceeds configured limit of 1000000.```; This is after having edited the `cromwell.conf` as suggested in [this thread](https://github.com/broadinstitute/cromwell/issues/2519). The configuration file used is as follows (edited to remove the main script):; ```; include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 300; runtime-attributes = """"""; Int cpu; Int memory_mb; String? lsf_queue; String? lsf_project; String? docker; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; module load tools/singularity/3.8.3; SINGULARITY_MOUNTS='<redacted>'; export SINGULARITY_CACHEDIR=$HOME/.singularity/cache; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock. export SINGULARITY_DOCKER_USERNAME=<redacted>; export SINGULARITY_DOCKER_PASSWORD=<redacted>. flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec docker://${docker} \; echo ""Sucessfully pulled ${docker}"". bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; singularity exec --containall $SINGULARITY_MOUNTS ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:703,configurat,configuration,703,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['configurat'],['configuration']
Deployability,"Dear Cromwell dev team,. In an attempt to use cromwell for running genomics pipeline(s) I encountered a few issues that prevent me from using these otherwise wonderful tools. Specific to my case, I would like to use Google Life Sciences v2beta in the region `europe-west4` due to data localisation needs. I mention this because of another recent Hellow World related issue #6462 where the demo did seem to work. 1) The documentation for the [Hello World](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) example is out of date. In `google.conf` it still lists the configuration for ""JES"" backend. 2) In the same tutorial ([Setting up PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2)), the instructions for which roles to assign to the GCP service account are outdated. 3) Once the user puzzles together which parts to replace, the execution is still failing (for me at least).; I run the following command ` java -Dconfig.file=cromwell.BROADexamples.v4.conf -jar cromwell-66.jar run hello.wdl -i hello.inputs`, which results in the following `Request contains an invalid argument.` error (abbreviated to the relevant section):; ```; [2021-08-13 10:44:39,31] [info] Running with database db.url = jdbc:hsqldb:mem: ...; ...; [2021-08-13 10:44:54,04] [info] Reference disks feature for PAPIv2 backend is not configured.; [2021-08-13 10:44:54,46] [info] Slf4jLogger started; [2021-08-13 10:44:54,73] [info] Workflow heartbeat configuration:; ...; [2021-08-13 10:44:55,42] [info] Running with 3 PAPI request workers; ...; [2021-08-13 10:44:55,79] [info] Unspecified type (Unspecified version) workflow a15c46b7-5f93-46d6-94a2-28f656914866 submitted; ...; [2021-08-13 10:44:56,46] [info] Request manager PAPIQueryManager created new PAPI request worker PAPIQueryWorker-58e6b395-916e-4ba4-965a-0ec8f1c0760d with batch interval of 3333 milliseconds; ...; [2021-08-13 10:44:56,67] [info] MaterializeWorkflowDescriptorActor [a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:76,pipeline,pipeline,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,2,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"Dear Cromwell dev team,. This is an enhancement suggestion. . When using the google backend for resources allocation, one can specify `gpuCount` and `gpuType` to request for specific resources. I am currently trying to design a task that optionally needs to access a GPU (function of input/parameters). I tried different approach to dynamically schedule GPUs, but `gpuCount` seems constrain to a non-null positive integer. https://github.com/broadinstitute/cromwell/blob/bfef756ca35b46570dff3fda57f77dd4b2b0d25c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L190 . https://github.com/broadinstitute/cromwell/blob/bfef756ca35b46570dff3fda57f77dd4b2b0d25c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/GpuValidation.scala#L28-L40. To allow for dynamic access to GPUs, I propose to extend `gpuCount` type to allow for a null value, and to check for a non-null value for resource allocation. https://github.com/broadinstitute/cromwell/blob/bfef756ca35b46570dff3fda57f77dd4b2b0d25c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L193. Please let me know if such a feature is not desired for any reason. . \* I tried accessing the Jira tracker but `doesn't have access to Jira on broadworkbench.atlassian.net.`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6679:537,pipeline,pipelines,537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679,8,"['Pipeline', 'pipeline']","['PipelinesApiRuntimeAttributes', 'pipelines']"
Deployability,"Dear WDL team,. I am using joint-discovery-gatk4.wdl for running jointcalling on 6000 plus samples using LSF as scheduler. I have lsf configuration file which was working with small set of samples. When I tried running Jointcalling on 6000+ samples using WDL, it is giving the below error.; ; **[2019-12-04 10:59:03,89] [ESC[38;5;220mwarnESC[0m] JobExecutionTokenDispenser - High load alert. Freeze token distribution.**; ; And I changed the concurrent-job-limit = 5000 in lsf configuration, now it is giving; ; **[2019-12-01 07:08:46,91] [info] WorkflowExecutionActor-b2c84d70-611d-4dad-bb18-78a6648e4113 [^[[38;5;2mb2c84d70^[[0m]: Starting JointGenotyping.ImportGVCFs (195 shards); [2019-12-01 07:15:32,84] [^[[38;5;1merror^[[0m] Failed to summarize metadata; java.sql.SQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded**. Could you please help me to proceed further. Thanks In Advance; Fazulur Rehaman",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5305:134,configurat,configuration,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5305,2,['configurat'],['configuration']
Deployability,"Dear cromwell developers, ; I assume that it is possbile to configure cromwell use the podman instead of docker as a backend via configuration file? (https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#configuration-in-detail) So far I have not manage to accomplish it starting by modifying the https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf backend section maybe someone can provide a template for thish? . Best, Eugene",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6660:129,configurat,configuration,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660,2,['configurat'],"['configuration', 'configuration-in-detail']"
Deployability,"Dear cromwell team, I was looking for means to re-submit failed workflow but did not find one, are there such options?. It is a valuable mode, possible use cases:. 1) I'm trying to debug a task at the end of the long workflow - currently every small change in a code force me to re-run the whole pipeline to see the results. It would be way easy to manually delete given `call` folder from cromwell working directory `cromwell-execution/piplene/wfID/call-xxx` and resubmit the workflow to server with something like that: `submit --resubmit wfID newimport.zip`. 2) my workflow failes on some task due to bug in the code – I’m editing the code and resubmitting it so workflow just continue where it stops. 3) I'd like to change parameters of some of the later steps of the pipeline but do not want to waste time on re-running the beginning of the pipeline (which usually has a computationally heavy tasks like mapping, calling, etc). I have an experience in using Snakemake and have to say that there those problems do not exist as far as I can always re-run desired rules of the pipeline changing Snakemake file or manually deleting given output. Best, Eugene",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6752:296,pipeline,pipeline,296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6752,4,['pipeline'],['pipeline']
Deployability,"Debrief here from a face-to-face w/ @geoffjentry :. So the config flag should be set _AND_ the call should be using docker to do the override. To be clear, the truth table here is . Using Docker on this call | Configuration Flag is set to true | Should reassign; --|--|--; T|T|T; T|F|F; F|T|F; F|F|F. Config flag should be `docker.override_umask_when_creating_directories`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-376196463:210,Configurat,Configuration,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-376196463,1,['Configurat'],['Configuration']
Deployability,"Debugging or profiling a running Cromwell has always been extremely painful due to the need to `apt install` all the tools manually. This is particularly infeasible when working with an unstable instance that is restarting and continually erasing those modifications (this was me yesterday). All of the image changes are in limited in scope to `installDebugFacilities` function. The rest of the diff is rationalizing our build system. Previously, we had `isSnapshot`, `isRelease`, and ""neither of those"". The logic was confusing around which took precedence. I replaced the bag o' booleans with a type system representing the types. The ""neither of those"" now has a name `Standard`. And finally I augmented it with `Debug`. No external interface is changing, as mentioned in the Markdown running with no parameters still gets you a `Snapshot` build. There is a stowaway enhancement to add the `develop` tag to merged commits. We used to set this tag <a long time ago> and I meant to fix it in https://github.com/broadinstitute/cromwell/pull/7362, but forgot. Product of running all of the commands documented in Markdown:. ![Screenshot 2024-05-01 at 17 23 16](https://github.com/broadinstitute/cromwell/assets/1087943/79f9e8c0-f33f-4e75-a686-067830945584)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7417:100,install,install,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7417,2,['install'],"['install', 'installDebugFacilities']"
Deployability,Debugging updates for stalled tests BT-140,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6202:10,update,updates,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6202,1,['update'],['updates']
Deployability,Decide the future of PAPI upgrade tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4513:26,upgrade,upgrade,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4513,1,['upgrade'],['upgrade']
Deployability,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3999:1841,configurat,configuration,1841,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999,1,['configurat'],['configuration']
Deployability,Definitely a `Hotfix Candidate`!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3037#issuecomment-350774716:14,Hotfix,Hotfix,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3037#issuecomment-350774716,1,['Hotfix'],['Hotfix']
Deployability,Delocalization bug for function basename() introduced between Cromwell release 68-8e12ab5 and 69,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6507:71,release,release,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6507,1,['release'],['release']
Deployability,"Dependency, Test, and CI updates",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4599:25,update,updates,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4599,1,['update'],['updates']
Deployability,"Depending on the issue, a cloud SQL issue could cause anything from a failed migration and delayed release to data loss and restore from backup. (The first is extremely more likely.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328538475:99,release,release,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328538475,1,['release'],['release']
Deployability,Depends on #1114. Update the WorkflowStore to resubmit non-complete jobs as restarts when the system comes up.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1118:18,Update,Update,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1118,1,['Update'],['Update']
Deployability,"Deployed in the FC dev repo, PR 349.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2073#issuecomment-287393971:0,Deploy,Deployed,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2073#issuecomment-287393971,1,['Deploy'],['Deployed']
Deployability,"Description:; * Adds a statistics recorder into the WriteMetadataActor to count rows being sent per workflow. If the counter goes about a given limit, we get an alert back which the write actor converts into a log message. . Food for reviewers' thoughts:. * Does the set of configuration options make sense?; * And what might be sensible default values?; * I'm not a huge fan of how subworkflows' parents are detected here. Is there a more direct way to find out a parent?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6641:274,configurat,configuration,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6641,1,['configurat'],['configuration']
Deployability,Design a way for Cromwell to pass language specific configuration to the various bindings.; The use case for this is read_*** bytes limitation that are different for different WDL functions which Cromwell has no knowledge of anymore.; CWL also requires the same type of limitation.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2611:52,configurat,configuration,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2611,1,['configurat'],['configuration']
Deployability,"Despite the Github eliding, `StandardCacheHitCopyingActor.scala` and `PipelinesApiBackendCacheHitCopyingActorSpec` do warrant review. 🙂",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-635284404:70,Pipeline,PipelinesApiBackendCacheHitCopyingActorSpec,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-635284404,1,['Pipeline'],['PipelinesApiBackendCacheHitCopyingActorSpec']
Deployability,Develop - Update detritus value data type closes #1862,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1869:10,Update,Update,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1869,1,['Update'],['Update']
Deployability,"Did I submit the PR to the wrong repo? Is the ""official"" released AWS proxy image built from somewhere other than the `supportedBackends/aws/src/main/resources/ecs-proxy` directory in the cromwell repo?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4335#issuecomment-434253780:57,release,released,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4335#issuecomment-434253780,1,['release'],['released']
Deployability,"Did anyone ever figure out why Cromwell would fail with an `EntityStreamSizeException`? When and how was this issue with the CI fixed?. I managed to make a similar `EntityStreamSizeException` happen using Cromwell release 66 in `run` mode, without providing a configuration, with a workflow input that specified HTTPS URLs for large files as file inputs. Is this exception ever supposed to happen? And, if so, when and why?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4298#issuecomment-892074494:214,release,release,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4298#issuecomment-892074494,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,"Directory structure:. ```; WDLTesting; -src; --wdl; ---Workflow.wdl; ---WriteTask.wdl; ---Child; ----ChildWF.wdl; ----ChildTask.wdl; ```. Under draft 2 (which is to say, no version specified), Workflow.wdl has the following import statements:; ```; import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write; import ""WDLTesting/src/wdl/Child/ChildWF.wdl"" as Child; ```; ChildWF.wdl has the following import statement:; `import ""WDLTesting/src/wdl/Child/ChildTask.wdl"" as Child`. This works correctly. it works letting things be access from the file system, and it works if we zip up WDLTesting and pass it to Cromwell as --imports. Under version development, this doesn't work. Workflow.wdl's imports still work, but ChildWF.wdl's do not. I must trim it down to ; `import ""Child/ChildTask.wdl"" as Child`; Before it will work. This completely breaks our pipeline, especially since workflows calling files up and down the tree, and even in other projects that start at the same level as ours. Why was this horrible breaking change made, and how do we specify an import that works with respect to the --imports .zip?. Thank you",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6441:845,pipeline,pipeline,845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6441,1,['pipeline'],['pipeline']
Deployability,Disable and update docs regarding Alibaba testing BA-6345,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5466:12,update,update,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5466,2,['update'],['update']
Deployability,"Discussed at standup 2019-02-04. Right now, bcbio uses:; - pinned CWL (checked in); - floating docker; - floating data (not controlled by us). As such, it breaks pretty frequently due to external factors, negating its usefulness as a regression test. For this ticket, update our bcbio configuration to be two separate jobs - one fully floating to test compatibility at the cutting edge, one fully pinned to serve as a stable regression test. Floating job:; - pull CWL from Brad's repo; - floating docker [same as now]; - floating data [same as now]. Pinned job:; - pinned CWL [same as now]; - our own stable copy of the data; - our own stable copy of the dockers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4613:268,update,update,268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4613,2,"['configurat', 'update']","['configuration', 'update']"
Deployability,"Discussed with @katevoss - as we don't have our long range documentation plan hashed out yet for now we'll be:; - Making a separate doc (name tbd) with two initial bits. One is the content from @delocalizer describing a user based setup. We'll make it clear that this is their set up and not ours, both from the indemnity angle that @cjllanwarne was concerned about but more importantly because it helps to demonstrate the vibrant community which is building around Cromwell; - I'll add a second section describing Firecloud's security model; - That doc will be linked from the README; - We'll set up a blog post on the main site describing security/auth options in Cromwell and directly referencing this doc. Readers/users will be encouraged to ask questions, provide alternate suggestions, etc. The security doc (for lack of a better word atm) will be more of a living doc. @delocalizer ... I don't want to make extra work for you here. If you wanted to update this PR to reflect the first and third bullet points great, otherwise I can pick up this PR and do first and third while i'm doing the second. If you're going with the former hold off until I confirm the name of the file :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531:956,update,update,956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531,2,['update'],['update']
Deployability,Do not return 40x for `releaseHold` on `Running` workflow,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4983:23,release,releaseHold,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4983,1,['release'],['releaseHold']
Deployability,Do we want this for the release ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/593#issuecomment-202949745:24,release,release,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/593#issuecomment-202949745,1,['release'],['release']
Deployability,Do we want to keep this? (from Checklist for FC to accept Cromwell doc); ```; Run long-running submissions immediately before a dev deploy and observe that they pass after the new Cromwell version picks them up and completes them.; ```; This was added after the Cromwell 34 release meltdown. There were some changes in it that caused all the running submissions to fail.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-490124790:132,deploy,deploy,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-490124790,2,"['deploy', 'release']","['deploy', 'release']"
Deployability,"Doc updates pre-30 release, closes #2888",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2982:4,update,updates,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2982,2,"['release', 'update']","['release', 'updates']"
Deployability,Docker integration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/31:7,integrat,integration,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/31,1,['integrat'],['integration']
Deployability,Docker private registry configuration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3574:24,configurat,configuration,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3574,1,['configurat'],['configuration']
Deployability,Dockerize release WDL,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4869:10,release,release,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4869,1,['release'],['release']
Deployability,"Docs:; - Fixed a bunch of broken links; - I think the Scala Steward updates gave us a new version of doc generation that is more strict; - There are more broken links than just these, did not attempt to be comprehensive; - IntelliJ's markdown validation is helpful:. ![Screen Shot 2020-08-19 at 12 15 43 PM](https://user-images.githubusercontent.com/1087943/90661978-d2613d00-e215-11ea-8c1d-5ae4c842213e.png). Error messages:; - Attempted to make them more concise and consistent; - Sometimes we didn't make it obvious that a limit is configurable; - Did not attempt to be comprehensive",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5779:68,update,updates,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5779,1,['update'],['updates']
Deployability,Document Docker image deployment of Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4682:22,deploy,deployment,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4682,1,['deploy'],['deployment']
Deployability,Document per-workflow backends for Release,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/629:35,Release,Release,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/629,1,['Release'],['Release']
Deployability,Document that we now release to CAAS as well as Terra [BA-5902],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5124:21,release,release,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5124,1,['release'],['release']
Deployability,"Documentation only change. I've tested this on our local instance with cromwell-88.; Defined here: https://github.com/broadinstitute/cromwell/blob/c8d36d5c7df25f79e87ea5f6373ab5ee1556a6fb/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpBatchWorkflowPaths.scala#L20. (Compared to the same value for the PipelinesAPI): https://github.com/broadinstitute/cromwell/blob/c8d36d5c7df25f79e87ea5f6373ab5ee1556a6fb/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiWorkflowPaths.scala#L19. In reality the best thing would be to duplicate the table, but the other options seem similar.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7435:340,Pipeline,PipelinesAPI,340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7435,4,"['Pipeline', 'pipeline']","['PipelinesAPI', 'PipelinesApiWorkflowPaths', 'pipelines']"
Deployability,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:51,update,updates,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,9,"['Release', 'release', 'update']","['Releases', 'release', 'releases', 'updates']"
Deployability,Don't accept labels update for archived workflows [BW-559],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6305:20,update,update,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6305,1,['update'],['update']
Deployability,Don't forget to update `cromiam.yaml`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3945#issuecomment-409299130:16,update,update,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3945#issuecomment-409299130,1,['update'],['update']
Deployability,Double Quoted file inputs cause PipelineAPI workflows to run forever. Closes #2178,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2251:32,Pipeline,PipelineAPI,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2251,1,['Pipeline'],['PipelineAPI']
Deployability,Doug will soon be changing the SAM resource type for workflows from `workflow` to something like `workflow-collection`. CromIAM will need to be updated [here](https://github.com/broadinstitute/CromIAM/blob/a65b4569c40b10bd3477e0394f2c55dd1ddd234a/src/main/scala/cromiam/sam/SamClient.scala#L47). This change must be synchronized with the change to SAM in the dev environments. NOTE: Doug has also mentioned that the existing workflow resources will not be migrated in SAM during this change.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2629:144,update,updated,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2629,1,['update'],['updated']
Deployability,"Draft PR TODOs:; - [x] Add some unit tests; - [X] ImportResolver; - [X] GithubAuthVendingSupport; - [x] Find a better way to do the await result; - [x] Remove from standard config and allow the system to work in the absence of a github auth vending service OR allow it to be configured OFF in config and make that the default 🤔 ; - [x] Move the auth vending message and helper classes out of `impl`. To test this - ; - Go to github and create a limited scope personal access token; - Scope to a single repo; - Add readonly access for Content, and no other permissions; - Update the config of a running instance with the raw token under `GithubAuthVending.config.access-token`. NB don't include the `Bearer` before the token in the config file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7365:571,Update,Update,571,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7365,1,['Update'],['Update']
Deployability,Draft of blog post is here: https://gatkforums.broadinstitute.org/dsde/discussion/10143/cromwell-29-released-making-way-for-big-changes/p1?new=1. Branch of changelog in github is here: https://github.com/broadinstitute/cromwell/tree/kv_crom29_relnotes,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2530#issuecomment-321527666:100,release,released-making-way-for-big-changes,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2530#issuecomment-321527666,1,['release'],['released-making-way-for-big-changes']
Deployability,"Due to some updates that got in cromwell version 49 the regex that creates the relative outputs was evaluated differently (correctly, to be precise). This made it so that any directory structure created by the user is lost. This broke all the testing on biowdl.; I have updated the test cases to notice such a regression in the future. I also fixed the regex to behave like it is described in the documentation.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5437:12,update,updates,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437,2,['update'],"['updated', 'updates']"
Deployability,"Dump the dependency tree before and after upgrade: ; ```; sbt coursierDependencyTree > coursier_dependency_tree_$(date +%s).txt; ```. look for all the `:netty-codec:`s, with a dash of `sed` and `sort -u` to squash duplicates:. before:; ```; $ grep ':netty-codec:' coursier_dependency_tree_1586897217.txt | sed -E 's/.*:(.*)/\1/' | sort -u; 4.1.30.Final -> 4.1.33.Final; 4.1.32.Final -> 4.1.33.Final; 4.1.33.Final; $; ```. after:; ```; $ grep ':netty-codec:' coursier_dependency_tree_1586898071.txt | sed -E 's/.*:(.*)/\1/' | sort -u; 4.1.32.Final -> 4.1.46.Final; 4.1.46.Final; $; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5483:42,upgrade,upgrade,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5483,1,['upgrade'],['upgrade']
Deployability,"During the 30 release, the previous code repositories should be archived. For each repository:; - Create a new commit on the default branch linking future visitors to cromwell; - Ensure all PRs and Issues are closed/migrated as they will be read-only for all users after archiving; - [Archive](https://help.github.com/articles/archiving-a-github-repository/) the repository making it read-only. Old repositories:; - https://github.com/broadinstitute/lenthall; - https://github.com/broadinstitute/wdl4s; - https://github.com/broadinstitute/wdltool; - https://github.com/broadinstitute/centaur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2980:14,release,release,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2980,1,['release'],['release']
Deployability,"During the doc-a-thon, the version was manually added to [cromwell.yaml](https://github.com/broadinstitute/cromwell/blob/c3f7e2de32fa9129a15999fe04dde287388302d2/engine/src/main/resources/swagger/cromwell.yaml#L8). However the release WDL was not updated to account for this change. Either this version should be [taken back out](https://github.com/broadinstitute/cromwell/blob/2c83f323f202b767a67b0f3058578a797d97082d/engine/src/main/resources/swagger/cromwell.yaml#L11) so it does not go out of sync with the code, or the version should be updated to the latest version by the [release WDL](https://github.com/broadinstitute/cromwell/blob/c3f7e2de32fa9129a15999fe04dde287388302d2/release/release_workflow.wdl#L135-L140).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2930:227,release,release,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2930,5,"['release', 'update']","['release', 'updated']"
Deployability,"During the testing hackathon, we discovered a number of problems caused by the eventual consistency of the metadata service. One specific case of this is the granularity of the events. . On one side, we have a publisher who has a whole collection of events that they would like to push. They push them one event at a time to the MD service. Because even things like array elements are pushed one update at a time because of MD format, we run into the situation where a consumer can see half of an array. Taken to the extreme, we could push every char of a string as a separate event. The fundamental problem with these partial updates is that a downstream consumer can not tell if an update is complete. Do they wait? How long? Can they check if the data is done?. While this touches on a larger problem in distributed computing, I think we are shooting ourselves in the foot by making every piece of a single update an async, isolated event. Taken to the extreme, we could push every char of a string as a separate event. The proposal is to extend the PutMetadataAction to take in a Seq/Varargs of MetadataEvents with the contract that these will be made available atomically (e.g. in a single Slick transaction for our implementation). Then in places where we basically unrolling a bundle of events to publish, we should use this API (e.g. WorkflowExecutionActor) to do that atomically. . In theory, this should also help with the scalability as the MD service can persist things with batchinserts in single transaction. For larger workflows, currently this would be hundreds or thousands of transactions.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/930:396,update,update,396,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/930,4,['update'],"['update', 'updates']"
Deployability,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3657:46,configurat,configuration,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657,2,['configurat'],['configuration']
Deployability,Either update it or remove it:; http://broadinstitute.github.io/cromwell/,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1687:7,update,update,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1687,1,['update'],['update']
Deployability,Enable configuration of the temporary directory.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2657:7,configurat,configuration,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2657,1,['configurat'],['configuration']
Deployability,Enabled GCP batch integration tests by uncommenting out step in GHA.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7199:18,integrat,integration,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7199,1,['integrat'],['integration']
Deployability,Engine/PAPI upgrade testing script hotfix support.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4719:12,upgrade,upgrade,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4719,2,"['hotfix', 'upgrade']","['hotfix', 'upgrade']"
Deployability,Engine/PAPI upgrade testing script hotfix support. [37],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4720:12,upgrade,upgrade,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4720,2,"['hotfix', 'upgrade']","['hotfix', 'upgrade']"
Deployability,Engine/PAPI upgrade testing script hotfix support. [38],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4721:12,upgrade,upgrade,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4721,2,"['hotfix', 'upgrade']","['hotfix', 'upgrade']"
Deployability,Enhance release WDL to create firecloud-develop PR,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4779:8,release,release,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4779,1,['release'],['release']
Deployability,"Ensure GCS file systems use custom configuration.; When an exception/timeout occurs during asyncHashing, report it as a failure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2512:35,configurat,configuration,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2512,1,['configurat'],['configuration']
Deployability,Equivalent update applied.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5621#issuecomment-670986762:11,update,update,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5621#issuecomment-670986762,1,['update'],['update']
Deployability,Er whoops totally forgot to update the docs,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/114#issuecomment-124223761:28,update,update,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/114#issuecomment-124223761,1,['update'],['update']
Deployability,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:248,configurat,configuration,248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578,2,['configurat'],['configuration']
Deployability,"Error message: A timeout occurred waiting for a future to complete. Queried 100 times, sleeping 100 milliseconds between each query. tc: ServicesStore should not deadlock. https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/566/. Update 10/22; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/708/. Update 10/28:; https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/831/. Update 11/03:; https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/1003/. Update 11/06:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1076/. Update 11/09:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1166/. Further:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1337; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1422; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1445; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1489; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1525; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1590",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328:253,Update,Update,253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328,5,['Update'],['Update']
Deployability,Error running the GATK WDL best practices pipeline,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2064:42,pipeline,pipeline,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2064,1,['pipeline'],['pipeline']
Deployability,"Error... not reading the whole file probably will not produce the right behavior in the pipeline being run. > On Apr 15, 2017, at 11:41 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364:88,pipeline,pipeline,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364,2,['pipeline'],['pipeline']
Deployability,Error: Could not load UVM kernel module. Is nvidia-modprobe installed?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4935:60,install,installed,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935,1,['install'],['installed']
Deployability,"Even so, I'd still update the Lenthall dep if it's updated (so we can easily see it's a SNAPSHOT version)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254530925:19,update,update,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254530925,2,['update'],"['update', 'updated']"
Deployability,Exception: Is a directory; Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.FileDispatcherImpl.read0(Native Method); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.IOUtil.read(IOUtil.java:197); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.$anonfun$hash$3(ConfigHashingStrategy.scala:82); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.$anonfun$tryWithResource$1(TryWithResource.scala:16); Mar 09 00:55:25 web start-cromwell.sh[110916]: at scala.util.Try$.apply(Try.scala:209); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3383:1648,update,updateDigest,1648,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383,1,['update'],['updateDigest']
Deployability,"Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDisp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:1480,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,ExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:2655,pipeline,pipeline,2655,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,Excluded #5797 for the same reason #5775 was excluded in #5798: major version upgrade to a component we don't much care about that doesn't have tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5799:78,upgrade,upgrade,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5799,1,['upgrade'],['upgrade']
Deployability,Exists to run the horicromtal upgrade tests - which only occur on PRs. But this isn't ready for review yet.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4882:30,upgrade,upgrade,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4882,1,['upgrade'],['upgrade']
Deployability,Explicitly install pip for Travis [BT-451],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6569:11,install,install,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6569,1,['install'],['install']
Deployability,"Extending mcovarr's work in #6366 . Big shoutout to mcovarr!!!. [Per @mbookman]; This pull request is an initial update to address:. CROM-6718: FR: Add flag for minimizing chance of GCP cross-region network egress charges being incurred. This PR specifically focuses on the risks of egress charges incurred due to call caching. The framing of the approach here, which is a bit broader than originally noted in CROM-6718, is:; Make call caching location-aware, prioritizing copies that minimize egress charges.; Add a workflow option enabling control of what egress charges can be incurred for call cache copying.; The new workflow option would be:. call_cache_egress: [none, continental, global]. where the values affect whether call cache copies can incur egress charges:; none: only within-region copies are allowed, which generate no egress charges; continental: within content copies are allowed; within-content copies have reduced costs, such as $0.01 / GB in the US; global: copies across all regions are allowed. Cross-content egress charges can be much higher (ranging from $0.08 / GB up to $0.23 / GB). ### CURRENT STATUS OF PR:; With the changes in this PR, Cromwell successfully checks the location of the source and destination file to be copied, compares the location, and makes a decision of whether or not it should be copied based on the call_cache_egress option. If it should be copied, the files are copied as normal. If it should not be copied, the cache attempt fails and the workflow runs instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6369:113,update,update,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6369,2,['update'],['update']
Deployability,"Extracting name of the file (usually - in order to save it with another extension or suffix) is so common in pipelines, that I think it should be part of wdl standard functions",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2264:109,pipeline,pipelines,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2264,1,['pipeline'],['pipelines']
Deployability,"FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:116); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-05-25 12:18:24,94] [info] WorkflowManagerActor WorkflowActor-e52409b4-c85a-4285-9453-f47c6b0ae86c is in a terminal state: WorkflowFailedState; [2017-05-25 12:18:24,94] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c#-297741123] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-e52409b4-c85a-4285-9453-f47c6b0ae86c#772660809] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-05-25 12:18:26,88] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; Workflow e52409b4-c85a-4285-9453-f47c6b0ae86c transitioned to state Failed. I recognize the brackets are the problem, but the tutorial doesn't seem to offer suggestions on removing those. When I did remove them it seemed the key values in the json were the problem. Any help is greatly appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2296:3370,configurat,configuration,3370,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2296,1,['configurat'],['configuration']
Deployability,"FYI @katevoss we are seeing this in FireCloud (Alpha environment, ""special snowflake Cromwell 26 hotfix 2"" aka 70741da6). Not often: on the order of 1 out of 10,000.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-298697710:97,hotfix,hotfix,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-298697710,1,['hotfix'],['hotfix']
Deployability,FYI The conformance test that's failing can't pass until the `OutputManipulator` is also updated - I'm doing that today,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3139#issuecomment-357999800:89,update,updated,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3139#issuecomment-357999800,1,['update'],['updated']
Deployability,"FYI for reviewers: need to retrofit `/v1` to still fill in `Some(""WDL"")` instead of `None` to fix tests, and will update docs to suggest ""draft-2"" as a workflow type version.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2578#issuecomment-325698953:114,update,update,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2578#issuecomment-325698953,1,['update'],['update']
Deployability,"FYI out of curiosity I'm going to also run the full suite of our centaur tests ([removing the `-i includes`](https://github.com/broadinstitute/cromwell/blob/44/src/ci/bin/testCentaurBcs.sh#L19-L20)) to see if additional tests pass with our credentials. If you can see our test results on the Alibaba servers you may see some failures, but as long as the existing tests pass I'll be satisfied. Separately, an entry should be added to the CHANGELOG.md with a short line pointing users to the updated documentation. ([Example](https://github.com/broadinstitute/cromwell/blob/44/CHANGELOG.md#stackdriver-instrumentation)) I've been holding off suggesting this change because that file changes _a lot_ and is subject to frequent merge conflicts. Now that this PR is close enough to merging I think it's time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512249469:490,update,updated,490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512249469,1,['update'],['updated']
Deployability,"FYI there's a hidden watermark at the top of the file that one can use in PRs to tell if the RESTAPI.md was manually or automatically updated. Example: https://github.com/broadinstitute/cromwell/blame/31/docs/api/RESTAPI.md#L1-L8. Also if one doesn't have a dev environment locally they can still use any public sbt docker. It will take a while as it downloads ~the entire internet~ all of the un-cached cromwell dependencies, but something like this will work:. ```shell; docker \; run \; --rm \; -v $PWD:$PWD \; -w $PWD \; hseeberger/scala-sbt \; sbt generateRestApiDocs; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389043:134,update,updated,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389043,1,['update'],['updated']
Deployability,"FYI- the failing test suite ""centaurJes"" is due to a know limitation in our test setup, but the other three look good, including the ""centaurLocal"" tests. Again, without a dockerized ""centaurTes"" we'll certainly try to avoid issues, but there won't be any guarantees that upgrades to the standard backend API don't break TES. Also, the 87 commits will need to be rebased and squashed correctly to a minimal set, on the order of 1 commit, but otherwise let us know when you're ready for re-review. Let us know if you have more questions or if we can provide any other assistance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897:272,upgrade,upgrades,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897,1,['upgrade'],['upgrades']
Deployability,"F_PREFIX,; in_cores=CORES,; in_disk=DISK,; in_mem=MEM; }. output {; File sample = reads_extraction_and_merging.fastq_file; File genotype = genome_inference.vcf_file; }; }. task reads_extraction_and_merging {; input {; String in_container_pangenie; File in_forward_fastq; File in_reverse_fastq; String in_label; Int in_cores; Int in_disk; Int in_mem; }; command <<<; cat ~{in_forward_fastq} ~{in_reverse_fastq} | pigz -dcp ~{in_cores} > ~{in_label}.fastq; >>>; output {; File fastq_file = ""~{in_label}.fastq""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; }; }. task genome_inference {; input {; String in_container_pangenie; File in_reference_genome; File in_pangenome_vcf; String in_executable; File in_fastq_file; String prefix_vcf; Int in_cores; Int in_disk; Int in_mem; }; command <<<; echo ""vcf: ~{in_pangenome_vcf}"" > /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""reference: ~{in_reference_genome}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo $'reads:\n sample: ~{in_fastq_file}' >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""pangenie: ~{in_executable}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""outdir: /app/pangenie"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; cd /app/pangenie/pipelines/run-from-callset; snakemake --cores ~{in_cores}; >>>; output {; File vcf_file = ""~{prefix_vcf}.vcf""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; preemptible: 1 # can be useful for tools which execute sequential steps in a pipeline generating intermediate outputs; }; }; ```; **_Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL:_**; ![Screenshot from 2022-12-09 10-52-16](https://user-images.githubusercontent.com/98895614/206773588-2e8dbf89-03a9-4021-9495-42f2bc0b801d.png). Please help me out on how to set",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966:3112,pipeline,pipelines,3112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966,1,['pipeline'],['pipelines']
Deployability,Fail on missing outputs 34 hotfix edition.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4149:27,hotfix,hotfix,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4149,1,['hotfix'],['hotfix']
Deployability,"False alarm, this was because I was because I was running the upgraded jar in a different directory!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963#issuecomment-348553915:62,upgrade,upgraded,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963#issuecomment-348553915,1,['upgrade'],['upgraded']
Deployability,"Feature request from @gsaksena:; If a preemptible VM is killed because it hit the time-out (of 24 hours), then it should automatically be upgraded to a full price VM (non-preemptible). Currently it would retry the number of times it would try, per the preemptible runtime attribute. So if preemptible is 3, then it would take 3 days to upgrade to a non-preemptible VM.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168:138,upgrade,upgraded,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168,2,['upgrade'],"['upgrade', 'upgraded']"
Deployability,"Feedback from today's workshop: `womtool validate` uselessly prints a few newlines on successful exit. Various workshop participants thought:; 1. It should exit with no output to harmonize with Unix CLI tool conventions; 2. It should confirm success in a way that is obvious to users who are e.g. biologists first, programmers second. Either would be an improvement over the current state. cc @rmeffan @ndbolliger ; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4040:1125,configurat,configuration,1125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4040,1,['configurat'],['configuration']
Deployability,Fetch new apt keys before installing jq [BW-628],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6302:26,install,installing,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6302,1,['install'],['installing']
Deployability,Figured out that I need to update the contents of `gs://centaur-cwl-conformance` to match the latest commit,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4823#issuecomment-484288758:27,update,update,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4823#issuecomment-484288758,1,['update'],['update']
Deployability,"Figuring that out is what this ticket is for :); Right now it involves at least having a cromwell available, a github access token, running the release WDL, monitoring that everything goes well and that the WDL succeeds. This could be automated using jenkins for example.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328:144,release,release,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328,1,['release'],['release']
Deployability,"Filed https://broadinstitute.atlassian.net/browse/DSPTO-710. After a discussion with our devops representatives in channel deploy-horicromtal, I cut the autoscaling part for now because managed instance groups don't work in our env for reasons I don't fully understand (and can certainly be revisited later!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4798#issuecomment-492413869:123,deploy,deploy-horicromtal,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4798#issuecomment-492413869,1,['deploy'],['deploy-horicromtal']
Deployability,"Finally I figure out one solution, but it is a little bit ugly and still look for an elegant way:. - Global variables WDL as below:. ```; workflow global {; }. task init {; command { }; output {; String version = ""v1.0""; String reference = ""hg19"". }; }; ```. - Pipeline WDL as below:; ```; import ""global.wdl"" as global. workflow pipeline {; # Global variables; call global.init; String version = init.version; String reference = init.reference; # Pipeline variables; String sample_id = ""Sample_001""; call snp { input: version = version, reference = reference, sample_id = sample_id }; }. task snp {; String version; String reference; String sample_id; command { echo ""SNP_${version} for ${sample_id} on ${reference}!"" }. output { String out = read_string(stdout()) }; }. ```; The final result is:; ```; [2018-11-21 18:23:14,32] [info] BackgroundConfigAsyncJobExecutionActor [a225847apipeline.snp:NA:1]: echo ""SNP_v1.0 for Sample_001 on hg19!""; ```. And you can see global variables are passed to the pipeline WDL while there are some workaround such as empty global workflow and helper task of init.; Is there any other solution?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4416#issuecomment-440767243:261,Pipeline,Pipeline,261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4416#issuecomment-440767243,4,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline']"
Deployability,"Find the original post [here](http://gatkforums.broadinstitute.org/wdl/discussion/7690/running-in-server-mode-jobs-that-have-localization-error-become-immortal#latest). ---. **User Report**. Running cromwell in server mode, with default configuration in each case, I can reproduce the following behaviour in 0.18, 0.19 and 0.19_hotfix (HEAD):. Submit a workflow that has non-existent file as input to a task, e.g.:. ```; task BillyBob {; File bbInput; command { echo ""done"" }; }; workflow badLocalization {; call BillyBob { input: bbInput=""/foo/bar/baz"" }; }; ```. The server log shows ""Failures during localization"" error (below) - as expected initially, I guess - but then _repeats_ the error every 30 seconds or so, forever, and hitting the API `<workflowId>/status` endpoint shows the job in a ""Running"" state, forever. I would expect this error to cause the task, and then the workflow, to die. example of a single block of the server log error: . ```; 2016-05-27 11:08:57,269 cromwell-system-akka.actor.default-dispatcher-5 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localize$1(SharedFileSystem.scala:242) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(SharedFileSystem.scala:264) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue(SharedFileSystem.scala:271) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:237,configurat,configuration,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,1,['configurat'],['configuration']
Deployability,First engine upgrade CI test.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4132:13,upgrade,upgrade,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132,1,['upgrade'],['upgrade']
Deployability,"Fix $Home variable, hotfix version",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3429:20,hotfix,hotfix,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3429,1,['hotfix'],['hotfix']
Deployability,Fix SingleWorkflowRunnerActorSpec test HOTFIX,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/967:39,HOTFIX,HOTFIX,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/967,1,['HOTFIX'],['HOTFIX']
Deployability,Fix TES backend to load preemptible setting from configuration [BA-6004],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5270:49,configurat,configuration,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270,1,['configurat'],['configuration']
Deployability,Fix TES backend to load preemptible setting from configuration [BA-6004] [CI clone],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5314:49,configurat,configuration,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5314,1,['configurat'],['configuration']
Deployability,Fix and test artifact deployment.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1661:22,deploy,deployment,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1661,1,['deploy'],['deployment']
Deployability,"Fix broken doc links, update error messages [no JIRA]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5779:22,update,update,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5779,1,['update'],['update']
Deployability,Fix hotfix PR builds [BA-6178],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5369:4,hotfix,hotfix,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5369,1,['hotfix'],['hotfix']
Deployability,Fix release WDL [BW-648],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6315:4,release,release,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6315,1,['release'],['release']
Deployability,Fix release WDL for subprojected wdl4s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2472:4,release,release,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2472,1,['release'],['release']
Deployability,Fix release wdl and custom label summarizing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2298:4,release,release,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2298,1,['release'],['release']
Deployability,Fix submit-docker to point to the right script in example configuration [BA-5689],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5015:58,configurat,configuration,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5015,1,['configurat'],['configuration']
Deployability,Fix the horicromtal engine upgrade test.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4883:27,upgrade,upgrade,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4883,1,['upgrade'],['upgrade']
Deployability,Fix wdltool release,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2393:12,release,release,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2393,1,['release'],['release']
Deployability,"Fix/update for [WX-1210](https://broadworkbench.atlassian.net/browse/WX-1210). Turns out that the `head_commit` attribute is not available on `pull_request` actions, which is why the JIRA ID check kept failing. I'm opting to use `github.event.pull_request.title` which is accessible on `pull_request`. [WX-1210]: https://broadworkbench.atlassian.net/browse/WX-1210?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7184:4,update,update,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7184,1,['update'],['update']
Deployability,Fixed in a point update. Apologies for the inconvenience. https://github.com/broadinstitute/cromwell/releases/tag/53.1,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-686246049:17,update,update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-686246049,2,"['release', 'update']","['releases', 'update']"
Deployability,"Fixed in develop. Question for @katevoss - is this worthy of a hotfix (and if so, should we wait to see if any more WOM issues bubble up in the next few days and patch them all in one go)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349472979:63,hotfix,hotfix,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349472979,2,"['hotfix', 'patch']","['hotfix', 'patch']"
Deployability,Fixed in develop. The fix will appear in 30.2 and the upcoming 31 releases.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3190#issuecomment-360554624:66,release,releases,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190#issuecomment-360554624,1,['release'],['releases']
Deployability,"Fixes #3039 . As far as I can tell, expressions that lookup subworkflow outputs sometimes cause an error message to be written which is subsequently ignored. Something in Cromwell 30 caused building that error message to fail because it can no longer find the sub-workflow line to point to in the error message. This hotfix PR lets us build a partial error message with no ""pointing to the error"" line. I think this is ok since we're ignoring the resulting message anyway for subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3048:317,hotfix,hotfix,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048,1,['hotfix'],['hotfix']
Deployability,"Fixes a minor oversight that became apparent on attempting to deploy `cromwell-drs-localizer` on the new weekly cycle. To view the images that `dockerBuildAndPush` will make, issue this command. `perf` is designated [""build but don't push""](https://github.com/broadinstitute/cromwell/blob/95c550e0defdf653ba25f0528648e77b46148cae/build.sbt#L367) which is why it is not present in Docker Hub. (It may be completely obsolete at this point, but that's a story for a different day.); ```; > show docker::imageNames; [info] server / docker / imageNames; [info] 	ArrayBuffer(broadinstitute/cromwell:85, broadinstitute/cromwell:85-443a6fc); [info] cromiam / docker / imageNames; [info] 	ArrayBuffer(broadinstitute/cromiam:85, broadinstitute/cromiam:85-443a6fc); [info] perf / docker / imageNames; [info] 	ArrayBuffer(broadinstitute/perf:85, broadinstitute/perf:85-443a6fc); [info] womtool / docker / imageNames; [info] 	ArrayBuffer(broadinstitute/womtool:85, broadinstitute/womtool:85-443a6fc); [info] cromwell-drs-localizer / docker / imageNames; [info] 	ArrayBuffer(broadinstitute/cromwell-drs-localizer:85, broadinstitute/cromwell-drs-localizer:85-443a6fc); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6919:62,deploy,deploy,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6919,1,['deploy'],['deploy']
Deployability,Fixes a problem where we'd sometimes paste the entire contents of our changelog into the changelog of a particular release. Also tweaks release process doc to reflect current SOP.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6799:115,release,release,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6799,2,['release'],['release']
Deployability,"Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2322351550:54,release,release,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2322351550,1,['release'],['release']
Deployability,Fixes to `SlickDataAccessSpec` plus some other patches.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/941:47,patch,patches,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/941,1,['patch'],['patches']
Deployability,"Fixes to `SlickDataAccessSpec` plus some other patches.; `SlickDataAccessSpec` should talk to the currently tested `SlickDatabase with DataAccess`, NOT the `service` layer, that in turn talks to the global singleton.; The `root` project was not aggregating `database`.; The `root` project with `Main` only actually depends on `engine` and `core` tests.; More tests fixed that were waiting for the wrong message.; Changed waiting for `start` to `Starting`.; Changed waiting for `call.name` to `call.name:NA:1`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/941:47,patch,patches,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/941,1,['patch'],['patches']
Deployability,Fixes to allow the 1.0 GOTC single sample pipeline to complete,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3628:42,pipeline,pipeline,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3628,1,['pipeline'],['pipeline']
Deployability,Fixup AWK command in perf deploy scripts [BA-4842],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5116:26,deploy,deploy,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5116,1,['deploy'],['deploy']
Deployability,Fixup perf configuration around statsd instrumentation [BA-4788 follow-up],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5024:11,configurat,configuration,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5024,1,['configurat'],['configuration']
Deployability,"Flexible date times no longer work in the cromwell 0.20. (e.g. ""2016-06-20"" is no longer a valid date). Either update the README and notify customers of this change or revert to the flexible date time parser. On behalf of green team, Vivek votes for keeping the stricter date time parser.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1064:111,update,update,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1064,1,['update'],['update']
Deployability,Follow up to [this PR](https://github.com/broadinstitute/cromwell/pull/7151). Forgot to to set the runner to be `self-hosted` like it is in our release pipeline.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7152:144,release,release,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7152,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"Following on from the conversation at #4039, I've started a ""Getting started with Containers"" on the Cromwell docs. My thoughts are it might be a good place to put thoughts about containers, how they can be used and any caveats about them. This tutorial follows the [user goal](https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330341279) (from #2177):; > As a **user with images in Singularity**, I want **Cromwell to support using Singularity images (either via Singularity Hub and the command line, or connecting via API)**, so that I can **use Singularity images and not have to duplicate them in Docker**. However, without any code changes, just purely through configuration. . I think it's worth touching on using these container technologies with job schedulers, so I've included the ConfigurationFile and the HPCIntro tutorials as prerequisites.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635:684,configurat,configuration,684,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635,2,"['Configurat', 'configurat']","['ConfigurationFile', 'configuration']"
Deployability,"Following up on [WX-330](https://broadworkbench.atlassian.net/jira/software/c/projects/WX/boards/174?modal=detail&selectedIssue=WX-330). . - kshakir's deploy key isn't in Github secrets any more.; - Since we deploy through GHA now instead of Travis, I took the liberty of removing some unused code directly related to that key and process. . [WX-330]: https://broadworkbench.atlassian.net/browse/WX-330?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7084:151,deploy,deploy,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7084,2,['deploy'],['deploy']
Deployability,"For Joel, set google project [24 Hotfix]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1907:33,Hotfix,Hotfix,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1907,1,['Hotfix'],['Hotfix']
Deployability,"For any table where there are update queries made (not insert) where the rows represent extra-workflow information, make sure that the table is locked when those updates are happening. Examples of such a table would be the workflow store or call cache stuff. . A counterexample would be the metadata table, which is insert-only",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3342:30,update,update,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3342,2,['update'],"['update', 'updates']"
Deployability,For discussion. The current default `StandardHealthMonitorServiceActor` checks both Docker and the Engine database. Depending on deployment the first check may be completely unnecessary or undesirable (see #4626) and the second check is only really useful if there are queries to Cromwell's status endpoint to read the status. If the basic idea is accepted there would need to be additional documentation announcing the change in default and how to restore the previous default behavior.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4632:129,deploy,deployment,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4632,1,['deploy'],['deployment']
Deployability,"For later when we want to begin implementing this feature:. The [Papi docs](https://cloud.google.com/genomics/docs/how-tos/migration#accessing_the_containers) recommend adding an action that looks like this:. ```json; {; ""pipeline"": {; ""actions"": [; {; ""imageUri"": ""gcr.io/cloud-genomics-pipelines/tools"",; ""entrypoint"": ""ssh-server"",; ""flags"": [; ""RUN_IN_BACKGROUND""; ],; ""portMappings"": {; ""22"": 22; }; }; ]; }; }; ```. via: https://groups.google.com/forum/#!topic/google-genomics-discuss/1nkIxKrqBk0. Permalinks versions of the floating-""master""-links in that thread:; - [""Handling of the command line flag""](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L402-L404); - [""Container (""action"") added""](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L759-L766)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-492863979:222,pipeline,pipeline,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-492863979,6,['pipeline'],"['pipeline', 'pipelines', 'pipelines-tools']"
Deployability,"For our use cases, I’d say put responsibility on the pipeline developer. If there’s a collision, which file “wins” may be undefined, at least in the beginning. In the future, it could be useful to have the status set to Failed. However, we’d like to distinguish “pipeline failure” from “output failure” in an automated way. So if it’s recognized as a Failure, then it should codify the error status to determine the cause of failure without having to parse the error message. Perhaps one could introduce FailedWithWarnings status or something better.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467:53,pipeline,pipeline,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467,2,['pipeline'],['pipeline']
Deployability,For some reason the patch coverage works now. Weird that it intermittently works and not works.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505357240:20,patch,patch,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505357240,1,['patch'],['patch']
Deployability,"For the integration tests Cromwell runs, it would be good for the test definition to assert the hash of the expected output and actual output. This particular case is to ensure that there are no changes to the engine that could cause the contents of the outputs to change over time, and alert the team when such a case happens.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4904:8,integrat,integration,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4904,1,['integrat'],['integration']
Deployability,"For this one, I basically just worked around it. The other one is still an issue. I'll send it to you with the workaround in place. We can discuss rolling back the workaround when issue #3039 is sorted. I'm hoping that is a typo.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3038#issuecomment-350738471:147,rolling,rolling,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3038#issuecomment-350738471,1,['rolling'],['rolling']
Deployability,"For those that come across this PR in the future, a subset of the [docs](https://docs.codecov.io/v4.3.6/docs) referred to in this PR:; - Commit Status; - Project Status; - [Target](https://docs.codecov.io/v4.3.6/docs/commit-status#section-target); - [Threshold](https://docs.codecov.io/v4.3.6/docs/commit-status#section-threshold); - [Base](https://docs.codecov.io/v4.3.6/docs/commit-status#section-base); - [Patch Status](https://docs.codecov.io/v4.3.6/docs/commit-status#section-patch-status); - Coverage Configuration; - [Range](https://docs.codecov.io/v4.3.6/docs/coverage-configuration#section-range); - [Rounding](https://docs.codecov.io/v4.3.6/docs/coverage-configuration#section-rounding); - Notifications; - [Slack](https://docs.codecov.io/v4.3.6/docs/notifications#section-slack); - Pull Request Comments; - [Disable comment](https://docs.codecov.io/v4.3.6/docs/pull-request-comments#section-disable-comment) (aka Github emails); - CodeCov YAML; - [Default Branch](https://docs.codecov.io/v4.3.6/docs/codecov-yaml#section-default-branch); - [Validate your repository yaml](https://docs.codecov.io/v4.3.6/docs/codecov-yaml#section-validate-your-repository-yaml)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2779#issuecomment-339091206:409,Patch,Patch,409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2779#issuecomment-339091206,5,"['Configurat', 'Patch', 'configurat', 'patch']","['Configuration', 'Patch', 'configuration', 'patch-status']"
Deployability,"For those who are curious yet lazy, here's a link to see the updated formatting [in-place](https://github.com/broadinstitute/cromwell/blob/aednichols-patch-1/CONTRIBUTING.md)!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4937#issuecomment-489165255:61,update,updated,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4937#issuecomment-489165255,2,"['patch', 'update']","['patch-', 'updated']"
Deployability,"Forgot to update this. I'm fairly certain of two things:; - This is an artifact of using mock jes; - There's a subtle bug somewhere. I've not been able to replicate this. I'm still not sure _what_ happened exactly nor why but I will jot down what I saw in case this comes up again. There were 2 jobs out of the 20k scatters which found their way back into the engine with FailedRetryable errors. In the code there are only 2 places where those are created and both involve preemption. On the engine side at the moment there's a direct assumption when this happens that the job was indeed preempted. I'm not certain how exactly this led to wacky behavior (and admittedly don't completely remember the details) but it appeared that the original ""preempted"" jobs did in fact complete and the preempted jobs never ran. In the DB the errors were CromwellFatalErrors w/ 500 messages in them. This should be impossible considering that when these FailedRetyrable things are created they're stuffed with a preempted error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225:10,update,update,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225,1,['update'],['update']
Deployability,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2548,configurat,configuration,2548,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974,1,['configurat'],['configuration']
Deployability,"Found the forum entry looking for a solution for mounting a docker volumen. In my case I would like to run Ensembl VEP with Cromwell/WDL. Using VEP in cache/offline mode has many advantages, among them much better performance. When running VEP in cache mode it is necessary to have a large set of files locally installed. Downloading these files using the provided INSTALL.pl will be very inefficient. I plan for now to tar everything together and download and untar from a google bucket every time I run the task. However, it would be much better if I could mount a docker volume to the container running the task. The way I see it I would be able to define an snapshot in the runtime section of the task definition. I would also be able to define the mount point (docker run -v *:{mount point}) where this snapshot would be available as a docker volume. In the background Cromwell would provision a disk using the snapshot, mount it to the VM and use the correct `docker run -v /path/to/disk:/requested/mount/point` docker run command. Hope this helps defining this issue. Thanks for considering raising the priority of this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349:311,install,installed,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349,2,"['INSTALL', 'install']","['INSTALL', 'installed']"
Deployability,"From @droazen . Alright, the commit to use to build the jar to run GenomicsDBImport (using the instructions above) is: d4d97fcbb59efd9acbf8fabca7361b59512755bb. The tool is passing integration tests at this point, and it is completely worth your while to profile the current version and see how it compares to the SelectVariants approach. It's worth mentioning that in the next week or so we will add one additional argument to the tool which might further help performance. You can track the status of this here: https://github.com/broadinstitute/gatk/issues/2613. Hand-off complete -- have a good weekend everyone!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007:181,integrat,integration,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007,1,['integrat'],['integration']
Deployability,"From Dion, regarding how we construct a Pipeline for every Run:. Could you convert that into the ephemeral Run version? RunPipelineRequest.ephemeral_pipeline instead of pipeline_id. Will save you one RPC call too.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/856:40,Pipeline,Pipeline,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/856,1,['Pipeline'],['Pipeline']
Deployability,"From Henry --. My comment is a little different than the commenter above. So I will try to provide some context in order to differentiate. If one looks at Cromwell as being part of a larger application infrastructure/service such as a Genomics Pipeline where sequenced data is constantly being processed through the system or providing a service where users on the Internet can launch workflows ""as a Service"" - whenever they way. In these systems, uptime and availability are critical - at all times. Below are several scenarios where I could see running multiple Cromwells would be very beneficial. High availability:; Having multiple Cromwells running where jobs are ""load balanced"" between them would allow us to continue operate when there are systems issues or failures with one of the Cromwell instances. Zero-downtime deployments:; Supporting multiple Cromwells running different versions of the code, could provide the ability to upgrade Cromwell with little or no user impact. Essentially the new version is deployed to a new server, it is started up and at an appropriate time traffic (via a load balancer or proxy maybe) is directed away from the ""old version"" Cromwell to the new. Similar to item 2: being able to introduce infrastructure changes (host OS, security patches, host resizing,..) more seamlessly:; If I can support multiple instances of Cromwell, I can build a new instance of the host with all the updates and changes I require - deploy cromwell to the new node and cut over. A corollary to this request is also the ability to ""move"" workflows from one Cromwell instance to another. Maybe this is just workflows not in flight or active - but waiting to run. This capability could make it easier to retire older cromwell instances (once multiple cromwell instances support is in place). Some workflows may take days to run, being able to ""relocate"" these workflows to the ""new"" cromwell - allows us to decommission ""old"" cromwells faster.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/691:244,Pipeline,Pipeline,244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/691,7,"['Pipeline', 'deploy', 'patch', 'update', 'upgrade']","['Pipeline', 'deploy', 'deployed', 'deployments', 'patches', 'updates', 'upgrade']"
Deployability,"From [a user's forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/12577/stdout-stderr-output-in-bucket-does-not-seem-to-be-updated-while-task-is-running#latest):. 1. `stderr` and `stdout` do not appear to show up until a workflow finishes. ; 2. In some cases `stderr` and `stdout` are of type `application/octet-stream` rather than `text/plain`, not allowing the content to be viewable without downloading. . I was able to see these by running a [five-dollar-genome-analysis-pipeline](https://portal.firecloud.org/#workspaces/fccredits-silver-pumpkin-7172/five-dollar-genome-analysis-pipeline_copy) as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3967:143,update,updated-while-task-is-running,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3967,2,"['pipeline', 'update']","['pipeline', 'updated-while-task-is-running']"
Deployability,"From discussion with @geoffjentry, the purpose is to take this kind of logging out of the code; if someone wants to see these messages, they change the configuration of akka, instead (see http://doc.akka.io/docs/akka/current/java/logging.html#Auxiliary_logging_options). I've updated the PR description to reflect this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4260#issuecomment-430401423:152,configurat,configuration,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4260#issuecomment-430401423,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,From investigation of https://broadinstitute.atlassian.net/browse/DSDEEPB-2736. We decided to have these values in the configuration but to keep them commented out so GotC can optionally tune these.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/442:119,configurat,configuration,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/442,1,['configurat'],['configuration']
Deployability,"From my testing, it seems that anything that runs a ""chmod""-like command disrupts the ACL-controlled permissions, leading to permission denied and/or other errors. I think if the configuration option wrapped any commands that did this, it would fix the issue. In the meantime I was able to come up with a few workarounds to fix the permissions so that we were happy with the system (moved some files around so cromwell wasn't accessing or trying to move anything past our ACL, and added a ""chmod o-wrx..."" command to my submit script), but a configuration option that did this by default would be great!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374703828:179,configurat,configuration,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374703828,2,['configurat'],['configuration']
Deployability,"From user reports, this is upsetting cost estimation (and is scary anyway wrt re-writing history!. Needs validation and a reproducible case, but presumably something like:. - Run a long workflow, one shard fails; - Re-run the same workflow, most shard call cache; - The shards in the original workflow get updated to have the call-cache timings, rather than the original long timings.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4141:306,update,updated,306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4141,1,['update'],['updated']
Deployability,"GKILL` due to OOM.; I also tried with `true` though. Here is my simple OOM tester WDL. I tested it with PAPIv2 beta based on Life Sciences API. ```wdl; version 1.0. workflow mem_retry {; call fail_oom; }. task fail_oom {; command {; set -e; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero # <====== This WDL works fine without this line; }; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; continueOnReturnCode: [0, 137]; }; }; ```. Google backend (PAPI2 beta) in `backend.conf`, ; ```; config {; memory-retry {; error-keys = [""OutOfMemoryError"", ""Killed""]; multiplier = 1.5; }; }; ```. STDERR of task:; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/stderr; /cromwell_root/script: line 28: 17 Killed tail /dev/zero; ```. RC of task. It's weird that this is not caught in `metadata.json`.; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/rc; 137; ```. `memory_retry_rc`: So Cromwell found that it's failed due to OOM.; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/memory_retry_rc; 0; ```. `metadata.json`; ```; {; ""workflowName"": ""mem_retry"",; ""workflowProcessingEvents"": [; {; ""timestamp"": ""2020-08-29T00:00:38.724Z"",; ""cromwellVersion"": ""53"",; ""cromwellId"": ""cromid-0a29b92"",; ""description"": ""PickedUp""; },; {; ""description"": ""Finished"",; ""cromwellId"": ""cromid-0a29b92"",; ""timestamp"": ""2020-08-29T00:04:06.072Z"",; ""cromwellVersion"": ""53""; }; ],; ""metadataSource"": ""Unarchived"",; ""actualWorkflowLanguageVersion"": ""1.0"",; ""submittedFiles"": {; ""workflow"": ""version 1.0\n\nworkflow mem_retry {\n call fail_oom \n}\n\ntask fail_oom {\n command {\n set -e\n # This one-liner triggers 137 (SIGKILL due to OOM)\n # https://askubuntu.com/a/823798\n tail /dev/zero\n }\n runtime {\n cpu: 1\n memory: \""2 GB\""\n d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:1492,pipeline,pipeline-test-runs,1492,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,1,['pipeline'],['pipeline-test-runs']
Deployability,"GOTC was running the test for staging PAPI (Pipelines API). This test is launching 50 Single Sample workflows at once and 4 of our workflows failed with this error.; ```; ""message"": ""429 Too Many Requests\n{\n \""code\"" : 429,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""reason\"" : \""rateLimitExceeded\""\n } ],\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""status\"" : \""RESOURCE_EXHAUSTED\""\n}""; ```. All 4 of the jobs that failed were non-premptible whereas there are preemptible jobs that ran into this error and just went from attempt 1 to attempt 2 or w/e. . I don't think we would want this error to count towards our attempt count and we definitely don't want it to fail non preemptible tasks. @kcibul for prioritization",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763:44,Pipeline,Pipelines,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763,1,['Pipeline'],['Pipelines']
Deployability,Gary [reports](https://broadworkbench.atlassian.net/browse/QA-1244) that the updates to Cromwell <> Martha glue code have regressed in that they fail to accept date formats found in the wild:. ![Screenshot_2020-08-31 Job Manager](https://user-images.githubusercontent.com/1087943/91759930-7a74ef80-eba0-11ea-97e0-4c512d291ae0.png). Made this [fix suggested by Khalid](https://broadinstitute.slack.com/archives/CNZREM6TZ/p1598900079041200).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5819:77,update,updates,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5819,1,['update'],['updates']
Deployability,Generate coverage for integration tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2943:22,integrat,integration,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2943,2,['integrat'],['integration']
Deployability,"Generated from [this forum post](https://gatkforums.broadinstitute.org/wdl/discussion/13716/could-not-find-job-id-from-stdout-file-cromwell-with-an-unusual-backend#latest). It appears that the regex provided must match the entire stdout string in order to find the Job ID (from [CromwellAsyncJobExecutionActor.scala](https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala#L223)):; ```scala; val jobIdRegex = configurationDescriptor.backendConfig.getString(JobIdRegexConfig).r; val output = stdout.contentAsString.stripLineEnd; output match {; case jobIdRegex(jobId) => StandardAsyncJob(jobId); ```. But per the forum post, sometimes the output is multi-line:; ```; #> cat /.../cromwell-executions/myWorkflow/33ee300a-782d-47ae-a1e9-35a494e8bf11/call-myTask/execution/stdout.submit; mxq_group_id=...; mxq_group_name=...; mxq_job_id=16988030; ```. This fails using what is, in my opinion, a correct regex in their configuration file: `job-id-regex = ""mxq_job_id=(\\d+)""`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4436:514,configurat,configurationDescriptor,514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4436,2,['configurat'],"['configuration', 'configurationDescriptor']"
Deployability,Get languages to come in from the configuration file rather than be statically known.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3195:34,configurat,configuration,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3195,1,['configurat'],['configuration']
Deployability,Getting ready to release 25,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2051:17,release,release,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2051,1,['release'],['release']
Deployability,"Give Centaur-managed Cromwell more time to restart and a custom exit code.; Publish artifacts again on each build tag.; Login to docker before trying to push images.; Functions using secure variables ensure that xtrace is not enabled, thus no longer need a subshell, thus do not need to be exported.; Artifactory and Docker Hub credentials added to vault.; Docker login can use environment variables or vault once dsde-toolbox is public.; Split setup_secure_environment into setup_common_environment and setup_secure_resources.; Sbt environment variables prefixed as CROMWELL_SBT_*.; Print out a warning instead of exiting when vault resources cannot be rendered when testing locally.; Minor updates for more consistent shell variable usage.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3661:692,update,updates,692,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3661,1,['update'],['updates']
Deployability,"Given that this is being considered an error (I'd prefer ""Failure"") rather than truncation, I'm going to try to push one more time the idea that this should be a configuration option rather than some hard-coded magic number in WDL (which will inevitable need to be raised at some point, and then will different WDL versions have different `read_string` limits even when run on the same Cromwell?). If we were truncating then the worry about different results would be correct, end of story. But what we could be saying now is ""Failure. Your WDL engine isn't able to process this workflow because the file is too large. Try increasing resources or restructuring your WDL""... not really any different from ""Failure. You didn't have enough memory to run that scatter so wide..."", or indeed any other resource constraint. This is just me TOL of course... but it does seem a lot more elegant to me than having a special case for file sizes written in the WDL spec but every other resource limit being implicit and left up to the engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185:162,configurat,configuration,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185,1,['configurat'],['configuration']
Deployability,"Given that:; - The WDL spec today is un-opinionated on the size of an `Int`; - The WDL `Int` is defined as 64 bit in the upcoming https://github.com/openwdl/wdl/pull/250; - NB: swap the red and green diffs in your head because Jeff merged it too soon and this is a placeholder ""if we for some reason needed to revert"" PR; - And assuming that people are not currently writing workflows specifically relying on an explosion if their Ints are above 2^16. I suggest you just default to making anything WDL produce`WomLong`s in all new situations, since we'll probably sweep through and update everything else at some point soon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4489#issuecomment-446668372:582,update,update,582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4489#issuecomment-446668372,1,['update'],['update']
Deployability,"Given the eventually consistent scheme in place for deriving workflow status, it's likely that a status check soon after workflow submission will fail with a 404 response. From a UX perspective a bogus 404 error due to eventual consistency feels wrong; a lag in status update is one thing, but Cromwell probably shouldn't disavow the existence of the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/962:269,update,update,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/962,1,['update'],['update']
Deployability,Going to merge. Centaur has been updated to address the test failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229669314:33,update,updated,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229669314,1,['update'],['updated']
Deployability,Good news first: the `centaurPapiV2` build passed 🎉 ; Bad news: the other 4 PAPI v2 builds failed 😢 . The horicromtal builds are running with Cromwell configured to Carbonite but Centaur not configured to wait for Carboniting. While this might not have been intentional it's IMHO kind of appealing as a real-world scenario. I have no idea why the builds seem to hang until timeout as if some workflows were never completing. Conformance: `Unexpected failing tests: (6)`. No idea what happened with the engine upgrade test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305:509,upgrade,upgrade,509,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305,1,['upgrade'],['upgrade']
Deployability,"Good point of discussion. I defaulted to ""`sbt test` by default runs all tests, for now"". Will leave it up to whomever you recommend as second reviewer to decide if integration tests should be excluded from `sbt test` as part of this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169449078:165,integrat,integration,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169449078,1,['integrat'],['integration']
Deployability,"Good work! For me the problem was caused because I use the conda installation of cromwell and I made a typo in the `_JAVA_OPTIONS` environment variable, so it was running locally instead of on the cluster :man_facepalming: . Still I managed to find the issue because of your issue :smile: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-645263460:65,install,installation,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-645263460,1,['install'],['installation']
Deployability,Google Auth Updates BA-5681,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5013:12,Update,Updates,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5013,1,['Update'],['Updates']
Deployability,Google Cloud SDK updates surrounding gsutil and python3 [CROM-6785],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6479:17,update,updates,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6479,1,['update'],['updates']
Deployability,"Google Genomics fails when transferring the String output from a task to a bucket (Cromwell 25, using the wdl_runner pipeline). Here is the offending WDL file:; ```yaml; task Print_String {; command {; echo hello; }; runtime {; docker: ""ubuntu:14.04""; cpu: ""1""; memory: ""1 GB""; disks: ""local-disk "" + 10 + "" HDD""; }; output {; String out_string = read_string(stdout()); }; }. workflow My_Workflow {; call Print_String {; }; }; ```. This results in the error log:; ```; 2017-03-31 03:39:42,716 wdl_runner INFO: Workflow output files = [u'hello']; 2017-03-31 03:39:42,716 file_util INFO: Copying [u'hello'] to gs://ccdg-100-samples-trios-pilot-crams-mgi/outputs/string-out-1/; 2017-03-31 03:39:45,722 file_util WARNING: Copy [u'hello'] to gs://ccdg-100-samples-trios-pilot-crams-mgi/outputs/string-out-1/ failed: attempt 0; 2017-03-31 03:39:48,695 file_util WARNING: Copy [u'hello'] to gs://ccdg-100-samples-trios-pilot-crams-mgi/outputs/string-out-1/ failed: attempt 1; 2017-03-31 03:39:51,643 file_util WARNING: Copy [u'hello'] to gs://ccdg-100-samples-trios-pilot-crams-mgi/outputs/string-out-1/ failed: attempt 2; ERROR: copying files from [u'hello'] to gs://ccdg-100-samples-trios-pilot-crams-mgi/outputs/string-out-1/ failed: CommandException: No URLs matched: hello. (exit status 1); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2115:117,pipeline,pipeline,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2115,1,['pipeline'],['pipeline']
Deployability,"Google configuration; google {. application-name = ""cromwell-demo"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""***@***.gserviceaccount.com""; json-file = ""/***/***.json""; }; ]; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # of interest that you can find in the cromwell.example.backends folder; # folder at https://www.github.com/broadinstitute/cromwell; # Other backend providers include SGE, SLURM, Docker, udocker, Singularity. etc.; # Don't forget you will need to customize them for your particular use case.; backend {; # Override the default backend.; default = ""PAPIv2"". # The list of providers.; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # Google project; project = ""***-***"". # Base bucket for workflow executions; root = ""gs://*****/cromwell-execution"". # Make the name of the backend used for call caching purposes insensitive to the PAPI version.; name-for-call-caching-purposes: PAPI. # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.; slow-job-warning-time: 24 hours. # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 10000. # Polling for completion backs-off ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:10552,Pipeline,PipelinesApiLifecycleActorFactory,10552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiLifecycleActorFactory']
Deployability,"Got it. . So in 36 you're a bit stuck in that it's hardcoded into the `v2alpha1` version of Pipelines API. You could use the `v1alpha2` PAPI backend, depending on if you're using PAPIv2 for a specific reason or just because it's newer (side note: Google would really prefer people to be using `v2alpha1`). As of 36.1 (just released today) that docker image is only pulled when running CWL (unclear if you're using CWL or WDL) and is configurable in the configuration file via `CWL.versions.VERSION.output-runtime-extractor.docker-image`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466545513:92,Pipeline,Pipelines,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466545513,3,"['Pipeline', 'configurat', 'release']","['Pipelines', 'configuration', 'released']"
Deployability,Gracefully skip tests requiring AES 256 CBC if it's not installed.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/206:56,install,installed,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/206,1,['install'],['installed']
Deployability,Great. Building the code instead of downloading the latest release works like a charm :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-247018681:59,release,release,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-247018681,1,['release'],['release']
Deployability,Green integration testing with Centaur,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2336:6,integrat,integration,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2336,1,['integrat'],['integration']
Deployability,Green integration testing with Tyburn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2337:6,integrat,integration,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337,1,['integrat'],['integration']
Deployability,"Green team is seeing many errors detailed [here](https://partnerissuetracker.corp.google.com/issues/122571609):. The workaround is:. >... cromwell should migrate to pipelines-io, but in the meantime, if you want to make a quick fix for this you can do:. ` rm -f $HOME/.config/gcloud/gce`. > immediately before invoking gsutil inside your retry loop. This would save approximately 10% of the failures being seen in Green Team efforts per @tbl3rd and the bug",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4641:165,pipeline,pipelines-io,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4641,1,['pipeline'],['pipelines-io']
Deployability,"Guys, you should update mysql connector! Most of my workflow failures were because of mysql connection loss, it is such a pain to have a pipeline running for >1 day and having stuff crashed because ""cromwell lost connection to mysql""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4690#issuecomment-468618807:17,update,update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4690#issuecomment-468618807,2,"['pipeline', 'update']","['pipeline', 'update']"
Deployability,"H. On Fri, Feb 17, 2017, 10:58 AM Thib <notifications@github.com> wrote:. > ------------------------------; > You can view, comment on, or merge this pull request online at:j; >; > https://github.com/broadinstitute/cromwell/pull/2006; > Commit Summary; >; > - fail file hashing if the file does not exist; >; > File Changes; >; > - *M*; > supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala; > <https://github.com/broadinstitute/cromwell/pull/2006/files#diff-0>; > (17); >; > Patch Links:; >; > - https://github.com/broadinstitute/cromwell/pull/2006.patch; > - https://github.com/broadinstitute/cromwell/pull/2006.diff; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/2006>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AFIEGBYy1Z6suJGLDtusapP1VvcT0mSfks5rdcOhgaJpZM4MEbxA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369:522,Patch,Patch,522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369,2,"['Patch', 'patch']","['Patch', 'patch']"
Deployability,"HANGELOG.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/b48aba70ec793405c98788a322d160987ba51d3e/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.27).; You might want to review and update them manually.; ```; dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala; docs/developers/bitesize/ci/CaaS_DEV_CD.svg; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; </details>. labels: test-library-update, old-version-remains",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6281:2478,configurat,configuration,2478,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6281,6,"['configurat', 'update']","['configuration', 'update', 'updates']"
Deployability,"HI, having the same issue. Are there plans for an update? Happy to put in a MR to fix the type here! let me know ! Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3860#issuecomment-1728318541:50,update,update,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3860#issuecomment-1728318541,1,['update'],['update']
Deployability,HOTFIX: Forcibly abort workflows stuck in aborting for longer than 10 minutes…,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/986:0,HOTFIX,HOTFIX,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/986,1,['HOTFIX'],['HOTFIX']
Deployability,"Had a convo w/ Seth yesterday and looked into a few similar things (e.g. cwltool's support). I think the proper plan is as follows:. - Explore the path you've been looking at, by changing the configuration of a Cromwell backend to use Singularity instead of docker, but just for docker containers. This would cover the most common use cases; - Separately continue the [conversation at OpenWDL](https://github.com/openwdl/wdl/pull/237) to explore what support for native Singularity containers might look like in WDL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412307363:192,configurat,configuration,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412307363,1,['configurat'],['configuration']
Deployability,"Hah, you perturbed (WOTD) the hash, the test needs to be updated",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/332#issuecomment-165262625:57,update,updated,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/332#issuecomment-165262625,1,['update'],['updated']
Deployability,HandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.ja,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:5238,pipeline,pipeline,5238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,3,['pipeline'],['pipeline']
Deployability,Happy to back out the `SampleWdl.DeclarationsWorkflow` fix within this PR if it's being updated elsewhere.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/105#issuecomment-123483523:88,update,updated,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/105#issuecomment-123483523,1,['update'],['updated']
Deployability,Has anyone found a solution to this? I've experienced this issue since Cromwell was released for AWS and haven't found a good solution other than transferring all input files and references into the specific Cromwell S3 execution bucket.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-601996944:84,release,released,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-601996944,1,['release'],['released']
Deployability,Have y'all tried `womtool womgraph`?. ~My understanding is that `womgraph` is designed to be the successor to `graph`. This is similar to how `wdltool` became `womtool`. We have failed to communicate this migration adequately.~ Not fully correct as it turns out; but `womgraph` may fill a need until `graph` is updated.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561257861:311,update,updated,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561257861,1,['update'],['updated']
Deployability,"Haven't used udocker yet myself, but I'm pretty sure `${docker_script}` should be used there also instead of `${script}`. As for HPC and docker, there are some renegades out there running nodes with docker. The very few I've come across are not on-prem, spin up private larger machines on cloud resources, add HPC+docker, and then run whatever scripts on top of that. This includes our CI... until we get one of the non-root LXC implementations going. Related side-note: I would love one day for our CI to also install-then-test udocker, [rootless docker](https://github.com/moby/moby/blob/fc01c2b481097a6057bec3cd1ab2d7b4488c50c4/docs/rootless.md), singularity, etc. If anyone comes across this and knows the magic spell to fully-install-and-configure any of these container tools, or other HPC like HTCondor, GridEngine, etc., (especially on Travis!), please drop a gist or a PR. For example, here's the installation others helped me scrape together for SLURM:; - [Cromwell SLURM CI Installation Script](https://github.com/broadinstitute/cromwell/blob/3c3a3f85ba1229738265b11c3573ccb538b719c2/src/ci/bin/test_slurm.inc.sh); - [Cromwell SLURM CI Conf](https://github.com/broadinstitute/cromwell/blob/3c3a3f85ba1229738265b11c3573ccb538b719c2/src/ci/resources/slurm_application.conf) with commands to run/kill/check jobs; - [Cromwell SLURM CI Test Runner](https://github.com/broadinstitute/cromwell/blob/3c3a3f85ba1229738265b11c3573ccb538b719c2/src/ci/bin/testCentaurSlurm.sh)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470292981:511,install,install-then-test,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470292981,4,"['Install', 'install']","['Installation', 'install-and-configure', 'install-then-test', 'installation']"
Deployability,"Having a hard time finding the right place to unit/integration test these changes using existing specs/mocks/centaur. If tests are required, a good bit of test refactoring will need to follow to cut through the Akka-HTTP-`Route`-to-`SubmitActor`-to-`ServicesActor`s layers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439:51,integrat,integration,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439,1,['integrat'],['integration']
Deployability,"Having default runtime attributes in jes config caused faulty WARN messages about ""Unrecognized configuration key(s) for Jes"". This PR should fix those.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3662:96,configurat,configuration,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3662,1,['configurat'],['configuration']
Deployability,Heads up that updating the cromwell.yaml doesn't update the docs. We can mostly autogenerate docs but @katevoss did some hand editing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2832#issuecomment-342612911:49,update,update,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2832#issuecomment-342612911,1,['update'],['update']
Deployability,"Hearing that the Cromwell team will reject this (and then hence the original PR) is pretty disappointing. The configuration option is specific, doesn't explicitly relate to proxies, but allows us to run Cromwell, despite the lack of proxy support.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1547154827:110,configurat,configuration,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1547154827,1,['configurat'],['configuration']
Deployability,"Heh, I've seen it used around github-- and I now use it also-- as a soft version of thumbs-up. ""I don't really understand 100% of what this patch does, but it's small enough so...""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/320#issuecomment-165149877:140,patch,patch,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/320#issuecomment-165149877,1,['patch'],['patch']
Deployability,"Hello @alartin,. The ""local"" backend is essentially the resources on the computer where you're running Cromwell. If one runs local backend without docker, cpu & memory can't be restricted. However, if you are using the local backend with Docker, you can configure Cromwell to add the memory/cpu constraints to the run command. . Here's an example configuration that runs the Local Backend with Memory & CPU constrains applied: ; [Local Backend With Docker](https://gist.github.com/ruchim/568caa82513099b9d58e9cb89cadee26#file-localwithdocker-conf). There's a particular stanza that sets [default values for required runtime attributes](https://gist.github.com/ruchim/568caa82513099b9d58e9cb89cadee26#file-localwithdocker-conf-L90-L91). Choosing CPU default to be 1 memory to 4 MB (docker required minimum), so that these are the values applied to tasks that don't have cpu/memory explicitly defined. You can see that cpu/memory have been added as [custom runtime attributes](https://gist.github.com/ruchim/568caa82513099b9d58e9cb89cadee26#file-localwithdocker-conf-L25-L26). And it's also possible to see the exact way they are wired into [docker run command](https://gist.github.com/ruchim/568caa82513099b9d58e9cb89cadee26#file-localwithdocker-conf-L36-L37), where memory is always converted to MB (managed by the usage of the term `memory_mb`).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-440471436:347,configurat,configuration,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-440471436,1,['configurat'],['configuration']
Deployability,Hello @salonishah11 @cjllanwarne . It is currently not possible to mention networks from different GCP project(not the project where pipeline is running). PAPIv2 is accepting networks from from another project as well if we mention the value as; projects/PROJECT_ID_DIFFERENT_FROM_PIPELINE/global/networks/NETWORK_NAME. I know it is not possible to mention whole network using labels in project. Any chance this can be included google backend?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4806#issuecomment-616204371:133,pipeline,pipeline,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806#issuecomment-616204371,1,['pipeline'],['pipeline']
Deployability,"Hello Cromwell People, . Currently I believe Cromwell has retry logic for I/O issues or preemptible VMs issues for Google Cloud,. However, when Cromwell jobs that are executed via GridEngine dispatcher will fail with no re-try if the return code is deemed as a error code,. I am submitting a potential patch where Cromwell can retry failed jobs running on GridEngine with user specified retries (""backend.max-job-retries""), . I'm not sure how the configurations should be organized but here is my starting point; let me know what you guys think. Thanks,; Paul",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176:302,patch,patch,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176,2,"['configurat', 'patch']","['configurations', 'patch']"
Deployability,"Hello Cromwell Team, . I've updated a prior pull request: https://github.com/broadinstitute/cromwell/pull/6225 in order to support shared-vpc (https://cloud.google.com/vpc/docs/shared-vpc) in GCP. The changes include _shared-project-id_ and _shared-region_ parameters in order to form a Subnetwork label. Since the parameters depend on other some validations checks were updated.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6465:28,update,updated,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6465,2,['update'],['updated']
Deployability,"Hello Cromwell Team, . Our bioinformatics team have been reporting a single retry after preemptible attempts have been exhausted. They've added logic in the task itself that introspects the vm in the event the job ends up on a non-preemptible VM and promptly exists. This isn't ideal as starting a VM still incurs cost. . I've made the follow changes in:; ```diff; --- a/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala; +++ b/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala; @@ -882,8 +882,11 @@ class PipelinesApiAsyncBackendJobExecutionActor(override val standardParams: Sta; else {; val msg = s""$baseMsg The maximum number of preemptible attempts ($maxPreemption) has been reached. The "" +; s""call will be restarted with a non-preemptible VM. Error code $errorCode.$prettyPrintedError)""; - FailedRetryableExecutionHandle(StandardException(; - errorCode, msg, jobTag, jobReturnCode, standardPaths.error), jobReturnCode, kvPairsToSave = Option(preemptionAndUnexpectedRetryCountsKvPairs)); + FailedNonRetryableExecutionHandle(; + StandardException(errorCode, msg, jobTag, jobReturnCode, standardPaths.error),; + jobReturnCode,; + kvPairsToSave = Option(preemptionAndUnexpectedRetryCountsKvPairs); + ); }; ```. and tested with a trivial WDL and tasks such as (trying out multiple premptible / maxRetries):. ```wdl; task crash {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" && sleep infinity ; }; output {; String message = read_string(stdout()); }; runtime {; preemptible: 3; maxRetries: 0; docker: ""ubuntu:latest""; }; }. workflow wf_preempt {; call crash. output {; crash.message; }; }. ```. Let me know if I'm going in the right direction for a pull request.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6666:396,pipeline,pipelines,396,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666,7,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipelines']"
Deployability,"Hello I am trying to re-use an existing workflow for Mutect2 available here: https://app.terra.bio/#workspaces/terra-outreach/CHIP-Detection-Mutect2 to run on SLURM with Singularity configuration. There are multiple steps similar to Mutect2 public workflow available here: https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl , but still attaching the modified WDL with additional steps. . So when we run this with the given configuration using the following; export SINGULARITY_CACHEDIR=$PWD/singularity_cache; export SINGULARITY_TMPDIR=$PWD/tmpdir; module load singularity; rm -rf nohup.out && nohup java -Dconfig.file=$PWD/cromwell_singularity.conf -jar $PWD/cromwell-84.jar run $PWD/mutect2_modified.wdl --inputs $PWD/inputs.json &. The issue is that the first step of splitting intervals runs fine, but as it starts mutect2, it starts copying of the complete execution directory making here is the directory structure. cromwell-executions/; └── Mutect2; └── e5769b79-5e02-44a5-a4f8-38745e152beb; ├── call-M2; │ └── shard-0; │ ├── execution; │ └── inputs; │ ├── -1816294717; │ ├── 1855713868; │ │ └── run_cromwell_only.tmp; │ │ └── cromwell-executions; │ │ └── Mutect2; │ │ └── e5769b79-5e02-44a5-a4f8-38745e152beb; │ ├── 2035192126; │ └── 891763929; └── call-SplitIntervals; ├── execution; │ ├── glob-0fc990c5ca95eebc97c4c204e3e303e1; │ └── interval-files; ├── inputs; │ └── -1816294717; └── tmp.c9d96672. As you can see that run_cromwell_only.tmp is being made and that happens to fall in an endless loop and eventually, it errors stating the file name is too long to copy. Can you help me how to avoid this behavior of making circular paths when copying files for execution? Also, note it does not happen in the first step of SplitIntervals but happens in the Mutect2 call. [mutect2_gatk.wdl.txt](https://github.com/broadinstitute/cromwell/files/9813528/mutect2_gatk.wdl.txt); [cromwell_singularity.conf.txt](https://github.com/broadinstitute/cromwell/files/981352",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6934:182,configurat,configuration,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6934,2,['configurat'],['configuration']
Deployability,"Hello again,. Just an update: error persists even when I set the `project` name.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435543802:22,update,update,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435543802,1,['update'],['update']
Deployability,"Hello all, . This fixes BA-6144; EDIT: also fixes #4969. At LUMC we are currently creating [wdl-packager](https://github.com/biowdl/wdl-packager/tree/init). This program uses miniwdl to determine any file based imports in a WDL file and package them into a zip. This was done to make the release and deployment of BioWDL pipelines less unwieldy. Also this provides ready-made zip files for people who want to run BioWDL on a cromwell server. When testing I discovered that [Cromwell does not properly unpack zip files](https://broadworkbench.atlassian.net/projects/BA/issues/BA-6144). Zips with nested directories can not be unpacked. This is due to a bug in better-files 2.17.1 that was fixed in 3.0.0 and higher versions. In this PR the version of better-files is updated to 3.8.0 and any incompatibilities should be fixed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5312:288,release,release,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5312,4,"['deploy', 'pipeline', 'release', 'update']","['deployment', 'pipelines', 'release', 'updated']"
Deployability,"Hello cromwell dev team,. I'm currently working with the reference-disks option using GCPBatch as my backend. I executed the create_images.sh script from the documentation (https://cromwell.readthedocs.io/en/develop/backends/GCPBatch/) to generate my reference-disk-localization-manifests. I'm also using Cromwell v87, as specified in the same documentation. I also tested with the current `develop` build. While the manifest is correctly configured in the Cromwell config, and the reference disk appears to be mounting successfully, I’m encountering a failure with the umount command. I’m not sure why this command is being invoked in the first place. Mount Image; <img width=""573"" alt=""Screenshot 2024-09-26 at 16 07 46"" src=""https://github.com/user-attachments/assets/ce43be4a-132b-4c87-a27d-718a376171f7"">. **Error Message:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. Below is an example of what I have included in my Cromwell configuration (cromwell.conf). . ```; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/private-test-cromwell/global/images/omics-reference-disk-image"",; ""diskSizeGb"" : 10,; ""files"" : [ ; {; ""path"" : ""test-cromwell-references/hg38/v0/Homo_sapiens_assembly38.dict"",; ""crc32c"" : 2158779318; },; {; ""path"" : ""test-cromwell-references/hg38/v0/Homo_sapiens_assembly38.fasta"",; ""crc32c"" : 420322484; },; {; ""path"" : ""test-cromwell-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai"",; ""crc32c"" : 1970999569; }; ]; }; ]; ```. @mcovarr and @aednichols, I have seen issue [#7502](https://github.com/broadinstitute/cromwell/pull/7502)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7548:1049,configurat,configuration,1049,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7548,1,['configurat'],['configuration']
Deployability,Hello! Is there an update or workaround on this?; I am experiencing the same issue,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-841387132:19,update,update,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-841387132,1,['update'],['update']
Deployability,"Hello! We are still seeing this issue on version `36-fde91e6` using Cromwell-as-a-Service. For example, in the workflow ""d182afeb-33ae-45ac-8e78-ba9e0a7da7ab"", the ""SmartSeq2ZarrConversion"" task has an end time with no seconds or milliseconds but the start time is the full date-time stamp:; ```; ""start"": ""2019-01-28T10:24:40.084Z"" ; ""end"": ""2019-01-28T10:47Z""; ```. We noticed this issue in our pipeline that submits analysis results to the HCA, since the JSON schema that we follow to format the results requires that the timestamps are all in the full datetime format. . We could handle this inconsistency in our pipeline, but having Cromwell return timestamps in a consistent format would be ideal because then the tools that consume this information would not have to implement individual work-arounds to standardize the dates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2743#issuecomment-459418808:397,pipeline,pipeline,397,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2743#issuecomment-459418808,2,['pipeline'],['pipeline']
Deployability,"Hello! We seem to be running into a similar issue on cromwell `34-bda9485`. After humming along without a problem for a while, we all of a sudden stopped being able to run workflows with zipped WDL imports. Looking at the metadata, we get:; ```json; ""failures"": [; {; ""message"": ""Workflow input processing failed"",; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""pipelines%2Fdna_seq%2FUnmappedBamToAlignedBam.wdl: Name or service not known""; },; {; ""causedBy"": [],; ""message"": ""java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)""; },...; ```; Is this the same issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457239244:363,pipeline,pipelines,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457239244,1,['pipeline'],['pipelines']
Deployability,"Hello, I am running Cromwell 36 configured with the GCS/JES backend to run jobs on GCP. When running massive batches of workflows, I frequently encounter the IP-address quota from Google. To avoid this, I've reconfigured my default VPC to allow private google access (see #1325). I've added the following to my Cromwell configuration (other unrelated configuration entries removed):; ```; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; default-runtime-attributes {; noAddress: true; }; }; }; }; }; ```. This appears to have the desired effect, as my instances are now launching without an external IP, however, the jobs end up failing because docker cannot fetch the image `stedolan/jq` (as it resides on docker hub). Is there a way to configure Cromwell to use a different image for that pipeline action?. I could reconfigure the VPC to allow access to docker(hub), but that would require connecting a NAT instance which would increase the cost of using Cromwell. ---. Edit: Cromwell 36. Sorry!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676:320,configurat,configuration,320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676,5,"['Pipeline', 'configurat', 'pipeline']","['PipelinesApiLifecycleActorFactory', 'configuration', 'pipeline', 'pipelines']"
Deployability,"Hello, I am trying to deploy and use cromwell as a docker swarm stack and I have some serious problems with it. ; My configuration that I have in production is [https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml](https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml) where I have /pipelines folder (with cromwell-executions and data/mysql folders inside of it); As current broad container does not have docker inside (and thus cannot spawn tasks with docker runtimes) I had to:; * make a custom cromwell container that inherits from the official one and contains docker ( https://github.com/antonkulaga/cromwell-client/tree/master/services/cromwell ); * use ; ```yml; volumes:; - /var/run/docker.sock:/var/run/docker.sock; ```; trick, to spawn docker containers as sibling to cromwell container.; Unfortunately when running this setup I discovered that when I configure cromwell execution directory to an absolute path, like ""/pipelines/cromwell-executions"" the the script file that was generated by cromwell for each task still used /cromwell-executions I had to mount an extra volume to /cromwell-executions to trick it.; The other problem is that it constantly having errors like this:; ```; Workflow failed. WorkflowFailure(Unable to determine that 190 is alive, and /pipelines/cromwell-executions/vsearch/1ab35317-b0ae-4e7b-8b09-3403cdaff125/call-global_search/execution/rc does not exist.,List()); ```; while rc file does exist (checked it both on host system and volume). My bet is that it is somehow related with having cromwell inside docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3910:22,deploy,deploy,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3910,7,"['configurat', 'deploy', 'pipeline']","['configuration', 'deploy', 'pipelines']"
Deployability,"Hello, and apologies if this is not the correct place for this query. Cromwell engine version 46. We are currently looking at a Proof of Concept with Cromwell and AWS batch.; We trying to understand the **CallCaching** trigger(s), as we require this if the next step in a multi-step workflow breaks. Currently, we have set up the in-memory version and are not using any form of database. We have added the following to the configuration file:; `call-caching {; enabled = true; invalidate-bad-cache-results = true; }`. **Question 1.**. Can call caching be initiated if there is only the in-memory database, as below is not clear regarding this?; _""Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs, it is best to configure Cromwell to point to a MySQL database instead of the default in-memory database. This way any invocation of Cromwell (either with run or server subcommands) will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create iss",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5280:423,configurat,configuration,423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280,1,['configurat'],['configuration']
Deployability,"Hello, does anyone tried to implement a Job Array with LSF scheduler within Cromwell?. I am adapting my pipeline (that uses job array and the LSF scheduler) written in bash for Cromwell. To be sure that every task runs smoothly I have used only 1 sample and everything seems to work fine. Now I would like to create a job array with multiple samples, e.g. given a file with a list of unique ID (as I usually do in bash with LSF scheduler). Can anyone share his/her own experience?. Thanks in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087:104,pipeline,pipeline,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087,1,['pipeline'],['pipeline']
Deployability,"Hello, the cromwell team use a [JIRA](https://broadworkbench.atlassian.net/projects/BA/issues) now to triage issues - I'm as much of a fan of it as you are. 👎. My suggestion would be to use the the workflow options json to configure your workflow at runtime.; See here: https://cromwell.readthedocs.io/en/stable/wf_options/Overview/. This would extend your curl command with; `-F ""workflowOptions=@options.json""`. This may be a parameter however that needs to be set in the configuration file that is read by the server when it is launched. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-675797428:474,configurat,configuration,474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-675797428,1,['configurat'],['configuration']
Deployability,"Hello,. I am having a problem that has been already discussed but I haven't been able to solve it using the suggestions. Basically, In the wdl workflow, I have two tasks (at the moment). The first works fine but the second is not starting because the output of the first task cannot be 'linked' or 'copied'. This cause the workflow to fail. The interesting part is that in the input folder of the second task there are two subfolders: 1 is empty named as `13016223` and the other is not accessible `-1976550098`. The workflow to run needs installed:; `cutadapt` and the script named `moveBarcodeToID.pl` that can be downloaded from here:. https://drive.google.com/open?id=1AizxTwjOEhL5XA7rsx-wbY97p0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcod",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:539,install,installed,539,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['install'],['installed']
Deployability,"Hello,. I am new to cromwell and trying to run a test workflow on GPC. I am using the PAPIv2 backend and here is my config:; ```; $ cat genomics.conf | grep -v '#' | sed '/^$/d'; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxxxx""; }; }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; project = ""xxxxx""; root = ""gs://xxxx/cromwell_execution""; virtual-private-cloud {; network-label-key = ""xxx""; subnetwork-label-key = ""xxx""; auth = ""application-default""; }; name-for-call-caching-purposes: PAPI; slow-job-warning-time: ""24 hours""; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600; request-workers = 3; genomics {; auth = ""application-default""; endpoint-url = ""https://genomics.googleapis.com/""; location = ""us-west1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""application-default""; project = ""xxxx""; caching {; duplication-strategy = ""copy""; }; }; http { }; }; default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""us-west1-a"", ""us-west1-b""]; }; include ""papi_v2_reference_image_manifest.conf""; }; }; }; }; ```; When I run with the above config using:; ```; java -Dconfig.file=genomics.conf -jar cromwell-66.jar run cumulus.wdl -i cumulus_inputs.json; ```; I am getting the following error message:; ```; [2021-08-24 22:05:33,60] [info] WorkflowManagerActor: Workflow 6cc303b4-295d-49fa-a996-b5cf7ec9beea failed (during ExecutingWorkflowState): java.lang.Exception: Task cumulus.cluster:NA:1 failed. The job was stopped before",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477:532,pipeline,pipelines,532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477,2,"['Pipeline', 'pipeline']","['PipelinesApiLifecycleActorFactory', 'pipelines']"
Deployability,"Hello,. I think the problem is solved in release 78 of Cromwell. I had this problem when running the [mocha workflow](https://github.com/freeseek/mocha/blob/6679b1fcdacda4096148a75d5bb08ad1241de988/wdl/mocha.wdl#L899-L903) at Cromwell server 74. After updating to 78 the workflow completed the problematic tasks. - Cromwell 74:. ```; +--------------------+---------+------------+---------------------+; | TASK | ATTEMPT | ELAPSED | STATUS |; +--------------------+---------+------------+---------------------+; | batch_id_lines | 1 | 5m34.003s | Done |; | batch_sorted_tsv | 1 | 4m45.648s | Done |; | csv2bam (Scatter) | - | 10m51.838s | 1/1 Done | 0 Failed |; | green_idat_lines | 1 | 5m34.003s | Done |; | gtc | 1 | 5m27.897s | Done |; | gtc_reheader | 1 | 5m26.257s | Failed |; | idat | 1 | 5m27.897s | Done |; | idat2gtc (Scatter) | - | 10m58.206s | 0/1 Done | 1 Failed |; | red_idat_lines | 1 | 5m34.002s | Done |; | ref_scatter | 1 | 4m39.394s | Done |; | sample_id_lines | 1 | 5m34.003s | Done |; | sample_sorted_tsv | 1 | 4m42.453s | Done |; +--------------------+---------+------------+---------------------+; ❗You have 1 issue:. - Workflow failed; - GCS output file not found: gs://bioinfo-dev-temp/mocha/a224bb3e-fc20-4b0a-8846-ee2b4b603933/call-gtc_reheader/maps; - GCS output file not found: gs://bioinfo-dev-temp/mocha/a224bb3e-fc20-4b0a-8846-ee2b4b603933/call-idat2gtc/shard-0/gtcs; ```. - Cromwell 78. ```; +----------------------------+---------+-----------------+-----------------------+; | TASK | ATTEMPT | ELAPSED | STATUS |; +----------------------------+---------+-----------------+-----------------------+; | batch_id_lines | 1 | 16.37s | Done |; | batch_sorted_tsv | 1 | 15.288s | Done |; | call_rate_lines | 1 | 5m34.525s | Done |; | computed_gender_lines | 1 | 5m34.523s | Done |; | csv2bam (Scatter) | - | 49.958s | 1/1 Done | 0 Failed |; | flatten_sample_id_lines | 1 | 5m29.56s | Done |; | get_max_nrecords (Scatter) | - | 5m32.076s | 1/1 Done | 0 Failed |; | green_idat_l",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6677#issuecomment-1116554918:41,release,release,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6677#issuecomment-1116554918,1,['release'],['release']
Deployability,"Hello,. I wonder if it is possible to specify the GCP Batch task scheduling policy via Cromwell configuration. In specific, can I set `taskCountPerNode` to be `1` to enforce one job per VM (as in https://cloud.google.com/batch/docs/reference/rest/v1/projects.locations.jobs#taskgroup)?. Sincerely,; Yiming. <!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7521:96,configurat,configuration,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7521,2,['configurat'],['configuration']
Deployability,"Hello,. I'm running a Cromwell service as Google Cloud VM instance. The Cromwell's version is 68, with the following conf:. ```; include required(classpath(""application"")). webservice {; interface = xx.xxx.xxx.xx; port = xxxx; }. google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""gred-cumulus-sb-01-991a49c4""; }. }; }. workflow-options {; workflow-log-temporary = false; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; 	url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; 	user = ""root""; 	password = ""cromwell""; 	connectionTimeout = 5000; }; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; # Google project; project = ""gred-cumulus-sb-01-991a49c4"". # Base bucket for workflow executions; root = ""gs://gred-cumulus-output/cromwell_execution"". virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; auth = ""application-default""; }. # Make the name of the backend used for call caching purposes insensitive to the PAPI version.; name-for-call-caching-purposes: PAPI. # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.; slow-job-warning-time: ""24 hours"". # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Querie",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:926,pipeline,pipelines,926,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,2,"['Pipeline', 'pipeline']","['PipelinesApiLifecycleActorFactory', 'pipelines']"
Deployability,"Hello,. I'm trying to connect cromwell with Gloud life science (v2beta) everything works as it should except for the problem that it takes almost 3 minutes to complete a simple hello world task. I am using the config file recommended by the documentation to run. . I would like to ask for advice on how to solve this problem. ; Thank you a lot. config file:. ```; google {. application-name = ""xxxxx-xxxxx"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""xxxxx-xxxxx""; scheme = ""service_account""; service-account-id = ""xxxxx@xxxxx-xxxxx.iam.gserviceaccount.com""; pem-file = ""/xxxxx/xxxxx.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # Google project; project = ""xxxxx-cromwell"". # Base bucket for workflow executions; root = ""gs://xxxxx-cromwell_bucket"". # Make the name of the backend used for call caching purposes insensitive to the PAPI version.; name-for-call-caching-purposes: PAPI. # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.; slow-job-warning-time: 24 hours. # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 25000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Opt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:811,pipeline,pipelines,811,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,2,"['Pipeline', 'pipeline']","['PipelinesApiLifecycleActorFactory', 'pipelines']"
Deployability,"Hello,. I'm using Cromwell with Google Pipelines as backend and sometimes (maybe when more than 30 analysis running at same time) I'm getting workflow error (~2 of the 30). When inspecting the metadata for the workflow I can see a error message that contains ""ServiceException: 401 Requester pays bucket access requires authentication."". **Edit:** Using Cromwell 35. Has anyone had a similar problem? Here are the WDL task that are affected (from Broad's five dollar genome workflow):. ```wdl; task BaseRecalibrator {; File input_bam; File input_bam_index; String recalibration_report_filename; Array[String] sequence_group_interval; File dbSNP_vcf; File dbSNP_vcf_index; Array[File] known_indels_sites_VCFs; Array[File] known_indels_sites_indices; File ref_dict; File ref_fasta; File ref_fasta_index; Int disk_size; Int preemptible_tries. command {; /usr/gitc/gatk4/gatk-launch --javaOptions ""-XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal \; -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails \; -Xloggc:gc_log.log -Xms4000m"" \; BaseRecalibrator \; -R ${ref_fasta} \; -I ${input_bam} \; --useOriginalQualities \; -O ${recalibration_report_filename} \; -knownSites ${dbSNP_vcf} \; -knownSites ${sep="" -knownSites "" known_indels_sites_VCFs} \; -L ${sep="" -L "" sequence_group_interval}; }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.2-1510681135""; memory: ""6 GB""; disks: ""local-disk "" + disk_size + "" HDD""; preemptible: preemptible_tries; }; output {; File recalibration_report = ""${recalibration_report_filename}""; }; }; ```. And here is my cromwell server config:. ```scala; include required(classpath(""application"")). webservice {; port = 8000; }. system {; workflow-restart = true; }. engine {; filesystems {. gcs {; auth = ""service-account""; }. http {}. local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }. backend {; default = ""Local""; providers {. Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBack",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336:39,Pipeline,Pipelines,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336,1,['Pipeline'],['Pipelines']
Deployability,"Hello,. I'm wondering if cromwell has support for loading environment modules somehow? I need to port the same workflow between AWS (so, the docker runtime attribute is handy) and a slurm cluster (which uses module environment). Can they be specified as a runtime parameter for example? Can they be part of the configuration file to cromwell? If yes, any suggestions as to how?. Thank you, ; Azza",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997:311,configurat,configuration,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997,1,['configurat'],['configuration']
Deployability,"Hello,. It seems to me that cromwell (at least `cromwell-37.jar`) can run `version 1.0` WDL scripts. Would you confirm this? It would also be helpful if you had this info readily on the ReadTheDocs page (https://github.com/broadinstitute/cromwell/blob/develop/docs/LanguageSupport.md). Thank you,; Azza . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4678:1014,configurat,configuration,1014,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4678,1,['configurat'],['configuration']
Deployability,"Hello,. So that I don't have to manually check to see if this has been updated, can the following snippet be moved into a seperate file?. https://github.com/broadinstitute/cromwell/blob/97b7d324c5cbec4b29093aa26c5b10f5086b54bb/src/bin/travis/testCentaurCwlConformanceLocal.sh#L32",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3632:71,update,updated,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3632,1,['update'],['updated']
Deployability,"Hello,. when I try to run my workflow using a config file using. `$ java -Dconfig.file=../config/LSF.conf cromwell.jar cromwell run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json`. I get the following message; ```; Error: Could not find or load main class cromwell.jar; Caused by: java.lang.ClassNotFoundException: cromwell.jar; ```. Without specifying a config file, the pipeline runs without any problems. ; I installed Cromwell (version 79) using conda. I also tried the following:. `$ java -Dconfig.file=../config/LSF.conf cromwell-79.jar run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json `. ```; Error: Could not find or load main class cromwell-79.jar; Caused by: java.lang.ClassNotFoundException: cromwell-79.jar; ```. I also checked where the cromwell.jar file is saved in my conda environment and tried the following:. `; java -Dconfig.file=./LSF.conf /path/to/env/share/cromwell/cromwell.jar cromwell run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json `. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. This is the config file LSF.config:; ```; include required(classpath(""application"")). backend {. # Override the default backend.; default = LSF. # The list of providers. Copy paste the contents of a backend provider in this section; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; 	submit = ""bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /usr/bin/env bash ${script}""; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; # Second backend provider would be copy pasted here!. }; }; ```. I have not much experienced with cromwell and would be very grateful for help. Thank you,; Johannes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6796:135,pipeline,pipelines,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6796,9,"['configurat', 'install', 'pipeline']","['configuration', 'installed', 'pipeline', 'pipelines']"
Deployability,"Hello,; I'm try to run Cromwell on ""Win7 Home Premium SP1, 32bit"". Java:. ```; > java -version; java version ""1.8.0_73""; Java(TM) SE Runtime Environment (build 1.8.0_73-b02); Java HotSpot(TM) Client VM (build 25.73-b02, mixed mode); ```. Cromwell: (https://github.com/broadinstitute/cromwell/releases/tag/0.19.3). ```; >java -jar cromwell.jar run hello.wdl hello.json; [2016-08-08 08:33:03,503] [info] Slf4jLogger started; [2016-08-08 08:33:03,533] [info] RUN sub-command; [2016-08-08 08:33:03,533] [info] WDL file: hello.wdl; [2016-08-08 08:33:03,533] [info] Inputs: hello.json; [2016-08-08 08:33:03,573] [info] SingleWorkflowRunnerActor: launching workflow; [2016-08-08 08:33:04,203] [info] Running with database db.url = jdbc:hsqldb:mem:7e19faf1-d831-4edc-83fa-086ef9b16cd3;shutdown=false;hsqldb.tx=mvcc; [2016-08-08 08:33:08,947] [info] WorkflowManagerActor submitWorkflow input id =None, effective id = 4e20eafc-baae-4605-a010-adfa5f32ae46; [2016-08-08 08:33:09,687] [←[38;5;220mwarn←[0m] Failed to get application default credentials; java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.; at com.google.api.client.googleapis.auth.oauth2.DefaultCredentialProvider.getDefaultCredential(DefaultCredentialProvider.java:93); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:213); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:191); at cromwell.util.google.GoogleCredentialFactory.forApplicationDefaultCredentials(GoogleCredentialFactory.scala:125); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme$lzycompute(GoogleCredentialFactory.scala:64); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1261:292,release,releases,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261,1,['release'],['releases']
Deployability,"Hello,; I'm using Cromwell to run a pipeline on an LSF cluster.; Depending on the input, jobs can take longer than the average case. I was therefore wondering if it is possible to increase the LSF time limit after a job fails (due to LSF time limit which has to be specified in my case).; Follow-up question: Is it possible to access the previous number of failed attempts for a job?. I saw the option to increase memory after a job fails, but not for other parameters and only for Google Cloud backends. Many thanks in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6825:36,pipeline,pipeline,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6825,1,['pipeline'],['pipeline']
Deployability,"Here is a slightly more general script (it assumes the lock file and saved image are in the current directory). It also does the pull into a temp file with a rename into the destination name at the end so that for a large image the -f check won't trigger for a partial download. I do some work here to deduce a filename that should match (as I understand the rules anyway) the one that the pull would create. I also include an option to force the path, since in my automation I tend to wish to define everything for my self. The derived or given image filename is echoed at the end. YOUR_HOST is the name of the sregistry host (this is the context I'm doing this in). ```; #!/bin/bash . lock_dir=. if [[ $# -ne 1 && $# -ne 2 ]] ; then; echo ""Usage: $0 image-name [output-file]"" 1>&2; exit 1; fi; name=$1; output_file=$2. if [[ ""$output_file"" = """" ]] ; then. # deduce filename . output_file=`basename $name`; if [[ $output_file =~ (.*):([^:]+)$ ]] ; then; base=${BASH_REMATCH[1]}; tag=${BASH_REMATCH[2]}; else; base=$output_file; tag=latest; fi; output_file=""${base}_$tag.sif""; fi. url=shub://YOUR_HOST/$name. # declare a very similar path (.lock) where Cromwell can access ; lock=$lock_dir/$output_file.lock; tmp=$output_file.tmp. if [ ! -f ""$output_file"" ]; then # If we already have the image, skip everything ; (; flock --exclusive 200; if [ ! -f ""$output_file"" ]; then # do a second check once the lock has been released ; singularity pull --nohttps $tmp $url; mv $tmp $output_file; fi; ) 200>$lock; fi. rm -f $lock. echo $output_file; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921:1416,release,released,1416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921,1,['release'],['released']
Deployability,"Here is how I've configured GCP:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; project = ""***""; root = ""***""; genomics {; auth = ""application-default""; endpoint-url = ""https://us-west2-lifesciences.googleapis.com/""; location = ""us-west2""; }; filesystems {; gcs {; auth = ""application-default""; project = ""***""; }; }; }; }; }; }; ```. And yet the worker instances are running on us-central1. Which also means it's copying data across regions (because my data is in us-west2). <img width=""2017"" alt=""image"" src=""https://user-images.githubusercontent.com/125854/171462418-63cdc51b-1f9e-4a9c-90c6-479a36946628.png"">",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6774:134,pipeline,pipelines,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6774,2,"['Pipeline', 'pipeline']","['PipelinesApiLifecycleActorFactory', 'pipelines']"
Deployability,Here is the [Google Doc](https://docs.google.com/document/d/14cOvS3zdG5R_54R5PuM83piE1QjzTzCk8-NMKOx_Qp0/edit?ts=59692ac4) with existing ideas. . @danbills did you make any other issues for release improvement?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332001490:190,release,release,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332001490,1,['release'],['release']
Deployability,"Here is the permalink for the document that was already written by @cjllanwarne about [How to Make Your Own Backend](https://github.com/broadinstitute/cromwell/blob/9a734c4f36ae122153736004e8d54fc44af8fbde/MakingABackend.MD). I would like to release it around the same time that we publish the [AWS repo](https://github.com/broadinstitute/aws-backend), which is pending until the [documentation is improved](https://docs.google.com/document/d/1qwY0QBo04WIAvsACbvtIshfxbs1OJRvNp93oclxCRk8/edit#heading=h.pqdhlnltxuz5). This blog post would show up on the WDL blog, which is developer focused. . @sooheelee I am thinking of including a message to the effect of ""This blog post is about our story, it is not a living document. If you decide to write your own backend you should contact us"". Then if many people contact us then we can make a living document about how to write your own backend for Cromwell. This is instead of making this blog post into a living document from the outset, which would take effort in its upkeep, without any clear need for it. What do you think?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2017:242,release,release,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2017,1,['release'],['release']
Deployability,Here's the URL to the EPAM pipeline builder: http://pb.opensource.epam.com/. I assume this can be closed but crommers can feel free to reopen if I am in error,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-320522599:27,pipeline,pipeline,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-320522599,1,['pipeline'],['pipeline']
Deployability,"Hey @EvanTheB -- the docs around backends need more changes, and the updates will be easier to make on my own branch. I'm going to take your changes, adjust surrounding docs and have you review that instead -- hence closing out this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3537#issuecomment-384634147:69,update,updates,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3537#issuecomment-384634147,1,['update'],['updates']
Deployability,"Hey @Horneth -- would you mind replacing the command line 7 of install.sh to say ""sbt assembly""? Just another sneaky fix for Jose!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-250545327:63,install,install,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-250545327,1,['install'],['install']
Deployability,"Hey @SHuang-Broad, yes you are right it should be pointing to 36.1. There was some conversation going on for forcing HomeBrew to recognize 36.1 as the newest version. But that has became unnecessary now as a new release should be out soon. You can look at the HomeBrew conversation [here](https://github.com/Homebrew/homebrew-core/pull/37175).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4713#issuecomment-470603661:212,release,release,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4713#issuecomment-470603661,1,['release'],['release']
Deployability,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:189,install,install-linux,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702,8,"['configurat', 'install']","['configuration', 'install', 'install-linux']"
Deployability,"Hey @TMiguelT, I made a few small changes to udocker, added the notes we discussed and a next steps section to follow the general template of the other tutorials. I wanted to add a small section about the caching of udocker images but don't know udocker well enough to really assert this:. > #### Caching in udocker; > udocker caches images within the install or user directory, thus reducing the need to pull and build the docker containers at every stage. Clarification is required on whether udocker will concurrently write to the same cache directory for largely scattered workflows. So I've just left it out. I also think it might be worth saying more explicitly that Singularity is technically user-installable (just without `setuid`, as I didn't realise until our conversation. If you're happy with what's there now, I'll remove the WIP and put it up for review again. If there's anyone out there reading, we'd love to get your feedback or clarification on any points.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232:352,install,install,352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232,4,['install'],"['install', 'installable']"
Deployability,"Hey @Xophmeister, sorry for the slow response time!. This error message is actually coming from our SFS (shared filesystem) backend (so I'll ping @kshakir). I'm not familiar with the `mounts` attribute in the SFS. However, I think the answer to your question is that none of the attributes asked for by the SFS backend are arrays, and so arrays are not a supported attribute type. . I actually could only find reference to the `mounts` attribute outside of the SFS backend in places like BCS and Google cloud. I wonder whether you just need to move this attribute out of your configuration file and into your WDL task itself?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411:576,configurat,configuration,576,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411,1,['configurat'],['configuration']
Deployability,"Hey @agraubert, the best way to know that a ticket will be worked on is when the pipeline for it changes to backlog bucket. . Just as an FYI, we are focused on scale improvements and small bug fixes for the near future. Out of curiosity, are you interested in contributing this feature yourself, especially with the guidance of a Cromwell developer? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-478161147:81,pipeline,pipeline,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-478161147,2,['pipeline'],['pipeline']
Deployability,"Hey @antonkulaga, are you running this in Cromwell 30?. The good news: this was indeed a known issue for a long time but I believe it's finally been fixed as of https://github.com/broadinstitute/cromwell/pull/3175. ; The bad news: that won't be available until the next Cromwell release. If you're comfortable building from develop you're welcome to do that and try it out!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367441100:279,release,release,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367441100,1,['release'],['release']
Deployability,"Hey @dheiman - I've updated the JES behaviour of `write_lines` to match the local and SGE behaviour. . Since this seems to have unearthed a slight difference of opinion regarding the expected behaviour in a variety of situations, I made a list of scenarios in [this doc](https://docs.google.com/a/broadinstitute.org/document/d/1WWxtVwZQKrotvJLXfIflYOwidbasf-arDZUByv0Dt14/edit?usp=sharing) - and it'd be awesome if you would consider adding your opinions too! Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-296700787:20,update,updated,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-296700787,2,"['a/b', 'update']","['a/broadinstitute', 'updated']"
Deployability,Hey @geertvandeweyer. I'm preparing a new release with more functionalities based on cromwell 78. It should be ready in the next few days. . Are you in the cromwell slack?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-1065009387:42,release,release,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-1065009387,1,['release'],['release']
Deployability,"Hey @geoffjentry. The formula downloads the .jar from the Github releases page and [creates a little wrapper script](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L11) that's used to make `cromwell` available as a command-line tool. There's no need to compile from source. In order to update the formula for future releases, you can just submit a PR to Homebrew by updating the [url](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L4), [SHA](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L5), and [install steps](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L9-L12). I'd be happy to do this in the future for future releases -- just include this step in whatever release checklist you may use. Homebrew is a pretty well-established community, so there's not much to contribute on that end. Cheers!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166033076:65,release,releases,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166033076,6,"['install', 'release', 'update']","['install', 'release', 'releases', 'update']"
Deployability,"Hey @indraniel -- this isn't yet supported --but the plan is to add it eventually. For now, it may be helpful to add something like an rsync at the top of your task, to see what logs are being produced by your task, or if things have gone quiet. It's also possible to add a monitoring script to run in the background that includes info about cpu/memory usage ; https://cromwell.readthedocs.io/en/stable/wf_options/Google/#google-pipelines-api-workflow-options (see ""monitoring_script"")",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-491980358:429,pipeline,pipelines-api-workflow-options,429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-491980358,1,['pipeline'],['pipelines-api-workflow-options']
Deployability,"Hey @mr-c - I'm looking to update things. But this raises another question - since CWL 1.0.X is by definition backwards compatible, why would our reliance on an older version of `schema-salad` be an error on the tester? It shouldn't matter? . For instance we've talked in the past about embedding a particular version of the python code in our JAR just to ensure stability, but if I'm understanding what's causing issues on the CI (and I'm not sure I **am**, but it's my current theory at least) then if we did such a thing this would come up regardless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-411546386:27,update,update,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-411546386,1,['update'],['update']
Deployability,"Hey @rhpvorderman, your changes look fine to me. One thing I noticed is that we've used `${script}` when I believe it should actually be `${docker_script}` to get the container relevant path for the script. Can you confirm in your configurations?. Otherwise I'm happy to click approve from my side (if I'm allowed to do that?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630620299:231,configurat,configurations,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630620299,1,['configurat'],['configurations']
Deployability,"Hey @sychan, definitely unfortunate this got lost. 1. This wasn't relevant for me because our pipelines (generated from Janis) inserted the digest at transpilation time so it wasn't relevant. 2. Would make sense to me, but Broad wanted to treat this feature as unstable and generally unsupported. 3. When I made the PR, I was a contributor on this repo, hence the internal branch so it could run tests, unfortunately I'm not on this repo so effectively can't touch the branch, unless I have recreated it in my local fork. 3.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435387948:94,pipeline,pipelines,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435387948,1,['pipeline'],['pipelines']
Deployability,"Hey All -- I think this is a great feature. If I'm understanding correctly, this should also be a big help locally because this mode will use the same credentials that are used for gcloud. That is, if you have done a 'gcloud login' before, it will use those credentials. . Thinking about that... what if we just remove the 'user' mode for google altogether? It's sort of a pain to do, and having gcloud installed seems a really low bar for someone who wants to run using a google backend as they will likely need gcloud to transfer data, manage their account, etc. If we don't want to remove features... I think we should at least rename them so that 'application-default' sounds more appealing so that is what people do (maybe even call it default).; - k",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166117677:403,install,installed,403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166117677,1,['install'],['installed']
Deployability,"Hey Conrad - Thanks, this is awesome. To give some insight on how things are playing out in the hopefully-not-too-long-term, we're planning on cutting an alpha release of the PBE stuff imminently (perhaps today?) which is something we feel is stable enough to start poking at but is missing a few features we need in our production use cases (restart/recovery, call caching), backends other than local/JES, and with some known warts we need to hammer out. I'm guessing you're looking at roughly a month for something more stable than that, although I'm famous for my ""about a month"" predictions. However, since you're already pretty up to speed with what's going on, I'd say that the 0.20 alpha should be stable enough to work up a backend. It'd at least be a good test case as someone who _did_ figure out how to make one in the old system if the new system is inscrutable or not. In terms of what to do with this PR, I'll somewhat leave it up to you. We're hoping to close the 0.19 books as much as possible once the alpha thing is out, but if you feel like it'll provide value to folks over the next month or so I'm happy to take some time to review it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813:160,release,release,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813,1,['release'],['release']
Deployability,"Hey Jaeyoung--is there a public version of such an image I can test with on; Google? As for AWS, I think there's a real feature request to make Cromwell; compatible with docker images with an entrypoint on AWS. On Tue, Jun 25, 2019 at 11:39 PM Jaeyoung Chun <notifications@github.com>; wrote:. > I'm definitely having this problem with AWS Backend. Not sure how newest,; > but I believe I had this problem during the HCA Pipeline Surges as well.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2461?email_source=notifications&email_token=ADR7XTPGWVEOCY34LELUVMDP4LQHRA5CNFSM4DTKAP42YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYSHD6I#issuecomment-505704953>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ADR7XTLSYROEYXWXSLAFQ2LP4LQHRANCNFSM4DTKAP4Q>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-505867640:421,Pipeline,Pipeline,421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-505867640,1,['Pipeline'],['Pipeline']
Deployability,"Hey Miguel -- happy to brainstorm/tagdeam on the tools for this if you need help. I think the ""SELECT... FOR UPDATE"" semantics are what we need here. See https://dev.mysql.com/doc/refman/5.7/en/innodb-locking-reads.html. Basically when you do the something like . SELECT ... FROM workflow_store WHERE [is new or dead workflow] ORDER BY submission_time LIMIT n FOR UPDATE. will prevent two things from touching that workflow at the same time. Then just make sure the update of status happens in the same transaction and you're all set. . Give a shout... my next two days are shockingly meeting light!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3342#issuecomment-371248824:109,UPDATE,UPDATE,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3342#issuecomment-371248824,3,"['UPDATE', 'update']","['UPDATE', 'update']"
Deployability,"Hey develop team, thank you for develop this good software.; I have built a cromwell by using docker-compose.; here is my docker-compose.yml; ```version: '2'; services:; cromwell:; build: ; context: ./compose/cromwell; volumes:; - ./cromwell-executions:/cromwell-working-dir/cromwell-executions; - /data1:/data1; command: [""/wait-for-it/wait-for-it.sh mysql-db:3306 -t 120 -- java -Dconfig.file=/app-config/cromwell-application.conf -jar /app/cromwell.jar server""]; links:; - mysql-db; ports:; - ""80:8000""; mysql-db:; image: ""mysql:5.7""; environment:; - MYSQL_ROOT_PASSWORD=cromwell; - MYSQL_DATABASE=cromwell_db; volumes:; - ./compose/mysql/init:/docker-entrypoint-initdb.d; - ./compose/mysql/data:/var/lib/mysql; ports:; - ""3307:3306""; ```. and here is my crowell config file:. ```include required(classpath(""application"")). # Note: If you spot a mistake in this configuration sample, please let us know by making an issue at:; # https://github.com/broadinstitute/cromwell/issues. call-caching {; enabled = false; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = true; runtime-attributes = ""String? docker Int? max_runtime = 2""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". # Root directory where Cromwell writes job results. This directory must be; # visible and writeable by the Cromwell process as well as the jobs that Cromwell; # launches.; root: ""cromwell-executions"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]. caching {; duplication-strategy: [; ""soft-link""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be h",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7006:865,configurat,configuration,865,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7006,1,['configurat'],['configuration']
Deployability,"Hey everyone, I did manage to do some testing today (after some discussion with @TMiguelT as well). Some small notes about my setup:. - I'm using Singularity that has the ability to store and run an OCI container to/from a file.; - I have one place `/path/to/containers/*` where I store all my containers.; - I transform the container digest (returned by Cromwell as `${docker}`) to generate a filename and use that to uniquely reference the container (per this PR: #4797). Notes about my (slightly modified) config below:. - My `$image` var has slashes in it (because it's a path to a file) which isn't correct, as `flock` expects a valid path, so I've just used `$docker_subbed` which is the transformed docker file.; - I didn't have write permission to `/var/lock/$imagename`, I've opted instead for the container directory.; - I wanted the output of `flock` to be redirected to Cromwell's `stderr.submit`.; - I do the second image check for when the lock is released, the locked processes will find the image and skip the pull (per @rherban's [comment](https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509677104)); ```bash; # transformed docker digest; docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); # output path of container (.sif); image=/path/to/containers/$docker_subbed.sif; # declare a very similar path (.lock) where Cromwell can access; lockpath=/path/to/containers/$docker_subbed.lock . if [ ! -f ""$image"" ]; then # If we already have the image, skip everything; (; flock --verbose --exclusive 200 1>&2; if [ ! -f ""$image"" ]; then # do a second check once the lock has been released ; singularity pull ""$image"" docker://${docker}; fi; ) 200>/var/lock/$lockpath; fi; ```. Hope this helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637:962,release,released,962,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637,2,['release'],['released']
Deployability,"Hey there, thanks for the bug report! I believe this is actually fixed in our later versions (we just released v22) - could you confirm whether upgrading resolves this issue?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1555#issuecomment-253861510:102,release,released,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1555#issuecomment-253861510,1,['release'],['released']
Deployability,"Hey, not part of the Cromwell team but thought I'd try to help out. To clarify, you've:; - Built a Docker container with SGE + mysql; - Where `qsub` is not available through your `$PATH`, but installed at `/opt/gridengine/bin/lx-amd64/qsub`; - A (_virtual?_) SGE cluster is running within the container; - Running Cromwell inside this container; - Asking the cluster inside your docker container to spin up another Docker container. If this is correct, I'm struggling to understand the motivations behind it, but a few pointers:. - What does intermittent errors mean?; - You should avoid running Docker-in-Docker (SO: [Is it ok to run docker from inside docker?](https://stackoverflow.com/questions/27879713/is-it-ok-to-run-docker-from-inside-docker)); - It might be more predictable add `qsub` to the docker's path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484:192,install,installed,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484,1,['install'],['installed']
Deployability,"Hi , ; When submitting jobs requiring GPU, we specified in the runtime session: ; gpuCount: 2; gpuType: ""nvidia-tesla-k80""; the jobs failed with following errors:; 2019/05/03 14:40:50 E: command failed: nvidia-docker | 2019/05/03 14:40:50 Error: Could not load UVM kernel module. Is nvidia-modprobe installed?. The same WDL file (with same docker and runtime attributes) used to work before. Please help!. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4935:299,install,installed,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935,2,"['configurat', 'install']","['configuration', 'installed']"
Deployability,"Hi ,; Im running the GATK [warp joint genotyping pipeline ](https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/joint_genotyping/JointGenotyping.wdl)cromwell on GCP backend . The pipeline fails because cromwell cannot localize files with certain data types like ` Array[Array[String]]`. This issue was reported on the [terra website](https://support.terra.bio/hc/en-us/community/posts/4409388371611-How-do-I-pass-an-array-array-file-to-another-task-) too and the workaround was to write a task to read file into an array, I have performed the workaround but are there plans to fix this issue in any upcoming releases. **Cromwell version tested :** 85. **Are you seeing something that looks like a bug? Please attach as much information as possible.** ; `""Failed to evaluate 'sample_name_map_lines' (reason 1 of 1): Evaluating read_tsv(sample_name_map) failed: Failed to read_tsv(\""gs://wgs/test/sample_map.txt\"") (reason 1 of 1): java.lang.IllegalArgumentException: Could not build the path \""gs://wgs/test/sample_map.txt\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, LinuxFileSystem. Failures: \nHTTP: gs://wgs/test/sample_map.txt does not have an http or https scheme (IllegalArgumentException)\nLinuxFileSystem: Cannot build a local path from gs://wgs/test/sample_map.txt (RuntimeException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""`. **Which backend are you running?** ; GCP. **Link to the workflow if possible**; https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/joint_genotyping/JointGenotyping.wdl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7364:49,pipeline,pipeline,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7364,5,"['pipeline', 'release']","['pipeline', 'pipelines', 'releases']"
Deployability,"Hi - sorry I had missed the original question from @myazinn and needed to dig into ad hoc files a bit before opining on first #5057 from @Kirvolque and now #5064. I'm still not in a position where I can answer definitively but my first question would be why not follow the pattern from the [GCP backend](https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L716) and [TES backend](https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/tes/src/main/scala/cromwell/backend/impl/tes/TesAsyncBackendJobExecutionActor.scala#L95) and modify the `mapCommandLineWomFile` function in `AwsAsyncBackendJobExecutionActor`. . It's entirely possible that something about the AWS backend's wiring would lead to that not being the right choice, but that's what I want to understand.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509691549:385,pipeline,pipelines,385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509691549,3,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipelines']"
Deployability,"Hi - thanks to both of you. In this case my procrastination paid off, I've been meaning to find a way to work this example into something like the examples conf or the docs, so having this updated example is great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438719642:189,update,updated,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438719642,1,['update'],['updated']
Deployability,"Hi - yes, I can confirm this. Cromwell has had WDL 1.0 support for not quite a year now. You're right that this should be updated (pinging @cjllanwarne )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4678#issuecomment-466792777:122,update,updated,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4678#issuecomment-466792777,1,['update'],['updated']
Deployability,"Hi @AlekseiLitkovetc,. > I would like to know which two tests should pass with non-default credentials. Back in March when I filed the ticket we were only [including tests for four](https://github.com/broadinstitute/cromwell/blob/38/src/ci/bin/testCentaurAws.sh#L24-L27) of our dozens of Centaur tests. After the AWS hackathon, the test coverage expanded to only [exclude a few](https://github.com/broadinstitute/cromwell/blob/39/src/ci/bin/testCentaurAws.sh#L25-L53) remaining tests. So, the original minimum A/C was to see these tests passing with non-default creds:; - [hello](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/standardTestCases/hello.test); - [long_cmd](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/standardTestCases/long_cmd.test); - [haplotypecaller.aws](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/integrationTestCases/haplotypecaller.aws.test); - [singlesample.aws](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/integrationTestCases/singlesample.aws.test). > could you please prompt where I can find a link to Jenkins?. Our Jenkins servers are only internally accessible to Broad employees because of Jenkins continued problems with security. Perhaps one day we'll move off Travis to another public CI-as-a-Service that will run tests longer than 180 minutes, such as CircleCI or Google Cloud Build. Then we could migrate all tests to one place.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-503831164:916,integrat,integrationTestCases,916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-503831164,2,['integrat'],['integrationTestCases']
Deployability,"Hi @Redmar-van-den-Berg, you're correct, there appears to be a bug in our draft-2 parser which is failing to catch this. To answer ""which is correct"", the requirement to wrap values in arrays was not being enforced correctly but it now is. In your example you can do this with the array literal syntax, eg:; ```wdl; call ls {; input: files = [ i ]; }; ```. I have added a test for our WDL 1.0 support which **is** catching this properly, so if you're able to upgrade your workflows from WDL draft-2 to WDL 1.0, then `womtool validate` will give you the correct answer. If not, I'll leave this open as a bug since it certainly *should* be picked up by womtool. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-454545219:459,upgrade,upgrade,459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-454545219,1,['upgrade'],['upgrade']
Deployability,"Hi @SergeySdv, we have marked your PR as ""back with originator"" so that you can update it based on the discussion. Once it is ready for review just leave a comment and we will take a look.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-543962574:80,update,update,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-543962574,1,['update'],['update']
Deployability,"Hi @TMiguelT - I fully agree that Cromwell should support some of the more degenerate base images like busybox and alpine. This is partially historical (at one point Pipelines API required Bash so we had no reason to not just use bash ourselves), and going forward it's more been a lack of putting in enough testing of these sorts of containers to keep ourselves honest - we **intend** to be more agnostic but things fall through here and there",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-453554372:166,Pipeline,Pipelines,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-453554372,1,['Pipeline'],['Pipelines']
Deployability,"Hi @aednichols , ; Sorry for the long delay in response. This PR definitely fixes an issue that we found running Cromwell in AWS (see my description [above](https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-543478047)). . I don't know if this scenario is covered in the existing tests. I had a quick look at the CI build and it looks to me as if the AWS CI Job is failing due to a job timeout, rather than actual tests failing. The last line in the log is:; > The job exceeded the maximum time limit for jobs, and has been terminated. The other successful jobs have the test results summary and an overall success message. Can you adjust the CI settings & run it again to see what the results are?. If this scenario is not actually covered by the existing tests, do I need to add an integration test in order to have the PR merged?. thanks; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170:799,integrat,integration,799,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170,1,['integrat'],['integration']
Deployability,"Hi @aednichols, . For us there's a large price difference between regular vs Spot VM on GCP hence the pursuit of purely pre-emptible pipelines.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030166346:133,pipeline,pipelines,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030166346,1,['pipeline'],['pipelines']
Deployability,"Hi @antonkulaga - you're right, and likely more than just Pairs. It's been a while since we've updated the plugin so it's not totally on board with newer WDL constructs at the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-299453584:95,update,updated,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-299453584,1,['update'],['updated']
Deployability,"Hi @antonkulaga, your `application.conf` file is missing `--cidfile ${docker_cid}` inside submit-docker, which creates `docker_cid` file. So if you update your `submit-docker` to ""docker run --rm **--cidfile ${docker_cid}** -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"", it should work! You can look at Cromwell's default config for submit-docker [here](https://github.com/broadinstitute/cromwell/blob/e6c7704b9300db8852ae9ac7fbefe39ef6ba71d7/core/src/main/resources/reference_local_provider_config.inc.conf#L14-L34). Let me know if this doesn't work!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4011#issuecomment-418474050:148,update,update,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4011#issuecomment-418474050,1,['update'],['update']
Deployability,"Hi @azzaea,. The AWS backend for Cromwell integrates with AWS Batch for job scheduling and execution. As such it pretty much only uses tasks that use Docker containers. My understanding is that Cromwell can be configured with multiple backends (e.g. AWS and FilesystemLocal) and that tasks can be parameterized via inputs to the workflow to choose which backend it runs on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502847475:42,integrat,integrates,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502847475,1,['integrat'],['integrates']
Deployability,Hi @carolynlawrence. Would a configuration option to remove this privilege re-assignment be sufficient to fix this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374695606:29,configurat,configuration,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374695606,1,['configurat'],['configuration']
Deployability,"Hi @chapmanb - sorry for the delay in responding here. I was able to get http inputs to work in CWL against a default (ie no custom config specified) instance of Cromwell in server mode. The test case I used is in the linked PR (#4392). I wonder whether you could confirm:; - Whether this test case works for you, and if so:; - Is your use of HTTP inputs different somehow?; - How can I enhance my test case to cover whatever is different?; - Or, whether this test case does not work for you, and if so:; - We might try to work out what is different between your configuration and the default which might be breaking things",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-439193089:563,configurat,configuration,563,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-439193089,1,['configurat'],['configuration']
Deployability,"Hi @cjllanwarne Thanks for the update. I looked at #5468 and #5554 changes made. They all S3 related changes/enghancements. This fix is EFS specific and hence not addressed by the above two. So this change is needed. ; Thanks,; Vanaja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5522#issuecomment-665306474:31,update,update,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5522#issuecomment-665306474,1,['update'],['update']
Deployability,"Hi @cjllanwarne, thanks for taking a look!. Yeah the link to ""google's implementation"" in the [description](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L759-L766) is where the Action came from; its essentially a direct port into Scala. You can see [here](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L402) where that flag is handled, if its set to true, then the Action is added. . The `ssh-server` entrypoint is google's own implementation for the pipelines api, found [here](https://github.com/googlegenomics/pipelines-tools/tree/749315a73e6c3bd5277351e32a365f42198db1ae/ssh-server). As far as I'm aware there's no direct switch on the existing API to have it do that automatically.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625:142,pipeline,pipelines-tools,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625,6,['pipeline'],"['pipelines', 'pipelines-tools']"
Deployability,"Hi @danbills !; > I'm a little wary of introducing 2 different AWS sdk's into the project.; > ; > Is the reason to use Transfer Manager? Besides logging updates what is the benefit of it?; > ; > Besides using transfer manager is there a need to pull in the ""old"" SDK? We were told the current 2.X series is the one to use. I rewrote files copying using `TransferManager` just because @wleepang adviced it's usage in original issue ([this](https://github.com/broadinstitute/cromwell/issues/4982)). > Oh I think I see that the existing version doesn't support copying directories. Maybe there have been updates since this code was written though?. If you are talking about [this comment](https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-530285103), I think that it is about old version, before my changes. Actually, I haven't tested my fix with ` Array[File]` type, only with `File`, will do it tomorrow. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537262399:153,update,updates,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537262399,2,['update'],['updates']
Deployability,"Hi @dinvlad ...; - The ""Docker"" backend referenced in application.conf is just using straight docker on the host machine. There's also ""JES"" backend which is using the google genomics pipelines API, which is effectively docker-as-a-service; - There are indeed plans to support both AWS and Azure. Some (probably crude at first) support for one of the two is expected within a couple of months. Over the course of then ext few quarters we expect to support both as well as other cloud vendors as well. In terms of how they'd be done, the answer is It Depends. There's the budding [GA4GH Task Execution API](https://github.com/ga4gh/task-execution-schemas) which was heavily inspired by the google genomics pipeline API. Our hope is to see other cloud vendors support this API, which would make our lives easier. Assuming that doens't happen, we've experimented a bit with Azure, and the remote docker approach has made the most sense. The actual outcome is not set in stone at the moment. Does this answer your questions?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239537352:184,pipeline,pipelines,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239537352,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Hi @dspeck1 ,. Thanks for your reply. The is the cromwell outputs when things happened:. ```; =======================log start============. status_events {; description: ""Job state is set from QUEUED to SCHEDULED for job projects/A_JOB_ID.""; event_time {; seconds: 1713287682; nanos: 566509009; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from SCHEDULED to RUNNING for job projects/A_JOB_ID.""; event_time {; seconds: 1713287919; nanos: 96623968; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from RUNNING to FAILED for job projects/A_JOB_ID. Job failed due to task failures; . For example, task with index 0 failed, failed task event description is Task state is updated from RUNNING to FAILED on zones/A_INSTANCE_ID due to Spot VM; preemption with exit code 50001.""; event_time {; seconds: 1713288624; nanos: 767597866; }; type: ""STATUS_CHANGED""; }. task_groups {; key: ""group0""; value {; counts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:729,update,updated,729,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630,1,['update'],['updated']
Deployability,"Hi @ffinfo would you might rolling back the `RetryAbortedJobs` changes and submitting them again as a separate PR? . I suspect that you're conflating `abort` as something external vs `abort` as something that Cromwell does itself (eg from a REST request or while it's shutting itself down) - and we need to be careful to get all of those interactions right - especially if this affects other backends. In any case, I think it's worth having it properly reviewed as its own change (rather than having it delay an otherwise approved PR 😄).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427009938:27,rolling,rolling,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427009938,1,['rolling'],['rolling']
Deployability,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:56,patch,patch,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,4,"['configurat', 'patch']","['configuration', 'patch']"
Deployability,Hi @geoffjentry Thanks for your response. I was trying to walk throught `[Server Mode](http://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/)`. On the `Start the job` section I wasn't able to choose files. I downloaded 33.1 from [(https://github.com/broadinstitute/cromwell/releases)]. Here is the system version:; ```; $ lsb_release -a; No LSB modules are available.; Distributor ID:	Ubuntu; Description:	Ubuntu 16.04.4 LTS; Release:	16.04; Codename:	xenial; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403045308:284,release,releases,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403045308,2,"['Release', 'release']","['Release', 'releases']"
Deployability,"Hi @gn5 , please see `delete_intermediate_output_files` on this page: https://cromwell.readthedocs.io/en/stable/wf_options/Google/. Note that this feature only works on Google Pipelines API v2 at the moment. This is because PAPIv2 is the primary use case for Cromwell within the Broad Institute. When the option is enabled, Cromwell is smart enough to remove the files automatically from being considered in call caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3881#issuecomment-620053705:176,Pipeline,Pipelines,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881#issuecomment-620053705,1,['Pipeline'],['Pipelines']
Deployability,"Hi @henriqueribeiro , . Do you think I can compile this branch with functionality of the latest cromwell release (77) ? I'm now using the branch based on cromwell 58, but more recent versions have retry strategy that's interesting as well: ; - you retry logic handles spot kills; - cromwell retry handles the fetch_and_run is a directory problem. . => both would be great, but since it doesn't get approved, I hope to make a new custom build. However, it says here there are conflicts... . Greetings, ; geert",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-1065001775:105,release,release,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-1065001775,1,['release'],['release']
Deployability,"Hi @huangzhibo, thanks for your contribution! I have two comments:. ---. 1. I think this feature is OK if the cromwell owner wants to allow it, but not in all cases. ; - Eg if I'm hosting a Cromwell instance I might not want users to be able to specify an arbitrary location on my filesystem to write their workflow contents. ; - I want to allow others to comment on this rather than making any solid statements... personally I think at the very least we'd want to require an opt-in in the configuration to enable this - something like `""allow_workflow_specified_execution_root""`?. ---. 2. Could you consider adding a centaur test for this? This should help get you started:; - Have a look in `centaur/src/main/resources`; - Make a new test directory for the new test; - Add a workflow that can determine the workflow root - off the top of my head, maybe running `pwd` from within the `command` and selecting the workflow root from the output?; - Add a new workflow options file alongside the workflow file and use it to specify an execution root.; - Create a new `.test` file next to the others, and use it to specify your workflow and your workflow options as inputs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4379#issuecomment-438388028:490,configurat,configuration,490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4379#issuecomment-438388028,1,['configurat'],['configuration']
Deployability,"Hi @jainh,. The runtime section is very backend-implementation specific, but to answer your question in the abstract:. The wdl spec [says](https://github.com/broadinstitute/wdl/blob/develop/SPEC.md#runtime-section) that ""Values can be any expression …"". For example:. Valid wdl string:; ```; runtime {; my_key: ""a 'b c' d""; }; ```. Valid wdl array:; ```; runtime {; my_key: [""a"", ""b c"", ""d""]; }; ```. The following however is **invalid** according to the spec as the keys are duplicated:; ```; runtime {; my_key: ""a""; my_key: ""b c""; my_key: ""d""; }; ```. It is then up to the backend to decide and implement what keys and values it will accept. The config backend is currently implemented to [only](https://github.com/broadinstitute/cromwell/blob/839ea1e456b929d6149430f4d7d3805f8c235d3f/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/DeclarationValidation.scala#L43-L50) support primitive wdl types (WdlInteger, WdlString, WdlFloat, WdlBoolean) and their optional wrappers. While it does not support them, one could update that code to support arrays of values too. Meanwhile, the JES backend already does support arrays for some attributes, e.g. for [`zones`](https://github.com/broadinstitute/cromwell/blob/839ea1e456b929d6149430f4d7d3805f8c235d3f/supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesRuntimeAttributes.scala#L127) and [`disks`](https://github.com/broadinstitute/cromwell/blob/839ea1e456b929d6149430f4d7d3805f8c235d3f/supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesRuntimeAttributes.scala#L142).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-332307343:1039,update,update,1039,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-332307343,1,['update'],['update']
Deployability,"Hi @leepc12 - This is a really good question. Internally our interest in requester pays has been fully under the umbrella of a separate team so we haven't run into this. I suspect that this would require the Pipelines API (i.e. JES) to support it and isn't something we can change directly. We can dig into this a bit. Note that Pipelines API has a new version coming out soon which will open up all kinds of functionality, I suspect this is already on the list. Again, we can dig into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916#issuecomment-345524845:208,Pipeline,Pipelines,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916#issuecomment-345524845,2,['Pipeline'],['Pipelines']
Deployability,"Hi @multimeric ,. Thank you so much for providing the current location of those links. . I still have a question regarding the solution proposed here. In the current documentation website, I could not find the guidance or CloudFormation for creating a custom AMI, which is described above. And moreover, I also could not find the description saying that ""The AMI type needs to be specified as 'cromwell' and the Scratch mount point needs to be specified as \cromwell_mount"", which was mentioned above as the solution. . Is it because the whole creation procedure has been changed since then? In the current documentation, I only found [this link](https://docs.opendata.aws/genomics-workflows/core-env/create-custom-compute-resources.html#custom-amis) which only briefly talked about creating a custom AMI but not gave any CloudFormation link. Do you know where I should look for this information? Thanks!. Also, thank you for letting me know about the Genomics CLI. I have to say that the current deployment procedure on AWS is way more complicated than GCP, especially given that my company's cloud team puts more restrictions which complicates the standard procedure that used to work in my personal AWS account. I'm looking forward to hearing from the development of Genomics CLI. Sincerely,; Yiming",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-922527156:997,deploy,deployment,997,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-922527156,2,['deploy'],['deployment']
Deployability,"Hi @rasmuse - a few thoughts. In terms of the submit time keep in mind things like JVM initialization. Calling individual invocations of a java program like that for what's effectively a blink of an eye operation is never going to be ideal from a performance perspective. If you're submitting to a Cromwell server consider using something like `curl` instead. . On the second part, there are a few things potentially going on here. First is that Cromwell doesn't necessarily immediately start a workflow. It scans every `n` seconds for new workflows to start, which defaults to `20`. In a worst case scenario `21` of your `20` seconds could be due to that, although that seems unlikely. You can make that time window shorter by overriding the `system.new-workflow-poll-rate` configuration setting to something smaller, e.g. `1`. Even then, there's a some overhead in there as ultimately we're trying to optimize for a case that's not running single, extremely short tasks. I just ran the moral equivalent of a hello world workflow locally with that config setting set to 1 second. The workflow was picked up for execution at `11:08:44` and registered as complete at `11:08:51` with exactly half of that time spent with the system running the underlying job (i.e. not in Cromwell) so it might be worth revisiting this w/ a combination of using `curl` and speeding up the workflow polling rate. That said while I'd love you to continue to use Cromwell/WDL, it might not wind up being the best tool for your job. If these workflows are purely for yourself & you don't intend on building them up over time and/or distributing them to others, you might want to check out Snakemake which is more intended to be a direct Makefile replacement.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378636936:775,configurat,configuration,775,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378636936,1,['configurat'],['configuration']
Deployability,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:117,configurat,configuration,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334,2,['configurat'],['configuration']
Deployability,Hi @salonishah11 ; I've updated PR and resolved conflicts,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-533906281:24,update,updated,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-533906281,1,['update'],['updated']
Deployability,"Hi @seandavi - That does seem like it should work. Thinking back in my past I've definitely encountered tools which expect TMPDIR to exist and aren't smart enough to create it themselves. Also in JES there shouldn't be any issues with permissions, etc. We'd certainly welcome a PR if you're game for it, either (or both) against `0.19_hotfix` or `develop`. On that note, I should point out that a new release (currently `develop`) is imminent and for all but one use case (call caching) we beliee it to be more robust/stable that 0.19. I'd personally recommend people who don't need call caching work with the new system, but I understand that some people aren't comfortable working with code which isn't yet released.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687458:401,release,release,401,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687458,2,['release'],"['release', 'released']"
Deployability,"Hi @tseemann - . Thanks, we're aware. We followed the instructions on brew's site and used the revision flag, but eventually worked it out with @ilovezfs to use `28.1` in this casee. We don't, at least for now, release our software to bioconda. We've since decided to switch to an x.y system as our x versions are meaningful internally and shouldn't be changing for things like critical bug fixes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2480#issuecomment-317223254:211,release,release,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2480#issuecomment-317223254,1,['release'],['release']
Deployability,"Hi @vsoch - here's what I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possibl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:291,install,install,291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,2,"['configurat', 'install']","['configuration', 'install']"
Deployability,"Hi @vsoch - it shouldn't require too much of a deep dive into the scala, we know that it already can be made to work with `udocker` by just changing the configuration like you've done. Let me know if you've not seen the udocker example and I'll track it down for you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720:153,configurat,configuration,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720,1,['configurat'],['configuration']
Deployability,"Hi @wleepang !; I've updated a pull-request with two commits from my colleagues @TimurKustov and @SergeySdv . Their contribution includes an addition of `AWS` filesystem support for `fileSystemCheck` option in a `centaur` test cases files and also a refactoring of `CheckFiles`.; Because `AWS` is now supported in fileSystemCheck `centaur`'s option I've added some integration tests which are checking that copying of workflow results, workflow logs and call logs (options `final_workflow_outputs_dir`, `final_workflow_log_dir`, `final_call_logs_dir` from options.json file) is now correct on 3 backends (GCP, AWS and local backend). There are placeholders for paths in these test cases and options used by them, because I didn't came with any better options without creating a public buckets on GCP and AWS.; Hope that you can take a look on this soon!; Thanks in advance, best regards!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921:21,update,updated,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921,2,"['integrat', 'update']","['integration', 'updated']"
Deployability,"Hi @wleepang . Yes, you are right. The problem I have is particularly when using a slurm backend, as I don't find an easy way to load environment modules except to actually modify the individual command section of each task definition in my workflow. This is inconvenient because when I'm running the same pipeline on AWS batch, there are no environment modules, so my tasks fail unless I explicitly remove all the `module load <module name>` from the command part of each task. I would like to switch back and forth between cloud and cluster without having to touch the pipeline script (i.e. individual tasks) itself. Put differently, can I specify a runtime attribute called `module` much like the `docker` attribute, and then somehow modify the backend configuration settings to have cromwell load this module? . Did I explain myself better this time?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502902782:306,pipeline,pipeline,306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502902782,3,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,Hi @wleepang @cjllanwarne @aednichols !; Could you please take a look at updated changes?; Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-529430432:73,update,updated,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-529430432,1,['update'],['updated']
Deployability,"Hi @zfrenchee, you can find the docker images for Cromwell 36.1 in our Releases tab [here](https://github.com/broadinstitute/cromwell/releases/tag/36.1). Is this the information you were looking for?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-467576363:71,Release,Releases,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-467576363,2,"['Release', 'release']","['Releases', 'releases']"
Deployability,"Hi All,. Im running into an issue with my deployment of Cromwell 65. I am running scripts connecting to a local MySQL(also tested on MariaDB). Upon running a reasonably complex pipeline I am receiving a number of database errors:. ```; java.sql.BatchUpdateException: Data truncation: Data too long for column 'METADATA_KEY' at row 6; 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:78); 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499); 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480); 	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192); 	at com.mysql.cj.util.Util.getInstance(Util.java:167); 	at com.mysql.cj.util.Util.getInstance(Util.java:174); 	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchedInserts(ClientPreparedStatement.java:755); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:426); 	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:796); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$18(JdbcActionComponent.scala:542); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:425); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:420); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:489); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6545:42,deploy,deployment,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6545,2,"['deploy', 'pipeline']","['deployment', 'pipeline']"
Deployability,"Hi Atlas Team,. . I have installed the Atlas(apache-atlas-sources-2.1.0) in; our server by following in the link; ""https://atlas.apache.org/2.0.0/InstallationSteps.html. After all the setup; have been done, we ran the atlas-start.py and the atlas is running on port; 21000.When we are accessing the atlas, we are facing 503 Service Unavailable; Error. We checked the logs from application.log file. We got below issue . . 2020-12-13 06:29:02,309 WARN - [main:] ~ JMX is not enabled to receive; remote connections. Please see cassandra-env.sh for more info.; (CassandraDaemon:81). 2020-12-13 06:29:02,310 ERROR - [main:] ~ cassandra.jmx.local.port missing; from cassandra-env.sh, unable to start local JMX service.null; (CassandraDaemon:87). . We see there is no ""cassandra-env.sh"" file in atlas and we; tried other ways and didn't find any solution for the above error. . Could you please help us in rectify these problem so that it; will be very helpful to us.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6133:25,install,installed,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6133,2,"['Install', 'install']","['InstallationSteps', 'installed']"
Deployability,Hi Brad. In addition to the `http` entry in the backend filesystems config the `http` filesystem also needs to be defined system-wide. Cromwell's `reference.conf` [defines this already](https://github.com/broadinstitute/cromwell/blob/35_hotfix/core/src/main/resources/reference.conf#L290) so as long as that's being pulled into your configuration and you're not overriding the filesystems you should be set.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-426052644:333,configurat,configuration,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-426052644,1,['configurat'],['configuration']
Deployability,"Hi Brad. To revert to the external cwltool process you can specify this in your configuration: . ```hocon; cwltool-runner {; class = ""cwl.CwltoolProcess""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4183#issuecomment-426012816:80,configurat,configuration,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4183#issuecomment-426012816,1,['configurat'],['configuration']
Deployability,"Hi ChenYong,. I'm closing this issue here on GitHub as Cromwell [30-16f3632 / 30.2](https://github.com/broadinstitute/cromwell/releases/tag/30.2) seems to run fine against a basic MariaDB 5.5.56. I also tried changing the database initialization script to run with-and-without setting `SET GLOBAL sql_mode = 'ANSI_QUOTES';`. If you're still running into problems, can you please create a post over in the [Ask the WDL team](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team) forum? There please provide as many logs and configuration files as possible (without passwords) so that your issue may be reproduced. ---. To test Cromwell with MariaDB I combined the following files and ran them from an `issues_3346/` directory with `docker-compose up`. - `issues_3346/compose/cromwell/app-config/application.conf`; - `issues_3346/compose/cromwell/Dockerfile`; - `issues_3346/compose/mysql/init/init_user.sql`; - `issues_3346/docker-compose.yml`. The files are in this archive: [issues_3346.tar.gz](https://github.com/broadinstitute/cromwell/files/2190721/issues_3346.tar.gz). Cromwell started and connected to the db. I was able to browse to `http://localhost:80` and submit a workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3346#issuecomment-404688457:127,release,releases,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3346#issuecomment-404688457,2,"['configurat', 'release']","['configuration', 'releases']"
Deployability,"Hi Chetana,. in my case, the problem was the variable""file_format"" that I was passing to cutadapt . `cutadapt -f ${file_format}`. in the json file, one of the input was: `""scMeth.file_format"": ""fastq""`, but cutadapt didn't like it. Therefore I have substituted the initial command above with:. `cutadapt -f fastq`. or I have substitute `File file_format` with `String file_fomat` in the first step of the pipeline. Basically I was passing a file but in reality, was just a string for cutadapt. I don't know if this might help. If you type the error from Cromwell maybe I can help you better. Best; Tommaso",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402:405,pipeline,pipeline,405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402,1,['pipeline'],['pipeline']
Deployability,"Hi Cromwell Team; ; I am writing in respect to an issue that I am having with using AWS + Cromwell + MySQL server. Not sure if this is the best place to ask because I’m not sure if the issue is Cromwell specific. It might be related to AWS backend configuration. But I couldn’t figure out the problem and I figured you might be able to provide some insight.; ; I have a docker image that has MySQL server installed. I’m using percorna-server-5.6 specifically because I need to use MySQL5.6. There is a Cromwell task which does the following:; 1. Start mysql server. The first line of the WDL task is literally `service mysql start`; 2. Initialize the database and load Vcf files into the database.; 3. Run some SQL query and perform some analysis; ; And the above Cromwell task need to be run for multiple samples. ; ; So, first I did step 1-3 manually without using WDL just to make sure that I can run multiple MySQL docker container just fine. And it worked. So then the next step is test the whole WDL using LOCAL backend. And it ran fine. But when I submit the same WDL script to AWS Batch, the first task will always succeed and the subsequent tasks will always fail with port connection error because all the containers are connecting to port 3306 and port 3306 is already used. Do you know why it is trying to connect to port 3306? My issue was that it worked when running locally. So, I’m wondering if there’s something with how the docker run command was submitted to AWS Batch or EC2 is configured?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4688:248,configurat,configuration,248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4688,2,"['configurat', 'install']","['configuration', 'installed']"
Deployability,"Hi Everyone,; We have updated Cromwell from version 51 to 82 recently, and changed the following line in Dockerfile:. -----------------------------------------------------------------------; FROM broadinstitute/cromwell:51 --> FROM broadinstitute/cromwell:82; -----------------------------------------------------------------------. Then we had an issue with the parameter scriptBucketName in aws.conf which seems to be a new parameter introduced. So we modified the aws.conf file as follows:. aws.conf; -----------------------------------------------------------------------; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {. concurrent-job-limit = 10000. numSubmitAttempts = 6; numCreateDefinitionAttempts = 6. // Base bucket for workflow executions; root = ${EXECUTION_BUCKET_ROOT_URL}. // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""xxxxxx"". default-runtime-attributes {; queueArn: ${AWS_BATCH_QUEUE}; scriptBucketName: ""${SCRIPT_BUCKET_NAME}""; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }. # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in the cloud.; slow-job-warning-time: 3 hours; }; },; -------------------------------------------------------------------------; Q1. What is scriptBucketName ? I know it says in the documentation that it is where the scripts are stored/written by Cromwell.; For example, if our root bucket is s3://1234-bla-bla-executor/cromwell-execution, should scriptBucketName be ""1234-bla-bla-executor"" ? I understand that we are giving the full path in the root bucket, but is it related or completely unrelated to scriptBucketName ?. It looks like Cromwell is able to create script and reconfigured-script.sh files in the",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6832:22,update,updated,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6832,1,['update'],['updated']
Deployability,"Hi Guys,. This is more of a question/request than a bug report. Apologies if this is not the place to ask. Im trying to run Cromwell with an AWS backend. A number of our workflows make extensive use of very large reference files. To avoid localising the same huge file over and over (wasting time and space) I want to copy these reference files to an additional volume during batch node initialisation and mount to each container (rather than using File arguments I would use a simple String argument to prevent localisation - I appreciate this is a hack). I am already doing this with a different pipeline framework with some success, however it requires the JobDefinition to specify the mount locations between the node(host) and job container. Is it possible to provide additional mount/volume instructions to the aws batch backend in the cromwell.conf?. If this is possible, I cannot see any specific examples in the Cromwell docs. If this is not currently possible, could I request adding the ability to define additional mount points as a feature request??. Kind Regards,; Jon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6334:598,pipeline,pipeline,598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334,1,['pipeline'],['pipeline']
Deployability,"Hi Guys. We are running pipelines through Cromwell on AWS Batch using S3 and have noticed some behaviour we didn't initially expect. We have a task that has quite a significant setup cost. As such we want to process a number of samples through this task rather than instantiating the task for every sample. We can then parallelise this task to process batches of samples. The task takes an Array of structs:. ```; struct Sample {; String id; File file1; File file2; }; ```. The struct is serialised to the task using write_json() and the tool consumes the resulting json before processing the samples one after the other. It is important that the output files can be matched back to their original inputs via the supplied id. The tool outputs a single file per sample to a directory and produces a reports.json that looks like:. ```; [; {; ""id"": ""1""; ""file"": ""outputs/report.txt""; },; ...; ]; ```. I was hoping we could use the read_json() function to parse the output.json into an array of the following struct:. ```; struct Report {; String id; File file; }; ```. and pass this to the next task (or drive a scatter) in the pipeline. However, the File objects parsed in this manner are not resolved to actual task outputs and neither have their address updated or delocalised at the end of the task. Conceptually, it seems like resolving Files within read_* generated structs would be handled the same way as raw File outputs. However, looking at how the delocalisation occurs in the Cromwell task script I understand why this would be difficult to implement. The wdl spec dose not specifically state that File outputs generated this way will be respected but then again it does not say that they won't. a) Could I put forward a feature request for the spec to detect File outputs generated from read_* functions and delocalise them?; b) Or put a note in the wdl/Cromwell spec that File objects generated from read_*() functions may not be detected in the output?. Thanks,; Jon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6795:24,pipeline,pipelines,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6795,3,"['pipeline', 'update']","['pipeline', 'pipelines', 'updated']"
Deployability,"Hi I did try to build DAG for a task from WARP pipeline, and wom failed with an error; https://github.com/broadinstitute/warp/issues/438; Turns out it it no a problem of a pipeline, but possibly a bug in a wom. Best, Eugene",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6499:47,pipeline,pipeline,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6499,2,['pipeline'],['pipeline']
Deployability,"Hi Jeff,. You've built this great Actor system, but it needs to be actors on both ends. The round-robin pool of actors is not an actor system anymore if you cannot pass a Promise/Future/ActorRef to the other side, even if the API/channel capacity is limited. The status response should happen in less than a second, not an hour. We both know that we can have [millions of Actors in Akka/Scala](http://doc.akka.io/docs/akka/snapshot/general/actor-systems.html#What_you_should_not_concern_yourself_with), and the throughput on the Google network is huge. Thus no API limits should be prohibitive. I suggested using Pub/Sub API here:. https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152. And if that's not an option, you can implement the whole Google Genomics Pipeline API super-easy as an ephemeral GCE instance, which really is just becomes a promise/future. I even broke it down in a [post here](https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50), specifically in the paragraph that starts with **_So you might ask what exactly is an Operation resource_**:. https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50. You know my philosophy, always build it yourself to bypass any limitations :). `p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027:784,Pipeline,Pipeline,784,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027,1,['Pipeline'],['Pipeline']
Deployability,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:419,Pipeline,Pipeline,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789,4,"['Pipeline', 'update']","['Pipeline', 'updated']"
Deployability,"Hi Kristian,. I understand, but what you're asking is very possible - see my previous discussion here about creating 1 billion simultaneous connections, and anything that is not accessible can be pre-cached via buckets during idle periods (i.e. nightly):. https://github.com/googlegenomics/utils-java/issues/62#issuecomment-220444203. So you should be able to create your own Pipeline implementation very easily via `gloud create`, [VM metadata startup scripts](https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts) and/or Dataflow Pipelines, and mimic JES:. https://cloud.google.com/sdk/gcloud/reference/compute/instances/create. https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts. https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#applying-transforms-to-process-pipeline-data. If you look at the JES API, you'll notice most of it mirrors the `gcloud` commands and parameters:. https://www.googleapis.com/discovery/v1/apis/genomics/v1alpha2/rest. Again the concepts to speed up searches on dynamically streaming (processed) analysis results has a foundation via inverted indices, which search engines use all the time - I posted a couple of these here:. https://github.com/ga4gh/schemas/pull/253#issuecomment-97525342. https://github.com/ga4gh/schemas/issues/142#issuecomment-55518571. This way your searches are always fresh and would operate without any delay. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605:376,Pipeline,Pipeline,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605,12,"['Pipeline', 'deploy', 'pipeline']","['Pipeline', 'Pipelines', 'deployment-manager', 'pipeline', 'pipeline-data', 'pipelines']"
Deployability,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:799,pipeline,pipeline,799,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443,4,['pipeline'],['pipeline']
Deployability,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:761,pipeline,pipelines,761,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994,4,"['Pipeline', 'pipeline']","['Pipeline', 'pipelines']"
Deployability,"Hi Want to run my pipeline in gcp with nvidia-tesla-a100. Get errors for insert machine. Trake into the code since cromwell set n2-custom machine meanwhile a100 require a2-highgpu-1g. I guess it is not a big change but can extend capbilities. runtime {bootDiskSizeGb: 100; disks: ""/mnt 3000 HDD""; gpuType: ""nvidia-tesla-a100""; gpuCount: 1; nvidiaDriverVersion: ""418.87.00""; zones: [""us-central1-c""]; } (edited) . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6558:18,pipeline,pipeline,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6558,2,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"Hi Will,. I see, but unfortunately I still get an error - I ran the your updated workflow without the Docker portion and specifically sent an `SIGINT`, and it looks like it ended with an error. Below are the steps - hope this does not affect the launch schedule:. ``` Bash; $ cat error_continue.wdl; task hello {; String addressee; command {; echo ""Hello ${addressee}!"" && kill -SIGINT $BASHPID; }; output {; String salutation = read_string(stdout()); }; runtime {; continueOnReturnCode: true; }; }. workflow w {; call hello; }; $; $ java -jar cromwell.jar inputs error_continue.wdl; This functionality is deprecated and will be removed in 0.18. Please use wdltool: https://github.com/broadinstitute/wdltool; {; ""w.hello.addressee"": ""String""; }; $; $ java -jar cromwell.jar inputs error_continue.wdl > error_continue.json; This functionality is deprecated and will be removed in 0.18. Please use wdltool: https://github.com/broadinstitute/wdltool; $; $ java -jar cromwell.jar run error_continue.wdl error_continue.json; [2016-01-31 16:37:25,449] [info] RUN sub-command; [2016-01-31 16:37:25,469] [info] WDL file: error_continue.wdl; [2016-01-31 16:37:25,471] [info] Inputs: error_continue.json; [2016-01-31 16:37:25,989] [info] Slf4jLogger started; [2016-01-31 16:37:26,86] [info] SingleWorkflowRunnerActor: launching workflow; [2016-01-31 16:37:27,345] [info] Running with database db.url = jdbc:hsqldb:mem:748afb13-e3af-4e9d-af14-5c2b3bd209a9;shutdown=false;hsqldb.tx=mvcc; [2016-01-31 16:37:28,247] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 2a89a995-aa89-4172-a5e1-1054cbccd9e0; [2016-01-31 16:37:28,291] [info] WorkflowManagerActor Found no workflows to restart.; [2016-01-31 16:37:28,660] [info] WorkflowActor [2a89a995]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#-896492658])) message received; [2016-01-31 16:37:28,788] [info] WorkflowActor [2a89a995]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwell-system/user/SingleWorkfl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:73,update,updated,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['update'],['updated']
Deployability,"Hi all, . I try to modify & deploy my cloud function. . But I get this error:. ![image](https://user-images.githubusercontent.com/8334979/70418119-bcec1e80-1a9d-11ea-8f13-d6809efe63e7.png). Then I [check my bucket information](https://cloud.google.com/storage/docs/using-requester-pays#enable), the ""requester pays"" is disabled. Also, I try to create a new bucket & deploy a new cloud function. The error remains. So, I am wondering is there any other possibilities that could cause this situation?; (maybe MIS team accidentally change some related settings); I can not deploy any cloud function(s) successfully since early today. Please give me some suggestion, thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5311:28,deploy,deploy,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5311,3,['deploy'],['deploy']
Deployability,"Hi all,. As discussed with @TimothyTickle, @ruchim, @benjamincarlin, @gsaksena, @abaumann, @kshakir, @geoffjentry, and others at the Broad retreat and DSP holiday hackathon, we're putting a proposal for a new feature that reports task call resource utilization metrics to Stackdriver Monitoring API. This serves 2 important goals:. 1) Users can easily plot real-time resource usage statistics across all tasks in a workflow, or for a single task call across many workflow runs, etc. This can be very powerful to quickly determine outlier tasks that could use optimization, without the need for any configuration or code (or any changes to the workflow). It's also much easier than the current state-of-the-art, i.e. parsing task-level monitoring logs. 2) Scripts can easily get aggregate statistics on resource utilization and could produce suggestions based on those. This could provide a path towards automatic runtime configuration based on the models trained with historical data. One could also detect situations like out-of-memory calls and automatically adjust resources according to those. It would also be pretty easy to add logic for estimation of task call-level cost based on the pricing of associated resources. This could provide a long-sought feature of real-time cost monitoring/control (thanks to @TimothyTickle for the suggestion). Monitoring is done using the new ""monitoring action"" for PAPIv2, which currently uses the hard-coded [quay.io/broadinstitute/cromwell-monitor](https://quay.io/repository/broadinstitute/cromwell-monitor) image, built from https://github.com/broadinstitute/cromwell-monitor (I wasn't sure if that code belonged here or in a separate repo). This is advantageous to just using it as a _monitoring_script_, because it removes all assumptions on the ""user"" Docker image (for the task itself). For example, we don't have to assume a particular distribution or presence of Python and its libraries. So it should work exactly the same for any task. Per @geoffj",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510:598,configurat,configuration,598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510,2,['configurat'],['configuration']
Deployability,"Hi all,; I test cromwell with local backend and in my workflow I define:; runtime; {; cpu: 1; memory: ""128 MB""; docker: ""ubuntu:latest""; }; I have docker installed on local server, but I notice the log file shows:; [warn] Local [d023ba19]: Key/s [cpu, memory] is/are not supported by backend. Unsupported attributes will not be part of job executions.; I wonder why local backend does not support cpu and memory limit for the docker? Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413:154,install,installed,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413,1,['install'],['installed']
Deployability,"Hi all,; I wonder if there is a mechanism of global variables setting and importing like workflow importing or application conf including. I just tried to ways:; 1. setting global variables in customized conf which can be used as conf option with cromwell cmd but I don't know how to reference them in the WDL file; 2. put all global variables in a separated workflow WDL file using workflow variables and import them in other WDL files, while it looks like only imported tasks can be called and no way to just use the variables; My purpose is that we have many pipelines and there are some shared variables (a.k.a global variables) and I prefer to reference them in some separated WDL/conf rather than repeat them again and again, and let individual pipeline developer not to modify them by accident and only focus on his pipeline specific variables and reference them by importing. How can I do that in an elegant way? Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4416:562,pipeline,pipelines,562,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4416,3,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Hi all. I've been playing the CWL ResourceRequirement with cromwell under SLURM env.; https://www.commonwl.org/v1.0/CommandLineTool.html#ResourceRequirement. So, if I submit a CWL with; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000; ramMax: 2000; ```; The cromwell will send the command to SLURM with a ridiculously high value of memory; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152000 \; ```; And most likely it will fail because we do not have such a beefy flavor node, since SLURM reads the memory in MB.; ```; sbatch: error: Batch job submission failed: Requested node configuration is not available; ```; More tests like using different units or numbers will lead to similar errors:; Submit; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2GB; ramMax: 2GB; ```; will give me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```; and; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2; ramMax: 2; ```; gives me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152 \; ```; more for; ``` - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000MB; ramMax: 2000MB; ```; it will give; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```. So, it seems cromwell will read the numbers in MB and transfer it in bytes to SLURM. However, SLURM probably needs memory in MB as well. (I assume it is a bug when mapping CWL resource key to WDL). Currently, I manage to use an expression in the config `memoryMin/1000000` to circumvent the issue.; I don't think it is a very major issue nor a complex one, so feel free to fix it whenever. The cromwell version is the latest release, cromwell-34, and please find my working config if necessary. [cromwell.slurm.conf.gz](https://github.com/broadinstitute/cromwell/files/2368103/cromwell.slurm.conf.gz)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4080:643,configurat,configuration,643,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4080,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,"Hi all;; I'm running into a database write error on some systems. Similar to #3584, this does not cause the pipeline to stop, but is quite noisy and presumably affecting the ability to re-run and query run status. A CWL pipeline that triggers this is `somatic` from this set of minimal CWL test sets (https://github.com/bcbio/test_bcbio_cwl) but it appears to be system specific, rather than data specific. The same pipeline works fine on most systems, but only reports this database write error on some. When running we get this message on the completion of tasks:; ```; [2018-05-09 10:16:59,44] [[38;5;1merror[0m] 1bd683a6:prep_samples_to_rec:-1:1: Failure writing to call cache: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; java.sql.BatchUpdateException: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607:108,pipeline,pipeline,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607,3,['pipeline'],['pipeline']
Deployability,"Hi all;; I'm testing out a CWL run (https://github.com/bcbio/bcbio_validation_workflows/tree/master/somatic-giab-mix) with a SLURM backend using file based caching:; ```; [2018-05-02 13:10:20,09] [info] Running with database db.url = jdbc:hsqldb:file:/projects/ngs/oncology/dev/bcbio_validation_workflows/somatic-giab-mix/cromwell_work/persist/metadata;shutdown=false;hsqldb.tx=mvcc; ```; and running into a hash exception at a consistent spot in the pipeline:; ```; [2018-05-02 15:16:51,49] [info] WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f [[38;5;2mbc4644da[0m]: Starting batch_for_variantcall; [2018-05-02 15:16:52,47] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: Unrecognized runtime attribute keys: memoryMax, tmpDirMin, cpuMax, cpuMin, tmpDirMax, outDirMin, memoryMin, outDirMax; [2018-05-02 15:16:55,18] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2mbc4644da[0mbatch_for_variantcall:NA:1]: [38;5;5m'bcbio_nextgen.py' 'runfn' 'batch_for_variantcall' 'cwl' 'sentinel_runtime=cores,1,ram,3839.9999999999995' 'sentinel_parallel=multi-batch' 'sentinel_outputs=batch_rec:resources;description;reference__fasta__base;config__algorithm__variantcaller;reference__snpeff__GRCh37_75;config__algorithm__coverage_interval;genome_resources__variation__train_hapmap;genome_resources__variation__encode_blacklist;metadata__batch;genome_resources__variation__lcr;metadata__phenotype;vrn_file;reference__twobit;config__algorithm__validate;config__algorithm__validate_regions;genome_build;genome_resources__aliases__human;config__algorithm__tools_off;genome_resources__variation__dbsnp;genome_resources__variation__polyx;genome_resources__variation__cosmic;reference__genome_context;analysis;config__algorithm__tools_on;config__algorithm__effects;config__algorithm__variant_regions;genome_resources__aliases__ensembl;config__algorithm__exclude_regions;reference__rtg;genome_resources__variation__train_indels;genome",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:451,pipeline,pipeline,451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['pipeline'],['pipeline']
Deployability,"Hi all;; I've been testing the new release 35 and was especially excited about the removal of the cwltool dependency for pre-processing CWL workflows. Unfortunately the new approach results in pre-processing runtimes that are 10x or more the previous approach. They were less than a minute and are now 10 minutes or more. For an example the `somatic-workflow` CWL directory here: https://github.com/bcbio/test_bcbio_cwl/tree/master/gcp takes 12 minutes with release 35:; ```; [2018-09-28 12:04:23,81] [info] Pre Processing Workflow...; [2018-09-28 12:04:24,00] [info] Pre-Processing /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/somatic-workflow/main-somatic.cwl; [2018-09-28 12:04:45,04] [info] Pre-Processing /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/somatic-workflow/steps/alignment_to_rec.cwl; ../somatic-workflow/steps/alignment_to_rec.cwl:26:3: checking item; ../somatic-workflow/steps/alignment_to_rec.cwl:26:3: Field `class` contains undefined reference to `https://www.dnanexus.com/cwl#InputResourceRequirement`; [2018-09-28 12:05:15,77] [info] Pre-Processing /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/somatic-workflow/wf-alignment.cwl; [2018-09-28 12:05:15,99] [info] Pre-Processing /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/somatic-workflow/steps/prep_align_inputs.cwl; ../somatic-workflow/steps/prep_align_inputs.cwl:26:3: checking item; ../somatic-workflow/steps/prep_align_inputs.cwl:26:3: Field `class` contains undefined reference to `https://www.dnanexus.com/cwl#InputResourceRequirement`; [2018-09-28 12:05:42,85] [info] Pre-Processing /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/somatic-workflow/steps/process_alignment.cwl; ../somatic-workflow/steps/process_alignment.cwl:27:3: checking item; ../somatic-workflow/steps/process_alignment.cwl:27:3: Field `class` contains undefined reference to `https://www.dnanexus.com/cwl#InputResourceRequirement`; [2018-09-28 12:06:10,83] [info] Pre-Processing /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/somati",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4183:35,release,release,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4183,2,['release'],['release']
Deployability,"Hi all;; In testing release 35 with CWL inputs I've also been looking at supporting remote URL references. This is working correctly for GS URLs but not for http URLs. I've put together a test case that demonstrates the problem:. https://github.com/bcbio/test_bcbio_cwl/tree/master/gcp. The `somatic-workflow-http` CWL workflow uses http URLs and doesn't work, while the comparable `somatic-workflow` CWL workflow uses GS URLs referencing the same data and does work. The workflow fails with:; ```; java.io.FileNotFoundException: Cannot hash file https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genom; es/hg19/seq/hg19.fa; ```; when running tasks. The files get downloaded to the input directories but get numerical values instead of the original file names so never seem to sync over and get translated correctly to the workflow; ```; ls -lh cromwell_work/cromwell-executions/main-somatic.cwl/eaa632df-52a8-4aae-826f-647a42fa7145/call-prep_samples_to_rec/inputs/1515144/; total 136K; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 225050424226294657; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 2612405277530248055; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 503001634356675169; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 5802330287039666628; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 5809676514510180826; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 6090832304768530540; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 6105514522473810611; -rw------- 3 chapmanb chapmanb 37K Sep 26 14:07 6807576659333162957; -rw------- 3 chapmanb chapmanb 150 Sep 26 14:07 6853384576121493061; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 7483350933664987331; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 7538690575330349970; -rw------- 3 chapmanb chapmanb 37K Sep 26 14:07 7691692211431528147; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 7783203266940950463; -rw------- 3 chapmanb chapmanb 150 Sep 26 14:07 8389565043859020157; -rw------- 2 chapmanb chapmanb 43 Sep 26",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184:20,release,release,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184,1,['release'],['release']
Deployability,"Hi cromwell developers,. I'm having persistent errors similar to those in #2034 : stderr shows a line like this.; ; bash: path/to/my/cwd/cromwell-executions/my_workflow_name/some_hexadecimal_garbage/call-my_task_name/execution/script: Permission denied. Can you suggest a diagnosis or a workaround for this problem? I've tried to configure cromwell with LSF, and I suspect that's where the issue lies. Here's my complete configuration file. Thanks for your help!. include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 16; ; runtime-attributes = """"""; Int cpu; Int nthreads; Float? memory_kb; """"""; ; submit = """"""; bsub \; -J ${job_name} \; -cwd ${cwd} \; -R rusage[mem=${memory_kb}] \; -n ${nthreads} \; -W ${cpu} \; -o ${out} \; -e ${err} \; ""${script} ""; """"""; ; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3185:421,configurat,configuration,421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3185,1,['configurat'],['configuration']
Deployability,"Hi everyone, . When I use cromwell + PBS backend I found the way that jobs are scheduled has some differences from using PBS alone. For example, I have a pipeline of 2 steps, A-B, where B depends on A. Now I want to submit this pipeline for 2 times which will generate 4 jobs A1 B1 A2 B2. Let's assume the cluster only have resource to run one of the jobs at a time. . When I use PBS alone all of the 4 jobs will be in the queue, at the beginning A1 gets to run and the others waiting. When A1 is done B1, A2 both have a chance to run depending on the priority PBS assigns to them. So the order of the four jobs might be A1-B1-A2-B2 or A1-A2-B1-B2. When I use cromwell + PBS backend cromwell will first send A1 and A2 to the queue without B1 and B2 since they won't be ready to run until A1 and A2 are done. When A1 is done A2 gets to run because it's the only job in the queue while B1 is on its way to the queue. So in this case the order of these jobs can only be A1-A2-B1-B2. This is not big issue when there are only a few pipelines to run. However, when I have, say 100 such pipelines, B1 has to wait until A100 to finish since when A1 finishes A2-A99 are already in the queue waiting and B1 has just set off. This means the finishing time of pipeline1(A1-B1) will be affected a lot by the total number of pipelines submitted to cromwell engine. . Is there any way for cromwell to change this situation (like sending all jobs to the backend without blocking any of them)? I really don't want to wait until all ""A""s to finish to get the first result of submitted pipelines. Hope I have made this problem clear. I have read the documents of cromwell and googled quit a bit but didn't find any solution. . Any help would be appreciated!. Yue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6339:154,pipeline,pipeline,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6339,6,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Hi everyone, this is as far as I can get on my own trying to introduce SQLite support into Cromwell. I have highlighted reasons to support Sqlite on Cromwell elsewhere. This is as far as I could get without help:. + Migration scheme correctly implemented. All the necessary tables with all the correct constraints (foreign key, unique, primary key) are created on startup.; + Updated upstream liquibase in order to allow unique constraints to be defined properly.; + Made sure all the types were converted in SQLite types (TEXT, INTEGER, BLOB etc.); + Updated the testing to understand SQLite types properly. So far so good. Unfortunately the testing does not recognize the foreign key, primary key or unique constraints, even though they are defined (clearly visible in the sqlitebrowser). . Since the testing is just testing, I also decided to run cromwell with a workflow, but that does not work: ; ```; [ERROR] [07/20/2020 14:01:02.134] [cromwell-system-akka.dispatchers.engine-dispatcher-50] [akka://cromwell-system/user/cromwell-service/WorkflowStoreActor/WorkflowStoreEngineActor] Error trying to fetch new workflows; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:1010); at org.sqlite.core.DB.newSQLException(DB.java:1022); at org.sqlite.core.DB.throwex(DB.java:987); at org.sqlite.core.NativeDB.prepare_utf8(Native Method); at org.sqlite.core.NativeDB.prepare(NativeDB.java:134); at org.sqlite.core.DB.prepare(DB.java:264); at org.sqlite.core.CorePreparedStatement.<init>(CorePreparedStatement.java:45); at org.sqlite.jdbc3.JDBC3PreparedStatement.<init>(JDBC3PreparedStatement.java:30); at org.sqlite.jdbc4.JDBC4PreparedStatement.<init>(JDBC4PreparedStatement.java:19); at org.sqlite.jdbc4.JDBC4Connection.prepareStatement(JDBC4Connection.java:35); at org.sqlite.jdbc3.JDBC3Connection.prepareStatement(JDBC3Connection.java:241); at org.sqlite.jdbc3.JDBC3Connection.prepareStatement(JDBC3Conne",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5582:376,Update,Updated,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5582,2,['Update'],['Updated']
Deployability,"Hi folks,; I am running cromwell 36 with AWS batch. Doing the hello world example from the following:. https://aws.amazon.com/blogs/compute/using-cromwell-with-aws-batch/. I am able to submit from the swagger UI and am getting the following erro:. `2018-10-30 00:39:25,929 INFO - jobQueueArn: arn:aws:batch:us-east-2:365166883642:job-queue/GenomicsHighPriorityQue-0c2108973103ca2; 2018-10-30 00:39:25,929 INFO - taskId: wf_hello.hello-None-1; 2018-10-30 00:39:25,929 INFO - hostpath root: wf_hello/hello/bcc91ab0-fd91-41a8-b3e6-cbf091cb511d/None/1; 2018-10-30 00:39:25,965 cromwell-system-akka.dispatchers.backend-dispatcher-229 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(bcc91ab0)wf_hello.hello:NA:1]: Error attempting to Execute; software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: batch.default.amazonaws.com: Name or service not known`. Any idea the source of this error?; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4334:1633,configurat,configuration,1633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4334,1,['configurat'],['configuration']
Deployability,"Hi team,. I try to configure cromwell to run ExomeGermlineSingleSample_v3.1.9.wdl on Slurm, and I follow your guide, but I have an error that ${docker_script} : No such file or directory; /cromwell-executions/ExomeGermlineSingleSample/118135f5-ce0e-437b-9fd2-332dd614bded/call-GenerateSubsettedContaminationResources/execution/script : No such file or directory; I attached the run file; #!/bin/bash; #SBATCH --nodes=1; #SBATCH --time=2:00:00. module load jdk. java -Dconfig.file=/mainfs/wrgl/broadinstitute_warp_development/tutorials/cromwell-slurm_5.config \; -jar /mainfs/wrgl/broadinstitute_warp_development/tutorials/cromwell-85.jar \; run /mainfs/wrgl/broadinstitute_warp_development/warp/ExomeGermlineSingleSample_v3.1.9.wdl \; -i /mainfs/wrgl/broadinstitute_warp_development/tutorials/Exom_test.json. #### Configuration file ###. include required(classpath(""application"")). system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false; }. backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; temporary-directory = ""$(mktemp -d /tmp/tmp.XXXXXX)"". runtime-attributes = """"""; Int runtime_minutes = 60; Int cpu = 1; Int memory_mb = 3900; String? docker; """""". submit = """""" \; 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap ""/bin/bash ${script}""'; """""". submit-docker = """""" \. # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; module load apptainer; if [ -z $APPTAINER_CACHEDIR ];; then CACHE_DIR=$HOME/.apptainer/cache; else CACHE_DIR=$APPTAINER_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; LOCK_FILE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:814,Configurat,Configuration,814,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,1,['Configurat'],['Configuration']
Deployability,"Hi thanks for reporting this issue, could you post your Cromwell configuration ? Specifically the [call caching part of the backend configuration](https://github.com/broadinstitute/cromwell/blob/cea07d69919a609362d2e374888f9ed8c4220564/cromwell.examples.conf#L332) (if any)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393883431:65,configurat,configuration,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393883431,2,['configurat'],['configuration']
Deployability,"Hi there,. I'm new to both WDL and Cromwell and am trying to get an analysis pipeline up and running. I'm using call-caching to speed up my development, so that I don't have to repeat multi-hour steps. However, I'm currently seeing ~8 minute delays for processing cache hits. With multiple steps in serial, this means that nothing in my pipeline starts running till 14 minutes after I start the run. Can you help me fix that?. Thank you for the help!. Happy to provide any more info than the below if that's helpful. I'm running with cromwell 84. Here's the command I'm running `java -Dconfig.file=workflow/cromwell.conf -jar utilities/cromwell-84.jar run workflow/expanse_workflow.wdl`. Here's my configuration (ignore the SLURM part, I'm not using it yet). Potentially important bits:; * I'm running with the local backend.; * I'm using symlink caching so that should be fast, with path+timestamp hash codes so the whole file doesn't need to be read; * I'm using the file based Hsql database. I don't see why that should matter, but maybe it does.; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # only use double quotes!; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; }. ## file based persistent database; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 120000; numThreads = 1; }; }. call-caching {; enabled = true; }. backend {; default = ""Local""; providers { ; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10; run-in-background = true; submit = ""/usr/bin/env ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:77,pipeline,pipeline,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,3,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"Hi there,. Just to make sure there is no duplication of effort: I have a [branch](https://github.com/rhpvorderman/cromwell/tree/sqlite) on which I am working on sqlite support for cromwell. There are some difficulties:; - The liquibase dependency currently installed in cromwell crashes when using sqlite (liquibase 3.6.3) ; - Higher versions of liquibase fix this problem, but cause other problems. Using the newest version causes dependency clashes in the jar.; - Liquibase does not support a way for defining unique constraints when creating tables. Currently all tables get their contstaints added with addUniqueConstraint, however in SQLite constraints have to be defined at table creation. Foreign key constraints can be applied at table creation, so luckily there are no problems here.; - To solve the above problem I have a PR open at liquibase https://github.com/liquibase/liquibase/pull/1059. It may take some time before this is reviewed. Even after liquibase has the necessary support, it will take some time to make sure everything in cromwell works with this newest version of liquibase. So this will take some time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5490:257,install,installed,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490,1,['install'],['installed']
Deployability,"Hi!. First of all, thanks for merging the PR on the cached-copy localization strategy. We have been using at as a patch for about a month now in the LUMC on our SGE cluster. Since our cluster filesystem also interacts with Windows PC's which have a hard-link limit of 1024, the maximum hardlink limit on our cluster is set to 1000. Unfortunately we run into this limit fairly often when using the `cached-copy` strategy. When this happens cromwell will fallback to copying. This is good design, but unfortunately it uses a lot of disk space and takes a lot of time. This PR addresses that issue. When the hard-link limit is reached the hard-link in the cache will be removed. This hard-link will have pointed for example to inode 13820. Because there are plenty of hard-links still pointing to 13820, this inode is not removed, and all the jobs dependent on these files will still function.; Cromwell will copy the required file to the same path in the cache. This means the same path will now point to a different inode (for example: 13835). This inode has only 1 hardlink, so the hardlinking from this file in the cache can start with renewed vigor. Because the same path in the cache can be used, complex additional code in cromwell is not needed. Ideally I could get the maximum hard-link limit that Cromwell uses from the filesystem itself. But I could not find a way to do that. The limit is made to be configurable with a default of 950. Take the example case of having 9000 jobs that require the 3.8 GB reference genome, using a hard-link limit of 1000. Before this PR:. strategy | result; --- | ---; hard-link | fails when cromwell-executions is on a different disk. Allows up to 1000 hardlinks if on the same disk.; cached-copy | Allows up to 1000 hard-links regardless of which disk the original file is stored on.; soft-link | does not work with containers and is excluded.; copy | since hard-link or cached-copy can only handle a 1000 links, copy will be used 8000 times. Using 8000*3,8 =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5043:114,patch,patch,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043,1,['patch'],['patch']
Deployability,"Hi!. I'm having some trouble request s3 objects that are outside my current region (I get a Status Code: 301). . Backend: AWS Batch; Filesystem: S3; Region : `ap-southeast-2`. I'm attempting to run a small genomics pipeline that is trying to request some of the [`broad-reference` open data set](; https://registry.opendata.aws/broad-references) on AWS S3. I can see that open data set exists in `us-east-1`. . Specifically, I'm requesting (`s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta`) and I'm receiving the same error 5 times.; ```; [2019-03-12 11:27:21,50] [error] WorkflowManagerActor Workflow 434834fb-cb24-4bd2-ba44-8a1c929b11f5 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null). 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:217); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:187); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:182); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4731:215,pipeline,pipeline,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731,1,['pipeline'],['pipeline']
Deployability,"Hi, . I downloaded the jar file from the GitHub release page:. > wget https://github.com/broadinstitute/cromwell/releases/download/34/cromwell-34.jar. sha256sum results in the hash mentioned by @Horneth. Actually I have two java versions on my system:. ```; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode); ```; and. ```; openjdk version ""1.8.0_141""; OpenJDK Runtime Environment (build 1.8.0_141-b16); OpenJDK 64-Bit Server VM (build 25.141-b16, mixed mode); ```. It fails with the first one, however I can launch the server with the openjdk version.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-420540300:48,release,release,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-420540300,2,['release'],"['release', 'releases']"
Deployability,"Hi, . I was wondering if I could submit a patch to Cromwell to support Broad's internal queue UGER, . Currently, the GridEngine job submission backend (qsub) for Cromwell doesn't take a queue argument (-q) nor a project id (-P); both of these arguments are necessary for my group to be able to submit jobs to UGER queue successfully. . I realize that I'm submitting this patch under 0.19_hotfix and not dev branch; and also not sure how the Cromwell team wants to organize the application.conf file to accommodate UGER arguments. . So putting this forth initial pull request to get feedback/instructions to add this patch, . Thanks,; Paul",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/875:42,patch,patch,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/875,3,['patch'],['patch']
Deployability,"Hi, . I wonder is it possible to increase frequency of check for rc file? Sometimes I see a gap of several minutes (3-5) between the time of rc generation in one task and the start of subsequent (dependent) task. I would like to increase the frequency of that check to at least 1 min or 30sec. . If I understand that thread correctly (https://github.com/broadinstitute/cromwell/issues/4877) there is no simple configuration option for that setting. . Best, Eugene",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7144:410,configurat,configuration,410,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7144,1,['configurat'],['configuration']
Deployability,"Hi, ; can we define Directory in workflow input section? I met error ""Cannot coerce expression of type 'String' to 'Directory'"".; my wdl file, cromwell 59 is used: ; ```; version development. workflow pipeline {; input {; Directory index_dir = ""/home/danny.gu/PycharmProjects/nestcmd/tests/testdata/index/""; }. call getFastqInfo{}. scatter (each in keys(getFastqInfo.fastq_info)) { ; String sample = each; File read1 = getFastqInfo.fastq_info[each][0][0]; File read2 = getFastqInfo.fastq_info[each][1][0]. call fastp {; input: ; read1 = read1,; read2 = read2; }. call salmon {; input: ; indexDir = index_dir,; read1 = fastp.out1,; read2 = fastp.out2; }. }. call MergeTranscriptTPM {; input: ; quants = salmon.outDir; }. call MergeTranscriptCount {; input: ; quants = salmon.outDir; }. meta {; name: ""PipelineExample""; desc: ""This is a simple pipeline for fast gene/transcript quantification. workflow = [fastq -> Fastp -> Salmon]""; author: ""unknown""; source: ""source URL for the tool""; version: ""unknown""; }. output{; Array[File] fastp_out1 = fastp.out1; Array[File] fastp_out2 = fastp.out2; Array[File] salmon_transcript = salmon.transcript; Array[Directory] salmon_outDir = salmon.outDir; File MergeTranscriptTPM_result = MergeTranscriptTPM.result; File MergeTranscriptCount_result = MergeTranscriptCount.result; }. }. task getFastqInfo{; input {; Array[Directory]? fastq_dirs; Array[File]? fastq_files; String r1_name = '(.*).read1.fastq.gz'; String r2_name = '(.*).read2.fastq.gz'; String docker = 'gudeqing/getfastqinfo:1.0'; }. command <<<; set -e; python /get_fastq_info.py \; ~{if defined(fastq_dirs) then ""-fastq_dirs "" else """"}~{sep="" "" fastq_dirs} \; ~{if defined(fastq_files) then ""-fastq_files "" else """"}~{sep="" "" fastq_files} \; -r1_name '~{r1_name}' \; -r2_name '~{r2_name}' \; -out fastq.info.json; >>>. output {; Map[String, Array[Array[File]]] fastq_info = read_json(""fastq.info.json""); File fastq_info_json = ""fastq.info.json""; }. runtime {; docker: docker; }. ; }; ; task fastp{; i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6501:201,pipeline,pipeline,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501,3,"['Pipeline', 'pipeline']","['PipelineExample', 'pipeline']"
Deployability,"Hi, I added a recipe for cromwell to the bioconda channel:; https://github.com/bioconda/bioconda-recipes/tree/master/recipes/cromwell. Updating it is currently a manual affair, mostly consisting of updating the package source, version number, and hash. It would be nice if Travis could update the recipe automatically upon successful builds of new tagged releases. We do this in our viral-ngs recipe, and render a [jinja2 template](https://github.com/broadinstitute/viral-ngs/tree/master/packaging/conda-recipe) based on conda requirement files, though we [publish](https://github.com/broadinstitute/viral-ngs/blob/master/travis/deploy.sh) to a separate channel on anaconda.org, and not bioconda. Is automatic deployment of a cromwell conda package something the team would support?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2258:286,update,update,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258,4,"['deploy', 'release', 'update']","['deploy', 'deployment', 'releases', 'update']"
Deployability,"Hi, I see you've indeed created a service account and gotten a json file, but I'm not seeing how you're passing it to Cromwell.; Your configuration uses `application_default` as authentication mode, and you are logged in using your personal gmail it seems.; Did you use this json in any way ?. To use the service account in Cromwell you'd want to either; 1 - Recommended) Change your configuration to use the service account instead of application default; You can see how to do that [here](https://cromwell.readthedocs.io/en/develop/backends/Google/); It is slightly outdated, instead of `pem-file` use `json-file` and the path to your json.; 2) You can keep application default and use [`gcloud auth activate-service-account`](https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account) to authenticate as the service account on your machine. Also could you print the result of `gcloud auth list` ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-392038451:134,configurat,configuration,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-392038451,2,['configurat'],['configuration']
Deployability,"Hi, I updated this patch with suggestions made by @mcovarr and also updated README.md documenting the new SGE backend optional parameters. Let me know what you guys think!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/875#issuecomment-222772287:6,update,updated,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/875#issuecomment-222772287,3,"['patch', 'update']","['patch', 'updated']"
Deployability,"Hi, can someone please clarify on LSF support?; It was introduced in 0.19 hotfix,... then it came back again.; But it does not seem to be in the main branch e.g. in the current development branch.; Apologies if I've missed something obvious.; Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1667:74,hotfix,hotfix,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1667,1,['hotfix'],['hotfix']
Deployability,"Hi,. I am wondering if there was progress made on that issue? . Running GATK pipelines uses a lot of disk space for intermediate (bam) files, which is problematic for large cohorts. It seems that removing those files before the pipeline complete would break the Cromwell cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3881#issuecomment-620049532:77,pipeline,pipelines,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881#issuecomment-620049532,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Hi,. I encountered the same problem randomly. In a scattered task, some attempts passed while some attempts failed. ; As you mentioned, I think the script was not locally available on the worker at the moment of mounting, due to a network problem.; It would be better if Cromwell can re-try on this. . > When this occurs it is because the fetch_and_run.sh script is not available on the worker nodes of the batch compute environment so when docker mounts that it mounts as a directory because there is no file. Possible causes that I can think of: 1. The script is not available in the S3 bucket you used for the genomics workflow core setup 2. When you ran the Cromwell install you didn't use the exact same namespace that you used for the genomics workflow core so the required scripts are not available.; > […](#); > On Sat, Sep 19, 2020 at 9:26 AM openbioinfomatics for more people who need it ***@***.***> wrote: version: v53 backend: aws [image: image] <https://user-images.githubusercontent.com/45682016/93668392-8d535380-fabe-11ea-870e-36786c6c3d9d.png> i think this part code may go wrong. mount file indeed. [image: image] <https://user-images.githubusercontent.com/45682016/93668410-a1975080-fabe-11ea-9571-b7ce8b9080ef.png> — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5872>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EOOFWDEMRNZCUXJ5CDSGSWPNANCNFSM4RTAXSGA> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410:671,install,install,671,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410,1,['install'],['install']
Deployability,"Hi,. I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/. If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. . I have checked Cromwell documents and some materials from AWS, as well as issue #4323. But none of them works for me. Thank you in advance for any suggestions.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974:181,pipeline,pipeline,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974,2,['pipeline'],['pipeline']
Deployability,"Hi,. I just moved to a new cluster (no sudo) and try to run a WDL script with Cromwell-31.; Everything worked fine on my previous cluster (same Cromwell / WDL / Java versions and same script). After creating the wdl script, I validated it and generated a JSON file (with wdl-0.14), no problem. After running `java -jar cromwell-31.jar run my_script.wdl -i my_script.JSON` the workflow stops, outputting `Permission denied` for the program I call in my .wdl script (which is called from the 'cromwell_executions' folder during the process). I changed the permission to 777 for this program located in my `/usr/bin`, but still the same issue. The program, once located in the 'cromwell_executions' folder, loses the execution permission for the owner.; Same issue no matter if I submit a PBS job or run it interactively. Is there anything to mention in a cromwell configuration file or something to tune in the cluster?. Thanks !",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3500:862,configurat,configuration,862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3500,1,['configurat'],['configuration']
Deployability,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:23,update,updated,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310,4,"['Install', 'configurat', 'release', 'update']","['Installing', 'configuration', 'release', 'updated']"
Deployability,"Hi,. Running a workflow on WSL/Ubuntu 20.04 using conda-installed Cromwell:. `cromwell run ngs-ubuntu-20-04/iletisim/warp/pipelines/broad/dna_seq/germline/single_sample/exome/local_newGCP_ExomeGermlineSingleSample_deneme6_bcftools.wdl -i ngs-ubuntu-20-04/iletisim/json/S736Nr1.json -o ngs-ubuntu-20-04/iletisim/json/options2.json`. Getting the error:. ```; [2023-02-04 08:55:00,61] [info] Running with database db.url = jdbc:hsqldb:mem:bc9ad7e3-efc7-4f37-aecb-b283b104cbcd;shutdown=false;hsqldb.tx=mvcc; [2023-02-04 08:55:06,54] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2023-02-04 08:55:06,55] [info] [RenameWorkflowOptionsInMetadata] 100%; [2023-02-04 08:55:06,64] [info] Running with database db.url = jdbc:hsqldb:mem:a487ea75-b617-4523-a254-d0e694e68ff9;shutdown=false;hsqldb.tx=mvcc; [2023-02-04 08:55:06,92] [info] Slf4jLogger started; [2023-02-04 08:55:07,18] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-b625dba"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2023-02-04 08:55:07,22] [info] Metadata summary refreshing every 2 seconds.; [2023-02-04 08:55:07,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2023-02-04 08:55:07,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2023-02-04 08:55:07,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2023-02-04 08:55:07,63] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2023-02-04 08:55:07,64] [info] SingleWorkflowRunnerActor: Version 34-unknown-SNAP; [2023-02-04 08:55:07,65] [info] SingleWorkflowRunnerActor: Submitting workflow; [2023-02-04 08:55:07,68] [info] Unspecified type (Unspecified version) workflow 48f62f22-25fe-4f0f-b5fe-21191f035abd submitted; [2023-02-04 08:55:07,72] [info] S",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:56,install,installed,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,2,"['install', 'pipeline']","['installed', 'pipelines']"
Deployability,"Hi,. Sorry for submitting an issue here but I'm consistently getting a ""Something has gone wrong"" error trying to log in to your Jira. I'm hoping someone can offer some guidance for an issue I'm having running a CWL workflow with Cromwell on GCP. I'm using bcbio to generate CWL to do joint calling. This worked fine when I tested it with a single sample to shake out any issues with the pipeline. However when scaling up to a 20 sample batch there's an issue with the get_parallel_regions_jointvc step. This step appears to be localizing multiple copies of the reference genome data (one for each sample) to the same disk. This really blows up the storage requirements as the number of samples increase and ends up exhausting the storage allocated to the worker instance. Is this expected behaviour or is there some kind of configuration I'm missing that would avoid this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5131:388,pipeline,pipeline,388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5131,2,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:445,update,updated,445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,4,"['Install', 'configurat', 'release', 'update']","['Installing', 'configuration', 'release', 'updated']"
Deployability,"Hi,. _I'll manually synchronise this issue with Jira, I've raised it here as I think it has better exposure, might be useful as a reference and I'm going to reference it from a different issue: https://broadworkbench.atlassian.net/browse/BA-6172_. I'm trying to get call-caching working for my workflows, and having some trouble identifying a config that will work for the following requirements:. - Using containers (both Singularity and Docker); - Initial localisation strategy: `[hard-link, cached-copy]`; - Local SFS environment; - My input files can be fairly large (~250GB per Bam with up to 16 Bams). . If I can get this working, I'll happily document and update the CallCaching documentation page with what I've found. ## Background information. Version: Cromwell-47. Documentation:; - https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; - https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options; - https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem. Cache duplication strategies:; - `hard-link`; - `soft-link` - This strategy is not applicable for tasks which specify a Docker image and will be ignored.; - `copy`; - ~`cached-copy`~ - This is non-cache duplication strategy. Cache hashing strategies:; - `file` - (default) computes an md5 hash of the file content. [Code: `tryWithResource(() => file.newInputStream) { DigestUtils.md5Hex }`]; - `path` - computes an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"".; - `path+modtime` - compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. [Code: `md5Hex(file.toAbsolutePath.pathAsString + file.lastModifiedTime.toString)`]. Other caching options:. - `system.file-hash-cache` - Prevent repeatedly requesting the hashes of the same files multiple times. - `backend.providers.Local.caching.check-sibling-md5` - will check if a sibling file with t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:663,update,update,663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['update'],['update']
Deployability,"Hi,. any update here? I will be really interested in these answers, as well. Thanks a lot!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5280#issuecomment-553954232:9,update,update,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280#issuecomment-553954232,1,['update'],['update']
Deployability,"Hi,; ### TL;DR. **Cromwell should allow for the configuration of Docker resource / environment flags at run-time.**. ---. I have a use-case where I'd like to run Cromwell jobs in a cluster environment via Docker Swarm. Since Swarm doesn't require any additional configuration outside of standard `docker run` commands, it's trivial to distribute Cromwell jobs across Swarm nodes. However, Swarm provides a series of [filters](https://github.com/docker/swarm/tree/master/scheduler/filter) and constrains that control how the scheduler distributes containers to nodes. For example, I might be interested in limiting the execution of a Cromwell job to a specific region / datacenter. This requires you to specify filters in the `docker run` command with the environment flag, `-e`. For example, to run a container on Swarm nodes that run in the `us-east` region:. ```; › docker run -d --name my_image -e constraint:region!=us-east* my_container; ```. Obviously, this configuration should _not_ be managed in the WDL document. Instead, it would be great for the Cromwell command-line tool and REST API to support additional runtime options for specifying Docker environment variables. For example:. ```; › cromwell run --docker-env ""constraint:region!=us-east*"" my_workflow.wdl -; ```. > Hint: Docker supports daemon labels. In the above case, the workflow would; > execute on a Swarm node whose Docker daemon that was started with:; > ; > ```; > docker daemon --label region=us-east; > ```. As for the API, the POST action to `/api/workflows/:version` would allow for multiple Docker env strings. The other feature I would like to request is translating `memory` and `cpu` configuration options (at the task level) to Docker via `--memory` and `--cpuset-cpus` `docker run` flags, respectively. These options are currently only used for the JES backend, but it seems as though they can also be used for the Local backend if Docker is specified. So, to summarize:; 1. Allow Docker `-e` flags to be specifie",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/375:48,configurat,configuration,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/375,3,['configurat'],['configuration']
Deployability,"Hi,; ; I'm experiencing the same problem on AWS Batch. My workflow has 2 subworkflows. Even I don't change any part of my workflow/subworkflow, the caching only works for the first task in subworkflows. The subsequent tasks cannot be recognized by hashing. I guess this is because the subworkflow id is also involved in the task inputs, so it change hash. . Does anyone have any update or workaround for this problem? Thank you in advance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4699#issuecomment-716072581:379,update,update,379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4699#issuecomment-716072581,1,['update'],['update']
Deployability,"Hi,; I have a local Singularity mirror file named qc.sif. I did not find any examples of configuring the local .sif file into the cromwell.example.conf file in the documentation you gave me. I checked some configurations and failed. I would like you to give an example of the configuration and a short runtime section of the.wDL file. Thank you very much!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6685:206,configurat,configurations,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685,2,['configurat'],"['configuration', 'configurations']"
Deployability,"Hi,; I run cromwell for warp pipeline using slurm HPC and apptainer, and I got errors of localization:. (i) hard-link try to bind the cwd with wrong name, the real one has a suffix (.tmp); (ii) copy and cashe-copy have error: file-name too long (I can see the generated file path has loops, that makes the file name too long). B.W.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7097:29,pipeline,pipeline,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7097,1,['pipeline'],['pipeline']
Deployability,"Hi,; I'm seeing a number of compilation errors with head dev etc. I've attached the output from debug. Scala version; scala-2.12.0-1.noarch. Java version; java 10.0.1 2018-04-17; Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10); Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10.0.1+10, mixed mode). CentOS Linux release 7.5.1804 (Core). Error output attached. Any thoughts ? Could be something obvious as I'm new to scala.; Thanks. [comp.log](https://github.com/broadinstitute/cromwell/files/2109138/comp.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3791:318,release,release,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3791,1,['release'],['release']
Deployability,"Hi,; Is there any update on this PR? Can you give me an approximate timeline; for looking into this PR for review and integration?. Thanks,; Vanaja. On Mon, Jul 15, 2019 at 9:34 AM Adam Nichols <notifications@github.com>; wrote:. > Hi @vanajasmy <https://github.com/vanajasmy> and thanks for your; > contribution.; >; > Codecov is a nice-to-have, we report it as a useful indicator but don't; > mandate that every single PR continue a monotonic march towards 100%. The; > real measure we care about is a matter of judgment - i.e. does all; > functionality have reasonable tests, and does critical functionality have; > exhaustive tests.; >; > In order to set expectations, it may be a bit before we have cycles to; > review this PR. Reviewing does take a substantial team effort and has to be; > included into the schedule alongside other tasks.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATS73U3ASS2XEBHYMJLP7SRI3A5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ6H4XA#issuecomment-511475292>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ALILATQDJNZOYLMFIMT5C5TP7SRI3ANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852:18,update,update,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852,2,"['integrat', 'update']","['integration', 'update']"
Deployability,"Hi-. The HuBMAP consortium has been implementing some workflows in CWL and running these via `cwltool` -- we're quite interested in storing the provenance information for a workflow run in Research Object format. This would include the inputs and outputs for a certain run, in addition to (a normalized version of) the workflow itself. This is already implemented in `cwltool` and accessible through its `--provenance` flag; is anything like this planned for Cromwell?. Some of the HuBMAP tissue mapping centers are interested in or have been using pipelines written in WDL (e.g. [ENCODE's ATAC-seq pipeline](https://github.com/ENCODE-DCC/atac-seq-pipeline), and we would like to support these without giving up the ability to store workflow run provenance in a standard format. Is anything like this planned for Cromwell? I didn't see anything in the issue/forum/PR searches I've been doing. Thank you!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5052:549,pipeline,pipelines,549,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5052,3,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Hi. Call caching is turned off by default, did you turn it on in your configuration? Also call caching will require the use of a MySQL-like database to preserve caching information between runs. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3740#issuecomment-395410031:70,configurat,configuration,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3740#issuecomment-395410031,1,['configurat'],['configuration']
Deployability,Hi. I forgot to add this to the changelog when making the pr #4815 . Can this be retroactively be added to the changelog on the releases page as well? Thanks!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4878:128,release,releases,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4878,1,['release'],['releases']
Deployability,"Hi. I have been using WDL/Cromwell/Terra for over a year. It works great for ""simple"" pipeline. It has saved me a lot of time and money. I need to run something more complicated. I want to use the scatter gather pattern. I wrote a simple workflow to figure out how to implement scatter-gather. My scatter task works as expected however, my gather task does not. I have each of my tasks in separate files. This made it easier to debug my task separately. The only way I could iterate over an Array was to set the first line of my wdl file to be 'version 1.0' and use the following syntax. ```; version 1.0; workflow loopWorkflow {; call loopTask; }. task loopTask {; Array[String] csvParts = [""a"", ""b"", ""c""]; Int n = 123. command <<<; set -euxo pipefail. exec 2>&1; echo ""shell: $SHELL""; $SHELL -h. echo ""AEDWIP The value of N should be 123 n = ~{n}"". for part in ~{sep=' ' csvParts}; do; echo ""AEDWIP $part AEDWIP""; done. echo ""\n\n********* create a bashArray""; bashArray=('~{sep=""' '"" csvParts}'); for part in ""${bashArray[@]}""; do; echo ""bashArray loop aedwip $part aedwip""; done. ls -l >results.csv; >>>. output {; File results_csv='results.csv'; }; }; ```. setting the version to 1 prevents me from being able to import my loopTask into my workflow. ```; [2023-03-20 19:23:37,02] [info] MaterializeWorkflowDescriptorActor [c74751ef]: Parsing workflow as WDL draft-2; [2023-03-20 19:23:37,26] [info] WorkflowManagerActor: Workflow c74751ef-5494-46d1-8064-25e7d4334405 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; ERROR: Finished parsing without consuming all tokens. version 1.0; ^; ```. Also when I set the version to 1.0 I am no longer able to use womtool-85 to generate template input. . Also I can not use cromwell-85 run with --inputs with version 1. comments and suggestions appreciated. Kind regards. Andy",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7093:86,pipeline,pipeline,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7093,1,['pipeline'],['pipeline']
Deployability,"Hi; We are trying to setup the cromwell + wdl for genomic analyses at the [National Computational Infrastructure HPC facility](https://nci.org.au/systems-services/peak-system/raijin/) in Australia. This HPC runs bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967:325,configurat,configuration,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967,2,['configurat'],['configuration']
Deployability,"Hi;; I'm running Cromwell 34 and hitting an issue when reading from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResults",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4023:593,pipeline,pipeline,593,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023,1,['pipeline'],['pipeline']
Deployability,Highly related to the [summarizer and worker Cromwell](https://github.com/broadinstitute/cromwell/issues/4781) issue and probably a better place to start since this is our existing configuration.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4780#issuecomment-478984741:181,configurat,configuration,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4780#issuecomment-478984741,1,['configurat'],['configuration']
Deployability,"Hi！; I am glad to see this issue, and I have also tried using PBS as the backend to run it. But I'm not very good at it.; Can you show me how the configuration file for cromwell is defined when using PBS as the backend?; Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-2105600521:146,configurat,configuration,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-2105600521,2,['configurat'],['configuration']
Deployability,"Hm. This looks like a conf bug on our side, but [is your config file importing application.conf](https://cromwell.readthedocs.io/en/stable/tutorials/ConfigurationFiles/#creating-your-first-configuration-file)? That file contains other overrides that cromwell should have over the default `akka` configuration. The bug here is that application.conf is only supposed to contain overrides, while reference.conf should contain newly defined resources. Since the `services` block are cromwell's services, they should be newly defined in reference.conf. That would then allow anyone who accidentally doesn't pick up our application.conf to *at least* have the reference `services`, plus the original `akka` values with degraded cromwell performance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577#issuecomment-457755274:149,Configurat,ConfigurationFiles,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577#issuecomment-457755274,3,"['Configurat', 'configurat']","['ConfigurationFiles', 'configuration', 'configuration-file']"
Deployability,Hmm yeah you're right. Scrolling up it looks like this ticket was referencing hotfix and not develop. And yeah we're not even trying to load this key on develop so Cromwell certainly shouldn't fail for its absence.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224691653:78,hotfix,hotfix,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224691653,1,['hotfix'],['hotfix']
Deployability,"Hmm, ok, sounds like the documentation might need an update - otherwise, can you double check that you can connect to the mysql instance from outside of its docker container (ie running `mysql -u cromwell -p` from where Cromwell will run instead of where `mysql` is running?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-373069972:53,update,update,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-373069972,1,['update'],['update']
Deployability,"Hmmm, still stuck on this - any updates from your guys' end? I tried cloning and resubmitting, still getting the same error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-840874050:32,update,updates,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-840874050,1,['update'],['updates']
Deployability,Hopefully fixes the release of wdltool as part of the release process,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2393:20,release,release,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2393,2,['release'],['release']
Deployability,Horicromtal engine upgrade Centaur testing with docker-compose.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4812:19,upgrade,upgrade,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4812,1,['upgrade'],['upgrade']
Deployability,"Horicromtal wants, please see also (and give preference to) the issue for our [existing configuration](https://github.com/broadinstitute/cromwell/issues/4780).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4781:88,configurat,configuration,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4781,1,['configurat'],['configuration']
Deployability,Horizontal integration test should confirm horizontality has been tested,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4783:11,integrat,integration,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4783,1,['integrat'],['integration']
Deployability,Hotfix -- configuring *optional* WF restarts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/889:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/889,1,['Hotfix'],['Hotfix']
Deployability,Hotfix -- configuring restarts,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/841:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/841,1,['Hotfix'],['Hotfix']
Deployability,Hotfix 30.1 cherry picks,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3081:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3081,1,['Hotfix'],['Hotfix']
Deployability,Hotfix closes #3190,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3194:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3194,1,['Hotfix'],['Hotfix']
Deployability,Hotfix edition of #5591,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5593:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5593,1,['Hotfix'],['Hotfix']
Deployability,Hotfix edition of https://github.com/broadinstitute/cromwell/pull/4927,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4931:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4931,1,['Hotfix'],['Hotfix']
Deployability,Hotfix edition: Add valid hostname to DockerHubFlow,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3182:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3182,1,['Hotfix'],['Hotfix']
Deployability,Hotfix for #856 - switch to using ephemeral JES pipelines,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/860:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/860,2,"['Hotfix', 'pipeline']","['Hotfix', 'pipelines']"
Deployability,Hotfix for Cromwell 51 [BA-6475],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5544:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5544,1,['Hotfix'],['Hotfix']
Deployability,Hotfix support for public http-based imports,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2734:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2734,1,['Hotfix'],['Hotfix']
Deployability,Hotfix version,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3828#issuecomment-401064686:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3828#issuecomment-401064686,1,['Hotfix'],['Hotfix']
Deployability,Hotfix version: #4157,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4156#issuecomment-424930302:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4156#issuecomment-424930302,1,['Hotfix'],['Hotfix']
Deployability,Hotfix worthy?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5050#issuecomment-506500477:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5050#issuecomment-506500477,1,['Hotfix'],['Hotfix']
Deployability,Hotfix: Move GCS uploads to async requests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/779:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/779,1,['Hotfix'],['Hotfix']
Deployability,How about rolling your two new fields up with @Horneth's new field idea and making a composite object that has all the caching stuff in one logical grouping that only increases arity by 1 instead of 3? :smile:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/585#issuecomment-198528803:10,rolling,rolling,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/585#issuecomment-198528803,1,['rolling'],['rolling']
Deployability,How will The Great Horizontaling happen? Will we release Cromwell 4X and then sometime later change the configuration of that same version to be horizontal? Or will we release Cromwell 4X leaping into horizontal for the first time? Or different scenarios for different environments?. Whichever paths to horizontal are supported should have integration tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4800:49,release,release,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4800,4,"['configurat', 'integrat', 'release']","['configuration', 'integration', 'release']"
Deployability,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:130,configurat,configuration,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885,10,['configurat'],['configuration']
Deployability,"I actually only ever got this issue once, and that's it. I am currently re-running the same WDL pipeline and it is working fine. But if you send me a new JAR, I am more than happy to run that from now on and report back if I encounter the same issue again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760328444:96,pipeline,pipeline,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760328444,1,['pipeline'],['pipeline']
Deployability,I added a commit to this branch with some updates to the `README.md` which gives a scatter example,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/170#issuecomment-137723052:42,update,updates,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/170#issuecomment-137723052,1,['update'],['updates']
Deployability,"I added a test to `WdlFileToWomSpec`. However, I don't know how to set the configuration flag `wom-parse.convert-nested-scatter-to-subworkflow` inside of it. I can do it manually, and it works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511051253:75,configurat,configuration,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511051253,1,['configurat'],['configuration']
Deployability,"I added the `concurrent-job-limit` to the `reference.conf` and I will add it to the [Configuration draft on the WDL website](http://gatkforums.broadinstitute.org/dsde/discussion/8687/how-to-configure-cromwell), tracked in [DSDE-docs #1524](https://github.com/broadinstitute/dsde-docs/issues/1524).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191:85,Configurat,Configuration,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191,1,['Configurat'],['Configuration']
Deployability,"I agree with Lee here. I think we could do much better at getting people rolling rather than pointing them at a mega-file inside of our source tree. For example -- slimming down what a user needs to have in their conf file, and also providing template conf files for common configurations (SGE, JES, Local, etc). This issue needs more refinement before being ready for development",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178:73,rolling,rolling,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178,2,"['configurat', 'rolling']","['configurations', 'rolling']"
Deployability,"I also encountered the udocker-singularity route in the discussion on cwltool singularity integration. Maybe it is an idea to take a closer look on the udocker-singularity implementation as a starting point for workflow tool singularity usage. . Or maybe not, because you will lose HPC friendly singularity features this way!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310052:90,integrat,integration,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310052,1,['integrat'],['integration']
Deployability,I also hit this while experimenting with using singularity with cromwell. Just replicating https://github.com/kundajelab/atac-seq-pipeline/blob/master/docs/tutorial_local_singularity.md locally. Get the error with cromwell-37 and with a fresh build of develop branch. Works with cromwell-36.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464943989:130,pipeline,pipeline,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464943989,1,['pipeline'],['pipeline']
Deployability,"I also saw this problem. The VM is not a preemptible and I'm using Cromwell v32. There's a lot of shards spending 10 minutes in ""Waiting for quota"" when this problem happens. The instance that gives PAPI Error Code 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.Batching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:850,pipeline,pipelines,850,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,3,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipelines']"
Deployability,"I am constantly having issues when I run docker-compose of cromwell (I use https://github.com/broadinstitute/cromwell/tree/develop/scripts/docker-compose-mysql where I provide my configutation). Even though I do not have docker-user in my application.conf file when I run my pipelines, I get:; ```; cromwell_1 | [ERROR] [05/20/2017 12:57:46.015] [cromwell-system-akka.dispatchers.engine-dispatcher-22] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-f7fdd305-d137-4128-bf68-7c39fd6c834b/WorkflowInitializationActor-f7fdd305-d137-4128-bf68-7c39fd6c834b/Local] Error parsing generated wdl:; cromwell_1 | task submit {; cromwell_1 | ; cromwell_1 | String job_id; cromwell_1 | String job_name; cromwell_1 | String cwd; cromwell_1 | String out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | /bin/bash ${script}; cromwell_1 | }; cromwell_1 | }; cromwell_1 | ; cromwell_1 | ; cromwell_1 | task submit_docker {; cromwell_1 | ; cromwell_1 | String job_id; cromwell_1 | String job_name; cromwell_1 | String cwd; cromwell_1 | String out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | ; cromwell_1 | String docker_cwd; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | ; cromwell_1 | docker run \; cromwell_1 | --rm -i \; cromwell_1 | ${""--user "" + docker_user} \; cromwell_1 | --entrypoint /bin/bash \; cromwell_1 | -v ${cwd}:${docker_cwd} \; cromwell_1 | ${docker} ${script}; cromwell_1 | ; cromwell_1 | }; cromwell_1 | }; cromwell_1 | java.lang.RuntimeException: Error parsing generated wdl:; cromwell_1 | task submit {; cromwell_1 | ; cromwell_1 | String job_id; cromwell_1 | String job_name; cromwell_1 | String cwd; cromwell_1 | String out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | /bin/bash ${script}; cromwell_1 | }; cromw",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284:275,pipeline,pipelines,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284,1,['pipeline'],['pipelines']
Deployability,"I am developing a pipeline and want to share google storage buckets for genome data (bowtie2 index tar ball and other big files) with users (Google authenticated) but want users to pay for the network traffic to download genome data. My pipeline works fine if storage bucket for genome data is set as ""owner pays"". But if I set it ""requester pays"" then I get the following error `BadRequestException: 400 Bucket is requester pays bucket but no user project provided`. I googled it and found that [`gsutil` must use JSON API (not CLI)](https://cloud.google.com/storage/docs/requester-pays) for ""requester pays"" buckets. Is there any plan to support ""requester pays"" buckets for JES backend? . ```; [2017-11-18 16:01:09,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: python $(which encode_bowtie2.py) \; /cromwell_root/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-trim_adapter/shard-0/glob-019a547c7b0dda79121d0398158a07d0/ENCFF439VSY.trim.merged.R1.fastq.gz \; \; --multimapping 4 \; \; --nth 4; [2017-11-18 16:01:09,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: python $(which encode_bowtie2.py) \; /cromwell_root/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-trim_adapter/shard-1/glob-019a547c7b0dda79121d0398158a07d0/ENCFF463QCX.trim.merged.R1.fastq.gz \; \; --multimapping 4 \; \; --nth 4; [2017-11-18 16:01:18,97] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: job id: operations/EJSf0Yz9Kxjs8__E9aKWivQBILWN-vrbGyoPcHJvZHVjdGlvblF1ZXVl; [2017-11-18 16:01:18,97] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: job id: operations/EJWg0Yz9KxiXpeC4gsSenC4gtY36-tsbKg9wcm9kdWN0aW9uUXVldWU; [2017-11-18 16:01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916:18,pipeline,pipeline,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916,3,['pipeline'],"['pipeline', 'pipeline-genome-data']"
Deployability,"I am experiencing a similar issue. Due to private AWS ECR registries not being supported, the hash lookup would not work with the remote hash lookup which was causing call-caching to not work. To bypass this, I installed a Docker CLI on the Cromwell server and enabled the local lookup, but this library/ prefix kept being added. I was able to patch it by modifying `dockerHashing/src/main/scala/cromwell/docker/local/DockerCliFlow.scala`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6172#issuecomment-782111450:211,install,installed,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6172#issuecomment-782111450,2,"['install', 'patch']","['installed', 'patch']"
Deployability,"I am experiencing an issue that may also be related to this, using WDL draft-2 spec and cromwell-39.; Here is a dummy example I created off a real error I received, it is minimal but hopefully descriptive enough:. ```WDL; task example {; Map[String, File] sample_files; Array[Array[String]]? tax_id_and_name; String? summary_report_name. String default_summary_report = select_first([summary_report_name, 'summary_report.txt']). command <<<; set -ex; example_command \; -o ${default_summary_report} \; -i ${write_json(sample_files)} \; ${ if defined(tax_id_and_name) then '-t ' + write_tsv(tax_id_and_name) else '' }; >>>; runtime {; docker: ""<local or private image name with the custom `example_command` installed>""; }; output {; File summary_report = ""${default_summary_report}""; }; }; ```; The run fails and the offending log output from Cromwell says:. ```commandline; example_command -o summary_report.txt -i /cromwell-executions/test_example_workflow/1d0ebc28-df3c-4e8c-9ade-7cae41513fcc/call-example/execution/write_json_b428b2ef25b3a99656256ecf58545736.tmp -t /Users/myuser/projects/wdl_example/cromwell-executions/test_example_workflow/1d0ebc28-df3c-4e8c-9ade-7cae41513fcc/call-example/execution/write_tsv_c317cbd4e3102b89210776bbc6430eeb.tmp; E Unable to open file /Users/myuser/projects/wdl_example/cromwell-executions/test_example_workflow/1d0ebc28-df3c-4e8c-9ade-7cae41513fcc/call-example/execution/write_tsv_c317cbd4e3102b89210776bbc6430eeb.tmp for reading (No such file or directory). Stopped at /usr/bin/example_command line 192.; ```. `write_json()` has no issue creating a path within the container, while `write_tsv()` returns a host path which is not found within the container.; I am able to workaround this at the moment by using `basename(write_tsv())` since the file is still in the execution directory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-484095411:706,install,installed,706,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-484095411,1,['install'],['installed']
Deployability,"I am experimenting by running some workflows with a MySQL database to avoid problems like #3387, but after a successful run and several days without re-running the pipeline (or a similar one) I would like to clean up the database to free some space. I would appreciate if this is included in the cromwell documentation...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3415:164,pipeline,pipeline,164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3415,1,['pipeline'],['pipeline']
Deployability,I am getting java.lang.ClassNotFoundException: languages.cwl.CwlV1_0LanguageFactory while using Cromwell 85 and 86 jar. ; [cromwell-85.log](https://github.com/broadinstitute/cromwell/files/13271608/cromwell-85.log); PFA for cromwell.log; Cromwell jar build completes without any errors. While using the jar to start Cromwell on Premise system we get the CWL error (command: nohup java -jar -Dconfig.file=/fastdata/02/genomics/cromwell/reference-84.conf /fastdata/02/genomics/cromwell/cromwell-85-f34251c-SNAP-original.jar server 2>&1 >> cromwell.log &).; https://github.com/broadinstitute/cromwell/releases notes for Cromwell 85 says: CWL implementation removed :This release removes the cwl top-level artifact. Some nonfunctional references may remain and will be addressed over time. Looks like some references is causing the issue?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7247:598,release,releases,598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7247,2,['release'],"['release', 'releases']"
Deployability,I am having the same issue. Was there a solution for this error?; cromwell version: 47; MySQL version: 5.5.64-MariaDB; centos-release-7-7.1908.0.el7.centos.x86_64,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084#issuecomment-625457700:126,release,release-,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084#issuecomment-625457700,1,['release'],['release-']
Deployability,"I am having the same problem. [I think the issue may be here.](https://github.com/broadinstitute/cromwell/blob/b4ec53e0f038c3e27a7a3a8b483066c962cc164d/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/PipelinesApiAsyncBackendJobExecutionActor.scala#L135). This seems to be where we check which outputs are type File or Directory, I think it's perhaps missing File-typed outputs within structs?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-694897196:177,pipeline,pipelines,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-694897196,3,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipelines']"
Deployability,"I am looking to switch over from the Google Life Sciences API to Batch backend, as I recently saw that the service will be deprecated. When I look at the stable documentation, I see documentation for PAPIv2 and I can only access GCPBatch documentation on the development docs. https://cromwell.readthedocs.io/en/develop/backends/GCPBatch/. I have tried to run a few test workflows using GCPBATCH backend with cromwell-85 release. Before I get too far, I wanted to inquire to see if there is full support for GCPBATCH or if this is something that will be supported in the future and we should keep using PAPIv2 for the time being?. In the cromwell.examples.conf listed providers I also do not see GCPBATCH https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf#L324C3-L341C30. While there is an example for this backend:; https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/GCPBATCH.conf. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7215:421,release,release,421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7215,1,['release'],['release']
Deployability,"I am of two minds on this. I would be happy to chat. Nonetheless, for now, let's not create impediments within the sprint by having outside work being a burden. Next sprint you can create a task that has only a few points relating to its integration not the actual coding and testing. . Thumb typed for added typos. > On Jun 19, 2015, at 10:19 AM, Scott Frazer notifications@github.com wrote:; > ; > Heh yeah.. though that velocity will be artificially high because a lot of the work I do for Cromwell (including this PR and the next PR) I do off hours.; > ; > —; > Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/44#issuecomment-113549678:238,integrat,integration,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/44#issuecomment-113549678,1,['integrat'],['integration']
Deployability,I am running into an issue whereby this feature would be helpful. I am developing workflows using CWL but one of the command line tools being used in CWL requires the runtime attribute 'bootDiskSizeGb' to be set to 100 instead of 10 for this particular task. As CWL doesn't have an equivalent attribute the only way I can set this is in the 'default-runtime-attributes' section of my configuration file but now all tasks for the workflow will use 100GB for their boot disk size instead of the single task that actually requires it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511067941:384,configurat,configuration,384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511067941,1,['configurat'],['configuration']
Deployability,"I am running the latest version of gatk4-genome-processing-pipeline locally.; I am running Cromwell on Debian Buster and It seems that workflow or Comwell is buggy, it fails on the link creation between MarkDuplicates and SortSampleBam. The output of MarkDuplicates is owned by root, yet the user (non-root) is creating link account.; ```; Localization via hard link has failed; ```; ```; [2020-11-27 17:29:01,18] [warn] Localization via hard link has failed:; SortSampleBam/inputs/705716460/sample.aligned.unsorted.duplicates_marked.bam -> MarkDuplicates/execution/sample.aligned.unsorted.duplicates_marked.bam: Operation not permitted; ```; ```; -rw-r--r-- 3 user user 147389840442 CollectQualityYieldMetrics/shard-0/inputs/-23977409/sample_1.unmapped.bam; -rw-r--r-- 3 user user 65217121989 CollectQualityYieldMetrics/shard-1/inputs/-373160914/sample_2.unmapped.bam; -rw-r--r-- 1 user user 170032310245 CollectUnsortedReadgroupBamQualityMetrics/shard-0/inputs/-406949147/sample_1.unmapped.aligned.unsorted.bam; -rw-r--r-- 1 user user 75403137686 CollectUnsortedReadgroupBamQualityMetrics/shard-1/inputs/499592884/sample_2.unmapped.aligned.unsorted.bam; -rw-r--r-- 1 user user 170032310245 MarkDuplicates/inputs/-406949147/sample_1.unmapped.aligned.unsorted.bam; -rw-r--r-- 1 user user 75403137686 MarkDuplicates/inputs/499592884/sample_2.unmapped.aligned.unsorted.bam; -rw-r--r-- 1 user user 170032310245 GatherMonolithicBamFile/inputs/1049412208/sample_1.unmapped.bam; -rw-r--r-- 3 user user 147389840442 SamToFastqAndBwaMemAndMba/shard-0/inputs/-23977409/sample_1.unmapped.bam; -rw-r--r-- 1 user user 75403137686 GatherMonolithicBamFile/inputs/-32225409/sample_2.unmapped.bam; -rw-r--r-- 3 user user 65217121989 SamToFastqAndBwaMemAndMba/shard-0/inputs/-373160914/sample_2.unmapped.bam. -rw-r--r-- 1 root root 170032310245 GatherMonolithicBamFile/execution/sample_1.unmapped.aligned.unsorted.bam; -rw-r--r-- 1 root root 75403137686 GatherMonolithicBamFile/execution/sample_2.unmapped.aligned.unso",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6094:59,pipeline,pipeline,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6094,1,['pipeline'],['pipeline']
Deployability,"I am testing the cromwell workflow on NAVER Cloud Platform (NCP). I tried setting using custom configuration of your documentation in github (https://github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends), but I failed. I would like to set configuration of backend including docker container and object storage (comparable with AWS S3) to the same level as AWS or GCP configuration in cromwell documentation. How can I do this?. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6548:95,configurat,configuration,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6548,4,['configurat'],['configuration']
Deployability,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406:380,configurat,configuration,380,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406,1,['configurat'],['configuration']
Deployability,"I am trying to run a wdl pipeline using Cromwell (v.84). The wdl file link: https://github.com/gatk-workflows/intel-gatk3-4-germline-snps-indels/blob/master/PairedSingleSampleWf_noqc_nocram_optimized.wdl. But I am facing this issue and I don't know how to proceed with it. `[2022-11-10 13:45:54,44] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:15:1]: job id: 95887; [2022-11-10 13:45:54,44] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:14:1]: job id: 95962; [2022-11-10 13:45:54,44] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:1:1]: job id: 96075; [2022-11-10 13:45:54,44] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:16:1]: job id: 96073; [2022-11-10 13:45:54,44] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:8:1]: job id: 95871; [2022-11-10 13:45:54,45] [error] KvWriteActor Failed to properly process data; cromwell.core.CromwellFatalException: java.sql.SQLDataException: data exception: string data, right truncation; table: JOB_KEY_VALUE_ENTRY column: STORE_VALUE; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:55); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:46); at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:490); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.sql.SQLDataExcep",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6947:25,pipeline,pipeline,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6947,1,['pipeline'],['pipeline']
Deployability,"I am trying to run the GATK4 workflow on Cromwell using the Spark backend. The WDL is from the Broad's GitHub repo:; https://github.com/broadinstitute/gatk4-data-processing/blob/master/processing-for-variant-discovery-gatk4.wdl. The following is my cromwell conf file where cromwell is running on the namenode of the spark cluster:; ```json; include required(classpath(""application"")). backend {; default = ""Spark""; providers {; Spark {; actor-factory = ""cromwell.backend.impl.spark.SparkBackendFactory""; config {; root: ""/mnt/data/cromwell"". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }; master: ""yarn""; deployMode: ""cilent""; }; }; }; }; ```. It looks like the shell script cromwell is generating to submit the job to spark using spark-submit has a syntax error in it, so the workflow fails immediately. ```bash; [ec2-user@ip-10-66-51-33 execution]$ pwd; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ ls -l; total 8; -rwxr--r-- 1 root root 1182 Feb 4 19:37 script; -rw-r--r-- 1 root root 296 Feb 4 19:37 stderr; -rw-r--r-- 1 root root 0 Feb 4 19:37 stdout; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ cat stderr; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: 3: /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: Syntax error: ""("" unexpected; ```. ```; [ec2-user@ip-10-66-51-33 execution]$ cat script ; #!/bin/sh; cd /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; spark-submit --master yarn --total-executor-cores 1 --deploy-mode cilent --class GATK4 --executor-memory 1gb InstantiatedCommand(# Not setti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4611:719,deploy,deployMode,719,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4611,1,['deploy'],['deployMode']
Deployability,"I am using Cromwell version 86 to run my workflow localy, but I encountered an issue. After the workflow completes, a warning appears, and it has been bothering me for a long time. Can you please provide guidance on how to resolve this?. ![image](https://github.com/broadinstitute/cromwell/assets/41176704/bb754226-83e6-4058-ad57-cf6da9333473). I have also checked my WDL file and found no issues. And I am not using a configuration file. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7349:419,configurat,configuration,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7349,1,['configurat'],['configuration']
Deployability,"I am using exactly the wdl and json offered by gatk GitHub page for gatk4-germline-snps-indels, locally, I got this error, intervals-hg38.even.handcurated.20k.intervals is larger than 128000 Bytes. Maximum read limits can be adjusted in the configuration under system.input-read-limits.; I tried to change it via type this in command line: java -Dsystem.input-read-limits=500000 -jar /cromwell-34.jar ; Didn't work.; Who can tell me how to fix it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-413173960:241,configurat,configuration,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-413173960,1,['configurat'],['configuration']
Deployability,"I am using latest cromwell develop docker container. When I run ""abort"" command it internall executes docker.kill script that looks like:; ""; #!/bin/bash ; docker kill `cat /pipelines/cromwell-executions/vsearch/81c51e4e-756c-47f7-8dd6\; -57b9c2981162/call-global_search/execution/docker_cid`; ""; However, docker_cid is never created. So, all the abort commands that I do stop the cromwell tasks but never stop docker containers that were started by it. My docker-stack configuration is https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml. It uses slightly modified cromwell:develop container https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/Dockerfile. I also share docker sockets there and everything functions well with the exception of abort.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4011:174,pipeline,pipelines,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4011,3,"['configurat', 'pipeline']","['configuration', 'pipelines']"
Deployability,"I am using this code and trying various configurations to write workflow logs, but I find myself confused a lot... Currently the README has [three environment variables](https://github.com/broadinstitute/cromwell/blob/ks_copy_logs/README.md#logging) listed (`LOG_ROOT`, `LOG_MODE`, `LOG_LEVEL`). It appears that `logback.xml` expects `LOG_MODE` to be `pretty` or `standard` but `WorkflowDescriptor` is expecting it to be `server`. I also couldn't figure out how to both output logs to stdout and also write to a workflow log (a feature I'd be particularly interested in!). Also it appears that we only honor `LOG_ROOT` when `LOG_MODE` is `server`. Can we maybe have a tech talk whenever you get a chance to understand how all these options play together?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-188390329:40,configurat,configurations,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-188390329,1,['configurat'],['configurations']
Deployability,"I am using v2alpha1, and WDL. If I understand you correctly, does that mean that if I upgrade to 36.1, it should stop adding that action to the pipelines request?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466548089:86,upgrade,upgrade,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466548089,2,"['pipeline', 'upgrade']","['pipelines', 'upgrade']"
Deployability,I backed out the name-mangling change because it was redundant in fixing the actual bug and had far-reaching consequences.; - The upgrade script was very broken because it makes extensive use of anonymous node names to come up with real names for what to put in the WDL; - String concatenation and string comparison feel like gross tools to use when we have types at our disposal... i.e. evaluating `.isInstanceOf[AnonymousExpressionNode]`. I can imagine a future where we have a `canLinkWith` function that evaluates name and type to return a boolean,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4075#issuecomment-420680044:130,upgrade,upgrade,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4075#issuecomment-420680044,1,['upgrade'],['upgrade']
Deployability,"I believe @hjfbynara is coordinating this, but make sure we have something to deploy CromIAM to firecloud environments. Namely the docker-compose.yml includes CromIAM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4268:78,deploy,deploy,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4268,1,['deploy'],['deploy']
Deployability,I believe the patch coverage check failure is due to an incidental fix to the formatting of a log statement.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7555#issuecomment-2376759111:14,patch,patch,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7555#issuecomment-2376759111,1,['patch'],['patch']
Deployability,I believe this has been done already: https://github.com/broadinstitute/cromwell/blob/develop/release/release_workflow.wdl#L24-L33. @Horneth please re-open if I misunderstood.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-424931955:94,release,release,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-424931955,1,['release'],['release']
Deployability,"I believe this is complete with the release of Cromwell 25, is this ready to be closed @ruchim?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1986#issuecomment-286130513:36,release,release,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1986#issuecomment-286130513,1,['release'],['release']
Deployability,"I believe this might be a consequence of the fact that the `WorkflowExecutionActor` is responsible for sending status updates to the metadata, and under the current load it accumulates those updates in its mailbox. Those updates then get processed and it's possible that 2 status updates close to each other in the mailbox end up generating the same timestamp. Those timestamps reflect the `WEA` view of the world, which might be delayed compared to reality if it's very busy. If that's not the desired behavior we could maybe have each job (EJEA) independently send status updates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559:118,update,updates,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559,5,['update'],['updates']
Deployability,I believe we auto release womtool now. @Horneth please re-open if I missed something.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-424933416:18,release,release,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-424933416,1,['release'],['release']
Deployability,"I believe we want to patch this off of the cromwell hash we are currently; using - 3eb1623; https://github.com/broadinstitute/cromwell/commit/3eb1623d9a5ffdf0fc3626820eab84ae6560b2cd. On Fri, Mar 18, 2016 at 12:01 PM, mcovarr notifications@github.com wrote:. > Sounds good; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198426842",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198459262:21,patch,patch,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198459262,1,['patch'],['patch']
Deployability,I came here to say what I apparently said a few months ago already :). We don't have access to a SLURM cluster so anything we put together would be a guess on our part. I know folks have been able to pretty easily get LSF & PBS working based on our example of an SGE configuration so my assumption is that it's not hard but I have no way of knowing. If someone were to get it working and submit docs we'd happily accept them but we have no way of handling that ourselves.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-281771190:267,configurat,configuration,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-281771190,1,['configurat'],['configuration']
Deployability,"I came out with a custom and dirty way of going around this issue. In my configuration file, I changed the `backend.providers.Local.config.submit-docker` script for the following:. ```bash; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc; ```. Maybe this could be the default value in the [reference configuration file](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf) to solve the problem, but maybe it is better to have a `post-docker` configuration which is added to the pipeline similar to the `script-epilogue`. This would make easier the configuration of docker runs, separating submission and checks. By now, I will use the following local configuration to continue my work with the cromwell runner:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 15; # set the root directory to the run; filesystems.local {; ## do not allow copy (huge files); localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker containe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526:73,configurat,configuration,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526,6,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,I can add a test.; I'm all for making PRs to CWL conformance tests but it's going to increase the merge time of our PRs if we want to wait for it to be in the CWL repo. Also we'd need to unpin the hash for conformance test or update it every time..,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374939225:226,update,update,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374939225,1,['update'],['update']
Deployability,"I can also offer to help, in whatever form is useful! If you just need to use / pull, then Singularity image support via installing it should fit the bill. Users can use Github to host images via Singularity Hub. If you want to host your own registry, then Singularity Registry is the way to go! Let me know if I can help, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330348650:121,install,installing,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330348650,1,['install'],['installing']
Deployability,I can reproduce this problem on Cromwell 29 but happily this runs fine on the forthcoming Cromwell 30. We're hoping to release this **very** soon. 🙂,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972#issuecomment-349014226:119,release,release,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972#issuecomment-349014226,1,['release'],['release']
Deployability,"I can't seem to reproduce this - I have a workflow running, stop cromwell, restart it, and no matter how many times I do it I don't see anything wrong with the sequence of events:. ```. 2016-07-27 16:48:47,144 INFO - Slf4jLogger started; 2016-07-27 16:48:48,456 cromwell-system-akka.actor.default-dispatcher-3 INFO - Bound to /0.0.0.0:8009; 2016-07-27 16:48:48,460 ForkJoinPool-2-worker-15 INFO - Cromwell service started...; 2016-07-27 16:48:48,703 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell_new; 2016-07-27 16:48:50,251 INFO - Reading from cromwell_new.DATABASECHANGELOG; 2016-07-27 16:48:50,337 INFO - Successfully acquired change log lock; 2016-07-27 16:48:50,400 INFO - Successfully released change log lock; 2016-07-27 16:48:50,689 cromwell-system-akka.actor.default-dispatcher-3 INFO - 1 new workflows fetched; 2016-07-27 16:48:50,689 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowManagerActor Restarting workflow UUID(c6eb4949-cb81-4a56-b3de-11b1cde3e13e); 2016-07-27 16:48:50,693 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowManagerActor Successfully started WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e; 2016-07-27 16:48:50,773 cromwell-system-akka.dispatchers.engine-dispatcher-15 INFO - WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-07-27 16:48:51,258 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: Call-to-Backend assignments: hello.hello -> JES; 2016-07-27 16:48:51,284 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-07-27 16:48:51,291 cromwell-system-akka.dispatchers.engine-dispatcher-15 INF",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544:715,release,released,715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544,1,['release'],['released']
Deployability,I cannot test this change as I do not have the build environment set up. Is is intentional that this 'cloud' feature is inherited by the SGE backend? I am fixing an issue caused by a feature I do not need. I wonder if it could be placed in the epilogue for user configurations. Isn't that the purpose of epilogue and configuration?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3864:262,configurat,configurations,262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864,2,['configurat'],"['configuration', 'configurations']"
Deployability,"I cant get the sra filesystem to work. Here is the error:. ```; [2020-08-21 11:08:59,62] [info] WorkflowManagerActor Workflow dbd5cdc0-c79a-42cd-b929-56ddb1115467 failed (during InitializingWorkflowState): common.exception.AggregatedMessageException: Failed to instantiate backend filesystem:; Cannot find a filesystem with name sra in the configuration. Available filesystems: ftp, s3, gcs, oss, drs, http; 	at common.validation.Validation$ValidationChecked$.$anonfun$unsafe$2(Validation.scala:98); 	at cats.syntax.EitherOps$.valueOr$extension(either.scala:66); 	at common.validation.Validation$ValidationChecked$.unsafe$extension(Validation.scala:98); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories$lzycompute(backend.scala:109); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories(backend.scala:108); 	at cromwell.backend.BackendConfigurationDescriptor.pathBuilders(backend.scala:120); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders$lzycompute(StandardInitializationActor.scala:62); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders(StandardInitializationActor.scala:62); 	at cromwell.backend.google.pipelines.common.PipelinesApiInitializationActor.$anonfun$workflowPaths$2(PipelinesApiInitializationActor.scala:137); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(Abst",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793:340,configurat,configuration,340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793,1,['configurat'],['configuration']
Deployability,"I compiled and tested this, and it works correctly. As I'm not familiar with java/scala, I cant provide a full review unfortunately. I did notice some warnings when starting cromwell, but as everything works, maybe that's not a problem ? . 2021-03-13 12:17:25,630 WARN - Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, default-runtime-attributes.awsBatchRetryAttempts, awsBatchRetryAttempts, filesystems.s3.duplication-strategy, numSubmitAttempts, default-runtime-attributes.scriptBucketName. Thanks by the way ! This was exactly what we were waiting for",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-799139288:284,configurat,configuration,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-799139288,1,['configurat'],['configuration']
Deployability,"I compiled the develop branch (after commit b246001).; (I had previously used the 0.19 release and 0.19_hotfix, but both refused to soft link or hard link.). When cromwell-0.20-b246001 is invoked with. ```; cromwell -jar ~/java/cromwell-0.20-b246001.jar \; -Dbackend.shared-filesystem.localization.0=soft-link; ```. Some of the files are soft linked, while other files are hard-linked in the same run of a task. The fasta file is soft-linked:. ``` bash; $ ls -l seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta; lrwxrwxrwx 1 dshih broad 72 Jun 27 10:44 seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta -> /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta*; ```. However, the pon file in the same task is hard-linked:. ``` bash; $ ls -l xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; -rw-r--r--+ 3 dshih broad 1020487624 May 10 11:35 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon; ```. The number of references for this file is 3 (instead of 1), because I ran the workflow twice and both times, the localized file is hard-linked. All 3 files have the same inode number (14733355). I looked at the log file, but it was not terribly revealing... No error had occurred. No attempt at soft linking was logged. I was able to manually soft link the file in question. ``` bash; $ uname -a; Linux cga02 2.6.32-573.26.1.el6.x86_64 #1 SMP Tue A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1072:87,release,release,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072,1,['release'],['release']
Deployability,"I considered filing an issue, but it seems like the best avenue to get the maintainers thoughts. How do you all feel about creating a page where community-backed projects can be highlighted? The idea occurred to me as I was thinking about our impending release of `oliver` (not quite there yet). The only other project I was aware of was `cromshell`, not sure if there are any others you would want to add 🤷‍♂ .",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5427:253,release,release,253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5427,1,['release'],['release']
Deployability,"I created very simple pipelines.; ```; workflow worms {. Array[File] inputFiles. scatter (oneFile in inputFiles) {; call extract { input: sraFile=oneFile }; }; }. task extract {. File sraFile. command {; /opt/sratoolkit/sra-stat ${sraFile} > stats.txt; }; runtime {; docker: ""itsjeffreyy/sratoolkit""; }. output {; File stats = ""stats.txt""; }; }; ```; There I am supposted to get sra-stat output that is ; ```; 52687861:1896762996:1896762996|0:0|0:0|0:07; ```; As in input you can put any sra files.; But what I get is; ```; /cromwell-executions/worms/ddb1ae2d-e4c2-4a6e-8577-18b7faf69a84/call-extract/inputs/home/shelluser/nematodes/SRR2040663.sra||52687861:1896762996:1896762996|0:0|0:0|0:0; ```; With some internal trash in the beginning. I do not need to get this trash, I just want to get sra-stat output.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2188:22,pipeline,pipelines,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2188,1,['pipeline'],['pipelines']
Deployability,"I did a configuration using a local MySQL server without docker. I guess that the problem was that my docker machine does not connect the `localhost` address from the VM to the host machine. Thus, the `localhost` port was not providing the connection to the MySQL server. This might be a common problem in MacOS computers, which are running `boot2docker`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-373318603:8,configurat,configuration,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-373318603,1,['configurat'],['configuration']
Deployability,"I did not have any experience with this little Maven-based utility so I performed the following steps for post-upgrade verification.; ```; sdk install maven; mvn compile; mvn test; mvn package // I think this is a superset of `compile` and `test` but they all take just a few seconds so 🤷‍♂️ ; ```; Closing automatic PR https://github.com/broadinstitute/cromwell/pull/6743 in favor of this one because we can trivially upgrade to the latest version, not just a security-hotfixed older version.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6747:111,upgrade,upgrade,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6747,4,"['hotfix', 'install', 'upgrade']","['hotfixed', 'install', 'upgrade']"
Deployability,"I did some further debugging and I found that cromwell creates a SlickDatabase object twice. Once for the database and once for the metadata. That is probably were the conflict comes from. Using the following configuration the metadata database is kept separate and the problem could not be reproduced anymore. I am now trying it on a production workflow on our cluster. ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell-metadata.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; }. ```. I am currently looking in how to make the metadata and engine use the same connection when they are using the same configuration. EDIT: The code hierarchy concerning both the engine database and metadata database is quite complex, it is not straightforward to share a connection. Using a separate metadatabase seems to be a faster workaround for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734858027:209,configurat,configuration,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734858027,2,['configurat'],['configuration']
Deployability,"I did that and began encountering the following error:; ```; 2019-02-25 18:17:52,693 cromwell-system-akka.actor.default-dispatcher-29 ERROR - No configuration setting found for key 'services'; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor: exception during creation; 	at akka.actor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorF",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:145,configurat,configuration,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['configurat'],['configuration']
Deployability,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:244,integrat,integrations,244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633,2,['integrat'],['integrations']
Deployability,I don't know why Github conversation screen is not being updated with new commits.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1424#issuecomment-247664605:57,update,updated,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1424#issuecomment-247664605,1,['update'],['updated']
Deployability,"I don't know. I usually mentally bundle that into the general ""metadata updates get missed"" issue but that doesn't really answer the ""root cause"" question.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-478634137:72,update,updates,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-478634137,1,['update'],['updates']
Deployability,I don't think it closes anything no. It should be enough for the release though ? We'll see if/where we need more retries in the logs if this error pops up again.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/625#issuecomment-202554850:65,release,release,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/625#issuecomment-202554850,1,['release'],['release']
Deployability,"I don't think the pull/build should be built in to the command that is run at scale - the image should be pulled /built once, and then the direct path passed into the script / pipeline that is run at scale.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463887263:176,pipeline,pipeline,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463887263,1,['pipeline'],['pipeline']
Deployability,"I don't think this needs reviews, instead direct comments to the centaur PR for the 29 hotfix",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2848#issuecomment-343969150:87,hotfix,hotfix,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2848#issuecomment-343969150,1,['hotfix'],['hotfix']
Deployability,"I dropped a [question into the Google Genomics google group](https://groups.google.com/forum/#!topic/google-genomics-discuss/OL18jPoPvPE) about the ""stuck"" Google Genomics Pipeline operations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260142269:172,Pipeline,Pipeline,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260142269,1,['Pipeline'],['Pipeline']
Deployability,"I dug up the source for my statement about autocommit:. >If autocommit mode is enabled, each SQL statement forms a single transaction on its own. https://dev.mysql.com/doc/refman/5.7/en/innodb-autocommit-commit-rollback.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4508#issuecomment-452773467:211,rollback,rollback,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4508#issuecomment-452773467,1,['rollback'],['rollback']
Deployability,"I exposed this as `monitoring_image` workflow option, and included the Dockerfile & script in `supportedBackends/google/pipelines/v2alpha1/src/main/resources/cromwell-monitor/`. Should we also add a CI script that builds the image?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451601217:120,pipeline,pipelines,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451601217,1,['pipeline'],['pipelines']
Deployability,I found the code relating to the format. Please update the documentation :o). https://github.com/broadinstitute/cromwell/blob/32d5d0cbf07e46f56d3d070f457eaff0138478d5/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiDockerCacheMappingOperations.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321211381:48,update,update,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321211381,4,"['Pipeline', 'pipeline', 'update']","['PipelinesApiDockerCacheMappingOperations', 'pipelines', 'update']"
Deployability,"I found two logback.xml files in cromwell. They were the exact same, but my PR only updates the one under engine. Is because I did not update the other one as well - the reason that travis is failing with the following error?. [error] 1 error was encountered during merge; …java.lang.RuntimeException: deduplicate: different file contents found in the following:; logback.xml; logback.xml",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546:84,update,updates,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546,2,['update'],"['update', 'updates']"
Deployability,"I gave your travis test a nudge since I don't think it's your fault that that specific test case failed. I don't know where circle CI came from, but since the error is ""there's no configuration"" I think it's safe to ignore that one too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027:180,configurat,configuration,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027,1,['configurat'],['configuration']
Deployability,"I get a lot of coercion errors area just when I want to get a file inside the Directory, like dir + ""/"" +""filename"" that makes the Directory totally useless for the pipelines for me.; the error is:; ```; Workflow failed. WorkflowFailure(Failed to evaluate job outputs,List(WorkflowFailure(Bad output 'methylation_extraction.out': IllegalArgumentException: No coercion defined from wom value(s) '""/data/cromwell-executions/bs_map_fast/1ea51f16-2197-4703-be02-ee3e59c448c1/call-methylation_search/execution/output/output_CpG.bedGraph""' of type 'Directory' to 'File'.,List()))); ```; I enclose the pipeline ( main WDL there is bs_map_run_fast.wdl) and the input; [bs-seq.zip](https://github.com/broadinstitute/cromwell/files/3317934/bs-seq.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5041:165,pipeline,pipelines,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5041,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,I got the same issue when update to version 46. Any idea? Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540112608:26,update,update,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540112608,1,['update'],['update']
Deployability,"I guess I'm still confused... is there anything for me to do with this ticket? On both hotfix and develop, I can leave out the `system.workflow-restart` and it works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224696780:87,hotfix,hotfix,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224696780,1,['hotfix'],['hotfix']
Deployability,I guess one way to test this that would go with the grain of the conventional release process would be to create a config option that's disabled by default and selectively enable it on alpha on-instance for testing. It could be removed once we're confident it works in prod. (I fully own that I have questioned the value of config options in the past; I think this is a bit different because it's designed to be temporary.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440:78,release,release,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440,1,['release'],['release']
Deployability,"I had the same issue. I got the same error message:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:91,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,3,"['Pipeline', 'configurat']","['PipelinesApiAsyncBackendJobExecutionActor', 'configuration']"
Deployability,"I have a following pipeline which fails validation, and I cannot see what can be wrong with it:. ```; version 1.0. ## ; # Git URL import; import ""https://raw.githubusercontent.com/DSLituiev/five-dollar-genome-analysis-pipeline/master/tasks/to_uBam.wdl"" as touBam. # WORKFLOW DEFINITION; workflow WGUnMap {; input {; File mapped_bam; String bam_base; }. call touBam.unMap {; input:; File mapped_bam=mapped_bam,; String unmapped_base=bam_base; }. # Outputs that will be retained when execution is complete; output {; File unmapped_bam = touBam.out; }; }; ```. The error is:; ```; java -jar `which womtool.jar` validate unmap.wdl; ERROR: Unexpected symbol (line 46, col 7) when parsing '_gen19'. Expected rbrace, got ""File"". File mapped_bam=mapped_bam,; ^. $call_body = :input :colon $_gen19 -> CallBody( inputs=$2 ); ```. Any ideas? Why brace is expected to be closed right after the colon?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5256:19,pipeline,pipeline,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5256,2,['pipeline'],['pipeline']
Deployability,"I have a generic (run-anywhere) wdl now that exhibits the behaviour (runs under previous develop and release 22 & 23 versions, but not with 340a5cf): ; ```; workflow dna_mapping_38 {. call createInputs. scatter (arg in createInputs.alignedReadGroup) {; call mapping { input: inFile=arg }; }. call groupItemsByKey as groupArgsByLibrary {; input:; keys=createInputs.library,; items=mapping.outFile; }. scatter (libset in groupArgsByLibrary.groups) {; call markDup as libraryMerge {; input:; inputBams=libset.right,; outputBam=""library_${libset.left}.bam""; }; }. output {; Array[File] libMerged = libraryMerge.markDupedBam; }; }. #########; # TASKS #; #########. task createInputs {; command {; for i in `seq 1 5`; do echo ""lib1""; touch arg$i; done; }; output {; Array[File] alignedReadGroup = glob(""arg*""); Array[String] library = read_lines(stdout()); }; }. task mapping {; File inFile; command {; echo ""dummy mapping""; }; output {; File outFile=inFile; }; }. task groupItemsByKey {. Array[String] keys; Array[String] items. meta {; description: ""return pairs of (key, all-items-with-that-key)""; }. command <<<; python <<CODE; import itertools; import sys; keys = ""${sep='\t' keys}"".split(""\t""); items = ""${sep='\t' items}"".split(""\t""); assert len(items) == len(keys); theKey = lambda x: x[0]; theItem = lambda x: x[1]; data = sorted(zip(keys, items), key=theKey); for key, group in itertools.groupby(data, theKey):; sys.stderr.write(key + ""\n""); sys.stdout.write(""\t"".join(theItem(i) for i in group) + ""\n""); CODE; >>>. output {; Array[Pair[String, Array[String]]] groups = zip(read_lines(stderr()), read_tsv(stdout())); }; }. task markDup {. Array[File] inputBams; String outputBam. command {; echo ""dummy marking duplicates""; touch ${outputBam}; }. output {; File markDupedBam = ""${outputBam}""; }; }; ```; running:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run scatterTest.wdl - - - -; ```. succeeds but running with new version: ; ```; java -jar workspace/cr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:101,release,release,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['release'],['release']
Deployability,"I have a working cromwell/AWS batch configuration.; I have a simple workflow called three_task_sequence.wdl which I am able to run on AWS backend, and see the outputs in s3. However, submitting this job to my cromwell server:; `curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:36,configurat,configuration,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,2,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3417:275,pipeline,pipeline,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417,4,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"I have checked in a new version. Will make a pull request for it soon,. https://github.com/broadinstitute/cromwell/blob/mjs-AWS-config-example-fix/cromwell.example.backends/AWS.conf. On Wed, Sep 16, 2020 at 6:14 AM openbioinfomatics for more people who need; it <notifications@github.com> wrote:. > would you guys update ""cromwell/cromwell.example.backends/AWS.conf""; >; > it seams this file for old version .; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5857>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EK6P5Z7C4RDBY2BIVDSGCFZTANCNFSM4ROS34OQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5857#issuecomment-693513576:314,update,update,314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5857#issuecomment-693513576,1,['update'],['update']
Deployability,"I have configured a SLURM backend for Cromwell and have encountered unusual behavior while trying to configure memory as a runtime attribute. . I define a runtime attribute Int with default value in my Cromwell configuration file and attempt to override this in my task WDL. Whether the override succeeds seems to depend on the variable name used! This is very confusing behavior; I expect to either receive a message that a variable name is not allowed, or the override should succeed. . Fails: ""memory_mb""; Succeeds: ""requested_memory_per_core"". I am verifying whether the override succeeds by checking the Cromwell output [task]/execution/script.submit. . ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int bsub_cpu = 1; Int memory_mb = 1000; String queue; """"""; ; submit = """"""; 			sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; 			${""-n "" + bsub_cpu} \; 			--mem-per-cpu=${memory_mb} \; 			--wrap ""/bin/bash ${script}""; 		""""""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. ```; workflow tutorialWorkflow{; 	call task_A { input : in=""testing"" }; 	call task_B { input : in=task_A.out }; 	call task_C { input : in=task_A.out }; }. task task_A{; 	String in. 	command{; 		echo 'This is task A ${in}.'; 	}	; 	output{; 		String out='This is task A ${in}'; 	}; 	runtime{; 		bsub_cpu: 1; 		runtime_minutes: 10; 		memory_mb: 100; 		queue: ""short""; 	}; }. task task_B{; 	String in; 	command{; 		echo 'This is task B ${in}.'; 	}; 	runtime{; 		bsub_cpu: 2; 		runtime_minutes: 15; 		memory_mb: 110; 		queue: ""short""; 	}; }. task task_C{; 	String in; 	command{; 		echo 'This is task C ${in}.'; 	}; 	runtime{; 		runtime_minutes: 25; 		memory_mb: 210; 		queue: ""short""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2068:211,configurat,configuration,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068,1,['configurat'],['configuration']
Deployability,"I have configured localstack in Docker with S3, IAM and EC2 services on localhosts(AWS Batch is not supported). I would like to understand how Cromwell can be configured to interact with existing file systems instead of AWS. I found this option, but I do not really understand how it can be used correctly and whether it will work at least with S3:. **### Configuration**. _Cromwell's default configuration defines an instance of the HTTP filesystem named `http`. There is no additional configuration; required for the HTTP filesystem itself so adding HTTP filesystem support to a backend is a simple as; adding a reference to this filesystem within the backend's `filesystems` stanza. e.g. Cromwell's default `Local` shared filesystem; backend is configured like this (a PAPI version 2 backend would be configured in a similar way):_. ```; backend {; default = ""Local""; providers {; Local {; ...; config {; filesystems {; local {; ...; }; http { }; }; }; ...; }; ...; }; }; ```. Can I solve this problem without touching the source code?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5055:356,Configurat,Configuration,356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5055,3,"['Configurat', 'configurat']","['Configuration', 'configuration']"
Deployability,"I have confirmed and worked-around by changing the configuration -o and -e parameters to ; ```; -o ${out}.cromwell; -e ${err}.cromwell; ```; identical duplicated files are now written to the work directory. (Identical except in the case of error, which is when I need these files anyway). My request is to remove the >(tee) lines, but I understand they probably exist to serve some other backend. The ability to turn them off would be appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393064752:51,configurat,configuration,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393064752,1,['configurat'],['configuration']
Deployability,"I have created a new lablels file and using that to pass the VPC/subnet info but still get the same error:; ```; $ grep -i label genomics.conf; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; $ cat labels.json; {; ""my-private-network"": ""xxxx"",; ""my-private-subnetwork"": ""xxxx""; }; ```; and updated my cromwell command to the following:; ```; java -Dconfig.file=genomics.conf -jar cromwell-66.jar run cumulus.wdl -i cumulus_inputs.json -l labels.json; ```; I still get the same error though. Is this even possible or am I missing something?. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905041782:332,update,updated,332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905041782,1,['update'],['updated']
Deployability,"I have encountered this issue when running the GATK 4 [Merge BAM Alignment](https://github.com/gatk-workflows/gatk4-data-processing/blob/master/processing-for-variant-discovery-gatk4.wdl#L334) task, on Cromwell 36 running on Google Cloud. For some reason, it seems that the docker image `broadinstitute/gatk` is too large for the disk. What I'm not sure about is which disk this is referring to. Is it the machine running Cromwell, or is it the pipelines API worker machine? If it's the latter, how can I stop this error happening?. ```; cromwell_1 | 2018-10-25 06:53:52,612 cromwell-system-akka.dispatchers.engine-dispatcher-440 ERROR - WorkflowManagerActor Workflow 28605745-a8d2-43c4-ab02-70e5c5c032fe failed (during ExecutingWorkflowStat; e): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:0:1 failed. The job was stopped before the command finished. PAPI error code 5. 8: Failed to pull image broadinstitut; e/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: ""docker pull broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71"" failed: exit status 1: sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: Pulling from broadinstitute/gatk; cromwell_1 | ae79f2514705: Pulling fs layer; cromwell_1 | 5ad56d5fc149: Pulling fs layer; cromwell_1 | 170e558760e8: Pulling fs layer; cromwell_1 | 395460e233f5: Pulling fs layer; cromwell_1 | 6f01dc62e444: Pulling fs layer; cromwell_1 | 98db058f41f6: Pulling fs layer; [...]; cromwell_1 | failed to register layer: Error processing tar file(exit status 1): write /root/.cache/pip/http/5/1/d/8/2/51d82969228464b761a16257d5eefe8e2b3dde3c1ad733721353e785: no space left on device; cromwell_1 |; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsy",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:445,pipeline,pipelines,445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,1,['pipeline'],['pipelines']
Deployability,I have just had a run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API and which resulted in several `504 Gateway Timeout` errors while attempting to read `rc` and `stdout` output files. Is this something that should be looked into?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957:84,pipeline,pipelines,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957,2,"['Pipeline', 'pipeline']","['PipelinesApiLifecycleActorFactory', 'pipelines']"
Deployability,"I have no idea why the ""codecov/patch"" test is failing, or how to fix it - the ""details"" link isn't very helpful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488093257:32,patch,patch,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488093257,1,['patch'],['patch']
Deployability,"I have no reason to believe develop is any less stable today than it was a few weeks ago, but if anyone feels otherwise please speak up! . Branching GotC releases from a branch based at this hash means we'll need to put fixes on both the GotC branch and develop going forward. That will become increasingly difficult as these branches diverge. And eventually PBE Cromwell would be released with the full bolus of ported fixes that were never previously tested at scale.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198489018:154,release,releases,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198489018,2,['release'],"['released', 'releases']"
Deployability,"I have not seen it in a while. And I think your suggestion would work.; Feel free to close this ticket. On Sun, Feb 26, 2017 at 1:35 PM, kshakir <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Any more updates on this ticket?; >; > In general we were wondering if a gcloud logout and then gcloud login; > helped.; >; > If this is no longer an issue, mind closing this one, and open another in; > the future with current wdl / details / version-info?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282576487>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk8Bmz3FbZVh7tyQAJ63aCZyquTaoks5rgcYPgaJpZM4KnP3t>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282735570:233,update,updates,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282735570,1,['update'],['updates']
Deployability,"I have not used this configuration in some time. On Aug 6, 2017 14:39, ""Geraldine Van der Auwera"" <notifications@github.com>; wrote:. @LeeTL1220 <https://github.com/leetl1220> Do you have a reproducible test; case? Otherwise we probably need to close this. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320524441>,; or mute the thread; <https://github.com/notifications/unsubscribe-auth/ACDXk2gj4A8fOPuWbRAQvNF1k1H9Ct9Aks5sVgh-gaJpZM4LrbMZ>; .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320647147:21,configurat,configuration,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320647147,1,['configurat'],['configuration']
Deployability,"I have run both strategies with a workflow that generates around about 2000 jobs (100 samples) using GATK best practices for RNA variant calling. . ### Method. The Cromwell instance ran with a SLURM cluster backend. All jobs were run using singularity containers. The cromwell process was limited to 3 akka threads and 1 GC thread (by default it grabs al threads on the login node, and this is not fair to other users). The HSQLDB memory database with persistance file was used. Said SLURM cluster has its storage connected via NFS. Two configurations of cromwell were used. One with the xxh64 strategy, and one with the fingerprint strategy. Each cromwell instance was executed in its own directory, with its own database and own cromwell-executions folder. The [BioWDL RNA-seq](https://github.com/biowdl/rna-seq) workflow was run. After running, the workflow was run again to see if the call-caching worked correctly. ### Results; Both `xxh64` and `fingerprint` strategies were able to rerun the workflow with a 100% Cache hit. The fingerprint strategy however was much quicker:; `time` results for fingerprint; ```; real 23m26.269s; user 15m31.229s; sys 2m43.406s; ```; `time` results for xxh64; ```; real 69m12.478s; user 56m7.371s; sys 52m6.262s; ```. ### Conclusion; Using xxh64 as a strategy requires some calculation but one hour for 100 samples on 2000 jobs is quite acceptable. What is obvious is that the system IO (`sys` time) takes a lot of time as well. This cluster has very fast optimized ISILON storage, but on clusters without this, any hashing strategy can be quite slow because of this. The fingerprint works very well for HPC environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438:537,configurat,configurations,537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438,1,['configurat'],['configurations']
Deployability,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:53,Configurat,Configuration,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302,2,"['Configurat', 'configurat']","['Configuration', 'configuration-examples']"
Deployability,"I have to note that I didn't make this configuration (@rhpvorderman will know more about this); This is in the backend section:; ```; caching {; duplication-strategy: [ ""soft-link"", ""copy"", ""hard-link"" ]; hashing-strategy: ""file""; }; ```; This is on the top level:; ```; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393886848:39,configurat,configuration,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393886848,1,['configurat'],['configuration']
Deployability,"I have to update my ""workflow inputs"" branch to update that YAML file... :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/183#issuecomment-140139777:10,update,update,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/183#issuecomment-140139777,2,['update'],['update']
Deployability,"I have tried the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) to run Cromwell on Google Cloud. I did not get very far. I have followed the long set of instructions. I have logged in with my `<google-user-id>`, I have set my own `<google-project-id>`. I have created my own bucket. I have generate my service account key with the command:; ```; gcloud iam service-accounts keys create sa.json --iam-account ""$EMAIL""; ```. Then I run the hello.wdl with the command:; ```; GOOGLE_APPLICATION_CREDENTIALS=sa.json; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```. But I get the following error:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.create(HttpStorageRpc.java:308); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:213); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:210); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.StorageImpl.internalCreate(StorageImpl.java:209); 	at com.google.cloud.storage.StorageImpl.create(StorageImpl.java:171); 	at cromwell.filesystems.gcs.GcsPath.request$1(GcsPathBuilder.scala:196); 	at cromwell.filesystems.gcs.GcsPath.$anonfun$writeContent$2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:704,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,704,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"I have zero idea whether this is the correct place to post an issue. The gatk forums [say to post here](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team). There are messages here saying post on Jira?. I am attempting to set cromwell up to run with singularity. This is in an HPC environment with a brand new install of cromwell, where I don't have the ability to access or overwrite any global files, i.e. the application.conf file with all the defaults in it. It's a documentation issue rather than a problem with cromwell, which runs fine on the default configuration. . Documentation [here](https://cromwell.readthedocs.io/en/develop/tutorials/Containers/#singularity) suggests that I need to add code similar to that found [here](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/singularity.conf) to a config block of the backend.providers section in a configuration file similar to the file [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf). Which, if you click that link, you'll see is broken. It might possibly be linked to [this issue](https://broadworkbench.atlassian.net/browse/BA-4810) on Jira?. As a result I have no idea what the conf file is supposed to look like, nor to be honest where it goes or how it's meant to be referenced. There's an issue [here](https://gatkforums.broadinstitute.org/wdl/discussion/12789/cromwell-configuration-on-slurm) which tells me I have to have ""include required(classpath(""application""))"" in the first line of the conf file, but apart from that I can't find anything on what the file should look like. . The documentation [here](https://cromwell.readthedocs.io/en/stable/tutorials/ConfigurationFiles/) and [here](https://cromwell.readthedocs.io/en/stable/Configuring/#overview) both suggest that the configuration files are for a server version of cromwell, whereas I have to run it from the command line, i.e. . ```; cromwell run <-o confi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5560:329,install,install,329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5560,3,"['configurat', 'install']","['configuration', 'install']"
Deployability,"I have, really weird behavior for File? I expect that if file does not exist I should get null for File? (as there are no wld functions that actually check for file existance), however instead I get an error:; ```; Workflow failed; WorkflowFailure(,List(WorkflowFailure(Could not process output, file not found: /pipelines/scripts/cromwell-executions/quantification/87596324-9f68-4419-b3ce-6ff47fe381b4/call-prepare_samples/execution/not_exist,List()))); ```; ```wdl; task prepare_samples {; File samples; File references; File samples_folder. command {; /scripts/run.sc --samples ${samples} --references ${references} --cache ${samples_folder}; }. runtime {; docker: ""quay.io/comp-bio-aging/prepare-samples@sha256:9aaa223ff520634bb0357500ffb90aa80315729e0870ebbc7da4a4b31c382a2c""; }. output {; File? invalid = ""invalid.tsv""; File? novel = ""novel.tsv""; #File? cached = ""cached.tsv""; File? cached = ""not_exist"" #I expect it to be null if the file does not exist; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3367:313,pipeline,pipelines,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3367,1,['pipeline'],['pipelines']
Deployability,"I haven't run the workflow manually, but I don't see any commits in the last few releases which would have helped this. I think we should mark this as a bug (even if the ""fix"" just ends up being a test case to prove that it's fixed)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4670#issuecomment-493539374:81,release,releases,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670#issuecomment-493539374,1,['release'],['releases']
Deployability,"I hear you @patmagee. Cromwell has had a SaaS continuous development model for a few years now, with new code going to Terra daily. We learned that most standalone Cromwell users at the Broad upgrade infrequently, such as every 6-12 months. Thus, we committed put in the effort for two ""shrink-wrapped"" releases per year so we can balance SaaS with standalone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506:46,continuous,continuous,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506,6,"['continuous', 'release', 'upgrade']","['continuous', 'releases', 'upgrade']"
Deployability,I heard chatter about a 30.2 release ...is there any chance this change can make it in that release? It's mostly for FC users as the current failure logs are sent to the server logs and basically the user never sees call caching fail even though the job succeeds.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3182#issuecomment-360157283:29,release,release,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3182#issuecomment-360157283,2,['release'],['release']
Deployability,"I included some changes in #4961 that might work for this enhancement request. The changes in that PR publish ""workflowProcessingEvents"" for workflow pickup, release, and completion. The first two events types can be multi-valued since Cromwell can be restarted and possibly upgraded during the execution of a workflow. Sample metadata from a simple run:. ```; {; ""workflowName"": ""wf_hello"",; ""workflowProcessingEvents"": [; {; ""cromwellId"": ""cromid-4db4123"",; ""timestamp"": ""2019-05-13T15:00:22.152Z"",; ""cromwellVersion"": ""41-07606c8-SNAP"",; ""description"": ""Finished""; },; {; ""cromwellId"": ""cromid-4db4123"",; ""description"": ""PickedUp"",; ""timestamp"": ""2019-05-13T15:00:10.879Z"",; ""cromwellVersion"": ""41-07606c8-SNAP""; }; ],; ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4668#issuecomment-491862476:158,release,release,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4668#issuecomment-491862476,4,"['release', 'upgrade']","['release', 'upgraded']"
Deployability,I just downloaded the `womtool-84.jar` at <https://github.com/broadinstitute/cromwell/releases> I didn't need to build the source.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1237279136:86,release,releases,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1237279136,1,['release'],['releases']
Deployability,"I just found out that Cromwell-Singularity integration will be on the agenda on Winter Codefest 2018, starting tomorrow! See https://docs.google.com/document/d/1RlDUWRFqMcy4V2vvkA1_ENsVo6TXge2wIO_Nf73Itk0/edit#heading=h.xg79ql4rt605. You can join in (also remotely) by checking this file: https://docs.google.com/spreadsheets/d/1o4xDUgl2iu_CgFuDpB1swtG8XVZK3aifvKlhh5qagyI/edit#gid=0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358296186:43,integrat,integration,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358296186,1,['integrat'],['integration']
Deployability,I just updated the description,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/441#issuecomment-182541538:7,update,updated,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/441#issuecomment-182541538,1,['update'],['updated']
Deployability,"I just want to point out that this used to work in Cromwell 29, so some sort of regression has happened such that sub workflows aren't working anymore. I'm not sure what kind of sub workflow integration tests you guys have, but it looks like they aren't comprehensive enough. Feel free to add this one to your test suite (it's actually not a super complicated sub workflow). . This is pretty important to some of the work we're doing with Gaddy to get the somatic genome pipeline ready (we can't run the samples for him). And the ultimate goal of this project is to bring more users to FireCloud...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358515823:191,integrat,integration,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358515823,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1692:173,update,update,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692,2,"['configurat', 'update']","['configurationFile', 'update']"
Deployability,"I mentioned this to Jeff earlier, but I had some sort of git calamity that prevents me from squashing down these commits in the usual way. I'll fix this before the actual merge to sprint2 with a brute force patching of a new branch of sprint2 with these changes, but I'd like to hold off on doing that until this is ready for merge to avoid losing your comments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/9#issuecomment-100315647:207,patch,patching,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/9#issuecomment-100315647,1,['patch'],['patching']
Deployability,I merged the DRS stability PR https://github.com/broadinstitute/cromwell/pull/7179 to `develop` and then updated this branch from it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1644510515:105,update,updated,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1644510515,1,['update'],['updated']
Deployability,"I might have missed it -- can you put together the files so we can; reproduce this (cromwell configuration, WDL and json)? We might need some; permissions, but I want to run this sort of thing continually. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Tue, Nov 8, 2016 at 12:32 PM, Lee Lichtenstein notifications@github.com; wrote:. > N=4/4 on my workflow...; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-259203566,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ABW4gwHvsAAXHshNNM4GFWqWhx1Cvrsgks5q8LI_gaJpZM4Ko1_r; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-259207344:93,configurat,configuration,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-259207344,1,['configurat'],['configuration']
Deployability,"I missed that the value for `client-payload` is in single quotes and does not interpolate. Consequently, `""version"": ""$CROMWELL_VERSION""` was passed as a literal to the `update-service` [action](https://github.com/broadinstitute/terra-helmfile/actions/runs/3190563369/jobs/5205838370). There, it was evaluated in double quotes `VERSION=""$CROMWELL_VERSION""`, found to be empty in that environment, and halted with `Error: empty version string`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6926:170,update,update-service,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6926,1,['update'],['update-service']
Deployability,"I narrowed it down to the fact that I don't have an alt contig for the reference file -- i was leaving that blank in the wdl input file. If i just fake it by using the human alt from the Broad's human genome reference in their pipeline, the weird nesting-copying doesnt happen. I'll leave this open because I don't know if this is expected behavior or not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-541156513:227,pipeline,pipeline,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-541156513,1,['pipeline'],['pipeline']
Deployability,I need to make an update to changelog,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/479#issuecomment-190455236:18,update,update,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/479#issuecomment-190455236,1,['update'],['update']
Deployability,I noticed an error in your tutorial:; https://cromwell.readthedocs.io/en/jg_add_http_doc/tutorials/AwsBatch101/. The aws.config you supply is missing:; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6; from the config portion:. It was generating a confusing error and should be updated.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4278:284,update,updated,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4278,1,['update'],['updated']
Deployability,"I noticed that if I give cromwell a container with existing entry-point (like quay.io/ucsc_cgl/fastqc@sha256:86d82e95a8e1bff48d95daf94ad1190d9c38283c8c5ad848b4a498f19ca94bfa , for instance) and give a file as an input it (both develop branch and latest release) fails to run the entry point properly. Here is an example of task; ```; task report {. String fileName; File file. command {; ${file}; }. runtime {; docker: ""quay.io/ucsc_cgl/fastqc@sha256:86d82e95a8e1bff48d95daf94ad1190d9c38283c8c5ad848b4a498f19ca94bfa""; #docker: ""quay.io/biocontainers/fastqc@sha256:bb57a4deeec90633e746afbc38c36fdb202599fe71f9557b94652e9c8f3c1a02""; }. output {; #File out = sub(file, ""\\.fastq.gz"", ""_fastqc.gz""); File out = ""${fileName}.fastqc.gz""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2256:253,release,release,253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256,1,['release'],['release']
Deployability,I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687:172,configurat,configuration-reference,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687,1,['configurat'],['configuration-reference']
Deployability,"I only updated test code, so I'm ignoring codecov's complaint as I have no idea what it's talking about.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6022#issuecomment-723262670:7,update,updated,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6022#issuecomment-723262670,1,['update'],['updated']
Deployability,I placed the configuration option into backend/abortJobsOnTerminate. If you guys want me to move or rename it to something else I'm happy to.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175077183:13,configurat,configuration,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175077183,1,['configurat'],['configuration']
Deployability,I posted a heads up in the WDL blog; will update WDL user docs when v29 is released. Any update on its ETA?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-320520487:42,update,update,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-320520487,3,"['release', 'update']","['released', 'update']"
Deployability,I pushed the updates that I think might fix the JES and bad label test cases. Still working on getting the sbt tests set up locally,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-497390203:13,update,updates,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-497390203,1,['update'],['updates']
Deployability,"I ran into a related issue while running the ENCODE HiC pipeline via Caper on SLURM. I opened an issue there too. On our HPC I need to `module load cuda/11.7` to use the `nvcc` binary. I tried `--wrap='module load cuda/11.7'` but while this gets passed to the `sbatch` command it returns a script argument not permitted error, possibly because `module` isn't a binary but a bash function? Are there any other options for using Caper/Cromwell with the `module` system?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-1430297902:56,pipeline,pipeline,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-1430297902,1,['pipeline'],['pipeline']
Deployability,"I ran into issues in the last release related to the calls to `git clone` and the URLs used to create the names of the new `.jar` files, so; - changed github clones back to ssh; - changed jar paths",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4270:30,release,release,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4270,1,['release'],['release']
Deployability,"I ran into the same issue as [#3876](https://github.com/broadinstitute/cromwell/issues/3876) running the chip-seq pipeline on beegfs, the workflow would stall looking for outputs after any individual task. I was able to resolve this by changing globLinkCommand to use soft instead of hard links, though not sure if soft links will cause problems in other contexts.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5250:114,pipeline,pipeline,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250,1,['pipeline'],['pipeline']
Deployability,"I ran the single-sample pipeline on the small (20K bam) and one of the BQSR jobs (BaseRecalibrator.8.retry-1) took much longer than it should (8 hours instead of a few minutes) it was then preempted and replaced by retry-1 which, indeed concluded after a few minutes. . it would be good to know if there are many such instances and if we are being billed for them... [longBQSR.metadata.txt](https://github.com/broadinstitute/cromwell/files/221526/longBQSR.metadata.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/711:24,pipeline,pipeline,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/711,1,['pipeline'],['pipeline']
Deployability,I realized that downstream libs won't automagically get the updated cats library so no need to relax this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2577#issuecomment-324989698:60,update,updated,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2577#issuecomment-324989698,1,['update'],['updated']
Deployability,"I recently attempted to run a workflow with files in a separate non public google project as inputs. I submitted it using both Swagger and curl but neither gave very informative error messages. Here's the curl command and output:. ```; curl -v https://cromwell.gotc-dev.broadinstitute.org/api/workflows/v1 -F wdlSource=@PairedSingleSampleWf.wdl -F workflowInputs=@ExampleOfIncorrectProject.json; * Trying 104.197.140.34...; * Connected to cromwell.gotc-dev.broadinstitute.org (104.197.140.34) port 443 (#0); * TLS 1.2 connection using TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384; * Server certificate: cromwell.gotc-dev.broadinstitute.org; * Server certificate: InCommon RSA Server CA; * Server certificate: USERTrust RSA Certification Authority; * Server certificate: AddTrust External CA Root; > POST /api/workflows/v1 HTTP/1.1; > Host: cromwell.gotc-dev.broadinstitute.org; > User-Agent: curl/7.43.0; > Accept: */*; > Content-Length: 38237; > Expect: 100-continue; > Content-Type: multipart/form-data; boundary=------------------------c2956aa00c34148a; > ; < HTTP/1.1 100 Continue; < HTTP/1.1 500 Internal Server Error; < Date: Fri, 18 Mar 2016 17:59:04 GMT; < Server: spray-can/1.3.2; < X-Frame-Options: SAMEORIGIN; < Access-Control-Allow-Origin: *; < Access-Control-Allow-Headers: authorization,content-type,accept,origin; < Access-Control-Allow-Methods: GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD; < Access-Control-Max-Age: 1728000; < Content-Type: text/plain; charset=UTF-8; < Content-Length: 69; < Connection: close; < ; * Closing connection 0; The server was not able to produce a timely response to your request.%; ```. The issue was that I was using the broad-gotc-dev project to run the workflow, but two of my input files were in broad-gp-gotc-pilot, but I had trouble figuring that out from this error message. Happy to provide the wdl and json files as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/589:1367,PATCH,PATCH,1367,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/589,1,['PATCH'],['PATCH']
Deployability,"I recently got into a trouble because of wrong caching in cromwell. In most of my pipelines I copy the results of last tasks to some folder which I define in the variable (unfortunately the option.json feature of cromwell to kind'of ""copy workflow results"" is useless for us as it copies all the nested trash, like ""id/execution"" instead of just resulting files). ; Unfortunately, my task that does the copying was cached even if the task on which it depends had different inputs (and was not cached). That means that we have a false positive caching and the copying task copies the same file from cache all over again.; Here is an example of my WDL. However, it fails in all WDLs where I use the copy-task for results; ```wdl; workflow Diamond_Blast {. Int threads; File db; File query; String result_name; String results_folder; String mode = ""blastx"". call diamond_blast {; input:; threads = threads,; database = db,; name = result_name,; query = query,; mode = mode. }. call copy as copy_results {; input:; files = [diamond_blast.out],; destination = results_folder; }. output {; File out = copy_results.out[0]; }. }. task diamond_blast {. Int threads; File database; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044:82,pipeline,pipelines,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044,1,['pipeline'],['pipelines']
Deployability,"I run a short bash pipeline like this : for CHROM in `cut -f1 $IN_FILE |grep -Pv '^@'| sort|uniq`; do. But I get this error : sort: cannot create temporary file in ‘/cromwell_root/tmp’: No such file or directory. when running in firecloud. should I ""mkdir -pv /cromwell_root/tmp"" first?. I see ""export TMPDIR=/cromwell_root/tmp"" in the exec.sh on the 3rd line.... I went ahead and issued before the pipeline starts the command ""mkdir -pv $TMPDIR"" and made the error go away!. Is it normal/expected to need to create TMPDIR ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/731:19,pipeline,pipeline,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/731,2,['pipeline'],['pipeline']
Deployability,"I saw a case where lack of Google Cloud resources caused a permanent workflow failure, for what seems like a transient condition that could be overcome by retries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.Ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:251,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,4,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBack', 'PipelinesApiAsyncBackendJobExecutionActor', 'pipelines']"
Deployability,"I saw that, but since `File?` isn't a compound type, I'm under the impression that example was for comparison to the actual compound types. That's the inconsistency I'm not groking -- since `File?` isn't an example of a compound type, it seems that example's existence implies that something that accepts a `File` should also accept a `File?`, which is indeed the case with Cromwell's integration for size() but not basename() or sub(). . If we relied entirely on what the spec's headings and examples said as being the only acceptable inputs, then basename() wouldn't work on `File` at all because the spec says it actually takes in a `String`, not a `File`, and has no `File` examples. Since basename() works on `File` it seems Cromwell is already going beyond what the 1.0 spec explicitly says.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233423450:385,integrat,integration,385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233423450,1,['integrat'],['integration']
Deployability,I see a unit test failure that looks like it could be the result of one of these upgrades:; ```; should not mix up credentials *** FAILED *** (44 milliseconds); [info] java.lang.NoSuchFieldException: credentials; [info] at java.base/java.lang.Class.getDeclaredField(Class.java:2411); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.credentialsForPath$1(GcsPathBuilderSpec.scala:326); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.$anonfun$new$7(GcsPathBuilderSpec.scala:334); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.flatspec.AnyFlatSpecLike$$anon$5.apply(AnyFlatSpecLike.scala:1832); [info] at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760:81,upgrade,upgrades,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760,1,['upgrade'],['upgrades']
Deployability,I see that in readme. ```; Configuring Spark Project; When using Spark backend uncomment the following Spark configuration in the application.conf file; ```. But I do not see it in application.conf Looks like this is outdated.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2033:109,configurat,configuration,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2033,1,['configurat'],['configuration']
Deployability,I snuck in a quick 31.1 GitHub release that includes the fix for this. Thanks again for reporting it!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3656#issuecomment-390436881:31,release,release,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3656#issuecomment-390436881,1,['release'],['release']
Deployability,"I solved this issue on my workstation by passing the $EUID variable as the docker_user parameter in the backend.providers.LocalExample.config section of the configuration file: ; ```; runtime-attributes = """"""; String? docker; #String? docker_user # Uncommenting to try the EUID fix for root files and inability to hardlink; String docker_user = ""$EUID""; """"""; ```; After that, docker outputs were no longer owned by root. . Hope it helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764894141:157,configurat,configuration,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764894141,1,['configurat'],['configuration']
Deployability,"I still need to get my [terminology straight](http://martinfowler.com/articles/mocksArentStubs.html), but either a mock or a stub would have probably sufficed. I mainly wanted to feel like the code was ""self-documented"" a little in the tests. Instead, I put in a detector for a `cromwell-account.conf` that when present runs an integration test against the live ""gcr.io"". TODO: I still need to clean up access token caching, but there's lots of other code that may be critiqued.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-161046172:328,integrat,integration,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-161046172,1,['integrat'],['integration']
Deployability,I suspect that making outputs and inputs a toggle-able option will kick this particular can significantly down the road.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/972#issuecomment-224427067:43,toggle,toggle-able,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/972#issuecomment-224427067,1,['toggle'],['toggle-able']
Deployability,I suspect that this might be a problem (or become a problem) with our deploys,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/226:70,deploy,deploys,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/226,1,['deploy'],['deploys']
Deployability,"I think I can do better! It's all on the branch [here](https://github.com/vsoch/wgbs-pipeline/tree/add/singularity). These would normally render into a nice site given being served from Github pages, but a markdown file will hopefully do for now :). https://github.com/vsoch/wgbs-pipeline/blob/add/singularity/docs/pages/docs/tutorial/getting-started/index.md. Let me know if you have questions!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416300815:85,pipeline,pipeline,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416300815,2,['pipeline'],['pipeline']
Deployability,"I think I found the one you are talking about. ![screen shot 2018-10-15 at 11 28 47 am](https://user-images.githubusercontent.com/2978948/46960909-92efc580-d06d-11e8-97fe-d81ef63da81a.png). The failure reason is . ```; Task requester_pays_engine_functions.functions:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""ubuntu@sha256:de774a3145f7ca4f0bd144c7d4ffb2931e06634f11529653b23eba85aef8e378""]: exit status 1 (standard error: ""error pulling image configuration: received unexpected HTTP status: 502 Bad Gateway...; ```. which is not related to the requester pays feature but rather yet another dockerhub flaky response that we should ask google to retry IMO.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429901811:553,configurat,configuration,553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429901811,1,['configurat'],['configuration']
Deployability,"I think I misunderstood how to use `flock` in a bash script, so I've updated this with a tentative improvement:. ```bash; (; flock --exclusive 200; # Build the image; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi; ) 200>/var/lock/$IMAGE; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509650419:69,update,updated,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509650419,1,['update'],['updated']
Deployability,"I think in April we chose Option 1 with a sprinkling of Option 3 in https://docs.google.com/a/broadinstitute.com/document/d/1feRDusWXQQ2pJ03sNHTNmrrnnwL3y-vtyF1fv_RdogU/edit?usp=sharing whereas this is clearly Option 2... (I'm not saying it's wrong, this seems to address all of the ""cons"" as I saw them with gusto... just pointing it out :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1099#issuecomment-229972340:92,a/b,a/broadinstitute,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1099#issuecomment-229972340,1,['a/b'],['a/broadinstitute']
Deployability,"I think it's extremely unlikely we will be attempting to support SELinux even on an infinite timescale. The vast majority of our install base runs in containers, either on Kubernetes or Docker. Sorry for the inconvenience.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6905#issuecomment-1717885213:129,install,install,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6905#issuecomment-1717885213,1,['install'],['install']
Deployability,"I think so, if you and @kshakir or someone else don't mind giving it a second look that'd be nice though because I updated it a bit last week after realizing it was broken in some case.; I should also probably update the changelog to advertise the introduction of `WaitingForQueueSpace` status",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-373188385:115,update,updated,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-373188385,2,['update'],"['update', 'updated']"
Deployability,"I think the ""CaaS"" comment at the end should be put into the headline (ie something like `CaaS wraps 404 into 500 on releaseHold requests`). And we should probably check this works in other cases where 404s might be returned, not just `releaseHold`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3911#issuecomment-406642863:117,release,releaseHold,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911#issuecomment-406642863,2,['release'],['releaseHold']
Deployability,I think the PAPI Centaur `/bin/bash` dependency is purely an artifact of having a job shell effectively hardcoded to `/bin/bash` for the previous 31 releases of Cromwell so that unintentionally `/bin/bash` dependent WDLs were written into the test suite. 🙂,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3697#issuecomment-392810761:149,release,releases,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697#issuecomment-392810761,1,['release'],['releases']
Deployability,"I think the minimum requirements are:; 1) Project-wide `Genomics Pipelines Runner` and `Compute Instance Admin (v1)` _roles_ for the service account used by Cromwell itself.; 2) `Service Account User` _permission_ on `Compute Engine default service account` for the Cromwell service account.; 3) `Storage Object Admin` _permission_ on the Cromwell execution and data buckets, as well as `Storage Object Viewer` on the GCR bucket (if used).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-439653262:65,Pipeline,Pipelines,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-439653262,1,['Pipeline'],['Pipelines']
Deployability,I think this should be fixed in 25 hotfix or develop. I entered #1945 a while back for what sounds like the same issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2162#issuecomment-293007654:35,hotfix,hotfix,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2162#issuecomment-293007654,1,['hotfix'],['hotfix']
Deployability,"I think what what we should do is once the hotfix is in place to replace the jar. Can have a note here that makes a mention of it, but honestly no one is going to come back here to read the changelog after they've already pulled the release, and as @mcovarr points out it's not even possible if they're using a MySQL based database.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2456#issuecomment-315820103:43,hotfix,hotfix,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2456#issuecomment-315820103,2,"['hotfix', 'release']","['hotfix', 'release']"
Deployability,"I think what you're running into is that the `endpoint-url` refers to where the Life Sciences / Genomics application _itself_ runs, which is different than where the compute VMs spin up. . You can try putting `us-west2` in the WDL runtime section, or in the workflow options JSON. Both of those should be readily Googleable in terms of documentation, if you get stuck definitely comment. > The location you specify is only used to store metadata about the pipeline operation. [Source.](https://cloud.google.com/life-sciences/docs/concepts/locations#available_locations)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6774#issuecomment-1143939365:456,pipeline,pipeline,456,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6774#issuecomment-1143939365,1,['pipeline'],['pipeline']
Deployability,"I think you're right, in order to allow for private IPs, the noAddress field needs to be added both in the pipeline and the run resources.; It's possible that the `pipelineArgs`resources don't contain what they're supposed to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1501#issuecomment-250503406:107,pipeline,pipeline,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501#issuecomment-250503406,2,['pipeline'],"['pipeline', 'pipelineArgs']"
Deployability,I updated README in my PR #2058,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-285116661:2,update,updated,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-285116661,1,['update'],['updated']
Deployability,"I upgraded to release 0.23 and did a workaround for #1754 ... I had the same issue as described here, however, I did see the exception below, which I do not think was displayed in previous versions of cromwell. Is this at all helpful? @kshakir ? I'd also like to point out that this file does not exist, but I have not done anything to that directory. There are log files from other runs (mostly local backend, though). The permissions are set appropriately for ``/home/lichtens/eval-gatk-protected/cromwell-workflow-logs/``. ```; [ERROR] [12/07/2016 22:51:59.735] [cromwell-system-akka.dispatchers.engine-dispatcher-53] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8968c364-; 3623-4242-b39e-228f43f5d4c3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(Wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:2,upgrade,upgraded,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486,2,"['release', 'upgrade']","['release', 'upgraded']"
Deployability,"I used cromwell-38.jar for testing. It looks like outputs from tasks with the same task-name and shard index in different scatter blocks share the same scratch directory in a container. Output files are written on this shared directory and globbed at the end of each task and then moved to S3 bucket together with all text/log files. This can result in globbing of wrong output files (from tasks with the same task-name but different alias).; ```; workflow cromwell_aws_glob_test {; scatter(i in range(2)) {; call t1 { input: i=i, alias = ""t1"" }; }. scatter(i in range(2)) {; call t1 as t2 { input: i=i, alias = ""t2"" }; }; }. task t1 {; Int i; String alias; command {; echo ""${alias},${i}"" > ${alias}-${i}.txt; }; output {; Array[File] outs = glob(""*.txt""); }; }; ```; On the execution directory for task t1 on S3. Everything looks okay. We don't have any t2 things.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/; PRE glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:30:54 0; 2019-04-11 20:32:48 30 glob-ef5df339533c1334f081dc8cc75ee4f3.list; 2019-04-11 20:30:54 2238 script; 2019-04-11 20:32:49 2 t1-0-rc.txt; 2019-04-11 20:32:51 0 t1-0-stderr.log; 2019-04-11 20:32:50 0 t1-0-stdout.log. But the glob directory has outputs from task t2.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:32:47 277 cromwell_glob_control_file; 2019-04-11 20:32:47 5 t1-0.txt; 2019-04-11 20:32:47 2 t2-0-rc.txt; 2019-04-11 20:32:47 5 t2-0.txt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4834:929,pipeline,pipeline-test-runs,929,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4834,2,['pipeline'],['pipeline-test-runs']
Deployability,"I wanted to follow-up on this error: I am now seeing this error after implementing the standard broad institute alignment pipeline on the HPC at my institute: https://portal.firecloud.org/?return=terra#methods/five-dollar-genome-analysis-pipeline-gilad/five-dollar-genome-analysis-pipeline-gilad/1. Specifically my error is: . [INFO] [08/12/2024 19:26:46.031] [cromwell-system-akka.dispatchers.engine-dispatcher-29] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Workflow 8b8c576b-50bc-4a33-b326-0f69be43ece9 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'CreateSequenceGroupingTSV.sequence_grouping': Failed to read_tsv(""sequence_grouping.txt"") (reason 1 of 1): Future timed out after [60 seconds]; Bad output 'CreateSequenceGroupingTSV.sequence_grouping_with_unmapped': Failed to read_tsv(""sequence_grouping_with_unmapped.txt"") (reason 1 of 1): Future timed out after [60 seconds]. Bad output 'GetBwaVersion.bwa_version': Failed to read_string(""/scratch/tpa239/Step123/TN_2036/TN2036_phylogenetics_8_10_testing/slurm/alignment/alignment_TN2036_sample106/cromwell-executions/WholeGenomeGermlineSingleSample/8b8c576b-50bc-4a33-b326-0f69be43ece9/call-UnmappedBamToAlignedBam/UnmappedBamToAlignedBam/207b9946-03a6-4969-bdab-318482635923/call-GetBwaVersion/execution/stdout"") (reason 1 of 1): Future timed out after [60 seconds]. I think it has to do with this read_tsv and function - sometimes an identical job will have this error and sometimes they don't, I think it has to do with how busy the cluster is. . Is there some setting I can change to increase this timeout? Should I increase the number of cpus or memory for these jobs failing?. I am using cromwell version 85. Thank you!. Toby",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2291515502:122,pipeline,pipeline,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2291515502,3,['pipeline'],"['pipeline', 'pipeline-gilad']"
Deployability,"I wanted to second this, as we're setting the same issue submitting to PBSPro clusters; with the latest Cromwell development version. Looking through the code, it appears to; originate from https://github.com/broadinstitute/cromwell/blob/33c58ef22b6a8edc4c1912c1416225c79d298f76/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala; which was tweaked since the last release in a change from @cjllanwarne (https://github.com/broadinstitute/cromwell/commit/33c58ef22b6a8edc4c1912c1416225c79d298f76#diff-39fe7186c2383fc1135f29a9c05e4e57) but I don't; grasp the scope of the change enough to know if this triggers it. In our CWL run, the jobs get submitted to the cluster and run okay based on the; work directories in `cromwell-execution` but the polling dies with:; ```; [2019-01-17 12:34:15,18] [info] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Status change from - to Running; [2019-01-17 12:34:16,27] [ESC[38;5;220mwarnESC[0m] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:418,release,release,418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['release'],['release']
Deployability,I was encountering the call caching issue with the latest released version 87. The issue is resolved with the develop branch 88-90ca58d.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7473#issuecomment-2239621915:58,release,released,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7473#issuecomment-2239621915,1,['release'],['released']
Deployability,"I was trying to label all GCP batch resources using `-o` in the CLI. (according to [this doc](https://github.com/broadinstitute/cromwell/blob/master/docs/wf_options/Google.md) and [this doc](https://github.com/broadinstitute/cromwell/blob/develop/docs/backends/GCPBatch.md) ); My content of options.json is ; ```; {""google_labels"":{""workflow-run-execution-id"":""97fed6c4-6442-4efe-9e73-7b3592a33480""}} ; ```; and my command is `java -Dconfig.file=config -jar /app/cromwell.jar run wf.wdl -i input.json -o options.json`. The error I am getting:; <img width=""1409"" alt=""Screen Shot 2024-01-04 at 9 58 25 AM"" src=""https://github.com/broadinstitute/cromwell/assets/1992953/e559bccf-dd96-4dea-a48a-6649e448a26f"">. After adding string prefix to my own uuid as label value, the error was gone and my workflow ran smoothly. However, I do need to pass in the uuid so downstream analysis pipeline can still work. . Related code reporting error is [here](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L65) and error is produce [here](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L77) and [this is the regex definition](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L21). I am wondering can the [code](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L77) be updated to only check the key of a label so it aligns with GCP which does not have this restriction on label values? Or use another regex `""[a-z0-9]([-a-z0-9]*[a-z0-9])?""` for label value check",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7351:877,pipeline,pipeline,877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7351,1,['pipeline'],['pipeline']
Deployability,"I was ultimately able to reproduce this on 36 by putting the value for `y` in the inputs JSON instead of as a default. The same scenario works fine on latest `develop`, so I am going to close this issue with the advice that you upgrade to 37.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474470766:228,upgrade,upgrade,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474470766,1,['upgrade'],['upgrade']
Deployability,"I would like to specify an optional array of strings as a runtime attribute. As such, I tried this in my provider configuration:. runtime-attributes = """"""; Array[String]? mounts = []; """""". Unfortunately, this fails horribly:. ```; [2019-02-27 16:26:09,15] [error] Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; java.lang.RuntimeException: Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; ```. The error clearly indicates that this type is unsupported. What is the reasoning behind that, or am I just doing it wrong?. (This is with Cromwell 36.1)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4685:114,configurat,configuration,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685,1,['configurat'],['configuration']
Deployability,"I would still love to see precomputed metadata for completed workflows (would need to be updated on label PATCHes, but ....). But I think that's a larger project",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429107279:89,update,updated,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429107279,2,"['PATCH', 'update']","['PATCHes', 'updated']"
Deployability,I'd also like to cherry pick this onto 32 hotfix if the shorebirds allow.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3789#issuecomment-397692617:42,hotfix,hotfix,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3789#issuecomment-397692617,1,['hotfix'],['hotfix']
Deployability,"I'd be surprised if these changes can't be migrated fairly easily to 0.19-hotfix. Metadata is an area under heavy construction on develop, and I suspect you're right this will need to be completely reimplemented once the dust settles. 😦 . I'd certainly thumbs up a 0.19-hotfix version of this with updates to the README.md. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219124036:74,hotfix,hotfix,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219124036,3,"['hotfix', 'update']","['hotfix', 'updates']"
Deployability,I'd buy that logic assuming the hotfix had been thumbed. 😛,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2458#issuecomment-315835050:32,hotfix,hotfix,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2458#issuecomment-315835050,1,['hotfix'],['hotfix']
Deployability,I'd double check what I say with @kshakir as he always corrects me but I'd vote for putting this in `src/bin/release`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1780#issuecomment-267122947:109,release,release,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1780#issuecomment-267122947,1,['release'],['release']
Deployability,"I'd just mark the original PR as ""Hotfix Candidate"" tbh, then we can do everything all in one commit",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3076#issuecomment-352557525:34,Hotfix,Hotfix,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3076#issuecomment-352557525,1,['Hotfix'],['Hotfix']
Deployability,"I'll start working with development, then. We don't have anything in; ""production"" that we cannot break. Thanks for the direction. On Aug 14, 2016 14:05, ""Jeff Gentry"" notifications@github.com wrote:. > Hi @seandavi https://github.com/seandavi - That does seem like it; > should work. Thinking back in my past I've definitely encountered tools; > which expect TMPDIR to exist and aren't smart enough to create it; > themselves. Also in JES there shouldn't be any issues with permissions, etc.; > ; > We'd certainly welcome a PR if you're game for it, either (or both); > against 0.19_hotfix or develop. On that note, I should point out that a; > new release (currently develop) is imminent and for all but one use case; > (call caching) we beliee it to be more robust/stable that 0.19. I'd; > personally recommend people who don't need call caching work with the new; > system, but I understand that some people aren't comfortable working with; > code which isn't yet released.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687458,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/AAFpE17tmdKE5bY42PWUnqMW6YV6rifAks5qf1jugaJpZM4INzbb; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687994:650,release,release,650,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687994,2,['release'],"['release', 'released']"
Deployability,"I'll update the issue title... this isn't about SGE per-se, but rather about running local + some other back end in a single workflow. Google JES is fine for that. Just to further clarify, this doesn't require being able to schlep files between the two (although it would be great to know if that works!) but as @delocalizer mentioned, a use case of running something locally that generates a non-file type (e.g. string) that can be passed as input to a task on another backend would be the thing to verify or get working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-253986986:5,update,update,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-253986986,1,['update'],['update']
Deployability,"I'm _still_ working on implementing [this](https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318533390) integration test. At the moment, the develop cromwell client won't let centaur pass bad JSON for the test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-319978778:120,integrat,integration,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-319978778,1,['integrat'],['integration']
Deployability,"I'm a :-1: on this idea. Seems like it is overly demanding as compilation is less of an ask than committing code. For instance, now our build servers must have git secrets installed where it is irrelevant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938418:172,install,installed,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938418,1,['install'],['installed']
Deployability,I'm a bit confused by this... it's labelled with 0.20 milestone but this is for a feature that's only in 0.19? This should be fixed on the 0.19 hotfix branch right?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224686926:144,hotfix,hotfix,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224686926,1,['hotfix'],['hotfix']
Deployability,"I'm a little wary of introducing 2 different AWS sdk's into the project. Is the reason to use Transfer Manager? Besides logging updates what is the benefit of it?. Besides using transfer manager is there a need to pull in the ""old"" SDK? We were told the current 2.X series is the one to use.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537142165:128,update,updates,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537142165,1,['update'],['updates']
Deployability,I'm caught in a release cycle but believe the relevant code belongs here for others watching:. https://github.com/broadinstitute/cromwell/blob/fc43c6954eb47912930e4c28f73781a112fdfbf8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L109,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-459409119:16,release,release,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-459409119,1,['release'],['release']
Deployability,I'm checking out WDL/Cromwell at the moment and this feature would make Cromwell definitely more interesting. It would make it much easier to run reproducible pipelines without relying on docker. (Docker is a no go on our cluster because it gives users root access.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-352784888:159,pipeline,pipelines,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-352784888,1,['pipeline'],['pipelines']
Deployability,"I'm closing this ticket because the issue is addressed in our next release (and our develop branch, in case you're feeling brave). I've also added and started working on https://github.com/broadinstitute/cromwell/issues/1940 for the `File?` inputs bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276804216:67,release,release,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276804216,1,['release'],['release']
Deployability,"I'm concerned that the awesomeness of this will be lost if there's not a dead-simple way to get the graph. But I can accept that as a feature request on this already nice PR. Put the code in `supportedBackends/google/pipelines/v2alpha1/src/main/resources/cromwell-monitor/`, please and thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451494152:217,pipeline,pipelines,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451494152,2,['pipeline'],['pipelines']
Deployability,"I'm confused. Sandbox mode doesn't/shouldn't require sudo. We included it because it requires the *least* permissions to use it. . As for `pull`, it seems to force rebuild the image every time, exactly the same as `build --force`. What we want ideally is a way to build the image if it doesn't exist or needs to be updated, but if neither is the case, then do nothing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463866252:315,update,updated,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463866252,1,['update'],['updated']
Deployability,"I'm definitely having this problem with AWS Backend. Not sure how newest, but I believe I had this problem during the HCA Pipeline Surges as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-505704953:122,Pipeline,Pipeline,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-505704953,1,['Pipeline'],['Pipeline']
Deployability,"I'm experiencing the issue, running the Broad ""gatk4-data-processing"" pipeline on their sample data, on Google Cloud. Repository with their code: https://github.com/gatk-workflows/gatk4-data-processing. The only change I made to the .wdl was setting Pre-emption to 0, although previous runs with ""3"" resulted in the the same error. I also doubled the size of the ""agg_large_disk"" to 800 GB, because I thought I was running out of space during merging, although the error seems consistent. Relevant log:. `PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:11:1]: Status change from Running to Success; 2019-01-18 18:43:32,761 cromwell-system-akka.dispatchers.backend-dispatcher-362 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:6:1]: Status change from Running to Success; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.sca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:70,pipeline,pipeline,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,3,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipeline']"
Deployability,"I'm fine with pushing this in without integration tests, assuming @tovanadler has manually tested. :+1: . Nominating @Horneth as second reviewer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/356#issuecomment-169420938:38,integrat,integration,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/356#issuecomment-169420938,1,['integrat'],['integration']
Deployability,"I'm getting an error from Cromwell `Failed to coerce one or more keys or values for creating a Map[String, Int?]` when defining an object as an input to a task that consists of a string, file and int. I would expect the Int to get converted into a string, but the error makes it seem as if Cromwell is trying to convert all of the values into integers. Here is the part of the WDL that's throwing the error: https://github.com/HumanCellAtlas/pipeline-tools/blob/c949cb5ffa2df8f2a7fc7d7a4c34478e8eadbf34/adapter_pipelines/cellranger/adapter.wdl#L222",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4131:442,pipeline,pipeline-tools,442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4131,1,['pipeline'],['pipeline-tools']
Deployability,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3060:161,pipeline,pipeline-tools,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060,2,['pipeline'],"['pipeline-tools', 'pipelines']"
Deployability,I'm going to close this MR until it is ready to be updated.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6466#issuecomment-927871838:51,update,updated,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6466#issuecomment-927871838,1,['update'],['updated']
Deployability,I'm going to overrule `codecov/project` here since it appears to be a hiccup. The patch coverage is at around 77% (which is higher than the usual project overall average) so I'm pretty confident this doesn't represent an actual overall coverage reduction.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-470343985:82,patch,patch,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-470343985,1,['patch'],['patch']
Deployability,"I'm having problems using Cromwell for fast iteration on small workflows. I realize I might be using Cromwell for things it was not supposed to do, but since it is so very slow I thought I might ask anyway. Just so you understand my application, I'm not running bioinformatics pipelines: I'm rather interested in using WDL instead of Makefiles to do various scientific calculations. It's often a bunch of relatively simple Python or R scripts that I want to connect in a chain, and then sweep some parameters, etc. While developing and debugging the workflows (the majority of the time) I have a need for running and re-running single tasks or small sub-workflows many times. I was hoping I could use Cromwell to do this, but at present it just is too slow. Let's say I run a tiny workflow like the [Hello world example](http://cromwell.readthedocs.io/en/develop/tutorials/FiveMinuteIntro/#step-2-writing-your-first-workflow-description) from Cromwell's docs:; ```wdl ; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. I'm on Ubuntu 16.04, I start a Cromwell server and wait for it to spin up. Then. ```bash; $ time /usr/lib/jvm/java-8-openjdk-amd64/bin/java -jar cromwell-31.jar submit wf.wdl ; [2018-04-04 15:37:15,92] [info] Slf4jLogger started; [2018-04-04 15:37:17,06] [info] Workflow ab6ec0d5-20d2-4113-a58a-0b0e15097476 submitted to http://localhost:8000. real	0m2.536s; user	0m5.316s; sys	0m0.292s; ```. Just submitting the job takes 2.5 seconds wall clock time. Then watching the server do the job takes another 21 seconds:. ```bash; 2018-04-04 15:37:17,001 cromwell-system-akka.dispatchers.api-dispatcher-117 INFO - WDL (Unspecified version) workflow ab6ec0d5, so essentially the job is equivalent to-20d2-4113-a58a-0b0e15097476 submitted. [various log outputs]. 2018-04-04 15:37:38,440 cromwell-system-akka.dispatchers.engine-dispatcher-6 INFO - WorkflowExecutionActor-ab6ec0d5-20d2-4113-a58a-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3477:277,pipeline,pipelines,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477,1,['pipeline'],['pipelines']
Deployability,"I'm not a Cromwell dev, but I've dealt with this quite a lot, so I have some experience here... When resource issues happen on local-Cromwell, it is usually due to scattered tasks either [all running at once (which is the default behavior)](https://broadworkbench.atlassian.net/browse/CROM-6716), or, if they're running one-at-a-time, [things getting stuck](https://github.com/broadinstitute/cromwell/issues/6946). But none of your tasks are scattered, so the usual easy fixes don't apply. Unfortunately, Cromwell ignores most of your runtime arguments when running in ""local mode"" including memory, cpu, and disk size. This isn't something you can configure, it just doesn't know how to handle them. You'll see warnings to that effect when the tasks launch, eg:. ```; [2022-12-13 12:11:22,26] [warn] LocalExample [5aba40a5]: Key/s [preemptible, disks, cpu, memory] is/are not supported by backend. Unsupported attributes will not be part of job executions.; ```. One thing you can try doing to get around this is to make sure Docker is getting as much memory as you can give it. If you're using Docker Desktop, you can do this in Preferences > Resources, then cranking the memory slider as far to the right as you feel comfortable doing. But I do notice you're using a Linux machine, so it's probably a good idea to be using [Docker Engine](https://docs.docker.com/engine/install/) instead of Docker Desktop [if this this issue with the Dockstore CLI, which that uses Cromwell to launch workflows, is any indication](https://github.com/dockstore/dockstore/issues/5135), which has a different way of configuring resources. If you're still having issues, please post a followup -- and others, please chime in too if you have ideas. Resource usage on local runs is a bit of a persistent issue with Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1349647888:1373,install,install,1373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1349647888,1,['install'],['install']
Deployability,"I'm not fully up to date on original issue, but this patch verifies that all hsqldb connections use `mvcc`, whether liquibase or slick created. Assigning @mcovarr as first reviewer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/122#issuecomment-125431277:53,patch,patch,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/122#issuecomment-125431277,1,['patch'],['patch']
Deployability,"I'm not great / experienced with Cromwell, and to be honest I'm not sure what native support would mean. What I was trying is to just treat a singularity container like an executable, and add it as a Local backend, sort of like this --> https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. That works to run the analysis step (in a singularity container) just using singularity like any executable. I don't totally understand the job_id so there is a bug, but my colleague @bek is going to take a look! The container is run to produce the output, so that's a good start at least (and probably I'm missing something huge here). So to answer your question... in my wdl at least, I'm just using the same local commands. It looks the same as it would running any Local backend configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571375:267,pipeline,pipeline,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571375,2,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"I'm not sure I understand what you mean by ""putting globs in arguments is still not supported"" ?. In the meantime you can override what the glob command is using this field in your backend configuration: `glob-link-command`; For instance: `glob-link-command = ""ls -L GLOB_PATTERN 2> /dev/null | xargs -I ? ln -s ? GLOB_DIRECTORY""`. This is not well documented, I'll fix that.; (`GLOB_PATTERN` and `GLOB_DIRECTORY` are placeholders that will be replaced at runtime with the right values)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4395#issuecomment-439384644:189,configurat,configuration,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4395#issuecomment-439384644,1,['configurat'],['configuration']
Deployability,"I'm not sure about the specifics your issue, but the latest [Cromwell release (version 86) ](https://github.com/broadinstitute/cromwell/releases) has some improvements related to pulling docker images.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7222#issuecomment-1743447572:70,release,release,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7222#issuecomment-1743447572,2,['release'],"['release', 'releases']"
Deployability,I'm proposing here the configuration I use for [TORQUE](https://www.adaptivecomputing.com/products/torque/).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5082:23,configurat,configuration,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5082,1,['configurat'],['configuration']
Deployability,"I'm running Ubuntu 18.04 on a local server and backend, on docker containers, using Cromwell release 52, and WDL ""version development"". I'm using the development ""Directory"" type to provide the path to the BLAST database directory to a Docker container running NCBI BLAST. The task definition below executes successfully, but the localization via soft/hard link fails, and all the blast database files (>100GB) get copied multiple times over, as this task is called by a scatter function. Other steps in the pipeline are able to successfully use caching and get localized via soft/hard links, which rules out a configuration issue, and issues are only found right when using the ""Directory"" Type. To help in understanding my setup, I've included:. **inputs:**; ```; {; ""good_donor_good_recipient.blastdb"": ""../../data/blast/blastdb"",; ""good_donor_good_recipient.fasta"": ""../../data/ref_genomes/pseudomonas.fasta"",; }; ```. **task definition:**; ```; version development; task n {; input {; File fasta ; Directory blastdb; String out_file = ""~{basename(fasta)}.blast""; }; command <<<; export BLASTDB=~{blastdb} ; blastn \; -query ~{fasta} -db nt -num_threads 24 -evalue 1 -outfmt '6' -out ~{out_file}; >>>; output { File out = out_file }; runtime { docker: ""ncbi/blast:2.10.1"" }; }; ```. **confiuration snippet - localization only:**; ```; filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; # Call caching strategies; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""md5""; check-sibling-md5: false; }; }; }; ```. **logs:**; ```; [2020-08-08 19:20:00,49] [error] Failed to hash ""../../data/blast/blastdb"": Is a directory; [2020-08-08 19:20:00,49] [warn] Localization via hard link has failed: /workflows/cromwell-executions/good_donor_good_recipient/f7947643-2729-483f-b987-44ef932f88bd/call-blaster/main/6e4fa8a1-0d72-486e-a9ae-254319c4915d/call-blaster/shard-20/inputs/2058596876/blastdb -> /data/blast/blastdb: Operation not permi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737:93,release,release,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737,3,"['configurat', 'pipeline', 'release']","['configuration', 'pipeline', 'release']"
Deployability,"I'm running a pipeline and for some of the tasks, sometime I get the following error:. `[E::hts_open_format] Failed to open file ...`. What I find weird is that if I re-run it, it runs successfully. It looks a ""stochastic"" error. Below you can find the full logs for that task and, as you can see, the file was successfully localized. ```; timestamp,message; 1608596940672,*** LOCALIZING INPUTS ***; 1608596942260,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz.tbi; 1608596944807,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz; 1608596946491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi to focal-gwf-core/cromwell-ex",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:14,pipeline,pipeline,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['pipeline'],['pipeline']
Deployability,"I'm running cromwell 46 with AWS, and having problems with call caching ... 2019-09-30 15:37:20,124 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Failed to hash ""s3://bdtx-scratch/Andrei/bdtx_dataset_1.00_anno_column_description.txt"": [Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null); 2019-09-30 15:37:20,125 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - 66419bab:count_lines.countLines:-1:1: Hash error ([Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null)), disabling call caching for this job. call caching settings:; call-caching {; enabled = true; invalidate-bad-cache-results = false; }. Thanks. ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5204:1779,configurat,configuration,1779,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5204,1,['configurat'],['configuration']
Deployability,"I'm running the ENCODE ATAC SEQ pipeline [https://github.com/ENCODE-DCC/atac-seq-pipeline.git](url) on a SGE cluster.; We don't allow hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:32,pipeline,pipeline,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,3,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"I'm specifically thinking of an environment variable - I don't see anything that looks like it might meet these criteria. The goal would be to alter the program behavior slightly depending on whether it is run in a workflow (in which case additional output may be helpful) or on its own (in which case the user probably doesn't want random machine-readable output files appearing in the run dir). (And yes, I am volunteering to add this if it does not exist, because it's a major obstacle to integrating Cromwell with other systems.)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5235:492,integrat,integrating,492,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235,1,['integrat'],['integrating']
Deployability,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4084:395,configurat,configuration,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084,6,['configurat'],['configuration']
Deployability,"I'm using [CaaS-dev](https://cromwell.caas-dev.broadinstitute.org/) when ran into the issue, with version: `34-66c0fa6-SNAP`. When using the `releaseHold` endpoint to release a workflow, if the workflow_id is in a good format but refers to a non-exist workflow (fake uuid), according to the swagger schema, Cromwell should return 404 to the user. However, now it returns a 500 code which seems to be a wrapper around the actual 404 error:; ```; CromIAM unexpected error: cromwell.api.CromwellClient$UnsuccessfulRequestException: Unmarshalling error: HttpResponse(404 Not Found,List(Server: akka-http/10.1.3, Date: Fri, 20 Jul 2018 13:58:13 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: f4272a19-37dd-4d2c-ba48-ff3844107bf8""; }),HttpProtocol(HTTP/1.1)); ```; This is not a big problem but just brings some incovenience to the error handling process to the users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3911:142,release,releaseHold,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911,2,['release'],"['release', 'releaseHold']"
Deployability,I'm using v28. The jar was downloaded from this page; https://github.com/broadinstitute/cromwell/releases/tag/28,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2487#issuecomment-317861750:97,release,releases,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2487#issuecomment-317861750,1,['release'],['releases']
Deployability,"I'm wondering if it'd make more sense to add it to the [release wdl](https://github.com/broadinstitute/cromwell/blob/3e577223845ee8d20cab38590579b65fa73fe64e/release/release_workflow.wdl). @tomkinsc what do you think, yo'ure just looking for this to be applied to official releases, correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301188532:56,release,release,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301188532,3,['release'],"['release', 'releases']"
Deployability,"I'm wondering if this is related to cromwell creating new job definitions for **every** new call, versus using parameter substitution to modify the inputs for a single job definition? There may be some sort of backend issue with the integration to the AWS APIs that and old job definition is being called incorrectly instead of yet another new definition being created with the correct inputs? . This would track with the workflow log saying that the job definition already exists and then re-using a job that has inputs for a completely different sample.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-501250451:233,integrat,integration,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-501250451,1,['integrat'],['integration']
Deployability,"I've been getting the following GATK error when running a CWL pipeline on Cromwell:; ```; ##### ERROR MESSAGE: Writing failed because there is no space left on the disk or hard drive. Please make some space or specify a different location for writing output files.; ``` ; To attempt to resolve this, I added the following requirement to the GATK tool in question:; ```yaml; - class: ResourceRequirement; coresMin: 2; ramMin: 8000; tmpdirMin: 100000; outdirMin: 100000; ```. However, this didn't resolve the GATK issue. I looked further into the Cromwell source and I found the following code: https://github.com/broadinstitute/cromwell/blob/44297611175c8a2a92ee71f0fa9c34419b69f0b8/cwl/src/main/scala/cwl/requirement/RequirementToAttributeMap.scala#L46-L57. To me, it looks like these values are being thrown out (I could be wrong). . Should we not convert these into a [`disk` statement](https://cromwell.readthedocs.io/en/develop/RuntimeAttributes/#disks) that Cromwell can understand and implement?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4507:62,pipeline,pipeline,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4507,1,['pipeline'],['pipeline']
Deployability,"I've been seeing the configurable epilogue more as a ""hey user, here's a place for you to add stuff for your specific setup"" rather than ""you absolutely need your epilogue config to have this if you want your backend to work"", but maybe they're not that far apart after all.; I'd be ok with making it a ""required"" epilogue for cloud backends as long as we make it pretty clear in the changelog that it's required and that a config update is necessary. Also I'm not too worried that anyone has been relying on this anyway since it's a hack to support empty directories which hopefully is not wildly used.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505:431,update,update,431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505,2,['update'],['update']
Deployability,"I've been working on testing the AWS Batch Cromwell support, following documentation from @wleepang (https://docs.opendata.aws/genomics-workflows) in the CWL hackathon with @cjllanwarne and @aednichols. The workflow is a small test with everything in an S3 bucket:; ; https://github.com/bcbio/test_bcbio_cwl/tree/master/aws. I'm happy to report that I made good progress and have bcbio-vm using CloudFormation templates to setup the Cromwell batch ready AMI and AWS Batch requirements. I can then generate the right Cromwell AWS configuration and launch jobs to AWS batch. I see them get submitted, EC2 resources get spun up and jobs get queued and run. Awesome. When they're all ready and prepped to run, the instances fail with not finding the `cwl.inputs.json` file staged into the working directory:; ```; [2019-01-25 13:53:43,03] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:53:59,61] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:58:25,58] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:39,11] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:40,06] [error] WorkflowManagerActor Workflow 2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 failed (during ExecutingWorkflowState): Job alignment_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-alignment_to_rec/alignment_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", lin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586:529,configurat,configuration,529,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586,1,['configurat'],['configuration']
Deployability,"I've gone through and updated the dependency graph of updating sttp, you can view diff at https://github.com/delagoya/cromwell/tree/update-depversions . Still one more set of errors to fix: ; ```; root(update-depversions)> | 31>; [info] Compiling 4 Scala sources to $HOME/src/cromwell/womtool/target/scala-2.12/classes...; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:46: value foldMap is not a member of Set[wdl.draft2.model.WdlGraphNode]; [error] val thisLevelNodesAndLinks: NodesAndLinks = callsAndDeclarations foldMap { graphNode =>; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:56: value foldMap is not a member of Set[wdl.draft2.model.WdlGraphNode]; [error] val subGraphNodesAndLinks: NodesAndLinks = subGraphs foldMap { wdlGraphNode =>; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:26: private val clusterCount in object GraphPrint is never used; [error] private val clusterCount: AtomicInteger = new AtomicInteger(0); [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:40: local default argument in method listAllGraphNodes is never used; [error] def upstreamLinks(wdlGraphNode: WdlGraphNode, graphNodeName: String, suffix: String = """"): Set[String] = wdlGraphNode.upstream collect {; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/WomGraph.scala:54: value foldMap is not a member of Set[wom.graph.GraphNode]; [error] graph.nodes foldMap nodesAndLinks _; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/WomGraph.scala:89: private method nodesAndLinks in class WomGraph is never used; [error] private def nodesAndLinks(graphNode: GraphNode): NodesAndLinks = {; [error] ^; [error] 6 errors found; [error] (womtool/compile:compileIncremental) Compilation failed; [error] Total time: 4 s, completed Apr 16, 2018 9:00:54 AM; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381592626:22,update,updated,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381592626,3,['update'],"['update-depversions', 'updated']"
Deployability,"I've had a request from @geoffjentry and @mcovarr to convert the ""validation response aggregation"" into a separate actor - see https://docs.google.com/a/broadinstitute.com/document/d/1qTXiPtiJcmfmWghLC_uBeWlL2SuCE2Ek1xBiT0j8-iQ/edit?usp=sharing - I'll rework this PR then re-open",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-207589678:151,a/b,a/broadinstitute,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-207589678,1,['a/b'],['a/broadinstitute']
Deployability,"I've made the builds of both toolsets, updated the dockerfile (see dsde-pipelines branch kc_jg_turbocharge), built and pushed the docker image for our testing (kcibul/tiledb-with-gcloud:2.2.5-1492828987) in the JG WDL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296491337:39,update,updated,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296491337,2,"['pipeline', 'update']","['pipelines', 'updated']"
Deployability,I've merged this as-is to make sure it gets into the release! 🎉,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2049#issuecomment-284017153:53,release,release,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2049#issuecomment-284017153,1,['release'],['release']
Deployability,"I've tested both WDLs [in the forum post](http://gatkforums.broadinstitute.org/gatk/discussion/8864/how-can-a-method-configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file) on firecloud-dev, which is running Cromwell 27. Neither of them work. Reopening this issue, but there isn't anything for us over in FC to do here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-307478031:117,configurat,configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-307478031,1,['configurat'],['configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file']
Deployability,"I've updated JES_STATUS too. It was initialised to """". It felt disingenuous to claim that an empty string represents a real status name so now it's explicitly None instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/194:5,update,updated,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/194,1,['update'],['updated']
Deployability,"I've updated JES_STATUS too. It was initialised to """". It felt disingenuous to claim that represents a real status name so now it's explicitly None instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/193:5,update,updated,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/193,1,['update'],['updated']
Deployability,"IIRC (which is really not guaranteed), as far as the localization of `cwl.inputs.json` is concerned in PAPI 2, [this bit](https://github.com/broadinstitute/cromwell/blob/aeca54929b5d85e7961ac01a784c08a129cfc265/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L686) is what maps the cloud path of ""ad hoc files"" to the localized path (which the AWS equivalent [doesn't have](https://github.com/broadinstitute/cromwell/blob/aeca54929b5d85e7961ac01a784c08a129cfc265/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchAsyncBackendJobExecutionActor.scala#L420)).; So it's possible the `cwl.inputs.json` is actually localized but not to the right place and so the tool can't find it.; Again my memory is fading quickly so don't take this as 💯 :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-459299224:236,pipeline,pipelines,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-459299224,3,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipelines']"
Deployability,IMO anything being updated in a shared environment should be protected but i also loathe shared mutable state so YMMV,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3344#issuecomment-437031892:19,update,updated,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3344#issuecomment-437031892,1,['update'],['updated']
Deployability,"IMO if we put any security recommendations in our README we should be very careful to disclaimer it. **Heavily**. Maybe with something along the lines of:. ```; Warning! ; - Only YOU are responsible for your own security! ; - Cromwell is NOT a security appliance! ; - What follows are ideas and starting points and not necessarily a secure system configuration in your situation""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538:347,configurat,configuration,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538,1,['configurat'],['configuration']
Deployability,"INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:1633,configurat,configuration,1633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['configurat'],['configuration']
Deployability,"Ideally directly written to GCS (see https://github.com/broadinstitute/gatk/issues/4478), but we could also periodically update. In a very imperfect world where all simple solutions are not feasible we can explore using something built for this purpose (many solutions exist, we're just trying to be quick & dirty here)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3343:121,update,update,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3343,1,['update'],['update']
Deployability,Idempotent Release process,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2573:11,Release,Release,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2573,1,['Release'],['Release']
Deployability,"If I'm understanding the concern correctly, you're worried about about Cromwell retrying a task based on the return code, even when the problem was not memory related. Cromwell requires a member of `system.memory-retry-error-keys` to be present, so it does not just use the return code. Note that memory retry was marked as an experimental feature and has experienced a breaking change since this issue was filed: https://github.com/broadinstitute/cromwell/releases/tag/56. Since I _think_ your concern is already addressed, I'm going to close the issue. Feel free to open if otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-794320092:457,release,releases,457,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-794320092,1,['release'],['releases']
Deployability,"If a java.util.concurrent.ExecutionException, check if it has an inner cause that is actually fatal.; Pre-update to SBT 1.x.; The cromwell multi-project with SBT 1.x consumes a lot of memory. To avoid OOME:; - Bump IntelliJ from 768 to something like 1786.; - Bump .sbtopts to something like 3072. Build updates while prepping fo sbt version bump, including using sbt-doge temporarily for simplified SBT 0.13 cross version build syntax.; DRYed out a bit of the module dependency graph.; Publish hotfix libraries, not just executables.; Replaced deprecated commons-lang with commons-text replacements.; Fixed test class errantly nested within its companion object.; Fixed test that was only liquibasing metadata, and not the original database.; Removed unused TestActorSystem.; Fixed missing test in ""common"".; Removed old ""common"" sbt files.; Cleaner version of assembly.; SwaggerUI injected into the cromwell-version.conf, so it only needs to be editted in one place.; Replaced usage of 'sbt ""project foo"" sometask' with 'sbt foo/sometask'.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2821:106,update,update,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2821,3,"['hotfix', 'update']","['hotfix', 'update', 'updates']"
Deployability,"If a timeout is not provided, GCP defaults to [setting a timeout of 7 days](https://developers.google.com/resources/api-libraries/documentation/genomics/v2alpha1/java/latest/com/google/api/services/genomics/v2alpha1/model/Pipeline.html), after which the pipeline will abort. Occasionally pipelines genuinely need to run >7 days. These changes allow this value to be user-configured.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5273:222,Pipeline,Pipeline,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273,3,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline', 'pipelines']"
Deployability,"If call cache writing is disabled and a cache miss is known, we could update the various File Hasher actors and tell them not to bother computing no-longer-required file hashes. - [X] Depends on #1307",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1316:70,update,update,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316,1,['update'],['update']
Deployability,"If it is intended that the `qsub` files also capture the `command` output, then I think the only 'bug' is the default configuration and docs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393106416:118,configurat,configuration,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393106416,1,['configurat'],['configuration']
Deployability,If it's a recurring issue whould it not make sense to have it configurable?; 1. Patch it on every cromwell release; 1. Hard-code a chmod reset in every WDL command,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394406690:80,Patch,Patch,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394406690,2,"['Patch', 'release']","['Patch', 'release']"
Deployability,"If some error occurs during release, it'd be nice to kick off the whole job again and have the process adjust accordingly. ## Should we release this project?; Would need to consult that these are correct and current:. * git tags; * release on github; * jar in JFrog; * brew Pr exists (for cromwell). This decision will determine version number for downstream projects.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2573:28,release,release,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2573,3,['release'],['release']
Deployability,"If someone wants to see these kinds messages, they can change the configuration of akka (see http://doc.akka.io/docs/akka/current/java/logging.html#Auxiliary_logging_options), instead. - went through the `whenUnhandled` handlers and removed logging calls that had no or very little info; - removed one test that was associated with one of those logs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4260:66,configurat,configuration,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4260,1,['configurat'],['configuration']
Deployability,If that test is chronically failing due to Docker Hub flakiness we should tag it as Integration so it doesn't break our builds.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166384762:84,Integrat,Integration,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166384762,1,['Integrat'],['Integration']
Deployability,If the AWS backend uses ioActor this may already be covered in configuration?; ```; system {; io {; # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # the quota availble on the GCS API; #number-of-requests = 100000; #per = 100 seconds. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }; }; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436885778:63,configurat,configuration,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436885778,1,['configurat'],['configuration']
Deployability,"If the direction is ok, I can add unit/integration tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2105413034:39,integrat,integration,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2105413034,1,['integrat'],['integration']
Deployability,"If you back Cromwell with a database (see [persisting data between restarts](https://cromwell.readthedocs.io/en/stable/tutorials/PersistentServer/)), you are able to stop Cromwell and restart without job information loss. After setting up this database, you can enable [call-caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) which if you modify your workflow, tools or inputs, will use previously computed results where:. - The command line used to generate the files is exactly the same; - The computed files still exist; - Other [call-caching configurations](https://cromwell.readthedocs.io/en/stable/wf_options/Overview/#call-caching-options) are valid. Alternatively, you could always stop your workflow, and modify it to run only from the point you've executed from with your already computed files. Straight forward, but could be a little tedious.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-558871640:578,configurat,configurations,578,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-558871640,1,['configurat'],['configurations']
Deployability,"If you use the example in the docs at https://github.com/broadinstitute/cromwell#database. you'll get a warning:. `Move the configuration directly under the 'database' element, and remove the key 'database.config'.`. The docs should be updated. Below is the config given in the readme. database {; config = main.mysql. main {; mysql {; db.url = ""jdbc:mysql://localhost:3306/cromwell""; db.user = ""root""; db.password = """"; db.driver = ""com.mysql.jdbc.Driver""; db.connectionTimeout = 5000 # NOTE: The default 1000ms is often too short for production mysql use; driver = ""slick.driver.MySQLDriver$""; }; }. test {; ...; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1730:124,configurat,configuration,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1730,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"If your service configuration is missing or invalid, Cromwell should fail to start up. Currently, nothing is seen until workflows are submitted and immediately fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/871:16,configurat,configuration,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/871,1,['configurat'],['configuration']
Deployability,"Ignoring codecov because there are some ""in case they're useful"" implementations here that aren't activated without specific configuration and not going to be used in anything other than manual testing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6047#issuecomment-736664007:125,configurat,configuration,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6047#issuecomment-736664007,1,['configurat'],['configuration']
Deployability,Impact of runtime params on versioning of method configuration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2268:49,configurat,configuration,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2268,1,['configurat'],['configuration']
Deployability,"Implement any necessary logic to calculate hashes. Follow the pattern of the FileHasherActor: The EJHA creates an actor which can make the hash. This actor sends each hash result back to the EngineJobHashingActor individually and lets it do the magic of combining everything together.; - [x] command string (one of the initial hashes in `EJHA`); - [x] all outputs expressions (one of the initial hashes in `EJHA`); - [x] all non-file inputs (included in the set of initial hashes in `EJHA`); - [x] dockerhub/GCR hash (`EngineDockerHashingActor`) or docker name (tuple3 of (namespace, repository, tag, one of the initial hashes in `EJHA`) depending on configuration; - [x] runtime attributes (`BackendRuntimeAttributeHashingActor`); - [x] file inputs (`FileHasherActor`, and onwards to GCS, SFS specific `XYZFileHasherActor`s)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1230:651,configurat,configuration,651,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230,1,['configurat'],['configuration']
Deployability,"Implement the [paging protocol](https://github.com/ga4gh/workflow-execution-service-schemas/pull/30) specified by Cromwell. If it makes sense to do so this could be a candidate to update Cromwell itself. To the extent that **that** could require a Cromwell API change, contact @ruchim if going down that path",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3843:180,update,update,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3843,1,['update'],['update']
Deployability,"Implement the `/describe` endpoint as described in the doc and discussed with @ruchim. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4333:796,configurat,configuration,796,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4333,1,['configurat'],['configuration']
Deployability,Implementer - please check out https://www.trivento.io/creating-settingsactor-configuration-properties-using-akka-extensions/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-231463505:78,configurat,configuration-properties-using-akka-extensions,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-231463505,1,['configurat'],['configuration-properties-using-akka-extensions']
Deployability,Implements the grammar changes for https://github.com/openwdl/wdl/pull/185 and https://github.com/openwdl/wdl/pull/163 for draft-3. Based on the grammar file updates in https://github.com/cjllanwarne/wdl/tree/cjl_draft3_grammar_changes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3274:158,update,updates,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3274,1,['update'],['updates']
Deployability,Import support in womtool upgrade,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3920:26,upgrade,upgrade,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3920,2,['upgrade'],['upgrade']
Deployability,Improve AWS Integration:. - Docker Hub Authentication; - awsBatchRetryAttempts; - ulimits; - Call Caching with ECR private ([geertvandeweyer](https://github.com/geertvandeweyer) and [markjschreiber](https://github.com/markjschreiber/cromwell/)); - revised localization functions to improve stability ([geertvandeweyer](https://github.com/geertvandeweyer)); - Extra failure handling for Batch ([geertvandeweyer](https://github.com/geertvandeweyer)); - AWS/Batch error handling improvements ([geertvandeweyer](https://github.com/geertvandeweyer)); - Correct retry logic for spot kills ([geertvandeweyer](https://github.com/geertvandeweyer)); - handling of very rare early/late job killing ([geertvandeweyer](https://github.com/geertvandeweyer)); - Sychronize multipart uploads between callcache and jobscripts ([geertvandeweyer](https://github.com/geertvandeweyer)),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6835:12,Integrat,Integration,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6835,1,['Integrat'],['Integration']
Deployability,Improve Cromwell Release Process,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2404:17,Release,Release,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404,1,['Release'],['Release']
Deployability,"In #4406, we added a cleanup routine that deletes unzipped imports when we're done with them. It would be nice to handle imports without touching the disk at all - see [this branch](https://github.com/broadinstitute/cromwell/tree/aen_4406_zipfs) for a mostly-working implementation. That implementation is 99% complete but seems to suffer from a [Scala bug](https://github.com/scala/bug/issues/10247) in certain packaging/deployment configurations - such as the one used in Travis!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4648:422,deploy,deployment,422,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4648,2,"['configurat', 'deploy']","['configurations', 'deployment']"
Deployability,"In Cromwell versions 67 and earlier `virtual-private-cloud` configuration exclusively specifies Google project label keys, not literal values. The actual values are specified in labels on the Google project. For example with a VPC config like:. ```hocon; virtual-private-cloud {; network-label-key = ""my-network-label-key""; subnetwork-label-key = ""my-subnetwork-label-key""; auth = ""application-default""; }; ```. As seen in the [labels page in GCP console](https://console.cloud.google.com/iam-admin/labels), there should be project labels with key/values of `my-network-label-key`/`my-private-network` and `my-subnetwork-label-key`/`my-private-subnetwork`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905062843:60,configurat,configuration,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905062843,1,['configurat'],['configuration']
Deployability,"In DSDE methods, there are four backend configurations we would like to support.; - local w/ docker; - local w/o docker; - SGE (req: no docker); - JES (req: docker). But we would like to support these without having to change the WDL file itself. Currently, this is done with complex scripts that choose default options and application configurations. But perhaps there is an easier way... by parameterizing runtime attributes. I hear cromwell already supports that, except for the case where we do not want to run in docker at all (e.g. local w/o docker backends). Assuming that you can specify runtime parameters as part of a workflow. In other words, assuming that the following is valid:. ```wdl. workflow yo {; String msg; String docker_image. call task1 {; input:; msg=msg,; docker_image=docker_image; }; }. # Run a message in an arbitrary docker container (e.g. ""broadinstitute/eval-gatk-protected:crsp_validation_latest""); task task1 {; String msg; String docker_image; ; command {; echo ${msg}; } ; ; runtime {; docker: ""${docker_image}""; memory: ""1GB""; }; }; ```; ```; {; ""yo.msg"": ""foo""; ""yo.docker_image"": ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; }; ```; The above WDL+json should work for JES and ""local w/ docker"" backends. However, to support local w/o docker, we need to be able to specify a ""null"" value, which cromwell will interpret as, ""do not use docker"" or ""docker was never specified"". For example:; ```; {; ""yo.msg"": ""foo""; ""yo.docker_image"": """"; }; ```; My understanding of cromwell is that this json + WDL above will cause a failure in cromwell. If this functionality already exists, please close this issue. This would make our WDLs more complicated, but it would increase flexibility and move runtime specification into the json file (which is easier than juggling default options).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1804:40,configurat,configurations,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1804,2,['configurat'],['configurations']
Deployability,"In a configuration like. ```wdl; workflow ifs_in_scatters {; call hello. scatter (n in range(5)) {; if (true) {; call goodbye { input: i = hello.out }; }; }; }; ```; When the conditional graph is created, all nodes outside of the scatter get ""wrapped"" in an `OuterGraphInputNode` that gets passed into the inner conditional graph so that nodes in the inner graph can reference nodes outside of the scatter.; The issue is that those OGINs are created with `preserveScatterIndex = true` even though the node they're pointing to is outside of the scatter. This was preventing the `ExecutionStore` from detecting the `i = hello.out` expression as being runnable because it was looking for a `hello.out` node in `Done` state at index `n`, which doesn't exist since `call hello` is outside the scatter.; This PR changes that to use the `preserveIndexForOuterLookups ` value of the conditional / scatter instead, which in this case will be `false`, because the scatter node does set `preserveScatterIndex = false` to build its inner graph (in this case the if).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3158:5,configurat,configuration,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3158,1,['configurat'],['configuration']
Deployability,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:341,update,updates,341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823,3,['update'],"['update', 'updates']"
Deployability,"In case @kshakir isn't the one managing the master patch today, reminder that lenthall needs to be deal with in that situation as well",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213449018:51,patch,patch,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213449018,1,['patch'],['patch']
Deployability,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:233,configurat,configuration,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288,3,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"In case someone is looking to acheive this: The ~wdl task that runs as part of configuration has access to 'job_name'. I have passed this through as an environment variable. . ```; submit-docker = """"""; docker run \; --entrypoint ${job_shell} \; -e CROMWELL_JOB_NAME=${job_name} \; ```. Side note it would be nice if the variables available to that conf script were documented - I am reverse engineering from the example configs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721:79,configurat,configuration,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721,1,['configurat'],['configuration']
Deployability,"In general, it seems like the swagger spec, which can be very useful, has not been updated to reflect recent changes to the cromwell API.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3328#issuecomment-369956933:83,update,updated,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3328#issuecomment-369956933,1,['update'],['updated']
Deployability,"In https://github.com/broadinstitute/cromwell/pull/4502 I ignored three tests related to the refresh token functionality because they were blocking all other merges and I had no idea how to fix them. My rationale for doing so is my impression the feature is unused/end-of-life. @ruchim to decide whether we; 1. Fix the tests; or; 2. Delete the feature . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4504:1063,configurat,configuration,1063,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4504,1,['configurat'],['configuration']
Deployability,"In most cases, the `-branch` build has a significantly shorter runtime than the `-pr` build: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/367/workflows/7b1a2a51-80b7-432a-b883-4c28c15741d4. Is the `-branch` build doing the right thing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776199989:118,pipeline,pipelines,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776199989,1,['pipeline'],['pipelines']
Deployability,"In order to allow the Private IP flag, it needs to be set both at pipeline creation time and run time. Run time resources override create time resources. However mount point must be set at create time but cannot be re-set at runtime... . ```; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""disks must not have mount points at run time"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""disks must not have mount points at run time"",; ""status"" : ""INVALID_ARGUMENT""; }; ```. This sets mount points to null for run time only. The rest is strictly identical to create time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1507:66,pipeline,pipeline,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507,1,['pipeline'],['pipeline']
Deployability,"In order to de-risk a time-sensitive deployment, I am staging this as an option",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5351:37,deploy,deployment,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5351,1,['deploy'],['deployment']
Deployability,"In order to deploy CromIAM, we need to add the deployment configurations to `firecloud-develop`. This means, at a minimum:; * a consul-template-ized `docker-compose.yml` for environments `live`, `fiab`, and `local`; * consul-template-ized `cromiam.conf` file ; * A stable-versioned, published docker image of cromiam. This should be submitted as a PR to `dev` branch, whereupon it makes its way through QA to the production environment. I've submitted my initial work(a skeleton) to the `db_add_cromiam` branch in the firecloud-develop repo.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4545:12,deploy,deploy,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4545,3,"['configurat', 'deploy']","['configurations', 'deploy', 'deployment']"
Deployability,"In other words, I'm looking for the *default* configuration of Cromwell to *never* run `isAlive` (except on server restarts, like today). A user should have to deliberately change their configuration to make it happen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424373365:46,configurat,configuration,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424373365,2,['configurat'],['configuration']
Deployability,"In situations where a workflow is being restarted, receives an abort command but is only at materialization stage or initialization stage we currently assume it's ok to stop right here and declare it `Aborted`. This assumption is wrong as jobs are probably running and we need to re-connect to them so we can 1) abort them 2) update their status.; In the same vein, if a workflow is in the workflow store waiting to be restarted and gets aborted, we need to still pick it up and reconnect to jobs to abort them.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2829:326,update,update,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2829,1,['update'],['update']
Deployability,In that case is it a won't-fix situation? We can keep patching if necessary.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336166266:54,patch,patching,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336166266,1,['patch'],['patching']
Deployability,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > …; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:192,configurat,configurations,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090,4,['configurat'],"['configuration', 'configurations']"
Deployability,"In the example configuration, the submit-docker tries to run. docker run ... ${docker} ${script}. but ${script} will not be accessible from the docker image unless, by coincidence, the location where we are running from in the local filesystem is the same as the dockerRoot. What we want to run instead is . docker run .... ${docker} ${docker_script}. Since this is the default configuration, it has the potential to cause a lot of unnecessary confusion (eg for me)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5015:15,configurat,configuration,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5015,2,['configurat'],['configuration']
Deployability,"In the latest releases of Cromwell (28+), the tmpDir variable is set to be created under the [current working directory of the workflow task](https://github.com/broadinstitute/cromwell/blob/master/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L186) by default. In a cluster environment, we have observed that this limits how quickly a task can execute when using a shared file system (NFS/Lustre/etc.) because these filesystems perform poorly with small reads and writes. In earlier versions of Cromwell, we worked around this by setting tmpDir within each task of the WDL to a path mounted on a local disk, but now the tmpDir variable seems to be set globally resulting in the local tmpDir variable getting overwritten. . We can modify the tmpDir path in StandardAsyncExecutionActor, but this requires recompiling Cromwell for each environment where Cromwell is deployed. Would it be possible to make the tmpDir variable configurable within reference.conf? That way a site admin can deploy Cromwell without needing to recompile, and quickly choose the tmpDir path that is optimal for their environment.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2655:14,release,releases,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2655,3,"['deploy', 'release']","['deploy', 'deployed', 'releases']"
Deployability,"In the logs, I noticed a few instances of `Changing status from Running to Running`. This turned out to be because `Running(None) != Running(someCostData)`. I updated the code so that the cost data from the old status is passed into the new status (which also improves the polling behavior that was happening)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7448:159,update,updated,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7448,1,['update'],['updated']
Deployability,"In version 31 I had in my configuration: ; ```temporary-directory = ""$(mkdir -p $TMPDIR && echo $TMPDIR)""```. Which resulted in:; ```; tmpDir=$(; set -e; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy_Workflow/f7b9ac1e-994c-46bd-bad5-e11ac6696165/call-happy/execution; tmpDir=""$(mkdir -p $TMPDIR && echo $TMPDIR)""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; ```. In version 32 the same config results in:; ```; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy/356aa17f-6276-44e0-9859-391c6c58cf49/call-happy/execution; tmpDir=`$(mkdir -p $TMPDIR && echo $TMPDIR)`; chmod 777 ""$tmpDir""; ```. which executes using my $() as well as the cromwell provided ``, causing an error (which is no longer caught by set -e). Then many subsequent errors as the script tries to write to / (lucky I did not rm -r!). I thought there was documentation on readthedocs but I cannot find it with the inbuilt search or google. If I just remove my $(), it should work, but as the change was not announced and there does not seem to be documentation, I wanted to check what the actual contract is. I would also prefer cromwell did not chmod my tmpdir, but that is a separate issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3784:26,configurat,configuration,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3784,1,['configurat'],['configuration']
Deployability,Initial Centaur Deployment,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/776:16,Deploy,Deployment,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/776,1,['Deploy'],['Deployment']
Deployability,Initial code for integration with cromwell agent,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3915:17,integrat,integration,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3915,1,['integrat'],['integration']
Deployability,Initial implementation complete. Not yet closing this issue as integration tests are not yet operable and there are still several TODOs in the code. See commit f788704.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3426#issuecomment-382881466:63,integrat,integration,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3426#issuecomment-382881466,1,['integrat'],['integration']
Deployability,"Initiated by conversation on http://gatkforums.broadinstitute.org/wdl/discussion/8546/using-different-service-account-on-compute-instance#latest . Currently, we are supplying 'default' to the RunPipelineArgs service_account email. This means the Pipelines API node will spin up using the default compute service account for the project. However (a) that account can be removed and (b) users may want to use a different service account. This should be configurable at both the cromwell server level (ie applies to all workflows) as well as by the workflow options (so it can vary by workflow). This new capability should also be documented in the README (or whever @katevoss says!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1656:246,Pipeline,Pipelines,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1656,1,['Pipeline'],['Pipelines']
Deployability,Inline reference image configuration [BT-122],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6176:23,configurat,configuration,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6176,1,['configurat'],['configuration']
Deployability,"Input caching feature of cromwell is unreliable and does not work with many tools, in particular salmon.; To reproduce it, you can take my quantification WDL pipeline ( https://github.com/antonkulaga/rna-seq/tree/master/pipelines/quantification ), for instance quant_sample ( https://github.com/antonkulaga/rna-seq/blob/master/pipelines/quantification/quant_sample.wdl ) with its dependencies ( https://github.com/antonkulaga/rna-seq/blob/master/pipelines/quantification/quant_run.wdl and https://github.com/antonkulaga/rna-seq/blob/master/pipelines/quantification/extract_run.wdl ) and the any input, for instance:; ```; {; ""quant_sample.experiment"": ""GSM2740704"",; ""quant_sample.salmon_max_memory"": 26,; ""quant_sample.salmon_threads"": 4,; ""quant_sample.key"": ""0a1d74f32382b8a154acacc3a024bdce3709"",; ""quant_sample.samples_folder"": ""/data/samples/EMBED"",; ""quant_sample.salmon_indexes"" : {; ""Homo sapiens"" : ""/data/indexes/salmon/1.3.0/ensembl_101/Homo_sapiens/GRCh38.p13_ensembl_101"",; },; ""quant_sample.transcripts2genes"" : {; ""Homo sapiens"" : ""/data/ensembl/101/species/Homo_sapiens/Homo_sapiens.GRCh38.101.gtf""; }; }; ```; Note: gtf -is regular Human ensemble gtf http://ftp.ensembl.org/pub/release-101/gtf/homo_sapiens/, while salmon index is build from ensemble human genome+transcriptome with https://github.com/antonkulaga/rna-seq/blob/master/pipelines/quantification/quant_index/quant_index.wdl. In all cases when I run local backend (OS is linux) it never caches salmon task properly and reruns it all the time, despite the files are the same.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6143:158,pipeline,pipeline,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6143,7,"['pipeline', 'release']","['pipeline', 'pipelines', 'release-']"
Deployability,"Input:. 1. Cromwell commit [`53c4693a0508518e9a6f957fbba2b1afc7dc90b5`](https://github.com/broadinstitute/cromwell/pull/6739/commits/53c4693a0508518e9a6f957fbba2b1afc7dc90b5); - I ran the job on my own branch for testing, but I updated the job to point to `develop` by PR time. Outputs:. 1. Commit to `main` in Cromwhelm: https://github.com/broadinstitute/cromwhelm/commit/0218cf0ccad9a7e9a74c785c2101ecb77ec56a13. 2. [Image in Docker Hub](https://hub.docker.com/layers/cromwell/broadinstitute/cromwell/79-cba6c97-SNAP/images/sha256-9cd6b3e404efbd1a8610b45ae606dde056dd172e6f1a83fbae36e340fb0103ea?context=explore). 3. [Complete log output of action.](https://github.com/broadinstitute/cromwell/runs/6097432895?check_suite_focus=true). ---. The action runs on self-hosted instances created by devops (`runs-on: self-hosted`) which are 2x as powerful as Travis. The whole build takes ~7 minutes, including `sbt server/docker`. My high spec Broad laptop is just a little faster at ~5 minutes. ~There is a prequisite PR https://github.com/broadinstitute/terraform-ap-deployments/pull/616 that is very close pending a naming discussion.~ Merged. ---. Now we are cool like the other repos that deploy continuously. This is near and dear to my heart:. <img width=""1159"" alt=""Screen Shot 2022-04-20 at 11 58 40 AM"" src=""https://user-images.githubusercontent.com/1087943/164273206-134f1b88-d1bb-447e-b56e-7d01c2e1cada.png"">",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6739:228,update,updated,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739,4,"['continuous', 'deploy', 'update']","['continuously', 'deploy', 'deployments', 'updated']"
Deployability,InputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:3139,Pipeline,PipelinesApiRequestWorker,3139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,"InsertAction.run(JdbcActionComponent.scala:527). 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:38:57,90] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-03-09 15:42:11,14] [info] Using noop to send events.; [2018-03-09 15:49:48,68] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:9041,update,updateCache,9041,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['update'],['updateCache']
Deployability,Installing the Cromwell To Use Local Scratch Device,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:0,Install,Installing,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['Install'],['Installing']
Deployability,Integrate travis with coveralls,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/10:0,Integrat,Integrate,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/10,1,['Integrat'],['Integrate']
Deployability,"Integration test added in #4488, and FYI a patch added in #4508.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4414#issuecomment-453374472:0,Integrat,Integration,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4414#issuecomment-453374472,2,"['Integrat', 'patch']","['Integration', 'patch']"
Deployability,IntelliJ automatically locates this file in the repo and shows it as a run configuration. More convenient to clone that template & modify than to fill in all these fields manually based on prose documentation. Tested by sending to Bec and it was recognized.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6304:75,configurat,configuration,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6304,1,['configurat'],['configuration']
Deployability,Intercept calls to patch the labels for workflows and disallow the updating of `caas_collection_name`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2840:19,patch,patch,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2840,1,['patch'],['patch']
Deployability,"Interesting - I thought it would only push as far as ""can I execute it"" if an inputs file is provided, but that appears to not be true. Thanks for raising this - we should be able to get a fix in for the Cromwell 33 release which should be sometime next week.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-396950006:216,release,release,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-396950006,1,['release'],['release']
Deployability,"Interesting point!. I like the ~~idea~~ philosophy of CC tables being engine only, and queries being completely and solely calculable from metadata. It would probably mean ~~piping~~ forwarding all CC hashes, toggles of ""allowResultReuse"", failures to copy results, etc to the metadata.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781:209,toggle,toggles,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781,1,['toggle'],['toggles']
Deployability,Investigate: Call Caching updates original workflow metadata for start/end time,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4141:26,update,updates,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4141,1,['update'],['updates']
Deployability,"Investigation update; ---. This [branch](https://github.com/broadinstitute/cromwell/tree/tj-metadata-stream-experiment-2) contains code attempting to stream metadata events from the database and build the json as events arrive. It does **not** stream the json itself back to the endpoint. The whole json is still built in memory and then returned (see the end for thoughts on that). Results:. ### The good; - Streaming the data from the database has a positive effect on memory (left CPU, right heap); The below graphs represent Cromwell's activity when it's building a metadata of around 2.8M metadata events. Building metadata without streaming:; ![screen shot 2018-10-17 at 11 16 32 am](https://user-images.githubusercontent.com/2978948/47925639-134b5e80-de95-11e8-8ce3-43f52c4a4067.png). We can see that memory builds up throughout the process of generating the JSON, with a larger burst towards the end. CPU activity is inexistent until the very end where a lot of CPU resources are needed to go through all the events and build the json. Building metadata with streaming:; ![screen shot 2018-10-17 at 10 08 08 am](https://user-images.githubusercontent.com/2978948/47925627-0d557d80-de95-11e8-8ad0-14444456c05a.png). In contrast, here there is moderate CPU activity throughout the process, as well as lots of a much more sawtooth-looking heap graph, indicating that objects are getting GCed a lot. The max memory used is also smaller than for the non streaming version. - Using a streaming approach allows the stream to be stopped at any point in time (say if we ran over the endpoint timeout).; Note that even without streaming data from the database, we can still build the json from the strict set of events using an fs2 stream and stop that if/when needed. Another graph where Cromwell was asked to build several large metadata jsons:. ![screen shot 2018-10-19 at 1 17 28 pm](https://user-images.githubusercontent.com/2978948/47926437-ee57eb00-de96-11e8-89b4-a7df8db9e164.png); Red is non str",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806:14,update,update,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806,1,['update'],['update']
Deployability,"Is it possible for you to inform me or Beri when this is released, so that we can update the folks on this thread: https://gatkforums.broadinstitute.org/firecloud/discussion/comment/52504#Comment_52504",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4166#issuecomment-425232775:57,release,released,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4166#issuecomment-425232775,2,"['release', 'update']","['released', 'update']"
Deployability,Is it still the case that call caching is not implemented for S3? Any update on this issue @delagoya?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-452039660:70,update,update,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-452039660,1,['update'],['update']
Deployability,"Is the cromwell.examples.conf biased to not include full examples for its clients? Having lots of experience reading documentation vs. getting code, my general preference is to find a complete config, copy paste, and customize it. Pointing the user to the docs is okay as long as the full configuration is there (not broken into sections.). On the other hand, I agree it's bad to confuse the user with too many options. My preference would be for consistency. If you have the examples file, you should provide all examples there, as this would be the expectation. If you decide to link to docs, this should be standard for other types, as the two could get disconnected (i.e. different versions across locations).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468832014:289,configurat,configuration,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468832014,1,['configurat'],['configuration']
Deployability,Is there an equivalent for JES runtime attributes validation that could need an update as well ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264853068:80,update,update,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264853068,1,['update'],['update']
Deployability,"Is there any timeline or plan that could be shared for support for modern versions of the WDL spec (e.g. 1.1, 1.1.1, 1.1.2)? There have been multiple feature requests for 1.1, but those are several years old with little update over that time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7409:220,update,update,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7409,1,['update'],['update']
Deployability,Is there any update to this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-453221742:13,update,update,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-453221742,1,['update'],['update']
Deployability,"Is there anything you can suggest with regards to ""looking at your DB""?; Are there queries I could issue myself?. If I understand correctly, when I run `docker-compose ... build`, the Dockerfile is just pulling `broadinstitute/cromwell:develop`. When I look at https://hub.docker.com/r/broadinstitute/cromwell/tags/, I don't see any 32 snapshots explicitly pushed, but I do see that `broadinstitute/cromwell:develop` has been updated. ```; $ docker run --rm -ti broadinstitute/cromwell:develop --version; cromwell 32-d30d9f0-SNAP; ```. I am planning on waiting for my current batch of workflows (that I newly submitted) to complete.; Then I will want to pull the more recent snapshot. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3673#issuecomment-391895624:426,update,updated,426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673#issuecomment-391895624,1,['update'],['updated']
Deployability,"Is this a question, bug, or feature request? As far as I know this does already work (as of release 23). Maybe it missed the changelog though?. Check out the wdlDependencies field of the batch endpoint.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265537710:92,release,release,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265537710,1,['release'],['release']
Deployability,Is this the SFS version of invalidate cache results? Can we also update the centaur test `invalidate_bad_caches` to include whatever causes this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1999#issuecomment-280664752:65,update,update,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1999#issuecomment-280664752,1,['update'],['update']
Deployability,Is this the sort of scheme you mean? ie considering the operation as idempotent if you ask to release twice? ; * `releaseHold` on a held workflow => 201; * `releaseHold` on a running workflow => 200,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4983#issuecomment-494025632:94,release,release,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4983#issuecomment-494025632,3,['release'],"['release', 'releaseHold']"
Deployability,"Issue was resolved and issue was due to launch template of EC2 and It; should be launched with ssm agent installed on it .; Better option is to use Amazon genomics cli . When you deploy a Context agc; will create batch queues and s3 for you . It's better to use that queue and; s3 in cromwell cofig .; It works better. Regards,; Divya. On Fri, Apr 8, 2022 at 3:13 AM thousand-petalled ***@***.***>; wrote:. > Hey @DivyaThottappilly <https://github.com/DivyaThottappilly> do you; > still have this issue? I'm trying to get up and running a basic Hello World; > but keeps getting an S3Exception null error (301).; >; > It seems like you've already past that stage and if you don't mind, could; > you help me setup this?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AGA4NTQVWHYKZC7RLKGT4OTVD7MCNANCNFSM5N3DE7QQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127:105,install,installed,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127,2,"['deploy', 'install']","['deploy', 'installed']"
Deployability,"It appears that the `evaluateFiles` method on `WomExpression` first tries to evaluate the expression, basically calling `evaluateValue`.; In the case of globs, this means trying to read the file containing the list of files that have been globbed, which is; 1) (Almost - see 2)) bound to fail since `evaluateFiles` is called before the task is run to determine the output files that _will_ be generated by an expression, therefore trying to evaluate the glob is pointless and generates unnecessary I/O.; 2) If for some reason the list file _is_ there but is invalid, the result of `evaluateFiles` will be invalid. Why 2) would happen is unknown at the moment, but some of our centaur integration test (dontglobinputs) present very odd failure mode consistent with 2)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4202:684,integrat,integration,684,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4202,1,['integrat'],['integration']
Deployability,"It is a bit unfortunate cromwell 50 was released before this was merged. It will break all testing everywhere since there is no way of running cromwell and knowing beforehand where the outputs will end up.; In biowdl all testing is already pinned to cromwell-48 to ensure continued operation, we were hoping we could unpin this with 50, but it seems we have to wait a little longer. EDIT: I do understand though with COVID-19 raging across the world that some other stuff deservedly gets priority.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-617607902:40,release,released,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-617607902,1,['release'],['released']
Deployability,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:118,integrat,integration,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833,1,['integrat'],['integration']
Deployability,"It is very inconvenient to provide a lot of separate files via cromwell REST or console and it often leads to many mistakes when one has to run many pipelines.; It would be much better to be able just zip whole folder with cromwell project and give one json config file that gives all pathes inside zip (where is subworkflows folder, where are options and where is the main workflow).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2410:149,pipeline,pipelines,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2410,1,['pipeline'],['pipelines']
Deployability,"It looks like the parent `PipelinesApiRequestManager` may be handling this okay but that should be confirmed. ```; ERROR akka.actor.OneForOneStrategy - 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917:26,Pipeline,PipelinesApiRequestManager,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917,1,['Pipeline'],['PipelinesApiRequestManager']
Deployability,It looks like with .21 there is a an extra `resources` key with default values in the pipeline args section when you describe a jes job. When I ran the 50 workflow test with .21 none of my jobs seemed to get preempted which seems unusual and I want to make sure this extra `resources` stanza with default values isn't causing issues. from .19:. ```; resources:; bootDiskSizeGb: 10; disks:; - autoDelete: true; mountPoint: /cromwell_root; name: local-disk; sizeGb: 208; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 10; preemptible: true; zones:; - us-central1-b; - us-central1-c; pipelineArgs:; clientId: ''; inputs:; exec: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/exec.sh; input_bam-0: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-GatherBamFiles/NWD989972.bam; input_bam_index-0: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-GatherBamFiles/NWD989972.bai; interval_list-0: gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0018_of_50/scattered.interval_list; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/HaplotypeCaller-17.log; outputs:; HaplotypeCaller-17-rc.txt: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/HaplotypeCaller-17-rc.txt; NWD989972.vcf.gz: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/NWD989972.vcf.gz; NWD98,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1501:86,pipeline,pipeline,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501,2,['pipeline'],"['pipeline', 'pipelineArgs']"
Deployability,It seems fixed with release 70,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6507#issuecomment-943935169:20,release,release,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6507#issuecomment-943935169,1,['release'],['release']
Deployability,"It seems that the HealthMonitor relies on a specific type of response from Pipelines API v2 to determine health of the sub-services. The PAPI response about operation metadata is different between v1 and v2, and the HealthMonitor expectations are tied to the type of response returned from v1. This essentially prevents one from running a HealthMonitor against the v2 backend --which is going to be a requirement of enabling Pipelines API v2 in FireCloud production. AC: Be able to perform HealthMonitoring on both the PAPI v1 and PAPI v2 google backend. Testing criteria: Enable HealthMonitoring for PAPI v1 and PAPI v2 centaur tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4029:75,Pipeline,Pipelines,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4029,2,['Pipeline'],['Pipelines']
Deployability,It seems the Swagger definition [here](https://github.com/broadinstitute/cromwell/blob/9de965affe5ee89f55b7e6213bf6093954b9a29f/engine/src/main/resources/swagger/cromwell.yaml#L323) does not reflect all of the statuses:; https://github.com/broadinstitute/cromwell/blob/bbfc747d5737b66ffd422d53d567cdbc38c62cba/core/src/main/scala/cromwell/core/ExecutionStatus.scala#L5 . It would be great if the YAML file could get updated.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3328:416,update,updated,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3328,1,['update'],['updated']
Deployability,It seems the sentry configuration is generating too much log from log.error due to log level set to WARN or above. So removing this configuration altogether from logback.xml.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5065:20,configurat,configuration,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065,2,['configurat'],['configuration']
Deployability,"It should be in-place (e.g. the source and destination tables are in the same database). . It would be nice to be automagic (e.g. liquibase), but just having a script in the repository to do the upgrade would be fine as well. It's really just for existing production customers who need to bridge the gap (GOTC/FC)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-226603031:195,upgrade,upgrade,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-226603031,1,['upgrade'],['upgrade']
Deployability,"It shouldn't be too involved - there's an example of backend configuration being used in the same file you're updating [here](https://github.com/broadinstitute/cromwell/blob/develop/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L132), which means it could be a per-backend option (see where the value could be set within a backend config [here](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf#L379)). . Does that give you what you needed?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-548064730:61,configurat,configuration,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-548064730,1,['configurat'],['configuration']
Deployability,"It turns out the original task above didn't work when `combined_gvcf=""gs://dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz""` even in V24. I updated the task to:. ```; task IndexVCF {; File combined_gvcf; Int disk_size. command {; /usr/gitc/tabix ${combined_gvcf}; }; runtime {; docker: ""broadinstitute/genomes-in-the-cloud@sha256:d7aa37fc8351074a2d6fb949932d3283cdcefdc8e53729dcf7202bee16ab660a""; memory: ""13 GB""; cpu: ""1""; disks: ""local-disk "" + disk_size + "" HDD""; }; output {; File gvcf_index = ""${combined_gvcf}.tbi""; }; }; ```. The thing is I still don't know why the first task in the comment above doesn't work. It would be nice to have a better error message.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178:197,update,updated,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178,1,['update'],['updated']
Deployability,It was suggested that we create both a develop PR as well as for a hotfix. Had I been aware of the differences between the two branches I probably would not have created this PR... but we had already finished. Similar work will be done for a PR on the hotfix branch. I'm happy to do whatever your team prefers. I suspect that this work will need to be duplicated in either case. :(,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219122627:67,hotfix,hotfix,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219122627,2,['hotfix'],['hotfix']
Deployability,"It will not be merged into Cromwell, just assumed this was the easiest way to get it reviewed. It's simply a start at automating the release process, lots of feedback is welcome. . Warning: Not been completely tested.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1780:133,release,release,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1780,1,['release'],['release']
Deployability,"It works with the standard backend. <!-- Which backend are you running? -->; SLURM. <!-- Paste/Attach your workflow if possible: -->; cwlVersion: v1.0; class: Workflow. requirements:; SubworkflowFeatureRequirement: {}. inputs:; fastqc_output_dir:; type: string; fastqc_input_files:; type: string[]; fastqc_thread_count:; type: int; multiqc_output_dir:; type: string; outputs: []. steps:; fastqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: fastqc_output_dir; out: [created_directory]; fastqc_execute:; run: fastqc-step.cwl; in:; input_files: fastqc_input_files; output_dir: fastqc_mkdir/created_directory; thread_count: fastqc_thread_count; out: [output_directory]; multiqc_mkdir:; run: mkdir-cmd.cwl; in:; directory: multiqc_output_dir; out: [created_directory]; multiqc_execute:; run: multiqc-cmd.cwl; in:; output_dir: multiqc_mkdir/created_directory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560:938,configurat,configuration,938,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560,1,['configurat'],['configuration']
Deployability,"It works! . Now everyone back away slowly, don't breathe, and never update develop ever again",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/473#issuecomment-188503022:68,update,update,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/473#issuecomment-188503022,1,['update'],['update']
Deployability,"It would be helpful, for our production pipeline, if Cromwell's copying of workflow outputs and logs had an option to ""flatten"" the outputs by writing all to a single directory, instead of including the directory hierarchy when copying. We are careful to avoid file name collisions in our output and logs so handling that case wouldn't be an issue for us -- but might for other users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641:40,pipeline,pipeline,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641,1,['pipeline'],['pipeline']
Deployability,"It would be nice if cromwell would have the option to incrementally increase the memory attribute for a job, based on the retries. This is related to #1991, but the conclusion there (if I'm understanding it correctly) seems to be adding a feature to the WDL specs. I feel that these types of features might be more appropriate as a backend configuration, since whether this is something you want to do or not will depend on the infrastucture your workflow is running on. One potential way to configure this, might be to add multipliers for certain runtime attributes in the backend configuration:; ```; [...]; config {; runtime-attributes = """"""; Int? cpu = 1; Int? memory = 4; """"""; runtime-attribute-retry-multipliers = {; memory: 1.5; }; [...]; }; [...]; ```. This would, for example, cause the memory attribute to be multiplied by `1.5` with each retry. For the first attempt it would be `4`, for the the second `6`, the third `9` etc. Another option might be that the values here indicate a fraction of the original attribute which it should be increased it by each retry, so in the above example it would be: `4` -> `10` -> `16` etc. You would probably want set a lower value in that case, though. Another option would be to supply a list of numbers with each indicating a multiplier for a certain attempt. A value of `[1.5, 2]` (or maybe `[1, 1.5, 2]`) would cause the value to be multiplied by `1.5` on the second attempt and `2` on the third, repeating the last multiplier if neccesary. (ie. `4` -> `6` -> `8` -> `8`). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL ht",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4346:340,configurat,configuration,340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4346,2,['configurat'],['configuration']
Deployability,"It would be very nice to support Mesos, as it is a very nice framework for in house cloud computing systems. Not everyone is able to launch their jobs into the cloud. I see you already have support for Yarn here: . http://cromwell.readthedocs.io/en/develop/backends/Spark/. And we also have this:; ```; A not so widely known fact is that Spark has its root in Mesos: it was ; initially developed at the AMPLab as a proof-of-concept Mesos ; framework to demonstrate how easy and fast ; it is to develop a distributed platform on top of Mesos; ```; taken from here : ; * https://mesosphere.com/blog/spark-mesos-shared-history-and-future-mesosphere-hackweek/. Spark and Mesos was really closely integrated, though I see that Spark has created their own scheduler, Mesos is still a very good way of running Spark jobs. It would be a very nice addition to the Chromwell framework! . Mesos is used in many other Big Data cloud environments outside of the Bioinformatics pipelines.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-388333576:692,integrat,integrated,692,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-388333576,2,"['integrat', 'pipeline']","['integrated', 'pipelines']"
Deployability,It's a release improvement ticket. Currently the release WDL creates a github release even if the build fails on the master branch right after merging,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-333241405:7,release,release,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-333241405,3,['release'],['release']
Deployability,It's expected that `wdltool 0.4` will not validate this as the `String main_output = hello_and_goodbye.hello_output` syntax in workflow outputs was introduced specifically for sub workflows which `wdltool 0.4` pre-dates.; Try to update to the latest version of wdltool and it should validate.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261:229,update,update,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261,1,['update'],['update']
Deployability,"It's happened for the 11k run, but the changes to the joint genotyping workflow haven't been merged into dsde-pipelines/master yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-313503722:110,pipeline,pipelines,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-313503722,2,['pipeline'],['pipelines']
Deployability,It's happening again. Last time this was the result of our LB blocking new pingdom servers. @hjfbynara could you please run update script?. I don't see any added in december here: https://help.pingdom.com/hc/en-us/community/posts/208953545-Pingdom-Probe-Servers-Pingdom-IPs.; But it seems too coincidental that the symptoms are exactly the same as they were last time. ![image](https://user-images.githubusercontent.com/165320/50653143-8f762700-0f56-11e9-9b2d-76da416c47f2.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-451223835:124,update,update,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-451223835,1,['update'],['update']
Deployability,"It's low effort, low reward. If someone does it in their ""spare"" time, awesome. Otherwise, imho, the cromwell.yaml can be manually patch in a commit post 30, and someone can also fix the migration script at that time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2930#issuecomment-347984022:131,patch,patch,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2930#issuecomment-347984022,1,['patch'],['patch']
Deployability,"It's not really a dupe -- one ticket is about the README/swagger and one is; specifically about 0.19->0.20+ upgrade instructions (which could go in the; readme as one option!). ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Wed, Jul 13, 2016 at 11:46 AM, Ruchi notifications@github.com wrote:. > @kcibul https://github.com/kcibul since this is a duplicate, is it okay; > if we just close this issue?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1043#issuecomment-232397729,; > or mute the thread; > https://github.com/notifications/unsubscribe/ABW4g2tMjutZYDtV_3n_DEN05wi6rjllks5qVQhjgaJpZM4I69py; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1043#issuecomment-232402414:108,upgrade,upgrade,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1043#issuecomment-232402414,1,['upgrade'],['upgrade']
Deployability,"It's really a question of time and cost efficiency, but since we've got the; actual GATK team and joint calling authors working on the pipeline we'll; definitely take advantage of all the features there are (and they'll write; the ones we need!). ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Jun 23, 2016 at 11:55 AM, Paul Grosu notifications@github.com; wrote:. > @kcibul https://github.com/kcibul I believe GATK can perform; > incremental joint calling, so then you should be able to use a collection; > of Cromwells submissions to build it up. Would that work?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228096103,; > or mute the thread; > https://github.com/notifications/unsubscribe/ABW4gxuO55o58LyKHTfSEVvS97Z1-7Nxks5qOqxWgaJpZM4I8rmu; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228109757:135,pipeline,pipeline,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228109757,1,['pipeline'],['pipeline']
Deployability,"It's very frustrating to try to patch labels on a workflow to which you have read but not write permission. Right now, there's no way for JM to know which workflows can or cannot be patched, without trying and failing. It would be awesome if the CromIAM `/query` could add into the response JSON an indication of the permissions of the requester w.r.t each workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4679:32,patch,patch,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4679,2,['patch'],"['patch', 'patched']"
Deployability,"JDOM was removed in https://github.com/broadinstitute/cromwell/pull/6785 and this branch is updated from `develop`:. ```; root(aen_bw_1228)> | 81> whatDependsOn org.jdom jdom2; [ ... ]; [error] whatDependsOn org.jdom jdom2; [error] ^; ```. ( This is the normal output when `whatDependsOn` does not find something, see https://github.com/broadinstitute/cromwell/pull/6775 https://github.com/broadinstitute/cromwell/pull/6776 ). For Protobuf, the new MySQL pulls in a safe version ≥ 3.16.1:. ```; +-mysql:mysql-connector-java:8.0.29; | +-com.google.protobuf:protobuf-java:3.19.4; ```. which evicts older versions used by other dependencies. ```; +-io.opencensus:opencensus-proto:0.2.0; | +-com.google.protobuf:protobuf-java:3.19.4; | +-com.google.protobuf:protobuf-java:3.5.1 (evicted by: 3.19.4); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6793:92,update,updated,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6793,1,['update'],['updated']
Deployability,JES backend not honoring the `default-zones` configuration parameter,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2527:45,configurat,configuration,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2527,1,['configurat'],['configuration']
Deployability,"JES has added an additional ""event"" to their metadata. For example:; events: ; - description: copied 1 file(s) to ""gs://some/path/sample.vcf"" ; startTime: '2016-08-15T19:39:49.261235611Z'. gcloud alpha genomics operations describe EJ7K1P3oKhjukpy15ueGiy0gw7vetLsXKg9wcm9kdWN0aW9uUXVldWU (for more details). Cromwell tries to record these execution events to the db, but sometimes the new events exceed the 255 char limit on the 'DESCRIPTION' column. . AC: Filter out these file copying events in 0.19_hotfix so that they don't clog up the execution events table, which can lead to downstream status update failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1299:599,update,update,599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1299,1,['update'],['update']
Deployability,"JES now allows for labels to be applied to pipeline runs. Labels are important because they allow us to tag pipeline tasks with metadata which is exposed in other Google APIs, specifically in billing exports that are visible in BigQuery. This will allow us to, for example, calculate the exact cost of a pipeline run which is immensely important for FireCloud and @abaumann . The changes to the pipelines API are described here:. https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines/run#RunPipelineArgs.FIELDS.labels. Cromwell should propagate the workflow level labels to the JES calls for downstream use. According to Google, we should be able to see these labels right away in the operations metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1624:43,pipeline,pipeline,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1624,5,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"JES/PAPI only has [two](https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines#DockerExecutor) current settings for running a docker container:; - `imageName`: Image name from either Docker Hub or Google Container Registry.; - `cmd`: The command or newline delimited script to run. The command string will be executed within a bash shell. This particular ticket may need to be escalated. The entrypoints present in the docker image do seem to cause failures with JES/PAPI. This could be mitigated by PAPI using [`docker run --entrypoint="""" …`](https://docs.docker.com/engine/reference/run/#entrypoint-default-command-to-execute-at-runtime). Also note: PAPI using `bash` effectively makes #1384 moot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-316418026:82,pipeline,pipelines,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-316418026,1,['pipeline'],['pipelines']
Deployability,"JIRA Ticket: https://broadworkbench.atlassian.net/browse/CROM-6867. Hi. . I've been running the GATK-SV pipeline with AWS backend and, sometimes, due to some intermittent errors the tasks are aborted half-way trough. Then Cromwell re-launches the task but some files, generated by the previous run, are already there what makes the pipeline to fail. With this in mind, I'm preparing do a change in cromwell that remove all the files (except for the script which gets created for each run/job) from the task folder before it starts and I would like to ask:; 1. if this makes sense?; 2. if there is any problem on doing this. can the same folder be used twice? or does each task has its own “workspace”? or Will this change impact any other downstream jobs as we will remove everything except “script” file?. Thanks in advance",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6651:104,pipeline,pipeline,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6651,2,['pipeline'],['pipeline']
Deployability,"JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.ru",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:3085,Update,UpdateVisitor,3085,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['Update'],['UpdateVisitor']
Deployability,Jackson version upgrade attempt (BW-935).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6577:16,upgrade,upgrade,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6577,1,['upgrade'],['upgrade']
Deployability,"Jeff, Thibault and all;; I'm testing the latest development Cromwell which supports GCP runs using CWL (#3724). Thank you so much for this support. My bcbio test pipelines are running but I'm hitting an issue during copying input files from some nested subworkflows. There appears to be a 1024 character limit which we hit due to the combination of the root execution directory and the nested sub-workflow path from Cromwell:; ```; Copying file:///cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-calculate_sv_coverage/shard-0/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-calculate_sv_bins/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-postprocess_alignment/shard-0/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-postprocess_alignment_to_rec/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-alignment/shard-0/wf-alignment.cwl/94c8f53c-dad4-4c48-9e09-f6927356f352/call-merge_split_alignments/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-alignment/shard-0/wf-alignment.cwl/94c8f53c-dad4-4c48-9e09-f6927356f352/call-process_alignment/shard-0/cromwell_root/align/Test2/Test2-sort.bam [Content-Type=application/octet-stream]...; / [0/1 files][ 0.0 B/ 3.7 MiB] 0% Done ; BadRequestException: 400 The maximum object length is 1024 characters, but got a name with 1055 characters: ''gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-...''; CommandException: 1 file/object could not be transferred.; Copying file:///cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-calculate_sv_coverage/shard-0/cromwell_root/bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295/call-calculate_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4471:162,pipeline,pipelines,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4471,1,['pipeline'],['pipelines']
Deployability,"JesBackend.scala:123); at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$createJesRun(JesBackend.scala:573); at cromwell.engine.backend.jes.JesBackend$$anonfun$35.apply(JesBackend.scala:707); at cromwell.engine.backend.jes.JesBackend$$anonfun$35.apply(JesBackend.scala:706); at scala.util.Success.flatMap(Try.scala:231); at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$runWithJes(JesBackend.scala:706); at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:362); at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:350); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-04-28 15:35:51,730] [warn] JesBackend [1cb9c1d2:jes_task]: Exception occurred while creating JES Run. Retrying in 4742 (9 more retries)...; ```. I've walked through the stacktrace and can't see anything obviously wrong with the JesAttachedDisk.scala code, which is just running an md5sum on the params of the disk, so that should always result in a string of characters that is legal to use for a disk name, according to the provided link. I have the sense that the problem may lie in the google pipelines api code, after the params are handed off to create a pipeline. The particular disk name that's causing a problem in this example is `4fd1d1e01455dfdd4eabcf02c1abaf55`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757:6283,pipeline,pipelines,6283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"JhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (ø)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInterpolator.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | ... and [301 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=footer). Last update [242206f...e9ad3d7](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:4363,update,update,4363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842,2,['update'],['update']
Deployability,Jira issue: https://broadworkbench.atlassian.net/browse/BA-6006. Please reference that ticket for updates! Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5182#issuecomment-533318396:98,update,updates,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182#issuecomment-533318396,1,['update'],['updates']
Deployability,"Jira: https://broadworkbench.atlassian.net/browse/WX-1670. ### Description. Cromwell will use information provided in the new GROUP_METRICS_ENTRY table to allocate new tokens for job requests whose hog group is not experiencing any cloud quota exhaustion. Note that this will be applied to jobs seeking ""execution"" tokens. Jobs seeking ""restart"" tokens are not affected by this change. . TODO:; - [x] test changes in BEE; - [x] fix unit tests; - [x] add new unit tests; - [x] update Changelog?. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7520:476,update,update,476,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520,6,"['Release', 'release', 'update']","['Release', 'release', 'update', 'updated']"
Deployability,Jira: https://broadworkbench.atlassian.net/browse/WX-1675. ### Description. - Records quota exhaustion for a hog group to the new `GroupMetrics` table when a job runs into `AwaitingCloudQuota` state; - This PR adds a new singleton Actor called `GroupMetricsActor` which takes in currently `RecordGroupQuotaExhaustion` message to record the group that ran into quota exhaustion into the new table. Example screenshot:; ![Screenshot 2024-08-20 at 4 27 49 PM](https://github.com/user-attachments/assets/fe403e65-d7f2-41b3-8279-38a05b3cbb6b). <!-- What is the purpose of this change? What should reviewers know? -->. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7501:617,Release,Release,617,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7501,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"JobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more; ```; This is our configuration for PBS:; ```; PBSPRO {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; runtime-attributes = """"""; Int cpu = 1; Int memory_mb = 2048; String queue = ""normal""; String account = """"; String walltime = ""48:00:00""; ; Int? cpuMin; Int? cpuMax; Int? memoryMin; Int? memoryMax; String? outDirMin; String? outDirMax; String? tmpDirMin; String? tmpDirMax; """"""; submit = """"""; qsub -V -l wd -N ${job_name} -o ${out} -e ${err} -q ${queue} -l walltime=${walltime} -l ncpus=${cpu} -l mem=${memory_mb}mb -- /usr/bin/env bash ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+).*""; filesystems {. local {; localization: [""soft-link""]; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }; ```; Thanks for any tips or pointers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:4312,configurat,configuration,4312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['configurat'],['configuration']
Deployability,Jobs failed with Unable to complete PAPI request due to system or connection error (PipelinesApiRequestHandler actor termination caught by manager),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6203:84,Pipeline,PipelinesApiRequestHandler,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6203,1,['Pipeline'],['PipelinesApiRequestHandler']
Deployability,"Jobs submitted by [dsub][0] and [Google Genomics Pipeline Tools][1] both have the ability to start an SSH container in the background to allow one to inspect the runtime environment of a google genomics job in real-time. This can often be a very helpful tool to aid in diagnosing various kinds of troublesome workflow jobs and appreciating workflow tasks containing complex scripts and programs. Does cromwell support this feature? If so, how would one enable this? Otherwise, would it be possible to incorporate this feature in future releases?. [0]: https://github.com/DataBiosphere/dsub/releases/tag/v0.2.1; [1]: https://github.com/googlegenomics/pipelines-tools",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4966:49,Pipeline,Pipeline,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4966,4,"['Pipeline', 'pipeline', 'release']","['Pipeline', 'pipelines-tools', 'releases']"
Deployability,Js green integration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2374:9,integrat,integration,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2374,1,['integrat'],['integration']
Deployability,"JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:51); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 common frames omitted; [2021-08-13 10:45:07,42] [warn] PAPI request worker had 1 failures making 1 requests:; Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; [2021-08-13 10:45:0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:4544,Pipeline,PipelinesApiRequestWorker,4544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,"JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:51); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 more. [2021-08-13 10:45:10,13] [info] WorkflowManagerActor: Workflow actor for a15c46b7-5f93-46d6-94a2-28f656914866 completed with status 'Failed'. The workflow will be removed from the workflow store.; [2021-08-13 10:45:13,98] [info] Si",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:7166,Pipeline,PipelinesApiRequestWorker,7166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,"Just OOO, is this mostly complementary, orthogonal, or replacing-of, the scala steward update PRs?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-895312885:87,update,update,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-895312885,1,['update'],['update']
Deployability,"Just a friendly reminder that the Changelog could use an update, possibly the Readme 🤓",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-277081924:57,update,update,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-277081924,1,['update'],['update']
Deployability,"Just before I merge this PR, I will coordinate disabling the existing automatic database updates with @coreone",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-170711740:89,update,updates,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-170711740,1,['update'],['updates']
Deployability,Just did a 0.8 release,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271884647:15,release,release,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271884647,1,['release'],['release']
Deployability,Just thinking out loud - there seems to be a lot of duplication between this backend and the configurable shared-filesystem backend. ; I don't know whether you've tried expressing this as a configuration file backend?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246796358:190,configurat,configuration,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246796358,1,['configurat'],['configuration']
Deployability,"Just to check, this is not yet addressed in the recently release Release 36?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-431895566:57,release,release,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-431895566,2,"['Release', 'release']","['Release', 'release']"
Deployability,"Just to clarify -- this isn't currently not possible on the JES backend, only Local. However, Pipelines APIv2 would allow for more flexible delocalization.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3065#issuecomment-353106597:94,Pipeline,Pipelines,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3065#issuecomment-353106597,1,['Pipeline'],['Pipelines']
Deployability,"Just wanted to re touch base on this quickly, since the :+1: were given I fixed a bug that prevented the ""filepassing.wdl"" from working on JES and updated the config template / README. Is updating the template enough to make any configuration change work when cromwell will redeploy ? Because several field names / location have been changed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162623180:147,update,updated,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162623180,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"Just wanted to report a behavior I saw while trying to scale Cromwell horizontally. I haven't had time to take a look to it but is related to metadata and the way that is written in the database. How to reproduce:; 1. Deploy:; 1 VM with Nginx to act as the load balancer; 2 VMs with Cromwell 29; 1 VM with MySQL 5.6.36. 2. Submit Hello World workflow 10000 times. 3. Try to get status for some/all workflows while workflows are being processed. 4. These issues are manifested:. Duplicated entry exception (this one happens repeatedly) =>; ``` ; 2017-07-13 22:07:39,149 cromwell-system-akka.dispatchers.service-dispatcher-243 ERROR - Failed to summarize metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'cromwell-workflow-id-cromwell-6d021019-ac3f-4a28-b034-d58fb92022' for key 'UC_CUSTOM_LABEL_ENTRY_CLK_CLV_WEU'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.Util.getInstance(Util.java:408); 	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:935); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2490); 	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858); 	at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079); 	at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013); 	at com.mysql.jdbc.PreparedSta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452:218,Deploy,Deploy,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452,1,['Deploy'],['Deploy']
Deployability,"Just wonder why the runtime block has to be key-value pairs? `spark-submit` has tons of attributes and new attributes may be added in the future. Can the runtime block just be wrapped as string and passed to `spark-submit`?. If it has to be key-value pair, can the key be something like ""additionalArgs"" and the value be a string of containing attributes the user wants to add? for example:; `""additionalArgs"": ""--conf 'xx -Dxx' --name xx""`; In this way, if `spark-submit` has new attributes in the future, cromwell doesn't need to updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-331604537:532,update,updated,532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-331604537,1,['update'],['updated']
Deployability,Knob for script epilogue. (hotfix edition),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2159:27,hotfix,hotfix,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2159,1,['hotfix'],['hotfix']
Deployability,"L "" sequence_group_interval}; }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.2-1510681135""; memory: ""6 GB""; disks: ""local-disk "" + disk_size + "" HDD""; preemptible: preemptible_tries; }; output {; File recalibration_report = ""${recalibration_report_filename}""; }; }; ```. And here is my cromwell server config:. ```scala; include required(classpath(""application"")). webservice {; port = 8000; }. system {; workflow-restart = true; }. engine {; filesystems {. gcs {; auth = ""service-account""; }. http {}. local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }. backend {; default = ""Local""; providers {. Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; max-concurrent-workflows = 1; concurrent-job-limit = 1; }; }. PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; project = ""bioinfo-XXXXXXX""; root = ""gs://XXXXXXXX""; genomics-api-queries-per-100-seconds = 1000; max-concurrent-workflows = 80; concurrent-job-limit = 200; maximum-polling-interval = 600. genomics {; # Config from google stanza; auth = ""service-account"". ; # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; localization-attempts = 3; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; }; }; }; }; }; }. # Google authentication; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""XXXXXXXXXXXXXX@XXXXXXXXXXXX.gserviceaccount.com""; json-file = ""/var/secrets/google/key.json""; }; ]; }. # database connection; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://cromwell-db/cromwell?rewriteBatchedStat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336:2170,Pipeline,PipelinesApiLifecycleActorFactory,2170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336,1,['Pipeline'],['PipelinesApiLifecycleActorFactory']
Deployability,LGTM! I updated [this doc](https://docs.google.com/document/d/1X1mpcUtsukeWez82UpXAXkX7SJpcSB-pZk1lFwd6LUA/edit) to reflect the change here.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7350#issuecomment-1877250668:8,update,updated,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7350#issuecomment-1877250668,1,['update'],['updated']
Deployability,Labels PATCH returns 500 for a bad request,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4714:7,PATCH,PATCH,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4714,1,['PATCH'],['PATCH']
Deployability,Language Support update,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4811:17,update,update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4811,1,['update'],['update']
Deployability,Language specific configuration (not enforcing read_X limits),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2611:18,configurat,configuration,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2611,1,['configurat'],['configuration']
Deployability,"Last time we tried to upgrade this we [ran into a bug](https://github.com/googleapis/java-storage-nio/issues/691), but it [has now been fixed](https://github.com/googleapis/java-storage-nio/pull/719) and we have a [unit test](https://github.com/broadinstitute/cromwell/pull/6491) for the library that verifies the fix. Besides generally keeping important libraries up to date, there is a chance that recent fixes to requester pays ([Google PR](https://github.com/googleapis/java-storage-nio/pull/858), [GATK PR](https://github.com/broadinstitute/gatk/pull/7730)) implemented by our own very Methods team may solve an issue users are seeing in Terra:; ```; User project specified in the request is invalid.; ```. [Terra User Liaison Slack link.](https://broadinstitute.slack.com/archives/CBJJ7U293/p1657731546766919); [DSP Methods Slack link.](https://broadinstitute.slack.com/archives/C1H82GH41/p1647028919917549?thread_ts=1645714642.593619&cid=C1H82GH41)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804:22,upgrade,upgrade,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804,1,['upgrade'],['upgrade']
Deployability,"Learned that a change in Pipelines API omitted the ""preemptible"" key from the operations metadata, and that change introduced a null pointer in the Cromwell code. . AC: As a way to address this, it would be great if we could modify the Cromwell code so that when its parsing operation metadata, that if certain keys are missing (such as Preemptible) -- we use the defaults where possible, else fail gracefully with an error that states which information couldn't be parsed, and that caused the workflow to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-477169804:25,Pipeline,Pipelines,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-477169804,1,['Pipeline'],['Pipelines']
Deployability,Legacy Cromwell server stopped responding to status checks after release,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4094:65,release,release,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4094,1,['release'],['release']
Deployability,Less metadata repetition during call cache copying: 49 hotfix edition [BA-6306],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5445:55,hotfix,hotfix,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445,1,['hotfix'],['hotfix']
Deployability,"Let's chat IRL later ~~today~~. To be clear, I'm totally fine mothballing this PR. In the context of PBE, conceivably this ticket was introduced so one could implement other final calls that run on a backend?. EDIT: No rush on this ticket. Making some patches, but will chat about next steps some other time when we can.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/694#issuecomment-210003861:252,patch,patches,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/694#issuecomment-210003861,2,['patch'],['patches']
Deployability,Liquibase Updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1462:10,Update,Updates,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1462,1,['Update'],['Updates']
Deployability,"Listing a few things I noticed were out of data or underspecified in the Cromwell Swagger and/or README:; - `GET .../metadata` {include,exclude}Key applies to all keys in the response including nested objects; - `PATCH .../labels` returns only the updated labels; - `GET .../metadata` failures and calls.failures are underspecified, actual definition is:. ```; FAILURES := [; {; message: """"; causedBy: FAILURES; },; ]; ```. - Subworkflows are not represented in the response; - Some fields I noticed were missing from Swagger yaml (not exhaustive):; - workflow.labels; - workflow.workflowName; - workflow.submittedFiles; - workflow.calls.shardIndex; - workflow.calls.subWorkflowMetadata (correspondingly, missing parentWorkflowId and workflowRoot from WorkflowMetadata); - workflow.calls.outputs; - workflow.calls.callCaching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228:213,PATCH,PATCH,213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228,2,"['PATCH', 'update']","['PATCH', 'updated']"
Deployability,"Local, metaGenPipe.taxonclass_task -> Local, qc_subworkflow.fastqc_task -> Local, metaGenPipe.multiqc_task -> Local; [2020-09-17 21:41:42,97] [error] Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait cat ${docker_cid}). # remove the container after waiting; docker rm cat ${docker_cid}. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill cat ${docker_cid}; }; }; java.lang.RuntimeException: Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:1867,configurat,configuration,1867,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['configurat'],['configuration']
Deployability,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:187,update,updates,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033,6,"['pipeline', 'update']","['pipeline', 'updates']"
Deployability,Logically revert #4263 since multi-Cromwell deployments no longer need a specific abort server. A simple git revert has a ton of conflicts but hopefully this shouldn't be too tough.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4596:44,deploy,deployments,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4596,1,['deploy'],['deployments']
Deployability,"Logs:; 2019-10-09 14:05:52,263 cromwell-system-akka.actor.default-dispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:146,update,updates,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,6,"['pipeline', 'update']","['pipeline', 'updates']"
Deployability,Looking at the database migration scripts there is a JES_JOB table for overseeing Google Pipelines work. Do we need a) an AWS_JOB or b) should this be more generic to accommodate all CSPs?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3288:89,Pipeline,Pipelines,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3288,1,['Pipeline'],['Pipelines']
Deployability,"Looking at this with Cromwell 24, the user can be specified with a config like:. ```; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; String? docker_user; """"""; submit-docker = """"""docker run --rm ${ ""--user "" + docker_user } -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""""""; .; .; .; ```. The WDL can pass in `docker_user` as a runtime attribute, which could be an expression involving an input or just a hardcoded value. But even with a container path not under `/root`, there are currently permissions problems that prevent this from working due to the different users inside and outside the Docker container. It may be possible to `chmod` and `umask` our way past these problems, but I need to think through the security implications of doing so. Maybe making this more liberal behavior an opt-in configuration value in the backend config would be okay?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/472#issuecomment-271364535:963,configurat,configuration,963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/472#issuecomment-271364535,1,['configurat'],['configuration']
Deployability,"Looking into it more closely, there's a good chance we fixed this issue already, we just haven't released a 30.3 jar yet. Meanwhile if you're able to build from the `30_hotfix` branch you might want to try that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-365310137:97,release,released,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-365310137,1,['release'],['released']
Deployability,"Looks good to me on a first pass. . Just to confirm, did you look into how the pipelines tool implements this (https://github.com/googlegenomics/pipelines-tools#ssh-into-the-worker-machine)? In the back of my mind there's a question over whether there's an existing API switch on the pipelines API for this (vs rolling your own action)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577747339:79,pipeline,pipelines,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577747339,4,"['pipeline', 'rolling']","['pipelines', 'pipelines-tools', 'rolling']"
Deployability,"Looks like it's here ""https://github.com/broadinstitute/firecloud-develop/blob/dev/run-context/live/configs/cromwell/docker-compose.yaml.ctmpl"" so it means it'll get promoted automatically on next release.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3709#issuecomment-394768106:197,release,release,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3709#issuecomment-394768106,1,['release'],['release']
Deployability,"Looks like read_json in current trunk still has some issues.; When I give a json like this:; ```json; {; ""Homo sapiens"": {; ""transcriptome"" : ""/pipelines/indexes/HUMAN/27/gencode.v27.transcripts.fa"",; ""gtf"": ""/pipelines/indexes/HUMAN/27/"",; ""salmon"": ""/pipelines/indexes/HUMAN/27/salmon""; },; ""Mus musculus"": {; ""transcriptome"" : ""/pipelines/indexes/MOUSE/M16/gencode.vM16.transcripts.fa"",; ""gtf"": ""/pipelines/indexes/MOUSE/M16/gencode.vM16.annotation.gtf"",; ""salmon"": ""/pipelines/indexes/MOUSE/M16/salmon""; }; }; ```; with wdl like this; ```; Map[String, Map[String, String]] indexes = read_json(references) ; ```; I get:; ```; Workflow input processing failed; WorkflowFailure(ERROR: indexes is declared as a Map[String, Map[String, String]] but the expression evaluates to a Object: Map[String, Map[String, String]] indexes = read_json(references) ^ ,List()); ```; I do not get what is wrong there, I've tried different type combinations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-370112127:144,pipeline,pipelines,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-370112127,6,['pipeline'],['pipelines']
Deployability,Looks like some tests need to be updated in response to these changes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6815#issuecomment-1195910343:33,update,updated,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6815#issuecomment-1195910343,1,['update'],['updated']
Deployability,Looks like this error was specific to my runtime+configuration.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4186#issuecomment-425939282:49,configurat,configuration,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4186#issuecomment-425939282,1,['configurat'],['configuration']
Deployability,"Looks like this is still an issue, correct? I’ve just stumbled upon it in one of our pipelines that uses PAPIv2 (on Cromwell 36). The pipeline step uses a Docker image based on `openjdk:13-alpine`, which does have its own entrypoint that we’d like Cromwell to ignore. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-478755517:85,pipeline,pipelines,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-478755517,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Looks like this was a bad error message, but the task works when I change my runtime+configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4185#issuecomment-425939958:85,configurat,configuration,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4185#issuecomment-425939958,1,['configurat'],['configuration']
Deployability,Looks like this was partially addressed in another merge and needs merge conflict updates: https://github.com/broadinstitute/cromwell/pull/6994,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6830#issuecomment-1421509457:82,update,updates,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6830#issuecomment-1421509457,1,['update'],['updates']
Deployability,"Looks ok as a patch-- but that's me not being an expert at FSMs. Pinging @salonishah11 to see if I can get her expert opinion? . When things calm down, I'll also want to know if we want to patch now and add tests (even unit tests?) later, or we want to wait and patch -and- test at the same time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078:14,patch,patch,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078,3,['patch'],['patch']
Deployability,"Made GoogleAuthMode scopes optional via a param/conf, still defaulting to the pipelines api scopes.; Removed unused scopes and data-dirs from google auth modes that do not uses them.; Moved access token TTL refresher from GcrAbstractFlow to GoogleAuthMode.; Fixed queryPostRoute logging method as ""GET"" instead of ""POST"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3369:78,pipeline,pipelines,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3369,1,['pipeline'],['pipelines']
Deployability,"Made a small change to the preStart method of WFMA, it only tries to re-start incomplete workflows when the config ""workflow-restart"" is set to true. I don't know if I loved how I named that field, and I'm open to suggestions. . Since we don't have restarts configured right now, I didn't have a chance to test this change. I didn't update the Readme yet either, since this is not a change users can use yet, I'm realizing now we do need some way to keep track of all the documentation changes we'll need to make.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/823:333,update,update,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/823,1,['update'],['update']
Deployability,"Made some changes to our Github Actions that should prevent certain slack messages from getting lost. Notes:; - Slack messages are sent to `#cromwell-integration-action`; - Slight change in behavior: We will get a message for any integration test that fails against `develop`, even if that failure didn't happen during a nightly run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7159:150,integrat,integration-action,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7159,2,['integrat'],"['integration', 'integration-action']"
Deployability,Mailbox(Mailbox.scala:258); 11:09:46 cromwell-test_1 | 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 11:09:46 cromwell-test_1 | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 11:09:46 cromwell-test_1 | Caused by: liquibase.exception.LockException: java.lang.NullPointerException; 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.acquireLock(StandardLockService.java:242); 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:170); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:196); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:192); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:58); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:3745,update,update,3745,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['update'],['update']
Deployability,Make a DevOps approved deployable CromIAM,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2133:23,deploy,deployable,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2133,1,['deploy'],['deployable']
Deployability,Make sure engine upgrade works in horizontal.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4786:17,upgrade,upgrade,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4786,1,['upgrade'],['upgrade']
Deployability,Make sure that w/ the configuration from #2142 that one can run the new JG WDL w/ 3 samples. Fix any issues which might come up in this process,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2143:22,configurat,configuration,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2143,1,['configurat'],['configuration']
Deployability,"Make sure we're going the DevOps blessed deployable route from the start, including whatever docker magic, etc they're looking for.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2133:41,deploy,deployable,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2133,1,['deploy'],['deployable']
Deployability,Make the release process a push button action (Jenkinsify ?),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2403:9,release,release,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403,1,['release'],['release']
Deployability,"Makes sense. This can wait for the official announcement of which way the feature is going. In the meantime, our users are gradually migrating from on-prem to Terra. Our Cromwell instance allows users to run workflows on GCP or GridEngine. We want to ensure our instance has feature parity with launching workflows in Terra, so we needed something like this commit. After the announcement, I'll update the PR to copy these auth config lines over to the Batch backend. Otherwise, Terra will have removed the checkbox for reference disks and we can close the PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107730127:395,update,update,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107730127,1,['update'],['update']
Deployability,Mandatory service configuration in current development branch,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:18,configurat,configuration,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,1,['configurat'],['configuration']
Deployability,"Many tools get input files and create the output files in the same folder as the input instead of the execution folder, as a result, I have nothing in execution folder but in the input folder I have a newly created file.; For instance, if I have a task; ```; task create_reference_fai {; input {; File reference; }. String name = sub(basename(reference, "".fasta""), "".fa"", """"). command { ; samtools faidx ~{reference}; }; runtime {; docker: ""quay.io/biocontainers/samtools@sha256:97b9627711c16125fe1b57cf8745396064fd88ebeff6ab00cf6a68aeacecfcda"" #1.2-0; }. output {; File out = name+"".fai""; }; }; ```; It will fail, because samtools faidx created the "".fai"" file in the Input folder as it is the same folder that is reference.fast ( /data/cromwell-executions/index_reference/8b9eaeee-bb8e-4754-9a6b-a853c772cf03/call-create_reference_fai/inputs/1627505414/ in my case) instead of execution ( /data/cromwell-executions/index_reference/8b9eaeee-bb8e-4754-9a6b-a853c772cf03/call-create_reference_fai/execution/ in my case) folder. I believe that Cromwell should control for this as it is against logic to have newly created files in the Input folder. Note: I use latest cromwell release with local backend on Ubuntu 18.10",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5482:1175,release,release,1175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5482,1,['release'],['release']
Deployability,"Maybe add an entry in the changelog, even just; ```; ## 33.1 Release Notes. ### Bug fixes; ```. So that something shows up in the release in github (if you do make a release)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3828#issuecomment-401099308:61,Release,Release,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3828#issuecomment-401099308,3,"['Release', 'release']","['Release', 'release']"
Deployability,"Meh. On Thu, Jun 27, 2019, 4:33 PM Chris Llanwarne <notifications@github.com>; wrote:. > Hotfix worthy?; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5050?email_source=notifications&email_token=AABILSEPI46IZV46NXKJVRLP4UP2FA5CNFSM4H363QWKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYYJK7I#issuecomment-506500477>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AABILSD3E6NIWBT63BF3Y3DP4UP2FANCNFSM4H363QWA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5050#issuecomment-506524118:89,Hotfix,Hotfix,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5050#issuecomment-506524118,1,['Hotfix'],['Hotfix']
Deployability,"Merge `develop` and re-push, you may be unlucky or your branch may be missing test updates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-763130944:83,update,updates,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-763130944,1,['update'],['updates']
Deployability,"Merge conflicts come with the territory of ""not in sprint"" weekend PRs 😅. RE: CROM-6777- As this PR mentions, the `nowarn` issues are a bug fixed by [scala/scala-collection-compat#426](https://dereferer.me/?https%3A//github.com/scala/scala-collection-compat/issues/426). But as of 2021-06-26 the fix [hasn't been released yet](https://github.com/scala/scala-collection-compat/releases). Whether the amelioration in this PR gets merged will depend on if upgrading scala/sbt appears on BT's priority backlog first, or if `@SethTisue` drops a new release of `scala-collection-compat` before then. For now, in this branch the test command `sbt centaurCwlRunner/assembly` with `2.12.14` Works For Me™.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698:313,release,released,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698,3,['release'],"['release', 'released', 'releases']"
Deployability,Merge to develop gated on a BT-219 release of Martha that supports access urls.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6312#issuecomment-827658854:35,release,release,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6312#issuecomment-827658854,1,['release'],['release']
Deployability,Merged an updated version of this missing documentation (See #6800),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6798#issuecomment-1181764470:10,update,updated,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6798#issuecomment-1181764470,1,['update'],['updated']
Deployability,Merging as the failing test is that same stupid integration test w/ dockerhub failing on develop,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166383014:48,integrat,integration,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166383014,1,['integrat'],['integration']
Deployability,Merging despite `codecov/patch` in this case. No tests necessary.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5145#issuecomment-525825381:25,patch,patch,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5145#issuecomment-525825381,1,['patch'],['patch']
Deployability,Merging despite the slurm test failure because:. * this is for a hotfix which is needed urgently; * the error is in a slurm test. Slurm is currently unavailable in Terra.; * the error is in a CWL test. CWL is currently unavailable in Terra.; * the test failure does not appear to be new in this PR (the same failure affects the otherwise unrelated build: https://travis-ci.com/broadinstitute/cromwell/builds/152505665),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120:65,hotfix,hotfix,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120,1,['hotfix'],['hotfix']
Deployability,"Merging for expediency despite two flaky test failures because (1) the 53_hotfix branch will be retested and added to before being deployed, and (2) the hotfix is not being used in situations relevant to the test failures. The tests which failed were:; * CWL conformance on PAPIv2 beta; * PAPIv2 beta on MariaDB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141:131,deploy,deployed,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141,2,"['deploy', 'hotfix']","['deployed', 'hotfix']"
Deployability,Metadata entry count safety limit should apply to matched rows only [52-hotfix],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5725:72,hotfix,hotfix,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5725,1,['hotfix'],['hotfix']
Deployability,Might or might not block release. We'll await Adam's verdict on Monday morning,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4940#issuecomment-491383867:25,release,release,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4940#issuecomment-491383867,1,['release'],['release']
Deployability,Minor Tes updates.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2012:10,update,updates,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2012,1,['update'],['updates']
Deployability,"Minor infix patches, and then unless anyone else chimes in, :+1: for merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/117#issuecomment-125171839:12,patch,patches,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/117#issuecomment-125171839,1,['patch'],['patches']
Deployability,Minor updates and then :+1: from me for merge.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/177#issuecomment-140108585:6,update,updates,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/177#issuecomment-140108585,1,['update'],['updates']
Deployability,Minor updates to YAML to fix capitalization.; Cleaned up sbt eviction warnings for both project and project/project.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2859:6,update,updates,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2859,1,['update'],['updates']
Deployability,Minor updates to track Java 9 and Java 12 STDLIB updates that conflic… BA-5976,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5158:6,update,updates,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5158,2,['update'],['updates']
Deployability,"Minor updates, plus (yet another) rebase. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/893/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-222332578:6,update,updates,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-222332578,1,['update'],['updates']
Deployability,"Minor updates, then :+1: . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/599/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/599#issuecomment-200356197:6,update,updates,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/599#issuecomment-200356197,1,['update'],['updates']
Deployability,"Minor updates, then LGTM",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/365#issuecomment-170287369:6,update,updates,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/365#issuecomment-170287369,1,['update'],['updates']
Deployability,Missing Service Configuration should fail Cromwell initialisation,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/871:16,Configurat,Configuration,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/871,1,['Configurat'],['Configuration']
Deployability,Mlc fix horicromtal engine upgrade,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4883:27,upgrade,upgrade,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4883,1,['upgrade'],['upgrade']
Deployability,Modify the WorkflowStore update mechanism on Cromwell startup,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3347:25,update,update,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3347,1,['update'],['update']
Deployability,More CI updates/fixes.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3661:8,update,updates,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3661,1,['update'],['updates']
Deployability,More Cromwells for Better Deployment,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/691:26,Deploy,Deployment,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/691,1,['Deploy'],['Deployment']
Deployability,More Scala steward updates [BW-927],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6567:19,update,updates,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6567,1,['update'],['updates']
Deployability,More readme updates [no JIRA],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6135:12,update,updates,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6135,1,['update'],['updates']
Deployability,More reassuring log message when pipelines request fails,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4976:33,pipeline,pipelines,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4976,1,['pipeline'],['pipelines']
Deployability,"Most of the example aws.conf file for AWSBatch does not actually provide a working example of a s3 ARN, they all seems to expect the user to specify <provide-your-s3-arn>. https://cromwell.readthedocs.io/en/stable/tutorials/AwsBatch101/. It would be helpful to provide actual working example as AWS support is new and most of us are just learning how to setup for AWS integration for Cromwell. Cheers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5226:368,integrat,integration,368,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5226,1,['integrat'],['integration']
Deployability,"Most of the genomic file types we work with in the variant discovery pipelines are typically accompanied by an index file with a conventionally predictable name (eg my_callset.vcf comes with my_callset.vcf.idx). Right now, as a WDL author, I have to supply these index files explicitly in my inputs json files and in several places in my workflows. This is very tedious, so it would be glorious to have Cromwell just automatically recognize when file inputs and outputs are one of a defined index-associated types, search for the corresponding indices based on given naming conventions, and implicitly co-localize the index files that it finds (but not fail if it doesn't find them, because sometimes we work without indices!).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1412:69,pipeline,pipelines,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1412,1,['pipeline'],['pipelines']
Deployability,"Most of the remaining backend dependencies on engine look straightforward to move to backend or core, though there are a few prizes like the direct DB updates from JES and Local backends. No heroics required, but do what's possible to remove unnecessary dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/554#issuecomment-197620844:151,update,updates,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/554#issuecomment-197620844,1,['update'],['updates']
Deployability,"Most of this is just wiring so start by looking at the changes in CallCacheReadActor. I've updated it and made it accessed via a global, routed actor in CromwellRoot. Tests will inject their own dummy version. Mainly this makes testing easier as it's easier to inject the actor rather than relying on it being created to aim at an empty database. Potential downside: actor hierarchy is a little bit more top heavy now :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1326:91,update,updated,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326,1,['update'],['updated']
Deployability,Move reference configuration for services back to reference.conf.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4589:15,configurat,configuration,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4589,1,['configurat'],['configuration']
Deployability,Move reference image configuration inline with Cromwell configuration rather than using an outboard configuration file that is not subject to version control.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6176:21,configurat,configuration,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6176,3,['configurat'],['configuration']
Deployability,Move workflow-level store updates behind an actor.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/32:26,update,updates,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/32,1,['update'],['updates']
Deployability,"Moved parts of the shadow local backend into a new background executor.; Patches to tests based on changed to local: now with job id, fewer processes running in three step, using sfs config.; Replaced SGE backend with a config based version and added LSF example.; Revert: Jes's poll backoff starting at 30s up to 10 minutes is completely inappropriate for shared file system polling.; Docker Hub appears to have busted the v1 enpoint so disabling the test for now.; Sleep a second before letting the `WorkflowExecutionActorSpec` check for metadata status.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1252:73,Patch,Patches,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1252,1,['Patch'],['Patches']
Deployability,Moved supervision from JES down into Standard trait.; Fixed some vals in the Standard trait.; Fixed non-Travised integration test `JesAttributesSpec`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1781:113,integrat,integration,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1781,1,['integrat'],['integration']
Deployability,Moves the time of the update poll closer to when the updates will actually be worked on for incorporation by the team.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6723:22,update,update,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6723,2,['update'],"['update', 'updates']"
Deployability,Multiple updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1379:9,update,updates,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1379,1,['update'],['updates']
Deployability,"My WDL pipeline failed to run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API with a long list of errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; I was under the expectation that this had been handled in issue #5344 and that Cromwell would retry to access the files until available (the files do indeed exist at the time of this writing).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154:7,pipeline,pipeline,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154,3,"['Pipeline', 'pipeline']","['PipelinesApiLifecycleActorFactory', 'pipeline', 'pipelines']"
Deployability,"My admin has added the policy serviceusage.services.use to my Service Account, whatever that means (I have no idea). Now I get this error:; ```; [2020-07-27 19:13:48,68] [error] PipelinesApiAsyncBackendJobExecutionActor [bf8fa2c2wf_hello.hello:NA:1]: Error attempting to Execute; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Futur",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:178,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"My group has been running Cromwell with AWS Batch as part of our pipeline development process and we've observed several cases of workflows ""silently"" failing where no Batch jobs have failed but the workflow log points to missing RC files. From our testing, this issue seems to affect ~10% of the samples that we try to process, although the issue appears ""randomly"" as there is no single set of samples that reproduces the error time after time. After digging through several logs, I believe I've traced the error to an issue where a batch job is being submitted, but it the service finds a previously run job that uses a completely different set of input files and runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:65,pipeline,pipeline,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['pipeline'],['pipeline']
Deployability,"My task are not using docker. Also I see no attempts at all to copy or softlink the files. Not in the log, and not in the cromwell-executions folder.; Also hard-linking seems to persist using the `SGE` backend. Even though the localization has the same configuration as above. So the error is not backend specific. Fortunately, all the other values in the config are used. Which makes me think that either my configuration file has some error (keys in wrong place). But I have checked this over and over again already with the example files and it seems to be correct (though I am not infallible of course).; Or the backend just ignores the values due to a bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-356221440:253,configurat,configuration,253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-356221440,2,['configurat'],['configuration']
Deployability,NB this will also require a fixup of https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/integrationTestCases/germline/joint-discovery-gatk/joint-discovery-gatk4.wdl#L341,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3326#issuecomment-368974102:120,integrat,integrationTestCases,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3326#issuecomment-368974102,1,['integrat'],['integrationTestCases']
Deployability,"NGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:2971,update,update,2971,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['update'],['update']
Deployability,NOTE: Centaur tests require https://github.com/broadinstitute/martha/pull/186 merged-and-deployed,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-691768881:89,deploy,deployed,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-691768881,1,['deploy'],['deployed']
Deployability,"NOTE: I haven't 100% confirmed that the application.conf isn't auto-included, but since we now have separated cromwell into artifacts, based on the HOCON docs it does not appear that we're using an ""application.conf"" as intended: https://github.com/typesafehub/config/blob/master/HOCON.md#conventional-configuration-files-for-jvm-apps. If someone needs to specify a backend via a new application.conf, they shouldn't need to do one of the two current workarounds:. 1) Know to include the _original_ application.conf.; 2) Copy/paste all of the previous application.conf, including things like akka dispatcher executors. The fix is to rename the core/src/main/resources/{application.conf => reference.conf}. When one then adjusts settings while running the fat-jar, one definitely doesn't need to re-include a reference.conf. The ticket says ""most of?"" because there may be some elements of the current core/.../application.conf that would be more appropriate for a fat-jar _only_, such as having the main.hsqldb and an included Local backend.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1389:302,configurat,configuration-files-for-jvm-apps,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1389,1,['configurat'],['configuration-files-for-jvm-apps']
Deployability,"NOTE: If anyone wants to follow up on #1499 and put in [less aggressive polling](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482), here is the [heart of the batch requesting](https://github.com/broadinstitute/cromwell/blob/de95456159c27a5145612b6acc95d4e713a5e9f8/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/api/PipelinesApiBatchHandler.scala) wired into the Google/PAPI backend that could be generalized for all backends.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-426347827:319,pipeline,pipelines,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-426347827,3,"['Pipeline', 'pipeline']","['PipelinesApiBatchHandler', 'pipelines']"
Deployability,"NOTE: The publishing-contract tests are failing due to active maintenance on the pact broker. That said, the updated query doesn't change any behavior or payloads from Cromwell so there shouldn't be any changes to the contracts to begin with.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867:109,update,updated,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867,1,['update'],['updated']
Deployability,"NOTE: This ticket refinement will require work with Google/Verily. Currently at the start of a call a PAPI-client must register a path to be delocalized via [`gsutil`](https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines#pipelineparameter). The PAPI `gsutil cp` does not use [`-r`](https://cloud.google.com/storage/docs/gsutil/commands/cp#description), so it does not work for nested directories. For POSIX-globs the cromwell ~~JES~~ PAPI backend is able to delocalize by creating a new folder containing matches of each of the outputs. We could try to flatten a directory in the same way to delocalize all the files nested inside the top level directory, but ideally we would maintain the paths to the directories. For example if a WDL command were to create a structure like:; ```; out/normal/sample.bam; out/tumor/sample.bam; ```. Even using our globs with `Array[File] bams = glob(""out/*/sample.bam"")` would cause problems right now with the way we flatten globs into a single directory. Cromwell creates a directory (sort-of) named `out/cromwell-glob-1/`, and then hard links in each of the `sample.bam` files. This won't work with the aforementioned glob as >1 file match the same name and will collide in the links. A/C: Delocalize globs and nested-directories with paths-in-the-cloud similar to the path-generated-by-the-command. A test should be created to ensure that a collision like the above delocalizes correctly. This will like require updates to the PAPI API, possibly even an implementation of a custom (de-)localizer in cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3173:226,pipeline,pipelines,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3173,3,"['pipeline', 'update']","['pipelineparameter', 'pipelines', 'updates']"
Deployability,"NOTE: While I need to describe these WDLs using our Cromwell instance, there is a CROM-4572 ticket already created that was previously closed, and [this comment](https://broadworkbench.atlassian.net/browse/CROM-4648?focusedCommentId=17048) mentions eliminating zip imports. While that's being figured out, I'm contributing this patch JIC someone else wants to cherry-pick this for their instance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7133#issuecomment-1540253229:328,patch,patch,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7133#issuecomment-1540253229,1,['patch'],['patch']
Deployability,"NOTE: if the PAPI v2 upgrade happens in all environments before horizontaling, this ticket becomes unnecessary and should be closed as a noop. OTHERWISE:. Adapt the existing Centaur test for PAPI v1 to PAPI v2 upgrade for a horizontal Cromwell configuration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4801:21,upgrade,upgrade,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4801,3,"['configurat', 'upgrade']","['configuration', 'upgrade']"
Deployability,NVIDIA Driver version upgrade,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4942:22,upgrade,upgrade,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4942,1,['upgrade'],['upgrade']
Deployability,Native install,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4803#issuecomment-480346756:7,install,install,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4803#issuecomment-480346756,1,['install'],['install']
Deployability,Need a HOWTO on creating a refresh token and how to modify a configuration file appropriately,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2100:61,configurat,configuration,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2100,1,['configurat'],['configuration']
Deployability,"Nevermind, just noticed some failures due to Docker rate-limiting, which I would love to completely avoid. I'm going to try installing Vault directly rather than using Docker.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603:124,install,installing,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603,1,['install'],['installing']
Deployability,New example configuration to reflect changes since v52. Signed-off-by: markjschreiber <markjschreiber@gmail.com>,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5858:12,configurat,configuration,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5858,1,['configurat'],['configuration']
Deployability,"Nice work! I'll answer questions before reviewing line-by-line in case it leads to changes. 1. `-e` is for exclude. There is a `papi_v2beta_gcsa.test` that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one).; 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid.; 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in `StandardAsyncExecutionActor#requestsAbortAndDiesImmediately`, it does seem like we may want the `false` behavior because it's responsible for some finalization activities around the job.; 4. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination).; 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130:269,configurat,configuration,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130,1,['configurat'],['configuration']
Deployability,"No longer blocked. It appears that the service account used by centaur tests for requester-pays testing is not compatible with the bucket used in `arrays` centaur-integration-test. Options:; - Use a workflow option to override-the-override, the service account back to the ""original"" service account specified in the JSON; - Reconfigure the centaur-integration-tests to not use an override of the requester-pays service account",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3846#issuecomment-409588696:163,integrat,integration-test,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3846#issuecomment-409588696,2,['integrat'],"['integration-test', 'integration-tests']"
Deployability,"No need to wait on the [dsp-jenkins PR](https://github.com/broadinstitute/dsp-jenkins/pull/524), that's just the Groovy code that can be used to regenerate the integration test jobs on fc jenkins. I manually generated those jobs on fc jenkins yesterday. This does need to wait on a newly documented flaky test though grumble grumble BT-241",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6321#issuecomment-825606134:160,integrat,integration,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6321#issuecomment-825606134,1,['integrat'],['integration']
Deployability,"No worries, but thank you for the update! We're on our current sprint and assuming we have some time leftover, we'll come back to it, otherwise it's been allotted towards the upcoming sprint. Thanks! @gemmalam",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-501727926:34,update,update,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-501727926,1,['update'],['update']
Deployability,"No worries, thanks for the update, I'll skip this workflow for now then :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436234476:27,update,update,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436234476,1,['update'],['update']
Deployability,"Not 100% sure what wasn't working at what point. I suspect that based on the order of the original commits<sup>1</sup>, the `RunMysql` and server should have both worked at ""4."". At that point I believe the config `url` still contained `useSSL=true`, the application config was being passed on the command line, and the mysql jdbc code should have been in the main assembly. By the time I was running ""11."" earlier today, the configuration `url` no longer contained `useSSL=true`, and connections within `SlickDataAccess` were returning the error combo:. ```; java.sql.SQLTimeoutException: Timeout after 1000ms of waiting for a connection.; ...; Caused by: java.sql.SQLException: Access denied for user '…'@'…' (using password: YES); ```. I did add another variable in ""11."" by always testing with `useSSL=true&requireSSL=true`, but according to the [logs](http://pastebin/209) of the latest 'RunMysql', `jdbcMain` and `jdbcRequireSsl` passed. So that _shouldn't_ have changed the results. Meanwhile, all test combinations of setting ssl worked for both slick and raw datasource connections, in tests via the url (*Ssl*), or via the dataSource properties (*Prop). So I think just setting back the `useSSL=true` is the minimum required fix, but I'd prefer to see `requiredSSL=true` added as well, as was successfully run in `slickSslDriver`. <sup>1</sup> What I believe is the previous order of the commits:; 1. Updated run.sh to pass in the mysql key & trust stores.; 2. log database config; 3. make mysql not test-only; 4. Add config file option in run.sh to make container use custom configuration; 5. debugging ""script""; 6. log actual uniquified config; 7. Test at JDBC level.; 8. hardcode use of SSL; 9. count rows in WORKFLOW_EXECUTION; 10. Logging the just the URL in SlickDataAccess, not the entire config.; 11. Added a suite of mysql ssl test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815:426,configurat,configuration,426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815,3,"['Update', 'configurat']","['Updated', 'configuration']"
Deployability,"Not it isn't, I believe @mcovarr is working on something that should make this ""go away"". ; In the meantime you can try to increase `database.db.queueSize` in the configuration. Results are not guaranteed though it's just giving slick more room but it might still fail. The default is 1000.; Also how big is your workflow ? Must be large to hit this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093:163,configurat,configuration,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093,1,['configurat'],['configuration']
Deployability,"Not necessarily the problem, but even non-obvious GCP quotas can limit how many workers are scheduled. Specifically, some things to look out for:. - Preemtible specific resources, like CPUs and Memory (if you're running preemtibles); - If you are using preemtibles, there may not be enough available instances; - Local SSD (GB); - Internal IP addresses; - In-use IP addresses; - List requests per 100 seconds. I thought mine were high enough, but from this page (replace `$region` with your region) you can click the ""Current Usage"" to sort by in-demand resources:. - https://console.cloud.google.com/iam-admin/quotas?project=portable-pipeline-project&location=$region. ![image](https://user-images.githubusercontent.com/22381693/72295508-d2132900-36ab-11ea-8380-256c2ad381b4.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305:635,pipeline,pipeline-project,635,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305,1,['pipeline'],['pipeline-project']
Deployability,Not sure about switching from the canonical repo to an unknown source for something like this. Also it might be nice to upgrade from jq 1.5 to the 1.6 that was released a couple of months ago if we're going to change this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4578#issuecomment-457335175:120,upgrade,upgrade,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4578#issuecomment-457335175,2,"['release', 'upgrade']","['released', 'upgrade']"
Deployability,"Not sure how hard it would be to do this, but another useful metric would be network usage (as represented by bytes going in/out at a moment in time). I haven't looked at this in cromwell workflow but at my last job we had a number of workflows that were bandwidth constrained (mainly due to network drive mounts) until we moved them to to SSD. Pipelines issue: https://broadinstitute.atlassian.net/browse/DSDEGP-1360",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-319378573:345,Pipeline,Pipelines,345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-319378573,1,['Pipeline'],['Pipelines']
Deployability,"Not sure what I should be doing. I have tried the following command:; ```; gcloud logging read 'timestamp>=""2020-09-01T00:00:00Z""' > logs; ```; And then:; ```; $ cat logs | grep 30148356615-compute@developer.gserviceaccount.com -A10 | grep -i permission | cut -d: -f2 | sort | uniq -c; 14 lifesciences.operations.cancel; 425 lifesciences.workflows.run; 12 storage.buckets.get; 30629 storage.objects.create; 30985 storage.objects.delete; 12819 storage.objects.get; 157 storage.objects.getIamPolicy; 6859 storage.objects.list; ```; It does seem to be the case that `storage.objects.delete` is requested many times, so that is definitely an issue when you only have roles `storage.objectCreator` and `storage.objectViewer` but not `storage.objectAdmin`. I did not observe any permission from role `iam.serviceAccountUser` but that role is indeed needed. And I observe some requests for permission `storage.buckets.get` that do end in ERROR, but it does not seem to affect the pipeline.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685986970:973,pipeline,pipeline,973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685986970,1,['pipeline'],['pipeline']
Deployability,Not sure why codecov/patch is reporting a low number. I have added unit tests for the modified code as possible but it doesn't seem to be recognizing it 🤔,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7392#issuecomment-2032583592:21,patch,patch,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7392#issuecomment-2032583592,1,['patch'],['patch']
Deployability,Not sure why the build is showing no code coverage on the patch. I tagged the new test the same way as the existing tests (as AwsTest). It ran successfully in my local env when I enable that tag.; Does the CI not run any of the AwsTests?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-566812068:58,patch,patch,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-566812068,1,['patch'],['patch']
Deployability,"Not terribly convinced of the value of the canary test when compared to the interesting five dollar genome case, but put here as the OKR asked for it. Also note that the original canary was a submission of 30k workflows, not a scatter. I modified it to fit in a single workflow run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5196:43,canary,canary,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5196,2,['canary'],['canary']
Deployability,Note before merging: Change WDL in DSDE-Pipelines w.r.t. commas in workflow outputs,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/309#issuecomment-161101612:40,Pipeline,Pipelines,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/309#issuecomment-161101612,1,['Pipeline'],['Pipelines']
Deployability,"Note for reviewers verifying testing changes: Travis, `sbt test`, and `sbt 'test-only cromwell.engine.db.slick.SlickDataAccessSpec'` no longer run the database integration tests. One must use either:. ``` bash; sbt 'integration:test-only cromwell.engine.db.slick.SlickDataAccessSpec'; ```. or:. ``` bash; sbt 'alltests:test-only cromwell.engine.db.slick.SlickDataAccessSpec'; ```. @cjllanwarne First reviewer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/398#issuecomment-173965505:160,integrat,integration,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/398#issuecomment-173965505,2,['integrat'],['integration']
Deployability,"Note for the others (since @aednichols and I are discussing this offline) -- `womgraph` includes inputs/outputs in the graph, which `graph` does not. My $0.02 is that this can be either a very good thing or a very bad thing depending on the complexity of your pipeline: for a simple one it' really nice, but for a very complex one it makes it really hard to read.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561407757:260,pipeline,pipeline,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561407757,2,['pipeline'],['pipeline']
Deployability,"Note how the wdl below attempts to use `output_dir` in its output for `reproducibility_files`. This causes a failure. If I remove the `String output_dir` from the task and update the output to a hardcoded path, `Array[File] reproducibility_files = glob(""reproducibility_output/*.*"")`, it works fine. . ``` wdl; task run_plot_reproducibility {; File file1; File file2; String output_dir; command {; run_plot_reproducibility ${file1} ${file2} ${output_dir} 3.7; }; output {; File reproducibility_table = ""${output_dir}/reproducibility.tsv""; File reproducibility_final_results = ""${output_dir}/final_results.tsv""; File reproducibility_plot = ""${output_dir}/reproducibility_Reproducibility.png""; #### HERE; Array[File] reproducibility_files = glob(""${output_dir}/*.*""); }; runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1302:172,update,update,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302,1,['update'],['update']
Deployability,"Note to reviewers: this is currently packaged as 4 commits. The ""Upgrade to ScalaTest 3.2.1"" commit was my shot in the dark at fixing a runtime error that turned out not to fix the runtime error but did produce an upgrade to ScalaTest 3.2.1. It's a huge and tedious commit that's not really related to what's happening in the other commits. You'll probably want to ignore the ScalaTest 3.2.1 upgrade commit and focus on the other 3 instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5751:65,Upgrade,Upgrade,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5751,3,"['Upgrade', 'upgrade']","['Upgrade', 'upgrade']"
Deployability,"Note: Treat this as a draft/early look/WIP rather than a request to merge immediately. I forgot to push the ""draft"" button. A bunch of bash scripts to be run by Jenkins and executed on VMs in order to get overnight centaur testing of our performance scripts. * Lets the overnight perf tests work again; * Splits the logic into stages so that we aren't doing everything on a single VM (unblocking the transition to horicromtal); * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-deploy-and-centaur and the sub-jobs which it calls (especially https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-run-centaur), which make use of these scripts.; * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-all-centaur-tests for a jenkins job which runs *all* of our centaur tests against ad-hoc perf Cromwells. TODOs:; - [ ] Document these changes in the google doc; - [ ] Better error reporting(?); - [x] Compile into a suite in jenkins(?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5075:512,deploy,deploy-and-centaur,512,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5075,1,['deploy'],['deploy-and-centaur']
Deployability,Note: We **must** wait for https://broadworkbench.atlassian.net/browse/BW-548 to be deployed before we merge this one.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6210:84,deploy,deployed,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6210,1,['deploy'],['deployed']
Deployability,"Note: the upgraded 1.0 version of this workflow passed validation, so it's something specific to the draft-2 WDL implementation",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3885#issuecomment-404899951:10,upgrade,upgraded,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3885#issuecomment-404899951,1,['upgrade'],['upgraded']
Deployability,Note: this set of learnings may change or increase as we start rolling the archiver across real workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6318:63,rolling,rolling,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6318,1,['rolling'],['rolling']
Deployability,"Note:. As it stands, this PR could return false positives for a workflow that has been archived from metadata but still has a summary. I think this is already true for the function converted in https://github.com/broadinstitute/cromwell/pull/4617. I'm not sure whether it's a problem per se, but certainly notable. **Update**: based on the analysis in https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053 the old function `validateWorkflowIdInMetadata` already returns `true` for archived workflows.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2435487930:317,Update,Update,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2435487930,1,['Update'],['Update']
Deployability,"Nothing really clever about this PR, just cashing in on past investments in separation of concerns. 1. Remove SBT projects; - Clean compile time 64s -> 53s on M1; - Dependencies removed, no longer subject to security updates or conflicts (see https://github.com/broadinstitute/cromwell/pull/6948); 2. Remove Centaur integration tests; - Slightly improved Travis build time; - Less stuff to port when we leave Travis; 3. Sever connections between CWL and the rest of Cromwell; - Because of Cromwell's extremely compartmentalized design, only two files really reference CWL directly:; - Entry point for server mode; - Entry point for command-line Womtool; - Only small logic updates needed; 4. Can now safely delete top-level `cwl` directory because nothing depends on it. ---. Reviewer's guide:; - Commits up through [Remove obsolete tests](https://github.com/broadinstitute/cromwell/pull/6955/commits/7a26149d9e70818edf852a16b114809ca9c0dc29) are self-contained and pass CI on their own; - [No longer minimal](https://github.com/broadinstitute/cromwell/pull/6955/commits/557d7b72a97651bcdca8ee27590ebfa29473ad05) removes most of the code; - [Remove *.cwl files](https://github.com/broadinstitute/cromwell/pull/6955/commits/eb4eaef0574ec06a256d38bb222d01ebc44a7e9f) speaks for itself",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6955:217,update,updates,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6955,3,"['integrat', 'update']","['integration', 'updates']"
Deployability,"Noticed the integration tests were failing. . After looking into it a little, I'm pretty sure that what happened is that when [this commit](https://github.com/broadinstitute/cromwell/commit/cc636457bf46d6fec001976ee830ad81eaed35b8) un-hardcoded the aws region, the integration tests started trying to submit batches to `batch.default.amazonaws.com` rather than `batch.us-east-1.amazonaws.com`. I think that whatever library is importing the credentials file for use with in AwsAuthMode.scala isn't defaulting to `us-east-1` as advertised. . I believe this will fix the issue but you might prefer to solve the problem by correcting the default behavior instead; if so feel free to close this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4326:12,integrat,integration,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4326,2,['integrat'],['integration']
Deployability,Now setting config directly in the `database` stanza.; Deprecated `database.config`.; Database migration now just uses the key `database.migration` for configuration settings.; Moved database migration defaults into the reference.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1440:152,configurat,configuration,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1440,1,['configurat'],['configuration']
Deployability,"Now that our python dependency has been updated, the drslocalizer should work again and our tests should pass again.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7148:40,update,updated,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7148,1,['update'],['updated']
Deployability,"Now that we've updated our WDLs to 1.0, we've found that `womtool graph` no longer works. It looks like it only supports draft2 and earlier WDL. `test.wdl`:; ```; version 1.0. workflow Test { }; ```. ```; $ java -jar womtool-35.jar graph /tmp/test.wdl ; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: ERROR: Finished parsing without consuming all tokens. version 1.0; ^; ; 	at wdl.draft2.parser.WdlParser.parse(WdlParser.java:2330); 	at wdl.draft2.parser.WdlParser.parse(WdlParser.java:2335); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:266); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:160); 	at scala.util.Try$.apply(Try.scala:209); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:160); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:156); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:571); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:94); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:48); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:125); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:130); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:18); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:18); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```; ; The `womgraph` command still works, but the output from that command is so verbose it's unusable for viewing our workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4234:15,update,updated,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234,1,['update'],['updated']
Deployability,"OK I think I got it working. . It turns out the custom AMI I created was incorrect. When making a custom AMI using the cloud formation stacks as described [here](https://docs.opendata.aws/genomics-workflows/aws-batch/create-custom-ami/). The AMI type needs to be specified as 'cromwell' and the Scratch mount point needs to be specified as `\cromwell_mount`. This information is stated [elsewhere](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/), but perhaps this [page](https://docs.opendata.aws/genomics-workflows/aws-batch/create-custom-ami/) should be updated to include this information?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435134332:583,update,updated,583,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435134332,1,['update'],['updated']
Deployability,OK will keep you updated. Looping in @abaumann as well.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1609#issuecomment-256070879:17,update,updated,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1609#issuecomment-256070879,1,['update'],['updated']
Deployability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172:40,release,release,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172,10,"['release', 'update']","['release', 'updates']"
Deployability,"OK, after discussion with @Horneth and @cjllanwarne:. ``` hocon; google {; application-name = ""cromwell"". auths = [; {; name = ""cromwell-service-account""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh""; client-id = ""secret_id""; client-secret = ""secret_secret""; }; ]; }. backend {; default = ""JES""; providers = [; {; name = ""JES""; class = ""cromwell.engine.backend.jes.JesBackend""; config {; ...; genomics {; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // Use the Cromwell service account for creating the Pipeline and manipulating auth JSONs.; auth = ""cromwell-service-account""; }; filesystems = [; {; gcs {; auth = ""user-via-refresh""; }; }; ]; }; },; ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203502798:624,Pipeline,Pipeline,624,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203502798,1,['Pipeline'],['Pipeline']
Deployability,"OM)\n# https://askubuntu.com/a/823798\ntail /dev/zero"",; ""shardIndex"": -1,; ""jes"": {; ""endpointUrl"": ""https://lifesciences.googleapis.com/"",; ""machineType"": ""custom-1-2048"",; ""googleProject"": ""encode-dcc-1016"",; ""monitoringScript"": ""gs://caper-data/scripts/resource_monitor/resource_monitor.sh"",; ""executionBucket"": ""gs://encode-pipeline-test-runs/caper_out_10"",; ""zone"": ""us-central1-b"",; ""instanceName"": ""google-pipelines-worker-ead27fbad8aa73b157bfc126cd63331f""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""[0,137]"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""1"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2 GB"",; ""memory"": ""2 GB""; },; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""hashes"": {; ""output count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""runtime attribute"": {; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327"",; ""docker"": ""A84529F7A095541F1249576699F24AA1"",; ""continueOnReturnCode"": ""614DAABB2D7AAB5D41921614A49E4F92""; },; ""input count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""backend name"": ""50F66ECBC45488EE5826941BFBC50411"",; ""command template"": ""F41FEBA57D556A16A5F6C4EEF68ED1E0""; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache""; },; ""inputs"": {},; ""backendLabels"": {; ""wdl-task-name"": ""fail-oom"",; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42""; },; ""labels"": {; ""wdl-task-name"": ""fail_oom"",; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42""; },; ""failures"": [; {; ""causedBy"": [],; ""message"": ""The compute backend terminated the job. If this termination is unexpected, examine likely causes such as preemption, running out of disk or memory on the compute instance, or exceeding the backend's maximum job duration.""; }; ],; ""jobId"": ""projects/99884963860/locations/us-central1/operations/1374639517116411519"",; ""monitoringLo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:3661,pipeline,pipeline-test-runs,3661,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,2,['pipeline'],"['pipeline-test-runs', 'pipelines-worker-']"
Deployability,ORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6a68865c-13ea-410e-a774-9b5e58f45523'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '669f97ea-1256-4732-a4ac-a565ff749e41'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c3994549-e8e1-4478-9bf3-ef46cc16f505'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '8d535b7f-4832-429c-9144-c3c39eeb7006'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd89fe8c2-3803-407d-99c6-3b77443ff20b'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '768dfe30-7072-4430-bb31-355499ca1443'; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4022:3383,update,update,3383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022,5,['update'],['update']
Deployability,"OS: **Centos 7**; Cromwell version: **cromwell 86** installed from conda-forge; Backend: **SFS**. Hello,. I'm configuring Cromwell to run on my group's SLURM cluster, and struggling to make `cpu`/`memory` runtime attributes optional. I believe this is supported because the ""Getting started on HPC Clusters"" documentation shows `memory_gb` as an optional `runtime-attribute`: https://cromwell.readthedocs.io/en/develop/tutorials/HPCIntro/. ```; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. My intent is for the user to only provide arguments for `cpu`, `memory`, `runtime_minutes`, and `partition` if they intend to override the SLURM cluster's defaults. I do not want to have cromwell supply defaults, because if these arguments are omitted from the call to `sbatch` then the cluster's defaults will be used. My understanding is that making them optional like `String? memory_mb` and then using syntax like `${""--mem "" + round(memory_mb) + ""m""} \` in the submit script means that argument will only be added if `memory` is defined, and will be omitted if `memory` is not defined. I've followed the documentation as closely as I can. However, when I try to submit a test job without `cpu` and `memory` set as a runtime attribute, I get a failure with these exceptions:; ```; cromwell.core.CromwellAggregatedException: Initialization Failure:; Runtime validation failed:; 	Task myTask has an invalid runtime attribute cpu = !! NOT FOUND !!; 	Task myTask has an invalid runtime attribute memory = !! NOT FOUND !!; 	at cromwell.engine.workflow.WorkflowActor$$anonfun$3.applyOrElse(WorkflowActor.scala:356); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$3.applyOrElse(WorkflowActor.scala:339); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); ```. Here is the test WDL I'",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7455:52,install,installed,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7455,1,['install'],['installed']
Deployability,"OUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:3057,update,updateSchema,3057,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['update'],['updateSchema']
Deployability,OUTPUT_DIR [-c CONTAMINANTS]; cromwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); cromwell_1 | at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); cromwell_1 | ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:2216,pipeline,pipelines,2216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['pipeline'],['pipelines']
Deployability,O_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); cromwell_1 | at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); cromwell_1 | at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); cromwell_1 |,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:2298,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,Oh I think I see that the existing version doesn't support copying directories. Maybe there have been updates since this code was written though?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537142572:102,update,updates,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537142572,1,['update'],['updates']
Deployability,"Oh funny, they just released 2.9.2 yesterday. I can give that a shot, this should wait until the release goes out anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6794#issuecomment-1177813011:20,release,released,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6794#issuecomment-1177813011,2,['release'],"['release', 'released']"
Deployability,"Oh great! Is the root cause of that behavior known, why sometimes workflow store updates but not workflow metadata? Or is it just that the workflow metadata update fails, and we know why?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-478633097:81,update,updates,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-478633097,2,['update'],"['update', 'updates']"
Deployability,Oh looks like the API client needs to be updated too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2357#issuecomment-310082912:41,update,updated,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2357#issuecomment-310082912,1,['update'],['updated']
Deployability,"Oh wow, thank you so much for this tip. I knew you could export to BigQuery, but I didn't realise that:; > if you use regular file export, you should be aware that regular file export captures a smaller dataset than export to BigQuery. (from [this page](https://cloud.google.com/billing/docs/how-to/export-data-bigquery)). In the end, I enabled BigQuery export, then ran this query:. ```sql; SELECT SUM(cost); FROM `PipelineBilling.gcp_billing_export_v1_BILLING_ACCOUNT_ID`, UNNEST(labels) as l; WHERE l.key = 'cromwell-workflow-id' AND l.value = 'cromwell-MY-WORKFLOW-ID'; ```. It works perfectly, and gives me a single cost for each pipeline! Thanks so much!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4394#issuecomment-440863484:416,Pipeline,PipelineBilling,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4394#issuecomment-440863484,2,"['Pipeline', 'pipeline']","['PipelineBilling', 'pipeline']"
Deployability,"Oh, and `sbt assembly` already had _all_ of its tests disabled by build.sbt, so @geoffjentry's request that ""when one checks out the code, sbt assembly not fail on an integration test"" holds true without any other modifications. :wink:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169441288:167,integrat,integration,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169441288,1,['integrat'],['integration']
Deployability,"Oh, and during rebase I switched from singularity-ce to apptainer, only because the latter's fork is easier to install in 2024.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6736#issuecomment-2105391075:111,install,install,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6736#issuecomment-2105391075,1,['install'],['install']
Deployability,Ok -- after discussion it seems worth it to enable compression on this field on hotfix so that they can scale to ~5000 samples,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/910#issuecomment-224691438:80,hotfix,hotfix,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/910#issuecomment-224691438,1,['hotfix'],['hotfix']
Deployability,Ok I've updated the PR.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-221913147:8,update,updated,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-221913147,1,['update'],['updated']
Deployability,"Ok, I see! Now as you mentioned, I think the problem here is to update the doc, to distinguish between execution statuses and workflow statuses, and elaborate execution statuses. Glad to know more about the internal mechanism of Cromwell and thanks for the explanation here!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3328#issuecomment-372387957:64,update,update,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3328#issuecomment-372387957,1,['update'],['update']
Deployability,"Ok, running the Docker daemon as root is normal (the docs says it's [required](https://docs.docker.com/engine/security/security/), actually). The issues with non-root default users should be fixed in [this PR](https://github.com/broadinstitute/cromwell/pull/1865), but that code is currently only on develop (the forthcoming 25 release). non-root default users should Just Work with the code from that PR, you shouldn't have to make any changes to your config. The `master` branch corresponds to the 24 release; do you mean you're running the 23 release?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835:328,release,release,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835,3,['release'],['release']
Deployability,"Ok. I'll revert [this](https://github.com/broadinstitute/cromwell/commit/2e3f45bbedeaa4c522751e9ff6f5594c57b88b35#diff-facc2160a82442932c41026c9a1e4b2bL28) change in behavior from a while ago, and update the code to reset logging type based on standard command line arguments. This goes against what the docs currently say, so I'll update those too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/330#issuecomment-165228352:197,update,update,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/330#issuecomment-165228352,2,['update'],['update']
Deployability,Okay updated again...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/363#issuecomment-170125680:5,update,updated,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/363#issuecomment-170125680,1,['update'],['updated']
Deployability,"Okay, I'll update and see if it works. Regardless, you can close this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321121155:11,update,update,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321121155,1,['update'],['update']
Deployability,"Okay, I've made more progress. But more issues are popping up. @cjllanwarne . You cannot ask for the filesize of an sra file to configure the disk space you'd like at runtime thats what causes the ; `[2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath` issue mentioned above. When I remove that line from my wdl things get better, but I'm running into two new separate issues. 1. Cromwell tries to chmod the mounted sra directory which is not allowed.; code:; https://github.com/broadinstitute/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:593,pipeline,pipelines,593,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,2,['pipeline'],['pipelines']
Deployability,"Okay, thanks. I look forward to hearing any updates from you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509815272:44,update,updates,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509815272,1,['update'],['updates']
Deployability,"On Cromwell version, ""34-5156b78-SNAP"", when releasing a workflow with ""On Hold"" status, if the workflow has been assigned with a label, after release, the label will disappear or get reset to the original value.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3880:143,release,release,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3880,1,['release'],['release']
Deployability,"On Cromwells which are not our own production instance, people seem to be more frustrated than delighted by having the initial read limits be arbitrarily restrictive. - [x] Will probably need to update `firecloud-develop` to set some values which are still relying on the previous low defaults back down to 128k",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5034:195,update,update,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5034,1,['update'],['update']
Deployability,"On Wed, Jun 13, 2018 at 4:08 PM Dan Billings <danb@broadinstitute.org> wrote:; I think we can patch it on our end, doesn't sound too bad. OK, thanks and sorry again.; ; On Wed, Jun 13, 2018 at 3:54 PM Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Ok good to know, doesn't sound too hard to patch but we'd also have to patch the previous release if we don't want it to be broken there too. I'm adding Dan and Ruchi who can decide on what to do. Thibault. On Wed, Jun 13, 2018 at 3:43 PM Aaron Kemp <kemp@google.com> wrote:; On Wed, Jun 13, 2018 at 3:38 PM, Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Thank you,. I just saw a bunch of those today:. PAPI error code 3. Execution failed: creating instance: inserting instance: Invalid value for field 'resource.disks[1].initializeParams.diskType': 'zones/us-central1-b/diskTypes/PERSISTENT_SSD'. The referenced diskType resource cannot be found. I don't think I've touched anything related to disk types. Is this a quota issue ?. No this was a change on our side. There was a bug in the v2alph1 backend where it was ignoring the disk type requested. I fixed it and pushed it out. The diskType in v2alpha1 is the standard GCE names (so in this case, pd-ssd) - in v1 they were a custom enumeration. This will probably require a cromwell change depending on where that value is coming from (eg, maybe it's in the WDL file itself?); I think we're translating it from WDL-speak. Where can I find the ""standard GCE names"" (e.g. ""pd-ssd""). ""gcloud compute disk-types list"" shows the full per-zone list, but in practice it's really just one of ""pd-ssd, pd-standard or local-ssd"".; ; and would it become ""zones/us-central1-b/diskTypes/pd-ssd"" or just ""pd-ssd"" ?. Pipelines adds the zonal resource chunk based on the zone we choose from the set you give us. So you just say 'pd-ssd' and we staple on the appropriate zone to the front. Aaron",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3776:94,patch,patch,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3776,5,"['Pipeline', 'patch', 'release']","['Pipelines', 'patch', 'release']"
Deployability,"On the servers where this will be used, . > Did you install the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files?; > http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171666835:52,install,install,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171666835,1,['install'],['install']
Deployability,"Once this review is complete, I'll back port the patch to the actual hotfix branches `0.21_hotfix` and `0.22_hotfix`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-260833983:49,patch,patch,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-260833983,2,"['hotfix', 'patch']","['hotfix', 'patch']"
Deployability,"One could make the argument that it'd hose people who don't want to update their options files, but I know that's not really the answer :'(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1167#issuecomment-253881280:68,update,update,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1167#issuecomment-253881280,1,['update'],['update']
Deployability,"One last question, @geoffjentry --is this a change just for hotfix branch or develop as well?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/755#issuecomment-218190886:60,hotfix,hotfix,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/755#issuecomment-218190886,1,['hotfix'],['hotfix']
Deployability,One option: we're currently using conditions by `files`. We could replace-or-update those conditions using `labels`: http://docs.pullapprove.com/groups/conditions/#labels,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392192566:77,update,update,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392192566,1,['update'],['update']
Deployability,"One possible solution: We should probably create a trait which loads all the configuration (once per application), and let classes mix it in to avoid doing ConfigFactory.load() at multiple places",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796:77,configurat,configuration,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796,1,['configurat'],['configuration']
Deployability,"Only store null instead of """" for a clob.; Only store null instead of Array.empty[Byte] for a blob.; All lob columns are now nullable.; Added test to ensure that all lob columns are nullable.; Updated SqlConverters for converting to/from Option and empty lobs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1927:193,Update,Updated,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1927,1,['Update'],['Updated']
Deployability,"Only the refresh-token mode requires new credentials to be created/validated for every workflow, all other modes can be validated only from the configuration. This ensures we don't re-create unnecessary auth modes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1429:144,configurat,configuration,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1429,1,['configurat'],['configuration']
Deployability,Oops - forgot that I only made this patch in my not-ready-yet cromwell branch but the check is live in centaur now,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1331:36,patch,patch,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1331,1,['patch'],['patch']
Deployability,"Options to fix it:; - Migration that changes all `executionStatus: Preempted` metadata entries into `executionStatus: RetryableFailure`.; - Migration that changes all `executionStatus: Preempted` metadata entries into `executionStatus: RetryableFailure` and also add `backendStatus: Preempted`.; - Unlike option 1, I think this will require a custom scala migration so might be a fair bit slower; - Change the CRDT to just be fine with `Preempted` as an `ExecutionStatus` again.; - Not intellectually honest, but easy to add as a hotfix and no migration necessary",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2438#issuecomment-315120152:530,hotfix,hotfix,530,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2438#issuecomment-315120152,1,['hotfix'],['hotfix']
Deployability,Order of heartbeats database updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4240:29,update,updates,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4240,1,['update'],['updates']
Deployability,Original Scala Steward MRs:. 1. https://github.com/broadinstitute/cromwell/pull/6520; Change notes: https://github.com/googleapis/java-storage/releases. 2. https://github.com/broadinstitute/cromwell/pull/6517; Change notes: https://github.com/googleapis/java-bigquery/releases?page=2. 3. https://github.com/broadinstitute/cromwell/pull/6516,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6567:143,release,releases,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6567,2,['release'],['releases']
Deployability,"Original ScalaSteward PR was https://github.com/broadinstitute/cromwell/pull/6527. Note though that it was only to version 4.4.3, but 4.4.3 still has the postgres unique constraint bug. Upgrading instead to the most recent release, which contains the bug fix.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6579:223,release,release,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6579,1,['release'],['release']
Deployability,"Originally posted this two in the JIRA issue tracker back in August. Reposting here since it didn't get a response over there: https://broadworkbench.atlassian.net/browse/BA-6548. > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl script which runs several subjobs in parallel. I believe the correct parlance is a scatter. I noticed that in some of the jobs of the scatter, some reference files failed to download from S3 even though they existed (Connection Reset by Peer). This failure caused the overall job to fail after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but recently, in June 2020, it appears the AWS Batch backend was majorly overhauled (by @markjschreiber, thanks! Also, tagging you because I suspect you might be the resident expert here :) ), and the previous fix (using the ecs proxy image) was supposedly obsoleted.; > ; > I also see that the s3fs library appears to be vendored into cromwell, and after digging around, it appears that one might be able to set retries via an environment variable(?). But even then, I feel like if that were to work, it would be much nicer if it was configurable through cromwell's config file somehow.; >; > So that brings me to my final question. Is there some configuration that allows me to retry failed downloads some number of times before failing the whole job? Or, perhap there is some alternative configuraiton which I've overlooked and someone could point me to it? Thanks!. In addition, just wondering if perhaps there is a service limit I might be running into?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946:1313,configurat,configuration,1313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946,1,['configurat'],['configuration']
Deployability,"Our Green friends are regularly seeing cases where Cromwell reports a workflow's `status` as `On Hold` but follow-on calls to `releaseHold` return a 403 HTTP status. This appears to be happen because `status` queries go to the potentially laggy workflow summary table but `releaseHold` goes straight to the workflow store. Cromwell [returns 403](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/webservice/routes/CromwellApiService.scala#L148) if the workflow for which `releaseHold` is invoked is [in the workflow store but not in `On Hold` or `Submitted`](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/workflowstore/SqlWorkflowStore.scala#L140) status. . 403 seems like a strange HTTP status to return in this case. If the thought was ""the workflow has to be on hold to be able to release the hold"" then we would also return 403 for workflows found in `Submitted`, but we don't; those get a 200. Since the status queries will always be laggy to some degree it seems to make more sense to interpret the endpoint as ""put this workflow in a runnable state"", where finding the workflow already in a `Running` state would not be cause for alarm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4983:127,release,releaseHold,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4983,4,['release'],"['release', 'releaseHold']"
Deployability,"Our WOMTool (previously WDLTool) has moved into the mega-mono-repo of Cromwell, but there's no way for Jamie User (like me) to find the latest of the WOMTool. . AC; - [x] have the WOMTool jar on the [Cromwell releases page](https://github.com/broadinstitute/cromwell/releases); - [x] update the [WOMtool docs](https://cromwell.readthedocs.io/en/develop/WOMtool/) to point to the releases page for the latest tool",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3031:209,release,releases,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3031,4,"['release', 'update']","['releases', 'update']"
Deployability,Our integration testing environments always stand up a Cromwell server. We need something to provide some testing for single workflow mode.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2411:4,integrat,integration,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2411,1,['integrat'],['integration']
Deployability,Our new saner TES polling defaults noticeably increased the runtime of our TES integration tests. Reverting CI to old behavior with config update.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7126:79,integrat,integration,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7126,2,"['integrat', 'update']","['integration', 'update']"
Deployability,Our team has a similar Singularity backend for SGE and SLURM.; https://github.com/ENCODE-DCC/atac-seq-pipeline/blob/master/backends/backend.conf,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438702335:102,pipeline,pipeline,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438702335,1,['pipeline'],['pipeline']
Deployability,"Overall, a code-based start of discussion. See also individual commits messages. Some or all could be integrated into #1198.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1199#issuecomment-234843562:102,integrat,integrated,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1199#issuecomment-234843562,1,['integrat'],['integrated']
Deployability,PAPI Updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3357:5,Update,Updates,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3357,1,['Update'],['Updates']
Deployability,PAPIv2 upgrade test requires secure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4196:7,upgrade,upgrade,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4196,1,['upgrade'],['upgrade']
Deployability,"PAPIv2; [2021-08-13 10:45:00,31] [info] Not triggering log of token queue status. Effective log interval = None; [2021-08-13 10:45:01,35] [info] WorkflowExecutionActor-a15c46b7-5f93-46d6-94a2-28f656914866 [a15c46b7]: Starting wf_hello.hello; [2021-08-13 10:45:02,34] [info] Assigned new job execution tokens to the following groups: a15c46b7: 1; [2021-08-13 10:45:04,75] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: echo ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; [2021-08-13 10:45:05,68] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); [2021-08-13 10:45:07,36] [error] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:3094,Pipeline,PipelinesApiRequestManager,3094,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiRequestManager']
Deployability,PBSPro Backend Configuration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967:15,Configurat,Configuration,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967,1,['Configurat'],['Configuration']
Deployability,"PI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaFork",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:1623,pipeline,pipelines,1623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['pipeline'],['pipelines']
Deployability,"PR 1 of 3:; 1. Remove cromwell-core dependency from cloud-support; 2. Run jes centaur on travis; 3. Generate coverage for integration tests. ---. Instead of credentials requiring WorkflowOptions, any String => String will do, including Map[String, String].; Retrieving credentials only requires actorSystem/executionContext when retrying.; Moved logback dependencies from common library over to testing.; Added mockito to all artifact tests.; Fixed akka-stream-testkit dependency appearing in core's main instead of test.; Split confusingly named baseDependencies into configDependencies ++ catsDependencies.; Other dependency cleanup to reduce duplicates and extra transitive dependencies.; Log stderr from centaur'ed cromwell failures.; The total attempt time to connect to cromwell for a test is now longer than the timeout of a cromwell restart.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2938:122,integrat,integration,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2938,1,['integrat'],['integration']
Deployability,"PR 2 of 3:; 1. Remove cromwell-core dependency from cloud-support; 2. Run jes centaur on travis; 3. Generate coverage for integration tests. ---. Centaur now uses configurable cloudSupport for auth, instead of always using application-default.; Refresh token is passed as a file path, then read by centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2948:122,integrat,integration,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2948,1,['integrat'],['integration']
Deployability,PR 3 of 3:; 1. Remove cromwell-core dependency from cloud-support; 2. Run jes centaur on travis; 3. Generate coverage for integration tests. ---. Publishing test validates executables and cross-versioned libraries/docs.; Updated run_tests_parallel.sh for centaur monorepo.; Quieted centaur runs of sbt assembly and coverage gen/upload.; Don't run problematic no_new_calls on TES.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2955:122,integrat,integration,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2955,2,"['Update', 'integrat']","['Updated', 'integration']"
Deployability,"PR Documentation describing the SparkBackend added to Cromwell. This PR adds support for execution of spark jobs as task in a workflow using the existing wdl format, with restrictions on the environment like having a local file system, a shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily bec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1339:394,configurat,configuration,394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339,3,"['configurat', 'deploy']","['configuration', 'configurations', 'deployMode']"
Deployability,"PR comments addressed, and tests re-passing. Changes include:. - Primitive and non-primitive file types.; - Primitive 'Dir' now 'Directory'.; - Split up and fixed test descriptions.; - Exception patches.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3079#issuecomment-355723031:195,patch,patches,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3079#issuecomment-355723031,1,['patch'],['patches']
Deployability,"PR now in for review #3772 - I can add another workaround in the meantime, since apparently this was only affecting the draft-2 language factory:. * upgrade to WDL 1.0. However, since an upgrade script is also coming in as part of Cromwell 33, maybe one of your other workarounds is less effort in the short term?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-396973479:149,upgrade,upgrade,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-396973479,2,['upgrade'],['upgrade']
Deployability,PS Basically I'd like to close these ugly gaps in my pipeline ; ![image](https://github.com/broadinstitute/cromwell/assets/57629300/ebe89fb6-8420-486d-b52f-653f29fc73c5). The gap between `rc` generation and the `Status change from Running to Done` message,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7144#issuecomment-1556087820:53,pipeline,pipeline,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7144#issuecomment-1556087820,1,['pipeline'],['pipeline']
Deployability,Papi network config updates and tests BT-372,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6476:20,update,updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6476,1,['update'],['updates']
Deployability,Part 1 of dividing #5769 to identify which is the problematic library update.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5800:70,update,update,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5800,1,['update'],['update']
Deployability,"Part II of the ""unexpected `JobFailedNonRetryable` during cache output copying"" saga. - Stop using the confusing `JobFailedNonRetryable` response when an output copy fails (it's not like `JobSucceeded` which correctly tells us that the job has already succeeded); - Update the tests to reflect this; - Add some sanity checks to the EJEA's handlers (specifically - did we copy the right outputs, and did we fetch the right outputs from the database)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4091:266,Update,Update,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4091,1,['Update'],['Update']
Deployability,Patch 3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3562:0,Patch,Patch,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3562,1,['Patch'],['Patch']
Deployability,Patch build docs and add a warning to womtool doc about graph failing on 1.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5299:0,Patch,Patch,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5299,1,['Patch'],['Patch']
Deployability,Patched swagger-ui routes and added tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/119:0,Patch,Patched,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/119,1,['Patch'],['Patched']
Deployability,Patches for problematic tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2975:0,Patch,Patches,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2975,1,['Patch'],['Patches']
Deployability,Patches to Workflow Store as a database table,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1199:0,Patch,Patches,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1199,1,['Patch'],['Patches']
Deployability,Patches to `SprayDockerRegistryApiClient` and spec.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/715:0,Patch,Patches,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/715,1,['Patch'],['Patches']
Deployability,Patches to offset date time usage.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/909:0,Patch,Patches,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/909,1,['Patch'],['Patches']
Deployability,"Path+modtime should guarantee that files are the same. . I have expanded the SFS test scala file so it properly tests the new `cached-inputs` strategy. I have added information on how to use the strategy in the docs, and added this PR to the changelog. ### Help still needed. There are two things that I could not figure out without cromwell developer help:. ~~* Checking whether a file exists and copying it to the cache should never be done by multiple threads simeltaneously. I have used the `synchronized` method to prevent this. I used an object for this, because I am sure it is unique within the JVM at cromwell runtime. This works fine, but I can imagine this can be solved in a nicer way using akka? However the akka documentation is an extensive jungle on its own, and requires quite some expertise to navigate. I could not find very quickly what I needed, and the `synchronization` primitive works fine. It is also **just 2 lines of extra code**. So if the akka solution is quite elegant as well I would like to learn about that. If not, well, it is not too bad having 2 lines of understandable commented code that is not ""the proper way of doing things(TM)"".~~. * I used the SFS scalatests to make sure everything worked correctly. However this did not test whether the thread safety was working correctly. I have added a test wdl in centaur: `standardTestCases/cached_copy/cached_copy.wdl`. This workflow creates 10 jobs that read the same input file. This workflow will crash if the `cached-inputs` cache is not used in a thread-safe way. I tested this manually with `java -Dbackend.providers.Local.config.filesystems.local.localization.0=""cached-copy"" -jar server/target/scala-2.12/cromwell-41-*-SNAP.jar run centaur/src/main/resources/standardTestCases/cached_copy/cached_copy.wdl` . Is there a way to integrate such a test in scalatest file? I have tried the `.par` method, but that did not quite work. I hope you will consider this PR as it solves an important issue for us. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900:3542,integrat,integrate,3542,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900,1,['integrat'],['integrate']
Deployability,"Per @vivster7 the bug report was against 0.19 hotfix, not develop. Not that this shouldn't be fixed here too. 😄 . @kcibul It seems #794 is also labeled 0.21 but actually applies to 0.19 hotfix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231123975:46,hotfix,hotfix,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231123975,2,['hotfix'],['hotfix']
Deployability,"Per discussion in https://github.com/broadinstitute/cromwell/pull/4039, it would be nice and clean to have different backend providers defined in an examples folder. The user:. - should more easily be able to inspect one backend, as it's not lost in a huge commented file; - from each backend example, a link to the docs (if they exist) should be provided; - per the files being separate, we don't ask the user to uncomment a million lines to use a backend (copy pasta, done). But given separation from the examples configuration file, we need to compensate by having really good instructions for doing this (I will write a nice README). I think this is the right way to go because it will more cleanly show the various backends that Cromwell provides, and how to use. Right now it's a bit overwhelming just looking at that file, and I can only imagine for a new user / someone not super keen on configuration files. @geoffjentry please assign me to this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4696:516,configurat,configuration,516,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4696,2,['configurat'],['configuration']
Deployability,Per discussion with PO merging this as is and possibly remove altogether for the release after next.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3985#issuecomment-411089609:81,release,release,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3985#issuecomment-411089609,1,['release'],['release']
Deployability,Per standup 2020-11-05 we're not going to bother with the 53 hotfix version of #6007 since 54 is imminent-ish.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6006#issuecomment-722575506:61,hotfix,hotfix,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6006#issuecomment-722575506,1,['hotfix'],['hotfix']
Deployability,"Per standup this morning we've decided to close this. The requested functionality is problematic on a few levels:. JSON runtime attributes are currently interpreted as WdlExpressions after going through coercion, which presently is a backend-dependent process as only the backend knows the accepted runtime attribute coercions. While currently it might appear safe to treat the default runtime attribute JSON values as WDL with a `WdlExpression.fromString`, that's an assumption of WDL / JSON expression equivalence not made elsewhere in Cromwell and it seems questionable to pioneer that here. Currently Cromwell's backend assignments happen at initialization time, but in the future Cromwell's backend dispatch is likely to become more sophisticated and dynamic. It therefore wouldn't be possible to say at workflow initialization time the backend to which a call was fated, which would mean the default runtime attribute handling would need to happen in the various `FooRuntimeAttributes` classes as it does now. The refactor proposed here would actually remove the default runtime attribute handling at call execution time and therefore make dynamic dispatch more difficult to add in the future. Finally, while it appears technically possible to doctor tasks with workflow option-derived default runtime attributes in `MaterializeWorkflowDescriptorActor`, this makes for some pretty hacky code. I discovered at least 3 spots that needed to be updated:; - backendAssignment values; - NamespaceWithWorkflow -> tasks; - NamespaceWithWorkflow -> workflow -> children -> tasks. That last item was particularly hideous since `Workflow#children` is explicitly write-once. I got around this with subclassing, but I felt bad about myself afterward.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247:1447,update,updated,1447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247,1,['update'],['updated']
Deployability,"Per the direction of the conversation on your integration testing doc, it sounds like there's a sentiment to disable integration (and maybe Docker) tests by default. . The wheel has chosen @scottfrazer but I suspect @geoffjentry may have some opinions here as well. :smile:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169450477:46,integrat,integration,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169450477,2,['integrat'],['integration']
Deployability,Perf CRON update,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4329:10,update,update,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4329,1,['update'],['update']
Deployability,"Perfect, thanks for the update",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-285912645:24,update,update,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-285912645,1,['update'],['update']
Deployability,Permission problem with LSF configuration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3185:28,configurat,configuration,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3185,1,['configurat'],['configuration']
Deployability,Persistence updates rebased on logging improvements.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/67:12,update,updates,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/67,1,['update'],['updates']
Deployability,Pic updated. FYI merging if/when tests pass.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5279#issuecomment-556439136:4,update,updated,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5279#issuecomment-556439136,1,['update'],['updated']
Deployability,Pin centaur to a specific version 0.21 hotfix.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1659:39,hotfix,hotfix,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1659,2,['hotfix'],['hotfix']
Deployability,Pin centaur to a specific version 0.22 hotfix.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1658:39,hotfix,hotfix,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1658,1,['hotfix'],['hotfix']
Deployability,"Pipeline developers like me would benefit from being able to parse the wdl language model so I can write tests that aren't covered by the current wdltool validate. . I hear this could be easily accomplished by wdl4s dumping to json, which users could then parse with custom tools. . Desired tests:; 1. Specified inputs exist and are accessible (depends on backend); 2. Docker images exist and can be read/pulled; 3. dependencies.zip contains all of the necessary files to run the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3502:0,Pipeline,Pipeline,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3502,1,['Pipeline'],['Pipeline']
Deployability,Pipelines API Version split,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3513:0,Pipeline,Pipelines,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3513,1,['Pipeline'],['Pipelines']
Deployability,Pipelines API version 2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3583:0,Pipeline,Pipelines,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3583,1,['Pipeline'],['Pipelines']
Deployability,"PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:3813,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,3813,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,PipelinesApiRequestWorker crash,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917:0,Pipeline,PipelinesApiRequestWorker,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,Please also update brew formula to 53.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5834:12,update,update,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5834,1,['update'],['update']
Deployability,"Please check this issue.; When I configure the aws, I followed up this page. (https://docs.opendata.aws/genomics-workflows/; ); I did not use the all-in-one template. With 3 step configure, I setup the cromwell server (Custom AMI -> VPC..and etc -> cromwell server instance). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; ```; task echoHello{; command {; echo ""Hello AWS!""; }; runtime {; docker: ""ubuntu:latest""; }. }. workflow printHelloAndGoodbye {; call echoHello; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell-server.log](https://github.com/broadinstitute/cromwell/files/2897001/cromwell-server.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4677:542,configurat,configuration,542,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4677,1,['configurat'],['configuration']
Deployability,Please consider the description of data flow and interpretation in this doc to be an equal part of this PR: https://docs.google.com/a/broadinstitute.com/document/d/1xXfnMCJMoBUUaU-l81Ljqr_yt1ApLmDOGkt6AA-QTT4/edit?usp=sharing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/793:132,a/b,a/broadinstitute,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/793,1,['a/b'],['a/broadinstitute']
Deployability,Please don't re-release files under same version,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2480:16,release,release,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2480,1,['release'],['release']
Deployability,Please investigate and see what the root cause is. Then let's decide if would still be a problem on 0.20+ and if we need to hotfix it in 0.19,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-229635762:124,hotfix,hotfix,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-229635762,1,['hotfix'],['hotfix']
Deployability,Please mention the change (and the update to metadata format) in the CHANGELOG.MD,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/412#issuecomment-181466777:35,update,update,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/412#issuecomment-181466777,1,['update'],['update']
Deployability,"Please upgrade Cromwell Pipelines API Backend to version 2. This will be necessary for our portal to launch workflows against GCS ""requester pays"" buckets with appropriate billing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3449:7,upgrade,upgrade,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3449,2,"['Pipeline', 'upgrade']","['Pipelines', 'upgrade']"
Deployability,Poke at the FC PAPI v1 => v2 Upgrade,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4459:29,Upgrade,Upgrade,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4459,1,['Upgrade'],['Upgrade']
Deployability,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4909:10,hotfix,hotfix,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909,1,['hotfix'],['hotfix']
Deployability,Preempted to backendStatus in existing metadata. 28 hotfix edition,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2444:52,hotfix,hotfix,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2444,1,['hotfix'],['hotfix']
Deployability,"Pretty sure this was fixed by @delocalizer way back in #4109. However during my debugging of `globbingBehavior` for #4854, it seemed something was rotten in the state of `GenomicsHighPriorityQue-c1ed17c72de5fcb`. I still don't 100% know the setup for the AWS queues, but I think a) perhaps we just never updated ecs-proxy over in quay?, and/or b) maybe the ARN ""fixes"" in #4896/#4902 pulled in Conrad's fixes?. Either way #4958 stops excluding the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4855#issuecomment-491122782:304,update,updated,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4855#issuecomment-491122782,1,['update'],['updated']
Deployability,Prototype to inform conversation on WDL directories in OpenWDL (https://github.com/openwdl/wdl/pull/241). TODO:; - [x] Add a test to ensure I didn't accidentally add this to WDL 1.0 (where `Directory` would be treated like a `struct` name). Limits:; - Pipelines API v1 cannot handle sub-directories.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3980:252,Pipeline,Pipelines,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3980,1,['Pipeline'],['Pipelines']
Deployability,Provide automated integration testing for cromwell `run` mode,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2411:18,integrat,integration,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2411,1,['integrat'],['integration']
Deployability,Provide better documentation/examples for logging configuration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1622:50,configurat,configuration,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1622,1,['configurat'],['configuration']
Deployability,Publish `SNAPSHOT`s obfuscated as `SNAP` releases.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1688:41,release,releases,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1688,1,['release'],['releases']
Deployability,Publish hotfix branches to docker for 0.21.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1681:8,hotfix,hotfix,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1681,1,['hotfix'],['hotfix']
Deployability,Publish hotfix branches to docker for 0.22.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1680:8,hotfix,hotfix,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1680,1,['hotfix'],['hotfix']
Deployability,Publish hotfix branches to docker.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1668:8,hotfix,hotfix,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1668,1,['hotfix'],['hotfix']
Deployability,Pull approve v2 featuring GH review integration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2477:36,integrat,integration,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2477,1,['integrat'],['integration']
Deployability,"Pull request for https://github.com/broadinstitute/cromwell/issues/4982 issue.; Changes:; - Implemented file transferring via TransferManager; - Removed assertions that caused `copying directories is not yet supported` exception; - Fixed incorrect work of `use_relative_output_paths` option; - Fixed logs and output files copying (tested manually on following backends: local, AWS, GCP).; - Added support for `AWS` filesystem in centaur's `fileSystemCheck` ; - Refactored `CheckFiles`; - Added an integration tests checking that workflow execution results and logs are correctly copied",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110:497,integrat,integration,497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110,1,['integrat'],['integration']
Deployability,"Question; -----------------; Is there any plan on speeding up `java -jar $JAR run`? Pipeline developer would definitely benefits from a faster startup. Symptom; -----------------; ```bash; time java -jar cromwell-49.jar run sub-flow.wdl -i input.json; ```. ```; real	1m12.833s; user	1m12.148s; sys	0m7.644s; ```. ```; $ cat input.json ; {; ""hello_and_goodbye.hello_and_goodbye_input"":""test1""; }; ```. Detail; ----------; backend: local. File: `sub-flow.wdl`; ```wdl; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5451:84,Pipeline,Pipeline,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451,1,['Pipeline'],['Pipeline']
Deployability,Quick update. I tweaked the config to be:; ```; system {; job-rate-control {; jobs = 1; per = 2 second; }; }; ```. and ran the test workflow above. I saw maximum concurrency - i.e. Batch requested the full number of vCPUs set in my compute environment (100). About 500 jobs succeeded before Cromwell threw an OOM exception. No Batch API Request Limit exceptions were encountered.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443927567:6,update,update,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443927567,1,['update'],['update']
Deployability,"Quite often I get ; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Docker image quay.io/biocontainers/star:2.5.3a--0@sha256:352f627075e436016ea2c38733b5c0096bb841e2fadcbbd3d4ae8daf03ccdf1b has an invalid syntax.""; }; ],; ```; Here is example of the task that caused this error. What is interested is that cromwell even did not start this task, so I suspect there is something wrong with docker parsing (I have this issue for this container both in develop and latest release of cromwell). ```; task star {. Int numberOfThreads = 8; File file; File genomeDir. command {; STAR --runThreadN ${numberOfThreads} --genomeDir ${genomeDir} --readFilesCommand gunzip -c --readFilesIn ${file}; }. runtime {; docker: ""quay.io/biocontainers/star:2.5.3a--0@sha256:352f627075e436016ea2c38733b5c0096bb841e2fadcbbd3d4ae8daf03ccdf1b""; }. output {; String result = ""STAR WORKS!""; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2254:472,release,release,472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2254,1,['release'],['release']
Deployability,"Quoth the CHANGELOG.md:. >You can now limit the number of concurrent jobs for a backend by specifying the following option in the backend's config stanza:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. But this is not generally true. The implementation in `BackendLifecycleActorFactory` defaults to an infinitely large job limit and does not look for this `concurrent-job-limit` configuration setting. `ConfigBackendLifecycleActorFactory` overrides this behavior to be consistent with CHANGELOG.md, but no other backend factory does.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1751:444,configurat,configuration,444,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751,1,['configurat'],['configuration']
Deployability,RC branch for 0.17 is here https://github.com/broadinstitute/cromwell/tree/release-0.17,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/405:75,release,release-,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/405,1,['release'],['release-']
Deployability,"RE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseExceptio",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606:1835,update,updateSchema,1835,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606,1,['update'],['updateSchema']
Deployability,README.md should be updated,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219088738:20,update,updated,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219088738,1,['update'],['updated']
Deployability,RROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.ama,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:1714,pipeline,pipeline,1714,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,1,['pipeline'],['pipeline']
Deployability,"RY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.sc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:35449,Update,UpdateVisitor,35449,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,1,['Update'],['UpdateVisitor']
Deployability,"Ran into this while working on #4119 . Before:; ```; Workflow input processing failed:; running cwltool on file /var/folders/g_/4k204pzx6sg6gcnqc2bqfykxx8k880/T/e0ae0c88-17c0-476b-9b74-bc5562b22ea3.temp.4377309718155227727/e0ae0c88-17c0-476b-9b74-bc5562b22ea3.cwl failed with null; ```; After:; ```; Workflow input processing failed:; running cwltool on file /var/folders/g_/4k204pzx6sg6gcnqc2bqfykxx8k880/T/c8eb79b9-eee9-4796-b374-4841ff18a27d.temp.3969925185320342713/c8eb79b9-eee9-4796-b374-4841ff18a27d.cwl failed with Traceback (most recent call last):; File ""/Users/anichols/Library/Caches/Coursier/v1/https/broadinstitute.jfrog.io/broadinstitute/libs-release/org/broadinstitute/heterodon/1.0.0-beta1/heterodon-1.0.0-beta1-single.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 12, in cwltool_salad; File ""/Users/anichols/Library/Caches/Coursier/v1/https/broadinstitute.jfrog.io/broadinstitute/libs-release/org/broadinstitute/heterodon/1.0.0-beta1/heterodon-1.0.0-beta1-single.jar/Lib/cwltool/load_tool.py"", line 279, in validate_document; File ""/Users/anichols/Library/Caches/Coursier/v1/https/broadinstitute.jfrog.io/broadinstitute/libs-release/org/broadinstitute/heterodon/1.0.0-beta1/heterodon-1.0.0-beta1-single.jar/Lib/schema_salad/ref_resolver.py"", line 915, in resolve_all; File ""/Users/anichols/Library/Caches/Coursier/v1/https/broadinstitute.jfrog.io/broadinstitute/libs-release/org/broadinstitute/heterodon/1.0.0-beta1/heterodon-1.0.0-beta1-single.jar/Lib/schema_salad/ref_resolver.py"", line 1087, in validate_links; schema_salad.validate.ValidationException: ../../../../../var/folders/g_/4k204pzx6sg6gcnqc2bqfykxx8k880/T/c8eb79b9-eee9-4796-b374-4841ff18a27d.temp.3969925185320342713/c8eb79b9-eee9-4796-b374-4841ff18a27d.cwl:11:1: checking field `steps`; ../../../../../var/folders/g_/4k204pzx6sg6gcnqc2bqfykxx8k880/T/c8eb79b9-eee9-4796-b374-4841ff18a27d.temp.3969925185320342713/c8eb79b9-eee9-4796-b374-4841ff18a27d.cwl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4190:658,release,release,658,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4190,2,['release'],['release']
Deployability,"Ran it just now, there were updates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-453140060:28,update,updates,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-453140060,1,['update'],['updates']
Deployability,"Rather than having very long ""Worker released"" events which don't mean very much, re-introduce ""Cromwell Polling Interval"" to fill the gap between ""GCS is done"" and ""Cromwell notices it"". ![Screen Shot 2019-10-09 at 5 02 36 PM](https://user-images.githubusercontent.com/13006282/66520194-b6015980-eab6-11e9-887c-7ee27f288d3b.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5220:37,release,released,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5220,1,['release'],['released']
Deployability,"Re ""centaurable""-- I'm confident by updating the local backend to use `${script}` that centaur is actually exercising a third of the variables updated in patch. We could also change centaur to completely run on a new backend that also uses the `${out}` and `${err}` paths, as outlined in issue #1126. But because of that issue, this new centaur test would require a separate, new backend definition in `local_centaur.conf`. FYI: I still don't have a full fix for #1126, but this is a step in that direction. When I looked at the reference.conf, I noticed the local backend wasn't using `${script}`, wanted to know why, and discovered this small issue. This patch will also make the workaround in that ticket work as expected, instead of pointing to paths outside the docker container like `/Users/kshakir/<path>` now pointing paths inside like `/cromwell_root/<path>`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954:143,update,updated,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954,3,"['patch', 'update']","['patch', 'updated']"
Deployability,"Re:. > The capoeira tests complete successfully but get unexpected cache hits. Caching is also tweaked in CI configs. For example:. https://github.com/broadinstitute/cromwell/blob/279909b1f35c8305dcfc23ac8534dcb00ce09771/src/ci/resources/local_provider_config.inc.conf#L6. Have you already tried the tests locally with the CI configs? For unicromtal, one can run the existing CI scripts with a bit of bootstrap:; - Setup vault; - Setup mysql locally (I'm using `brew install mysql`); - [Initialize a `travis` mysql user with granted permissions](https://dev.mysql.com/doc/refman/8.0/en/adding-users.html); - [Using the `travis` user create a `cromwell_test` schema](https://github.com/broadinstitute/cromwell/blob/279909b1f35c8305dcfc23ac8534dcb00ce09771/core/src/test/resources/application.conf#L24). From the cromwell source directory, with all of the above setup, one can try to run `src/ci/resources/testCentaurLocal.sh` and it will render the configs with vault and run the tests, including the restart tests that bring down/up cromwell. Also, if one just wants to ever use the CI configs with cromwell in IntelliJ, `sbt renderCiResources` will render configs into the folder `target/ci/resources`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472915580:467,install,install,467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472915580,1,['install'],['install']
Deployability,"Re:. > what of this?. Where I think this was [this](https://github.com/broadinstitute/cromwell/blob/develop/project/Settings.scala#L50) (same line but in 59). Via the scala compiler options linked below:. ```; -target:TARGET or --target:TARGET; Target platform for object files. ([8],9,10,11,12). Default: 8; ```. I'm guessing `1.8` worked for backwards compatibility just like the `1.x` synonyms mentioned in the `javac` docs later below. For consistency like the original rawls PR I removed the explicit `--target` setting. But according to the `scalac` docs that just means Rawls, Cromwell, etc. are still just emitting Java 8 bytecode from `.scala` files. Rawls also explicitly sets the `javac` options. Cromwell doesn't. The latest version of this PR did not make the `javac` options explicitly consistent between the two projects. Instead, Cromwell is consistent in that it does NOT specify the target bytecode for `scalac` nor `javac`. I did review the SBT docs, plus the java docs for the last community LTS (11) and the current (16) `javac`. Unlike `scalac` using `8`, from my understanding of the docs, not specifying `--source` / `--target` / `--release` implicitly means use ""the current Java SE release"". So Rawls, Cromwell, and others should be emitting Java 11 bytecode from `.java` files. - https://github.com/broadinstitute/rawls/pull/1372/files#diff-b0608ed4fcebc8b5aa969f0c92dc9809e860d963b04e73affd55bb51e4fd10a1L18-L27; - https://docs.scala-lang.org/overviews/compiler-options/index.html; - https://www.scala-sbt.org/1.x/docs/Java-Sources.html; - https://docs.oracle.com/en/java/javase/16/docs/specs/man/javac.html; - https://docs.oracle.com/en/java/javase/11/tools/javac.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6194#issuecomment-808338591:1157,release,release,1157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6194#issuecomment-808338591,2,['release'],['release']
Deployability,Reached out to Pipelines API for a potential retry on their end.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742#issuecomment-395994748:15,Pipeline,Pipelines,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742#issuecomment-395994748,1,['Pipeline'],['Pipelines']
Deployability,Readme update [no JIRA],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6130:7,update,update,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6130,1,['update'],['update']
Deployability,"Ready for review, satisfies A/C of:. 1. Process subworkflows separately from their parents (`IncludeSubworkflows.name -> ""true""`); 2. Start at the very oldest workflows (`NewestFirst.name -> ""false""`); 3. Allow for a “not before” time in configuration (`archiveDelay`, `deleteDelay`). ( The last A/C involving the config seems to have already been addressed on `dev` of `firecloud-develop` )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6230#issuecomment-807456524:238,configurat,configuration,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6230#issuecomment-807456524,1,['configurat'],['configuration']
Deployability,"Recently for an operation task that updating labels for ~2500 workflows, we have to write a loop to end ~2500 PATCH /label requests to the Cromwell, which took more than 3 hrs. (In Cromwell IAM, this is even worse since a single token will expire in 60mins, so you have to also deal with the token refreshment) . We tried to use multi-threading to speed it up but ended up getting transient 500 errors when using a thread pool with a size of >=4 threads. . So having this batch feature will make a lot of things much easier!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3755#issuecomment-451759631:110,PATCH,PATCH,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3755#issuecomment-451759631,1,['PATCH'],['PATCH']
Deployability,Recover from database failures when reading the workflow store. Hotfix edition,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2149:64,Hotfix,Hotfix,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2149,1,['Hotfix'],['Hotfix']
Deployability,"Red thumb required because I updated the `WomIdentifier` to have a ""workflow-local"" name (ie the identifier minus the workflow name. I'm not sure how CWL and WDL draft 2 did this, and the whole `WomIdentifier` thing seems inconsistently applied... I'm open to better ideas but it seems to work for now.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3491:29,update,updated,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3491,1,['update'],['updated']
Deployability,Redo of #4450 but Including interaction with `scatter` and `if` blocks. - [ ] Requires concurrent merging of:; - https://github.com/openwdl/wdl/pull/275 (containing the grammar update); - https://github.com/openwdl/wdl/pull/162 (the spec text change),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4483:177,update,update,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4483,1,['update'],['update']
Deployability,Reduce number of ExecutionStore updates generated by large scatters [BA-6612 trial],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5881:32,update,updates,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5881,1,['update'],['updates']
Deployability,Refactor publish/release graphs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4962:17,release,release,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4962,1,['release'],['release']
Deployability,Refactoring of Jes configuration validation,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/201:19,configurat,configuration,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/201,1,['configurat'],['configuration']
Deployability,Refactoring of Jes configuration validation V2 (Rebranched from develop),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/208:19,configurat,configuration,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/208,1,['configurat'],['configuration']
Deployability,Reference conda and docker as installation methods.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5423:30,install,installation,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5423,1,['install'],['installation']
Deployability,"Refinement update:. We are going to solve the first two points by adding a sort of `input_errors` map with input names as keys and error(s) as values. The absence of `errors` and presence of `input_errors` indicates the DA case where WDL is good and inputs bad. ~The last issue will split off into soliciting community feedback from non-workbench users and writing a nicer wrapper endpoint that's more compatible with curl (it is Adam's opinion, potentailly poorly supported, that this is hard right now)~ see https://github.com/broadinstitute/cromwell/issues/4892",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685:11,update,update,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685,2,['update'],['update']
Deployability,Relative import doc update,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4517:20,update,update,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4517,1,['update'],['update']
Deployability,Release .jar has same name for 0.19 and 0.19.3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1103:0,Release,Release,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1103,1,['Release'],['Release']
Deployability,Release 23,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1735:0,Release,Release,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1735,1,['Release'],['Release']
Deployability,Release 41,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4843:0,Release,Release,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4843,1,['Release'],['Release']
Deployability,Release 53: womtool.jar does not work!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5808:0,Release,Release,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5808,1,['Release'],['Release']
Deployability,Release WDL updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2986:0,Release,Release,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2986,4,"['Release', 'update']","['Release', 'updates']"
Deployability,"Release WDL uses a combination of GitHub token and ""environmental"" auth.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4870:0,Release,Release,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4870,1,['Release'],['Release']
Deployability,"Release WDL, Feedback requested",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1780:0,Release,Release,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1780,1,['Release'],['Release']
Deployability,Release doc updates [BT-396],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6502:0,Release,Release,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6502,2,"['Release', 'update']","['Release', 'updates']"
Deployability,Release docs fixups [BT-367],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6478:0,Release,Release,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6478,1,['Release'],['Release']
Deployability,Release notes--please review :),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1477:0,Release,Release,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1477,1,['Release'],['Release']
Deployability,Release process adjustment [BT-459],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6585:0,Release,Release,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6585,1,['Release'],['Release']
Deployability,Release process fixes [BT-501],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6644:0,Release,Release,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6644,1,['Release'],['Release']
Deployability,"Release script does not bump swagger's ""Cromwell version""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3820:0,Release,Release,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3820,1,['Release'],['Release']
Deployability,Releasing lenthall; This should be done if Lenthall has changed since the last release. Releasing wdl4s; This should be done if wdl4s has changed since the last release. Releasing wdltool; This should be done if either lenthall or wdl4s has changed since the last release. Further details:https://broadinstitute.atlassian.net/wiki/display/DSDEEPB/Cromwell+Release,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/608:79,release,release,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/608,4,"['Release', 'release']","['Release', 'release']"
Deployability,"Reminder: could you also make a non-hotfix version of this PR (leaving out the metrics change for now, since that's going to be covered separately/properly in https://broadworkbench.atlassian.net/browse/BA-6307)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5446#issuecomment-597169266:36,hotfix,hotfix,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5446#issuecomment-597169266,1,['hotfix'],['hotfix']
Deployability,Remove DRS uuids that fail to resolve [hotfix edition] [BA-6022],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5191:39,hotfix,hotfix,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5191,1,['hotfix'],['hotfix']
Deployability,Remove Deploy Key,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7084:7,Deploy,Deploy,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7084,1,['Deploy'],['Deploy']
Deployability,Remove Pipeline and use ephemeral Pipelines in JES. Closes #856,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/864:7,Pipeline,Pipeline,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/864,2,['Pipeline'],"['Pipeline', 'Pipelines']"
Deployability,Remove deprecated perf test from release docs [CROM-6795],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6488:33,release,release,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6488,1,['release'],['release']
Deployability,"Remove link to Building (which was in any case broken), and replace it earlier in the page with a link to the releases page. Most users will want to download the latest release of Cromwell rather than build it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6984:110,release,releases,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6984,2,['release'],"['release', 'releases']"
Deployability,Removed GCS configuration from HtCondor. Closes #1159.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1160:12,configurat,configuration,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1160,1,['configurat'],['configuration']
Deployability,Removed dead code related to configs such as the old `backend.backend`.; Updated unencrypted configuration files.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2165:73,Update,Updated,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2165,2,"['Update', 'configurat']","['Updated', 'configuration']"
Deployability,"Removed deadlock workaround.; Minimal updates to fix depracated usage.; Old Slick 3.1 style names are still used in scala, ex: `val driver: JdbcProfile`.; Added notes about SQL converters handling empty LOBs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2104:38,update,updates,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2104,1,['update'],['updates']
Deployability,Removing semicolon from migration query (hotfix edition),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2230:41,hotfix,hotfix,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2230,1,['hotfix'],['hotfix']
Deployability,Rename (at least for user visible aspects) the JES backend. As time goes on fewer people are going to be able to make the connection between JES and the google genomics pipelines api and this is going to lead to confusion,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2019:169,pipeline,pipelines,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019,1,['pipeline'],['pipelines']
Deployability,"Reopened PR because last build failed with strange error and after triggering re-build on travis everything was OK, but here status wasn't updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-520349943:139,update,updated,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-520349943,1,['update'],['updated']
Deployability,Replace Building link with link to releases page.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6984:35,release,releases,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6984,1,['release'],['releases']
Deployability,"Replaces #5075. For best results, review in conjunction with:. - Updated perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8) ; - NB: check whether this has been updated yet in the TODOs list below); - Refactored Jenkins scripts (see [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8)). TODOs:; - [x] Update perf [documentation](https://docs.google.com/document/d/1cv338uMqTNVVYVEC78k4iMTG_zyZloon3v9e4l3TdM8). TODOs (but not in this ticket):; - [ ] Come up with a test case to exercise horicromtal; - [ ] Add a load balancer in front of multiple readers? (for the api tests)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5077:65,Update,Updated,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5077,3,"['Update', 'update']","['Update', 'Updated', 'updated']"
Deployability,Request: hombrew formula installing womtool,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3254:25,install,installing,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3254,1,['install'],['installing']
Deployability,"Requester pays is now added to Cromwell. The user can set the billing project id using one of the methods listed below:. - It can be added as `'google_project':'project-id'` as part of workflow options during workflow submission; - It can be included inside configuration file as shown in Getting started on Google Pipelines API where you need to replace the <google-billing-project-id> with the project id; - If it is not mentioned using above 2 ways, Cromwell will use the default project that has been configured with gcloud. More information about Requester pays can be found [here](https://cloud.google.com/storage/docs/requester-pays)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3638:258,configurat,configuration,258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3638,4,"['Pipeline', 'configurat']","['Pipelines', 'configuration']"
Deployability,Resolved by installing Java with a more mainstream and supported github action.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6990#issuecomment-1405500533:12,install,installing,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6990#issuecomment-1405500533,1,['install'],['installing']
Deployability,Results from running this again with proper configuration: 75m down to 25. The entire run on the correct configuration appears to line up with the initial plateau from the `develop` results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441:44,configurat,configuration,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441,2,['configurat'],['configuration']
Deployability,RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.cl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:5484,pipeline,pipeline,5484,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,3,['pipeline'],['pipeline']
Deployability,RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:73); 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:3039,pipeline,pipeline,3039,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,Return 404 code for unrecognized workflow id for PATCH labels point,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2961:49,PATCH,PATCH,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2961,1,['PATCH'],['PATCH']
Deployability,Return all labels when using the PATCH labels endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2953:33,PATCH,PATCH,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2953,1,['PATCH'],['PATCH']
Deployability,Return all of a workflow's labels when updated via PATCH. Closes #2953,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3498:39,update,updated,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3498,2,"['PATCH', 'update']","['PATCH', 'updated']"
Deployability,Reuse ogins between if condition and the inner block [30 hotfix edition],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3194:57,hotfix,hotfix,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3194,1,['hotfix'],['hotfix']
Deployability,Revert akka version: 53 hotfix edition [WA-389],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5948:24,hotfix,hotfix,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5948,1,['hotfix'],['hotfix']
Deployability,Revert auto-commit metadata inserts [36 hotfix],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4656:40,hotfix,hotfix,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4656,1,['hotfix'],['hotfix']
Deployability,Revert auto-commit metadata inserts [37 hotfix],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4655:40,hotfix,hotfix,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4655,1,['hotfix'],['hotfix']
Deployability,Revert upgrade mysql connector (we have a better theory now),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4701:7,upgrade,upgrade,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4701,1,['upgrade'],['upgrade']
Deployability,Revert: Temporary patch: Use an encrypted variable for the vault token.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4585:18,patch,patch,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4585,1,['patch'],['patch']
Deployability,Reverts broadinstitute/cromwell#7123. Test to see if this fixes issues with our build+release process.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7146:86,release,release,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7146,1,['release'],['release']
Deployability,"Reverts broadinstitute/cromwell#7180. It's failing the github action, so the update clearly isn't working. Need to take a look and see what I missed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7181:77,update,update,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7181,1,['update'],['update']
Deployability,"Reverts broadinstitute/cromwell#7342. We found that our current version of Akka, [2.5.32](https://github.com/broadinstitute/cromwell/blob/develop/project/Dependencies.scala#L6), [does not support Java 17](https://akka.io/blog/news/2022/10/26/akka-22.10-released). We need at least 2.7.0.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7346:253,release,released,253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7346,1,['release'],['released']
Deployability,"Review complete. 👍 after one new ""PBE"" comment in the code, and a string interpolation patch. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/893/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-223408968:87,patch,patch,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-223408968,1,['patch'],['patch']
Deployability,"Review the documentation for 0.21 and make sure it is still accurate, and provide upgrade instructions for users coming from 0.19",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1120:82,upgrade,upgrade,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1120,1,['upgrade'],['upgrade']
Deployability,Review/Update README/Swagger,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1043:7,Update,Update,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1043,1,['Update'],['Update']
Deployability,"Right I just meant that in the tests the ratio (DB access / executedCode) may be higher compared to ""normal execution"" where we spend a lot of time waiting for calls to end. But yes production will definitely not be an easier environment than tests :); I kinda like the DataAccess actor option, although I think slick already manages its own pool of threads and everything, so maybe just by tweaking some configuration we could improve performance before going full Super Saiyan Actor Scaling mode.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143037444:405,configurat,configuration,405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143037444,1,['configurat'],['configuration']
Deployability,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:43,pipeline,pipeline,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719,2,['pipeline'],['pipeline']
Deployability,"Right, what I was suggesting is that maybe this is ""fixed"" already. Or at least we could wait and see if it still happens once a Cromwell with your change gets deployed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-417859698:160,deploy,deployed,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-417859698,1,['deploy'],['deployed']
Deployability,"Right. So it sounds like this is an issue with disk space on the PAPI worker. Since I'm using the official Broad pipelines, the way to configure this is with the `flowcell_medium_disk` input field to the `PreProcessingForVariantDiscovery_GATK4` workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337#issuecomment-434525496:113,pipeline,pipelines,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337#issuecomment-434525496,1,['pipeline'],['pipelines']
Deployability,Rollback underscores [24_hotfix],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1902:0,Rollback,Rollback,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1902,1,['Rollback'],['Rollback']
Deployability,Rollback workbench-util to 0.6-65bba14,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6481:0,Rollback,Rollback,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6481,1,['Rollback'],['Rollback']
Deployability,"Running a CWL workflow with local relative imports seems to fail. This is possibly related to #4308 but unsure. . Running with cromwell 36. . Process looks like the following; ```; $ git clone https://github.com/dockstore-testing/dockstore-workflow-md5sum-unified.git; $ cd dockstore-workflow-md5sum-unified; $ cwltool checker_workflow_wrapping_workflow.cwl md5sum.json; /usr/local/bin/cwltool 1.0.20180403145700; <snip>; Final process status is success; $ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; $ java -jar cromwell-36.jar run https://raw.githubusercontent.com/dockstore-testing/dockstore-workflow-md5sum-unified/develop/checker_workflow_wrapping_workflow.cwl --inputs md5sum.json; <snip>; [2018-11-07 14:34:25,13] [info] Pre-Processing /tmp/cwl_temp_dir_7264114231127246601/cwl_temp_file_a3cb6a14-3672-4132-8a24-2e0a4e66ff96.cwl; [2018-11-07 14:34:34,94] [error] WorkflowManagerActor Workflow a3cb6a14-3672-4132-8a24-2e0a4e66ff96 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; running cwltool on file /tmp/cwl_temp_dir_7264114231127246601/cwl_temp_file_a3cb6a14-3672-4132-8a24-2e0a4e66ff96.cwl failed with Traceback (most recent call last):; File ""/home/dyuen/dockstore_tools/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 12, in cwltool_salad; File ""/home/dyuen/dockstore_tools/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/cwltool/load_tool.py"", line 279, in validate_document; File ""/home/dyuen/dockstore_tools/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 915, in resolve_all; File ""/home/dyuen/dockstore_tools/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 1087, in validate_links; schema_salad.validate.V",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366:505,release,releases,505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366,1,['release'],['releases']
Deployability,"Running into this problem again today. Having true modularity would be nice. So I can make a workflow, that imports a workflow, that imports a workflow. Imports need to be evaluated at the file level to handle this. ; This could be set as a config option to not break backwards compatibility.; I have some experience in scala myself as most of the tools in our institute are programmed in scala as well. If given some pointers I could maybe help in implementing this?. Am I correct in thinking that imports are evaluated [here](https://github.com/broadinstitute/cromwell/blob/develop/wdl/model/draft2/src/main/scala/wdl/draft2/model/Import.scala)?; Default configuration is in [here](https://github.com/broadinstitute/cromwell/blob/2cd38db0eb818b07ad38463183fe7a8af4706899/core/src/main/resources/reference.conf) right? Can I just add a new key,value pair which I can call from `import.scala` or are some changes to a code file required?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-366954601:657,configurat,configuration,657,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-366954601,1,['configurat'],['configuration']
Deployability,"Running locally a scatter job with 2102 jobs causes an out-of-memory error after finishing job 1427 (reproduced twice) . The command that I run was using the wrapper script from **brew**:. ```; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Where the local configuration looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false; workflow-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:339,configurat,configuration,339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['configurat'],['configuration']
Deployability,"Running with release 24. As a minimal example, the following WDL:; ```; import ""foo.wdl"" as missing. workflow testMe {; 	call doNothing; }; task doNothing {; 	command {}; }; ```. and a zip file containing `baz.wdl` but not `foo.wdl`:; ```; [conradL@qimr13054 ~]$ unzip -l bar.zip ; Archive: bar.zip; Length Date Time Name; --------- ---------- ----- ----; 0 02-07-2017 13:47 bar/; 0 02-07-2017 13:47 bar/baz.wdl; --------- -------; 0 2 files; ```. submit to server:; ```; [conradL@qimr13054 ~]$ curl http://localhost:8000/api/workflows/V1 -FwdlSource=@badImport.wdl -FwdlDependencies=@bar.zip; {; ""id"": ""b701aafd-445c-4f49-8ba6-452d56e69fd3"",; ""status"": ""Submitted""; }; ```. causes the server process to die with this in the logs:; ```; 2017-02-07 13:52:41,842 cromwell-system-akka.actor.default-dispatcher-33 ERROR - guardian failed, shutting down system; wdl4s.exception.ValidationException: Failed to import workflow foo.wdl.:; File not found /tmp/640585481854205084.zip4511378926145376874/bar/foo.wdl; 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$tryResolve$1(WdlNamespace.scala:198); 	at wdl4s.WdlNamespace$$anonfun$17.apply(WdlNamespace.scala:208); 	at wdl4s.WdlNamespace$$anonfun$17.apply(WdlNamespace.scala:207); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.immutable.List.map(List.scala:285); 	at wdl4s.WdlNamespace$.apply(WdlNamespace.scala:207); 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$load(WdlNamespace.scala:177); 	at wdl4s.WdlNamespace$.loadUsingSource(WdlNamespace.scala:173); 	at wdl4s.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:542); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1958:13,release,release,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1958,1,['release'],['release']
Deployability,Runtime configuration parsing improvements.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/99:8,configurat,configuration,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/99,1,['configurat'],['configuration']
Deployability,S3fs can currently only be configured via environment variables - see https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/README.md. This should be wired through Cromwell configuration and the following TODO addressed: https://github.com/broadinstitute/cromwell/blob/aws_backend/filesystems/s3/src/main/scala/cromwell/filesystems/s3/S3PathBuilder.scala#L146,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3746:181,configurat,configuration,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3746,1,['configurat'],['configuration']
Deployability,SBT opts plus release wdl updates.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3720:14,release,release,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3720,2,"['release', 'update']","['release', 'updates']"
Deployability,SGE cluster configuration correction in documentation,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3100:12,configurat,configuration,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3100,1,['configurat'],['configuration']
Deployability,SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:1401,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1401,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"SM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:138); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I'm basically using the standard aws configuration file for Cromwell:. ```hocon; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://$bucketName/cromwell-execution""; auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes {; queueArn = ""arn:aws:batch:ap-southeast-2:$arn""; }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; ```. I've contacted AWS Support, to find out if I could fully (region) qualify the S3 locator (something like [these examples](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro): `s3://us-east-1.amazonaws.com/broad-references/.../file`. . AWS basically said no, and they",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4731:3543,configurat,configuration,3543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731,1,['configurat'],['configuration']
Deployability,"STAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 2019-01-31 20:30:56,617 INFO - changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Successfully released change log lock; 2019-01-31 20:30:56,631 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_I",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:1621,release,released,1621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['release'],['released']
Deployability,"S_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxx",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2116,update,updated,2116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['update'],['updated']
Deployability,"Same here.; This effects running picard intervallisttools with the scatter command for example. Might be os depent or cromwell version depent. affects cromwell 56 running with java 11 on CentOS Linux release 7.9.2009. lazy workaround is something like this:. ```bash; FILEINDEX=0; for FILE in ./scatter_list/*/*.interval_list; do; mv ""$FILE"" ""$(dirname $FILE)""""/""""$FILEINDEX""""_""""$(basename $FILE)""; FILEINDEX=$((FILEINDEX+1)); done; ```; ----. update also affects cromwell 79 running with java(openjdk) 11 on CentOS Linux release 7.9.2009",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6014#issuecomment-1814224479:200,release,release,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6014#issuecomment-1814224479,3,"['release', 'update']","['release', 'update']"
Deployability,"Sample of a possible new log message thread:; ```; [INFO] [...] [.../TestJesApiQueryManager-1262117937] Running with 1 PAPI request workers; [WARN] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller1 terminated. 0 run creation requests, 5 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; [INFO] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller1 has been removed and replaced by statusPoller2 in the pool of 1 workers; [WARN] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller2 terminated. 0 run creation requests, 5 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; [INFO] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller2 has been removed and replaced by statusPoller3 in the pool of 1 workers. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4671#issuecomment-479685147:472,pipeline,pipelines,472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4671#issuecomment-479685147,6,"['Pipeline', 'pipeline']","['PipelinesApiRequestHandler', 'PipelinesApiRequestManager', 'pipelines']"
Deployability,"Saw DSDEEPB-1549 was already filed. Minor time scaling issue for jenkins, then :+1: as far as I'm concerned. Btw, anyone have any idea what's up with travis-to-coveralls integration's SSL errors? We don't have coverage results anymore? :crying_cat_face:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/224#issuecomment-146276086:170,integrat,integration,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/224#issuecomment-146276086,1,['integrat'],['integration']
Deployability,"Sayeth @mcovarr: ; >The engine upgrade already has a horicromtal test, with some very recent bugfixes :wink:; so I think #4800 can be closed too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4800#issuecomment-484262549:31,upgrade,upgrade,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4800#issuecomment-484262549,1,['upgrade'],['upgrade']
Deployability,Scala 2.12 and sbt updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6328:19,update,updates,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6328,1,['update'],['updates']
Deployability,"Scala 2.13 upgrade per CROM-6036. The overwhelming majority of this is nitpicky noise, I added a few comments at sites where that is not the case. Sources of noise:. 1. `Traversable` and the whole `Traversable` family are gone.; 2. Stronger opinions about the presence of absence of parens in function invocations.; 3. `scala.collection.JavaConverters` moved to `scala.jdk.CollectionConverters`; 4. `MapView` introduced with some breaking changes to the usage of maps; 5. Conversion of `Array` to `IndexedSeq` now has to be explicit; 6. Instances of `case` matches being incomplete are now detected where previously they were not; 7. Right-biasing of `Either`s makes for a lot of annoying `toOption` / `swap` instead of `right` / `left`; 8. Type ascriptions required for some anonymous functions; 9. Explicit `.` now required for some method invocations; 10. `Stream` is no more, long live `LazyList`; 11. `Symbol` literal `'` syntax no longer supported; 12. etc etc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6724:11,upgrade,upgrade,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6724,1,['upgrade'],['upgrade']
Deployability,"Scala is very much not in my wheelhouse, so unfortunately this is not a task I'd be comfortable taking on. But I really do appreciate the status update",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-478681472:145,update,update,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-478681472,1,['update'],['update']
Deployability,"Scala steward updates [BW-930]. sbt-assembly, google-cloud-monitoring, guava, ficus, akka-http-circe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6571:14,update,updates,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6571,1,['update'],['updates']
Deployability,Scala steward updates batch 3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6913:14,update,updates,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6913,1,['update'],['updates']
Deployability,"Scala steward updates: awssdk, mouse, specs2-mock, sbt-scoverage [BW-932]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6573:14,update,updates,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6573,1,['update'],['updates']
Deployability,"Scala steward updates: scala-library, kind-projector, MockFtpServer, scalactic, swagger-parser [BW-937]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6576:14,update,updates,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6576,1,['update'],['updates']
Deployability,Scala-Steward: Update MockFtpServer from 2.7.1 to 2.8.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6271:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6271,1,['Update'],['Update']
Deployability,Scala-Steward: Update MockFtpServer from 2.8.0 to 3.0.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6528:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6528,1,['Update'],['Update']
Deployability,Scala-Steward: Update MockFtpServer from 3.0.0 to 3.1.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7071:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7071,1,['Update'],['Update']
Deployability,"Scala-Steward: Update akka-actor, akka-slf4j, akka-stream, ... from 2.5.31 to 2.6.8",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5755:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5755,1,['Update'],['Update']
Deployability,"Scala-Steward: Update akka-actor, akka-slf4j, akka-stream, ... from 2.6.8 to 2.6.9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5840:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5840,1,['Update'],['Update']
Deployability,"Scala-Steward: Update akka-actor, akka-slf4j, akka-stream, ... from 2.6.9 to 2.6.10",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5927:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5927,1,['Update'],['Update']
Deployability,"Scala-Steward: Update akka-http, akka-http-spray-json, ... from 10.1.12 to 10.2.0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5757:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5757,1,['Update'],['Update']
Deployability,"Scala-Steward: Update akka-http, akka-http-spray-json, ... from 10.1.12 to 10.2.1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5913:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5913,1,['Update'],['Update']
Deployability,"Scala-Steward: Update akka-http, akka-http-spray-json, ... from 10.1.15 to 10.2.9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6869:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6869,1,['Update'],['Update']
Deployability,Scala-Steward: Update akka-http-circe from 1.29.1 to 1.34.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5736:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5736,1,['Update'],['Update']
Deployability,Scala-Steward: Update akka-http-circe from 1.34.0 to 1.35.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5897:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5897,1,['Update'],['Update']
Deployability,Scala-Steward: Update akka-http-circe from 1.35.0 to 1.35.2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5975:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5975,1,['Update'],['Update']
Deployability,Scala-Steward: Update akka-http-circe from 1.35.2 to 1.35.3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6261:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6261,1,['Update'],['Update']
Deployability,Scala-Steward: Update akka-http-circe from 1.35.3 to 1.36.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6408:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6408,1,['Update'],['Update']
Deployability,Scala-Steward: Update akka-http-circe from 1.37.0 to 1.38.2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6523:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6523,1,['Update'],['Update']
Deployability,Scala-Steward: Update akka-http-circe from 1.38.2 to 1.39.2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6616:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6616,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-core from 4.3.9 to 4.5.7,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5773:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5773,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-core from 4.5.10 to 4.5.12,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5899:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5899,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-core from 4.5.10 to 4.5.13,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5936:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5936,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-core from 4.5.12 to 4.5.14,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5992:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5992,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-core from 4.5.12 to 4.5.15,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6032:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6032,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-core from 4.5.12 to 4.5.16,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6038:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6038,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-core from 4.5.12 to 4.5.17,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6100:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6100,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-core from 4.5.16 to 4.5.20,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6240:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6240,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-core from 4.5.20 to 4.5.22,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6386:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6386,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-core from 4.5.25 to 4.5.30,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6599:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6599,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-core from 4.5.7 to 4.5.10,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5854:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5854,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-core from 4.5.7 to 4.5.11,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5885:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5885,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-core from 4.5.7 to 4.5.8,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5816:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5816,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-core from 4.5.7 to 4.5.9,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5847:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5847,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-cr from 4.1.1 to 4.1.2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6121:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6121,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-java-sdk-cr from 4.1.2 to 4.1.3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6600:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6600,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-sdk-oss from 3.10.2 to 3.11.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5878:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5878,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-sdk-oss from 3.11.1 to 3.11.2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6122:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6122,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-sdk-oss from 3.11.1 to 3.11.3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6241:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6241,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-sdk-oss from 3.11.3 to 3.13.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6387:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6387,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-sdk-oss from 3.13.1 to 3.13.2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6601:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6601,1,['Update'],['Update']
Deployability,Scala-Steward: Update aliyun-sdk-oss from 3.4.2 to 3.11.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5774:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5774,1,['Update'],['Update']
Deployability,"Scala-Steward: Update alleycats-core, cats-core from 2.1.1 to 2.2.0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5829:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5829,1,['Update'],['Update']
Deployability,"Scala-Steward: Update alleycats-core, cats-core from 2.2.0 to 2.3.0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6098:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6098,1,['Update'],['Update']
Deployability,"Scala-Steward: Update alleycats-core, cats-core from 2.3.0 to 2.3.1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6278:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6278,1,['Update'],['Update']
Deployability,"Scala-Steward: Update alleycats-core, cats-core from 2.3.1 to 2.6.1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6425:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6425,1,['Update'],['Update']
Deployability,"Scala-Steward: Update alleycats-core, cats-core from 2.6.1 to 2.7.0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6632:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6632,1,['Update'],['Update']
Deployability,"Scala-Steward: Update alleycats-core, cats-core from 2.7.0 to 2.10.0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7320:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7320,1,['Update'],['Update']
Deployability,"Scala-Steward: Update alleycats-core, cats-core from 2.7.0 to 2.8.0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6897:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6897,1,['Update'],['Update']
Deployability,"Scala-Steward: Update alleycats-core, cats-core from 2.7.0 to 2.9.0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7077:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7077,1,['Update'],['Update']
Deployability,Scala-Steward: Update ammonite-ops from 1.6.9 to 1.9.9,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5733:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5733,1,['Update'],['Update']
Deployability,Scala-Steward: Update ammonite-ops from 1.9.9 to 2.2.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5787:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5787,1,['Update'],['Update']
Deployability,Scala-Steward: Update ammonite-ops from 2.2.0 to 2.3.8,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6096:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6096,1,['Update'],['Update']
Deployability,Scala-Steward: Update ammonite-ops from 2.3.8 to 2.4.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6403:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6403,1,['Update'],['Update']
Deployability,Scala-Steward: Update ammonite-ops from 2.4.0 to 2.4.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6613:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6613,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.10.91 to 2.14.2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5782:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5782,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.10.91 to 2.14.3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5789:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5789,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.10.91 to 2.14.4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5801:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5801,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.23 to 2.14.25",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5889:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5889,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.23 to 2.14.26",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5893:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5893,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.23 to 2.14.27",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5898:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5898,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.23 to 2.14.28",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5900:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5900,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.10",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5959:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5959,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.11",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5963:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5963,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5968:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5968,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.13",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5970:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5970,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.14",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5973:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5973,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5979:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5979,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.16",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5980:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5980,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.17",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5987:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5987,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.18",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6000:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6000,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.19",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6008:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6008,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6012:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6012,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6018:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6018,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.22",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6021:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6021,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.23",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6025:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6025,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.24",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6036:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6036,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.25",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6041:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6041,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.26",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6045:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6045,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.27",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6046:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6046,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.28",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6049:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6049,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.29",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6057:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6057,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.30",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6064:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6064,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.31",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6069:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6069,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.32",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6074:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6074,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.33",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6078:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6078,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.34",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6086:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6086,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.35",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6090:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6090,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.36",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6109:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6109,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.37",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6111:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6111,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.38",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6113:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6113,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.39",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6119:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6119,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.40",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6124:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6124,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.41",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6129:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6129,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.28 to 2.15.9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5955:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5955,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.10",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5827:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5827,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.11",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5831:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5831,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5832:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5832,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.13",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5836:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5836,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.14",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5843:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5843,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5846:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5846,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.16",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5849:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5849,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.17",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5852:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5852,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.18",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5856:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5856,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.19",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5860:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5860,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5865:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5865,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5871:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5871,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.22",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5876:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5876,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.23",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5880:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5880,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.24",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5883:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5883,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5806:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5806,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.6",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5811:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5811,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.7",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5814:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5814,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.8",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5820:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5820,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.14.3 to 2.14.9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5824:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5824,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.15.41 to 2.15.82",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6283:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6283,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.17.152 to 2.17.170",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6731:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6731,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.17.194 to 2.17.265",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6901:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6901,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.17.265 to 2.17.295",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7082:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7082,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.17.29 to 2.17.50",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6537:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6537,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:batch, ... from 2.17.50 to 2.17.102",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6637:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6637,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:cloudwatchlogs, awssdk:core, ... from 2.17.152 to 2.17.171",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6733:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6733,1,['Update'],['Update']
Deployability,Scala-Steward: Update awssdk:core from 2.17.152 to 2.17.181,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6751:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6751,1,['Update'],['Update']
Deployability,Scala-Steward: Update awssdk:core from 2.17.152 to 2.17.191,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6761:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6761,1,['Update'],['Update']
Deployability,"Scala-Steward: Update awssdk:core, awssdk:s3, awssdk:sts from 2.17.152 to 2.17.172",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6734:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6734,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-core from 1.40.0 to 1.45.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7261:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7261,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-core-http-okhttp from 1.11.10 to 1.11.17,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7262:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7262,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-core-management from 1.7.0 to 1.7.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6843:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6843,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-core-management from 1.7.1 to 1.10.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7027:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7027,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-core-management from 1.7.1 to 1.11.9,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7263:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7263,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-core-test from 1.18.0 to 1.18.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7264:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7264,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-identity from 1.4.2 to 1.4.6,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6844:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6844,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-identity from 1.4.6 to 1.8.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7028:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7028,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-identity from 1.9.1 to 1.9.2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7265:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7265,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-identity-extensions from 1.1.4 to 1.1.10,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7266:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7266,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-resourcemanager from 2.17.0 to 2.18.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6847:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6847,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-resourcemanager from 2.18.0 to 2.24.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7029:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7029,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-resourcemanager from 2.18.0 to 2.33.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7269:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7269,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-security-keyvault-secrets from 4.3.4 to 4.3.5,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6602:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6602,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-security-keyvault-secrets from 4.3.7 to 4.3.8,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6845:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6845,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-storage-blob from 12.23.0-beta.1 to 12.23.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7267:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7267,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-storage-blob-nio from 12.0.0-beta.18 to 12.0.0-beta.19,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6846:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6846,1,['Update'],['Update']
Deployability,Scala-Steward: Update azure-storage-common from 12.22.0-beta.1 to 12.22.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7268:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7268,1,['Update'],['Update']
Deployability,Scala-Steward: Update batch from 2.17.152 to 2.17.179,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6746:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6746,1,['Update'],['Update']
Deployability,Scala-Steward: Update batch from 2.17.152 to 2.17.189,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6759:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6759,1,['Update'],['Update']
Deployability,Scala-Steward: Update better-files from 3.9.1 to 3.9.2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7034:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7034,1,['Update'],['Update']
Deployability,Scala-Steward: Update cats-effect from 2.1.4 to 2.2.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5833:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5833,1,['Update'],['Update']
Deployability,Scala-Steward: Update cats-effect from 2.2.0 to 2.3.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6101:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6101,1,['Update'],['Update']
Deployability,Scala-Steward: Update cats-effect from 2.3.0 to 2.3.3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6279:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6279,1,['Update'],['Update']
Deployability,Scala-Steward: Update cats-effect from 2.3.3 to 2.5.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6426:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6426,1,['Update'],['Update']
Deployability,Scala-Steward: Update cglib-nodep from 3.2.7 to 3.2.12,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7259:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7259,1,['Update'],['Update']
Deployability,Scala-Steward: Update circe-config from 0.8.0 to 0.10.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7047:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7047,1,['Update'],['Update']
Deployability,Scala-Steward: Update circe-config from 0.8.0 to 0.10.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7291:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7291,1,['Update'],['Update']
Deployability,"Scala-Steward: Update circe-core, circe-generic, ... from 0.12.3 to 0.13.0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5739:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5739,1,['Update'],['Update']
Deployability,"Scala-Steward: Update circe-core, circe-generic, ... from 0.13.0 to 0.14.1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6410:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6410,1,['Update'],['Update']
Deployability,"Scala-Steward: Update circe-core, circe-generic, ... from 0.14.1 to 0.14.2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6874:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6874,1,['Update'],['Update']
Deployability,"Scala-Steward: Update circe-core, circe-generic, ... from 0.14.1 to 0.14.4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7048:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7048,1,['Update'],['Update']
Deployability,"Scala-Steward: Update circe-core, circe-generic, ... from 0.14.1 to 0.14.6",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7292:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7292,1,['Update'],['Update']
Deployability,Scala-Steward: Update circe-generic-extras from 0.12.2 to 0.13.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5743:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5743,1,['Update'],['Update']
Deployability,Scala-Steward: Update circe-generic-extras from 0.14.1 to 0.14.2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6908:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6908,1,['Update'],['Update']
Deployability,Scala-Steward: Update circe-generic-extras from 0.14.1 to 0.14.3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7049:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7049,1,['Update'],['Update']
Deployability,Scala-Steward: Update circe-optics from 0.14.1 to 0.15.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7293:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7293,1,['Update'],['Update']
Deployability,Scala-Steward: Update circe-yaml from 0.13.1 to 0.14.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6411:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6411,1,['Update'],['Update']
Deployability,Scala-Steward: Update circe-yaml from 0.14.1 to 0.14.2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7050:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7050,1,['Update'],['Update']
Deployability,Scala-Steward: Update cloudwatchlogs from 2.17.152 to 2.17.180,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6750:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6750,1,['Update'],['Update']
Deployability,Scala-Steward: Update cloudwatchlogs from 2.17.152 to 2.17.190,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6760:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6760,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-codec from 1.14 to 1.15,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5823:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5823,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-codec from 1.15 to 1.16.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7287:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7287,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-codec from 1.15 to 20041127.091804,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6060:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6060,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-csv from 1.9.0 to 1.10.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7059:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7059,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-io from 2.11.0 to 2.15.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7288:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7288,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-io from 2.7 to 2.8.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5841:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5841,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-io from 2.8.0 to 2.10.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6407:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6407,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-io from 2.8.0 to 20030203.000550,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6061:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6061,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-lang3 from 3.11 to 3.12.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6266:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6266,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-lang3 from 3.12.0 to 3.14.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7301:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7301,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-net from 3.7 to 3.7.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5915:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5915,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-net from 3.7.1 to 3.7.2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5958:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5958,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-net from 3.7.2 to 3.8.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6260:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6260,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-net from 3.8.0 to 3.10.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7289:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7289,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-net from 3.8.0 to 3.9.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7046:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7046,1,['Update'],['Update']
Deployability,Scala-Steward: Update commons-text from 1.10.0 to 1.11.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7302:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7302,1,['Update'],['Update']
Deployability,Scala-Steward: Update configs from 0.4.4 to 0.5.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6126:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6126,1,['Update'],['Update']
Deployability,Scala-Steward: Update configs from 0.5.0 to 0.6.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6244:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6244,1,['Update'],['Update']
Deployability,Scala-Steward: Update configs from 0.6.0 to 0.6.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6392:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6392,1,['Update'],['Update']
Deployability,Scala-Steward: Update delight-rhino-sandbox from 0.0.11 to 0.0.12,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5935:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5935,1,['Update'],['Update']
Deployability,Scala-Steward: Update delight-rhino-sandbox from 0.0.11 to 0.0.13,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6085:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6085,1,['Update'],['Update']
Deployability,Scala-Steward: Update delight-rhino-sandbox from 0.0.12 to 0.0.15,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6269:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6269,1,['Update'],['Update']
Deployability,Scala-Steward: Update diffson-spray-json from 4.0.3 to 4.1.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6415:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6415,1,['Update'],['Update']
Deployability,Scala-Steward: Update diffson-spray-json from 4.1.1 to 4.3.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7065:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7065,1,['Update'],['Update']
Deployability,Scala-Steward: Update diffson-spray-json from 4.1.1 to 4.4.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7309:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7309,1,['Update'],['Update']
Deployability,Scala-Steward: Update ficus from 1.4.7 to 1.5.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5777:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5777,1,['Update'],['Update']
Deployability,Scala-Steward: Update ficus from 1.5.0 to 1.5.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6522:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6522,1,['Update'],['Update']
Deployability,Scala-Steward: Update ficus from 1.5.1 to 1.5.2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6865:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6865,1,['Update'],['Update']
Deployability,Scala-Steward: Update flexmark-profile-pegdown from 0.36.8 to 0.62.2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5781:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5781,1,['Update'],['Update']
Deployability,Scala-Steward: Update fs2-io from 1.0.5 to 2.4.4,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5797:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5797,1,['Update'],['Update']
Deployability,Scala-Steward: Update fs2-io from 2.0.1 to 2.4.3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5775:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5775,1,['Update'],['Update']
Deployability,Scala-Steward: Update fs2-io from 2.0.1 to 2.4.5,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6035:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6035,1,['Update'],['Update']
Deployability,Scala-Steward: Update fs2-io from 2.0.1 to 2.4.6,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6088:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6088,1,['Update'],['Update']
Deployability,Scala-Steward: Update fs2-io from 2.4.6 to 2.5.4,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6239:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6239,1,['Update'],['Update']
Deployability,Scala-Steward: Update fs2-io from 2.5.4 to 2.5.7,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6385:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6385,1,['Update'],['Update']
Deployability,Scala-Steward: Update gax-grpc from 1.57.1 to 1.58.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5728:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5728,1,['Update'],['Update']
Deployability,Scala-Steward: Update gax-grpc from 1.57.1 to 1.58.2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5731:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5731,1,['Update'],['Update']
Deployability,Scala-Steward: Update gax-grpc from 1.58.2 to 1.58.3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5903:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5903,1,['Update'],['Update']
Deployability,Scala-Steward: Update gax-grpc from 1.58.3 to 1.59.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5953:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5953,1,['Update'],['Update']
Deployability,Scala-Steward: Update gax-grpc from 1.58.3 to 1.60.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5961:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5961,1,['Update'],['Update']
Deployability,Scala-Steward: Update gax-grpc from 1.58.3 to 1.60.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6075:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6075,1,['Update'],['Update']
Deployability,Scala-Steward: Update gax-grpc from 1.60.1 to 1.62.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6246:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6246,1,['Update'],['Update']
Deployability,Scala-Steward: Update gax-grpc from 1.62.0 to 1.66.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6393:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6393,1,['Update'],['Update']
Deployability,Scala-Steward: Update gax-grpc from 2.12.2 to 2.19.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6852:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6852,1,['Update'],['Update']
Deployability,Scala-Steward: Update gax-grpc from 2.19.0 to 2.19.6,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7036:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7036,1,['Update'],['Update']
Deployability,Scala-Steward: Update gax-grpc from 2.25.0 to 2.38.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7275:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7275,1,['Update'],['Update']
Deployability,Scala-Steward: Update gax-grpc from 2.4.0 to 2.4.1 [BW-921],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6515:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6515,1,['Update'],['Update']
Deployability,Scala-Steward: Update gax-grpc from 2.4.1 to 2.7.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6605:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6605,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-client-jackson2 from 2.1.4 to 2.2.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7276:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7276,1,['Update'],['Update']
Deployability,"Scala-Steward: Update google-api-client-jackson2, ... from 1.30.10 to 1.30.11",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5942:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5942,1,['Update'],['Update']
Deployability,"Scala-Steward: Update google-api-client-jackson2, ... from 1.30.11 to 1.31.3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6247:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6247,1,['Update'],['Update']
Deployability,"Scala-Steward: Update google-api-client-jackson2, ... from 1.31.3 to 1.32.1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6394:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6394,1,['Update'],['Update']
Deployability,"Scala-Steward: Update google-api-client-jackson2, ... from 1.32.1 to 1.32.2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6606:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6606,1,['Update'],['Update']
Deployability,"Scala-Steward: Update google-api-client-jackson2, ... from 1.33.2 to 1.33.4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6853:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6853,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-cloudkms from v1-rev20200609-1.30.10 to v1-rev20200803-1.30.10,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5726:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5726,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-cloudkms from v1-rev20200803-1.30.10 to v1-rev20200814-1.30.10,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5791:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5791,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-cloudkms from v1-rev20200814-1.30.10 to v1-rev20200903-1.30.10,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5870:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5870,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-cloudkms from v1-rev20200903-1.30.10 to v1-rev20201102-1.30.10,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6029:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6029,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-cloudkms from v1-rev20200903-1.30.10 to v1-rev20201102-1.31.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6117:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6117,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-cloudkms from v1-rev20201102-1.31.0 to v1-rev20210312-1.31.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6248:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6248,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-cloudkms from v1-rev20210312-1.31.0 to v1-rev20210622-1.31.5,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6395:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6395,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-cloudkms from v1-rev20210812-1.32.1 to v1-rev20210820-1.32.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6516:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6516,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-cloudkms from v1-rev20210820-1.32.1 to v1-rev20211130-1.32.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6607:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6607,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-cloudkms from v1-rev20220104-1.32.1 to v1-rev20220819-2.0.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6854:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6854,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-cloudkms from v1-rev20220104-1.32.1 to v1-rev20230127-2.0.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7037:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7037,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-cloudkms from v1-rev20230421-2.0.0 to v1-rev20231012-2.0.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7279:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7279,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-genomics from v2alpha1-rev20200330-1.30.9 to v2alpha1-rev20210322-1.31.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6249:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6249,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-genomics from v2alpha1-rev20210322-1.31.0 to v2alpha1-rev20210605-1.31.5,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6396:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6396,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-genomics from v2alpha1-rev20210811-1.32.1 to v2alpha1-rev20220328-2.0.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6855:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6855,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-genomics from v2alpha1-rev20210811-1.32.1 to v2alpha1-rev20220913-2.0.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7038:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7038,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-lifesciences from v2beta-rev20200603-1.30.10 to v2beta-rev20200806-1.30.10,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5778:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5778,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-lifesciences from v2beta-rev20200806-1.30.10 to v2beta-rev20201001-1.30.10,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5933:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5933,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-lifesciences from v2beta-rev20200806-1.30.10 to v2beta-rev20201105-1.30.10,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6066:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6066,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-lifesciences from v2beta-rev20200806-1.30.10 to v2beta-rev20201105-1.31.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6120:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6120,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-lifesciences from v2beta-rev20201105-1.31.0 to v2beta-rev20210319-1.31.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6250:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6250,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-lifesciences from v2beta-rev20210319-1.31.0 to v2beta-rev20210527-1.31.5,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6397:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6397,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-lifesciences from v2beta-rev20210813-1.32.1 to v2beta-rev20220401-2.0.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6856:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6856,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-lifesciences from v2beta-rev20210813-1.32.1 to v2beta-rev20220916-2.0.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7039:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7039,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-api-services-lifesciences from v2beta-rev20220916-2.0.0 to v2beta-rev20230707-2.0.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7280:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7280,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-auth-library-oauth2-http from 0.21.1 to 0.22.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5949:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5949,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-auth-library-oauth2-http from 0.21.1 to 0.22.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6116:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6116,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-auth-library-oauth2-http from 0.22.1 to 0.22.2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6251:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6251,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-auth-library-oauth2-http from 0.22.2 to 0.26.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6398:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6398,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-auth-library-oauth2-http from 1.1.0 to 1.3.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6608:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6608,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-auth-library-oauth2-http from 1.5.3 to 1.10.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6857:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6857,1,['Update'],['Update']
Deployability,Scala-Steward: Update google-auth-library-oauth2-http from 1.5.3 to 1.16.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7040:15,Update,Update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7040,1,['Update'],['Update']
