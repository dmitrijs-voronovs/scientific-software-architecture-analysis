quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability,"Here's a suggested set of things to look at as part of this ticket:. -See if we can avoid fetching all headers on startup by passing in the needed info via alternate args (https://github.com/broadinstitute/gatk/issues/2639). -Do profiling to find an appropriate value for the --batchSize argument,; once it's merged (https://github.com/broadinstitute/gatk/issues/2641). -Shrink NIO buffers (--cloudPrefetchBuffer and --cloudIndexPrefetchBuffer) down to the smallest values that still produce acceptable performance (https://github.com/broadinstitute/gatk/issues/2640). Thibault of red team aka @Horneth has agreed to take this on.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-298338616:445,down,down,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-298338616,1,['down'],['down']
Availability,"Here's how these scripts are organized and why they take the form it is now:. How to run; * `manage/project.sh` is the ""executable""; * paths for VCF files (zipped or not) from PacBio callsets on CHM haploids, and Manta's VCF on the mixture should be provided to `manage/project.sh`, and; * paths for two versions of GATK-SV callsets; one is fine, but scripts in the sub-directory `manage` must be modified. Two GATK-SV vcf files are requested because this would allow one to compare if a supposedly improvement would make our raw sensitivity/specificity better or worse, that was the use case [here](https://github.com/SHuang-Broad/GATK-SV-callset-regressionTest), and; * paths to where results are to be stored, one for each GATK-SV callset must be given and ; * path to where to store the results of comparing the two callsets; * several GNU bash utilities are expected, `guniq` and `gsort`, when run on a Mac, as well as `bedtools`. and what to expect; * the scripts checks the VCF files, prints to screen a slew of information that one can pipe, or simplely browse through.; * the scripts also outputs the ID's of variants from each of the two GATK callsets that are ""validated"" by PacBio haploid calls. Misc points:; * watch out for ""duplicated"" records, as sometimes different assembly contigs mapped to the same locations have slightly different alleles (SNP, for example) hence both would be output, but there aren't many such records based on experience; * there are also some variants that we output to the VCFs having size <50 or >50K, both of which are filtered upfront and saved separately.; * The scripts started when we first call insertions, deletions, inversions and small duplications, and back then PacBio call sets on the CHM haploids were not available, so Manta's calls were used as ""reference"", that explains why they are referred to throughout the project",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4406#issuecomment-365730030:1764,avail,available,1764,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4406#issuecomment-365730030,2,['avail'],['available']
Availability,"Here's some reports of similar errors:. https://sourceware.org/ml/libc-help/2008-05/msg00072.html; https://stackoverflow.com/questions/36221574/random-error-on-mutex-lock. It seems like most commonly this error is caused by forgetting to initialize a `pthread_mutexattr_t`. Although the second answer on this thread claims that it might be due to using the default mutex type instead of a recursive mutex. https://stackoverflow.com/questions/21825291/threading-issues. GenomicsDB may not use pthreads directly, but both tileDB and htslib do, so I think it's worth looking into it. I haven't found any instances of htslib or tileDb using an uninitialized `pthread_mutexattr_t` but it does look like tiledb uses default attributes for pthread_mutex_t instead of setting `PTHREAD_MUTEX_RECURSIVE`, so if that really is an issue maybe that's the problem?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4518#issuecomment-425560459:31,error,errors,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4518#issuecomment-425560459,3,['error'],"['error', 'error-on-mutex-lock', 'errors']"
Availability,"Here's what this PR does:. * Adds an argument to skip a locus rather than downsample if depth is too high. This handles horrible regions so rife with mapping errors that we wouldn't believe any calls from them. Basically, it blacklists on the fly intervals that should have been blacklisted ahead of time.; * Adds a stride argument for positional downsampling so that we downsample within a range of alignment start positions. This regularizes statistical fluctuations in coverage and, more importantly, makes positional downsampling much more useful for data where many reads share the same start position while most start positions have no reads. I believe @fleharty deals with this kind of stuff.; * ~~Deletes a few methods in the `ReadsDownsampler` API that were only used in their own unit tests.~~; * Adds an option to bias downsampling in favor of reads with high mapping quality.; * Beefs up the unit tests for `PositionalDownsampler`. The new functionality is a superset of the old, and the old stuff is unchanged. HaplotypeCaller is not affected, unless of course someone wants to run it with the new options. The new options allow Mutect2 to run much faster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3238:74,down,downsample,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238,6,"['down', 'error']","['downsample', 'downsampling', 'errors']"
Availability,"Hey @bbimber I will have to think on this. The most simple solution might be to add a feature context side input for the annotation in question but looking at how that code is threaded in the variant callers it would take a little bit of work to add it to those tools and probably introduce some complicated questions, (like for example: what is the correct featurecontext to send to annotate a variant that only covers one base of the site in question where the feature context object exists?). Its possible to do something like that for variant annotator a little bit more easily but i guess the question comes down to this: How generalized do you think this annotation will be? Does it need to be annotatable with variant annotator or could you write a separate tool that does the variant -> variant association and calculates the annotation without using the plugin framework? If it needs to be generalizable I would agree with @droazen that the easiest approach would be to add the side input as an argument and make the annotation object responsible for querying the feature context. This is inelegant but might be preferable to putting the entire walker context into the `annotate()` function.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754249851:613,down,down,613,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754249851,2,['down'],['down']
Availability,"Hey @ccartermices. Looking at that error message it appears that the genotype given alleles has tried to insert a '\*' allele into an assembled haplotype. (""TTTTGAC\*TTCGC"" in the error message). I suspect this is because the code is missing a check to filter symbolic alleles out of GGA inputs. Can you check your input `db_raw_call_bbe_6largest.vcf` for `\*` alleles? It should be possible to filter those out of your input file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6260#issuecomment-552999630:35,error,error,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6260#issuecomment-552999630,2,['error'],['error']
Availability,"Hey @davidbenjamin,. just a heads-up that we have generated some additional inights on this issue that relate to the effects of sample prep on false-positive rates under the changed error model. In short, it seems as if the error model is now much more (perhaps overly?) sensitive to sample-prep noise typical in real-world clinical sequencing data, leading to a 2-3x increase of false-positives in such WES (and, assuming, also other high-coverage) samples. This may have been an undesired side effect of overfitting the error model to the synthetic (and thus much cleaner) DREAM3 benchmark two years ago. We are presenting the results today at ISMB in Lyon, the world bioinformatics conference. Let me know if any of you guys are participating so that we can have a chat over a glass of wine, if you like!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1647525248:182,error,error,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1647525248,3,['error'],['error']
Availability,Hey @gokalpcelik thanks for writing in. So there are a few important differences that could be confounding the results you see for SplitNCigarReads between GATK3 and GATK4. The big one is that in GATK 4 the reads get soft clipped instead of hard clipped and the subsequent splits for the reads are marked as Supplementary reads (which does not seem to have been the case in GATK3). Can you check that you aren't filtering non-primary alignments from your output/IGV sessions? Many downstream tools that might operate on split reads must be careful to handle these differences correctly which can easily cause confusion when comparing gatk3.8 results to gatk4+ results. A simple way to confirm is to slect one of those softclipped reads in IGV and to search the output BAM for GATK4 for reads sharing the same name. You should see the matched mates. If it really does appear that the right overhangs are getting dropped as appears from your screenshots then it would be helpful if you could clarify what arguments you ran for both versions of the tool as well as sharing with us an example file where this is happening.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7323#issuecomment-865275697:481,down,downstream,481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7323#issuecomment-865275697,2,['down'],['downstream']
Availability,"Hey @jean-philippe-martin, this looks good. I've made some very small changes to avoid the back-and-forth of a review, and rebased the branch onto latest master. . The main change I made was in `IOUtils.getPath()`. It now traps the `IOException` and throws a `UserException` instead. This has the benefit of not requiring client code to put `throws IOException` (or catch the `IOException`) everywhere, and is more consistent with the rest of the codebase, which typically traps checked exceptions as early as possible and wraps them in unchecked exceptions (either `UserException` or `GATKException`, depending on whether it's likely to be the user's fault or not). Also made a small change in `NioBam` to make it use a logger instead of `System.out.println()` for debug output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-259798014:652,fault,fault,652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-259798014,1,['fault'],['fault']
Availability,"Hey @jemunro,. Thanks for sharing your fix. I tried it on my data but now I have this ERROR message:; ```; ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.8-0-ge9d806836):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: For input string: ""NaN\1SOR=0.693""; ##### ERROR ------------------------------------------------------------------------------------------; INFO 13:46:00,793 HelpFormatter - ---------------------------------------------------------------------------------- ; ```; Would not be enough to use this code instead?:; ```; bcftools view in.vcf.gz |; sed 's/=nan/=NaN/g' |; bgzip > fixed.vcf.gz; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5582#issuecomment-630137734:441,error,error,441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5582#issuecomment-630137734,14,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hey @mbabadi, I've answered this user question but I think my answer needs your review. Also, DR mentioned perhaps this option could be set automatically for users? Thanks. ---; Thanks @shlee for the update,; I used the latest jar from the gatk4 repo as recommended. And managed to derive the read count input file and sex genotype table. I just wanted to confirm whether Nd4j also needed to be installed if not using Spark. #script run; ./gatk-launch GermlineCNVCaller --contigAnnotationsTable ../gatk4_Hellbender/grch37_contig_annotations.tsv --copyNumberTransitionPriorTable ../gatk4_Hellbender/grch37_germline_CN_priors.tsv --jobType LEARN_AND_CALL --outputPath ./TS1 --input ../gatk4_Hellbender/target_cov.tsv --targets ../gatk4_Hellbender/targets.txt --disableSpark true --sexGenotypeTable ../gatk4_Hellbender/TS1_genotype --rddCheckpointing false --biasCovariateSolverType LOCAL. #I am getting the following error which seems to be linked with Nd4j:. Using GATK jar ~/localwork/playground/programs/gatk-protected/build/libs/gatk-protected-package-b4390fb-SNAPSHOT-local.jar; 102-b14; Version: 4.alpha.2-1136-gc18e780-SNAPSHOT; 16:55:21.931 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:21.932 INFO GermlineCNVCaller - Deflater: IntelDeflater; 16:55:21.932 INFO GermlineCNVCaller - Inflater: IntelInflater; 16:55:21.932 INFO GermlineCNVCaller - Initializing engine; 16:55:21.932 INFO GermlineCNVCaller - Done initializing engine; 16:55:21.933 INFO GermlineCNVCaller - Spark disabled. sparkMaster option (local[*]) ignored.; 16:55:23.448 INFO GermlineCNVCaller - Parsing the read counts table...; 16:55:24.876 INFO GermlineCNVCaller - Parsing the sample sex genotypes table...; 16:55:24.896",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3098:915,error,error,915,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3098,1,['error'],['error']
Availability,"Hey @mwalker174 ,. Do you have any suggestions about how to perform step 1? I naively tried to use picard's `MergeBamAlignment` using the PathSeq output BAM as the aligned bam and the PathSeq input BAM as the unmapped BAM but I get the following error message. `IllegalArgumentException: Do not use this function to merge dictionaries with different sequences in them. Sequences must be in the same order as well. Found [NZ_DS990135.1, NZ_AJSY01000035.1, ... `. I tried sorting both BAM files by queryname and removing the alignment for the input BAM using `RevertSam` but neither of these worked. I suspect that it's because of the PathSeq output BAM given the references to the microbial sequences. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6655#issuecomment-644426370:246,error,error,246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6655#issuecomment-644426370,1,['error'],['error']
Availability,"Hey Chris. Tagging you for some triage help. User has reported ""Figured it out, this is what you get when you run out of disk space. Please could this error message be made more helpful please?"" Is there anything you can do? A better error message would help, but not sure who to assign this to.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5064#issuecomment-408466937:151,error,error,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5064#issuecomment-408466937,2,['error'],['error']
Availability,"Hey folks,. I have a test dataset that interestingly core-dumps or JVM errors with `--smith-waterman FASTEST_AVAILABLE` but not with `--smith-waterman JAVA`. The only thing I can think of is somehow Intel's HMM has a length limitation, as I am using `--assembly-region-padding 1000` to GATK to call 100-1000bp indels (and it works!). I cannot share the test BAM unfortunately. What can I do to help debug further?. I'm using `gatk4-4.1.8.1-0` from `conda create -n debug-gatk4 -c defaults -c conda-forge -c bioconda gatk4`. ```; $gatk ... -version; The Genome Analysis Toolkit (GATK) v4.1.8.1; HTSJDK Version: 2.23.0; Picard Version: 2.22.8; $ java -version; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12); OpenJDK 64-Bit Server VM (build 25.152-b12, mixed mode); ```. First error motif:; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010efa9dc2, pid=23946, tid=0x000000000000a503; #; # JRE version: OpenJDK Runtime Environment (8.0_152-b12) (build 1.8.0_152-release-1056-b12); # Java VM: OpenJDK 64-Bit Server VM (25.152-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x3a9dc2] PhaseIdealLoop::set_ctrl(Node*, Node*)+0x10; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; #; # Compiler replay data is saved as:; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; ```. Second error motif:; ```; java(24057,0x7000035bd000) malloc: Incorrect checksum for freed object 0x7fd8a8193600: probably modified after being freed.; Corrupt value: 0x2e4630002e47e; java(24057,0x7000035bd000) malloc: *** set a breakpoint in malloc_error_break to debug; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733:71,error,errors,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733,5,['error'],"['error', 'errors']"
Availability,"Hey there!. I am currently working on a project involving mutagenesis analysis using the AnalyzeSaturationMutagenesis tool. Our rationale is that a single nucleotide exchange due to a sequencing error is more likely than two or three consecutive nucleotide changes in one mutated codon, making it important for us to adjust the quality standards based on the type of variant (single/double/triple exchanges). I would like to ask if there is any existing functionality within the AnalyzeSaturationMutagenesis tool to incorporate base-specific nucleotide quality into the analysis, especially in a way that allows for different quality thresholds based on the number of consecutively mutated bases in one codon. If such a feature is not available, I would appreciate any suggestions you may have for a potential workaround that would enable us to set a stricter quality threshold for single nucleotide variations in the current tool. Thank you in advance!. Best,; Benni. @MaximilianStammnitz; @tedsharpe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8995:195,error,error,195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8995,2,"['avail', 'error']","['available', 'error']"
Availability,"Hey there~ I have run the following command:. gatk HaplotypeCaller \; -R bbv18h27rm.fa \; -ERC GVCF \; --alleles db_raw_call_bbe_6largest.vcf \; -I bbe_off_xL3_68.concordant_withRG.bam \; -O test_call_off_xL3_68_allele_46samples.vcf.gz. and had some error:; ```; Using GATK jar /home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar HaplotypeCaller -R /db_students1/genetic_map/snp_calling/bbv18h27rm.fa -ERC GVCF --alleles /db_students1/gatk_out/db_raw_call_bbe_6largest.vcf -I /db_students1/genetic_map/reseqData_mapping_bam/bam/bbe_off_xL3_68.concordant_withRG.bam -O /db_students1/gatk_out/test_call_off_xL3_68_allele_46samples.vcf.gz; 17:04:13.440 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2019 5:04:15 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:04:15.137 INFO HaplotypeCaller - ------------------------------------------------------------; 17:04:15.138 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.3.0-25-g8d88f6e-SNAPSHOT; 17:04:15.138 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:04:15.138 INFO HaplotypeCaller - Executing as cc@hr18b on Linux v3.10.0-957.el7.x86_64 amd64; 17:04:15.138 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_212-b04; 17:04:15.138 INFO HaplotypeCaller - Start Date/Time: November 12, 2019 5:04:13 PM CST; 17:04:15.139 INFO HaplotypeCaller - --------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6260:250,error,error,250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6260,1,['error'],['error']
Availability,"Hey y'all! I think this will be a welcome addition. I did want to point out that to me it also makes sense to force things that have PL=0,0,0 (i.e. a flat genotype likelihood) to also be called ./. in preference to calling it 0/0 for downstream uses. I encountered plenty of cases where DP>0 but PL=0,0,0, so I check for both of those when setting genotypes to ./. in my workflows:. https://github.com/eriqande/mega-non-model-wgs-snakeflow/blob/2e9218e9211fa140bb11b59cb154181b7dc5f205/workflow/rules/calling.smk#L258-L259. (I also find it useful to count up the number of missing genotypes afterward.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1636106942:234,down,downstream,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1636106942,1,['down'],['downstream']
Availability,"Hi , I got the same error for chrY today (v4.3.0.0 ). For this chromosome, the ploidy was set to 1 , The genotypes have only one allele so we get an IndexOutOfBoundsException exception (`get(1)`) for this line:. https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/samples/MendelianViolation.java#L181. ```; gMom.getAlleles().contains(gChild.getAlleles().get(1)) && gDad.getAlleles().contains(childRef));; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7304#issuecomment-1497966273:20,error,error,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7304#issuecomment-1497966273,1,['error'],['error']
Availability,"Hi - @kaixinxiaonvwa-hub . I have a couple of questions:. - Can you post your full stack trace with the errors?; - Did you attempt to enable `gnomAD` data sources or is it doing this without any changes to the data sources directory? Did you do any other configuration steps after downloading the datasources and before running funcotator?. If you enable `gnomAD`, the datasources are hosted on google cloud. If you don't have an internet connection or google cloud is blocked, Funcotator will not be able to connect to read the gnomAD data and will show the error in your `1` case above.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1494703773:104,error,errors,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1494703773,3,"['down', 'error']","['downloading', 'error', 'errors']"
Availability,"Hi . I used CollectAllelicCounts with a bed file for the `-I` parameter and got the following error message:. `A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The stop position 29430911 is less than start 29430912 in contig chr2`. The only entry in my bed file dealing with this position is the following:. `chr2	 29430911	29430911	ENSE00001695104	3	ENST00000431873	ENSG00000171094	ALK`. There seems to be a problem, that CollectAllelicCounts add 1 base to the start of each interval. I tried it with a padding of 10 (`-ip 10`) but did not get it to work. Am I on the wrong way? Isn't it a problem with the bed file?`; Thanks in advance; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4504:94,error,error,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4504,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi ; This is an issue derived from the GATK forum. . https://gatk.broadinstitute.org/hc/en-us/community/posts/17996976455067-HaplotypecallerSpark-error?page=1#community_comment_18161302676123. User is having an issue with HaplotypeCallerSpark possibly a traversal issue. This problem was also present in the past in another github issue. https://github.com/broadinstitute/gatk/issues/7199. User tried with the latest version as well and even providing a bed file as in the former issue did not help solve the problem. HaplotypeCaller works fine with or without the bed file but Spark version does not. . Here is the error message. ```; **Caused by: java.lang.IllegalArgumentException: Sequence [VC HC_call @ NW_020555792.1:138 Q78.32 of type=SNP alleles=[T*, G] attr={AC=2, AF=1.0, AN=2, DP=2, ExcessHet=0.0000, FS=0.000, MLEAC=[1], MLEAF=[0.5], MQ=47.00, QD=29.09, SOR=0.693} GT=[[AR0111 G/G GQ 6 DP 2 AD 0,2 PL 90,6,0]] filters= added out of order currentReferenceIndex: 9, referenceIndex:11; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:368); at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:138); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538); at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135); ... 9 more; 01:50:24.007 INFO ShutdownHookManager - Shutdown hook called**; ```; User wa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8490:146,error,error,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8490,2,['error'],['error']
Availability,Hi @Nanderson246 ; This is a known issue that requires us to update the conda environment and its dependencies on our end which all rely on older versions of certain R and python tools. Direct intervention and changing this code breaks certain unit tests which result in failure to build our conda and docker environments. This is a works in progress for our newer conda and docker environment however for the time being you may manually edit your R scripts to solve it or use R from our conda or docker environments to generate images. I hope this helps. Regards.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8992#issuecomment-2391551502:271,failure,failure,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8992#issuecomment-2391551502,1,['failure'],['failure']
Availability,"Hi @SZLux,. The --spark-runner LOCAL option is only valid if you want to launch the GATK from the same machine you want to run on, i.e. on your laptop or on the head node of a cluster you have logged into via ssh. We do not currently support --spark-runner LOCAL if you want to, say, launch from the head node and run on another node. Your second issue is specific to PathSeq. Can you please open another ticket for that? That way if another user has the same issue it'll be easier to find the solution. P.S. Your new error may be because the .hss file could not be found on HDFS. You may need to give it the full HDFS URI i.e. hdfs://path/to/kmers.hss",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-384335260:518,error,error,518,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-384335260,1,['error'],['error']
Availability,"Hi @ScienceConnor - thanks for the feedback, this is Ilya from Ultima Genomics. ; From what I see, this seems to me to be more of the output format issue rather than the issue with HaplotypeCaller per se.; Would you mind pinging me over ilya dot soifer at ultimagen dot com and I am happy to help you out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8112#issuecomment-1332400762:221,ping,pinging,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8112#issuecomment-1332400762,2,['ping'],['pinging']
Availability,"Hi @Vzzarr ,. Thank you for reporting this. I've never heard of this happening before, but I suspect the difference is that you are using maven and we've only ever used gradle. @magicDGS @LeeTL1220 You both build downstream projects, have you ever seen this?. From what I can tell `com.github.fommil.netlib.all` is a POM only dependency, but it's not annotated as such in our generated POM. Gradle resolves it correctly, and then resolves it's transitive dependencies correctly, but maven seems to resolve it differently. . I've reproduced the issue locally, and it seems like it can be fixed by changing this line in our build file:. https://github.com/broadinstitute/gatk/blob/62e339fb9150db19f80d4b48aed8d47608461690/build.gradle#L220. to specify the exact dependencies we want instead of using the all artifact. I'll open a pull request with that. There's an additional catch though, you're using the latest release to maven central, which is actually a very old version. We haven't been able to release newer versions to central because they depend on a snapshot of a fork of a google library that isn't released to central. We publish these to our artifactory server which can be resolved like any other maven repository. Would you be able to work with an artifactory dependency? If you can you can A) use the newer and better gatk, B) I won't have to backport this change on top of beta.2 . We're going to fix the problem of not being in central eventually ( certainly before 4.0 release... ) but until then resolving from our artifactory is probably the best bet. If you can't / don't want to use the artifactory releases, let me know and I can release a patch version to maven that fixes this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-339056907:213,down,downstream,213,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-339056907,1,['down'],['downstream']
Availability,"Hi @bhanugandham . I wasn't able to reproduce your problem on my laptop. Based on the error message, it looks like a network config Spark issue. What environment are you running this in? If it's MacOS, can you post the contents of `/etc/hosts`? Also the error message looks like it's truncated. You might be able to get the full stack trace by adding `--verbosity DEBUG`, which would be helpful as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802#issuecomment-473419380:86,error,error,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802#issuecomment-473419380,2,['error'],['error']
Availability,"Hi @cccnrc, glad you found this discussion interesting and apologies for the very late reply. Unfortunately, there hasn't been much movement on this front, as I think we decided that the current model sufficed. I think we are moving in a direction so that we can obviate the need for a separate step to fit global (e.g., depth) and per-contig (e.g., contig ploidy) parameters for each sample. Recall that this is only necessary because each gCNV genomic shard needs these quantities to perform inference, but cannot infer them from the data they are responsible for fitting (which typically covers less than a contig). We are looking to reimplement gCNV in a more modern inference framework that could allow us to do away with such sharding entirely. We could thus fit global/per-contig quantities of interest jointly with the rest of the gCNV model. The timeframe for this work may be relatively long (~year), but I think it'll be worth it. That said, I think a key takeaway from this work is that genotype priors can be more powerful for breaking degeneracies than contig-ploidy priors, so we will probably try to incorporate that insight in future work. To answer your questions:; 1) There are no additional results much further beyond what is shown above.; 2) You can see snippets/comparisons of the genotype and contig-ploidy prior file above. If you're just looking for information about the contig-ploidy prior table used in the current pipeline, see an example at https://gatk.broadinstitute.org/hc/en-us/articles/360051304711-DetermineGermlineContigPloidy and feel free to modify it to be more/less strict as desired.; 3) Unfortunately I believe the samples discussed above are not publicly available. If you are having difficulties with the current model, it would probably be useful to hear about them!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-894432129:1700,avail,available,1700,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-894432129,1,['avail'],['available']
Availability,"Hi @chandrans, thanks for the reply! Could you try with GATK 4.0.2.1 please? It's the version that gives me the errors. ; Thanks again,. Cristina.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4514#issuecomment-373131454:112,error,errors,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514#issuecomment-373131454,1,['error'],['errors']
Availability,"Hi @chatchawit,. It's hard to tell what's going on from the information you provided. Could you include the exact commandline that you're trying to run. Also, the complete stack trace from both the original error and the new error message would be very helpful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-384991868:207,error,error,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-384991868,2,['error'],['error']
Availability,Hi @cmnbroad - I removed a space in the path and re-ran ValidateVariants. It went through throwing no errors. Many thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4657#issuecomment-380989518:102,error,errors,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4657#issuecomment-380989518,1,['error'],['errors']
Availability,"Hi @cmnbroad . First, I apologize for not making a correct reproducible example. In my first post, I modified some of the information about the path and filenames but should have tested if this was still generating the error which it does not (even on my side). However, my few tests made today resulted in interesting observations. ## Traces. Below the command and trace produced from my real case. I annonymized it but the number of characters in path were kept. ```java; (cerc_prod) [16:48 xxxxxxx@yyyyyy:test a]$ gatk MergeVcfs -I data/calling/cerc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; Using GATK jar /master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar MergeVcfs -I data/calling/cerc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; 16:48:58.710 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Mon Jun 22 16:48:58 CDT 2020] MergeVcfs --INPUT data/calling/cerc_prod2.SM_V7_1.vcf.gz --INPUT data/calling/cerc_prod2.SM_V7_ZW.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jun 22, 2020 4:48:58 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241:219,error,error,219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241,1,['error'],['error']
Availability,"Hi @cmnbroad Thanks for the quick response. I tried your suggestion, but still get the error :. **A USER ERROR has occurred: Couldn't read file ~/data/Illumina/GATK4/TEST/~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz. Error was: It doesn't exist.**. Seems the "":"" was converted to my current work directory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-252349649:87,error,error,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-252349649,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Hi @cwhelan , I've expanded this PR to do more than what it originally was trying to fix, and separated the patches by commits as usual:. * the originally proposed fix, which brings back the annotation that are available to simple variants but go missing due to a careless bug, is now done in commit 50f1b640a31ddb528dc763b83b26a9d98dce8556; this commit also accordingly refactors the giant class `CpxVariantDetector` into three new classes; * in the 2nd commit 734516383fb665a79796de76535560fc03cb754b, I did more refactoring on how we group the descriptions for the annotation keys, and updated the test VCF files accordingly.; * because of the refactoring, the review comments were gone, so I added them back in the 3rd commit b7619c45a949dfba21d65a5ed876bc72e832aa77, which contains the comments and my replies. They come in as TODO's but are going to be removed ultimately; * in the following commits, I added tests for the CPX code path, selecting three representative cases (there's no limit how complex the scenario can go). One particular commit 224c97c7b736e94ed6b4d8b067ec830a9f8f2403 is large but most of it is for adding a flat file that contains the chromosome names in hg38 and their lengths for building a bare bone sequence dictionary used in building test data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4330#issuecomment-372761525:211,avail,available,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4330#issuecomment-372761525,2,['avail'],['available']
Availability,Hi @danieljrichard. I suspect the error message you're seeing is a not the underlying problem. Sometimes when the tool crashes there are errors during standard shutdown cleanup that hide the real problem. There's a [recent patch](https://github.com/broadinstitute/gatk/commit/f7dea3c08b126c56b851c4d381cfdb5513e0584f) which makes it so that should happen much less but it's not in any currently released version. Are you able to build the current master branch of gatk and repeat your test with that version? I suspect it will clarify what's going on.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452177739:34,error,error,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452177739,2,['error'],"['error', 'errors']"
Availability,Hi @dantefff. We're planning on officially moving GATK to use java 11 by default in the relatively near future. I don't have an exact timeline. It's definitely annoying to not have it work out of the box on most linux systems and it's something we want to do soon. I have had very limited bandwidth lately and updating to 11 has been a casualty of that. Now that the world is rebooting things should start to tick along faster and I hope to have it updated soon.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7333#issuecomment-871615858:376,reboot,rebooting,376,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7333#issuecomment-871615858,1,['reboot'],['rebooting']
Availability,"Hi @david-wb . I reformatted your comment slightly to make the stack trace more legible, I hope you don't mind. I suspect your intuition about the System.exit(0) is entirely correct. I suspect we haven't noticed it because we typically run in yarn client mode and you're running in cluster mode. . Two questions:; 1. How often does it happen? Can you regularly reproduce it?; 2. Have you examined the output files to make sure they are correct and not truncated? . It looks like we'll probably have to add a check and wait for the spark context to properly shut down.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299493397:562,down,down,562,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666#issuecomment-299493397,2,['down'],['down']
Availability,"Hi @davidbenjamin . Mind having a look at this ticket, a user is receiving the error above and office hours mentioned this might be an issue on our end.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6419#issuecomment-578885614:79,error,error,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6419#issuecomment-578885614,1,['error'],['error']
Availability,"Hi @droazen ,; I am sorry for the late response. I used the docker container from dockerhub broadinstitute/gatk:4.4.0.0 .; The which python command produces the correct path. I re-loaded the container and tried it again without modification. Again the pipeline crashed with BaseRecalibrator and the above mentioned error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1691458235:315,error,error,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1691458235,1,['error'],['error']
Availability,"Hi @fgvieira there are some problems with our implementation of bcf that make it inadvisable to use it right now in GATK. We would like to fix it, and there has been substantial recent interest in using bcf, but we don't have a timeline fo when it will be available right now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7132#issuecomment-793024769:256,avail,available,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7132#issuecomment-793024769,1,['avail'],['available']
Availability,"Hi @fleharty! Just a friendly ping here, because I need this in my work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2154#issuecomment-252614571:30,ping,ping,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2154#issuecomment-252614571,1,['ping'],['ping']
Availability,"Hi @fpbarthel,. You have a few options, but I think they would all require a bit of manual processing on your part. For example, you could use CollectReadCounts with your initial list of bins, but then you'd have to do the averaging step manually. Hopefully, it should not be too trouble to put together your own script to do this; however, since it is somewhat of a custom operation, it's unlikely we'll support it as a feature. In any case, note that the rest of the downstream tools in both our germline and somatic pipelines are only set up to work with integer counts. I don't know your reasons for wanting to average counts over multiple bins, but since 1) averages contain less information than the full resolution bins, 2) the BAM I/O to perform count collection is expensive (so we don't want to discard information during the CollectReadCounts step that we might need later), and 3) we want to build generative models of the counts, our philosophy is to always retain the integer counts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5432#issuecomment-439892644:469,down,downstream,469,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5432#issuecomment-439892644,1,['down'],['downstream']
Availability,"Hi @gokalpcelik ; I've re-run the same operation on 4.3, and on 4.5, with the exact same error :/ ; I've been pointed to https://github.com/broadinstitute/gatk/issues/8164 which is the same issue",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8834#issuecomment-2112075352:89,error,error,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8834#issuecomment-2112075352,1,['error'],['error']
Availability,"Hi @jaideepjoshi ,; I was able to get it work with maprfs but using its hdfs path (maprfs is compatible with it). The weird things is: for a path `maprfs:///username/file`, it will be `hdfs://spark01:7222/user/username/file`.; For example:; ```./gatkRun.sh CountVariantsSpark -V hdfs://spark01:7222/user/axverdier/data/phalstedii/PLHAL710.710-20180213-1113.vcf```. gatkRun.sh is just a script to run gatk on spark with some parameters. As I didn't run a job recently (yes, I'm lazy ^^), it's possible it's not compatible with the current gatk version (for example, ` --spark-submit-command` was not).; ```; #! /bin/bash. # Run a gatk spark tool on the lipm's spark.; # The command is: ./gatkRun.sh <gatk spark tool name> <tool arguments>. # === gatk-launcheri and spark infos ===; launcher=""/usr/local/bioinfo/bin/gatk""; sparkNumExecutors=250; sparkMemExecutors=""5g"". # At least, the tool must be defined; if [[ $# -eq 0 ]]; then; echo ""ERROR: you must specify the gatk tool""; exit 1;; fi. # === Get args ===; # Get the tool, then remove it from arguments (with shift) so $@ only contains its parameters; tool=$1 # Get the tool name; name=""GATK_$tool""; shift; toolParams=$@. # === Running ===; sparkParams=""--name $name --num-executors $sparkNumExecutors --executor-memory $sparkMemExecutors --deploy-mode cluster"". gatkSparkParams=""--program-name $name --spark-runner SPARK --spark-master yarn"" # gatkspark parameters related to spark. cmd=""$launcher $tool $toolParams $gatkSparkParams -- $sparkParams""; echo -e ""\n$cmd\n""; $cmd; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-375218548:931,echo,echo,931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-375218548,3,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,"Hi @jjfarrell,. It's hard to know what might be going wrong in these files. Can you describe how you are running `StructuralVariationDiscoveryPipelineSpark` -- on a local Spark cluster, on GCS dataproc, or in Spark local mode? Can you provide the command line?. Is there anything you can identify as being different about the failing files, maybe from other WGS metrics: higher coverage, high duplicate rate or chimera rate, etc? Are these human germline or cancer samples?. One initial thought could be that something is running out of memory when processing these samples, or getting bogged down in garbage collection. You could try increasing the parameters you give for `--driver-memory`, `--executor-memory`, or `--conf spark.yarn.executor.memoryOverhead`. There may be other Spark parameters you could try adjusting as well. Our default scripts run with these Spark options on a GCS Dataproc cluster:. ```; -- \; --spark-runner GCS \; --cluster ""${CLUSTER_NAME}"" \; --num-executors ${NUM_EXECUTORS} \; --driver-memory 30G \; --executor-memory 30G \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; ```. You could try increasing those memory values (if you have the resources) and see if that helps.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380130398:593,down,down,593,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380130398,2,"['down', 'heartbeat']","['down', 'heartbeatInterval']"
Availability,"Hi @jjfarrell,; Unfortunately there are some shortcuts and approximations that happen in GVCF mode that do break down in a small number of cases. I suspect your issue may be related to #5171. I can take a look at this, but it could be a while. What would speed things along is if you could share an IGV screenshot and GVCF snippet for cram 1 around the GQ0 call.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5445#issuecomment-441671265:113,down,down,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445#issuecomment-441671265,1,['down'],['down']
Availability,Hi @jonn-smith I had some success getting outputs with this. However ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ is no longer accessible. The most recent version of I accessed on 3/19/2018 still contained at least one error. I'm trying to correct them myself as I go using a more recent build of GATK but it would be helpful if the data files required by this program were available. The one I found is in `gencode_xrefseq.config` where it references a source that doesn't exist and I fixed that. After that I was able to get outputs with hg38. Thanks for your work on this!. I'd also point out there are a lot of fields in the MAF with `__UNKNOWN__` as the entry,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383387645:242,error,error,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383387645,2,"['avail', 'error']","['available', 'error']"
Availability,"Hi @lbergelson ; Thanks for the response!; I tried building the current gatk from the master branch, and now the error message says something a little different. java.lang.ClassCastException: class htsjdk.samtools.BAMRecord cannot be cast to class java.lang.Comparable (htsjdk.samtools.BAMRecord is in unnamed module of loader 'app'; java.lang.Comparable is in module java.base of loader 'bootstrap'); 	at java.base/java.util.Arrays$NaturalOrder.compare(Arrays.java:105); 	at java.base/java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.base/java.util.TimSort.sort(TimSort.java:234); 	at java.base/java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746); 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); 	at java.base/java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:408); 	at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:736); 	at java.base/java.util.Arrays.parallelSort(Arrays.java:1183); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.base/java.lang.Thread.run(Thread.java:829); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); 		at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SA",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452463087:113,error,error,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452463087,1,['error'],['error']
Availability,"Hi @lbergelson ; Thanks so much for your help!; And thanks @droazen for add ""question"" label to my question!. I find that I can run BaseRecalibrator with -indelBQSR to generate a .grp file contains insertion type and deletion type, and with -enableBAQ to turn on BAQ function.; But I can't run ApplyBQSR with -indelBQSR, and the error information is:; ***********************************************************************. A USER ERROR has occurred: i is not a recognized option. ***********************************************************************. And if I run ApplyBQSR with no optional arguments, the output .bam file doesn't contain inserntion quality and deletion quality. How can I get a output bam file with indel quality?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2549#issuecomment-290631618:329,error,error,329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2549#issuecomment-290631618,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,Hi @lbergelson @psfoley. I have a user with a headerline error from GenomicsDB that may or may not be related. Would it be possible for one of you to take a glance and let me know if I should post a new bug report or if this PR will fix the issue? Thanks so much. ```; htsjdk.tribble.TribbleException: Could not decode field AS_RAW_ReadPosRankSum with value |5 of declared type Float; ```. User error is described at https://gatkforums.broadinstitute.org/gatk/discussion/11107/gatk4beta6-annotation-incompatibility-between-haplotypecaller-and-genomicsdbimport#latest.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3994#issuecomment-354931126:57,error,error,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3994#issuecomment-354931126,2,['error'],['error']
Availability,"Hi @lbergelson and thanks for considering my issue,. I'm sorry but I'm not familiar to artifactory dependency, if necessary I'll deepen about it; so I just inserted this dependency in the project's pom.xml; ```; <dependency>; <groupId>org.broadinstitute</groupId>; <artifactId>gatk</artifactId>; <version>4.beta.6-18-g2ee7724-20171025.162137-1</version>; </dependency>; ```; as reported in the [artifact repository](https://broadinstitute.jfrog.io/broadinstitute/webapp/#/artifacts/browse/tree/General/libs-snapshot-local/org/broadinstitute/gatk/4.beta.6-18-g2ee7724-SNAPSHOT/gatk-4.beta.6-18-g2ee7724-20171025.162137-1.jar), but when I execute `mvn clear install` in my folder project, I receive this error: ; ```; [ERROR] Failed to execute goal on project GATKpipe: ; Could not resolve dependencies for project uk.ac.ncl:GATKpipe:jar:0.0.1-SNAPSHOT: ; Could not find artifact org.broadinstitute:gatk:jar:4.beta.6-18-g2ee7724-20171025.162137-1 -> [Help 1]; ```. Am I making any mistake?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-339624024:702,error,error,702,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-339624024,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi @lbergelson thank you for looking into this. After a lot of trial and error that's what I figured as well. It would be interesting to know what assumptions are broken and if there is a way to avoid it or even to make the --remove-unused-alternates option compatible with FastaAlternateReferenceMaker.; What I still don't know is if the tool grabs the correct alternate when it sees more than 1 at a site. If yes, the above option is almost unnecessary, but it would be nice if it worked.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7433#issuecomment-904755077:73,error,error,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7433#issuecomment-904755077,1,['error'],['error']
Availability,Hi @lbergelson! I've just pushed an update to this branch that appears to resolve the test failures on my side.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4044#issuecomment-356109355:91,failure,failures,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4044#issuecomment-356109355,1,['failure'],['failures']
Availability,"Hi @lbergelson, would it be possible to have the indeximage for hs37d5 and hs38DH available for download ?; Thanks in advance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2414#issuecomment-282202005:82,avail,available,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2414#issuecomment-282202005,2,"['avail', 'down']","['available', 'download']"
Availability,"Hi @lbergelson,. We experienced the related issue in GATK 4.1.8 (it persisted since 4.1.5 or early version as far as we know) when running `FilterAlignmentArtifacts` in one of our cluster but not the other. We narrowed down the issue, using the CPU differences (the working one does not support AVX2), to `libgkl_smithwaterman.so`. Paths are shortened for clarity in the following commands. ```; bash faa.sh ; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /db/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter.test.vcf.gz --use-jdk-inflater true; 19:11:56.929 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:11:56.943 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 19:11:56.944 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 19:11:57.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 19, 2020 7:11:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:11:57.324 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.324 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:11:57.325 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:11:57.325 INFO Fil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:219,down,down,219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['down'],['down']
Availability,"Hi @ldgauthier,. Thanks for the fast reply. After updating GATK to the latest version and using GenomicsDB 0.9.2, the command works. However, now I get the previous error when I try to run this command:. gatk GenotypeGVCFs -R hsapiens.hs37d5.fasta -V gendb://gdbworkspace -O multi-germline-genotyped.vcf.gz. Here's the vidmap file that was generated with th GenomicsDBImport command:. {; ""fields"":[; {; ""name"":""ID"",; ""type"":[; ""char""; ],; ""length"":[; {; ""variable_length_descriptor"":""var""; }; ]; },; {; ""name"":""LowQual"",; ""type"":[; ""int""; ],; ""vcf_field_class"":[; ""FILTER""; ]; },; {; ""name"":""PASS"",; ""type"":[; ""int""; ],; ""vcf_field_class"":[; ""FILTER""; ]; },; {; ""name"":""AD"",; ""type"":[; ""Integer""; ],; ""vcf_field_class"":[; ""FORMAT""; ],; ""length"":[; {; ""variable_length_descriptor"":""R""; }; ]; },; {; ""name"":""GQ"",; ""type"":[; ""Integer""; ],; ""vcf_field_class"":[; ""FORMAT""; ],; ""length"":[; {; ""variable_length_descriptor"":""1""; }; ]; },; {; ""name"":""GT"",; ""type"":[; ""int""; ],; ""vcf_field_class"":[; ""FORMAT""; ],; ""length"":[; {; ""variable_length_descriptor"":""PP""; }; ]; },; {; ""name"":""MIN_DP"",; ""type"":[; ""Integer""; ],; ""vcf_field_class"":[; ""FORMAT""; ],; ""length"":[; {; ""variable_length_descriptor"":""1""; }; ]; },; {; ""name"":""PGT"",; ""type"":[; ""String""; ],; ""vcf_field_class"":[; ""FORMAT""; ],; ""length"":[; {; ""variable_length_descriptor"":""VAR""; }; ]; },; {; ""name"":""PID"",; ""type"":[; ""String""; ],; ""vcf_field_class"":[; ""FORMAT""; ],; ""length"":[; {; ""variable_length_descriptor"":""VAR""; }; ]; },; {; ""name"":""PL"",; ""type"":[; ""Integer""; ],; ""vcf_field_class"":[; ""FORMAT""; ],; ""length"":[; {; ""variable_length_descriptor"":""G""; }; ]; },; {; ""name"":""SB"",; ""type"":[; ""Integer""; ],; ""vcf_field_class"":[; ""FORMAT""; ],; ""length"":[; {; ""variable_length_descriptor"":""4""; }; ]; },; {; ""name"":""BaseQRankSum"",; ""type"":[; ""Float""; ],; ""vcf_field_class"":[; ""INFO""; ],; ""length"":[; {; ""variable_length_descriptor"":""1""; }; ]; },; {; ""name"":""ClippingRankSum"",; ""type"":[; ""Float""; ],; ""vcf_field_class"":[; ""INFO""; ],; ""length"":[; {; ""varia",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4514#issuecomment-372215582:165,error,error,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514#issuecomment-372215582,1,['error'],['error']
Availability,"Hi @ldgauthier,; I'm sorry for my delay with this topic. I was preparing myself to defend my Ph.D. (successfully done) and I wasn't looking GitHub these days... ; Anyway... we were using public human samples as input... There are 22 of them available on the web, we have the gvcfs on our servers generated by Dragen (these were generated by Dragen 3.6.3), but if you need them, I'll have to ask permission for sharing (let me know if it'd be easier for you, I must share them by email). Some of them ; ```; NA02718; NA07891; NA08618; NA09834; NA11661; NA12217; NA12878; NA14234; NA14626; NA14734; NA17819; NA18668; NA18949; NA20381; ```; Regarding chrY, in samples with XX karyotype, the DRAGEN pipeline makes a diploid variant call, but applies a ""PloidyConflict"" hard filter (and apparently all calls are either 0/0 or ./.), whereas the DRAGEN pipeline makes a haploid variant call for XY karyotype samples as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7690#issuecomment-1106778777:241,avail,available,241,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7690#issuecomment-1106778777,1,['avail'],['available']
Availability,Hi @lucifer9288. Thanks for pointing this out as its a very confusing message. Its an oversight in our logging that causes that error message and it should not mean that your execution of SplitNCigarReads was invalid. The inflater in this message is referring to the fact that one of the compressed blocks gets uncompressed with zero output bytes which happens with the very last block of a properly formatted BGZF file (as a your input bam should be). It looks like the issue is an errant message in the GKL intel inflater.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8126#issuecomment-1371196080:128,error,error,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8126#issuecomment-1371196080,1,['error'],['error']
Availability,"Hi @magicDGS. I went down this same path (composition of CountingFilter) when I originally implemented CountingReadFilter, but I abandoned it for the model we currently have. I think the current model is much simpler in a number of ways. This is an interesting problem, but I would say lets just implement a straight CountingVariantFilter/tests and not try to do the common implementation. . BTW, when looking at this PR I noticed two bugs in the existing code that we should make sure not to propagate to the Variant one (feel free to fix/test these as part of this PR):. - CountingReadFilter.resetFilterCount only resets the root filter count; it needs an override in BinOp to propagate the reset call to the lhs/rhs operands.; - there is a bug in the getSummary tests/code; you can see the fix [here](https://github.com/broadinstitute/gatk/commit/9ef1458271834aed9b64a5d66f94df33f025eafb). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272954313:21,down,down,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272954313,2,['down'],['down']
Availability,"Hi @mwalker174 Thank you. I re-runned pathseq pipline for different sample based on the same server and parameters. Some are success, some are failed with the same error like before. The size of the data are similar. ; I have checked my current limit of open files: ; $cat /proc/sys/fs/file-max; 78369196; I think itis enough for running one sample. Finally, I didn't solve this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-628256240:164,error,error,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-628256240,1,['error'],['error']
Availability,"Hi @nikolapeja6,; Sorry for the confusion. The pre-trained model that we distribute with the GATK was trained with those 7 default annotation keys so if you only change the `--info-annotation-keys` argument and don't pass a model expecting the different arguments you will get the error that you are seeing. In order to use a different set of arguments you would need to re-train a model that expects that set of annotations. This forum questions discusses what you would need to do to train a model on a set of custom annotations: https://gatkforums.broadinstitute.org/gatk/discussion/23821/possible-to-using-cnnscorevariants-with-pacbio-reads#latest",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5939#issuecomment-492687058:281,error,error,281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5939#issuecomment-492687058,1,['error'],['error']
Availability,"Hi @ruslan-abasov,. I believe your GermlineCNVCaller results should have inherited the correct dictionary from the count files. The issue is you created some GermlineCNVCaller shards (e.g., shard 4) with inappropriately ordered intervals (since these were instead ordered w.r.t. to the idiosyncratic dictionary you attached). However, I think if you just reshard and rerun GermlineCNVCaller for any such shards, you may be able to reuse most of your results. For example, you could take your shard 4 interval list, which contains intervals from chr18, chr19, and chr1, and reshard these intervals into two shards: 4a containing chr18-19 intervals, and 4b containing chr1 intervals. After rerunning 4a and 4b through GermlineCNVCaller, you should be able to use PostprocessGermlineCNVCalls to stitch together shards 4b, 1, 2, 3, and 4a, since these will be ordered w.r.t. the correct dictionary from the count files (i.e, they will contain intervals in the order chr1, chr10-19). Of course, you will want not want to perform this exact procedure; you'll want to generalize it to whatever will yield the correct order for all 10 of your shards across all contigs. Again, this may be error prone and I can't guarantee that it will be successful, since I haven't tried it myself. I would personally just rerun the pipeline. You might be able to cut down on runtime by using smaller shards (I believe we typically shard the entire genome into far more than 10 shards, which we usually run in parallel) and making sure you set parameters appropriately for WGS. @mwalker174 has the most experience running on WGS and should be able to provide you the latest recommendations, or you might be able to find them by searching GitHub or the GATK Forums. Your point is well taken about failing earlier, and I think I've outlined the best strategy above. It is impossible to catch all possible errors early, but for some we can certainly fail before the GermlineCNVCaller step.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720091802:1181,error,error,1181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720091802,3,"['down', 'error']","['down', 'error', 'errors']"
Availability,"Hi @samuelklee ; I updated my conda environment and it worked now. ; Sorry again for posting the error in github and not in the gatk forum, hopefully next time it will work. Thanks again; Stefan",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382736818:97,error,error,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382736818,1,['error'],['error']
Availability,"Hi @singha30, this error indicates that you are running GATK with the wrong version of Java. GATK 4.4 requires Java 17 to run, as described in https://github.com/broadinstitute/gatk#requirements",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8596#issuecomment-1836612620:19,error,error,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8596#issuecomment-1836612620,1,['error'],['error']
Availability,"Hi @stefandiederich,. My guess is that you may have created your GATK conda environment with an older version of the GATK that did not include the changes to the `PloidyModelWriter`. Can you try creating a new conda environment with 4.0.3.0 and re-running?. @vdauwera @sooheelee @chandrans Any idea what the forum error message is about?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382715149:314,error,error,314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382715149,1,['error'],['error']
Availability,"Hi @sujianed ; Can you try running the tool without the /bin/bash entry at the beginning of the command? That entry disables the proper initiation of the conda environment with the proper python objects therefore you get this error message. You can run the tool directly from the commandline like . `docker run --rm -it broadinstitute/gatk-dev:NVSCOREVARIANTS-PREVIEW-SNAPSHOT gatk NVScoreVariants ....`. Here is what happens when you have the entry. ```; $ docker run --rm -it broadinstitute/gatk-dev:NVSCOREVARIANTS-PREVIEW-SNAPSHOT /bin/bash; root@5d30f9f10f97:/gatk# python -c ""import scorevariants""; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; ModuleNotFoundError: No module named 'scorevariants'; root@5d30f9f10f97:/gatk# . ```. Without the entry here is the result; ```; $ docker run --rm -it broadinstitute/gatk-dev:NVSCOREVARIANTS-PREVIEW-SNAPSHOT ; This docker image is specially built as a pre-release preview image for running the new GATK tool NVScoreVariants, and comes with a Python environment specific to that tool. Other Python-based GATK tools cannot be run using this image: use the official GATK release images instead.; (nvscorevariants) root@0a9dcd961783:/gatk# python -c ""import scorevariants""; (nvscorevariants) root@0a9dcd961783:/gatk#; ```. See that the bottom entry has the proper conda environment nvscorevariants initialized.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8501#issuecomment-1697386717:226,error,error,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8501#issuecomment-1697386717,1,['error'],['error']
Availability,"Hi @tedsharpe ! . I also commented about it on the helpdesk but should probably reply directly here. The .bam file was aligned to a reference , the same reference I used to run the tool. I was wondering If the bam still contained unmapped reads and so used ; `samtools view -b -F 4` on the file to retain only mapped reads and re-run the GATK tool. However this did not improve the situation. Best,; Domniki. error log:. 22/03/11 06:13:57 INFO SparkUI: Stopped Spark web UI at http://10.222.0.104:4040; 22/03/11 06:13:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 22/03/11 06:13:58 INFO MemoryStore: MemoryStore cleared; 22/03/11 06:13:58 INFO BlockManager: BlockManager stopped; 22/03/11 06:13:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 22/03/11 06:13:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 22/03/11 06:13:58 INFO SparkContext: Successfully stopped SparkContext; 06:13:58.369 INFO FindBreakpointEvidenceSpark - Shutting down engine; [March 11, 2022 6:13:58 AM GMT] org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark done. Elapsed time: 3.28 minutes.; Runtime.totalMemory()=29312942080; java.lang.ArithmeticException: / by zero; at org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.removeUbiquitousKmers(FindBreakpointEvidenceSpark.java:640); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.addAssemblyQNames(FindBreakpointEvidenceSpark.java:507); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.gatherEvidenceAndWriteContigSamFile(FindBreakpointEvidenceSpark.; at org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.runTool(FindBreakpointEvidenceSpark.java:136); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:546); at org.broadinstitute.hellbender.engine.spark.SparkCommandLinePro",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7710#issuecomment-1064823936:409,error,error,409,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7710#issuecomment-1064823936,2,['error'],['error']
Availability,"Hi @tomwhite; thanks for pinging this upstream on https://github.com/bigdatagenomics/adam/issues/1093. As per my comment there, I believe that it's only the various RDD transforms in ADAM that are impacted by the Spark 2.0.0 shift. I believe that the issue that was reported here is a Scala 2.10 vs. 2.11 issue. The GATK build points at [ADAM 0.18.0 built for Scala 2.10](https://github.com/broadinstitute/gatk/blob/master/build.gradle#L128). If you shifted said dependency to `compile('org.bdgenomics.adam:adam-core_2.11:0.18.0')`, I believe the error above would disappear. However, this is just conjecture; I haven't tested it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2073#issuecomment-241779355:25,ping,pinging,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073#issuecomment-241779355,2,"['error', 'ping']","['error', 'pinging']"
Availability,"Hi @ycl6, please post your questions to the GATK forum at <http://gatkforums.broadinstitute.org/gatk>. . It has been removed in GATK4. GATK4 Mutect2 now takes a known population variants resource, e.g. gnomAD, with allele-specific frequencies. I've put such an example resource in the GATK bundle at <https://software.broadinstitute.org/gatk/download/bundle>, in the folder labeled `beta`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3207#issuecomment-312446583:342,down,download,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3207#issuecomment-312446583,1,['down'],['download']
Availability,"Hi Adam,. This is based on your feedback from Friday afternoon. ; Lots of issues are still to be addressed (multilevel collection, more test coverage, performance, etc).; If you are too busy to review, let me know and I'll bug @cwhelan or @tedsharpe or someone else. Branch travis log available [here](https://travis-ci.org/broadinstitute/gatk/builds/110702136). Thank you.; Steve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1514:285,avail,available,285,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1514,1,['avail'],['available']
Availability,"Hi Christina (@cristinaluengoagullo ),. The GenomicsDBImport in version 4.0.0.0 has a few bugs that you may or may not encounter, but I'd recommend you run GATK 4.0.1.0 or later. That's admittedly a very cryptic error message. We're working on doing a better job wrapping the GenomicsDB errors into readable GATK User Errors. In the meantime, I have a few ideas. Do you have any unusual annotations in that GVCF? The Vidmap takes information from the VCF header annotation descriptions to figure out how to store annotation values after they're parsed. It's complaining that something is wrong with the type of one of your annotations, so my guess is that there's an annotation that's not described correctly in the header. Can you send the gdbworkspace-gatk/vidmap.json file? A snippet of the input GVCF (ideally with the header) would be good too.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4514#issuecomment-371831740:212,error,error,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514#issuecomment-371831740,3,"['Error', 'error']","['Errors', 'error', 'errors']"
Availability,"Hi GATK Team. I got the following error when using MarkDupliactesSpark tool:. `A USER ERROR has occurred: Couldn't write file /media/Berechnungen/190218_NB501654_0110_AHT7VFBGX9/0046-19_Exom (Intelligenzminderung)/0046-19.dedup.bam because writing failed with exception File file:/media/Berechnungen/190218_NB501654_0110_AHT7VFBGX9/0046-19_Exom%20(Intelligenzminderung)/0046-19.dedup.bam.parts/header does not exist`. I looked at the specified path and the `header` file is in the right directory. ; Is it possible, that MDSpark replaces a space in the filepath to %20 and at the end does not replace it back to space, so it does not finde the `header` file?. Thanks in advance; Stefan . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23509/markduplicatespark-does-not-find-file-though-it-is-there/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5729:34,error,error,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5729,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi GATK developers:. Have 4 pacbio WGS bam files do to Haplotype calling. Each bam file was divided by chromosomes, but 3 parallele jobs failed due to java core dump:; - Syntax I ran was pretty basic, I also tried latest gatk version4.2.2.0, same result. Java version is ``` OpenJDK Runtime Environment (build 1.8.0_252-b09) ```; ```; /gatk-4.0.11.0/gatk --java-options ""-Xmx4G"" HaplotypeCaller \; -R GRCh38.p2.fa \; -I RT4_STD.bam \; -ERC GVCF \; -L chr16 \; -O RT4_STD.g.vcf \; -new-qual; ```; - Error message is also different; - First one is :; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002aaad9f1e54a, pid=7818, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libgkl_pairhmm_omp1890484777463615571.so+0x6954a] double compute_full_prob_avxd<double>(testcase*)+0x34a; #; # Core dump written. Default location: core or core.7818; #; # An error report file with more information is saved as:; # hs_err_pid7818.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; ```. -Second one is ; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00000035dfe84364, pid=160107, tid=0x00002aaaabdce700; #; # JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 ); # Problematic frame:; # C [libc.so.6+0x84364]; #; # Core dump written. Default location: core or core.160107; #; # An error report file with more information is saved as:; # hs_err_pid160107.log; #; # If you would like to submit a bug report, please visit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7515:498,Error,Error,498,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7515,2,"['Error', 'error']","['Error', 'error']"
Availability,"Hi GATK team,. I had error message as following with GATK4.1.0.0 on our local cluster: ; `; Using GATK jar /dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java1.8 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx5; g -jar /dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar SelectVariants -R /dsgmnt/llfs2/masterdata/geno/hg38/resources_broad_hg38_v0_Homo_sapiens_assembl; y38.fasta -L chr1 -V /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.gz -O /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joi; nt525_c1.SNP.VQSR.g.vcf.gz; 09:15:49.372 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/nati; ve/libgkl_compression.so; 09:15:51.131 INFO SelectVariants - ------------------------------------------------------------; 09:15:51.132 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.0.0; 09:15:51.132 INFO SelectVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:15:51.132 INFO SelectVariants - Executing as xhong@blade5-4-11.dsg.wustl.edu on Linux v2.6.32-573.12.1.el6.x86_64 amd64; 09:15:51.133 INFO SelectVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_31-b13; 09:15:51.133 INFO SelectVariants - Start Date/Time: June 27, 2019 9:15:49 AM CDT; 09:15:51.133 INFO SelectVariants - ------------------------------------------------------------; 09:15:51.133 INFO SelectVariants - ------------------------------------------------------------; 09:15:51.134 INFO SelectVariants - HTSJDK Version: 2.18.2; 09:15:51.134 INFO SelectVariants - Picard Version: 2.18.25; 09:15:51.134 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:15:51.135 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6021:21,error,error,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6021,1,['error'],['error']
Availability,"Hi GATK team,. I tried running your PathSeq pipeline (broadinstitute/gatk:4.1.8.0) on my cohort and almost half of the samples failed the scoring step with this error message:. `20/07/17 09:38:35 INFO NewHadoopRDD: Input split: file:/cromwell_root/fc-6e61d4b2-bdc8-4abd-bb94-18d8fa11d9b6/7c1b0faa-e956-4289-9e2d-4fb8b9eff6ff/PathSeqPipeline/0ca5578f-70d3-498e-b7cc-23590f0ab31f/call-PathSeqAlign/MMRF_2072_2_BM.microbe_aligned.paired.bam:33554432+33554432; 20/07/17 09:38:46 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 5); java.util.NoSuchElementException: next on empty iterator; 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:39); 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:37); 	at scala.collection.Iterator$$anon$13.next(Iterator.scala:469); 	at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$PeekingImpl.next(Iterators.java:1155); 	at org.broadinstitute.hellbender.utils.spark.SparkUtils.lambda$putReadsWithTheSameNameInTheSamePartition$7bd206b0$1(SparkUtils.java:190); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.execu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6319#issuecomment-660292360:161,error,error,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6319#issuecomment-660292360,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi GATK team,. I tried running your PathSeq pipeline (broadinstitute/gatk:4.1.8.0) on my cohort and almost half of the samples failed the scoring step with this error message:; `20/07/17 09:38:35 INFO NewHadoopRDD: Input split: file:/cromwell_root/fc-6e61d4b2-bdc8-4abd-bb94-18d8fa11d9b6/7c1b0faa-e956-4289-9e2d-4fb8b9eff6ff/PathSeqPipeline/0ca5578f-70d3-498e-b7cc-23590f0ab31f/call-PathSeqAlign/MMRF_2072_2_BM.microbe_aligned.paired.bam:33554432+33554432 20/07/17 09:38:46 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 5) java.util.NoSuchElementException: next on empty iterator at scala.collection.Iterator$$anon$2.next(Iterator.scala:39) at scala.collection.Iterator$$anon$2.next(Iterator.scala:37) at scala.collection.Iterator$$anon$13.next(Iterator.scala:469) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$PeekingImpl.next(Iterators.java:1155) at org.broadinstitute.hellbender.utils.spark.SparkUtils.lambda$putReadsWithTheSameNameInTheSamePartition$7bd206b0$1(SparkUtils.java:190) at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153) at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6709:161,error,error,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6709,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi Marissa - I think we're all in agreement that we'd like to find a way to make Intel-TF the default, but whether or not we can have CNNScoreVariants require AVX to run is less clear. Naturally, we'd prefer to not have to provide a custom TF distribution for a fallback, but there are 3 cases where we may not have a choice: user with old hardware, Travis/CI testing, and GCE. We may need to provide a fallback environment for those (I'll try to get resolution on that). If it turns out we do, I'm actually not suggesting the fallback be automatic (3 in your list), just that we have a graceful failure mode and an instructive error message. . In the meantime, there is still the issue that this PR fails to even build on Travis. It looks like it produces so much output building the Docker image that it exceeds the allowable Travis build log size. That will need to be resolved, and we'll also need to understand the impact of this change on the size of our Docker image, which is already large, and continues to be a challenge for us.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429451059:596,failure,failure,596,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429451059,4,"['error', 'failure']","['error', 'failure']"
Availability,"Hi Samuel,. Thanks for the reply. I have fixed that issue.; i am getting new issue while running ""DetermineGermlineContigPloidy"". Commandline: /home/ec2-user/data/gatk_cnv/gatk-4.1.4.0/gatk; DetermineGermlineContigPloidy --contig-ploidy-priors; ploidy_model/interval_list.tsv --interval-merging-rule OVERLAPPING_ONLY -L; preprocessed_intervals.interval_list --input sample.counts_ESI_17.tsv; --input sample.counts_ES_msc.tsv --output esi_ploidy --output-prefix; esi_cnvploidy --verbosity DEBUG. Error:; /home/ec2-user/miniconda3/envs/gatk/lib/python3.6/site-packages/h5py/__init__.py:36:; FutureWarning: Conversion of the second argument of issubdtype from `float`; to `np.floating` is deprecated. In future, it will be treated as; `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; Traceback (most recent call last):; File ""/tmp/cohort_determine_ploidy_and_depth.4606632406719651975.py"",; line 87, in <module>; sample_metadata_collection, args.sample_coverage_metadata); File; ""/home/ec2-user/miniconda3/envs/gatk/lib/python3.6/site-packages/gcnvkernel/io/io_metadata.py"",; line 78, in read_sample_coverage_metadata; sample_name, n_j, contig_list)); File; ""/home/ec2-user/miniconda3/envs/gatk/lib/python3.6/site-packages/gcnvkernel/structs/metadata.py"",; line 227, in add_sample_coverage_metadata; 'Sample ""{0}"" already has coverage metadata; annotations'.format(sample_name)); gcnvkernel.structs.metadata.SampleAlreadyInCollectionException: Sample ""$1""; already has coverage metadata annotations; 10:40:30.979 DEBUG ScriptExecutor - Result: 1; 10:40:30.980 INFO DetermineGermlineContigPloidy - Shutting down engine; [October 17, 2019 10:40:30 AM UTC]; org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy; done. Elapsed time: 0.93 minutes.; Runtime.totalMemory()=3354918912; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /tmp/; cohort_determine_ploidy",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217#issuecomment-543122687:495,Error,Error,495,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217#issuecomment-543122687,1,['Error'],['Error']
Availability,"Hi Stefan, how's it going.; Did you get anywhere with this one?; I'm getting a similar error but there's only one line with this genomic coordinate in my vcf. A USER ERROR has occurred: More then one variant context at position: X:523104. All the best; Phil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7249#issuecomment-881447572:87,error,error,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7249#issuecomment-881447572,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi Stefan,. If there is no obvious error (e.g., is `/media/Berechnungen/GATKTest/CN_transition_matrix_autosomal.tsvx` the correct filename, rather than `/media/Berechnungen/GATKTest/CN_transition_matrix_autosomal.tsv`? Does the file exist and is it correctly formatted?), then I would guess that this is likely an error with your nd4j configuration. Just to let you know, we have significantly revamped the both somatic and germline CNV pipelines for the release in January. If you would like a preview of the germline tool, you may want to look at this branch: https://github.com/broadinstitute/gatk/tree/sl_gcnv_ploidy_cli However, be aware that it is still under development.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3996#issuecomment-352760467:35,error,error,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3996#issuecomment-352760467,2,['error'],['error']
Availability,"Hi Ted, ; After we talked offline I went back and read the code again.; I still have some questions regarding the method; `static Cigar findLargeDeletions( final GATKRead read ) `. 1. It seems this will do redundant work in the following sense: for the same query sequence which has several alignments, each `GATKRead` (which is actually an alignment record) will be touched by this method, hence imagine one query sequence with two corresponding `GATKRead`s, the current implementation will emit two new `GATKRead`s. Seems unnecessary and double counting evidence.; 2. I don't see any check on same-chromosome-ness, i.e. `fields[0]`, which is formatted `SA:Z:<CHR_NAME>`.; 3. I see checking of overlap length on the query sequence between two alignment records, but I don't see a check of gap size on the query sequence between two alignment records. As we talked about this offline, it is a hard problem when the two alignment records are gapped away both on the reference and the query sequence, if you want to merge the two records into a single record.; 4. I am still not quite understanding line 1652 about why you are using the clip length to decide orders.; 5. Is there a reason to limit the del length to 2, which is the same as the threshold on `overlapLength`?. For the related method `recplaceCigar`, the method is not checking for negative values on `overlapLength`, which judged from `int Interval.overlapLength(Interval)` is possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6092#issuecomment-522145406:206,redundant,redundant,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6092#issuecomment-522145406,1,['redundant'],['redundant']
Availability,"Hi Tiffany,. That's fine because the fix wasn't until 4.1.5.0. On Mon, Mar 2, 2020 at 12:31 PM Tiffany Miller <notifications@github.com>; wrote:. > Another user; > <https://gatk.broadinstitute.org/hc/en-us/community/posts/360057968772-FilterMutectCalls-Error-java-lang-IllegalArgumentException-log10-p-Values-must-be-non-infinite-and-non-NAN>; > reported seeing this same issue for 4.1.4.1.; >; > —; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/6255?email_source=notifications&email_token=ACAMI2SZ4U3A456W743UTJLRFPUOXA5CNFSM4JKD626KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOENQGP5A#issuecomment-593520628>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACAMI2T42JWN4ESQQE6ZV4LRFPUOXANCNFSM4JKD626A>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6255#issuecomment-595346726:253,Error,Error-java-lang-IllegalArgumentException-,253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6255#issuecomment-595346726,1,['Error'],['Error-java-lang-IllegalArgumentException-']
Availability,"Hi [ShirelyI](https://github.com/ShirelyI),. Could you try running with the latest GATK release (4.4), and see if the issue persists? There have been a lot of fixes to GenomicsDB (which is where the error is coming from) between 4.2 and 4.4. @nalinigans / @mlathara, have either of you seen this error before when reading from GenomicsDB? What does it indicate?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-1648478289:199,error,error,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-1648478289,2,['error'],['error']
Availability,"Hi again, some updates! I've updated our PED with inferred rather than stated sex, which has resolved a number of these issues. As stated before, the samples which had incorrectly assigned sexes were not the named samples appearing the error logs, which made troubleshooting a challenge. . We're now running into issues with true sex aneuploidy, which the tool doesn't work for, as stated in [your docs page](https://gatk.broadinstitute.org/hc/en-us/articles/9570468120987-JointGermlineCNVSegmentation-BETA). Do you have a way to cleanly handle these sex aneuploidies so that we can retain calls for the rest of the genome, rather than excluding the sample(s) entirely? This might be a process question, so I'm going to raise separately on the GATK-SV repo. FYI @cassimons",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8834#issuecomment-2123742806:236,error,error,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8834#issuecomment-2123742806,1,['error'],['error']
Availability,"Hi again,; I tried installing java8 and switching to this version prior to running gatk. It runs and looks to be running the right Java, but spits out roughly the same error:. Thoughts?. /cold/drichard/gatk/./gatk --java-options ""-Xmx25g"" SplitNCigarReads \; -R /cold/drichard/VARIANTS/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam \; --tmp-dir /thing -O thing.bam; Using GATK jar /cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx25g -jar /cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar SplitNCigarReads -R /cold/drichard/VARIANTS/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam --tmp-dir /thing -O thing.bam; 15:34:59.974 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:35:00.220 INFO SplitNCigarReads - ------------------------------------------------------------; 15:35:00.226 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.3.0.0-44-g227bbca-SNAPSHOT; 15:35:00.226 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:35:00.226 INFO SplitNCigarReads - Executing as drichard@illuvatar on Linux v5.19.0-32-generic amd64; 15:35:00.226 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_362-8u362-ga-0ubuntu1~22.04-b09; 15:35:00.226 INFO SplitNCigarReads - Start Date/Time: March 2, 2023 3:34:59 PM EST; 15:35:00.226 INFO SplitNCigarReads - ------------------------------------------------------------; 15:35:00.226 INFO SplitNCigarReads - ------------------------------------------------------------; 15:35:00.227 INFO SplitNCigarReads - HTSJDK Version: 3.0.1; 15:35:00.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:168,error,error,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['error'],['error']
Availability,"Hi all, . I am using CNV detection with GATK v4.3.0.0 for quite a while very successfully. Now we changed the enrichment kit and I had to do a new model. Everything worked well for the model phase. . As I now run one sample against this model I got the following error at the CNV detection step:. ```gatk GermlineCNVCaller --run-mode CASE -contig-ploidy-calls /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_DGCP_noProbe-calls/ --model /media/Data/MasterV3/GCNV_noProbe-model/ --input /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 --output /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/ --output-prefix 0115-24_GCNV_noProbe --tmp-dir /media/Data/tmp/; Using GATK jar /usr/BioinfSoftware/GATK/4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /usr/BioinfSoftware/GATK/4.3.0.0/gatk-package-4.3.0.0-local.jar GermlineCNVCaller --run-mode CASE -contig-ploidy-calls /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_DGCP_noProbe-calls/ --model /media/Data/MasterV3/GCNV_noProbe-model/ --input /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 --output /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/ --output-prefix 0115-24_GCNV_noProbe --tmp-dir /media/Data/tmp/; 10:20:01.611 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/BioinfSoftware/GATK/4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:20:01.717 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:20:01.718 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0; 10:20:01.718 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:20:01.718 INFO GermlineCNVCaller - Executing as die9s@k-hg-srv3 on Linux v5.3.18-24.37-default am",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8740:263,error,error,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740,1,['error'],['error']
Availability,"Hi all, ; Is this problem solved yet? I have the same error ""A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1773992577:54,error,error,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1773992577,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi all,. Below error occurs trying to access Funcotator data source directory installed on lustre file system. We have a non-lustre mounted fs for cases like this, but I thought it was worth bringing up. ```; org.broadinstitute.hellbender.exceptions.GATKException: Unable to query the database for geneName: null; 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.cosmic.CosmicFuncotationFactory.createFuncotations(CosmicFuncotationFactory.java:244); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:404); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:316); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4413:15,error,error,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413,1,['error'],['error']
Availability,"Hi all. Apologies for writing in an old issue. ; has this been fixed?; With java 1.8 and GATK 4.1.3.0 I think I'm getting the same error (in this case with HaplotypeCallerSpark). Any idea on how to extend the size?. The errors are:. `org.broadinstitute.hellbender.exceptions.UserException: Max size of locatable exceeded. Max size is 5000, but locatable size is 8638. Try increasing shard size and/or padding. Locatable: Contig1:65711-74348; 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$5.computeNext(SparkSharder.java:293); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$5.computeNext(SparkSharder.java:281); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.TransformedIterator.hasNext(TransformedIterator.java:43); 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1811); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$5.hasNext(Iterators.java:547); 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1811); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2554#issuecomment-530773994:131,error,error,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2554#issuecomment-530773994,2,['error'],"['error', 'errors']"
Availability,"Hi all.; This looks like an issue presented by a forum post . [Haplotypecaller FORMAT:DP is affected by the interval in WGS](https://gatk.broadinstitute.org/hc/en-us/community/posts/27992393649051-Haplotypecaller-FORMAT-DP-is-affected-by-the-interval-in-WGS). User uploaded a toy data for us to test and I was able to recreate this issue under GATK 4.6.0.0. I have not tested it with any other versions. When whole contig is given as interval all variant sites in the multisample VCF is reported with DP value much less than what it is supposed to be in samples where no variation occur. Samples with variants are shown as expected DP. Setting ploidy 2 for this analysis restores the expected DP value for samples with HOMREF sites no matter what interval is used. Numbers can be seen in the figure as well as those variant contexts. . This is what user and I got with the whole contig given as interval. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	DA10_DA10	TDY1754_TDY1754; CP022325.1	69348	.	T	A	2920.63	.	AC=1;AF=0.500;AN=2;DP=85;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;QD=29.56;SOR=0.824	GT:AD:DP:GQ:PL	0:4,0:4:99:0,119	1:0,79:79:99:2931,0; ```; This is what comes when -L is set to `CP022325.1:69347-69349`. This is the same DP reported when ploidy is set to 2 no matter what interval is used. This is also the expected DP value. . ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	DA10_DA10	TDY1754_TDY1754; CP022325.1	69348	.	T	A	2920.63	.	AC=1;AF=0.500;AN=2;DP=98;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;QD=25.36;SOR=0.824	GT:AD:DP:GQ:PL	0:17,0:17:99:0,685	1:0,79:79:99:2931,0; ```. ![image](https://github.com/user-attachments/assets/269169f8-7de2-415c-ba60-8356937da561). User data is in the incoming folder with name `cmateusiak_20240805.tar.gz`. The reference genome is a fungal one from the below link. [Fungi reference C.NeoformansKN99](https://fungidb.org/common/downloads/release-68/CneoformansKN99/fasta/data/FungiDB-68_CneoformansKN99_Genome.fasta)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8943:1883,down,downloads,1883,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8943,1,['down'],['downloads']
Availability,"Hi all;; When validation runs on the GATK 4.0.0 release (congrats!) we're running into segfault issues on some `GenomicsDBImport` runs which look to be due to the length of the database path:; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f99a7642c5b, pid=7446, tid=0x00007f99fbfa0700; #; # JRE version: OpenJDK Runtime Environment (8.0_121-b15) (build 1.8.0_121-b15); # Java VM: OpenJDK 64-Bit Server VM (25.121-b15 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb8843204539247232071.so+0x4fdc5b] std::string::compare(char const*) const+0x1b; ```; Here is a self-contained test case that reproduces the issue:. https://s3.amazonaws.com/chapmanb/testcases/gatk/gatk4_genomicsdb_length.tar.gz. A standard small name and longer name of 105 characters work fine:; ```; gatk-launch --java-options '-Xms1g -Xmx2g' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path short_genomicsdb -L chr22:15069-15500 --variant Test1.vcf.gz --variant Test2.vcf.gz; gatk-launch --java-options '-Xms1g -Xmx2g' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path long_aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa_genomicsdb/works_aaaaaaaaaaaaaaaaaaaaaaaaaa -L chr22:15069-15500 --variant Test1.vcf.gz --variant Test2.vcf.gz; ```; But when you add an additional character, you trigger the segfault:; ```; gatk-launch --java-options '-Xms1g -Xmx2g' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path long_aaaaaa; aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa_genomicsdb/fails_aaaaaaaaaaaaaaaaaaaaaaaaaaa -L chr22:15069-15500 -; -variant Test1.vcf.gz --variant Test2.vcf.gz; ```; Thank you for looking at this and please let me know if I can provide any other information to help debug.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4160:208,error,error,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4160,1,['error'],['error']
Availability,"Hi everyone,. I put together as small a test case as I could here: https://www.dropbox.com/s/p7x3jt6lgxesc23/gatk-missingalt-testdata.tar.gz?dl=1. Some observations that might help track down the source of the bug:. 1. Subsetting the BAM file to within 100 bases of the call with the missing alt results in files that cannot reproduce this behavior, it needs to be a larger window.; 2. If you call individually instead of in batch mode none of the individual calls have the missing variant on this test data. This example dataset results in one call with a missing ALT:. ```; GL000216.1	67968	.	C	.	39.74	.	AN=38;DP=74;MMQ=60;MQ=59.82	GT:AD:DP	0/0:1:1	0/0:0:0	0/0:5:5	0/0:3:3	0/0:6:6	0/0:6:6	0/0:2:2	0/0:1:1	0/0:2:2	0/0:5:5	0/0:5:5	0/0:4:4	0/0:9:9	0/0:4:4	0/0:2:2	0/0:2:2	0/0:0:0	0/0:10:10	0/0:7:7; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5650#issuecomment-462148064:187,down,down,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5650#issuecomment-462148064,1,['down'],['down']
Availability,"Hi everyone. I try to run gatk 4.2.5.0 VariantAnnotator using gnomAD data. However I get this error message java.lang.IllegalStateException: Allele in genotype C not in the variant context [C*, CT] can you maybe advise whats going on? . java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx30G -jar /run/media/riadh/One Touch1/Analysis/gatk-4.2.4.1/gatk-package-4.2.5.0-local.jar VariantAnnotator -V PE69_chr3.vcf -R /run/media/riadh/One Touch/Reference_data_b38/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta --resource:gnomad /run/media/riadh/One Touch/Reference_data_b38/gnomad.genomes.v3.1.2.sites.chr3.vcf.bgz -E gnomad.nhomalt -E gnomad.ALT -E gnomad.AF -O PE69_ch3_vep_cadd_gnomad.vcf --resource-allele-concordance; 10:58:19.715 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/run/media/riadh/One%20Touch1/Analysis/gatk-4.2.4.1/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 17, 2022 10:58:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:58:19.796 INFO VariantAnnotator - ------------------------------------------------------------; 10:58:19.796 INFO VariantAnnotator - The Genome Analysis Toolkit (GATK) v4.2.5.0; 10:58:19.796 INFO VariantAnnotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:58:19.797 INFO VariantAnnotator - Executing as riadh@ikm-unix-1012.uio.no on Linux v5.16.12-200.fc35.x86_64 amd64; 10:58:19.797 INFO VariantAnnotator - Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1; 10:58:19.797 INFO VariantAnnotator - Start Date/Time: March 17, 2022 at 10:58:19 AM CET; 10:58:19.797 INFO VariantAnnotator - ------------------------------------------------------------; 10:58:19.797 INFO VariantAnnotator - -------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053:94,error,error,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053,1,['error'],['error']
Availability,"Hi februaryfang,; I am concerned about this question,too. I am focusing on pathseq currently, could we make some more discussion via email or wechat if you are available. Many thanks!; Best regards,; Kyle",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8339#issuecomment-1563260326:160,avail,available,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339#issuecomment-1563260326,1,['avail'],['available']
Availability,"Hi folks. @chandrans and I have laid out some plans towards updating GATK4 docs for the January 9 release. Our approach is to prioritize documentation around stable Best Practice Workflows. On the docket currently is the single stable workflow--germline SNP and indel calling from DNA data. We will of course update tool docs (excluding Spark and BWA tools) and supporting tutorials. Even for tools we are unfamiliar with, we aim to have at the least a basic description and an example command. Thanks for the documentation you have already done and the help you give us in updating these. If you are certain your workflow will be ready for the release, then please let us know immediately so we can plan accordingly. If your workflow will be ready later, then can you still give us an estimate for your release so we can plan ahead? Thanks. - @davidbenjamin, did I hear you correctly that you think somatic SNV and indel calling will be ready for the Jan 9 release?; - @samuelklee, I know major changes are currently afoot for somatic CNV. Will you make the Jan 9 release for the targeted exomes use-case? What about WGS?; - @mbabadi, is March, 2018 still the plan?; - @jonn-smith, what is the status on the Tool-That-Must-Not-Be-Named?; - @cwhelan @tedsharpe @SHuang-Broad, is SV on for next year or thereafter?. It would be most helpful to users if we also have validation of our workflows as applied to real data. Are there plans to make benchmarking stats available for each of your workflows?. Sheila and I have 30-man days we can give between us towards updating documentation by December 14. Besides Geraldine, we will rely on some of you to review further refinements to documentation from now to December 14. Thanks again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3769:1461,avail,available,1461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3769,1,['avail'],['available']
Availability,"Hi gatk team,. I just run a cfDNA sample and there is no variant being called, thus there is no "".stats"" file being generated so that when it comes to `filterMutectCalls`, it gives error. I wonder if it is wiser that we output an empty stats file, e.g.; ```; statistic	value; callable	0; ```. or simply reports . ```; ERROR: No callable variants detected!; ```. instead of reporting missing stats file? This would be more informative. Thanks!!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6170:181,error,error,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6170,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi gatk team,. I'm working with generating CNV for somatic with wdl, using this command:. `java -Xmx75G -Dconfig.file=gatk.conf -jar cromwell-46.1.jar run cnv_somatic_panel_workflow.wdl -i parameters.json `. But I got this error in which I don't know the exact reason for it:. ```; [2019-10-01 02:52:52,49] [info] Running with database db.url = jdbc:hsqldb:mem:e98d186c-96db-46ae-92e5-c326e7aa05d9;shutdown=false;hsqldb.tx=mvcc; [2019-10-01 02:53:01,19] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-10-01 02:53:01,20] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-10-01 02:53:01,31] [info] Running with database db.url = jdbc:hsqldb:mem:c4b3296a-4b73-4053-b6bf-d4eeb71c8956;shutdown=false;hsqldb.tx=mvcc; [2019-10-01 02:53:01,85] [info] Slf4jLogger started; [2019-10-01 02:53:02,22] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-876ccf5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-10-01 02:53:02,28] [info] Metadata summary refreshing every 1 second.; [2019-10-01 02:53:02,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,31] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-10-01 02:53:02,32] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-10-01 02:53:02,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-10-01 02:53:02,43] [info] SingleWorkflowRunnerActor: Version 46.1; [2019-10-01 02:53:02,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-10-01 02:53:02,49] [info] ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6189:223,error,error,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6189,4,"['error', 'failure', 'heartbeat']","['error', 'failureShutdownDuration', 'heartbeat', 'heartbeatInterval']"
Availability,Hi guys I am very interested in this discussion. It would be great to know:. 1. what were the results?; 2. is this contig-prior-probablity file you created available somehow?; 3. are the data for the samples you tested the pipeline on available?. Thank you so much in advance!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-649726755:156,avail,available,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-649726755,2,['avail'],['available']
Availability,"Hi hde08! Thanks for the answer. That is a bit weird indeed. When I use GenotypeGVCF -V gendb://07_genomicdb without setting -L option and I have a genomicDB with entire genome or with more scaffolds it works, but with only one scaffold it does not, and throws the error. In general, I don't understand what is the point of using GenomicDBimport if you have to recall again the intervals in GenotypeGVCF.; The DB is not named as the contig.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2220667000:265,error,error,265,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2220667000,1,['error'],['error']
Availability,"Hi lubocoix, did you fix the error? I got the same issue. Please let me know how did you fix the problem. Many Thanks!!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7147#issuecomment-938304267:29,error,error,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7147#issuecomment-938304267,1,['error'],['error']
Availability,"Hi mwalker174,; I tried both command lines. As lbergelson predicted, the one with --spark-runner LOCAL produces the same error as before (see below, could you explain me why?), while the one with --spark-runner SPARK runs smoothly. . Is this option ok with running a master-workers system? Can I use this option safely with . I have now a different issue with PathSeqPipelineSpark? As I tried, the first error solved, but I have another issue (I think it's better to open a new thred for that, since it is about an input file not found). Thank you! . ```; -bash-4.1$ ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt --spark-runner SPARK; Using GATK jar /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar; Running:; /home/int/eva/username/bin/spark-2.2.0-bin-hadoop2.7//bin/spark-submit --master spark://xx.xx.xx.xx:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:121,error,error,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,4,['error'],['error']
Availability,"Hi mwalker174. I tryed with CountReadsSpark, same problem indeed. I do not think it is a java version problem, as without the option --spark-master the software runs smoothly (I guess it uses an included spark and libraries set). So this command runs:; CountReadsSpark --input test_sample.bam --output output.readcount.txt --verbosity DEBUG. While this does not:; CountReadsSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --output output.readcount.txt --verbosity DEBUG ; And I get the error:; ```; 18/04/24 14:34:27 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/24 14:34:27 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 4.532 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:508,error,error,508,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494,4,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"Hi pbrennan13 and all,; I have same error in merging Mutect2.vcf files to one by using bcftools merge function. I am beginner for editing vcf files. . How did you change all 'A' and '1' -> '.' of Mutect2 vcf files?; Could you show me or write brief command here? . It would be great help for those who are beginner.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6931#issuecomment-749252178:36,error,error,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6931#issuecomment-749252178,1,['error'],['error']
Availability,"Hi team,. GATK have a GRCh38 version that available in Resource bundle. It is not clear if this version have a masked duplicates.; Could you provide a GRCH38 version, ready to use, with masked duplicates? that can deal with this issue that affect on variants recall in these regions, such as CBS gene. B.W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8043:42,avail,available,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8043,3,"['avail', 'mask']","['available', 'masked']"
Availability,"Hi team,. I'm trying to run `DetermineGermlineContigPloidy` on one sample that I have, but in `CASE` mode it requires `--model a_valid_ploidy_model_dir` in which here (https://gatkforums.broadinstitute.org/gatk/discussion/23471/the-parameter-model-of-determinegermlinecontigploidy-when-use-case-mode#latest). They said that it came from running `COHORT` mode first to have it, but in my case; I want to run `CASE` mode because I only have one sample, so to get --model from `COHORT` mode; I will not be able to have it since it couldn't run on one sample. `A USER ERROR has occurred: Bad input: Invalid combination of inputs: Running in cohort mode, but only a single sample was provided.`. So, how I could produce `-- model` for `CASE` mode to be used for one sample?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6162:564,ERROR,ERROR,564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6162,1,['ERROR'],['ERROR']
Availability,Hi team. ; This is an old topic from the forum . [LeftAlignIndels Alignments added out of order... Offending records are at [chr7:55268881] and [chr7:55268881]](https://gatk.broadinstitute.org/hc/en-us/community/posts/20834436731547-LeftAlignIndels-Alignments-added-out-of-order-Offending-records-are-at-chr7-55268881-and-chr7-55268881). ```; java.lang.IllegalArgumentException: Alignments added out of order in SAMFileWriterImpl.addAlignment for file:///Users/jcovino/Desktop/PLAA_2390/broken.bam. Sort order is coordinate. Offending records are at [chr7:55268881] and [chr7:55268881]; at htsjdk.samtools.SAMFileWriterImpl.assertPresorted(SAMFileWriterImpl.java:212); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:199); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); at java.base/java.lang.Thread.run(Thread.java:840); ```. The only current solution to this problem is feeding reads in queryname sorted way. When reads are coordinate sorted this error occurs. . There is a request from users that this tool should be able to work with coordinate sorted reads.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8975:1193,error,error,1193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8975,1,['error'],['error']
Availability,Hi thanks for your quick response.; I created a post and it can be found here: https://gatk.broadinstitute.org/hc/en-us/community/posts/360072760032-HaplotypeCaller-NullPointerException-Error.; I saw some similar posts but it seems that there is no solution.; Thanks again.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6292#issuecomment-682254421:186,Error,Error,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6292#issuecomment-682254421,1,['Error'],['Error']
Availability,"Hi there, sorry it was my mistake, there was one g.vcf file finished abnormally, that caused this error. After fixing this file, it is working gracefully. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6670#issuecomment-649205362:98,error,error,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6670#issuecomment-649205362,1,['error'],['error']
Availability,"Hi there,. I got a very similar error on the GABARAP gene. GATK version 4.3.0.0. I am running Funcotator on a vcf from illumina dragen. ```; htsjdk.samtools.SAMException: Query asks for data past end of contig. Query contig ENST00000571253.1|ENSG00000170296.10|OTTHUMG00000102156.4|OTTHUMT00000440082.2|GABARAP-204|GABARAP|837|UTR5:1-753|CDS:754-837| start:1 stop:854 contigLength:837; at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.getSubsequenceAt(CachingIndexedFastaSequenceFile.java:330); at org.broadinstitute.hellbender.engine.ReferenceFileSource.queryAndPrefetch(ReferenceFileSource.java:78); at org.broadinstitute.hellbender.engine.ReferenceDataSource.queryAndPrefetch(ReferenceDataSource.java:64); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.getFivePrimeUtrSequenceFromTranscriptFasta(GencodeFuncotationFactory.java:778); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createUtrFuncotation(GencodeFuncotationFactory.java:1619); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationOnSingleTranscript(GencodeFuncotationFactory.java:1025); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:847); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:831); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.lambda$createGencodeFuncotationsByAllTranscripts$0(GencodeFuncotationFactory.java:508); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.ja",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6345#issuecomment-1695460680:32,error,error,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6345#issuecomment-1695460680,1,['error'],['error']
Availability,"Hi y'all. I've been trying to run GATK GenotypeGVCFs but I keep encounter this error. I've increase memory but it does not seems to be enough. ; My script is ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.0.6.0-local.jar GenotypeGVCFs -ploidy 1 -R /project/uma_john_gibbons/john_gibbons/AORY_AFLA/REF/Aspergillus_oryzae.ASM18445v3.dna.toplevel.fa -O /project/uma_john_gibbons/chacon_vargas/AOR-AFLA/GATK/4-Consolidate_GVCF/combined_Chr1.vcf -V gendb://Chr1/. Any thoughts?. 15:06:40.794 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 05:40:05.630 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),2.0928187529999955,Cpu time(s),0.9772208090000016; [April 10, 2020 5:40:05 AM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 932.26 minutes.; Runtime.totalMemory()=31136546816; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculator.<init>(GenotypeLikelihoodCalculator.java:153); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:269); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.GeneralPloidyExactAFCalculator.computeLofK(GeneralPloidyExactAFCalculator.java:272); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.GeneralPloidyExactAFCalculator.calculateACConformationAndUpdateQueue(GeneralPloidyExactAFCalculator.java:187); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.GeneralPloidyExactAFCalculator.fastCombineMultiallelicPool(GeneralPloidyExactAFCalculator.java:148); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.GeneralPloidyExactAFCalculator.combineS",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5900#issuecomment-612103255:79,error,error,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5900#issuecomment-612103255,2,"['down', 'error']","['down', 'error']"
Availability,"Hi! I have the same issue as @chandrans.; When I run Mutect2 this is the error:; `(gatk) root@d387db9e4351:/Desktop# gatk Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; Using GATK jar /gatk/gatk-package-4.1.1.0-local.ja; Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.1.0-local.jar Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; 08:27:06.032 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.s; Apr 23, 2019 8:27:10 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngin; INFO: Failed to detect whether we are running on Google Compute Engine. 08:27:10.882 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.883 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.1.; 08:27:10.883 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/. 08:27:10.884 INFO Mutect2 - Executing as root@d387db9e4351 on Linux v4.9.125-linuxkit amd64. 08:27:10.884 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12. 08:27:10.885 INFO Mutect2 - Start Date/Time: April 23, 2019 8:27:05 AM UT; 08:27:10.885 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.886 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:73,error,error,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['error'],['error']
Availability,"Hi! I have used HaplotypeCaller to get gvcfs of a sample called xl4_70. They are gvcfs of scf1-128, scf1280-18, scf180-25 ... (use parameter ""-L"" to help with the division). Then i used CombineGvcfs to combine these 7 gvcfs with a sample into a gvcfs, but i got an error:. ```; Using GATK jar /home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar CombineGVCFs -R /db_students1/cc/genetic_map/snp_calling/bbv18h27rm.fa --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_1to128.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_1280to18.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_180to25.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_250to35.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_350to5.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_50to69.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_690to999.g.vcf.gz -O /db_students1/cc/gatk_out/raw_new52_off_xL4_70.g.vcf.gz; 17:52:22.080 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 11, 2020 5:52:23 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:52:23.841 INFO CombineGVCFs - ------------------------------------------------------------; 17:52:23.841 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.3.0-25-g8d88f6e-SNAPSHOT; 17:52:23.841 INFO CombineGVCFs - For support a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6368:265,error,error,265,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6368,1,['error'],['error']
Availability,"Hi! It would be great to see this issue addressed, as the error message that is generated is quite cryptic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6548#issuecomment-643229485:58,error,error,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6548#issuecomment-643229485,1,['error'],['error']
Availability,"Hi!. I am using the gatk tool AnalyzeSaturationMutagenesis to analyze some data produced with the MITE-seq technology. It works perfectly for my purpose, so I decided to include it as a step in my pipeline written in Nextflow. ; Strangely enough, when I try to run the SAME EXACT command line inside a Nextflow module, it gives a generic error for most of the samples (sometimes all of them, sometimes some of them). ; It looks like a random issue, because if I run the same code outside of Nextflow, it works perfectly on every sample. . I would really appreciate if someone may give me some hints on why this is occurring and, eventually, how to fix it. ## Bug Report. ### Affected tool(s) or class(es); AnalyzeSaturationMutagenesis . ### Affected version(s); gatk4-4.3.0.0. ### Description ; Here it follows the output from Nextflow that appears on screen:. ```; Error executing process > 'gatk_count (gatk)'. Caused by:; Process `gatk_count (gatk)` terminated with an error exit status (247). Command executed:. gatk AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//i ndex/genome.fa --orf 1-5610 -O ./MITE6_P1. Command exit status:; 247. Command output:; (empty). Command error:; WARNING: Not mounting requested bind point (already mounted in container): /home/tigem/f.panariello; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=f alse -Dsamjdk.compression_level=2 -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//index/genome.fa --orf 1-5610 -O ./MIT E6_P1; 09:36:03.173 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package -4.3.0.0-local.jar!/com/intel/gkl/native/libgk",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8357:338,error,error,338,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8357,3,"['Error', 'error']","['Error', 'error']"
Availability,"Hi!. I've stumbled upon the same error, on running Mutect2, the `.stats` is not generated and sometimes `FilterMutectCalls` will complain the file is missing and sometimes not. I find this quite a weird behaviour. The commands I use to run Mutect2 and FilterMutectCalls are the following:. ```; gatk Mutect2 --reference GRCh38.fa --input sample1_final.bam --input sample2_final.bam --normal-sample SAMPLE_NORMAL --output Mutect_unfiltered.vcf --germline-resource af-only-gnomad.hg38.vcf.gz --dont-use-soft-clipped-bases --panel-of-normals 1000g_pon.hg38.vcf.gz; ``` . ```; gatk FilterMutectCalls --variant Mutect_unfiltered.vcf --output Mutect.vcf --reference GRCh38.fa; ```. Best regards!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6170#issuecomment-760936135:33,error,error,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6170#issuecomment-760936135,1,['error'],['error']
Availability,"Hi!. Thank you for making this fix. . This is a blocking error for our [DepMap release](https://depmap.org/portal/) starting in 1 week. Would you have an ETA for when a dockerized version of gatk will be available with the fix?. Best,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1059127728:57,error,error,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1059127728,2,"['avail', 'error']","['available', 'error']"
Availability,"Hi, . I am experiencing exactly the same issue. I also run gatk on a cluster using singularity. When using --include-non-variant-sites the same error message is reported for all but the first chromosome. When exluding non variant sites it works fine. . ```; 05:04:16.405 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 05:04:16.556 INFO GenotypeGVCFs - ------------------------------------------------------------; 05:04:16.559 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.5.0.0; 05:04:16.559 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 05:04:16.563 INFO GenotypeGVCFs - Initializing engine; 05:04:16.929 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.5.1-84e800e; 16:04:16.979 INFO NativeGenomicsDB - pid=680685 tid=680686 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 16:04:16.979 INFO NativeGenomicsDB - pid=680685 tid=680686 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 16:04:16.979 INFO NativeGenomicsDB - pid=680685 tid=680686 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 05:04:17.059 INFO GenotypeGVCFs - Done initializing engine; 05:04:17.104 INFO ProgressMeter - Starting traversal; 05:04:17.105 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 05:04:17.124 INFO GenotypeGVCFs - Shutting down engine; [June 26, 2024 at 5:04:17 AM GMT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1126170624; java.lang.IllegalStateException: There are no sources based on those query parameters; at org.genomicsdb.reader.GenomicsDBFeature",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2191214079:144,error,error,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2191214079,1,['error'],['error']
Availability,"Hi, . I am testing the GATK beta 4.6 at the moment. It's a great improvement in time used to analyse data. Similarly I am interested in calling germline CNV events. I tried out the workflow with only two samples, just to find the right tools and see how it behaves. ; First I used `gatk-launch CalculateTargetCoverage` and `gatk-launch TargetCoverageSexGenotyper` and used the resulting files as input for `gatk-launch GermlineCNVCaller`. Unfortunatelly I got the following error when calling GermlineCNVCaller:. `A USER ERROR has occurred: Couldn't read file /media/Berechnungen/GATKTest/CN_transition_matrix_autosomal.tsvx. Error was: Could not read NDArray tsv file`. I found out that this error has something to do with Numpy. I have installed Numpy 1.13.1. ; Have you seen some error like this before?. Thanks in advance; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3996:474,error,error,474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3996,5,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Hi, . I am using `GATK ASEReadCounter` from GATK 4.2.0.0 and encounter the error . `A USER ERROR has occurred: More then one variant context at position: chrX:2774793`. As I googled, that means that there are 2 different entries with the same genomic coordinate in my VCF file. ; But by doing a `grep -P 'chrX\t2774793' 16-98_WGS.vcf | cut -f1-5` for my vcf file I only get one line:. `chrX 2774793 . G *,C`. The whole command I am using is as following . ```; gatk ASEReadCounter ; -R /media/Data/Referenzgenome/HG19/HG19.karyo.fasta ; -I /media/Data/Marco/16-98/16-98_iPSC_A.recal.rh.bam ; -I /media/Data/Marco/16-98/16-98_iPSC_B.recal.rh.bam ; -V /media/Data/Marco/16-98/16-98_WGS.vcf ; -O /media/Data/Marco/16-98/16-98_Out_iPSC_A_B.table ; -L chrX; ```. Do you have any idea how to solve this?; Thanks in advance ; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7249:75,error,error,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7249,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi, . I have the same issue reported here https://github.com/broadinstitute/gatk/issues/6766 that relates to CombineGVCFs. It was supposed to be fixed with the new version 4.1.9.0, however. I still get the same error when I tried it with the current version. . Here is the error report . > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/orange/reed/nhouse/Raw_seqs/SEQ9_samples/tmp; 11:30:50.248 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:30:50.478 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 26, 2020 11:30:50 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:30:50.791 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.791 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:30:50.792 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:30:50.792 INFO CombineGVCFs - Executing as nwijewardena@c3a-s8.ufhpc on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 11:30:50.792 INFO CombineGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_31-b13; 11:30:50.792 INFO CombineGVCFs - Start Date/Time: October 26, 2020 11:30:50 AM EDT; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.794 INFO CombineGVCFs - HTSJDK Version: 2.23.0; 11:30:50.794 INFO CombineGVCFs - Picard Version: 2.23.3; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRI",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6913:211,error,error,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6913,3,"['Redundant', 'error']","['Redundant', 'error']"
Availability,"Hi, . I was wondering if it's possible to use funcotator to annotate a VCF with structural variants. I'm trying to use funcotator (GATK 4.1.9.0) to annotate a VCF from manta but it fails with the first variant (SVTYPE=DEL):. ```; [...]; 17:07:32.003 ERROR GencodeFuncotationFactory - Problem creating a GencodeFuncotation on transcript ENST00000378191.5 for variant: chr1:4709859-185537688(G* -> <DEL>): Variant overlaps transcript but is not completely contained ; within it. Funcotator cannot currently handle this case. Transcript: ENST00000378191.5 Variant: [VC Unknown @ chr1:4709859-185537688 Q. of type=SYMBOLIC alleles=[G*, <DEL>] attr={CIEND=[0, 4], CIPOS=[0, 4], END=185537688, HOML; EN=4, HOMSEQ=TCCT, SOMATIC=true, SOMATICSCORE=141, SVLEN=-180827829, SVTYPE=DEL} GT=PR:SR 68,0:94,0 38,23:94,24 filters=; 17:07:32.003 WARN GencodeFuncotationFactory - Creating default GencodeFuncotation on transcript ENST00000378191.5 for problem variant: chr1:4709859-185537688(G* -> <DEL>); 17:07:32.009 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/0; 17:07:32.010 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/0; 17:07:32.010 INFO VcfFuncotationFactory - gnomAD_exome 2.1 cache hits/total: 0/0; 17:07:32.010 INFO VcfFuncotationFactory - gnomAD_genome 2.1 cache hits/total: 0/0; 17:07:32.136 INFO Funcotator - Shutting down engine; [14 January 2021 17:07:32 GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.77 minutes.; Runtime.totalMemory()=1426182144; java.lang.ArrayIndexOutOfBoundsException: 0; at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.getNonOverlappingAltAlleleBaseString(FuncotatorUtils.java:294); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.getGenomeChangeString(GencodeFuncotationFactory.java:2346); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationBuilderWithTri",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7040:250,ERROR,ERROR,250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7040,1,['ERROR'],['ERROR']
Availability,"Hi, . I'm trying to run ""cnv_germline_cohort_workflow"" from this workspace (https://app.terra.bio/#workspaces/help-gatk/Germline-CNVs-GATK4), and the workflow is keep failing at the ""CollectCounts"" step with the following error; in multiple shards. --------------------------------------------------------------------------------------------------------------------; A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; --------------------------------------------------------------------------------------------------------------------. This was weird since I set a correct index file as an input, but after some investigation, I realized that ; the error seemed to be occurring when the bucket path where BAM index was located was different from that of BAM file ; For example, if you look at this log below, . `Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam to /cromwell_root/dg.4DFC_3615e55e-6aa3-43e7-8d7b-6f2824071971/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam; Successfully activated service account; Will continue with download. Activated service account credentials for: [kd5mqbpsed8lzz0kyz9tvkht-3274@dcf-prod.iam.gserviceaccount.com]; Download complete!; 2021/09/29 15:46:14 Localizing input drs://dg.4DFC:ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1 -> /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Requester Pays project ID is Some(vanallen-firecloud-nih); Attempting to download gs://gdc-tcga-phs000178-controlled/KIRC/DNA/WXS/BI/ILLUMINA/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai to /cromwell_root/dg.4DFC_ab4d57fa-bfff-4a48-bd96-f2866ecfe0e1/C345.TCGA-A3-3373-11A-01D-1421-08.5.bam.bai; Successfully activated service account; Will continue with download. Activa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7487:222,error,error,222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7487,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi, . In the Mutect2.wdl file, the section of task definition for M2, I found the following argument maybe redundant, but I am not quite sure. -L ~{variants_for_contamination} . Best regards!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7731:107,redundant,redundant,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7731,1,['redundant'],['redundant']
Availability,"Hi, . Thanks @sooheelee for your offer, but I tried to do something ""exotic"" but maybe a bit too ambitious... And I got another ""problem"" ! :). ## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller. ### Affected version(s); - GATK4 4.0.5.1. ### Description ; Related to my previous error and the advices you gave me, I decided to split my data not per segments or batch but per chromosomes. I did 4 groups, with pretty much the same number of intervals in my target (Refseq bed file) for each group. The CollectReadCounts, DetermineGermlineContigPloidy and GermlineCNVCaller were a success. But something went wrong during the PostprocessGermlineCNVCalls. Here is an example with one of my four groups, with bam files containing only reads within the 1,3,9,17,Y and M chromosomes, a bed file containing only the intervals within the 1,3,9,17,Y and M chromosomes and a fasta reference file containing only the chromosomes 1,3,9,17,Y and M sequences. #### Steps to reproduce; Try to do a PostprocessGermlineCNVCalls with not all the autosomal chromosomes. #### Command. ```/home/tintest/miniconda2/bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar PostprocessGermlineCNVCalls -L Refseq_GrCh38_1-3-9-17-Y-M.bed -R hg38_1-3-9-17-Y-M.fasta --calls-shard-path GermlineCNVCaller/GermlineCNVCaller-calls/ --contig-ploidy-calls DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls/ --model-shard-path GermlineCNVCaller/GermlineCNVCaller-model --sample-index 274 --autosomal-ref-copy-number 2 --allosomal-contig Y --output-genotyped-intervals intervals/SAMPLE_274_PostprocessGermlineCNVCalls_interval.vcf --output-genotyped-segments segments/SAMPLE_274_PostprocessGermlineCNVCalls_segments.vcf```. #### Output; ```14:21:35.446 INFO NativeLibraryLoader - Loading libgkl_compression.so fro",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053#issuecomment-409558231:296,error,error,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053#issuecomment-409558231,1,['error'],['error']
Availability,"Hi, ; I am experimenting with submitting a PrintReadsSpark job to a yarn spark cluster in AWS. I run the job with the following command. ```; spark-submit --deploy-mode cluster --class org.broadinstitute.hellbender.Main --deploy-mode cluster --master yarn gatk-package-4.alpha.2-248-gcd449bf-SNAPSHOT-spark.jar PrintReadsSpark -I hdfs://chr1.bam -O hdfs://output.bam; ```. I can see from the output files that the job finished successfully, however the cluster tells me that it failed. It shows the following error message:. ```; 17/05/05 06:03:53 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); ```. I believe this may be due to the `System.exit(0)` statement at line 144 in hellbender.Main, though I am not sure. . Here is a more complete snippet from the stderr log. . ```; 17/05/05 06:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1; 17/05/05 06:03:52 WARN DFSClient: Caught exception ; java.lang.InterruptedException; 	at java.lang.Object.wait(Native Method); 	at java.lang.Thread.join(Thread.java:1249); 	at java.lang.Thread.join(Thread.java:1323); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546); 17/05/05 06:03:52 INFO FileOutputCommitter: Saved output of task 'attempt_20170505060336_0011_r_000001_0' to hdfs://ip-172-30-0-86.ec2.internal:8020/output.bam.parts/_temporary/0/task_20170505060336_0011_r_000001; 17/05/05 06:03:52 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 1921 bytes result sent to driver; 17/05/05 06:03:52 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 10260 ms on localhost (executor driver) (1/4); 17/05/05 06:03:53 INFO FileOutputCommitter: Saved output of task 'attempt_20170505060336_0011_r_000000_0' to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2666:509,error,error,509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666,1,['error'],['error']
Availability,"Hi, ; I need to run BaseRecalibrator as a part of the preprocessing of my RNAseq bam files before variant calling. But I experience difficulties with memory! Here is the error I get:. ```; 22:30:25.477 INFO BaseRecalibrator - Start Date/Time: March 8, 2024 at 10:30:25 PM GMT; 22:30:25.477 INFO BaseRecalibrator - ------------------------------------------------------------; 22:30:25.477 INFO BaseRecalibrator - ------------------------------------------------------------; 22:30:25.477 INFO BaseRecalibrator - HTSJDK Version: 4.1.0; 22:30:25.478 INFO BaseRecalibrator - Picard Version: 3.1.1; 22:30:25.478 INFO BaseRecalibrator - Built for Spark Version: 3.5.0; 22:30:25.478 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:30:25.478 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:30:25.478 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:30:25.478 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:30:25.478 INFO BaseRecalibrator - Deflater: IntelDeflater; 22:30:25.478 INFO BaseRecalibrator - Inflater: IntelInflater; 22:30:25.478 INFO BaseRecalibrator - GCS max retries/reopens: 20; 22:30:25.479 INFO BaseRecalibrator - Requester pays: disabled; 22:30:25.479 INFO BaseRecalibrator - Initializing engine; WARNING 2024-03-08 22:30:25 SamFiles The index file /mnt/storage/users/dockworker/mpedersen/work/RNAseq_variant_call/work/d6/362957b6215ad2e8193c27c895d42d/VR0024SA.withoutERCCs.withRG.markedDup.splitNcigar.bai was found by resolving the canonical path of a symlink: VR0024SA.withoutERCCs.withRG.markedDup.splitNcigar.bam -> /mnt/storage/users/dockworker/mpedersen/work/RNAseq_variant_call/work/d6/362957b6215ad2e8193c27c895d42d/VR0024SA.withoutERCCs.withRG.markedDup.splitNcigar.bam; 22:30:25.631 INFO FeatureManager - Using codec VCFCodec to read file file://1000G_phase1.snps.high_confidence.hg38.vcf.gz; 22:30:25.754 INFO FeatureManager - Using codec VCFCodec to read",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8726:170,error,error,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8726,1,['error'],['error']
Availability,"Hi, ; This is what i got when i run the command gatk --help ; (base) ameni@ameni-Aspire-A315-55G:~/Documents/pharmacogenomics$ gatk --help. Usage template for all tools (uses --spark-runner LOCAL when used with a Spark tool); gatk AnyTool toolArgs. Usage template for Spark tools (will NOT work on non-Spark tools); gatk SparkTool toolArgs [ -- --spark-runner <LOCAL | SPARK | GCS> sparkArgs ]. Getting help; gatk --list Print the list of available tools. gatk Tool --help Print help on a particular tool. Configuration File Specification; --gatk-config-file PATH/TO/GATK/PROPERTIES/FILE. gatk forwards commands to GATK and adds some sugar for submitting spark jobs. --spark-runner <target> controls how spark tools are run; valid targets are:; LOCAL: run using the in-memory spark runner; SPARK: run using spark-submit on an existing cluster ; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after -- ; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated ; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the ; java JVM at runtime. ; Java options MUST be passed inside a single string with space-separated values. --debug-port <number> sets up a Java VM debug agent to listen to debugger connections on a; particular port number. This in turn will add the necessary java VM arguments; so that you don't need to explicitly indicate these using --java-options.; --debug-suspend sets the Java VM debug agent up so that the run get immediatelly suspended; waiting for a debugger to connect. By default the port number is 5005 but; can be customized using --debug-port.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8280:439,avail,available,439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8280,1,['avail'],['available']
Availability,"Hi, ; When I used the `enable-all-annotations` option to run Mutect2, it failed.; ```; ~/software/gatktools/gatk-4.1.3.0/gatk Mutect2 \; -R ~/database/hg19/gatk_bundle/ucsc.hg19.fasta \; -I sample.recalibrated.bam -tumor sample -L tmp.bed \; --germline-resource ~/af-only-gnomad.raw.sites.hg19.vcf.gz \; -O gatk4p1p3p0.enable-all-annotations.vcf --enable-all-annotations true; ```; Error logs:; ```; ...; 15:52:06.727 INFO Mutect2 - Shutting down engine; [December 12, 2019 3:52:06 PM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=2494038016; org.broadinstitute.hellbender.exceptions.GATKException: Reference coordinate corresponds to a non-existent base in the read. This should never happen -- check read with alignment start: 6257722 and cigar: 5H134M; at org.broadinstitute.hellbender.utils.read.ReadUtils.getReadCoordinateForReferenceCoordinate(ReadUtils.java:830); at org.broadinstitute.hellbender.utils.read.ReadUtils.getReadCoordinateForReferenceCoordinate(ReadUtils.java:761); at org.broadinstitute.hellbender.utils.read.ReadUtils.getReadCoordinateForReferenceCoordinateUpToEndOfRead(ReadUtils.java:679); at org.broadinstitute.hellbender.tools.walkers.annotator.BaseQualityRankSumTest.getReadBaseQuality(BaseQualityRankSumTest.java:43); at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.AS_BaseQualityRankSumTest.getElementForRead(AS_BaseQualityRankSumTest.java:57); at org.broadinstitute.hellbender.tools.walkers.annotator.RankSumTest.getElementForRead(RankSumTest.java:108); at org.broadinstitute.hellbender.tools.walkers.annotator.RankSumTest.fillQualsFromLikelihood(RankSumTest.java:86); at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.AS_RankSumTest.annotate(AS_RankSumTest.java:47); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:278); at org.broadinstitute.hellbender.tools.walkers.mutec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6314:382,Error,Error,382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6314,2,"['Error', 'down']","['Error', 'down']"
Availability,"Hi, ; for those looking to run containers within a multi-user HPC environment, running a container with default root privileges presents a potential data security risk. Adding something like :. RUN useradd -ms /bin/bash gatk; WORKDIR /home/gatk; USER gatk. to the Docker file would greatly reduce the risk and bring the current containers in line with general best practice, e.g https://medium.com/@mccode/processes-in-containers-should-not-run-as-root-2feae3f0df3b. There should be no downsides to running in this manner. Singularity could help but the current configuration will be picked up and prevented from running by any site using a container security scanner, e.g. Aqua.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377:486,down,downsides,486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377,1,['down'],['downsides']
Availability,"Hi, @jamesemery .I downloaded the `GTF` file for `basic gene annotation` in the CHR regions from gencode. The version I obtained was `Release 43 (GRCh38.p13)`. After making some modifications as follow, I managed to run SVAnnotate without encountering any errors. ; ```; sed 's/; tag ""Ensembl_canonical""//g' gencode.v43.basic.annotation.gtf|sed 's/; tag ""overlaps_pseudogene""//g'|sed 's/; tag ""readthrough_gene""//g'|sed 's/; tag ""artifactual_duplication""//g' > gencode.v43.basic.modified_annotation.gtf; ```; However, the tool still fails to provide meaningful annotation information. The output file remains unchanged compared to the original file. Based on the SVAnnotate output as follow, I suspect that the issue might be caused by 'Current Locus unmapped.' ; ```; 16:11:10.044 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/Division/1user/2_Exome/Tools/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:11:10.072 INFO SVAnnotate - ------------------------------------------------------------; 16:11:10.074 INFO SVAnnotate - The Genome Analysis Toolkit (GATK) v4.4.0.0; 16:11:10.074 INFO SVAnnotate - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:11:10.074 INFO SVAnnotate - Executing as user@localhost.localdomain on Linux v3.10.0-1160.90.1.el7.x86_64 amd64; 16:11:10.074 INFO SVAnnotate - Java runtime: Java HotSpot(TM) 64-Bit Server VM v17.0.7+8-LTS-224; 16:11:10.075 INFO SVAnnotate - Start Date/Time: 2023年7月5日 CST 下午4:11:10; 16:11:10.075 INFO SVAnnotate - ------------------------------------------------------------; 16:11:10.075 INFO SVAnnotate - ------------------------------------------------------------; 16:11:10.075 INFO SVAnnotate - HTSJDK Version: 3.0.5; 16:11:10.075 INFO SVAnnotate - Picard Version: 3.0.0; 16:11:10.076 INFO SVAnnotate - Built for Spark Version: 3.3.1; 16:11:10.076 INFO SVAnnotate - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:11:10.076 INFO SVAnnotate - ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8394#issuecomment-1621377138:19,down,downloaded,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8394#issuecomment-1621377138,2,"['down', 'error']","['downloaded', 'errors']"
Availability,"Hi, @nalinigans ; By using the same data, procedure and software like @baoxingsong, I found that the super-indel(the length is 34461688 at chromosome 9; 10668738 at chromosome 10 respectively) lead to the same `Buffer overflow` error and could you tell me if `GenotypeGVCFs` can identify >10M indels and what parameters can set it ? thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1367030412:228,error,error,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1367030412,1,['error'],['error']
Availability,"Hi, GATK contributors. Thank you for the great software!. I had looked for the answer several days. . Question:; Several samples are new sequenced and `g.vcf` was called using `GATK` with `-ERC GVCF` models.; I want to add this new sample variants into the existing VCF file (no g.vcf avaliable) that downloaded from other researchers.; I have tested some commands like `CombineGVCFs`, `MergeVcfs` but all failed.; Could you give some advices?; Any respone would be helpful. Thanks ~",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7292:301,down,downloaded,301,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7292,1,['down'],['downloaded']
Availability,"Hi, GATK team! I'm working on GATK WGS somatic CNV calling pipeline. . When I tried gatk --java-options ""-Xmx2800g"" ModelSegments --denoised-copy-ratios ${tumor}.denoisedCR.tsv --allelic-counts ${tumor}.allelicCounts.tsv --normal-allelic-counts ${normal}.allelicCounts.tsv --output-prefix ${tumor} -O ${outdir}, I got this type of error:. 10:00:18.408 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/acct-medliuyb/medliuyb-user1/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 10, 2022 10:00:18 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:00:18.544 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.544 INFO ModelSegments - The Genome Analysis Toolkit (GATK) v4.2.0.0; 10:00:18.544 INFO ModelSegments - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:00:18.544 INFO ModelSegments - Executing as medliuyb-user1@huge2.pi.sjtu.edu.cn on Linux v3.10.0-1062.el7.x86_64 amd64; 10:00:18.545 INFO ModelSegments - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 10:00:18.545 INFO ModelSegments - Start Date/Time: January 10, 2022 at 10:00:18 AM CST; 10:00:18.545 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.545 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.545 INFO ModelSegments - HTSJDK Version: 2.24.0; 10:00:18.545 INFO ModelSegments - Picard Version: 2.25.0; 10:00:18.545 INFO ModelSegments - Built for Spark Version: 2.4.5; 10:00:18.545 INFO ModelSegments - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7633:331,error,error,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7633,1,['error'],['error']
Availability,"Hi, I also face this problem:. ```; Runtime.totalMemory()=8598323200`; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.3561179774649616878';source('/mnt/filename.snps.plots.R');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ▆; 1. ├─base::source(""/mnt/filename.snps.plots.R""); 2. │ ├─base::withVisible(eval(ei, envir)); 3. │ └─base::eval(ei, envir); 4. │ └─base::eval(ei, envir); 5. └─ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ├─ggplot2::continuous_scale(...); 7. │ └─ggplot2::ggproto(...); 8. │ └─rlang::list2(...); 9. └─scales::pal_seq_gradient(low, high, space); 10. └─scales::pal_gradient_n(c(low, high), space = space); 11. └─lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. └─lifecycle:::deprecate_stop0(msg); 13. └─rlang::cnd_signal(...); Execution halted. ```; My versions of R and packages are; R = 4.2.3; ggplot2 = 3.5.0 . Did you already find a solution to this problem?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8664#issuecomment-2046888579:287,Error,Error,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664#issuecomment-2046888579,1,['Error'],['Error']
Availability,"Hi, I am encountering a similar error attempting to run `GenotypeGVCFs` in `gatk v4.1.2.0`. It runs very briefly and writes a handful of variants from a single scaffold to the output file but then exits with `java.lang.ArrayIndexOutOfBoundsException` (see below). I have also tried adding the `-L` flag and an interval list, which performs similarly but outputs variants from a different scaffold. Any idea why this is happening or what I can do to overcome this problem? I have run `GenomicsDBImport` and `GenotypeGVCFs` successfully in the past (same version, same computer) on a different dataset, so I'm not sure what about this data is causing the problem. Any guidance is much appreciated!. Thanks,; Jessie. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /nfs/data1/jsalt/3RAD/colinus_virginianus_13May2017_V3Fw6_newchrom.fasta -V gendb://odont_cyr_8_snp_db -O odont_cyr_8_snp_db.vcf; 14:59:47.866 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 03, 2020 2:59:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:59:59.674 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:59:59.675 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 14:59:59.675 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:00:09.686 INFO GenotypeGVCFs - Executing as jsalt@mustard on Linux v3.10.0-957.1.3.el7.x86_64 amd64; 15:00:09.686 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:00:09.687 INFO GenotypeGVCFs - Start Date/Time: F",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640:32,error,error,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640,2,['error'],['error']
Availability,"Hi, I am getting similar errors but I can get output file. I wonder what this error message actually means? Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6292#issuecomment-682144031:25,error,errors,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6292#issuecomment-682144031,2,['error'],"['error', 'errors']"
Availability,"Hi, I am working with WES data with 130 samples. I've been following GATK4 best practices and also using the GRCh38 reference from the GATK bundle. I've been able to pre-process all the samples and to use Haplotypecaller for the 130 samples, then I proceed to merge all into a single gVCF file to then perform a join-call of SNPs and INdels. However, I got the following error message when using ""GenotypeGVCF"" ; Thank you; Cristian. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); GATK v4.0.5.2; ### Description ; 12:37:00.202 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 12:37:00.306 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home-1/cvalenc1@jhu.edu/apps/GATK4/gatk-4.0.5.2/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libg; kl_compression.so; 12:37:00.524 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.524 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 12:37:00.524 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:37:00.529 INFO GenotypeGVCFs - Executing as cvalenc1@jhu.edu@compute0207 on Linux v2.6.32-696.28.1.el6.x86_64 amd64; 12:37:00.530 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 12:37:00.530 INFO GenotypeGVCFs - Start Date/Time: July 12, 2018 12:37:00 PM EDT; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Version: 2.16.0; 12:37:00.530 INFO GenotypeGVCFs - Picard Version: 2.18.7; 12:37:00.530 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:37:00.531 INFO GenotypeGVCFs - HTSJDK Defau",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5009:371,error,error,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009,2,"['Redundant', 'error']","['Redundant', 'error']"
Availability,"Hi, I do not recall seeing any of those errors during the GenomicsSBImport phase nor can I find any errors readily logged, although I can recreate the database for Chromosome 3 and tell you for sure if I find any errors logged. I am running GenomicsDBImport on a Linux v3.10.0-1127.19.1.el7.x86_64 amd64 server - and yes, I can rerun a debugged version.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-758364641:40,error,errors,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-758364641,3,['error'],['errors']
Availability,"Hi, I downloaded the newest version of GATK [""https://github.com/broadinstitute/gatk.git""] and I combined g.vcf files into vcf with the command; <pre>; gatk GenomicsDBImport --sample-name-map /work7_P1/GATK_RegionCall/Variant_call/cohort.sample_map --reader-threads 5 --batch-size 50 --genomicsdb-workspace-path /work7_P1/GATK_RegionCall/DBimport/chr01_directory --intervals chr01; </pre> . From the perspective of site 1660261, we have a missing C from 1660261 to 1660272. However, from site 1660263, we have a long deletion of ""CTCTCTCTC"", which is a conflict with information from 1660261. As I was trying to construct a personalized genome, tools I know can only deal with the first variant while ignoring overlapping variant. A good way to deal this might be isolating specific sample and trim the same tailing bases, for example, converting GTAAC->GAAC to GT->G. <pre>; chr01 <b>1660261</b> . TCTCTCTCTCTC TTCTCTCTCTC,T,* 93054.98 . AC=382,4,2;AF=0.070,7.294e-04,3.647e-04;AN=5484;BaseQRankSum=0.947;DP=21527;ExcessHet=-0.0000;FS=0.000;InbreedingCoeff=0.6279;MLEAC=463,3,1;MLEAF=0.084,5.470e-04,1.823e-04;MQ=58.57;MQRankSum=0.00;QD=27.98;ReadPosRankSum=0.00;SOR=0.533 GT:AD:DP:GQ:PGT:PID:PL:PS 0|1:4,14,0,0:18:99:<b>0|1:1660261_TC_T</b>:507,0,114,519,156,675,519,156,675,675:1660261; chr01 1660262 . C *,T 6047.70 . AC=388,26;AF=0.070,4.712e-03;AN=5518;BaseQRankSum=1.38;DP=21571;ExcessHet=-0.0000;FS=0.000;InbreedingCoeff=0.5827;MLEAC=466,19;MLEAF=0.084,3.443e-03;MQ=59.84;MQRankSum=0.00;QD=2.47;ReadPosRankSum=0.431;SOR=0.510 GT:AD:DP:GQ:PGT:PID:PL:PS 0|1:4,14,0:18:99:<b>0|1:1660261_TC_T</b>:507,0,114,519,156,675:1660261; chr01 <b>1660263</b> . TCTCTCTCTC TTCTCTCTC,T,* 120442.78 . AC=102,382,6;AF=0.018,0.069,1.085e-03;AN=5530;BaseQRankSum=0.916;DP=21553;ExcessHet=-0.0000;FS=0.000;InbreedingCoeff=0.6783;MLEAC=119,455,4;MLEAF=0.022,0.082,7.233e-04;MQ=58.72;MQRankSum=0.00;QD=32.35;ReadPosRankSum=0.00;SOR=0.482 GT:AD:DP:GQ:PGT:PID:PL:PS <b>0|2</b>:4,0,14,0:18:99:<b>0|1:1660261_TC_T</b>:50",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5944:6,down,downloaded,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5944,1,['down'],['downloaded']
Availability,"Hi, I encountered a similar error attempting to run GenotypeGVCFs in gatk v4.1.4.1. ; It ran very briefly and writed few variants to the output file but then exited with java.lang.ArrayIndexOutOfBoundsException. . I solved the problem (I created a merged vcf) by performing bgzip (and tabix) of each gvcf file before running GenomicsDBImport and GenotypeGVCFs.; I don't know if it could be a solution also for your issues. command:; for P in my_gvcf.list; do; bgzip -c $P.g.vcf > $P\.g.vcf.gz. tabix -p vcf $P.g.vcf.gz; done. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport --genomicsdb-workspace-path my_database --sample-name-map sample_map --batch-size 50 --consolidate true --tmp-dir=tmp/ --reader-threads 5 -L file.bed. gatk --java-options ""-Xmx12g -Xms12g"" GenotypeGVCFs -R hg19.fa -V gendb://my_database -O global.vcf --new-qual --tmp-dir=tmp/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-583260728:28,error,error,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-583260728,1,['error'],['error']
Availability,"Hi, I encountered the following error while running GATK.; It is hard for me to say what exactly is wrong, and extensive searching has not been helpul. Thanks in advance!. ```; gatk ValidateVariants -V ../../data/geno/phased/chr1-22.phased.rename.reheader.vcf.gz -R ../../../../index/hg19.fa.gz; Using GATK jar ~/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar ValidateVariants -V ../../data/geno/phased/chr1-22.phased.rename.reheader.vcf.gz -R ../../../../index/hg19.fa.gz; 19:53:34.379 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 25, 2020 7:53:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:53:34.606 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.606 INFO ValidateVariants - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:53:34.606 INFO ValidateVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:53:34.607 INFO ValidateVariants - Executing as zepengmu@midway2-login1.rcc.local on Linux v3.10.0-1127.8.2.el7.x86_64 amd64; 19:53:34.607 INFO ValidateVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 19:53:34.607 INFO ValidateVariants - Start Date/Time: October 25, 2020 7:53:34 PM CDT; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - ------------------------------------------------------------; 19:53:34.607 INFO ValidateVariants - HTSJDK Version: 2.22.0; 19:53:34.607 INFO ValidateVarian",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:32,error,error,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['error'],['error']
Availability,"Hi, I got the same problem wih **4.1.6.0** : https://gatk.broadinstitute.org/hc/en-us/community/posts/360060174372-Haplotype-Caller-4-1-6-0-java-lang-IllegalStateException-Smith-Waterman-alignment-failure-. ```. java.lang.IllegalStateException: Smith-Waterman alignment failure. Cigar = 275M with reference length 275 but expecting referenc; e length of 303 ref = GGACTGGTCATCAGCTACCCCGAGACCAGGACTGGTCATCAGCTACCCCGAGACCAGGACTGGTCATCAGCTACCCCGAGACCAGGACTGGTCATCAGCTACCCCG; AGACCAGGACTGGTCATCAGCTACCCCGAGAACAGGACTGGTCATCAGCTACCCCGAGACCAGGACTGGTCATCAGCTACCCCGAGACCAGGACTGGTCATCAGCTACCCCGAGACCAGGACTGGTCA; TCAGCTACTCCGAGACCAGCATGGAGAGGTTTGCTGATGGTTGGCTGACTGCTAGTGTGAGCACTTGTC path GGACTGGTCATCAGCTACCCCGAGACCAGGACTGGTCATCAGCTACCCCGAGA; CCAGGACTGGTCATCAGCTACCCCGAGACCAGGACTGGTCATCAGCTACCCCGAGAACAGGACTGGTCATCAGCTACCCCGAGAACAGGACTGGTCATCAGCTACCCCGAGACCAGGACTGGTCATCA; GCTACCCCGAGACCAGGACTGGTCATCAGCTACTCCGAGACCAGCATGGAGAGGTTTGCTGATGGTTGGCTGACTGCTAGTGTGAGCACTTGTC; at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.findBestPaths(ReadTh; readingAssembler.java:354); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAn; dHaplotypeCall(ReadThreadingAssembler.java:196); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(Rea; dThreadingAssembler.java:146); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCalle; rUtils.java:269); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.ja; va:541); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:212); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(Assemb",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6490#issuecomment-605486643:197,failure,failure,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6490#issuecomment-605486643,2,['failure'],['failure']
Availability,"Hi, I got this as well but with Mutect2 (gatk 4.1.6.0):. java.lang.IllegalStateException: Smith-Waterman alignment failure. Cigar = 348M with reference length 348 but expecting reference length of 378 ref = CATACGCGTATACACACAATATAGGC; ATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAG; GCATTGCATACGCGTATACACACAAAATAGGCATTGTATACGCGTATACACACAAAATAGGCATTGTATACGCGTATACACACAAAATAGGCATTGTATACGCGTATACACACAAAATAGGCATTGTATACGCGTATACAATATAGGCATTGTATACGCGTATACAATATAGGC pat; h CATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGC; ATTGCATACGCGTATACACACAAAATAGGCATTGCATACGCGTATACACACAAAATAGGCATTGTATACGCGTATACACACAAAATAGGCATTGTATACGCGTATACACACAAAATAGGCATTGTATACGCGTATACAATATATGCATTGTATACGCGTATACAATATATGC; at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.findBestPaths(ReadThreadingAssembler.java:354); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:196); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:146); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:269); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:226); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:299); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GAT",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6490#issuecomment-605700399:115,failure,failure,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6490#issuecomment-605700399,1,['failure'],['failure']
Availability,"Hi, I want to use HaplotypeCaller and GenotypeGVCFs to call SNPs and meet a problem and my GATK version is v3.3-0-g37228af . Here is my script and I have 427 sample：. $JAVA -Xmx8g -jar $GATK -T HaplotypeCaller -R Chr06.fa -I $NOW/${RIL}.final.bam -ERC GVCF -o $NOW/${RIL}.raw.g.vcf --genotyping_mode DISCOVERY -variant_index_type LINEAR -variant_index_parameter 128000 -nct 24; $JAVA -Xmx4g -jar $GATK -T GenotypeGVCFs -nt 24 -R $REF/Chr06.fa \; --variant $NOW/w-1.raw.g.vcf \; --variant $NOW/w-10.raw.g.vcf \; --variant $NOW/w-100.raw.g.vcf \; -o KF427.raw.vcf. I got a error like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7315:571,error,error,571,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi, I'm interested in your keras model for variant calling. Is there anyway I can download it to study it?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4511:82,down,download,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4511,1,['down'],['download']
Availability,"Hi, are there any available updates on this bug ? ; I have run into it many times with gatk-4.1.0.0.; Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-463926822:18,avail,available,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-463926822,1,['avail'],['available']
Availability,"Hi, everyone~ Is this problem solved now? It seems that I've encounted similiar problems. I'm using GATK4.2 and hg38 data. 11:43:25.661 ERROR GencodeFuncotationFactory - Problem creating a GencodeFuncotation on transcript ENST00000441716.2 for variant: chr6:167976552-167976594(ACAGTGGGGGTCATTCCCCCTGCAGTGTGTTGGGAGGAGGAGG* -> A): Variant overlaps transcript but is not completely contained within it. Funcotator cannot currently handle this case. Transcript: ENST00000441716.2 Variant: [VC Unknown @ chr6:167976552-167976594 Q. of type=INDEL alleles=[ACAGTGGGGGTCATTCCCCCTGCAGTGTGTTGGGAGGAGGAGG*, A] attr={AS_FilterStatus=SITE, AS_SB_TABLE=[43, 26|2, 2], DP=94, ECNT=1, GERMQ=93, MBQ=[31, 20], MFRL=[288, 110], MMQ=[60, 60], MPOS=56, NALOD=1.37, NLOD=6.17, POPAF=4.6, ROQ=93, TLOD=10.97} GT=GT:AD:AF:DP:F1R2:F2R1:SB 0/1:46,4:0.07:50:14,3:10,0:28,18,2,2 0/0:23,0:0.041:23:8,0:5,0:15,8,0,0 filters=; 11:43:25.661 WARN GencodeFuncotationFactory - Creating default GencodeFuncotation on transcript ENST00000441716.2 for problem variant: chr6:167976552-167976594(ACAGTGGGGGTCATTCCCCCTGCAGTGTGTTGGGAGGAGGAGG* -> A); 11:44:04.904 INFO ProgressMeter - chr8:677091 4.5 3000 666.0; 11:45:35.226 INFO ProgressMeter - chr11:62279639 6.0 4000 665.6; 11:46:54.284 INFO ProgressMeter - chr15:19905537 7.3 5000 682.4; 11:48:12.767 WARN FuncotatorUtils - createAminoAcidSequence given a coding sequence of length not divisible by 3. Dropping bases from the end: 2 (size=293, ref allele: G); 11:48:16.949 ERROR GencodeFuncotationFactory - Problem creating a GencodeFuncotation on transcript ENST00000379751.5 for variant: chr20:3786474-3786537(TGGGGCCCATCCCGGCGCGCCCCCCGCCCCGGGGCCCGGCGCCGCCGCCGCCGCCCCGGGGCGG* -> T): Cannot yet handle indels starting outside an exon and ending within an exon.; 11:48:16.949 WARN GencodeFuncotationFactory - Creating default GencodeFuncotation on transcript ENST00000379751.5 for problem variant: chr20:3786474-3786537(TGGGGCCCATCCCGGCGCGCCCCCCGCCCCGGGGCCCGGCGCCGCCGCCGCCGCCCCGGGGCGG* -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-887961422:136,ERROR,ERROR,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-887961422,1,['ERROR'],['ERROR']
Availability,"Hi, getting the same error trying to run this new mutect2 pipeline on CCLE. We will not annotate mutect2 with funcotator in the meantime but would also be very useful to us if this problem is solved! (it impacts ~15% of our samples). Thank you @jonn-smith !",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-943666506:21,error,error,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-943666506,1,['error'],['error']
Availability,"Hi, i can't install gatk via conda/mamba. couldyou pls help; pls see steps that i took. ```; $conda config --add channels conda-forge; $conda config --add channels bioconda; $conda config --add channels defaults; $conda config --set channel_priority strict; ```. install command; ```; bash:iscxf001:/data1/greenbab/users/ahunos/apps/gatk-4.5.0.0 1023 $ conda env create -n gatk -f gatkcondaenv.yml; ```. ```; Channels:; - conda-forge; - defaults; - bioconda; Platform: linux-64; Collecting package metadata (repodata.json): done; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - conda-forge::typing_extensions==4.1.1; - conda-forge::theano==1.0.4; - pkgs/main::tensorflow==1.15.0; - conda-forge::scipy==1.0.0; - conda-forge::scikit-learn==0.23.1; - conda-forge::python==3.6.10; - bioconda::pysam==0.15.3; - conda-forge::pymc3==3.1; - conda-forge::pip==21.3.1; - conda-forge::pandas==1.0.3; - conda-forge::numpy==1.17.5; - conda-forge::mkl-service==2.3.0; - conda-forge::mkl==2019.5; - conda-forge::matplotlib==3.2.1; - conda-forge::keras==2.2.4; - conda-forge::joblib==1.1.1; - pkgs/main::intel-openmp==2019.4; - conda-forge::h5py==2.10.0; - conda-forge::dill==0.3.4. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/r/linux-64; - https://conda.anaconda.org/bioconda/linux-64; - https://conda.anaconda.org/bioconda; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https:/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8838:613,avail,available,613,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8838,1,['avail'],['available']
Availability,"Hi, recently I was trying expand the annotation data source when using funcotator, however, the document didn't give much information or example. Now I was trying to add CADD to the data source folder. After running the funcotator, I got the error:. ```; org.broadinstitute.hellbender.exceptions.GATKException: Error initializing feature reader for path file: funcotator_dataSources.v1.6.20190124s/cadd/hg19/cadd.config; at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:353); at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:305); at org.broadinstitute.hellbender.engine.FeatureDataSource.&lt;init&gt;(FeatureDataSource.java:256); at org.broadinstitute.hellbender.engine.FeatureManager.addToFeatureSources(FeatureManager.java:234); at org.broadinstitute.hellbender.engine.GATKTool.addFeatureInputsAfterInitialization(GATKTool.java:957); at org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils.createAndRegisterFeatureInputs(DataSourceUtils.java:328); at org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils.createDataSourceFuncotationFactoriesForDataSources(DataSourceUtils.java:277); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.onTraversalStart(Funcotator.java:774); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1037); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223:242,error,error,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223,2,"['Error', 'error']","['Error', 'error']"
Availability,"Hi, regarding making --linked-de-bruijn-graph the default, I wanted to share that I had recently run mutect2 (gatkv4.2.6.1) on a larger cohort of samples with that option, some of which had variant calls from a previous mutect2 run (gatkv4.1.0.0) without this option. I noticed that plenty of known cancer drivers (e.g. KRAS p.G12C or PIK3CA p.E545* or BRAF p.V600*) that were present in a substantial number of samples (>10%ish) in the old calls were completely absent in the new calls. I had to add the option --recover-all-dangling-branches to recover those known hotspot mutations. They also all have very sufficient coverage (O(100)x) and high VAF to make them obvious true positives. I'd expect from mutect2 to be always able to call presence or absence of known hotspot mutations, so you should either look into further testing/debugging the linked de Bruijn graph option or also make it a default to recover all dangling branches.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7809#issuecomment-1125355262:514,recover,recover-all-dangling-branches,514,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7809#issuecomment-1125355262,3,['recover'],"['recover', 'recover-all-dangling-branches']"
Availability,"Hi, thank you very much for the reply! . The content in the interval with the problem looks like this: ; chr9$number (a folder); chr10$number (a folder); chr8$number (a folder); chr7$number (a folder); callset.json; vidmap.json; vcfheader.vcf; _tiledb_workspace.tdb. If within each chr, the order is right, I wonder if the problem comes later: ; When CreateSomaticPanelOfNormals or GenotypeGVCFs reads the output from the GenomicsDBImport, the reading order is wrong? Although, I could not manually manage the order. . The error I got is that the output vcf file from CreateSomaticPanelOfNormals or GenotypeGVCF has chr 10 ahead of chr 7. ; (CreateSomaticPanelOfNormals or GenotypeGVCF reads the GenomicsDBImport outputs)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8416#issuecomment-1652145479:523,error,error,523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8416#issuecomment-1652145479,1,['error'],['error']
Availability,"Hi, thanks for the quick reply! The reason is indeed in the fasta file. I unzipped it when running another program but then re-zipped using gzip instead of bgzip. The error message disappears when I used bgzip to compress my fasta file again.; Thanks again!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911#issuecomment-716831825:167,error,error,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911#issuecomment-716831825,1,['error'],['error']
Availability,"Hi, there:. I am using **R v4.3.4, scales v1.3.0, ggplot2 v3.4.4**. Can you please kindly let me know how to resolve the issue mentioend above: ; `The space argument of pal_gradient_n() only supports be ""Lab"" as of scales 0.3.0.`. I hope that I don't need to download my R version, that will make a lot of other scripts not work. Thanks!; JH",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8664#issuecomment-2210206426:259,down,download,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664#issuecomment-2210206426,1,['down'],['download']
Availability,"Hi, when I executed the BwaAndMarkDuplicatesPipelineSpark command, there are some issues, I am not sure the issue related to the Spark environment or to the files I used. The ""ucsc.hg19.fasta.img"" is generated by the ""gatk-launch BwaMemIndexImageCreator ucsc.hg19.fasta"" command. The ""1982.unmapped.bam"" is generated by the picard command FastqToSam.jar :1234: . ```; ""java -jar /opt/NfsDir/BioDir/picard-tools-1.119/FastqToSam.jar F1=/opt/NfsDir/UserDir/lvxy/pipeline_test/BRCA/1982.R1.clean.fastq.gz F2=/opt/NfsDir/UserDir/lvxy/pipeline_test/BRCA/1982.R2.clean.fastq.gz V=Standard O=/opt/NfsDir/UserDir/wujh/1982.unmapped.bam SM=R1""; ```. Are there some examples I can follow?; ------------------------; ```; [sun@tele-1 download]/opt/NfsDir/BioDir/GATK4/gatk/gatk-launch BwaAndMarkDuplicatesPipelineSpark --bwamemIndexImage hdfs:///user/sun/ucsc.hg19.fasta.img -I hdfs:///user/sun/1982.unmapped.bam -R hdfs:///user/sun/ucsc.hg19.fasta -O hdfs:///user/sun/17F02897_17F02897M_WES_img.bwa.bam -- --sparkRunner SPARK --sparkMaster yarn --sparkSubmitCommand spark2-submit --driver-memory 4G --num-executors 4 --executor-cores 6 --executor-memory 16G --conf spark.dynamicAllocation.enabled=false; Using GATK jar /opt/NfsDir/BioDir/GATK4/gatk/build/libs/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar; Running:; spark2-submit --master yarn --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.kryoserializer",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:723,down,download,723,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['down'],['download']
Availability,"Hi, when I run gatk Funcotator, A USER ERROR has occurred; ```; Using GATK jar /export/.conda/envs/wes/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /export/bioinfo-team/home/liuhw/.conda/envs/wes/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar Funcotator --data-sources-path /export2/liuhw/software/gatk_Funcotator/funcotator_dataSources.v1.8.hg38.20230908s/ -V /export2/liuhw/wes_test//Mutect2_filter/K001137N_somatic_filtered.vcf.gz -R /Shared_Software/ref_genome/GATK_GRCh38/Homo_sapiens_assembly38.fasta --ref-version hg38 -O /export2/liuhw/wes_test//annotate/K001137N.funcotator.maf --output-file-format MAF; 02:55:31.904 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/export/bioinfo-team/home/liuhw/.conda/envs/wes/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 02:55:32.062 INFO Funcotator - ------------------------------------------------------------; 02:55:32.062 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.5.1-0.0.3; 02:55:32.062 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:55:32.062 INFO Funcotator - Executing as liuhw@RichardLi-Lab01 on Linux v5.4.0-74-generic amd64; 02:55:32.062 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_412-b08; 02:55:32.063 INFO Funcotator - Start Date/Time: July 12, 2024 2:55:31 AM EDT; 02:55:32.063 INFO Funcotator - ------------------------------------------------------------; 02:55:32.063 INFO Funcotator - ------------------------------------------------------------; 02:55:32.063 INFO Funcotator - HTSJDK Version: 2.15.1; 02:55:32.063 INFO Funcotator - Picard Version: 2.18.2; 02:55:32.063 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:55:32.063 INFO Funcotator - HTSJDK Defaults.USE_AS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8913:39,ERROR,ERROR,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8913,1,['ERROR'],['ERROR']
Availability,"Hi, you have probably deleted out the previous comment with the `No space left on device` error, but was wondering if you could check if you have enough space, import to a new workspace and turn on `--genomicsdb-shared-posixfs-optimizations` with GenomicsDBImport?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-759831424:90,error,error,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-759831424,1,['error'],['error']
Availability,"Hi,. Any updates on this? I'm running into the same error with v4.0.1.2. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4312#issuecomment-365632438:52,error,error,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4312#issuecomment-365632438,1,['error'],['error']
Availability,"Hi,. First I filter snp.raw.vcf with `VariantFiltration` command below: ; `gatk-4.1.2.0/gatk --java-options ""-Djava.io.tmpdir=/tmp/"" VariantFiltration -R genome.fa -V snp_rmnan.raw.vcf --filter-expression ""QUAL < 30.0 || QD < 2.0 || FS > 60.0 || MQ < 40.0 || SOR > 4.0 || ReadPosRankSum < -8.0"" --filter-name ""my_snp_filter"" --missing-values-evaluate-as-failing true -O snp_rmnan.raw.vcf.tmp.vcf `. This command runs successfully. But when I'm using `SelectVariants` command to extact the filtered site:; `gatk-4.1.2.0/gatk --java-options ""-Djava.io.tmpdir=/tmp/"" SelectVariants -R genome.fa -V snp_rmnan.raw.vcf.tmp.vcf --exclude-filtered -O snp_rmnan.raw.vcf.filter.vcf `. I get this java ERROR below, even without the wrong line number and do not know how to deal with it......... o(╥﹏╥)o，Thank you very much!. ~~~; 11:15:52.195 INFO ProgressMeter - Chr01:15144308 19.9 541000 27161.2; 11:16:04.187 INFO ProgressMeter - Chr01:15388212 20.1 547000 27189.6; 11:16:12.515 INFO SelectVariants - Shutting down engine; [May 9, 2019 11:16:12 AM CST] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 20.37 minutes.; Runtime.totalMemory()=2814377984; java.lang.NumberFormatException: For input string: ""1,0""; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); at java.lang.Integer.parseInt(Integer.java:580); at java.lang.Integer.parseInt(Integer.java:615); at htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:734); at htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132); at htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158); at htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148); at htsjdk.variant.variantcontext.GenotypesContext.iterator(GenotypesContext.java:465); at org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants.initalizeAlleleAnyploidIndicesCache(Sele",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5929:691,ERROR,ERROR,691,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5929,1,['ERROR'],['ERROR']
Availability,"Hi,. I am having the error below when I try to run PrintReadsSpark using an external spark cluster. ```; ./gatk-launch PrintReadsSpark -I ~/storage/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam -O ~/storage/output.bam -- --sparkRunner SPARK --sparkMaster spark://192.168.1.110:7077; Using GATK jar /home/centos/gatk-4.beta.5/gatk-package-4.beta.5-spark.jar; Running:; spark-submit --master spark://192.168.1.110:7077 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/centos/gatk-4.beta.5/gatk-package-4.beta.5-spark.jar PrintReadsSpark -I /home/centos/storage/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam -O /home/centos/storage/output.bam --sparkMaster spark://192.168.1.110:7077; 05:27:50.924 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 05:27:51.034 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/centos/gatk-4.beta.5/gatk-package-4.beta.5-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [October 3, 2017 5:27:51 AM UTC] PrintReadsSpark --output /home/centos/storage/output.bam --input /home/centos/storage/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam --sparkMaster spark://192.168.1.110:7077 --readValidationStringe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3651:21,error,error,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3651,1,['error'],['error']
Availability,"Hi,. I am on GATK v4.3.0.0 (CentOS) and would like to implement GermlineCNVCaller in my work. In an attempt to set up the conda environment (gcnvkernel) from zip using the command `conda env create -f gatkcondaenv.yml`, I got an error like this: `Found conflicts! Looking for incompatible packages.` . I also tried installing with tar, but I couldn't find gatkPythonPackageArchive.zip as required in the yml. Any help on this issue is much appreciated!. Java version: 1.8.0_201",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8091:229,error,error,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8091,1,['error'],['error']
Availability,"Hi,. I am trying to call germline CNVs for a set of samples. After running DetermineGermlineContigPloidy and GermlineCNVCaller, I am using PostprocessGermlineCNVCalls to generate the VCF files with CNV calls. The ""interval"" VCF files are generated successfully. But I got the following error message when segmenting contigs:. org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /tmp/shulik7/segment_gcnv_calls.2338024416841754264.py --ploidy_calls_path /scratch/users/shulik7/test_GATK_CNV/Postprocess/../DetermineGermlineContigPloidy/model/test_run-calls/ --model_shards /scratch/shulik7/test_GATK_CNV/Postprocess/../GermlineCNVCaller/cnvs/test_run-model --calls_shards /scratch/shulik7/test_GATK_CNV/Postprocess/../GermlineCNVCaller/cnvs/test_run-calls --output_path /tmp/shulik7/gcnv-segmented-calls28280883609685538 --sample_index 0; Stdout: 11:32:16.728 INFO segment_gcnv_calls - Loading ploidy calls...; 11:32:16.729 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:32:16.730 INFO segment_gcnv_calls - Instantiating the Viterbi segmentation engine...; 11:32:18.585 INFO gcnvkernel.postprocess.viterbi_segmentation - Assembling interval list and copy-number class posterior from model shards...; 11:32:25.158 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 11:32:27.543 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano forward-backward function...; 11:32:34.406 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano Viterbi function...; 11:32:40.598 INFO gcnvkernel.postprocess.viterbi_segmentation - Compiling theano variational HHMM...; 11:32:42.862 INFO gcnvkernel.postprocess.viterbi_segmentation - Processing sample index: 0, sample name: test_sample_0...; 11:32:43.631 INFO gcnvkernel.postprocess.viterbi_segmentation - Segmenting contig (1/24) (contig name: 1)... Stderr: Traceback (most recent call last):; File ""/t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4724:286,error,error,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4724,1,['error'],['error']
Availability,"Hi,. I am trying to test the pathseq tutorial following the tutorial on [this]( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ""this"") link. I ran the following commands. bioinfo@bioinfo$ conda activate gatk; (gatk) bioinfo@bioinfo$ gatk PathSeqPipelineSpark \; > --input test_sample.bam \; > --filter-bwa-image hg19mini.fasta.img \; > --kmer-file hg19mini.hss \; > --min-clipped-read-length 70 \; > --microbe-fasta e_coli_k12.fasta \; > --microbe-bwa-image e_coli_k12.fasta.img \; > --taxonomy-file e_coli_k12.db \; > --output output.pathseq.bam \; > --scores-output output.pathseq.txt. And encountered below error:. Using GATK jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar PathSeqPipelineSpark --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --scores-output output.pathseq.txt; 18:57:39.629 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802:655,error,error,655,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802,1,['error'],['error']
Availability,"Hi,. I am using GATK `version gatk4-4.0.6.0-0` as part of the bcbio-nextgen pipeline for RNA-seq variant calling. There is one step in the pipeline i.e. `gatk GenomicsDBImport` that's been failing consistently no matter how less or many resources in terms of memory and cores I provide. I have tried to run the command as part of the pipeline and in stand-alone mode (like below) and both produce the same error:. ```; [rathik@reslnrefo01 log]$ gatk --java-options '-Xms454m -Xmx3181m -XX:+UseSerialGC' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Using GATK jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms454m -Xmx3181m -XX:+UseSerialGC -jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/tmp/rathik; 11:49:24.784 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:49:25.130 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045:406,error,error,406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045,1,['error'],['error']
Availability,"Hi,. I am using GATK version 4.0.3.0 using a shell script. I am working with a haploid organism. I created a single sample BAM file by first aligning PE reads using HISAT2; samtools sort .SAM to .BAM files; then marked duplicate reads using picard. . I keep getting the error message:. A USER ERROR has occurred: Argument --emitRefConfidence has a bad value: Can only be used in single sample mode currently. Use the sample_name argument to run on a single sample out of a multi-sample BAM file. ```; time gatk --java-options ""-Xmx4g"" HaplotypeCaller \ ; -R reference.fa \ ; -I sample1.md.bam \ ; -O sample1.raw.g.vcf \; -ERC GVCF. ```. What am I doing wrong? . ```; Using GATK jar /usr/local/apps/eb/GATK/4.0.3.0-Java-1.8.0_144/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /usr/local/apps/eb/GATK/4.0.3.0-Java-1.8.0_144/gatk-package-4.0.3.0-local.jar HaplotypeCaller -R Af293.41.fa -I eAF01_md.bam -O eAF01.raw.g.vcf.gz -ERC GVCF; 22:25:20.396 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/apps/eb/GATK/4.0.3.0-Java-1.8.0_144/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:25:20.633 INFO HaplotypeCaller - ------------------------------------------------------------; 22:25:20.634 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.3.0; 22:25:20.634 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:25:20.635 INFO HaplotypeCaller - Executing as sek53827@n583.ecompute on Linux v3.10.0-229.20.1.el7.x86_64 amd64; 22:25:20.635 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_144-b01; 22:25:20.635 INFO HaplotypeCaller - Start Date/Time: October 29, 2018 10:25:20 PM EDT; 22:25:20.635 INFO HaplotypeCaller - ------------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5372:270,error,error,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5372,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi,. I don't know if it's a bug, but whenever I try to do a GenomicsDBImport I get an error, no matter which file I use. The command I run is (using GATK 4.0):. gatk GenomicsDBImport -V AD0616.10.g.vcf.gz --genomicsdb-workspace-path gdbworkspace-gatk -L 10. And the error I get is:. Using GATK jar /apps/GATK/4.0/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /apps/GATK/4.0/gatk-package-4.0.0.0-local.jar GenomicsDBImport -V /scratch/production/cluengo/genomicsdb/AD0616.10.g.vcf.gz --genomicsdb-workspace-path gdbworkspace-gatk -L 10; 17:00:53.658 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/apps/GATK/4.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:00:53.770 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.771 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.0.0; 17:00:53.771 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:00:53.771 INFO GenomicsDBImport - Executing as cluengo@login1 on Linux v2.6.32-696.13.2.el6.Bull.128.x86_64 amd64; 17:00:53.771 INFO GenomicsDBImport - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_102-b14; 17:00:53.771 INFO GenomicsDBImport - Start Date/Time: March 8, 2018 5:00:53 PM CET; 17:00:53.771 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.771 INFO GenomicsDBImport - ------------------------------------------------------------; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Version: 2.13.2; 17:00:53.772 INFO GenomicsDBImport - Picard Version: 2.17.2; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 17:00:53.772 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:00:53.772 INFO GenomicsDBImport - HTS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4514:86,error,error,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514,2,['error'],['error']
Availability,"Hi,. I got a java.lang.IllegalStateException: Smith-Waterman alignment failure in Mutect2 (gatk 4.1.6.0).; When running the same input with the same options using gatk 4.1.4.1 the files get processed without error. Here the exception output:; java.lang.IllegalStateException: Smith-Waterman alignment failure. Cigar = 348M with reference length 348 but expecting reference length of 378 ref = CATACGCGTATACACACAATATAGGC; ATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAG; GCATTGCATACGCGTATACACACAAAATAGGCATTGTATACGCGTATACACACAAAATAGGCATTGTATACGCGTATACACACAAAATAGGCATTGTATACGCGTATACACACAAAATAGGCATTGTATACGCGTATACAATATAGGCATTGTATACGCGTATACAATATAGGC pat; h CATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGCATTGCATACGCGTATACACACAATATAGGC; ATTGCATACGCGTATACACACAAAATAGGCATTGCATACGCGTATACACACAAAATAGGCATTGTATACGCGTATACACACAAAATAGGCATTGTATACGCGTATACACACAAAATAGGCATTGTATACGCGTATACAATATATGCATTGTATACGCGTATACAATATATGC; at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.findBestPaths(ReadThreadingAssembler.java:354); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:196); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:146); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:269); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:226); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:299); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6529:71,failure,failure,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6529,3,"['error', 'failure']","['error', 'failure']"
Availability,"Hi,. I have the allele counts from ASEReadCounter and I would like to do some statistical test on the output. You suggest MAMBA in your manual but in their manual they say the input file should have the required field: `EXON_INFO - variant annotation label`. . Can you recommend any other downstream tools to analyze ASEReadCounter output?. Thanks,; Komal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3267:289,down,downstream,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3267,1,['down'],['downstream']
Availability,"Hi,. I made an alignment with my pair-end reads with this command line i have found on the internet:. ```; #!/bin/bash. header=$(zcat $1 | head -n 1); id=$(echo $header | head -n 1 | cut -f 1-4 -d"":"" | sed 's/@//' | sed 's/:/_/g'); sm=$(echo $header | head -n 1 | grep -Eo ""[ATGCN]+$""); echo ""Read Group @RG\tID:$id\tSM:$id""_""$sm\tLB:$id""_""$sm\tPL:ILLUMINA"". bwa mem \; -M \; -t 8 \; -v 3 \; -R $(echo ""@RG\tID:$id\tSM:$id""_""$sm\tLB:$id""_""$sm\tPL:ILLUMINA"") \; ""$path_genome_dir"" \; $1 $2 | samblaster -M | samtools fixmate - - | samtools sort -O bam -o ""mapped-bwa.bam""; ```; Then, I created "".fai"" and "".dict"" extension of my genome by using,. `java -jar picard.jar CreateSequenceDictionary R= genome.fa O= genome.fa.dict`. `samtools faidx genome.fa`. Then I used this command to use HaplotypeCaller feature of gatk (4.1.3.0): `""./gatk --java-options ""-Xmx8G"" HaplotypeCaller -R genome.fa -I mapped-bwa.bam -O mapped-bwa.vcf""`. Then, this gave me an error: `java.lang.IllegalArgumentException: VCFHeaderLine: ID cannot contain an equals sign; 	at htsjdk.variant.vcf.VCFSimpleHeaderLine.initialize(VCFSimpleHeaderLine.java:108); 	at htsjdk.variant.vcf.VCFSimpleHeaderLine.<init>(VCFSimpleHeaderLine.java:91); 	at htsjdk.variant.vcf.VCFContigHeaderLine.<init>(VCFContigHeaderLine.java:66); 	at htsjdk.variant.vcf.VCFHeader.setSequenceDictionary(VCFHeader.java:294); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.makeVCFHeader(HaplotypeCallerEngine.java:423); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.writeHeader(HaplotypeCallerEngine.java:435); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.onTraversalStart(HaplotypeCaller.java:229); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6127:156,echo,echo,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6127,5,"['echo', 'error']","['echo', 'error']"
Availability,"Hi,. I managed to run the latest gatk (_SelectVariants_) on our data but ran into the following error: . `singularity exec /scratch/DBC/BCRBIOIN/SHARED/software/gatk/gatk-4.1.8.1/gatk_latest.sif gatk --java-options ""-Xmx12G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" SelectVariants -R /scratch/DBC/BCRBIOIN/SHARED/genomes/homo_sapiens/GRCh38/dna/GRCh38.d1.vd1.fa -V /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.orientation_filtered.vcf.gz --exclude-filtered -O /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.passed.vcf.gz. [######@dav002(davros) exec]$ singularity exec /scratch/DBC/BCRBIOIN/SHARED/software/gatk/gatk-4.1.8.1/gatk_latest.sif gatk --java-options ""-Xmx12G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" SelectVariants -R /scratch/DBC/BCRBIOIN/SHARED/genomes/homo_sapiens/GRCh38/dna/GRCh38.d1.vd1.fa -V /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.orientation_filtered.vcf.gz --exclude-filtered -O /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.passed.vcf.gz; Using GATK jar /gatk/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx12G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /gatk/gatk-package-4.1.8.1-local.jar SelectVariants -R /scratch/DBC/BCRBIOIN/SHARED/genomes/homo_sapiens/GRCh38/dna/GRCh38.d1.vd1.fa -V /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.orientation_filtered.vcf.gz --exclude-filtered -O /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.passed.vcf.gz; 10:52:40.838 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685695328:96,error,error,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685695328,1,['error'],['error']
Availability,"Hi,. I ran into this issue when running VariantRecalibrator in GATK4. I used the same command as recommended in the GATK3 best practice, but seems the command can't recognize the track information of the resources:. **java -jar ~/Softwares/gatk/build/libs/gatk-package-4.alpha.2-49-g491f7f2-SNAPSHOT-local.jar VariantRecalibrator \; -R ~/Resources/genome_b38/genome.fa \; -V ${op}/DUKE_6954540_200400624.vcf.gz \; -resource hapmap,known=false,training=true,truth=true,prior=15.0 ~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz \; -resource omni,known=false,training=true,truth=true,prior=12.0 \; ~/Resources/genome_b38/1000G_omni2.5.b38.vcf.gz \; -resource 1000G,known=false,training=true,truth=false,prior=10.0 \; ~/Resources/genome_b38/tmp/1000G_phase3.snps.high_confidence.b38.rh.vcf.gz \; -resource dbsnp,known=true,training=false,truth=false,prior=2.0 \; ~/Resources/genome_b38/dbsnp_142.b38.vcf.gz \; -an DP \; -an QD \; -an FS \; -an MQRankSum \; -an ReadPosRankSum \; -mode SNP \; -tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 90.0 \; -O DUKE_6954540_200400624.snp.output.recal \; -tranchesFile DUKE_6954540_200400624.snp.output.tranches** . The error I got is:. **""A USER ERROR has occurred: Invalid command line: Invalid argument '~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz'"".** . It seems the command treat **hapmap,known=false,training=true,truth=true,prior=15.0** and **~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz** as different arguments. And if I only provided ""**~/Resources/genome_b38/hapmap_3.3.b38.vcf.gz**"" to '**-resource**', the job shut down immediately. Is this a bug? Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2199:1157,error,error,1157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2199,3,"['ERROR', 'down', 'error']","['ERROR', 'down', 'error']"
Availability,"Hi,. I tried to build a gCNV model and got the error `python exited with 139` but can#t figure out what is the cause of this error. Can you please help me with that error message? I attended the whole command and output here. ```; (gatk4.2.0.0) k-hg-srv3:/media/Data/AnnotationDBs/CNV/Genom/hdf5 # gatk GermlineCNVCaller --run-mode COHORT -L ../Genom.filtered.interval_list -I 0028-21.hdf5 -I 0045-21.hdf5 -I 0098-18.hdf5 -I 0156-21.hdf5 -I 0429-20.hdf5 -I 0779-18.hdf5 -I 1030-20.hdf5 -I 1098-13.hdf5 -I 1450-20.hdf5 -I 1495-17.hdf5 -I 1575-20.hdf5 -I 1586-18.hdf5 -I 1658-14.hdf5 -I 1974-20.hdf5 -I 2008-20.hdf5 -I 0030-21.hdf5 -I 0058-21.hdf5 -I 0129-20.hdf5 -I 0249-04.hdf5 -I 0614-20.hdf5 -I 0834-19.hdf5 -I 1080-20.hdf5 -I 1331-18.hdf5 -I 1460-18.hdf5 -I 1498-18.hdf5 -I 1576-20.hdf5 -I 1592-20.hdf5 -I 1716-15.hdf5 -I 1985-20.hdf5 -I 2167-20.hdf5 -I 0038-21.hdf5 -I 0094-21.hdf5 -I 0139-18.hdf5 -I 0345-20.hdf5 -I 0641-18.hdf5 -I 0949-20.hdf5 -I 1081-20.hdf5 -I 1416-20.hdf5 -I 1491-20.hdf5 -I 1553-18.hdf5 -I 1577-20.hdf5 -I 1600-20.hdf5 -I 1720-20.hdf5 -I 1995-20.hdf5 --contig-ploidy-calls ../ploidy-calls/ --annotated-intervals ../Genom.annotated.tsv --interval-merging-rule OVERLAPPING_ONLY --output /media/Data/AnnotationDBs/CNV/Genom --output-prefix CNV --tmp-dir /media/Data/tmp/; Using GATK jar /usr/BioinfSoftware/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /usr/BioinfSoftware/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar GermlineCNVCaller --run-mode COHORT -L ../Genom.filtered.interval_list -I 0028-21.hdf5 -I 0045-21.hdf5 -I 0098-18.hdf5 -I 0156-21.hdf5 -I 0429-20.hdf5 -I 0779-18.hdf5 -I 1030-20.hdf5 -I 1098-13.hdf5 -I 1450-20.hdf5 -I 1495-17.hdf5 -I 1575-20.hdf5 -I 1586-18.hdf5 -I 1658-14.hdf5 -I 1974-20.hdf5 -I 2008-20.hdf5 -I 0030-21.hdf5 -I 0058-21.hdf5 -I 0129-20.hdf5 -I 0249-04.hdf5 -I 0614-20.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7234:47,error,error,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234,3,['error'],['error']
Availability,"Hi,. I tried to use your new released GATK4 package to do CNV analysis. I know it's a beta tool... I followed the tools documentation and did ""CollectFragmentCounts"" first to get hdf5 files. After that I tried to run ""DetermineGermlineContigPloidy"" in cohort mode with these files. I created a ploidy_priors.tsv file like described in the documentation and used 8 hdf5-files. A few seconds after starting the tool I get the following error:. ```; [11. Januar 2018 16:42:24 MEZ]; org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=2294808576; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /tmp/die9s/cohort_determine_ploidy_and_depth.9149389425697869853.py --sample_coverage_metadata=/tmp/die9s/samples-by-coverage-per-contig814612566493224652.tsv --output_calls_path=/media/Berechnungen/CNV_analysis/GATK4/normal_cohort-calls --mapping_error_rate=1.000000e-02 --psi_s_scale=1.000000e-04 --mean_bias_sd=1.000000e-02 --psi_j_scale=1.000000e-03 --learning_rate=5.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.990000e-01 --log_emission_samples_per_round=2000 --log_emission_sampling_rounds=100 --log_emission_sampling_median_rel_error=5.000000e-04 --max_advi_iter_first_epoch=1000 --max_advi_iter_subsequent_epochs=1000 --min_training_epochs=20 --max_training_epochs=100 --initial_temperature=2.000000e+00 --num_thermal_epochs=20 --convergence_snr_averaging_window=5000 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=1 --caller_update_convergence_threshold=1.000000e-03 --caller_admixing_rate=7.500000e-01 --disable_caller=false --disable_sampler=false --disable_annealing=false --interval_list=/tmp/die9s/intervals671187352630642175.tsv --contig_ploidy_prior_table=/media/Berechnungen/CNV_analysis/GATK4/ploidy_priors.tsv --output_model_path=/media/Berechnungen/CNV_analysis/GATK",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4125:434,error,error,434,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4125,1,['error'],['error']
Availability,"Hi,. I'm trying to run BaseRecalibratorSpark (gatk-4.1.7.0) but I'm hitting a problem where the process carshe with a ""too many open files"" error. Here is my command:. ulimit -n 4096; /usr/local/bioinf/gatk/gatk-4.1.7.0/gatk BaseRecalibratorSpark \; --java-options '-Xmx64G' \; --tmp-dir /local/scratch/rieder \; -I Normal_fixed.bam \; -R GRCh38.d1.vd1.fa \; -L S07604514_Regions_merged_padded.interval_list \; -O P45507_normal_bqsr.table \; --known-sites Homo_sapiens_assembly38.dbsnp138.vcf \; --known-sites Homo_sapiens_assembly38.known_indels.vcf \; --known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf \; --spark-master local[8] \; --conf 'spark.executor.cores=8' \; --conf 'spark.local.dir=/local/scratch/rieder'. Here is the (hopefully) relevant log extract:. 20/04/29 01:51:51 INFO TaskSetManager: Finished task 578.0 in stage 0.0 (TID 578) in 2720 ms on localhost (executor driver) (576/1585); 20/04/29 01:51:51 INFO NewHadoopRDD: Input split: file:/data/projects/2019/NeoAG/VCF-phasing/work/b8/d8dc550b7ba5f57d935d04e27b756a/Normal_fixed.bam:19562233856+33554432; 01:51:51.374 INFO FeatureManager - Using codec VCFCodec to read file file:///local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Homo_sapiens_assembly38.dbsnp138.vcf; 01:51:51.431 INFO FeatureManager - Using codec VCFCodec to read file file:///local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Homo_sapiens_assembly38.known_indels.vcf; 01:51:51.451 INFO FeatureManager - Using codec VCFCodec to read file file:///local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Mills_and_1000G_gold_standard.indels.hg38.vcf; 01:51:51.457 INFO FeatureManager - Using codec VCFCodec to read file file:///local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Homo_sapiens_assembly38.dbsnp138.vcf; 01:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6578:140,error,error,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6578,1,['error'],['error']
Availability,"Hi,. Using GATK mutect2's wdl file on Terra (version 21 on agora) I keep getting the same error:; ""pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket"" . Here is part of the stacktrace : ; ```; 20:59:48.744 INFO Mutect2 - Inflater: IntelInflater; 20:59:48.744 INFO Mutect2 - GCS max retries/reopens: 20; 20:59:48.744 INFO Mutect2 - Requester pays: enabled. Billed to: broad-firecloud-ccle; 20:59:48.744 INFO Mutect2 - Initializing engine; 20:59:54.630 INFO FeatureManager - Using codec VCFCodec to read file gs://depmapomicsdata/1000g_pon.hg38.vcf.gz; 20:59:55.629 INFO Mutect2 - Shutting down engine; [October 4, 2021 8:59:55 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=876609536; code: 403; message: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; reason: forbidden; location: null; retryable: false; com.google.cloud.storage.StorageException: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:406); at com.google.cloud.storage.StorageImpl$4.call(StorageImpl.java:217); ...; ```. This happens while it runs the command:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7492:90,error,error,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492,2,"['down', 'error']","['down', 'error']"
Availability,"Hi,. When I try to run Funcotator, using the below command:. gatk Funcotator \; --variant /rsrch5/home/tdccct/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz \; --reference /rsrch5/home/tdccct/ppshah/shared/gencode/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \; --ref-version hg38 \; --data-sources-path /rsrch5/home/tdccct/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s \; --output /rsrch5/home/tdccct/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_funcotated.vcf \; --output-file-format VCF; ; I get the following error:; ; Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.4.0.0-local.jar Funcotator --variant /home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz --reference /home/ppshah/shared/gencode/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --ref-version hg38 --data-sources-path /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s --output /home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_funcotated.vcf --output-file-format VCF; 16:36:22.352 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:36:22.392 INFO Funcotator - ------------------------------------------------------------; 16:36:22.396 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8647:808,error,error,808,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647,1,['error'],['error']
Availability,"Hi,; Copying and running the `spark-submit` command run by gatk-launch, I found out the parameter `spark.driver.userClassPathFirst` is set to true, and if I set to false it runs well (well, my job has an error but because of my data ^^) when running in cluster mode.; Here, my command which works:; > spark-submit --master yarn \; > --conf 'spark.driver.userClassPathFirst=false' --conf 'spark.io.compression.codec=lzf' --conf 'spark.driver.maxResultSize=0' --conf 'spark.executor.extraJavaOptions=""-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1""' \; > --conf 'spark.driver.extraJavaOptions=""-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1""' \; > --conf 'spark.kryoserializer.buffer.max=512m' --conf 'spark.yarn.executor.memoryOverhead=600' \; > --conf 'spark.submit.deployMode=cluster' \; > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-package-4.beta.6-spark.jar BwaSpark --programName gatk4-bwa-test --input hdfs://spark01:7222/user/axverdier/data/phalstedii/PLHAL710.710.unmappedReads.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_BWA_BwaSpark.bam --reference hdfs://spark01:7222/user/axverdier/data/phalstedii/Plhal710r1.1.fa. I didn't find out how to override `spark.driver.userClassPathFirst` to false in the gatk-launch command, it seems to be ignored or replaced by true.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350717821:204,error,error,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350717821,1,['error'],['error']
Availability,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:70,error,error,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,9,"['ERROR', 'FAILURE', 'error']","['ERROR', 'FAILURE', 'error']"
Availability,"Hi,; I am working on GATK VariantsToTable tool and my VCF file consists of 12 chromosomes but the output shows only one chromosome. Could you please help me out.; OS:-Ubuntu 20.04; GATK version:-4.1.9.0; Java:-open jdk version 11.0.8; Command:-gatk VariantsToTable -R '/home/india/Downloads/Reference.fasta' -V '/home/india/Downloads/Galaxy57-[Merged_file.vcf].vcf' -F CHROM -F POS -F REF -F ALT -GF AD -GF DP -GF GQ -GF PL -O bothbulks_new.table. Using GATK jar /home/india/Downloads/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/india/Downloads/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar VariantsToTable -R /home/india/Downloads/Reference.fasta -V /home/india/Downloads/Galaxy57-[Merged_file.vcf].vcf -F CHROM -F POS -F REF -F ALT -GF AD -GF DP -GF GQ -GF PL -O bothbulks_new.table; 16:46:03.294 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/india/Downloads/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 16, 2020 4:46:04 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:46:04.315 INFO VariantsToTable - ------------------------------------------------------------; 16:46:04.316 INFO VariantsToTable - The Genome Analysis Toolkit (GATK) v4.1.9.0; 16:46:04.316 INFO VariantsToTable - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:46:04.317 INFO VariantsToTable - Executing as india@india-HP-ProBook-445-G1 on Linux v5.4.0-26-generic amd64; 16:46:04.317 INFO VariantsToTable - Java runtime: OpenJDK 64-Bit Server VM v11.0.8+10-post-Ubuntu-0ubuntu120.04; 16:46:04.317 INFO VariantsToTable - Start Date/Time: 16 October 2020 at 4:46:02 PM IST; 16:46:04.318 INFO VariantsToTable - ----------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6897:281,Down,Downloads,281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6897,6,['Down'],['Downloads']
Availability,"Hi,; I need to include the SnpCluster filter in VariantFiltration but the filtration is not working for me. I have included the other filters but I'm not sure about the usage of this particular filter. The command that I have used:; --genotype-filter-name ""SnpCluster"" \; --genotype-filter-expression ""clusterSize=3"" \. The error that I'm facing:; 12:52:33.595 WARN JexlEngine - ![0,15]: 'clusterSize = 3;' context is readonly. ![image](https://github.com/broadinstitute/gatk/assets/125788800/9cd32db5-0010-489f-be8c-091b53e02c99)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8500:324,error,error,324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8500,1,['error'],['error']
Availability,"Hi,; I ran into the following error when combining gVCF files generated by the HaplotypeCaller:. > htsjdk.tribble.TribbleException$InvalidHeader: Your input file has a malformed header: Discordant field size detected for field AS_RAW_ReadPosRankSum at chr1:13417. Field had 2 values but the header says this should have 1 values based on header record INFO=<ID=AS_RAW_ReadPosRankSum,Number=1,Type=String,Description=""allele specific raw data for rank sum test of read position bias"". Similar number info was found for several allele-specific annotations:; ```; ##INFO=<ID=AS_InbreedingCoeff,Number=A,Type=Float,Description=""allele specific heterozygosity as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation; relate to inbreeding coefficient"">; ##INFO=<ID=AS_QD,Number=A,Type=Float,Description=""Allele-specific Variant Confidence/Quality by Depth"">; ##INFO=<ID=AS_RAW_BaseQRankSum,Number=1,Type=String,Description=""raw data for allele specific rank sum test of base qualities"">; ##INFO=<ID=AS_RAW_MQ,Number=1,Type=String,Description=""Allele-specfic raw data for RMS Mapping Quality"">; ##INFO=<ID=AS_RAW_MQRankSum,Number=1,Type=String,Description=""Allele-specfic raw data for Mapping Quality Rank Sum"">; ##INFO=<ID=AS_RAW_ReadPosRankSum,Number=1,Type=String,Description=""allele specific raw data for rank sum test of read position bias"">; ##INFO=<ID=AS_SB_TABLE,Number=1,Type=String,Description=""Allele-specific forward/reverse read counts for strand bias tests"">; ```; I assume, the correct annotation should be ""Number=A"". The gVCF files were generated with HaplotypeCaller using; ```; --emit-ref-confidence GVCF \; --annotation-group StandardAnnotation \; --annotation-group AS_StandardAnnotation \; --annotation-group StandardHCAnnotation \; ```. GATK version 4.0.0.0 (downloaded from GATK website)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4162:30,error,error,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4162,2,"['down', 'error']","['downloaded', 'error']"
Availability,"Hi,; I recently started getting error messages when running a Nextflow pipeline for WGS analysis. I am using GATK 4.0.1.2 and was wondering whether:. /bin/env python - too many levels of symbolic links. may have to do with a broken conda environment (which GATK seems to use)? This happens for tools such as GenomicsDBImport. If I run the job in question outside of Nextflow, it seems to start just fine. But as far as I know Nextflow does not use python, so doesn't look like the obvious culprit.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4459:32,error,error,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4459,1,['error'],['error']
Availability,"Hi,; I recently used GATK4 Spark local version for somatic variant call. The machine has 40 cores and 160g mem. I tried 20 and 10 cores for each tumor/normal pair in the BQSR step (BaseRecalibratorSpark) and the two samples are processed at the same time. However, the pipeline frequently failes (errors like outofmemory, cannot allocate a page) unless I use 4 cores for each sample. I think the problem should be solved by tuning Spark and JAVA parameters. I considered options like `--conf spark.driver.memory=10g`, `-XX:ParallelGCThreads=10` but had no luck. Can someone suggest the parameter options that I should look at? . Thanks,. -Han",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3465:297,error,errors,297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3465,1,['error'],['errors']
Availability,"Hi,; I run the PathSeqPipelineSpark on a SPARK HPC with a master and several workers. I downloaded SPARK 2.2.0 with hadoop 2.7.3; Java is 1.8.0_131; I set the java classpath (I think correctly). The command runs well without the --spark-master option, so the files are at the right place, but when I run the following command line:; `gatk PathSeqPipelineSpark --spark-master spark://XX.XX.XX.XX:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt -- --spark-runner SPARK`. I get the following error:; ```; 18/04/24 17:55:54 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 4, xx.xx.xx.16, executor 3): **org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112)**; at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:88,down,downloaded,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,3,"['Error', 'down', 'error']","['Error', 'downloaded', 'error']"
Availability,"Hi,; I run the code below to to skip optical duplicate detection during marking duplicate.; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=trueDsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar MarkDuplicatesSpark --spark-master local[28] --conf spark.local.dir=/datatmp/ -I ./A.sort.bam -O ./A.sort.bam.Mdup.bam -M ./A.sort.bam.Md.metrics.txt --tmp-dir /datatmp/ --conf spark.network.timeout=200h --conf spark.executor.heartbeatInterval=100h --read-name-regex null`; It reports the error below.; `20/12/15 11:43:00 ERROR Executor: Exception in task 15.0 in stage 7.0 (TID 12538); java.lang.NullPointerException; at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$handleFragments$12(MarkDuplicatesSparkUtils.java:395); at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:372); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:479); at java.util.stream.ReferencePipeline.max(ReferencePipeline.java:515); at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.handleFragments(MarkDuplicatesSparkUtils.java:396); at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$markDuplicateRecords$fa45b352$1(MarkDuplicatesSparkUtils.java:304); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); at org.apache.spark.api.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7001:527,heartbeat,heartbeatInterval,527,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7001,3,"['ERROR', 'error', 'heartbeat']","['ERROR', 'error', 'heartbeatInterval']"
Availability,"Hi,; I tried to use Funcotator to annotate a vcf file. The vcf file had just 10 records. The program kept running without errors but no results got. . **The command I used:**. /technology/software_tools/gatk-4.2.0.0/gatk Funcotator \; --variant P01.mutect2.somatic.filterMutectCalls.indels.vcf.gz \; --reference /technology/dependent_resource/genome/hsa/ensembl/GRCh37.p13_GATK/genome.fa \; --ref-version hg19 \; --data-sources-path /technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s \; --output P01.mutect2.somatic.filterMutectCalls.indels.funcotator.vcf \; --output-file-format VCF. **Screen output:**. Using GATK jar /technology/software_tools/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /technology/software_tools/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar Funcotator --variant P01.mutect2.somatic.filterMutectCalls.indels.vcf.gz --reference /technology/dependent_resource/genome/hsa/ensembl/GRCh37.p13_GATK/genome.fa --ref-version hg19 --data-sources-path /technology/dependent_resource/variation/hg19/funcotator_dataSources.v1.7.20200521s --output P01.mutect2.somatic.filterMutectCalls.indels.funcotator.vcf --output-file-format VCF; 10:25:49.484 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/technology/software_tools/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 09, 2021 10:25:49 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:25:49.665 INFO Funcotator - ------------------------------------------------------------; 10:25:49.665 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 10:25:49.665 INFO Funcotator - For support and documentation go to https://software.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7135:122,error,errors,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7135,1,['error'],['errors']
Availability,"Hi,; I tried your commands (and many adaptions / changements) but I always get the same problem:; If the command line includes `--`, I get the JNI linkage error as if the spark related parameters were not parsed.; I tried many things, as:; > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass -- --sparkRunner SPARK --sparkMaster yarn --deploy-mode cluster; > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass --sparkRunner SPARK --sparkMaster yarn -- --master yarn --deploy-mode cluster. > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass --sparkRunner SPARK --sparkMaster yarn -- --master yarn --deploy-mode cluster --conf spark.driver.extraJavaOptions='-Dmapr.library.flatclass' --conf spark.executor.extraJavaOptions='-Dmapr.library.flatclass'. > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass --sparkRunner SPARK --sparkMaster yarn -- --master yarn --deploy-mode cluster --driver-java-options '-Dmapr.library.flatclass'. It's a non-exhaustive list, I tried a lot of configurations similar to these ones.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350227061:155,error,error,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350227061,1,['error'],['error']
Availability,"Hi,; I use the gatk4.0.2.1 to detect variant and the command line:; time gatk-4.0.2.1/gatk --java-options ""-XX:ParallelGCThreads=5 -Xmx30G"" HaplotypeCaller --input rice.RGAP7.R01.dedup.bam --output rice.RGAP7.1.0.g.vcf --reference ref.genome.fa --native-pair-hmm-threads --emit-ref-confidence GVCF --indel-size-to-eliminate-in-ref-model 50 --sample-ploidy 2 --intervals rice.RGAP7.chr_allocation.1.list --TMP_DIR tmp --verbosity ERROR. **real	32m47.986s**; user	32m56.767s; sys	0m22.567s. I ues the gatk3.8 to detect variant and the command line:; time java -XX:ParallelGCThreads=5 -Djava.io.tmpdir=tmp -Xmx30G GenomeAnalysisTK/3.8/GenomeAnalysisTK.jar -T HaplotypeCaller -R ref.genome.fa --indelSizeToEliminateInRefModel 50 --emitRefConfidence GVCF --sample_ploidy 2 -nct 4 -o rice.RGAP7.1.0.g.vcf -L rice.RGAP7.chr_allocation.1.list -I rice.RGAP7.R01.dedup.realn.bam . **real	8m49.673s**; user	35m42.770s; sys	0m21.607s. Theoretically，the gatk4 runtime is faster than gatk3.x .; Why do I get the opposite result?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5090:429,ERROR,ERROR,429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5090,1,['ERROR'],['ERROR']
Availability,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:714,down,download,714,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,3,"['down', 'echo']","['download', 'echo']"
Availability,"Hi,; I'm running a command:; ```; java -jar gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar SelectVariants \; -R ref/human_g1k_b37_20.fasta \; -V dupa.raw.indels.snps.vcf \; -selectType INDEL \; -O dupa_raw.INDEL.vcf. ```; and I get error:; ```; ***********************************************************************. A USER ERROR has occurred: s is not a recognized option. ***********************************************************************; ```; Tested on various input files. ```; java -version; openjdk version ""1.8.0_152""; OpenJDK Runtime Environment (Zulu 8.25.0.1-linux64) (build 1.8.0_152-b16); OpenJDK 64-Bit Server VM (Zulu 8.25.0.1-linux64) (build 25.152-b16, mixed mode); ```. Looks like it's a bug?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4705:229,error,error,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4705,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi,; I've got some error messages about ""tranches"" when I'm running VariantRecalibrator. Here is my command lines:; ```; gatk=/public/home/fan_lab/shali/gatk/gatk-4.1.8.1/gatk; reference=/public/home/fan_lab/shali/gatk/gatk_bundle/Homo_sapiens_assembly38.fasta; GATK_bundle=/public/home/fan_lab/shali/gatk/gatk_bundle. indir=/public/home/fan_lab/shali/NGS_new; outdir=/public/home/fan_lab/shali/NGS_new. gatk VariantRecalibrator \; -R $reference \; --variant $outdir/population/24samples.HC.vcf.gz \; --resource:hapmap,known=false,training=true,truth=true,prior=15.0 $GATK_bundle/hapmap_3.3.hg38.vcf.gz \; --resource:omni,known=false,training=true,truth=false,prior=12.0 $GATK_bundle/1000G_omni2.5.hg38.vcf.gz \; --resource:1000G,known=false,training=true,truth=false,prior=10.0 $GATK_bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz \; --resource:dbsnp,known=true,training=false,truth=false,prior=6.0 $GATK_bundle/dbsnp_146.hg38.vcf.gz \; -an DP -an QD -an FS -an SOR -an ReadPosRankSum -an MQRankSum \; --mode SNP \; --tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 95.0 -tranche 90.0 \; --rscript-file $outdir/population/24samples.HC.snps.plots.R \; --tranches-file $outdir/poplation/24samples.HC.snps.tranches \; -O $outdir/poplation/24samples.HC.snps.recal; ```; The mission ended quickly.When I look at the log file, I find the following error message:; ```; A USER ERROR has occurred: /public/home/fan_lab/shali/NGS_new/poplation/24samples.HC.snps.tranches; ```; Any suggestions for me? Look forward to your reply. Thanks ever so much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7225:19,error,error,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7225,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi,; May I ask when this mitochondria pipeline will be available? ; In the meanwhile, if I use Mutect2 for calling human MT variants (including indel), which is the best choice of parameters?; I have MT bam file to start with and read coverage is in thousand scale for MT.; Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-427039803:55,avail,available,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-427039803,1,['avail'],['available']
Availability,"Hi,; Our spark installation use a mapr filesystem ( hdfs compatible ).; GATK spark tools does not seems to recognize it.; When running the following command:; > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input maprfs://spark-ics/user/axverdier/data/710-PE-G1.bam --output maprfs://spark-ics/user/axverdier/testOutGATK_CountReadsSpark --sparkRunner SPARK --sparkMaster yarn --javaOptions -Dmapr.library.flatclass; I got the following error!. > Driver stacktrace:; > 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1436); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1424); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); > 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); > 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); > 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1423); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); > 	at scala.Option.foreach(Option.scala:257); > 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1651); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1606); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1595); > 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); > 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); > 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); > 	at org.apache.spark.SparkCo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936:490,error,error,490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936,1,['error'],['error']
Availability,"Hi,; The operating system is ubuntu 20.04.; java version is openjdk ""17.0.11"".; If the process of GATK best practice has been interrupted, I could see the; error messages always.; But, in this time, the process was interrupted without giving any messages.; This is quite weird. I checked this several other chromosomes.; My callset has about 430 samples.; I could run GenotypeGVCFs in GATK 4.5.0.0 version without any problem.; But, in GATK 4.6.0.0, the process was successful in 3-4 chromosomes (which; is smaller one I think). The process has been interrupted; in incomplete stages.; I could not find any ERR files in the folder.; Thanks; Jinu Han. On Fri, Jul 19, 2024 at 7:01 PM Gökalp Çelik ***@***.***>; wrote:. > Can you provide more details on what operating system you are using and; > other related information such as java version etc?; >; > Even if the process gets interrupted by the system there must be a java; > segfault message at some point thrown by the process. Did you observe any; > files with names ERR around the output file?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2238814358>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AG7IXWSQYT56QW4Q4YCZUPTZNDPXPAVCNFSM6AAAAABLBRETECVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDEMZYHAYTIMZVHA>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2238826908:156,error,error,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2238826908,1,['error'],['error']
Availability,"Hi,; This is a part of my script and I get the mistakes after I run it:; for sample in $samples ; do ; sample_gvcfs=${sample_gvcfs}"" --variant /data/users/zhanglei/species/Medicago/result/${sample}.HC.g.vcf.gz ""; done; time gatk CombineGVCFs \; -R /data/users/zhanglei/species/Medicago/Medicago.fa \; ${sample_gvcfs} \; -O $outdir/population/${outname}.HC.g.vcf.gz && echo ""** ${outname}.HC.g.vcf.gz done **"" &&. 18:08:13.704 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/software/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:08:13.872 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.873 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.0.3.0; 18:08:13.873 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:08:13.873 INFO CombineGVCFs - Executing as zhanglei@GenEngine on Linux v3.10.0-327.el7.x86_64 amd64; 18:08:13.873 INFO CombineGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_201-b09; 18:08:13.874 INFO CombineGVCFs - Start Date/Time: May 18, 2019 6:08:13 PM CST; 18:08:13.874 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.874 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.874 INFO CombineGVCFs - HTSJDK Version: 2.14.3; 18:08:13.874 INFO CombineGVCFs - Picard Version: 2.17.2; 18:08:13.874 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:08:13.875 INFO CombineGVCFs - Deflater: IntelDeflater; 18:08:13.875 INFO CombineGVCFs - Inflater: IntelInflater; 18:08:13.875 INFO CombineGVCFs - GCS max retries/reopens: 20; 18:08:13.875 I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947:368,echo,echo,368,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947,1,['echo'],['echo']
Availability,"Hi,; This is what I get from java -version. openjdk version ""11.0.18"" 2023-01-17; OpenJDK Runtime Environment (build 11.0.18+10-post-Ubuntu-0ubuntu122.04); OpenJDK 64-Bit Server VM (build 11.0.18+10-post-Ubuntu-0ubuntu122.04, mixed mode). So are you saying I'd need to downgrade my java version and try again?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452495903:269,down,downgrade,269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452495903,1,['down'],['downgrade']
Availability,"Hi,; Where Can I download the GATK3 ,I want to use Realigner and IndelRealigner tools in GATK.However the GATK4 is not available.; Anyone who can tell me where to download the GATK3 version?; --; Thank you!--",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6373:17,down,download,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6373,3,"['avail', 'down']","['available', 'download']"
Availability,"Hi,; While I test a new data set from fastq using bwamem, samtools and gatk4, I use the following scripts:. % step1: bwa mem. > bwa mem -t 12 -Ma -R '@RG ID:HCC1954 LB:HCC1954 SM:HCC1954' human_g1k_v37.fasta HCC1954_1.fq HCC1954_2.fq > HCC1954.sam. % step2: sort by queryname. > samtools sort -n HCC1954.sam -@ 24 > HCC1954.readnamesort.bam . % step3: gatk4 ReadsPipelineSpark. > /gatk-launch \; > ReadsPipelineSpark \; > -I HCC1954.readnamesort.bam \; > -R /benchmark/human_g1k_v37.2bit \; > -O HCC1954.bam \; > --knownSites /benchmark/dbsnp_138.b37.excluding_sites_after_129.vcf \; > --shardedOutput false \; > --emit_original_quals \; > --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES \; > --sparkRunner SPARK \; > --driver-memory 8G \; > --executor-memory 60g \; > --num-executors 1 \; > --executor-cores 4 \; > --sparkMaster local[4]. I encounter problem like this:. org.broadinstitute.hellbender.exceptions.UserException$MalformedRead: A USER ERROR has occurred: Read C097FACXX111207:2:2301:17281:179267 1:1139151-1139251 is malformed: The input .bam file contains reads with no platform information. First observed at read with name = C097FACXX111207:2:2301:17281:179267; at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.parsePlatformForRead(RecalUtils.java:510); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:122); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:33); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apach",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1949:953,ERROR,ERROR,953,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1949,1,['ERROR'],['ERROR']
Availability,"Hi,; during compilation of 3.8 sources I get. ```; [INFO] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4686:411,down,downstream,411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686,1,['down'],['downstream']
Availability,"Hi,; first of all, I find it very awkward that after 3.8 release there is 3.8-1. Why the dash instead of a dot, as usual? It only complicates automated package downloads which in general work with numbers separated by dots. You just mix together two schemes. Is that really necessary?. Anyway, the pom.xml is broken:. ```; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelPar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4685:160,down,downloads,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685,3,"['ERROR', 'down']","['ERROR', 'downloads']"
Availability,"Hi- I'm using `FilterMutectCalls` on a vcf produced by mutect2 (same gatk version). I get the following exception from `FilterMutectCalls`:. ```; INFO annotation 'MFRL' contains a non-int value '7.97254e+06'; ```. This is odd since the MFRL is a FORMAT tag, not INFO and anyway is of type float:. ```; zcat gatk/TT001T03.snv.vcf.gz | grep '##' | grep MFRL; ##FORMAT=<ID=MFRL,Number=R,Type=Float,Description=""median fragment length"">; ```. The faulty record:. ```; zcat gatk/TT001T03.snv.vcf.gz | grep '7.97254e+06'; chr6	32197424	.	G	A	.	.	DP=626;ECNT=1;NLOD=88.13;N_ART_LOD=1.78;POP_AF=0.001;P_GERMLINE=-86.27;TLOD=4.57;	GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:SA_MAP_AF:SA_POST_PROB	0/0:323,3:0.035:172,3:151,0:41:141,254:60:27:.:.	0/1:264,3:0.038:127,1:137,2:41:124,7.97254e+06:60:27:0.01,0,0.011:0.001247,0.011,0.987; ```. **EDIT**: It appears that the int 7972545 is converted to 7.97254e+06 after I passed the vcf file through a custom filter, so not really a fault of gatk (apologies). Still '7.97254e+06' should be a valid float. Full stack trace:. ```; gatk FilterMutectCalls --variant gatk/TT001T03.snv.vcf.gz --output test.vcf.gz. Using GATK jar /home/db291g/applications/gatk/gatk-4.0.1.0/gatk-package-4.0.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /home/db291g/applications/gatk/gatk-4.0.1.0/gatk-package-4.0.1.0-local.jar FilterMutectCalls --variant gatk/TT001T03.snv.vcf.gz --output test.vcf.gz; 10:10:10.424 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/db291g/applications/gatk/gatk-4.0.1.0/gatk-package-4.0.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:10:10.528 INFO FilterMutectCalls - ------------------------------------------------------------; 10:10:10.528 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.0.1.0; 10:10:10.528 INFO FilterMutectCalls - For support and docum",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4363:443,fault,faulty,443,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4363,2,['fault'],"['fault', 'faulty']"
Availability,Hi. ; I also encounter this error on gatk v 4.6.0.0. ; Did you find a workaround or has any progress been made to address this?; Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8699#issuecomment-2374702208:28,error,error,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8699#issuecomment-2374702208,1,['error'],['error']
Availability,"Hi. After creating a GenomicsDB with 36 gvcf withGenomicsDBimport, I used GEnotypeGVCF on the GenomicsDB folder but get the following error: ""ERROR: Couldn't create GenomicsDBFeatureReader"". See below for the complete output. FYI, I don't get this error when importing only six samples in GenomicsDB and then running GenotypeGVCF. . FYI, also added ""TILEDB_DISABLE_FILE_LOCKING=1"" to environment (export TILEDB_DISABLE_FILE_LOCKING=1) before executing GenomicsDBimport command but that dit not help either. Any suggestion would be highly appreciated. ##GenotypeVCF not working on 36 samples; (base) xxxxxx@galaxy:~$ gatk --java-options ""-Xmx30g"" GenomicsDBImport \; > -V output/B7_2.g.vcf.gz \; > -V output/B7_6.g.vcf.gz \; > -V output/B8_7.g.vcf.gz \; > -V output/B8_5.g.vcf.gz \; > -V output/B8_17.g.vcf.gz \; > -V output/B8_9.g.vcf.gz \; > -V output/B7_9.g.vcf.gz \; > -V output/B7_10.g.vcf.gz \; > -V output/B8_13.g.vcf.gz \; > -V output/B7_15.g.vcf.gz \; > -V output/B8_8.g.vcf.gz \; > -V output/B8_10.g.vcf.gz \; > -V output/B8_11.g.vcf.gz \; > -V output/F_A.g.vcf.gz \; > -V output/B7_3.g.vcf.gz \; > -V output/B8_19.g.vcf.gz \; > -V output/A8.g.vcf.gz \; > -V output/B7_4.g.vcf.gz \; > -V output/B8_4.g.vcf.gz \; > -V output/B8_4g.g.vcf.gz \; > -V output/B8_16.g.vcf.gz \; > -V output/A9_1.g.vcf.gz \; > -V output/X2.g.vcf.gz \; > -V output/X1.g.vcf.gz \; > -V output/7_5_2.g.vcf.gz \; > -V output/C3.g.vcf.gz \; > -V output/C1.g.vcf.gz \; > -V output/C2.g.vcf.gz \; > -V output/C3.g.vcf.gz \; > -V output/A9-10.g.vcf.gz \; > -V output/P3.g.vcf.gz \; > -V output/P4.g.vcf.gz \; > -V output/X4.g.vcf.gz \; > -V output/X5.g.vcf.gz \; > -V output/X6.g.vcf.gz \; > -V output/X7.g.vcf.gz \; > --genomicsdb-workspace-path ABchroneALL \; > --intervals pseudochromosome_1 \; > --intervals pseudochromosome_2 \; > --intervals pseudochromosome_3 \; > --batch-size 6; Using GATK jar /data/xxxxxx/miniconda3/share/gatk4-4.1.6.0-0/gatk-package-4.1.6.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6616:134,error,error,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6616,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi. I encounter the same error with GATK4.0.4.0 and the python environment created by gatkcondaenv.yml. ```; 14:49:35.361 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:/tmp/--------/1d_cnn_mix_train_full_bn.4552731615279398677.json and weights:/tmp/--------/1d_cnn_mix_train_full_bn.2635334538442041575.hd5; 14:49:36.747 INFO ProgressMeter - Starting traversal; 14:49:36.747 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:49:36.768 INFO ProgressMeter - unmapped 0.0 1 3157.9; 14:49:36.769 INFO ProgressMeter - Traversal complete. Processed 1 total variants in 0.0 minutes.; 14:49:36.772 INFO CNNScoreVariants - Shutting down engine; [May 9, 2018 2:49:36 PM CST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=41160278016; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError: ('Error! Unknown code:', '\x00'); >>>; 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:390); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368:25,error,error,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368,2,"['down', 'error']","['down', 'error']"
Availability,"Hi. I failed to build GATK4. . I am a very beginner of bioinformatics and data science. ; I am using google VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8346:123,down,downloaded,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346,3,"['FAILURE', 'down', 'error']","['FAILURE', 'downloaded', 'errors']"
Availability,"Hi. I had a question on Java version to use for GATK/PIcard. . https://gatk.broadinstitute.org/hc/en-us/articles/360035889531-What-are-the-requirements-for-running-GATK- ; states that `the Java runtime version should be at 1.8 exactly`. As you might be aware, Java 1.8 has reached its end of life - https://endoflife.date/java . Given this , I tried running MarkDuplicates and some other tools using Amazon Corretto 17 and they seem to work fine. More importantly, I did not encounter the `Unsupported major.minor version` error as mentioned here: https://gatk.broadinstitute.org/hc/en-us/articles/360035532332. Also happened to see this - https://github.com/broadinstitute/gatk/issues/7436. Question: Am I OK using Amazon Corretto for GATK/Picard or do I need to absolutely stick on to Java 1.8 for GATK/Picard. ```; java -version; openjdk version ""17.0.2"" 2022-01-18 LTS; OpenJDK Runtime Environment Corretto-17.0.2.8.1 (build 17.0.2+8-LTS); OpenJDK 64-Bit Server VM Corretto-17.0.2.8.1 (build 17.0.2+8-LTS, mixed mode, sharing); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7842:523,error,error,523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7842,1,['error'],['error']
Availability,"Hi. Same Error on 4.0.3.0; ```; java -jar /usr/hpc-bio/gatk/gatk-package-4.0.3.0-local.jar Mutect2 --verbosity WARNING -R /usr/bio-ref/GRCh38.p0.dnaref/dnaref.fa --germline-resource /usr/bio-ref/GRCh38.p0.dnaref/common.vcf --max-reads-per-alignment-start 100 -L X -I /biowrk/BaseSpace/bam.bwa/HiSeqX-PCR-free-v2.5-NA12878/md.bam -tumor HiSeqX-PCR-free-v2.5-NA12878 -O mutect2.tumor-only.vcf; 23:50:01.301 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; [March 28, 2018 11:50:04 PM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=2354577408; java.lang.IndexOutOfBoundsException: Index: 0, Size: 0; at java.util.ArrayList.rangeCheck(ArrayList.java:657); at java.util.ArrayList.get(ArrayList.java:433); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.isActive(Mutect2Engine.java:316); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-376937140:9,Error,Error,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-376937140,1,['Error'],['Error']
Availability,"Hi. The VCF output file of HaplotypeCaller is not same when different interval split. It seems that smaller region will product more var into vcf file.; split1:chr unit; 5,089,533 var in vcf; split2: gatk4.SplitIntervals.sh(500 unit); 5,142,322 var in vcf. fastq files; HiSeqX-PCR-free-v2.5-NA12878 70x from BaseSpace. gatk version:4.0.3.0. By the way, there are some many diff site just in QD value. ; such as QD=28.13 vs QD=25.36, and other attr are same. Reason:; download sample without a fixed seed for every site?; or https://github.com/broadinstitute/gatk/issues/4614?; or others?. Best Regards",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4617:467,down,download,467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4617,1,['down'],['download']
Availability,"Hi; I tried to run the recalibration using the gatk-generated vcf file as a known vcf file. I got an error which has been previously described ""The covariates table is missing ReadGroup V300019285_L2_ in RecalTable0"" but without the solution. ; I am wondering if the solution has been found. Anyone has the experience to fix this issue.; Thank ; **This is the batch file** ; java -Xmx16g -jar /scratch/ddo/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar ApplyBQSR \; -R /scratch/ddo/refgenomenew/New_IDs.fasta \; -I /scratch/ddo/markedsam/C18-436P.sort.rmdup.bam \; --bqsr-recal-file /scratch/ddo/reclibration/gatkmf01_C18-436P.recal_data.table \; -O /scratch/ddo/reclibration/C18-436P.bqsr.maf01.bam . **This is the log file**; -----------------------------------------------------------------------------------------------------; Picked up JAVA_TOOL_OPTIONS: -Xmx2g; 04:59:42.641 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/ddo/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 08, 2021 4:59:43 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 04:59:43.044 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.045 INFO ApplyBQSR - The Genome Analysis Toolkit (GATK) v4.1.9.0; 04:59:43.045 INFO ApplyBQSR - For support and documentation go to https://software.broadinstitute.org/gatk/; 04:59:43.045 INFO ApplyBQSR - Executing as on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 04:59:43.045 INFO ApplyBQSR - Java runtime: OpenJDK 64-Bit Server VM v13.0.2+8; 04:59:43.045 INFO ApplyBQSR - Start Date/Time: November 8, 2021 at 4:59:42 a.m. PST; 04:59:43.045 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.045 INFO ApplyBQSR - ------------------------------------------------------------; 04:59:43.046 INFO ApplyBQSR - HTSJDK Version: 2.23.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7549:101,error,error,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7549,1,['error'],['error']
Availability,"Hi; I'm using HaplotypeCaller to do mutation calling in a large single-cell RNA-seq dataset. Im wondering if there is a way to get HaplotypeCaller to spit out _amino acid_ coordinates instead of _genome_? Can we maybe include a column such as 'Mutation.AA' that would contain entries like this? ; <img width=""151"" alt=""screen shot 2018-11-16 at 5 51 52 pm"" src=""https://user-images.githubusercontent.com/33501625/48655137-54309080-e9c8-11e8-9431-6e83b5afd9d7.png"">; It would be very useful for downstream applications like searching for clinically relevant SNPs/indels. ; Thanks; Lincoln",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5431:494,down,downstream,494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5431,1,['down'],['downstream']
Availability,"Hiya,. I downloaded some VCF files for SNP detection in GATK. However when I tried to use them at the recalibration step it said I needed an index, when I try an run the index feature function it gives me the error: Input file is not in valid block compressed format. The files are .VCF.gz. Is there a way of reformatting please?. Best wishes,; B",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7500:9,down,downloaded,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7500,2,"['down', 'error']","['downloaded', 'error']"
Availability,"Hi， @tomwhite ; when I executed this command, I encountered some error:; /gatk-launch BQSRPipelineSpark --knownSites /data/NfsDir/PublicDir/1000g/1000G_phase1.indels.hg19.vcf --knownSites /data/NfsDir/PublicDir/1000g/Mills_and_1000G_gold_standard.indels.hg19.sites.vcf --knownSites /data/NfsDir/PublicDir/dbsnp/dbsnp_138.hg19.vcf -I 1983.align.reorder.sorted.makrdup.bam -O 1983.align.reorder.sorted.makrdup.bqsr.bam -R ~/Tools/hg19.2bit. -------------; 17/10/18 17:35:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]; java.lang.OutOfMemoryError: Java heap space; 	at htsjdk.samtools.SAMUtils.compressedBasesToBytes(SAMUtils.java:146); 	at htsjdk.samtools.BAMRecord.decodeReadBases(BAMRecord.java:346); 	at htsjdk.samtools.BAMRecord.getReadBases(BAMRecord.java:275); 	at org.broadinstitute.hellbender.utils.read.SAMRecordToGATKReadAdapter.getLength(SAMRecordToGATKReadAdapter.java:222); 	at org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary$MatchingBasesAndQualsReadFilter.test(ReadFilterLibrary.java:64); 	at org.broadinstitute.hellbender.engine.filters.ReadFilter$ReadFilterAnd.test(ReadFilter.java:70); 	at org.broadinstitute.hellbender.engine.filters.ReadFilter$ReadFilterAnd.test(ReadFilter.java:70); 	at org.broadinstitute.hellbender.engine.filters.ReadFilter$ReadFilterAnd.test(ReadFilter.java:70); 	at org.broadinstitute.hellbender.engine.filters.ReadFilter$ReadFilterAnd.test(ReadFilter.java:70); 	at org.broadinstitute.hellbender.engine.filters.WellformedReadFilter.test(WellformedReadFilter.java:77); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.lambda$getReads$e4b35a40$1(GATKSparkTool.java:213); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool$$Lambda$93/2063469002.call(Unknown Source); 	at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:76); 	at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:76); 	at scala.collection.Iterator$$",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749:65,error,error,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hm. Just yesterday we updated from TF 1.4 to 1.9. Although this makes it more compelling to switch the default to Intel-optimized, we may still have an issue for the reasons outlined in the previous PR (academic users, not all GCS zones guaranty AVX hardware, and its still unclear to me if Travis, which uses both GCS and EC2, makes such a guaranty). It [sounds like](https://github.com/tensorflow/tensorflow/issues/18689) the failure mode is to crash. For running inference at least (training may be a different story), we may need something better. Another option is that it sounds like its possible to build our own distribution without AVX dependencies to use as a fallback.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429316465:428,failure,failure,428,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429316465,1,['failure'],['failure']
Availability,"Hmm, I am also getting intermittent build errors like the following much more often:. ```; Could not determine the dependencies of task ':sparkJar'.; > Could not resolve all files for configuration ':sparkConfiguration'.; > Could not download gson.jar (com.google.code.gson:gson:2.2.2); > Could not get resource 'https://repo.maven.apache.org/maven2/com/google/code/gson/gson/2.2.2/gson-2.2.2.jar'.; > Could not GET 'https://repo.maven.apache.org/maven2/com/google/code/gson/gson/2.2.2/gson-2.2.2.jar'. Received status code 403 from server: Forbidden; > Could not download core.jar (com.github.fommil.netlib:core:1.1); > Could not get resource 'https://repo.maven.apache.org/maven2/com/github/fommil/netlib/core/1.1/core-1.1.jar'.; > Could not GET 'https://repo.maven.apache.org/maven2/com/github/fommil/netlib/core/1.1/core-1.1.jar'. Received status code 403 from server: Forbidden; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601832142:42,error,errors,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601832142,3,"['down', 'error']","['download', 'errors']"
Availability,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:444,failure,failure,444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205,1,['failure'],['failure']
Availability,"Hmm, an interesting test failure—seems like serialization of Guava ImmutableList might not be cross compatible between Java 8 and 11 (the test file attempting to be deserialized here was generated using Java 11). @cmnbroad what’s the timeline for getting rid of Java 8?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1353536569:25,failure,failure,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1353536569,1,['failure'],['failure']
Availability,"Hmm, it looks like we could perform more checks upstream in GermlineCNVCaller as well. I think it might be also possible to introduce inconsistencies there by using read counts and sharded interval lists with different dictionaries or contig orders (which might have happened here). The canonical dictionary is taken from the first read count file, but the intervals pass through the `-L` machinery and I think any dictionary information is not checked (probably because it will not be present if Picard interval lists were not used as input); intervals are also just (re)sorted by the read-count dictionary to use in subsetting counts. EDIT: Seems like checking intervals for the dictionary may be something we should properly add at the engine level. See e.g. the following `TODO`:. ````; /**; * Returns the ""best available"" sequence dictionary or {@code null} if there is no single best dictionary.; *; * The algorithm for selecting the best dictionary is as follows:; * 1) If a master sequence dictionary was specified, use that dictionary; * 2) if there is a reference, then the best dictionary is the reference sequence dictionary; * 3) Otherwise, if there are reads, then the best dictionary is the sequence dictionary constructed from the reads.; * 4) Otherwise, if there are features and the feature data source has only one dictionary, then that one is the best dictionary.; * 5) Otherwise, the result is {@code null}.; *; * TODO: check interval file(s) as well for a sequence dictionary; * ...; * @return best available sequence dictionary given our inputs or {@code null} if no one dictionary is the best one.; */; public SAMSequenceDictionary getBestAvailableSequenceDictionary() {; ````. We could also just require a sequence dictionary for the relevant tools and/or somehow try to reconcile with dictionaries from other inputs, but I think it's better to check and fail.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718792218:816,avail,available,816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718792218,2,['avail'],['available']
Availability,"Hmm, looks like I deleted the CRAM `/humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/cram/HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.cram`. (I was a high storage cost offender and asked to clean up.) Luckily, I have another CRAM available that I left for testing purposes. I can recapitulate the error with this other CRAM using the old `gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT` jar. ```; -bash-4.1$ /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-launch PrintReads -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram -O /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/test_decram_20171002.bam; ```. Gives the same error at an approximately similar region:; ```; ...; 10:50:54.603 INFO ProgressMeter - chr1:224042054 2.2 11004000 5016945.0; 10:51:04.609 INFO ProgressMeter - chr1:248061327 2.4 11905000 5044206.5; ERROR	2017-10-02 10:51:05	Slice	Reference MD5 mismatch for slice 0:248574592-248771907, AGTGGATGAG...TGTCGGTATG; 10:51:06.940 INFO PrintReads - Shutting down engine; [October 2, 2017 10:51:06 AM EDT] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 2.45 minutes.; Runtime.totalMemory()=5995233280; htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248574592, span 197316, expected MD5 cc8ace0545facc11349da783af07a076; 	at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); 	at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); 	at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:601); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.hasNext(SAMRecordToReadIterator.java:24); 	at java.util.Iterator.forEachRemaining(Iterator.java:115); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.cop",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-333560828:234,avail,available,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-333560828,3,"['avail', 'error']","['available', 'error']"
Availability,"Hmm, looks like we lose events 1 and 3 with CollectReadCounts at 250bp using analogous ModelSegments parameters. However, I experimented with tweaking the segmentation to work on the copy ratios (rather than the log2 copy ratios), which seems to recover them. Although one of the goals of having evaluations backed by SV truth sets is to tune such parameters/methods, I'm beginning to think that SV integration might benefit from using the CNV tools in a more customized pipeline---especially if maximizing sensitivity at resolutions of ~100bp jointly with breakpoint evidence is the goal. For example, you might imagine a tool that directly uses CNV backend code to collect coverage over regions specified by `-L`, builds a PoN, denoises, and segments on the fly. Or we can put together a custom WDL optimized for sensitivity. Let's discuss in person?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372875222:246,recover,recover,246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372875222,1,['recover'],['recover']
Availability,"Hmm, running `./gradlew --info ...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compres",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:137,down,downsampling,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088,1,['down'],['downsampling']
Availability,"Hmm, tests are failing on Travis. To be precise, the allelic-count-only multisample segmentation test is failing---but only in Java 11 (it passes in Java 8). ""Write once, run anywhere"" indeed!. This is one reason why I’m not a fan of exact-match numerical tests… (Another is that they often impede future development by defining behavior too rigidly or adding maintenance costs.) Let me look into it and get back to you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1023649096:360,mainten,maintenance,360,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1023649096,1,['mainten'],['maintenance']
Availability,"Hmm, thanks for suggesting the addition of a regression test @fleharty. This caused me to realize that I actually missed another gap in the previous filtering logic that might have yielded NaNs (resulting from division by zero interval medians) in this particular edge case, which actually takes effect before the rounding error I originally fixed. However, because of how HDF5 writes NaN values as 0, this apparently doesn't lead to any catastrophic failures. We should definitely check that behavior is reasonable in this case (i.e., when interval medians are zero); I've filed #6878. In the end, I added a regression test that only passes with the changes to address the rounding error. This was a bit of a pain because we use simulated data in the tests that cover this code, and the filters are applied in sequential order only on those elements that passed the previous filter. Note that there are many other possible filtering combinations that would be impractical to test. I think that all of this filtering logic was ported over from GATK CNV (I only rewrote the code to perform the filtering in-place to improve memory usage), and I'm not sure that all edge-case behavior was well defined by the original logic (which probably implicitly assumed typical, well formed data, i.e., using more than one sample, without too many uncovered intervals). Fortunately, these edge-case usages (i.e., using a single sample to build the PoN, mistakenly including too many uncovered intervals, and/or disabling various filters) are probably not too common.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6624#issuecomment-705843882:323,error,error,323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6624#issuecomment-705843882,3,"['error', 'failure']","['error', 'failures']"
Availability,"Hmm, that's strange, now only `testGenomicsDBImportFileInputs_newMQ()` is failing. `testGenomicsDBImportFileInputsAgainstCombineGVCFWithNonDiploidData()`, which was failing for me this morning, is now passing despite no additional changes. Error is ""Attribute RAW_MQandDP expected [353426] but found [352585]"". See https://storage.googleapis.com/hellbender-test-logs/build_reports/master_24137.4/tests/test/index.html",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5397#issuecomment-452881372:240,Error,Error,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5397#issuecomment-452881372,1,['Error'],['Error']
Availability,"Hmmm, rather than making assumptions, the aligner could expose its SW parameters and then it comes down to a line of arithmetic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5466#issuecomment-443324822:99,down,down,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5466#issuecomment-443324822,1,['down'],['down']
Availability,"Hmn. That's not the greatest error. It should be failing something more along the lines of ""Your sequence dictionaries don't match, please make sure your files are all aligned using the same reference""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4431#issuecomment-367366377:29,error,error,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4431#issuecomment-367366377,1,['error'],['error']
Availability,Hmn. This is failing with 403 unauthorized errors. Seems like something about authentication changed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6042#issuecomment-511979513:43,error,errors,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6042#issuecomment-511979513,1,['error'],['errors']
Availability,"Hopefully I have attached the files correctly, I also tried increasing the -Xmx option but received the same error. Thanks again for all of your help!; [__array_schema.zip](https://github.com/broadinstitute/gatk/files/5791887/__array_schema.zip). [__book_keeping.tdb.zip](https://github.com/broadinstitute/gatk/files/5791889/__book_keeping.tdb.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-757373484:109,error,error,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-757373484,1,['error'],['error']
Availability,How did you download the funcotator resources? Did you use the official download tool?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8963#issuecomment-2312158845:12,down,download,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8963#issuecomment-2312158845,2,['down'],['download']
Availability,How do we do this? What happens if a job runs locally but fails in the cloud? How do we log and understand errors?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/276:107,error,errors,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/276,1,['error'],['errors']
Availability,How do you want to test this? The error was triggered only if a large number of intervals (~1000) was imported by the tool.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-406751089:34,error,error,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-406751089,1,['error'],['error']
Availability,"How much does count collection cost at the desired bin size? How does this compare to bincov? Perhaps we could eliminate one of these steps if redundant. Note that the read counts are read once and stored in memory, so unless this takes a significant amount of time, then indexing is probably not the highest priority here (although I agree it would be nice to have in general). One related issue, as you mention, is file localization---since each shard only operates on a portion of the counts in each sample, it is a bit wasteful to localize the whole file. But how much does file localization cost? I can't imagine that it is the lowest hanging fruit. One of the more important issues, which you also mention, is optimizing parameters for inference. This includes not only the minimum number of epochs for training, but also things like the learning rate, annealing schedule, iterations per epoch, conditions for epoch convergence, etc. I'll be talking about how to tune these inference parameters---as well as other things in the pipeline---at the next BSV meeting. Let's brainstorm more things to try and prioritize them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932:143,redundant,redundant,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932,2,['redundant'],['redundant']
Availability,"How much memory are you giving java (-Xmx parameter) and how much total; memory does the VM have? I've seen odd behavior when the VM doesn't have; enough headroom (~1-1.5gb) and processes start to die once Java consumes; everything it can right before it OOMs. Might be why you're seeing hanging; on OOM errors?. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Wed, May 3, 2017 at 8:59 AM, Thib <notifications@github.com> wrote:. > I updated the doc with some more info.; > TLDR: with 100 samples there is a visible difference between different; > buffer size in terms of memory usage. However the tool seems to not exit; > properly when it runs out of memory, leaving the VM hanging.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298903456>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g2eDXRdEgkD0zMDRl44RjDvg-4cbks5r2HoxgaJpZM4NNEOf>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298912532:304,error,errors,304,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298912532,1,['error'],['errors']
Availability,"Huh, test failure:. ```; java.io.IOException: GenomicsDB JNI Error: VCFAdapterException : Could not copy contents of VCF header filename gs://hellbender-test-logs/staging/2a733750-eaef-4df5-b971-2437358f5583/vcfheader.vcf to temporary file /tmp/TileDBTapu1R; 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHeadersForQuery(GenomicsDBFeatureReader.java:200); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.<init>(GenomicsDBFeatureReader.java:85); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.getGenomicsDBFeatureReader(GenomicsDBImportIntegrationTest.java:927); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:551); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:521); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testWriteToAndQueryFromGCS(GenomicsDBImportIntegrationTest.java:1104); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:133); 	at org.testng.internal.TestInvoker.invokeMethod(TestInvoker.java:584); 	at org.testng.internal.TestInvoker.invokeTestMeth",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6652#issuecomment-672024253:10,failure,failure,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6652#issuecomment-672024253,2,"['Error', 'failure']","['Error', 'failure']"
Availability,Huh. Our tests should be catching this. Looks like we have a failure in our test pipeline somewhere.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7338#issuecomment-876531256:61,failure,failure,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7338#issuecomment-876531256,1,['failure'],['failure']
Availability,I WAS running this commande : java -jar /Users/mac/Downloads/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar RealignerTargetCreator \ -R /Users/mac/Desktop/LmjFwholegenome_20070731_V5.2.fasta -I /Users/mac/Desktop/NGS/marked-duplicates42.bam -O SRR6369642_realtarget.list ; I get : ; A USER ERROR has occurred: RealignerTargetCreator is no longer included in GATK as of version 4.0.0.0. Please use GATK3 to run this tool. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6703:51,Down,Downloads,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6703,2,"['Down', 'ERROR']","['Downloads', 'ERROR']"
Availability,"I _believe_ that the user error text didn't get updated when the `--sequence-dictionary` argument got added. It would be great to mention that argument in GATKTool::initializeIntervals(), which currently says `""We require a sequence dictionary from a reference, a source of reads, or a source of variants to process intervals. "" +; ""Since reference and reads files generally contain sequence dictionaries, this error most commonly occurs "" +; ""for VariantWalkers that do not require a reference or reads. You can fix the problem by passing a reference file with a sequence dictionary "" +; ""via the -R argument or you can run the tool UpdateVCFSequenceDictionary on your vcf.""`. I would change the last sentence to:; `You can fix the problem by passing a reference file with a sequence dictionary via the -R argument, a *.dict dictionary file via the --sequence-dictionary argument, or you can run the tool UpdateVCFSequenceDictionary on your vcf.""`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4507:26,error,error,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4507,2,['error'],['error']
Availability,"I _believe_ your issue is that you are assigning 600GB to execution of cromwell, but the error is with the call to **VariantRecalibrator** in one of the tasks not having enough memory. A few tasks call **VariantRecalibrator**, do you know which task failed? Can you post the java call from the STDERR file? For me, it was task **SNPsVariantRecalibrator** which was assigned only 3.5GB of memory by default. In [joint-discovery-gatk4.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl), the memory assigned for each task can be set via ""machine_mem_gb"", but it looks like the current [input.json](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json) does not have that variable, but instead ""mem_size"" for each task. . A simple solution would be to replace ${java_mem} with a static value in calls to **VariantRecalibrator** (lines 564 & 684). For example, replace:. `${gatk_path} --java-options ""-Xmx${java_mem}g -Xms${java_mem}g""`. with. `${gatk_path} --java-options ""-Xmx100g -Xms100g""`. I'm not certain this will help, but I think it's a step in the right direction.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6165#issuecomment-571396381:89,error,error,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6165#issuecomment-571396381,2,['error'],['error']
Availability,"I accidentally deleted this comment that @davidbenjamin left:. The index out-of-bounds error is caused by a problem with the germline resource VCF. I found the following line:. chr38 353546 . TGGGGGG TGGG,TGGGGG,TG,TGG,TGGGG,T 18995.20 PASS AC=30,5,4,5,3,2;AF=0.385,0.064,0.077,0.038,0.026;AN=76;BaseQRankSum=0;ClippingRankSum=0;DP=5904;ExcessHet=5.0369;FS=29.914;InbreedingCoeff=0.5846;MLEAC=46,6,10,7,5,3;MLEAF=0.069,0.009036,0.015,0.011,0.00753,0.004518;MQ=32.34;MQRankSum=0;QD=25.69;ReadPosRankSum=0.674;SOR=1.63; Note how there are 6 alt alleles but only 5 values of AF. Strangely, there are 6 values of MLEAF. Do you know why one of the AFs is missing?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098#issuecomment-541103429:87,error,error,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098#issuecomment-541103429,1,['error'],['error']
Availability,"I added -m to `gsutil cp` in a previous PR but missed the `gsutil mv` step post-`bq load` - so here that is. tested it from the command line, works well. also confirmed that it will throw an error if one (or more) files has an error:. ```; $ cat test_files_bucket.txt | gsutil -m mv -I gs://dsp-fieldeng-dev/test_mv/. If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. Copying gs://dsp-fieldeng-dev/test_cp/test1.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test2.txt [Content-Type=text/plain]...; CommandException: No URLs matched: gs://dsp-fieldeng-dev/test_cp/test4.txt; Copying gs://dsp-fieldeng-dev/test_cp/test3.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test5.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test6.txt [Content-Type=text/plain]...; Removing gs://dsp-fieldeng-dev/test_cp/test1.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test2.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test3.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test5.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test6.txt...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Operation completed over 5 objects/37.0 B.; CommandException: 1 file/object could not be transferred.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7129:191,error,error,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7129,3,"['avail', 'error']","['available', 'error']"
Availability,"I added a PR to Barclay (https://github.com/broadinstitute/barclay/pull/95) to both change the extension and to switch off the behaviour if need it. That may solve other errors in the future too, if other downstream project want to use the extension for other purposes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3555#issuecomment-331451475:170,error,errors,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3555#issuecomment-331451475,2,"['down', 'error']","['downstream', 'errors']"
Availability,"I added a test according to #4642 , but can't reproduce the error. The user also noted that the error message was badly formed, which is true because it ended with a colon. Now it looks like:; ""Input src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleBad.vcf fails strict validation of type CHR_COUNTS: the Allele Count (AC) tag is incorrect for the record at position 1:985447, 1 vs. 2""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6076:60,error,error,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6076,2,['error'],['error']
Availability,I added some logging to show the partition boundaries and highlight the one the tripped the error (partition 631):. ```; partition 0: 0:9951; partition 1: 0:1085271; partition 2: 0:1900519; partition 3: 0:2658854; partition 4: 0:3369191; partition 5: 0:4271199; partition 6: 0:5234375; partition 7: 0:6165442; partition 8: 0:7043366; partition 9: 0:7945558; partition 10: 0:8834692; partition 11: 0:9712270; partition 12: 0:10597163; partition 13: 0:11491456; partition 14: 0:12370805; partition 15: 0:13369623; partition 16: 0:14335062; partition 17: 0:15273859; partition 18: 0:16143005; partition 19: 0:16852569; partition 20: 0:17683999; partition 21: 0:18625819; partition 22: 0:19514451; partition 23: 0:20434486; partition 24: 0:21333429; partition 25: 0:22209324; partition 26: 0:23102764; partition 27: 0:23983047; partition 28: 0:24866543; partition 29: 0:25761444; partition 30: 0:26663082; partition 31: 0:27541379; partition 32: 0:28425761; partition 33: 0:29319492; partition 34: 0:30281360; partition 35: 0:31192475; partition 36: 0:32077299; partition 37: 0:32961582; partition 38: 0:33876305; partition 39: 0:34855082; partition 40: 0:35781231; partition 41: 0:36672237; partition 42: 0:37620948; partition 43: 0:38541211; partition 44: 0:39457650; partition 45: 0:40367874; partition 46: 0:41269622; partition 47: 0:42168199; partition 48: 0:43088153; partition 49: 0:43971528; partition 50: 0:44872517; partition 51: 0:45791428; partition 52: 0:46682008; partition 53: 0:47642633; partition 54: 0:48631636; partition 55: 0:49673572; partition 56: 0:50675533; partition 57: 0:51594369; partition 58: 0:52498726; partition 59: 0:53406199; partition 60: 0:54327933; partition 61: 0:55241888; partition 62: 0:56229391; partition 63: 0:57216062; partition 64: 0:58233271; partition 65: 0:59158762; partition 66: 0:60117689; partition 67: 0:61089610; partition 68: 0:62043586; partition 69: 0:62985433; partition 70: 0:63941420; partition 71: 0:64889811; partition 72: 0:65830604; partiti,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3717#issuecomment-337936683:92,error,error,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3717#issuecomment-337936683,1,['error'],['error']
Availability,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:61,error,error,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208,2,['error'],['error']
Availability,"I addressed the crash (caused by calling shift with too large a number and set-ing strict error checks in bash). I also added the requested options. Due to the larger number of options I decided that using only positional arguments would be impractical (requiring counting the number of empty """" default arguments) so I added a unix-like option parsing scheme to manage_sv_pipeline.sh; --quiet or -q will cause the script to run without any user prompting; --save [bucket/path] or -s [bucket/path] will save results and initialization scripts within the specified bucket/path",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3370#issuecomment-319452179:90,error,error,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3370#issuecomment-319452179,1,['error'],['error']
Availability,"I addressed your comments, @cmnbroad. In addition, to make more consistent the errors for malformed `GATKRead`, I updated `ReadMissingReadGroup` and removed `MalformedRead` in two separated commits. Like that, every offending read is passing by `MalformedBAM` for consisten error messages. Back to you and many thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2282#issuecomment-268452263:79,error,errors,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2282#issuecomment-268452263,2,['error'],"['error', 'errors']"
Availability,I agree a lot of the detailed content is redundant and not worth maintaining in separate places. Reducing to 2 or 3 lines seems a bit too drastic though -- I would love to see something like a half-page high-level overview to serve as cliff notes for the WDL.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5889#issuecomment-484356108:41,redundant,redundant,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5889#issuecomment-484356108,1,['redundant'],['redundant']
Availability,I agree that it is a good security measure to use fixed signed dependencies for repeatable builds. GATK depends on gradle 3.1.: [download](https://services.gradle.org/distributions/gradle-3.1-bin.zip) [shaw256](https://services.gradle.org/distributions/gradle-3.1-bin.zip.sha256),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5483#issuecomment-444208372:129,down,download,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5483#issuecomment-444208372,1,['down'],['download']
Availability,"I agree with all Chris has said, and think that it's very likely that you're running out of memory on the executors. You might try cutting back on --num-executors, and bumping up --executor-memory.; If you can figure out your adapter sequence, you can specify that as --adaptor-sequence, and sometimes that helps with this stage.; We're laying down asphalt, and you're driving on the hot pavement just behind us. Thanks for trying out this tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314:344,down,down,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314,1,['down'],['down']
Availability,I also encounter this error when most samples have been imported. I ran importing in batches '--batch-size 50 --consolidate '. The error occured at the last batch. Can I reuse some of the imported data files or have to rerun the whole importing again?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6519#issuecomment-641090074:22,error,error,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6519#issuecomment-641090074,2,['error'],['error']
Availability,"I also just noticed that tests are failing on the branch because they still reference the old constants in a number of places:. ```; symbol: variable READ_NAME_LONG_NAME; location: class ReadNameReadFilter; /gatk/src/test/java/org/broadinstitute/hellbender/cmdline/GATKPlugin/GATKReadFilterPluginDescriptorTest.java:117: error: cannot find symbol; { PlatformReadFilter.class.getSimpleName(), ""--"" + PlatformReadFilter.PL_FILTER_NAME_LONG_NAME, ""fakePlatform"" }, ; ```. You'll need to update these references in order to get tests passing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4103#issuecomment-360806542:321,error,error,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4103#issuecomment-360806542,1,['error'],['error']
Availability,"I also removed the obsolete errorProbability variable line of code in the SomaticGenotypingEngine.java and noted this argument is deprecated in the M2ArgumentCollection. Somewhat relatedly, see request in #3123.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3124:28,error,errorProbability,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3124,1,['error'],['errorProbability']
Availability,I also run into this error. I am using the `gatk `launcher script. My command is `gatk Mutect2 -R /bio/bcbio/genomes/Hsapiens/hg38/seq/hg38.fa -I chenmeifang-ready.bam -max-mnp-distance 0 -O normal_1.vcf.gz`. Is there any method to work through this error?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5900#issuecomment-541411050:21,error,error,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5900#issuecomment-541411050,2,['error'],['error']
Availability,"I also tried the following approach which did not generate an error:. 1. imported the 10 not-reblocked gvcfs from chr16 into genomicsdb ; 2. GenotypeGVCFs with the same command line as number 3 above. . So the error appears to be related to the reblocking of the gvcfs. ```; gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:105582-211160 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-105582-211160.vcf.gz; 07:46:18.893 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 07:46:18.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 7:46:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:46:19.128 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.128 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 07:46:19.128 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 07:46:19.129 INFO GenotypeGVCFs - Executing as farre",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278:62,error,error,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278,2,['error'],['error']
Availability,I am also getting the same error upon using the CollectReadCounts command. Can someone please tell when will issues with m1 macs be solved?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7297#issuecomment-1257263962:27,error,error,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7297#issuecomment-1257263962,1,['error'],['error']
Availability,"I am also getting this error on chromosome 11:; ```; 11:49:44.992 INFO gcnvkernel.postprocess.viterbi_segmentation - Segmenting contig (5/12) (contig name: 9)...; 11:49:44.996 INFO gcnvkernel.postprocess.viterbi_segmentation - Segmenting contig (6/12) (contig name: 11)... Stderr: Traceback (most recent call last):; File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/compile/function_module.py"", line 903, in __call__; self.fn() if output_subset is None else\; File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 963, in rval; r = p(n, [x[0] for x in i], o); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 952, in p; self, node); File ""scan_perform.pyx"", line 215, in theano.scan_module.scan_perform.perform; NotImplementedError: We didn't implemented yet the case where scan do 0 iteration. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/segment_gcnv_calls.6491270870870970325.py"", line 79, in <module>; viterbi_engine.write_copy_number_segments(); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/gcnvkernel/postprocess/viterbi_segmentation.py"", line 234, in write_copy_number_segments; for segment in self._viterbi_segments_generator():; File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/gcnvkernel/postprocess/viterbi_segmentation.py"", line 160, in _viterbi_segments_generator; log_prior_c, log_trans_contig_tcc, copy_number_log_emission_contig_tc); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/gcnvkernel/models/theano_hmm.py"", line 88, in perform_forward_backward; prev_log_posterior_tc, admixing_rate, temperature))); File ""/ngc/projects/gm/data/res",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-613371282:23,error,error,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-613371282,1,['error'],['error']
Availability,"I am also interested in. 1. Rescue rare variants.; 2. For variants that are not called in a sample, get the REF/ALT counts for them for downstream analysis. and I didn't notice this issue and [opened my own](https://github.com/broadinstitute/gatk/issues/7847). However, isn't it Best Practice to use HaplotypeCaller for germline variants?. ![image](https://user-images.githubusercontent.com/631218/169218653-9be2caa1-5c29-4c56-8fa6-8d10e52b3e29.png)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7825#issuecomment-1131250367:136,down,downstream,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7825#issuecomment-1131250367,1,['down'],['downstream']
Availability,"I am also running this on a VCF w/ ~500k variants and Funcotator seems to be producing annotations. Definitely, seems to have made progress beyond the original point of failure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4723#issuecomment-385578759:169,failure,failure,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4723#issuecomment-385578759,1,['failure'],['failure']
Availability,"I am attempting to create fasta files from specific regions in approximately 270 VCF files. For every other region/gene I've looked at, I have not had this issue. For one particular region (mrr1), I am getting the error seen below. I checked the coverage of the bam file and viewed the vcf in IGV viewer, but notice no problems. Can you please advise? Thank you. Similar to this issue, but am still not sure how to approach it?; https://github.com/broadinstitute/gatk/issues/6260#issue-521418442. Bash script:; ```; #!/bin/bash --login; #SBATCH --time=1:00:00 # limit of wall clock time - how long the job will run; #SBATCH --ntasks=1 # number of tasks - how many tasks (nodes) that you requir; #SBATCH --cpus-per-task=1 # number of CPUs (or cores) per task (same as -c); #SBATCH --mem=50G # memory required per node - amount of memory (in bytes); #SBATCH --job-name=VCF_FastaNEP_CCR; #SBATCH --mail-user=lukaskon@msu.edu; #SBATCH --mail-type=ALL; #SBATCH -o SpeciesID_CCR7_slurm. cd /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/. module load Java/JDK12. for sample in AI7 W18 B5 BU9 I9 R23 Y1; do; base=$(basename ${sample}). gatk-4.2.5.0/gatk SelectVariants -R /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/0_DNAscripts/ReferenceGenome/Botrytis_cinerea.ASM83294v1.dna.toplevel.fa -V /mn; t/research/Hausbeck_group/Lukasko/BotrytisDNASeq/10_FilteredVCF/Plates123/BcinereaP123.SNVonly.filteredPASS_renamed.vcf -sn ${sample} --remove-unused-alternates --exclu; de-sample-name /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/CCR7/ConservedGenes/ExcludeList.args -O /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/CCR7/Cons; ervedGenes/VCFs/${base}.vcf. gatk-4.2.5.0/gatk FastaAlternateReferenceMaker -R /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/0_DNAscripts/ReferenceGenome/Botrytis_cinerea.ASM83294v1.dna.tople; vel.fa -O /mnt/research/Hausbeck_group/Lukasko/BotrytisDNASeq/CCR7/ConservedGenes/mrr1/${base}_mrr1.fasta -L 5:680219-684662 -V /mnt/research/Hausbeck_group/Lu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8427:214,error,error,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8427,1,['error'],['error']
Availability,"I am available to help in this process @jonn-smith. Just to give you an idea of the evolution of tutorials, here are some example tutorials that focus on somatic CNV calling. - Alpha tutorial that Lee wrote:; <https://gatkforums.broadinstitute.org/gatk/discussion/7387/description-and-examples-of-the-steps-in-the-acnv-case-workflow>; - A demo-like tutorial that Sam Lee planned and developed (also Mehrtash was involved so don't be shy about asking around for help) and I encased in writing:; <https://gatkforums.broadinstitute.org/gatk/discussion/9143/how-to-call-somatic-copy-number-variants-using-gatk4-cnv/p1>; - A tutorial to highlight the factors in PoN creation currently used in workshops that I planned, developed and wrote: [GATK4_SomaticCNV_worksheet.pdf](https://github.com/broadinstitute/gatk/files/1435691/GATK4_SomaticCNV_worksheet.pdf). Depending on how much responsibility you want to take (I think it nice for you to post a tutorial on the forum under your name for posterity), you can choose to get my review only or have me help in brainstorming, organizing and/or formatting the content. I have a template available for copy-pasting with formatting elements at <https://gatkforums.broadinstitute.org/dsde/discussion/9140/how-to-tutorial-template-a-la-soo-hee-for-copy-pasting#latest>. Notice this document is only visible to DSDE members and similarly, you can draft a tutorial in private first then move it to a public forum. Here are some steps I go through in developing a tutorial. You may choose to skip certain elements, e.g. example data. I have to say that having example data really helps the users.; - Find small test data to illustrate important features of the tool; ensure data is publically sharable; - Write out commands that include recommended parameters; - Explain the important parameters and the impact of changing them; - Illustrate with results and screenshots; - Get reviewed early by others and incorporate changes; - A section of links to related or help",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3774:5,avail,available,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3774,1,['avail'],['available']
Availability,"I am encountering a similar error: ; ```; Using GATK jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /lustre/haven/proj/UTHSC0013/Tristan_GATK/reference/genome.fa -V gendb:///lustre/haven/proj/UTHSC0013/Tristan_GATK//DB/chr7 -G StandardAnnotation --use-new-qual-calculator -O /lustre/haven/proj/UTHSC0013/Tristan_GATK//gvcf//merged//joint_called_gvcfs_chr7.vcf; 23:15:47.053 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 23:15:47.249 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 07, 2020 11:15:49 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:15:49.543 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.545 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 23:15:49.546 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:15:49.547 INFO GenotypeGVCFs - Executing as hchen3@acf-knl002 on Linux v3.10.0-514.26.1.el7.x86_64 amd64; 23:15:49.548 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 23:15:49.548 INFO GenotypeGVCFs - Start Date/Time: January 7, 2020 11:15:47 PM EST; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.551 INFO GenotypeGVCFs - HTSJDK Version: 2.19.0; 23:15:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057:28,error,error,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057,2,"['Redundant', 'error']","['Redundant', 'error']"
Availability,"I am experiencing this issue, which I thought was solved. I have used version 4.1.7.0 haplotype caller, then when I use combineGVCFs I receive the error: A USER ERROR has occurred: Bad input: Combining gVCFs containing MNPs is not supported. Unknown contained a MNP at A1:684599. I didn't set any value for -max-mnp-dist as the docs say that default is already set to 0. Any idea how to get around this error?. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6510#issuecomment-646951652:147,error,error,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6510#issuecomment-646951652,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I am facing same problem when I tried to merge Mutect2 vcf files (from several patients) to one. I tried to use bcftools merge, but I got a same error. I used Mutect2 of GATK4.1.9.0. From when does Mutect2 have this error? . I am thinking of downgrade GATK tools such as 4.1.7.0 or older version.; Does anyone have information? Which GATK4 Mutect2 version should I try?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6857#issuecomment-749244150:145,error,error,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6857#issuecomment-749244150,3,"['down', 'error']","['downgrade', 'error']"
Availability,I am getting the following error when creating the gatk environment with conda env create -n gatk -f scripts/gatkcondaenv.yml. NoPackagesFoundError: Package missing in current linux-64 channels:; - intel-openmp 2018.0.0*. Below is the complete listing. Any suggestions?. .```; /gradlew createPythonPackageArchive; :createPythonPackageArchive UP-TO-DATE. BUILD SUCCESSFUL. Total time: 13.183 secs. conda env create -n gatk -f scripts/gatkcondaenv.yml; Using Anaconda API: https://api.anaconda.org; Fetching package metadata ............. NoPackagesFoundError: Package missing in current linux-64 channels:; - intel-openmp 2018.0.0*; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4822:27,error,error,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4822,1,['error'],['error']
Availability,I am getting the same error... it might be caused by something else?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1155623336:22,error,error,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1155623336,1,['error'],['error']
Availability,I am getting the same issue when I use `--genotyping-mode GENOTYPE_GIVEN_ALLELES` even if the `--alleles` file contains only SNPs and no indels. It is very difficult to get around / debug as I don't know what variants the GATK HaplotypeCaller was working on when it shut down. Was this bug introduced with version 4.1.0.0?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6037#issuecomment-512350612:271,down,down,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037#issuecomment-512350612,1,['down'],['down']
Availability,"I am getting the similar error (gatk 4.0.8.1), and in my case, it does not help even if I trim end by 1-base. Does trimming one base also apply to length of an entire chromosome if I am supplying `-L 1:1-100000`?. ```; [August 28, 2018 6:10:37 PM EDT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.45 minutes.; Runtime.totalMemory()=2649751552; java.lang.StringIndexOutOfBoundsException: String index out of range: 177; at java.lang.String.substring(String.java:1963); at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:48); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:270); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:176); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:211); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:212); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:291); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:267); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:979); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5130#issuecomment-416759214:25,error,error,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5130#issuecomment-416759214,1,['error'],['error']
Availability,I am getting this error with HaplotypeCaller 4.0.0.0. Was this bug fix released in that version? Otherwise there is more work to do. . I recently reported the error in the GATK Forum....; https://gatkforums.broadinstitute.org/gatk/discussion/11207/gatk-4-0-0-0-haplotypecaller-error-with-l-chrm#latest,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3845#issuecomment-358443805:18,error,error,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3845#issuecomment-358443805,3,['error'],"['error', 'error-with-l-chrm']"
Availability,"I am having a similar issue with GATK `4.1.4.1` that persists after updating to `4.2.0.0`:; ```; 00:33:06.768 INFO BaseRecalibrationEngine - The covariates being used here:; 00:33:06.768 INFO BaseRecalibrationEngine - ReadGroupCovariate; 00:33:06.768 INFO BaseRecalibrationEngine - QualityScoreCovariate; 00:33:06.768 INFO BaseRecalibrationEngine - ContextCovariate; 00:33:06.768 INFO BaseRecalibrationEngine - CycleCovariate; 21/03/28 00:33:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on fend04.cluster:42128 in memory (size: 35.5 KB, free: 5.2 GB); 21/03/28 00:33:14 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1); java.lang.IllegalArgumentException: Table1 1,3 not equal to 2,3; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.combineTables(RecalUtils.java:560); at org.broadinstitute.hellbender.utils.recalibration.RecalibrationTables.combine(RecalibrationTables.java:144); at org.broadinstitute.hellbender.utils.recalibration.RecalibrationTables.inPlaceCombine(RecalibrationTables.java:178); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction2$1.apply(JavaPairRDD.scala:1037); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190); at org.apa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854#issuecomment-808817724:578,ERROR,ERROR,578,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854#issuecomment-808817724,1,['ERROR'],['ERROR']
Availability,"I am having an issue with the same bug, where these `MPOS=-2147483648` fields are causing an error in downstream analysis. If anyone has any update or a workaround I'd appreciate it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6342#issuecomment-584699134:93,error,error,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6342#issuecomment-584699134,2,"['down', 'error']","['downstream', 'error']"
Availability,"I am having the same problem.; The last lines read as follow:; ```; 12:15:53.217 INFO ProgressMeter - chr3:73018046 285.5 2140110 7497.0; 12:16:03.395 INFO ProgressMeter - chr3:73445027 285.6 2141720 7498.2; 12:16:13.407 INFO ProgressMeter - chr3:75481384 285.8 2149100 7519.7; 12:16:16.285 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 4.453178435; 12:16:16.286 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 2290.3264339380003; 12:16:16.286 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 4541.79 sec; 12:16:16.289 INFO Mutect2 - Shutting down engine; [April 20, 2020 12:16:16 PM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 285.86 minutes.; Runtime.totalMemory()=8561098752; java.lang.IndexOutOfBoundsException: Index: 0, Size: 0; 	at java.util.ArrayList.rangeCheck(ArrayList.java:657); 	at java.util.ArrayList.get(ArrayList.java:433); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.lambda$getGermlineAltAlleleFrequencies$26(SomaticGenotypingEngine.java:339); 	at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546); 	at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); 	at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:508); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getGermlineAltAlleleFrequencies(SomaticGenotypingEngine.java:341); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getNegativeLogPopulationAFAnnotation(SomaticGenotypingEngine.java:324); 	at org.broadinstitute.hellbender.tools.walkers.mutect",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-616557568:606,down,down,606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-616557568,1,['down'],['down']
Availability,"I am looking at using GATK and first checked at the docker image using **_docker pull broadinstitute/gatk_**. this container image has 1460 vulnerabilities and a lot of them are critical. ; <img width=""1737"" alt=""Screenshot 2023-02-21 212830"" src=""https://user-images.githubusercontent.com/4427764/220508376-aeead13b-999b-4cfd-a7d6-295241df532a.png"">. Then I decided not to use this image and instead create my own image and just deploy the released version 4.2.6.1 from here (https://github.com/broadinstitute/gatk/releases/download/4.2.6.1/gatk-4.2.6.1.zip). Even this has many vulnerabilities include things stemming from log4j 1.2.17. These have been fixed by log4j team years back in version 2.17.1 onwards. I am really stunned that a popular library like gatk is not keeping up with basic security fixes. <img width=""854"" alt=""Screenshot 2023-02-21 212751"" src=""https://user-images.githubusercontent.com/4427764/220508300-7bfe331d-8286-4950-a6dc-e1f5f97c65d0.png"">. the latest version of docker desktop has integrated image scanning and can very easily highlight the issues listed above. Can we start addressing these issues sooner than later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215:525,down,download,525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215,1,['down'],['download']
Availability,"I am running GATK GenotypeGVCFs, v4.2.6.1. I am trying to call Genotypes on a GenomicsDB workspace with about 500 WGS samples. Note, this is the macaque MMul10 genome, so it has 2,939 contigs (including unplaced). We've run commands like this quite a lot before, though we periodically do have issues like this. We can consolidate on this workspace prior to running this (using a standalone tool @nalinigans provided on #7674). As you can see we ran java with relatively low RAM, but left ~150G for the C++ layer. I'm surprised this isnt good enough. . I'm going to try to interactively inspect this, but the error is from slurm killing my job, not a java memory error, which I believe means the -Xmx 92G isnt getting exceeded. I could be mistaken though. You'll also see: 1) I'm using --force-output-intervals, 2) I'm giving it -XL to excluded repetitive regions (and therefore also skipping some of the more gnarly and memory-intensive sites), and 3) I'm giving it a fairly small -L interval list (this is split into 750 jobs/genome). . ```; java8 \; -Djava.io.tmpdir=<folder> \; -Xmx92g -Xms92g -Xss2m \; -jar GenomeAnalysisTK4.jar \; GenotypeGVCFs \; -R 128_Mmul_10.fasta \; --variant gendb:///<path>/WGS_v2_db03_500.gdb \; -O WGS_v2_db03_500.temp.vcf.gz \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 \; -XL NCBI_Mmul_10.softmask.bed \; --max-alternate-alleles 6 \; --genomicsdb-max-alternate-alleles 9 \; --force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; -L 14:37234750-41196525 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations; ```. Each job gets about 250K to 800K variants into the data, and then they pretty consistently start to exceed memory and get killed. . Does anyone have suggestions on debugging or troubleshooting steps? Thanks in advance for any help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968:609,error,error,609,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968,2,['error'],['error']
Availability,"I am running Gatk SelectVariant with -L and -ip options to filter out variants that are not inside my bed defined region +- interval padding. I am running gatk version 4.1.0.0 and for some of my task fails and returns ``` rc 1 (exit code 1) ``` but there is no clear error message on any logs. . ``` Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Dsamjdk.compression_level=5 -Xms100g -Xmx100g -jar /root/gatk.jar SelectVariants -L /cromwell_root/mybucket/ref/bed_files/mybedfile.bed -R /cromwell_root/mybucket2/NGS/ref/hg38/v0/Homo_sapiens_assembly38.fasta -V /cromwell_root/mybucket/cromwell-execution/mypipeline/2baacdb4-d3c5-4d98-afb2-6578c3ddcda9/call-MT2/calling.Mutect2/a4839059-9209-42da-b106-a91393c47546/call-Filter/input.vcf -ip 20 -O output.vcf --verbosity DEBUG ; ```. Task seems to end prematurely but I can not find out why. Also output file is generated but it only has variants from chr 1 even though my sample is whole exome, which also supports the premature end of task theory. Stdout is empty and stderr seems to end prematurely. [failing_SelectVariants-stderr.log](https://github.com/broadinstitute/gatk/files/5652756/failing_SelectVariants-stderr.log)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6990:267,error,error,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6990,1,['error'],['error']
Availability,"I am running into an issue with MarkDuplicatesSpark where it runs successfully in local mode, but if I run it in standalone mode with files on the file system or hdfs I get unexpected results. It could be that I'm doing something very basic wrong here, so I'm pasting a bunch of contextual information in case it helps. Thanks in advance!. ```; # Output BAM is correct in local mode:; $GATK_DIR/gatk-launch MarkDuplicatesSpark -I NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam -O test.local.bam. # Outputs empty BAM (i.e. header but no alignments) in standalone mode:; $GATK_DIR/gatk-launch MarkDuplicatesSpark -I NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam -O test.standalone.bam -- --sparkRunner SPARK --sparkMaster spark://localhost:7077. # Errors out with a wrong FS type (stacktrace below):; $GATK_DIR/gatk-launch MarkDuplicatesSpark -I hdfs://bam/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam -O hdfs://bam/test.hdfs.bam -- --sparkRunner SPARK --sparkMaster spark://localhost:7077; ```. And the stacktrace for the HDFS case (I'm running HDFS in Pseudo-Distributed mode on the same host as I a running Spark standalone mode on): . ```; org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=1029177344; java.lang.IllegalArgumentException: Wrong FS: hdfs://bam/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam, expected: file:///; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:465); at org.apache.hadoop.fs.FilterFileSystem.makeQualified(FilterFileSystem.java:119); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:181); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1444:769,Error,Errors,769,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444,1,['Error'],['Errors']
Availability,"I am running sortsam and getting error Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2299:33,error,error,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299,2,['error'],['error']
Availability,"I am seeing this same error with gatk 4.1.9.0. At the very least, it would be nice if Funcotator caught the error and printed a warning, omitted the problematic variant, and continued instead of crashing and leaving truncated output. the VCF I'm annotating is WGS with 150+ samples, and it's crashing on a variant on chr9.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-783510524:22,error,error,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-783510524,2,['error'],['error']
Availability,"I am starting a Maven Project in which I would like to import your library; so I added this [dependency](http://search.maven.org/#artifactdetails%7Corg.broadinstitute%7Cgatk%7C4.beta.2%7C) to my pom.xml; ```; <dependency>;     <groupId>org.broadinstitute</groupId>;     <artifactId>gatk</artifactId>;     <version>4.beta.2</version>; </dependency>; ```; When I execute `mvn clear install` in my folder project, I receive this error: ; ```; [ERROR] Failed to execute goal on project GATKpipe: ; Could not resolve dependencies for project uk.ac.ncl:GATKpipe:jar:0.0.1-SNAPSHOT: ; Could not find artifact com.github.fommil.netlib:all:jar:1.1.2 in ; all (https://mvnrepository.com/artifact/com.github.fommil.netlib/all) -> [Help 1]; ```; and it seems that the problem is the dependency by com.github.fommil.netlib/all, indeed according to the output of `mvn clear install`, it attempt to download all-1.1.2.jar:; `Downloading: https://repo.maven.apache.org/maven2/com/github/fommil/netlib/all/1.1.2/all-1.1.2.jar`; but this jar is not available in the repository. I noticed that even in other [projects](https://github.com/amplab/ml-matrix/issues/11) have the same issue. How is possible to resolve this issue? . Thanks for your time,; Nicholas",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3724:426,error,error,426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3724,5,"['Down', 'ERROR', 'avail', 'down', 'error']","['Downloading', 'ERROR', 'available', 'download', 'error']"
Availability,"I am trying to do SNPs call using GATK but I am getting below error. Any idea how can I solve this error? . (base) glier_ubuntu@glierubuntu-Precision-7920-Tower:/media/glier_ubuntu/4TB/Javad_Final/7gatk/ScriptforSNP$ call_snps_gatk_firstpass.sh 6 '/media/glier_ubuntu/4TB/Javad_Final/5-trinity/Fastajavad_Trinity/Trinity.fasta' '/media/glier_ubuntu/4TB/Javad_Final/6bwa/2/filtered_merged.bam' ; Using GATK jar /home/glier_ubuntu/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -jar /home/glier_ubuntu/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar CreateSequenceDictionary -R /media/glier_ubuntu/4TB/Javad_Final/5-trinity/Fastajavad_Trinity/Trinity.fasta -O /media/glier_ubuntu/4TB/Javad_Final/5-trinity/Fastajavad_Trinity/Trinity.dict; 15:46:56.167 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/glier_ubuntu/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Wed May 13 15:46:56 EDT 2020] CreateSequenceDictionary --OUTPUT /media/glier_ubuntu/4TB/Javad_Final/5-trinity/Fastajavad_Trinity/Trinity.dict --REFERENCE /media/glier_ubuntu/4TB/Javad_Final/5-trinity/Fastajavad_Trinity/Trinity.fasta --TRUNCATE_NAMES_AT_WHITESPACE true --NUM_SEQUENCES 2147483647 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; May 13, 2020 3:46:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Wed May 13 15:46:57 EDT 2020] Executing as glier_ubuntu@glierubuntu-Precision-7920-Tower on Linux 4.15.0-99-generic am",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6604:62,error,error,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6604,2,['error'],['error']
Availability,"I am trying to run GATK (v4.1.8.0) BaseRecalibrator for my whole exome sequencing data. I generated the interval_list file using picard such that:. ```; dict=/data/anderslab/Annotation/GATK/Homo_sapiens_assembly38.dict; picard_jar=/nfs/software/helmod/apps/Core/picard-tools/2.4.1-gcb01/picard.jar; java -jar ${picard_jar} BedToIntervalList \; I=Padded.bed \; O=Padded.interval_list \; SD=${dict}; ```; But when I feed the file into the GATK I got an error:. ```; [August 18, 2020 10:31:17 AM EDT] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2224553984; ***********************************************************************. A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ; ""/data/refs/GATK/S31285117_hs_hg38/interval_list/Padded.interval_list"" is not valid for this input. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; ```; Some suggestions?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6755:451,error,error,451,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6755,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I am trying to use FilterVariantTranches in GATK 4.0.3.0 after running CNNScoreVariants. ```; ./gatk FilterVariantTranches \; -V test.cnnscore.vcf \; --snp-truth-vcf hapmap_3.3.hg19.sites.vcf \; --indel-truth-vcf Mills_and_1000G_gold_standard.indels.hg19.sites.vcf \; --info-key CNN_1D \; --tranche 99.9 --tranche 99.0 --tranche 95 \; --max-sites 8000 \; -O test.cnnscore.filtered.vcf; ```. There are also index files in the directory. ```; test.cnnscore.vcf.idx (generated by CNNScoreVariants); hapmap_3.3.hg19.sites.vcf.idx; Mills_and_1000G_gold_standard.indels.hg19.sites.vcf.idx; ```. and I got the error. ```; Traceback (most recent call last):; File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 124, in <module>; run(); File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 10, in run; write_tranches(args); File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 34, in write_tranches; v_scored = allele_in_vcf(allele, variant, vcf_reader); File ""/tmp/zzxzxzzxz/tranches.5887233932112211461.py"", line 84, in allele_in_vcf; variants = vcf_ram.fetch(variant.contig, variant.pos-1, variant.pos); File ""pysam/libcbcf.pyx"", line 4321, in pysam.libcbcf.VariantFile.fetch; ValueError: fetch requires an index. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.FilterVariantTranches.doWork(FilterVariantTranches.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4794:603,error,error,603,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4794,1,['error'],['error']
Availability,"I am trying version 4.2.3.0. I set `--java-options ""-Xmx128g""` and it gradually increased to consume all of the RAM. ```; PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND; 3286969 dario 20 0 136.7g 133.2g 27012 S 1478 26.4 725:33.44 java; ```. The input is a modest whole genome sequencing BAM file. ```; $ ls -lh CSCC_0163-B1.final*; -r-------- 1 dario stgrad 9.2M Dec 15 10:22 CSCC_0163-B1.final.bai; -r-------- 1 dario stgrad 86G Dec 15 10:21 CSCC_0163-B1.final.bam; ```. After about one hour without any progress messages, the process shuts down due to running out of heap space. ```; 12:03:07.666 INFO ProgressMeter - Starting traversal; 12:03:07.666 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; 13:08:22.336 INFO GetPileupSummaries - Shutting down engine; [December 15, 2021 at 1:08:22 PM AEDT] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 84.45 minutes.; Runtime.totalMemory()=30333206528; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; ```. The two other key parameters were `-L af-only-gnomad.hg38.vcf.gz -V af-only-gnomad.hg38.vcf.gz`. I think this module is far too inefficient to be in production, if this is how it typically works.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7606:550,down,down,550,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7606,2,['down'],['down']
Availability,"I am using ASEReadCounter to call allelic read counts on 1000 genome reference. But, I found ASEReadCounter generatd only header in output file. Here I enclosed my command and stderr log. Please help me to check it. Thank you!. If you are seeing an error, please provide(REQUIRED) :; a) GATK version used: 4.1.8.1; b) Exact command used:. java -Xmx8000m -Djava.io.tmpdir=/broad/hptmp/cbao \; -jar ${path2gatk}/gatk-package-4.1.8.1-local.jar \; ASEReadCounter \; -L scattered.interval_list \; -R Homo_sapiens_assembly19.fasta \; -V 1000G_phase1.snps.high_confidence.b37.vcf.gz \; -I downsample_10k.bam \; -O output.txt --verbosity INFO . c) Entire error log:; 19:13:25.991 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/broad/software/free/Linux/redhat_7_x86_64/pkgs/gatk_4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 14, 2021 7:13:26 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 19:13:26.217 INFO ASEReadCounter - ------------------------------------------------------------; 19:13:26.218 INFO ASEReadCounter - The Genome Analysis Toolkit (GATK) v4.1.8.1; 19:13:26.218 INFO ASEReadCounter - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:13:26.219 INFO ASEReadCounter - Executing as cbao@uger-c009.broadinstitute.org on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 19:13:26.219 INFO ASEReadCounter - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_181-b13; 19:13:26.219 INFO ASEReadCounter - Start Date/Time: June 14, 2021 7:13:25 PM ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7314:249,error,error,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7314,2,['error'],['error']
Availability,"I am using GATK 4.4.0.0 via the official docker release to reheader output from SVABA with an appropriate sequence dictionary. I am using `UpdateVCFSequenceDictionary` for this purpose with the following command: . ```; singularity exec -B ""$PWD"" broadinstitute-gatk-4.4.0.0.img gatk UpdateVCFSequenceDictionary --source-dictionary Mus_musculus.GRCm39.dna.primary_assembly.dict -V svaba.somatic.indel.vcf --replace true -O svaba.somatic.indel.vcf.reheaded.vcf; ```. I have encountered a curious behavior, where by the tool is not simply adjusting the sequence dictionary, but is also modifying a FORMAT field. . Original VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=String,Description=""Genotype quality (currently not supported. Always 0)"">; ```. Updated VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. From what I can see, the updated text is used frequently in your GATK VCF files, but I can't dig out the specific code where it is being set via `UpdateVCFSequenceDictionary`. I am wondering if there is a collision where `UpdateVCFSequenceDictionary` detects GQ and prints a stock header field to match expectation, rather than leaving it alone. I would expect the tool to simply replace the dictionary portion of the VCF without modifying the FORMAT/INFO fields. This is causing issues with downstream analysis because SVABA QC values are float/string not integer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8629:1346,down,downstream,1346,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629,1,['down'],['downstream']
Availability,"I am using GATK DepthOfCoverage tool for some of my samples.; I need to use genome reference GRCh37.; Everything is working fine but an error will occur whenever I run it (attached photos).; I have used all different type of references, including Ensembl, UCSC, NCBI, and GATK source itself but the same error is still there.; Also I know that I need to use a unique database and use that to create all fai, dict, bed file so that for sure the namings are the same in my all types of files. But I don't know how to create .bed file out of a reference genome (i.e. Homo_sapiens.GRCh37.dna.primary_assembly.fa); Would you please guide me what can I do about that?. . ![Screenshot from 2021-09-02 00-11-51](https://user-images.githubusercontent.com/87016284/131868327-660a9a9c-cc93-4c6e-a08c-0a67eddf2f47.png); ![Screenshot from 2021-09-02 00-12-00](https://user-images.githubusercontent.com/87016284/131868369-0a80d306-8a05-4a87-a566-02c3713561c4.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7453:136,error,error,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7453,2,['error'],['error']
Availability,"I am using the GATK GenotypeGVCFs command to generate vcf through gvcf, but two samples in the same batch of individuals have the following error:; ```; Using GATK jar /share/org/YZWL/yzwl_hanxt/software/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/org/YZWL/yzwl_hanxt/software/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar GenotypeGVCFs -R /share/org/YZWL/yzwl_hanxt/leizhou/ref/ARS1.2_chr30_2.fasta -V H-4.g.vcf.gz -O /share/org/YZWL/yzwl_hanxt/leizhou/variant/temp/H-4.vcf.gz; 09:55:48.764 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/org/YZWL/yzwl_hanxt/software/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:55:48.863 INFO GenotypeGVCFs - ------------------------------------------------------------; 09:55:48.865 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.6.0.0; 09:55:48.865 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:55:48.865 INFO GenotypeGVCFs - Executing as yzwl_hanxt@c01n0583 on Linux v4.18.0-513.5.1.el8_9.x86_64 amd64; 09:55:48.865 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v17.0.12+8-LTS-286; 09:55:48.866 INFO GenotypeGVCFs - Start Date/Time: September 3, 2024 at 9:55:48 AM CST; 09:55:48.866 INFO GenotypeGVCFs - ------------------------------------------------------------; 09:55:48.866 INFO GenotypeGVCFs - ------------------------------------------------------------; 09:55:48.866 INFO GenotypeGVCFs - HTSJDK Version: 4.1.1; 09:55:48.866 INFO GenotypeGVCFs - Picard Version: 3.2.0; 09:55:48.866 INFO GenotypeGVCFs - Built for Spark Version: 3.5.0; 09:55:48.866 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:55:48.867 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8969:140,error,error,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8969,1,['error'],['error']
Availability,"I am using zipped FASTQ files stored on GCP as inputs for ""Paired FASTQ to unmapped BAM"" tool in ""Sequence-Format-Conversion"" workspace and getting an error since I have to provide unzipped FASTQ files. ; Is it possible to modify this tool to allow inputting both zipped and unzipped FASTQ files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6509:151,error,error,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6509,1,['error'],['error']
Availability,"I appreciate the desire for minimal changes, but I would point out that tying the VariantContext to the source FeatureInput is likely to be a somewhat common need for MultiVariantWalkers. I realize you have a lot of existing example that dont need this capability. While I started this using VariantEval/VariantQC, I more recently tried to port CombineVariants to GATK4 and hit a similar roadblock. I have one or two other lab-specific walkers that would benefit from using the iteration pattern of MultiVariantWalkerGroupedOnStart, but also need some ability to retain the FeatureInput->VariantContext link. I believe GATK3 retained this. It would be nice to at least make this a capability available to all MultiVariantWalkerGroupedOnStart walkers. My suggestion above about making a FeatureInputAwareVariantContext wasnt necessarily meant to be introduced into every class. Would something conceptually like this pass GATK if I could introduce it more surgically, perhaps injected into MultiVariantDataSource alone?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823594870:692,avail,available,692,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823594870,1,['avail'],['available']
Availability,"I believe `sites` and `variants` should be equivalent. I see that there is a warning emitted by GATK4 at sites that are not het, but it shouldn't be an error. If you're getting an error can you please post the full message in this thread?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7747:152,error,error,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7747,2,['error'],['error']
Availability,"I believe `sites` and `variants` should be equivalent. I see that there is a warning emitted by GATK4 at sites that are not het, but it shouldn't be an error. If you're getting an error can you please post the full message in this thread? . ASEReadCounter will only process het sites in both GATK3 and GATK4, but it's possible that GATK3 silently skipped non-het sites rather than emitting a warning.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7712#issuecomment-1084552468:152,error,error,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7712#issuecomment-1084552468,2,['error'],['error']
Availability,"I believe `sites` and `variants` should be equivalent. I see that there is a warning emitted by GATK4 at sites that are not het, but it shouldn't be an error. If you're getting an error can you please post the full message in this thread? . ASEReadCounter will only process het sites in both GATK3 and GATK4, but it's possible that GATK3 silently skipped non-het sites rather than emitting a warning. _Originally posted by @meganshand in https://github.com/broadinstitute/gatk/issues/7712#issuecomment-1084552468_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7747:152,error,error,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7747,2,['error'],['error']
Availability,I believe it's fixed now. Just need to wait for it to become available.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3970#issuecomment-351834145:61,avail,available,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3970#issuecomment-351834145,1,['avail'],['available']
Availability,"I bumped into an error in a PR of mine due to a recent update in master. While I should make the code of the PR more robust I think that the approach take to compose approximate likehoods in ```VariantAnnotator.makeLikelihoods``` can and should be improved. Currently uses -Infility as ""unlikely"" lk (I would say rather ""impossible"" lk) and 0 as ""likely"" based on whether the read pileup does not match the allele or it does match the allele. . IMO the ""unlikely"" lk should never be less than the mapping quality of the read. And it can be further reduced by the base quality in case of an snp or the indel error probrability; by default is 45 Phred yet as part of the integration with Illumina/DRAGEN Dragstr, at least in germline, we can come out with indel penalties that are tailred to the reference, read context.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7312:17,error,error,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7312,3,"['error', 'robust']","['error', 'robust']"
Availability,"I calculated ASECount of genome resequence data by GATK-3.8 , but I want do the same test by GATK-4.0 , It's so strange when I use GATK-4.0 argument ""--variants"" to substitute ""sites"" of GATK-3.8 , the ERROR remaind me that the ""SNP site is not hetero"" , so l want to ask ; What is the mean of ASECountReader ""sites"" argument of GATK-3.8 ? and what is the corresponding argument in GATK-4.0 ? the answer is undocumented in instruction of ""GATK-3.8 --help"", so l want get exact answer, thank you !",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7712:202,ERROR,ERROR,202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7712,1,['ERROR'],['ERROR']
Availability,"I came across some notes I made 20170901, while developing the Helsinki Mutect2 tutorial, to report a particular bug, that I am just now getting around to. Here is the command I ended up using to solve the problem:; ```; gatk-launch PrintReads -I hcc1143_N_clean.bam -O hcc1143_N_chr17.bam -L chr17 -L chr11:915890-1133890 -L chr6:29941013-29946495 -L chr11_KI270927v1_alt -L HLA-A*24:03:01:1+; ```. Notice the protection tag `:1+` that I add to the last interval. If I do not add this, I get the following error:; ```; ***********************************************************************. A USER ERROR has occurred: Badly formed genome unclippedLoc: Contig 'HLA-A*24:03' does not match any contig in the GATK sequence dictionary derived from the reference; are you sure you are using the correct reference fasta file?. ***********************************************************************; ```; This error persists even if I provide a dictionary/ref to the command. . The solution comes from the pipelines team, in their [PESS workflow that I documented](https://gatkforums.broadinstitute.org/gatk/discussion/7899/reference-implementation-pairedendsinglesamplewf-pipeline), so credit goes to them. Perhaps our code can do this internally for HLA contigs. Sorry for the late notice. I've been extremely busy. I will assign @droazen since we touched upon this briefly last week.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3807:507,error,error,507,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3807,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I can add that my (rather empirical) tests on 3.8 vs 4.0 show a noticeable increase in runtime (currently around 4-5 hours for an Agilent OneSeq enrichment on a non-AVX capable CPU). I can explain it in part with the lack of downsampling (as mentioned in the last comments of the above GH ticket), but it's still far more than what I used to see with 3.8. Unlike the original reporter, however, I did not test the betas. Is there a standardized way to profile these results, so I can give out more concrete data?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4300#issuecomment-365974850:225,down,downsampling,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4300#issuecomment-365974850,1,['down'],['downsampling']
Availability,I can confirm that the fix works for me: I now see a user-friendly error message. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/357#issuecomment-91289762:67,error,error,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/357#issuecomment-91289762,1,['error'],['error']
Availability,"I can confirm that, GATK 4.1.3.0 with CollectRawWgsMetrics; ```; Exception in thread ""main"" java.lang.IllegalArgumentException: The requested position is not covered by this StartEdgingRecordAndOffset object.; ```. **Cmdline:**; ```; picard -Xms4000m \; CollectRawWgsMetrics \; INPUT=example.bam; VALIDATION_STRINGENCY=SILENT \; REFERENCE_SEQUENCE=Homo_sapiens_assembly38.fasta \; INCLUDE_BQ_HISTOGRAM=true \; INTERVALS=wgs_coverage_regions.hg38.interval_list \; OUTPUT=example.raw_wgs_metrics \; USE_FAST_ALGORITHM=true \; READ_LENGTH=250; ```. **Output:**; ```; 18:48:46.330 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/picard-2.20.7-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so; [Fri Sep 20 18:48:46 GMT 2019] CollectRawWgsMetrics INPUT=example.bam; [Fri Sep 20 18:48:46 GMT 2019] Executing as root@98e13b9f4ef1 on Linux 4.19.44+ amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.20.7-SNAPSHOT; [Fri Sep 20 18:49:00 GMT 2019] picard.analysis.CollectRawWgsMetrics done. Elapsed time: 0.24 minutes.; Runtime.totalMemory()=4054515712; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; Exception in thread ""main"" java.lang.IllegalArgumentException: The requested position is not covered by this StartEdgingRecordAndOffset object.; at htsjdk.samtools.util.AbstractRecordAndOffset.validateOffset(AbstractRecordAndOffset.java:146); at htsjdk.samtools.util.EdgingRecordAndOffset$StartEdgingRecordAndOffset.getBaseQuality(EdgingRecordAndOffset.java:112); at picard.analysis.FastWgsMetricsCollector.excludeByQuality(FastWgsMetricsCollector.java:189); at picard.analysis.FastWgsMetricsCollector.processRecord(FastWgsMetricsCollector.java:144); at picard.analysis.FastWgsMetricsCollector.addInfo(FastWgsMetricsCollector.java:105); at picard.analysis.WgsMetricsProcessorImpl.processFile(WgsMetricsProcessorImpl.java:93); at picard.an",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6163#issuecomment-533770047:1002,avail,available,1002,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6163#issuecomment-533770047,1,['avail'],['available']
Availability,"I can definitely appreciate that in some cases downstream analysis might be made easier if the original representations of GGA mode alleles were preserved. . Internally, HaplotypeCaller has to convert all variants at a position to share the same reference context so that read alignments can be compared to all possible alternate alleles simultaneously, and it would be a complex and error-prone process to re-map the unified alleles back to their original representations, and would also pose problems in terms of computing the correct values for INFO field annotations such as DP if the output VCF had to be split across multiple lines according to the how things were specified in the input files. I'm going to close this for now unless you strongly object or have an additional case that shows an error in GGA mode output. It's possible that in the future we could implement an enhancement in the form of a mode that preserves the GGA input allele representations, possibly under some stricter conditions upon the input, but that would likely be a tricky implementation. Pinging @ldgauthier to make sure she agrees.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5385#issuecomment-435902950:47,down,downstream,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5385#issuecomment-435902950,4,"['Ping', 'down', 'error']","['Pinging', 'downstream', 'error', 'error-prone']"
Availability,I can report that this worked well - the BQSR stage came down in time from 10 min to 4.7 min - a very significant improvement. I'm going to look at doing something similar with the reference.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-414678660:57,down,down,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-414678660,1,['down'],['down']
Availability,"I can reproduce these intermittent failures locally with Ubuntu 18.04 and Java 11. Here's a snippet of one of the hs_err_pid*.logs:. ```; Stack: [0x00007f663c024000,0x00007f663c125000], sp=0x00007f663c121ee0, free space=1015k; Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code); V [libjvm.so+0x8c4cb0]; C [libgkl_pairhmm_omp6531264563224332212.so+0x66e92] JavaData::getData(JNIEnv_*, _jobjectArray*&, _jobjectArray*&)+0x222. Java frames: (J=compiled Java code, j=interpreted, Vv=VM code); j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoodsNative([Ljava/lang/Object;[Ljava/lang/Object;[D)V+0; j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoods([Lorg/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder;[Lorg/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder;[D)V+4; j org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.computeLog10Likelihoods(Lorg/broadinstitute/hellbender/utils/genotyper/LikelihoodMatrix;Ljava/util/List;Ljava/util/Map;)V+221; j org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest.testLikelihoodsFromHaplotypes(Lorg/broadinstitute/hellbender/utils/pairhmm/PairHMM;Ljava/lang/Boolean;)V+297; v ~StubRoutines::call_stub; ```. I've experimented with changing the testing order of the implementations (e.g., from AVX, OMP, FPGA to OMP, AVX, FPGA), and I've seen failures in both AVX and OMP (i.e., libgkl_pairhmm*.so and libgkl_pairhmm_omp*.so). It seems like the .so that is complained about is switched (i.e, the AVX test segfaults on libgkl_pairhmm_omp*.so and vice versa), and that the test that runs first is always the failing one (if there is a failure). Not sure if this is a real pattern or if it will be useful in debugging, but it seems to be persistent.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772:35,failure,failures,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772,3,['failure'],"['failure', 'failures']"
Availability,I can see the reassurance of knowing that the input Locatable is constant and with a non-null contig... yet as a result we are often creating redundant simpleIntervals instances when our objects of interest are some other type of Locatable.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3541:142,redundant,redundant,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3541,1,['redundant'],['redundant']
Availability,"I can't pin down a public dataset with this many real alternate alleles, but in my unit testing with fake alternate alleles `(T, TT, ..., n*T)` I found that the merger encountered an OOM when the number of alternate alleles was 2000. I'm not sure where the exact cutoff is though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6962#issuecomment-730672154:12,down,down,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6962#issuecomment-730672154,1,['down'],['down']
Availability,"I can't reproduce this yet. I tried downloading the jar, unzipping it, and running the example command you gave, but I can't reproduce what you're seeing. I modified it for my local files:; ```; java -jar gatk-package-4.2.5.0-local.jar \; GenotypeGVCFs \; -R /Users/louisb/Workspace/gatk/src/test/resources/large/Homo_sapiens_assembly19.fasta.gz \; --variant gendb:///Users/louisb/Workspace/gatk/output \; -O out.vcf \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 \; --max-alternate-alleles 6 \; --force-output-intervals 20 \; -L 20 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations; ```; It runs to completion on my machine. ; My md5sum matches yours so that's not the problem. It's not clear to me what's going on here. Are the previous releases working on your cluster still?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042010522:36,down,downloading,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042010522,2,['down'],['downloading']
Availability,"I can't speak to the other GATK tools, but some follow up on the issue here. GenomicsDB currently only checks against upper case versions of the alleles and so returns `N` when we get a lower case `g` back from the reference. Question for @droazen/@bbimber/whoever else wants to weigh in - what should we return here? I'm not quite sure what makes the most sense in this case:; - Return the upper case `G` as CombineGVCFs does? Basically always upper case regardless of soft masking, etc within the reference?; - Return lower case `g`? Hew to what the reference said; - Decide between the two above options based on some user option?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7089#issuecomment-786782351:475,mask,masking,475,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7089#issuecomment-786782351,1,['mask'],['masking']
Availability,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8237:17,mask,mask,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237,19,"['Mask', 'mask']","['Mask', 'mask', 'mask-name', 'maskName', 'maskNames', 'masks']"
Availability,"I checked in the spark JAR, and `META-INF/services/java.nio.file.spi.FileSystemProvider` contains . ```; com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider; hdfs.jsr203.HadoopFileSystemProvider; ```. So it looks like the service loader file is being created correctly. I tried to reproduce on GCS by running (essentially the same as @vdauwera's command.). ```; time ./gatk-launch ApplyBQSRSpark \; -I gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R gs://gatk-legacy-bundles/b37/human_g1k_v37.2bit \; -O gs://gatk-demo-tom/TEST/gatk4-spark/recalibrated.bam \; -bqsr gs://gatk-demo/TEST/gatk4-spark/recalibration.table \; -apiKey $GOOGLE_APPLICATION_CREDENTIALS \; -- \; --sparkRunner GCS \; --cluster cluster-tom \; --num-executors 40 \; --executor-cores 4 \; --executor-memory 10g; ```. But I got another error earlier on. Any ideas what this could be? (I can see the input bam with `gsutil cp`). ```; org.broadinstitute.hellbender.exceptions.UserException: A USER ERROR has occurred: Failed to read bam header from gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam; Caused by:Error reading null at position 0; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:182); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:376); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:357); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:347); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.insta",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-264909676:855,error,error,855,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-264909676,1,['error'],['error']
Availability,"I cloned GATK4 into /humgen/gsa-scr1/gauthier/workspaces/gatk/ (from gsa5), then tried `./gatk-launch --list`, which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:288,error,error,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,5,"['Down', 'FAILURE', 'down', 'error']","['Download', 'FAILURE', 'download', 'error']"
Availability,"I collected coverage over 200 WGS BAMs on FC and didn't get a single failure, so hopefully this has indeed been fixed! Thanks @droazen, will close for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-466853197:69,failure,failure,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-466853197,1,['failure'],['failure']
Availability,"I confirmed gatk-4.4.0.0 HaplotypeCaller results with OpenJDK-17.0.7+07 ( Linux x64 and Linux arm version downloaded from https://adoptium.net/temurin/archive/?version=17 ) on x64 CPU ( AMD Ryzen Threadripper 1950X) and Arm CPU (ARMv8 Cortex-A53) .; A following variant was detected on Arm CPU but not detected on x64. `chr20 29521758 . A G 55.64 . AC=1;AF=0.500;AN=2;BaseQRankSum=0.311;DP=41;ExcessHet=0.0000;FS=8.502;MLEAC=1;MLEAF=0.500;MQ=56.14;MQRankSum=-4.689;QD=1.36;ReadPosRankSum=0.020;SOR=1.886 GT:AD:DP:GQ:PL 0/1:36,5:41:63:63,0,1373`. Additionally, QUAL value of some variants were different between on Arm CPU and x64 CPU. By Modifeing computeLogPenaltyScore in kBestHaplotype.java from Math.log10 to StrictMath.log10 as previous comment, same variant call results were produced on both CPUs.; https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1560470696. I placed vcf files in following URL. You can see these differences.; https://pezycomputing-my.sharepoint.com/:f:/g/personal/sakai_pezy_co_jp/Eo5Gvfau1BpMszGCcfDrD14BOfMgxvk7Mt2JCFqcDfgItQ?e=wzZbpL. On x64: PFDATCV2HG002.pz_pipeline.GRCh38.chr20-29520758-29522758.vcf; On x64 Modified to StrictMath: PFDATCV2HG002.pz_pipeline.GRCh38.chr20-29520758-29522758.StrictMath.vcf; On ARM: raspberrypi2.vcf; On ARM Modified to StrictMath: raspberrypi2.StrictMath.vcf",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1563841558:106,down,downloaded,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1563841558,1,['down'],['downloaded']
Availability,"I copied 60× BAM file to an interactive Linux server with 768 GB physical RAM and eighty cores and used version 4.5.0.0. ```; %Cpu(s): 1.3 us, 0.0 sy, 0.1 ni, 98.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st; GiB Mem : 754.5 total, 52.1 free, 107.3 used, 600.3 buff/cache ; GiB Swap: 931.3 total, 924.9 free, 6.4 used. 647.3 avail Mem . PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND ; 171365 dario 20 0 35.0g 31.3g 23040 S 100.0 4.1 32:18.12 java ; ```. I removed `-Xmx` and using `top` to see the process is consistently at about 32 GB. So, `-Xmx` is irrelevant to the problem. ```; 12:15:04.531 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; 12:57:32.208 INFO GetPileupSummaries - Shutting down engine; [January 13, 2024 at 12:57:32 PM AEDT] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 50.36 minutes.; Runtime.totalMemory()=20753416192; java.lang.OutOfMemoryError: Java heap space: failed reallocation of scalar replaced objects; ```. What does ""reallocation of scalar replaced objects"" mean? I don't think it could possibly have run out of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8654#issuecomment-1890249060:316,avail,avail,316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8654#issuecomment-1890249060,2,"['avail', 'down']","['avail', 'down']"
Availability,I deleted my previous comment because I did recreate their issue and I was not able to find a contig in the DBSNP VCF that was not present in the fasta sequence dictionary. Would you be able to add a better error message for this case @lbergelson so that we can troubleshoot and find the culprit?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7383#issuecomment-896377645:207,error,error,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7383#issuecomment-896377645,1,['error'],['error']
Availability,"I did a simple experiment and changed the version of Java used in the non-Docker (""17"", although again I'm not sure what this actually resolves to) to that used in the Docker (17.0.1+12). This causes both non-Docker and Docker tests to now fail, rather than just the Docker tests; see https://github.com/broadinstitute/gatk/pull/8174#issuecomment-1402974502. Moreover, the test failures produce exactly the same discrepant numerical results. I think we can probably conclude that the expected test results were generated with ""17"" and that changing to 17.0.1+12 generates different results. This is not too unreasonable; see the Slack thread linked in https://github.com/broadinstitute/gatk/pull/8111#issuecomment-1331407680, for example, which shows that we might be getting into pretty hairy territory and that even changes to things like how HotSpot Intrinsics are implemented in each JVM can cause the numerical differences we see here. So perhaps we can either 1) change the Docker version to the version corresponding to ""17"" or 2) change the non-Docker version to 17.0.1+12 and update the expected results?. Not sure about the failing WDL test yet, but hopefully this is enough to get us started!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8035#issuecomment-1403016955:378,failure,failures,378,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8035#issuecomment-1403016955,2,['failure'],['failures']
Availability,"I did not use the official download of the Funcotator datasources (1.2), but I did find some extraneous files that should be removed. Complete list is below:. ```; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 achilles/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cancer_gene_census/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 clinvar/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic_fusion/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic_tissue/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 dbsnp/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 dna_repair_genes/; -rwxrwx--- 1 root vboxsf 6148 Apr 30 15:38 .DS_Store*; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 familial/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode_xhgnc/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode_xrefseq/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 hgnc/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 .idea/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:38 oreganno/; -rwxrwx--- 1 root vboxsf 5274 Apr 30 15:38 README.txt*; drwxrwx--- 1 root vboxsf 0 Apr 30 15:38 simple_uniprot/; -rwxrwx--- 1 root vboxsf 1557 Apr 30 15:38 template.config*. ```; `.DS_Store` and `.idea` should not be in the official download.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4722:27,down,download,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4722,2,['down'],['download']
Availability,"I did some more work on the broadcast approach to see how feasible it would be, and found that Spark Dataflow made two unnecessary copies of the data (now fixed: https://github.com/cloudera/spark-dataflow/pull/60), which caused OOM errors when trying to broadcast the 3GB reference data. With this fixed, I ran a [pipeline called JoinReferencesDataflow](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/tools/dataflow/pipelines/JoinReferencesDataflow.java) on a small cluster that broadcasts the reference as a dataflow view. The code is a modified version of CountReadsDataflow that simply sends the view, and then doesn't use it, so we can see the cost of doing a broadcast (See the rest of the code in this branch: https://github.com/tomwhite/hellbender/tree/hadoop-references). JoinReferencesDataflow took 2 min 25s to run, of which 18s were for reading the reference from the local filesystem in the driver. For comparison, CountReadsDataflow took 17s on the same cluster. So broadcasting the reference takes less than 2 minutes. Note that this was just for one task, but Spark has [an efficient protocol for sending broadcast variables](http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf), which scales well with the number of nodes, so the approach looks feasible. Having said all that, we might still want to use the sharding approach, in order to share more code between the Google and Spark dataflow implementations. One way this could work would be to generalize `RefAPISource` and `RefAPIMetadata` to support reading reference data from a [ReferenceHadoopSource](https://github.com/tomwhite/hellbender/blob/hadoop-references/src/main/java/org/broadinstitute/hellbender/engine/dataflow/datasources/ReferenceHadoopSource.java), which is in line with @droazen's last comment. Am I right in thinking that the read pipeline work is being completed in https://github.com/broadinstitute/hellbender/tr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353:232,error,errors,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/567#issuecomment-120001353,1,['error'],['errors']
Availability,"I didn't realize before that there is no ""include"" (opt-in) validation type arg, only ""exclude"". So I'm not sure what the purpose of having ""ALL"" is in the first place, if the only thing you can usefully do with it is exclude it. I think the best longer term fix would be to add an ""--validation-type-to-include"" arg, and have it default to the everything except for IDs, and then construct the actual types based on merging include/exclude args. But thats a bigger change then just fixing the current (silent do-nothing) default behavior, and requires more error checking for conflicting args. Lets start with changing it so that in the default (no args) case, we log a warning message saying that IDs will be left out since no IDS were provided, and proceed with the remaining validations. Then if we want to get more ambitious we can talk about making the bigger change. Also, as part of the initial fix, it might be a good idea to change `calculateValidationTypesToApply` so that it doesn't modify the `excludeTypes` list directly, since this is the list provided by the user, and instead uses it's own temporary list.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5862#issuecomment-498685279:558,error,error,558,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862#issuecomment-498685279,1,['error'],['error']
Availability,"I discovered that one of the 345 input gvcfs failed VCF validation. When I removed that file and reran with no other changes, I did not get the ""terminate called without an active exception"" error. However, ImportGvcfs still fails; the failure seems to occur immediately after GenomicsDBImport logs success in importing all batches, in each shard. From all the Cromwell logs it looks like everything is working, but the top level workflow execution fails. I've been trying various configurations of memory, scatter count, and #nodes, so I don't have those log files around still. I can rerun with -DGATK_STACKTRACE_ON_USER_EXCEPTION=true and see if I get anything useful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1295310651:191,error,error,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1295310651,2,"['error', 'failure']","['error', 'failure']"
Availability,"I do have the same problem with samtools indexing, in order to use this for GATK I need it in .bai index, .csi index is not supported in GATK!; Error: ; samtools index ${filenm_root}.cutad.sort.bam; [E::hts_idx_push] Region 537233901..537233984 cannot be stored in a bai index. Try using a csi index with min_shift = 14, n_lvls >= 6",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6110#issuecomment-1047590250:144,Error,Error,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6110#issuecomment-1047590250,1,['Error'],['Error']
Availability,"I don't feel strongly about this anymore, and now the error should be searchable from this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4592#issuecomment-580901350:54,error,error,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592#issuecomment-580901350,1,['error'],['error']
Availability,"I don't have a strong opinion. I think I lean slightly toward ""site"" meaning no additional allele-specific filters _and_ the site failed, because the convention that populated filter fields (except for PASS) imply failure is ingrained. I could be persuaded otherwise, and in face of a stronger opinion wouldn't need to be persuaded.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6399#issuecomment-598549838:214,failure,failure,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6399#issuecomment-598549838,1,['failure'],['failure']
Availability,I don't know if it's the same issue but we have recently started seeing random 403 errors running dataproc jobs that appear to be internal to GCS dataproc services:. ```; ERROR: (gcloud.dataproc.jobs.submit.spark) HTTPError 403: cwhelan@broadinstitute.org does not have storage.objects.get access to dataproc-ed605f51-8ceb-44f7-b48c-a87bc588c1a6-us/google-cloud-dataproc-metainfo/dbfb2df5-060a-425e-ac31-77484354f264/jobs/0a5c53e9-f935-48f8-a39e-8a46d20b5ec9/driveroutput.000000010.; ```. For us the job keeps running on the dataproc cluster but the error crashes the client program that submitted the job.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3735#issuecomment-339034362:83,error,errors,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3735#issuecomment-339034362,3,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,"I don't remember an explicit fix, but I haven't seen seen that error in a long time and it hasn't come up in production. We can reopen if it pops up again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4518#issuecomment-590922424:63,error,error,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4518#issuecomment-590922424,1,['error'],['error']
Availability,"I don't see any reason that we need to make redundant disable be an error, since there is no harm in it. (We chose to throw for redundant **enable** because it has a small perf ramification; also, we can't just prune it out since if the redundant filter has args there will be ambiguity). @magicDGS I'd much prefer that we converge on the rules in this ticket before we make any more PRs, and then make one PR with all the changes, rather than fragmenting the fixes across multiple PRs. It makes it much easier to review, more likely that we'll get it right, and won't fragment the discussion across multiple tickets. Thx!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276987423:44,redundant,redundant,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276987423,4,"['error', 'redundant']","['error', 'redundant']"
Availability,"I don't think so. Network clients aren't normally expected to re-open files when connections are dropped, those are usually user-facing errors. Our re-opening the files is only legit because we *know* that the files aren't being written to but that's not an assumption we can make in general. Yes please file a ticket - most likely it'll be me implementing it but this way we'll remember. We have to think about whether we want a truly global setting or per-bucket, and what we want to do if libraries start changing the global setting...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-308816842:136,error,errors,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-308816842,1,['error'],['errors']
Availability,"I don't think that hiding/disable arguments would work in every case: sometimes, an argument shouldn't be exposed but still available to set programmatically, or maybe just reduce visibility making it `@Hidden` and/or `@Advance`. What is the problem of making an interface for the top-level argument to the GATK? Changing the interface or the `CommadnLineProgram` has the same effect, but the API user can still behave the same as before. It is much more extensible and downstream-friendly. What's about making the `CLPConfigurationArgumentCollection` an interface always returning defaults to be able to change it in a proper way? The cycle of development of a new argument will be: 1) add a new method to the interface with a default returning what will be expected from the previous behaviour, 2) add and return by the argument in the GATK implementation, 3) use the getter in the CLP for perform the operation. This only adds the first point, and operating in 3 classes instead of 3. For API user it is really easy to maintain the previous behavior when upgrading the dependency by just using their own implementation of the class, or include the top-level new arguments by using the GATK implementation. It is much more flexible and extensible (I always think about GATK also as a library). In addition, I think that this approach is also important for evolving GATK. For example, if a new top-level argument is tagged as experimental (still not supported but requested in Barclay), removing it would allow to keep the interface (no version bump) the same and final users can still operate with the experimental argument. The same applies to the `GATKTool` base class (https://github.com/broadinstitute/gatk/issues/4341), and for downstream projects the aim should be to be able to extend safely the `CommandLineProgram` directly to implement their own toolkit using the powerful GATK framework.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003:124,avail,available,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003,3,"['avail', 'down']","['available', 'downstream', 'downstream-friendly']"
Availability,"I double checked the scratch space so I'm not sure if that was the issue unless the server is incorrectly displaying the storage information. My `.bed` file had around 4,400 so I'm not sure if that would count as many, but I do observe a large number of files that are generated in the path I specify with `--genomicsdb-workspace-path` before `GenomicsDBImport` crashes. What I was intending to do was to keep the regions to those that are specified in exome capture kit target intervals. Interestingly, running `GenomicsDBImport` with `--merge-input-intervals` changed the overall runtime from >60 minutes to <60 seconds (and actually finishing without error compared to run with >60 minutes).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6950#issuecomment-727609620:654,error,error,654,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6950#issuecomment-727609620,1,['error'],['error']
Availability,"I doubt it but I can change the code to retry in those circumstances so we're super aggressive. That means that there we'd be retrying 40 times instead of 20, for an error that is not marked as retriable (so that normally wouldn't be retried at all).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315476254:166,error,error,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315476254,1,['error'],['error']
Availability,"I doubt it's your fault, but it could easily be mine for blithely giving M2 a GGA mode and figuring this kind of thing would just work out somehow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5513#issuecomment-447039462:18,fault,fault,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5513#issuecomment-447039462,1,['fault'],['fault']
Availability,I download 4.1.8.1 release tar.gz file but can't unzip.; ```; 63800K .......... .......... .......... .......... .......... 49.2K; 63850K 533G=21m48s. 2020-07-22 09:06:30 (48.8 KB/s) - ‘4.1.8.1.tar.gz’ saved [65382686]; ```; Here is Error:; ```; $tar -zxf 4.1.8.1.tar.gz . gzip: stdin: unexpected end of file; tar: Unexpected EOF in archive; tar: Unexpected EOF in archive; tar: Error is not recoverable: exiting now; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6719:2,down,download,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6719,4,"['Error', 'down', 'recover']","['Error', 'download', 'recoverable']"
Availability,"I downloaded and built genomicsdb_120_1 from source. I deleted my previous GenomicsDB database, and built a new one with the following options:. ```; gatk --java-options ""-Xmx128g -Xms128g"" GenomicsDBImport -R genome.fasta -V input1.gvcf.gz -V input2.gvcf.gz -V input3.gvcf.gz -V input4.gvcf.gz -V input5.gvcf.gz --genomicsdb-workspace-path my_database --tmp-dir /tmp -L chrom1 -L chrom2 -L chrom3 -L chrom4 -L chrom5 -L chrom6 -L chrom7 -L chrom8 -L chrom9 --verbosity DEBUG --max-num-intervals-to-import-in-parallel 9; ```. I then ran GenotypeGVCFs with the following options for one of the chromosomes:. ```; gatk --java-options ""-Xmx16g"" GenotypeGVCFs -R genome.fasta -V gendb://my_database -O allpools.chrom2.vcf.gz --tmp-dir=/tmp --sample-ploidy 24 -L chrom2; ```. It failed at the same region it was failing before, with this error message:. ```; 01:15:27.623 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),8476.664214527651,Cpu time(s),8391.206707930733; [January 14, 2020 1:15:30 AM BRT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 279.78 minutes.; Runtime.totalMemory()=16865820672; htsjdk.tribble.TribbleException: Invalid block size -122708061; at htsjdk.variant.bcf2.BCF2Decoder.readNextBlock(BCF2Decoder.java:66); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:134); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:181); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:49); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574113941:2,down,downloaded,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574113941,3,"['down', 'error']","['down', 'downloaded', 'error']"
Availability,"I downloaded it manually from the official Google cloud bucket. And I see the Kinar folders. And the files are getting annotated with clinvar columns, except they're all; empty.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8963#issuecomment-2312283669:2,down,downloaded,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8963#issuecomment-2312283669,1,['down'],['downloaded']
Availability,"I downloaded the GTF file for comprehensive gene annotation in CHR regions from Gencode. However, based on the errors I encountered earlier, I made some simple modifications to the GTF file in an attempt to identify the cause of the errors.; Here're the errors I encountered earlier and my codes to modify the GTF file.; ```; #error:; java.lang.IllegalArgumentException: Unexpected value: overlaps_pseudogene; #code:; grep -v '""overlaps_pseudogene""' gencode.v43.annotation.gtf >gencode.v43.annotation_nooverlaps_pseudogene.gtf; ```; ```; #error:; java.lang.IllegalArgumentException: Unexpected value: Ensembl_canonical; #code:; grep -v 'Ensembl_canonical' $newgtf_path>gencode.v43.annotation_nooverlaps_pseudogene_Ensembl_canonical.gtf; ```; ```; #error:; java.lang.NullPointerException: Cannot invoke ""org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfTranscriptFeature.getContig()"" because ""transcript"" is null; #code:; grep 'transcript' $newgtf_path>gencode.v43.annotation_transcript.gtf; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8394#issuecomment-1613939137:2,down,downloaded,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8394#issuecomment-1613939137,14,"['down', 'error']","['downloaded', 'error', 'errors']"
Availability,I downloaded the most recently gatk docker file yesterday and I met this problem today. I am wondering if you have solved this request? Thank you so much. /gatk/gatk-package-4.0.11.0-local.jar; singularity 2.5.0-dist,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433441078:2,down,downloaded,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433441078,1,['down'],['downloaded']
Availability,"I encountered a similar error while using UniProt variants BED file (available here: ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/genome_annotation_tracks/UP000005640_9606_beds/UP000005640_9606_variants.bed).; I indexed the BED file (using GATK IndexFeatureFile), but the problem not resolved.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-660625884:24,error,error,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-660625884,2,"['avail', 'error']","['available', 'error']"
Availability,"I encountered the error mentioned above when I was calling variants from whole genome sequencing data of three individuals in a pedigree, using GATK4 following the best practice. It happeded in the step of generating gvcf from the bam files after BQSR. Very interesting, it happened in two of three samples, at different position; for the other sample, gvcf was generated successfully.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582#issuecomment-991512241:18,error,error,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582#issuecomment-991512241,1,['error'],['error']
Availability,I found a pretty nasty funcotator bug as part of looking into this test failure. I think we will need to patch that as a separate pr.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-533669193:72,failure,failure,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-533669193,1,['failure'],['failure']
Availability,"I gave the two lines that caused the error. By removing these lines in VCF input, the program was able to proceed to the next. There was the next line that caused the error. Unfortunately, I did not know how many lines. Note that VCF input was produced from Mutect2. Complete VCF: https://drive.google.com/open?id=1UOUTtnNwVpBzTlJ9SPukon9aSHGOqJfW. chr1 959248 . A AAGAT . . DP=111;ECNT=4;POP_AF=0.016;P_GERMLINE=-8.838e-03;TLOD=3.19 GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:SA_MAP_AF:SA_POST_PROB 0/1:107,2:0.027:31,2:76,0:16:186,156:40:1:0.00,0.020,0.018:0.073,1.660e-03,0.925. chr1 1273803 . G GCGC . . DP=12;ECNT=1;IN_PON;POP_AF=0.016;P_GERMLINE=-1.931e-05;RPA=5,6;RU=CGC;STR;TLOD=6.39 GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:SA_MAP_AF:SA_POST_PROB 0/1:7,2:0.291:4,2:3,0:37:188,211:42:44:0.192,0.192,0.222:0.023,0.021,0.956",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-386198553:37,error,error,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-386198553,2,['error'],['error']
Availability,"I get : `500032 tests completed, 20 failed, 1 skipped`. The 2 extra failed tests are spark related (I don't have a spark setup so I assume this is normal); `ExampleAssemblyRegionWalkerSparkIntegrationTest. testExampleAssemblyRegionWalkerNonStrict`; `ExampleAssemblyRegionWalkerSparkIntegrationTest. testExampleAssemblyRegionWalkerStrict`; The skipped test is `testLikelihoodsFromHaplotypes[2](null, false)` in the `VectorPairHMMUnitTest`. I did install git-lfs and downloaded all the required files per the instructions, the files are up to date.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5511#issuecomment-446230121:465,down,downloaded,465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511#issuecomment-446230121,1,['down'],['downloaded']
Availability,"I get a consistent failure with BaseRecalibrator on a handful of samples. It occurs in both GATK 4 and GATK 3 (I checked the most current git repository of both). I also submitted this bug to the gatk forum before seeing that it affects GATK 4 and submitted this bug report.; [Forum link](https://gatkforums.broadinstitute.org/gatk/discussion/comment/44650). I've trimmed the command line down to the minimum necessary to generate the error, and I've trimmed the input files to the minimum section needed to generate the failure (a specific single read). You can find the failure below, but I also dug out the location of the failure with a proposed fix. ./gatk/src/main/java/org/broadinstitute/hellbender/utils/recalibration/covariates/ContextCovariate.java line 191 -->. ```; while (bases[currentNPenalty] != 'N') {; final int baseIndex = BaseUtils.simpleBaseToBaseIndex(bases[currentNPenalty]);; currentKey |= (baseIndex << offset);; offset -= 2;; currentNPenalty--;; }; ```. The current while loop allows the array index to become negative and walk right off the edge of the read. So a proposed fix is as follows (assuming it does not break the covariate logic) -->. ```; while (currentNPenalty > 0 && bases[currentNPenalty] != 'N') {; final int baseIndex = BaseUtils.simpleBaseToBaseIndex(bases[currentNPenalty]);; currentKey |= (baseIndex << offset);; offset -= 2;; currentNPenalty--;; }; ```. Minimal Command (test.bam attached - added txt extension just so site would let me attach it) -->. ```; gatk-launch BaseRecalibrator -I test.bam -O test.table -R GATK_Bundle_Build38/Homo_sapiens_assembly38.fasta --knownSites GATK_Bundle_Build38/dbsnp_146.hg38.vcf.gz; ```. Error message --> . ```; java.lang.ArrayIndexOutOfBoundsException: -1; 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ContextCovariate.contextWith(ContextCovariate.java:191); 	at org.broadinstitute.hellbender.utils.recalibration.covariates.ContextCovariate.recordValues(ContextCovariate.java:68); 	at org.broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4005:19,failure,failure,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4005,6,"['down', 'error', 'failure']","['down', 'error', 'failure']"
Availability,"I get tarballs from github, and then download dependencies and generate the intermediate tarball internally. We never clone git repositories as you might think due to security issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584408522:37,down,download,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584408522,1,['down'],['download']
Availability,"I get the same error when I try to run CountReads on dataproc. I get this even if using an older version of our code (all the way back to 2016-6-30, then I get a different error). Dataproc released a [new version on 2016-8-8](https://cloud.google.com/dataproc/docs/concepts/dataproc-versions), bumping up the Spark version - that's probably the cause of the error. It's possible to [request the older 1.0 version](https://cloud.google.com/dataproc/docs/concepts/versioning) and this fixes the problem for me. Of course, eventually we'll probably want to upgrade to the newer version of Spark ourselves as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2183#issuecomment-249261960:15,error,error,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183#issuecomment-249261960,3,['error'],['error']
Availability,"I got 348 samples to analyse their variants. I have read several turorials about how to use gatk to get a population vcf. At the beginning , I tried to use CombineGVCFs to get the Gvcf and use SelectVariants to pick the snps out. . CombineGVCFs truns to a error ""Exception in thread ""main"" java.lang.OutOfMemoryError"" .; then I chose to use GenomicsDBImport to do this job. It still doesn't work. First error is ""read_one_line_fully && ""Buffer did not have space to hold a line fully - increase buffer size""; I add ""--genomicsdb-vcf-buffer-size 16384000"" , it causes different error ""Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space"". This is my command and work log.; My java version is ; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12). GATK is very helpful in my research, and I really need some help to get it work. gatk --java-options ""-Xmx48g -Xms48G"" GenomicsDBImport -V C1_sentieon_gvcf.gz .......... -V SCAU-106.gvcf.gz -V SCAU-107.gvcf.gz -V SCAU-108.gvcf.gz -V SCAU-128.gvcf.gz --genomicsdb-workspace-path my_database.chr01 -R IRGSP-1.0_genome.fasta --genomicsdb-vcf-buffer-size 16384000 --intervals chr01. 11:48:08.245 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ayu/anaconda3/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.5.1; 11:48:09.327 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:48:09.327 INFO GenomicsDBImport - Executing as ayu@ayu on Linux v5.15.90.1-microsoft-standard-WSL2 amd64; 11:48:09.327 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:48:09.327 INFO GenomicsDBImport - Start Date/Time: November 26, 2023 11:48:08 AM CST; 11:48:0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8593:256,error,error,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593,3,['error'],['error']
Availability,"I got it to work by using the runtime switch --disable-sequence-dictionary-validation . . If that is not used it crashes. . . Docker commandline. . /gatk Funcotator --disable-sequence-dictionary-validation \. -R mydata/refs/Homo_sapiens_assembly19.fasta \. -V mydata/P50513_mutect2_filtered.vcf \. -O mydata/P50513_mutect2_funcotator.maf \. --output-file-format MAF \. --data-sources-path mydata/dataSourcesFolder/funcotator_dataSources.v1.6.20190124s/ --ref-version hg19. . . . From: Louis Bergelson <notifications@github.com> ; Sent: Wednesday, October 30, 2019 10:26 AM; To: broadinstitute/gatk <gatk@noreply.github.com>; Cc: rdbremel <rdbremel017@gmail.com>; Mention <mention@noreply.github.com>; Subject: Re: [broadinstitute/gatk] Funcotator shuts down (#6182). . @rdbremel <https://github.com/rdbremel> This got missed in the churn of issues. Does this happen repeatedly or is it a 1 time occurrence? We've seen similar issues in the past and tried to wrap them all in layers of retries, but sometimes things slip through. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub <https://github.com/broadinstitute/gatk/issues/6182?email_source=notifications&email_token=ANCR2VB4ZCHMAJUHBKE2SP3QRGRQFA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECUTZZI#issuecomment-547962085> , or unsubscribe <https://github.com/notifications/unsubscribe-auth/ANCR2VHRV5JESZYAYX55YHTQRGRQFANCNFSM4I2MRFQA> . <https://github.com/notifications/beacon/ANCR2VAS2WE5TDCUC6G5LETQRGRQFA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECUTZZI.gif>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548102382:753,down,down,753,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548102382,1,['down'],['down']
Availability,I got same error for gatk 4.1.8.0. Any solutions?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-688975546:11,error,error,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-688975546,1,['error'],['error']
Availability,"I got the following error and log file from BaseRecalibrator: . ```; Using GATK jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:gc_log.log -Xms4000m -jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar BaseRecalibrator -R /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-960e-17578f6b382c/call-SortAndFixSampleBam/CHMI_CHMI3_WGS2.aligned.duplicate_marked.sorted.bam --useOriginalQualities -O CHMI_CHMI3_WGS2.recal_data.csv -knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf -knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz -L chr11:1+; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.WXYB31; [July 20, 2017 2:18:26 PM UTC] BaseRecalibrator --useOriginalQualities true --knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf --knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz --output CHMI_CHMI3_WGS2.recal_data.csv --intervals chr11:1+ --input gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-960e-17578f6b382c/call-SortAndFixSampleBam/CHMI_CHMI3_WGS2.aligned.duplicate_marked.sorted.bam --reference /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --misma",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316:20,error,error,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316,1,['error'],['error']
Availability,"I got the following error when running on a cluster (Centos 6.6). The file _/lib64/libz.so.1_ is symlinked to version 1.2.3, which is too low (<1.2.3.3). When I rebuilt jbwa locally it worked fine. ```; java.lang.UnsatisfiedLinkError: /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so: /lib64/libz.so.1: version `ZLIB_1.2.3.3' not found (required by /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1937); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1822); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at org.broadinstitute.hellbender.utils.NativeUtils.loadLibraryFromClasspath(NativeUtils.java:63) ; at org.broadinstitute.hellbender.utils.bwa.BWANativeLibrary.load(BWANativeLibrary.java:14); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1916:20,error,error,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1916,1,['error'],['error']
Availability,I got the same problem during init the reference; The total memory is 991428608???. ```; org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=991428608; 18/09/15 17:21:28 ERROR yarn.ApplicationMaster: User class threw exception: java.lang.OutOfMemoryError: Java heap space; java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118); 	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93); 	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153); 	at org.broadinstitute.hellbender.relocated.com.google.common.io.ByteStreams.copy(ByteStreams.java:74); 	at org.broadinstitute.hellbender.relocated.com.google.common.io.ByteStreams.toByteArray(ByteStreams.java:115); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:40); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:42); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:500); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:468); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:459); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:207); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515#issuecomment-421544535:239,ERROR,ERROR,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515#issuecomment-421544535,1,['ERROR'],['ERROR']
Availability,"I got the same problem with GATK v 4.15.0, and for all my samples, I receive an error message like the following:. A USER ERROR has occurred: Read ST-E00318:149:HVLGNCCXX:7:1101:20791:63349 chr1:38902419-38902480 is malformed: read starts with deletion. Cigar: 3D59M91H. Although the SAM spec technically permits such reads, this is often indicative of malformed files. This is the bam file (aligned to hg38). Hope it help.; ST-E00318:149:HVLGNCCXX:7:1101:20791:63349 163 chr1 38902422 60 150M = 38902590 318 TTATTTTTTTTTTTTTGAGATAGAGTCTCGCTTTGTCACCCGGGCTGGAGTACAGTGGCGCAATCTCAGGTCACTGTAACCTCTGCTTCCCAGGTTTAAGCGATTCTCCTGCCTTGGTCTCCTGGGTAGCTTGGATTACAGGTTCACCCC AAAFFKKKKKKKKKKFF7FAK<F,F7<<,A,<FA<F7<AFA(<A,AFF,A,,7F<FFAAF7(,7AAKKK<F,7AAAF,77,FFKKA,<,,,,7<,7F,7A,A7AK,AA,A,77FK,A,,<<<FF,7,,<,<<FK,<<,,,,<A,,,,,,< MC:Z:150M MD:Z:21G19A73A1C6A18G3A2 PG:Z:bwamem RG:Z:A NM:i:7 MQ:i:60 UQ:i:89 AS:i:118; ST-E00318:149:HVLGNCCXX:7:1101:20791:63349 83 chr1 38902590 60 150M = 38902422 -318 TTTTGTATTTTTAATAGAGACGGGGTTTCACCATGTTGGTCGGGCTGGTCTCGAACTCCTGACCTGATGATCCGCCCACCTCGGCCTCCCCAAGTGCTGGGATTACAGGCATAAGCCACCACACCCAGCCTTTGTTATTTTTTTTTAAAG FF<7KKKKKFAKF<7KKF(<FKKKF<,KKFKKKKKKKKFFA<<KKK7FKKKKKFFFFKAAFKKKFFKKKFAKKFFFKKKKFFKKFAAKKKKKKFKKKKKKKKFKKKKKKKKKKKAKKKKKFFKKKKKKKKKKKKKKKKKK<KKKKFFFAA MC:Z:150M MD:Z:150 PG:Z:bwamem RG:Z:A NM:i:0 MQ:i:60 UQ:i:0 AS:i:150",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6490#issuecomment-597714610:80,error,error,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6490#issuecomment-597714610,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,I got this error running tests once. It's unclear what the problem was. It's nice that it bubbles up as a java exception but it's hard to know what the underlying issue was. ```; java.io.IOException: GenomicsDB JNI Error: std::exception; 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHeadersForQuery(GenomicsDBFeatureReader.java:200); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.<init>(GenomicsDBFeatureReader.java:85); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.getGenomicsDBFeatureReader(GenomicsDBImportIntegrationTest.java:927); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:551); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:521); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testWriteToAndQueryFromGCS(GenomicsDBImportIntegrationTest.java:1104); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:133); 	at org.testng.internal.TestInvoker.invokeMethod(TestInvoker.java:584); 	at org.testng.internal.TestInvoker.invokeTestMethod(TestInvoker.java:1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6745:11,error,error,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6745,2,"['Error', 'error']","['Error', 'error']"
Availability,"I guess that the AD counts that you are observing match the two non gray color you see in the plot. You can get the Records that represent the haplotypes grouped away if you group ""reads"" by sample. ""HC"" is the artificial sample assigned to haplotype encoding read records. I bet that one of them has the C mutation with at least one of the two mutations downstream and the other is just the reference. If that is the case then the question is why HaplotypeCaller did not reconstruct the haplotype with the downstream alternative only (no C mutation).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8238#issuecomment-1481316950:355,down,downstream,355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8238#issuecomment-1481316950,2,['down'],['downstream']
Availability,"I had a dumb permissions issue that had me spinning my wheels for a while because of some very terse logging. With the 5 reader threads we use in production I get:; `A USER ERROR has occurred: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts` ; (The error message has a placeholder for a path, but for some reason it's empty for me.). Finally I went to one thread, which called the serial FeatureReader creation method and gave me an error that could actually help solve my problem:; ```; com.google.cloud.storage.StorageException: All 20 retries failed. Waited a total of 1918000 ms between attempts; 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleRetryForStorageException(CloudStorageRetryHandler.java:108); 	at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:89); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); 	at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:243); 	at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:249); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:103); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:619); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersSerially(GenomicsDBImport.java:602); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:490); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4592:173,ERROR,ERROR,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592,5,"['ERROR', 'Error', 'Failure', 'error']","['ERROR', 'Error', 'Failure', 'error']"
Availability,"I had a look to the other branch, @droazen, and I think that it is more functional than this one:; - Check if the input already have a sequence dictionary, and only updates if `--replace` is provided. The version in this PR just overrides the dictionary.; - Check if all the variants agree with the new sequence dictionary, throwing an error if the contig is not present or the variant falls outside the chromosome range. This version does not account at all for that.; - It is a `VariantWalker`, and thus the code is simplest. But the pitfall of this is that if #2223 is implemented, that class will require a dictionary for the input as a `GATKTool`. I'm not sure how that is going to be done, but I guess that it will introduce problems in the class implemented by @cmnbroad. I think that the other version is more complete and I like it more because it is more concern about putative problems.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2232#issuecomment-257143389:336,error,error,336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2232#issuecomment-257143389,2,['error'],['error']
Availability,"I had similar problem using GATK4.1.0.0. I try to specify --resource with absolute path, yet it still give me error on path like following:. > A USER ERROR has occurred: Couldn't read file file:///dsgmnt/seq4_llfs/work/xhong/refinement/VQSR/script/hapmap,known=false,training=true,truth=true,prior; > =15:/dsgmnt/db/region/ftpGATK/resources_broad_hg38_v0_hapmap_3.3.hg38.vcf.gz. Error was: It doesn't exist.; > . The command I used is with absolute path as following:; ```; java1.8 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compres; sion_level=2 -Xmx4g -jar /dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar VariantRecalibrator \; -R /dsgmnt/llfs2/masterdata/geno/hg38/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -V /dsguser/xhong/llfs_workdir/refinement/VQSR/gatk4100v2/c1joint_c1.filtered.SiteOnly.vcf \; --resource hapmap,known=false,training=true,truth=true,prior=15:/dsgmnt/db/region/ftpGATK/resources_broad_hg38_v0_hapmap_3.3.hg38.vcf.gz \; --resource omni,known=false,training=true,truth=false,prior=12:/dsgmnt/db/region/ftpGATK/resources_broad_hg38_v0_1000G_omni2.5.hg38.vcf.gz \; --resource 1000G,known=false,training=true,truth=false,prior=10:/dsgmnt/db/region/ftpGATK/resources_broad_hg38_v0_1000G_phase1.snps.high_confidence.hg38.vcf.gz \; --resource dbsnp,known=true,training=false,truth=false,prior=2:/dsgmnt/db/region/ftpGATK/resources_broad_hg38_v0_Homo_sapiens_assembly38.dbsnp138.vcf \; -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -an DP \; -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 \; -mode SNP --max-gaussians 6 \; -O /dsguser/xhong/llfs_workdir/refinement/VQSR/gatk4100v2/c1joint_c1.snp.recal \; --output-model /dsguser/xhong/llfs_workdir/refinement/VQSR/gatk4100v2/c1joint_c1.snp.model \; --tranches-file /dsg",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-484197400:110,error,error,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-484197400,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"I had the same error on my files. I ran CollectReadCounts (without errors and next DetermineGermlineContigPloidy also without errors) for chr1 and chr6. I'm interested only in chr6, but GATKCNVCaller need 2 intervals at least and I haven't some target regions just whole chromosome 6. And when I run GATKCNVCaller in cohort mode it gave me this error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8097#issuecomment-1330372131:15,error,error,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8097#issuecomment-1330372131,4,['error'],"['error', 'errors']"
Availability,"I hate to say it: ""45 seconds but works with WGS"" is better than ""4; seconds but doesn't work with WGS"". I'm open to suggestions, though. On Fri, Jan 12, 2018 at 2:29 PM, samuelklee <notifications@github.com>; wrote:. > OK. @LeeTL1220 <https://github.com/leetl1220> How do you want to handle; > this? I'd strongly prefer to stick with data.table despite the GitHub issue; > above, since it's much faster than the usual read.table (e.g., 4 seconds; > vs. 45 seconds for your ~9.7M row WGS copy-ratio TSV that originally caused; > the error).; >; > Is there any other way we can increase /dev/shm size? @droazen; > <https://github.com/droazen> @jamesemery <https://github.com/jamesemery>; > @lbergelson <https://github.com/lbergelson> Any thoughts?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357324264>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkyJk2I2yxJOL7pV1UMN7egBCW7blks5tJ7GLgaJpZM4RclpR>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357332912:533,error,error,533,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357332912,1,['error'],['error']
Availability,"I have Java 8 installed, but it's not my _default_ Java version, so `gradle check` gives me this error message:. ```; :compileJava FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileJava'.; > invalid source release: 1.8; ```. `JAVA_HOME=$JAVA8_HOME gradle check` succeeded. . I would prefer an error message like ""Hellbender requires JAVA_HOME to point to a valid Java 8 installation"" to make it immediately obvious what needs to be done.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/489:97,error,error,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/489,3,"['FAILURE', 'error']","['FAILURE', 'error']"
Availability,"I have a few lines of code that dynamically sets the log4j level for command line tools to match the existing VERBOSITY arg, It seems to work in simple testing so I don't think we need to downgrade to do it. Let me know if you want the code, or if you haven't started you can reassign this to me.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/243#issuecomment-115810391:188,down,downgrade,188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/243#issuecomment-115810391,1,['down'],['downgrade']
Availability,I have a local test case using a modified version of @davidbenjamin 's interval list (cut down to 175000 intervals) and can see thats where the time is going. I'll run it and save the profile - it just takes forever to run.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3788#issuecomment-341741473:90,down,down,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3788#issuecomment-341741473,1,['down'],['down']
Availability,I have a new build of the GKL that I need to test and then integrate into a new gatk. It's not available yet though.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-814269292:95,avail,available,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-814269292,1,['avail'],['available']
Availability,"I have a question on why would a trio call is not considered De novo?. I visually inspected the variant (and other variants that follow the same pattern) and from the bam file the variants are well supported (see below the vcf output) with a good number of reads supporting each genotype. The vast majority of the reads were very high qual and high mapping scores (almost all near 60). . Based on the vcf output the reason for failure is LowGQ, but how would this fail that parameter when so many passed with similar outcomes (bellow is another variant that was called de novo). I found this issue in a large number of variants in our cohort (several thousands of variants some of which are variants of interest in our samples.) . . Outputs - LowGQ (failed to call De novo). ```; chrZ yy yy A G 532.25 PASS AC=1;AF=0.167;AN=6;BaseQRankSum=0.362;ClippingRankSum=-1.980e-01;DB;DP=101;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.167;MQ=59.54;MQRankSum=0.329;PG=0,0,0;QD=14.78;RAW_MQ=127600.00;ReadPosRankSum=-2.960e-01;SOR=0.542;VQSLOD=3.50 GT:AD:DP:FT:GQ:JL:JP:PL:PP 0/0:24,0:24:lowGQ:0:59:3:0,60,734:0,0,674 0/0:41,0:41:PASS:34:59:3:0,91,1289:0,34,1232 0/1:13,23:36:PASS:99:59:3:541,0,274:484,0,334. chrZ yy yy A G 342.25 PASS AC=1;AF=0.167;AN=6;BaseQRankSum=1.47;ClippingRankSum=0.825;DP=99;ExcessHet=3.0103;FS=5.073;MLEAC=1;MLEAF=0.167;MQ=60.00;MQRankSum=-8.590e-01;PG=0,0,0;QD=9.78;RAW_MQ=129600.00;ReadPosRankSum=1.26;SOR=1.623;VQSLOD=14.90;culprit=MQ . GT:AD:DP:FT:GQ:JL:JP:PL:PP . 0/0:42,0:42:PASS:51:59:3:0,108,1620:0,51,1563 0/0:21,0:21:lowGQ:0:59:3:0,60,690:0,0,630 0/1:21,14:35:PASS:99:59:3:351,0,575:294,0,635. ```. . Output - call trio (passed to call De novo). ```; chrZ yy1 yy1 C CA 46.57 PASS 90.07 PASS C=1;AF=0.167;AN=6;BaseQRankSum=-6.930e-01;ClippingRankSum=-3.900e-01;DP=120;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.167;MQ=60.00;MQRankSum=-8.800e-02;PG=0,0,0;QD=9.89;RAW_MQ=154800.00;ReadPosRankSum=-7.180e-01;SOR=0.709;VQSLOD=14.67;culprit=MQ;hiConfDeNovo=BS_RMKCB0F4 GT:AD:DP:GQ:JL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6424:427,failure,failure,427,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6424,1,['failure'],['failure']
Availability,"I have already get "".g.vcf "" through ""gatk Haplotype Caller"" but when I used the code ""./gatk GenotypeGVCFs -R /Users/lubo/sorgum/GCF_000003195.3_Sorghum_bicolor_NCBIv3_genomic.fna. ; -V /Users/lubo/sorgum/propinquum_variation.g.vcf; -O /Users/lubo/sorgum/propinquum.vcf"" to generate the output file ""propinquum.vcf"" ,A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 11733; please use the Haplotype Caller with gVCF output to generate appropriate records。 ; I don't know what's wrong with my code ,is that mean my input file have something wrong?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7147:325,ERROR,ERROR,325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7147,1,['ERROR'],['ERROR']
Availability,I have also stumbled over this. I am adding a detailed error log.; I think that the incompatibility of accelerated PairHMM with a tmp directory mounted noexec should be mentioned in ; the installation requirements. I found it well-documented in [the troubleshooting section](https://gatk.broadinstitute.org/hc/en-us/articles/18965297287067-How-to-setup-and-use-temporary-folder-for-GATK-local-execution). But everyone with this setup will experience falling back to the slow implementation for no other reason. . ```; INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/; miniconda2/envs/polyploidPhasing/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils9418239050694741169.so: /tmp/libgkl_utils9; 418239050694741169.so: failed to map segment from shared object: Operation not permitted); WARN IntelPairHmm - Intel GKL Utils not loaded; PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8453#issuecomment-1905717389:55,error,error,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8453#issuecomment-1905717389,1,['error'],['error']
Availability,"I have been able to get the connector working on GCP VMs where I have manually authenticated locally with my own account. I have not successfully gotten it working on a cromwell VM or ortherwise using manually supplied keyfiles. Anecdotal evidence, but its worth mentioning that both: `fs.gs.impl`; `fs.AbstractFileSystem.gs.impl`; seem to be optional for getting a run to work. It seems to have defaulted to the right things in the trials I've tested (though thats not to say the default will always work). I have put in a question on the issue tracker asking about available authentication inside a pipelines API VM.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500846568:567,avail,available,567,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500846568,1,['avail'],['available']
Availability,I have been trying to run the tutorial ( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ). When running without the --spark-master the turoail runs smoothly. Bu twhen I try my spark master I get an error. I downloaded SPARK 2.2.0 with hadoop 2.7.3; Java is 1.8.0_131; I set the java classpath (I think correctly); I am aware of this thread: https://github.com/broadinstitute/gatk/issues/3050. But noentheless I cannot get the error to solve. I tried to copy the jar files:; hbase-client-1.4.3.jar; hbase-common-1.4.3.jar; hbase-hadoop2-compat-1.4.3.jar; hbase-protocol-1.4.3.jar; hbase-server-1.4.3.jar; To my spark jar folder. Shall I do smething else? I am also a SPARK newbie. Thank you very much!. ***************** Here is the error log:. ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; Using GATK jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:243,error,error,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,4,"['down', 'error']","['downloaded', 'error']"
Availability,I have been unable to reproduce this error. I have tried a few different things but none of them have resulted in this null pointer exception.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5683#issuecomment-887898295:37,error,error,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5683#issuecomment-887898295,1,['error'],['error']
Availability,"I have corrected all the comments and modified code. For haplotypeCaller Tests should I add AVX_ENABLED for all tests?, apparently Travis is non AVX2 machine so it will skip the test if hardware is not available.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3701#issuecomment-337586625:202,avail,available,202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3701#issuecomment-337586625,1,['avail'],['available']
Availability,"I have developed a tool to write on maprfs using Hadoop libraries for our owns projects. But it's not really tested yet and the reader is not done. I prefer to wait it to be complete and to be sure it works perfectly. Btw, it's in scala. [Here a file](https://github.com/broadinstitute/gatk/files/1542351/HadoopWriter.scala.txt) to give you an idea, keep in mind it's not clean and complete. I tried with `maprfs:/` and `maprfs://`but still was getting an error but not the same:; >/home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input maprfs:///spark-ics/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass --sparkRunner SPARK --sparkMaster yarn. > ***********************************************************************; > ; > A USER ERROR has occurred: Failed to read bam header from maprfs:///spark-ics/user/axverdier/data/710-PE-G1.bam; > Caused by:/spark-ics/user/axverdier/data/710-PE-G1.bam; > ; > ***********************************************************************; > org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from maprfs:///spark-ics/user/axverdier/data/710-PE-G1.bam; > Caused by:/spark-ics/user/axverdier/data/710-PE-G1.bam; > 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:207); > 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:390); > 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:370); > 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:360); > 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); > 	at org.broadinstitute.hellbe",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-350232988:456,error,error,456,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-350232988,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I have found an error in the _ClippingOp_ class used by the _ReadClipper_. The offending function is _cleanHardClippedCigar_. In this function a logic error results in the returned _CigarShift_ object always having zero values for the _shiftFromStart_ and _shiftFromEnd_ members. The offending loop is shown below:; `. for (int i = 1; i <= 2; i++) {; final int shift = 0;; int totalHardClip = 0;; boolean readHasStarted = false;; boolean addedHardClips = false;. while (!cigarStack.empty()) {; final CigarElement cigarElement = cigarStack.pop();. if (!readHasStarted &&; cigarElement.getOperator() != CigarOperator.DELETION &&; cigarElement.getOperator() != CigarOperator.SKIPPED_REGION &&; cigarElement.getOperator() != CigarOperator.HARD_CLIP) {; readHasStarted = true;; } else if (!readHasStarted && cigarElement.getOperator() == CigarOperator.HARD_CLIP) {; totalHardClip += cigarElement.getLength();; } else if (!readHasStarted && cigarElement.getOperator() == CigarOperator.DELETION) {; totalHardClip += cigarElement.getLength();; } else if (!readHasStarted && cigarElement.getOperator() == CigarOperator.SKIPPED_REGION) {; totalHardClip += cigarElement.getLength();; }. if (readHasStarted) {; if (i == 1) {; if (!addedHardClips) {; if (totalHardClip > 0) {; inverseCigarStack.push(new CigarElement(totalHardClip, CigarOperator.HARD_CLIP));; }; addedHardClips = true;; }; inverseCigarStack.push(cigarElement);; } else {; if (!addedHardClips) {; if (totalHardClip > 0) {; cleanCigar.add(new CigarElement(totalHardClip, CigarOperator.HARD_CLIP));; }; addedHardClips = true;; }; cleanCigar.add(cigarElement);; }; }; }; // first pass (i=1) is from end to start of the cigar elements; if (i == 1) {; shiftFromEnd = shift;; cigarStack = inverseCigarStack;; }; // second pass (i=2) is from start to end with the end already cleaned; else {; shiftFromStart = shift;; }; }; }; `. Notice that the variable _shift_ is initialized, but never assigned to again for the duration of the loop. Thus _shiftFromSta",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6130:16,error,error,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6130,2,['error'],['error']
Availability,"I have found an error when using GATK4 Mutect2 ,error as follow,thanks and waitting your reply; Using GATK jar /home/vip/biosoft/gatk/gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx10g -jar /home/vip/biosoft/gatk/gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar Mutect2 -R /data/bigbiosoft/GATK/resources/bundle/hg19/ucsc.hg19.fasta -I 02_bamdst/zhaoxuelan.sorted.algn.bam --tumor zhaoxuelan --germline-resource /home/vip/data/Mutect2/af-only-gnomad.raw.sites.hg19.vcf.gz --max-reads-per-alignment-start 0 -L /home/vip/lxdata/bed/lung9_gene_format.bed -O 03_somatic/zhaoxuelan.sorted.algn.bam.raw.vcf --af-of-alleles-not-in-resource 0.00003125 --min-base-quality-score 20 --read-filter MateOnSameContigOrNoMappedMateReadFilter --create-output-variant-md5; 16:46:49.608 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/vip/biosoft/gatk/gatk-4.0.5.1/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:46:49.698 INFO Mutect2 - ------------------------------------------------------------; 16:46:49.698 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.0.5.1; 16:46:49.698 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:46:49.698 INFO Mutect2 - Executing as vip@zjm-System-Product-Name on Linux v4.18.0-25-generic amd64; 16:46:49.698 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10; 16:46:49.699 INFO Mutect2 - Start Date/Time: November 6, 2019 4:46:49 PM CST; 16:46:49.699 INFO Mutect2 - ------------------------------------------------------------; 16:46:49.699 INFO Mutect2 - ------------------------------------------------------------; 16:46:49.699 INFO Mutect2 - HTSJDK Version: 2.15.1; 16:46:49.699 INFO Mutect2 - Picard Version: 2.18.2; 16:46:49.699 INFO Mutect2 - HTSJDK D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6248:16,error,error,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6248,2,['error'],['error']
Availability,I have just asked the user for a bug report and will let you know when it is available.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6850#issuecomment-729922969:77,avail,available,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6850#issuecomment-729922969,1,['avail'],['available']
Availability,"I have just realized that this is indeed a regression from GATK 4.0 to GATK 4.1:; ```; wget https://github.com/broadinstitute/gatk/releases/download/4.0.12.0/gatk-4.0.12.0.zip; unzip gatk-4.0.12.0.zip; ```. Running this:; ```; gatk-4.0.12.0/gatk \; Mutect2 \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; -L chr1:233443225-233443225 \; --tumor SM; ```. Generates a VCF file with one variant:; ```; GT:AD:AF:DP:F1R2:F2R1:SAAF:SAPP; 0/1:3,15:0.799:18:3,7:0,8:0.818,0.818,0.833:0.028,0.025,0.948; ```; So that the AD counts make sense with the previous version.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6096#issuecomment-521801586:140,down,download,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6096#issuecomment-521801586,1,['down'],['download']
Availability,"I have managed to generate a minimal bam file that reproduces the issue. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/xae79hanumpireu/AABKo1l4Y-z5G5YLBqSpylRva?dl=0. Then the following code will reproduce the issue:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chr1\t97329945\t.\tT\tA\t.\t.\t.""; \; echo -e ""chr1\t97329967\t.\tC\tT\t.\t.\t."") | bgzip > input.vcf.gz && \; tabix -f input.vcf.gz. for score in 11 12; do; gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.$score.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz \; -L chr1:97329945-97329967 \; --min-base-quality-score $score && \; bcftools query \; -f ""[%CHROM\t%POS\t%REF\t%ALT\t%GT\t%AD\n]"" \; output.$score.vcf.gz \; -r chr1:97329945-97329967; done; ```. When the parameter `--min-base-quality-score 11` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	1/1	0,35; chr1	97329967	C	T	1/1	0,33; ```; When the parameter `--min-base-quality-score 12` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	0/1	9,10; chr1	97329967	C	T	0/1	6,11; ```; The first output is the output that makes sense. W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6045:99,down,download,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6045,4,"['down', 'echo']","['download', 'echo']"
Availability,"I have more examples of this now (90 and counting, ~1% of jobs) which seems to match with the above numbers:. `htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset, for input source: gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/1e300bb3-6990-4342-8959-118826efb3dd/PairedEndSingleSampleWorkflow/3b32519a-f910-49a6-a5fc-b7ec9700d281/call-GatherVCFs/S153-2.g.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:102); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:86); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.loadHeaderFromVCFUri(GenomicsDBImport.java:252); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.initializeHeaderAndSampleMappings(GenomicsDBImport.java:223); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onStartup(GenomicsDBImport.java:202); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:114); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:193,error,error,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['error'],['error']
Availability,"I have never used the GATK version CollectSequencingArtifactMetrics. So all of my errors stem from using the results of Picard's CollectSequencingArtifactMetrics, which we can expect our users to do for the foreseeable future, given the merging of the toolsets is not something in the works as far as I know. . @LeeTL1220 It seems peculiar to me that we should have to fix Picard's CollectSequencingArtifactMetrics for compatibility with this new tool FilterByOrientationBias. Rather, shouldn't you fix FilterByOrientationBias to accept either string?. Given other tools can read and write bgzipped VCFs, I would conjecture the way in which FilterByOrientationBias is utilizing dependencies is flawed, and not that dependencies require fixes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306531581:82,error,errors,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306531581,1,['error'],['errors']
Availability,"I have no problems whatsoever with the code, but I do have some concerns about the design:. In the SV group's pipeline, we distribute this multi-gig file from its home in the cloud once at cluster-creation time, and then reuse it for multiple client executions. There are no superfluous copies lying about anywhere, and no redundant copying operations. We can give it any name we wish, and put it anywhere we desire (except that the path must be the same on every worker). This code, if I'm reading it correctly, will redistribute the file from a non-permanent home on the master's local file system or on the HDFS (to which it must be copied redundantly at least once per cluster instantiation), and then it will further be redundantly copied to a temporary location on each worker's local file system with every client execution. I don't know if that's overhead that we can live with, or whether that might prevent us from writing clients with brief execution times. I'm just opening the issue for discussion. We also lose a little flexibility in that the image must live in the same directory as the reference, though I don't think that's a serious drawback -- it's a perfectly logical place for it. However, since we're just appending a fixed extension ("".img"") to the reference name we can only have one image file per reference, which may be a problem because different images need to be created for different versions of bwa and for various options such as the list of alt contigs. We can handle the first problem by insisting that all clients on a particular cluster stick to one version of bwa, which is probably a good idea, anyway, but I think we're stuck if clients need to specify various alt contig lists. It might be better to provide a default path of ""ref-name""+"".img"", but allow that default to be overridden. Also, just to twist the knife a bit, it's too bad we never reviewed my PR for gatk-bwamem-jni, which version-stamped the images for safety. It's now languished since July, a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3643#issuecomment-333598350:323,redundant,redundant,323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3643#issuecomment-333598350,3,['redundant'],"['redundant', 'redundantly']"
Availability,"I have noticed that when running spark tools (e.g. CountReadsSpark or MarkDuplicatesSpark) that running with an input in the form ""CountReadsSpark -I gs://my-bucket-dir/my-file.bam."" The tool crashes with the following unhelpful stacktraces:. ```; java.io.IOException: Error getting access token from metadata server at: http://metadata/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:208); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1072); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.SparkContext.withScope(SparkContext.scala:679); 	at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369:269,Error,Error,269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369,1,['Error'],['Error']
Availability,I have notified the user who reported this error of the fix.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316507748:43,error,error,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316507748,1,['error'],['error']
Availability,"I have problems running gatk Mutect2. . ### gatk version; - 4.1.8.0. #### command-line. `gatk Mutect2 -R /home/proj/stage/cancer/reference/GRCh37/genome/human_g1k_v37_decoy.fasta -L /home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg1`. ### Error; ```; Using GATK jar /home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar Mutect2 -R /home/proj/stage/cancer/reference/GRCh37/genome/human_g1k_v37_decoy.fasta -L /home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed -I consensus/concatenated_ACC5611A1_XXXXXX_consensusalign_ss_r2.bam -O mutect2/concatenated_ACC5611A1_XXXXXX_mutect2_unfiltered_ss_r2.vcf.gz; 09:39:55.358 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 03, 2020 9:39:55 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:39:55.559 INFO Mutect2 - ------------------------------------------------------------; 09:39:55.559 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.0; 09:39:55.559 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:39:55.559 INFO Mutect2 - Executing as ashwini.jeggari@hasta.scilifelab.se on Linux v3.10.0-1062.4.1.el7.x86_64 amd64; 09:39:55.560 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 09:39:55.560 INFO Mutect2 - Start Date/Time: July 3, 2020 9:39:55 AM CEST; 09:39:55.560 INFO Mutect2 - ------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695:277,Error,Error,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695,1,['Error'],['Error']
Availability,"I have run :; gatk MarkDuplicates -MAX_FILE_HANDLES 1000 -I ERR036185_sort.bam -O ERR036185_mark.bam -M ERR036185_mark_metrics.txt. but raise an error:OSError: [Errno 8] Exec format error: 'java'; i don‘t know why this error raise, as I can successfully run the command ""gatk"" without error,but if ""gatk MarkDuplicates"", the error raised again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7484:145,error,error,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7484,5,['error'],['error']
Availability,"I have solved my problem, I need see your code that I can know what’s wrong with it. > 2021年10月8日 上午11:13，ShuwenXia ***@***.***> 写道：; > ; > ; > Hi lubocoix, did you fix the error? I got the same issue. Please let me know how did you fix the problem. Many Thanks!!; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub <https://github.com/broadinstitute/gatk/issues/7147#issuecomment-938304267>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/ATGHJIZQKEUD7NC3KMLVOZ3UFZOUHANCNFSM4ZICDOSA>.; > Triage notifications on the go with GitHub Mobile for iOS <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. ; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7147#issuecomment-938600059:173,error,error,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7147#issuecomment-938600059,1,['error'],['error']
Availability,"I have tested several program functions by GATK4, some of them work pretty well. GATK4 does a great job to organize all steps by different tools like BWA, samtools, picard. Besides, it seems that there is also some optimization inside. Like ""cleanSam"" step, GATK4 cuts the time half compared to the one using Picard. (From 8 mins to 4 mins on same data) . However, the problem about GATK4 is that some programs fail due to java related problem (Maybe some reasons else). So far, the functions I failed are ""FastqToSam"" and ""ReadsPipelineSpark "". . Note that, I am using spark locally not scale-out cluster. So the command I am running ReadsPipelineSpark is as below(with oracle java8). Then it gives me java error. . ./bin/gatk/gatk­launch \; ReadsPipelineSpark \; -O hdfs:<PATH>CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr \; -I hdfs:<PATH>/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam \; -R hdfs:<PATH>human_g1k_v37.2bit \; --knownSites hdfs:<PATH>dbsnp_138.b37.excluding_sites_after_129.vcf \; --shardedOutput true \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES. All the failure seems to do with the java.lang.IllegalArgumentException with different error causes:; 1. readpipeline: java.lang.IllegalArgumentException: Null object is not allowed here; 2. fastqtosam: java.lang.IllegalArgumentException: Self-suppression not permitted",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1876:708,error,error,708,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1876,3,"['error', 'failure']","['error', 'failure']"
Availability,"I have tested this with a fresh `gcloud` client and have not been able to reproduce the error. I did find an article from someone else who got the `400: invalid_grant` error: https://blog.timekit.io/google-oauth-invalid-grant-nightmare-and-how-to-fix-it-9f4efaf1da35. The long and short of it is that it's an authentication issue. Can you verify that the authentication you're using on the terminal is valid? That is, can you get at other public resources on gcloud?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-725097056:88,error,error,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-725097056,2,['error'],['error']
Availability,"I have the following instruction in a handson tutorial:. > If you haven't already done so, create a symlink to the gatk-launch script. Navigate back to /gatk and test the symlink by listing the tools available.; ```; cd /usr/local/bin; ln -s /gatk/gatk-launch gatk-launch; cd /gatk; gatk-launch –-list; ```. @vdauwera says:; > wouldn't it be simpler to export to path?. My reply:; > Environmental variables persist ephemerally. I haven't tested persistence when containers are stopped and restarted. @vdauwera requests:; > hmm, could also add to path in the bash profile... we should ask the devs if it's possible to set that up in the docker itself, for next time. Could we have both an environmental variable and a symlink that invokes the launch script in the Docker from any location? Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3899:200,avail,available,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3899,1,['avail'],['available']
Availability,"I have the same error with HaplotypeCallerSpark gatk-4.2.0.0 without interval (-L) parameter:; gatk HaplotypeCallerSpark -R ucsc.hg19.fasta -I CHH43_recal.bam -O CHH43_raw.vcf.gz; But when I add -L CHH43.bed parameter the process finished correctly, without errors. Not Spark version of HaplotypeCaller ends correctly in all cases.; [HCSpark.log](https://github.com/broadinstitute/gatk/files/6638429/HCSpark.log)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199#issuecomment-859555158:16,error,error,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199#issuecomment-859555158,2,['error'],"['error', 'errors']"
Availability,I have the same error. Have you solved it yet?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8527#issuecomment-1773992861:16,error,error,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8527#issuecomment-1773992861,1,['error'],['error']
Availability,"I have the same error; let me know if it would be helpful to have my input files, logs, etc. I'm seeing it in version 4.1.4.0.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-577356900:16,error,error,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-577356900,1,['error'],['error']
Availability,"I have the same issues，but finaly I found that I made a mistake about ; `bwa -R ""@RG\tID:${x} \tPL:Illumina\tLB:${x}\tSM:${x}\tPG:bwa""` ; There is a space after the ID.; in fact ,result of gatk BaseRecalibrator Contains the space.; but when gatk ApplyBQSR read the table，; it will trim trailing spaces.; different names cause errors(I guess)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-1279736291:326,error,errors,326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-1279736291,1,['error'],['errors']
Availability,I have the same problem when I try BaseRecalibrator.; A USER ERROR has occurred: Can not read file://Users/....../file_name.vcf.tbi because no suitable codecs found,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6110#issuecomment-1158516691:61,ERROR,ERROR,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6110#issuecomment-1158516691,1,['ERROR'],['ERROR']
Availability,"I have the same problem. The error seems to occur when the number of alleles is exactly one greater than the value of ` --max-alternate-alleles`. Note that is the case in Andrius's output above: it throws a warning, but no error, when there are more than 7 alleles, but when there are exactly 7 alleles (one more than the default max of 6) it errors out. . I have tried this on a number of different chromosomes (small test cases with 4 Chinook salmon) and always get the error immediately after a warning about having 7 alleles. On one chromosome, an earlier warning about 8 alleles causes no error, UNLESS I set `--max-alternate-alleles` to 7, in which case, the position with 8 alleles causes the error immediately after the warning. This is evident from the two log files for two different runs, below. ### Log from run with value of `--max-alternate-alleles` left at default value of 6:; ```; on-chinookomes-dna-seq-gatk-variant-calling]--% gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R resources/genome.fasta -V gendb://results/genomics_db/chromosomes/CM031199.1 -O results/vcf_parts/CM031199.1.vcf.gz. Using GATK jar /home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar GenotypeGVCFs -R resources/genome.fasta -V gendb://results/genomics_db/chromosomes/CM031199.1 -O results/vcf_parts/CM031199.1.vcf.gz; 22:17:18.737 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-loc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059:29,error,error,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059,6,['error'],"['error', 'errors']"
Availability,"I have three main reasons to propose to move the arguments in CLP to an argument collection that is configurable by downstream tools/projects:. 1. Support hiding some arguments for downstream projects. For example, I do not want to support a config file by the user, but rather decide the settings for the framework and expose only some configuration.; 1. Set custom defaults for some downstream tools (including GATK). For example, a concrete tool might want to force the temp directory to be specified to avoid failures due to no space (and specify that in the documentation).; 1. Support old-style arguments (not kebab-case) for downstream projects that rely on the current argument definitions. I am specially affected by this one, because updating GATK to the 4.0.0 release of January will be a breaking change that will cause some nightmares for my users - and I don't want to do a major version bump yet (I have to re-work a bit my own framework before it). Thus, the first commit of this PR holds the proposal for the new argument collection. As I know that the team is also trying to normalize arguments and documentation, I included two more commits to help with the task (they can be removed if you think that it is better after the argument collection):; * Use `java.nio.Path` for temp directories (to support temp directories in HDFS, for example); * Change arguments moved to the collection to kebab-case (to help with #3853)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998:116,down,downstream,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998,5,"['down', 'failure']","['downstream', 'failures']"
Availability,"I have to deal with this component recently and I found the design rather awkward.... In general between GATK and htsjdk we don't seem to have a proper support for managing and querying Supplementary alignment information from read alignment records:. 1. Querying: implemented in htsjdk consists in forging artificial SAMRecords that contain only the alignment info in the SA tag element... It seems to me that it makes more sense to create class to hold this information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder already has defined a private inner class with that in mind ""SARead"" so why not flesh it out and make it public. 2. Writing: currently SATagBuilder gets attached to a read, parsing its current SA attribute content into SARead instances. It provides the possibility adding additional SAM record one by one or clearing the list. ... then it actually updates the SA attribute on the original read when a method (setTag) is explicitly called.; I don't see the need to attach the SATag Builder to a read... it could perfectly be free standing; the same builder could be re-apply to several reads for that matter and I don't see any gain in hiding the read SA tag setting process,... even if typically this builder output would go to the ""SA"" tag, perhaps at some point we would like to also write SA coordinate list somewhere else, some other tag name or perhaps an error message... why impose this single purpose limitation?; I suggest to drop the notion of a builder for a more general custom ReadAlignmentInfo (or whatever name) list. Such list could be making reference to a dictionary to validate its elements, prevent duplicates, keep the primary SA in the first position... etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3324:1395,error,error,1395,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324,1,['error'],['error']
Availability,"I have tried out also this version and as I mentioned it also results in an error. Here is the full output:. > (base) [pkus@wn45 mutect_test]$ ~/programs/gatk-4.1.8.0/gatk Funcotator --variant filtered_variants/P1.vcf.gz --reference ~/resources/hg38_for_bwa/hs38DH.fa --ref-version hg38 --data-sources-path ~/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s --output filtered_variants/P1.avcf.gz --output-file-format VCF; > Using GATK jar /home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar Funcotator --variant filtered_variants/P1.vcf.gz --reference /home/pkus/resources/hg38_for_bwa/hs38DH.fa --ref-version hg38 --data-sources-path /home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s --output filtered_variants/P1.avcf.gz --output-file-format VCF; > 12:28:16.251 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; > Jul 21, 2020 12:28:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; > INFO: Failed to detect whether we are running on Google Compute Engine.; > 12:28:16.537 INFO Funcotator - ------------------------------------------------------------; > 12:28:16.538 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.8.0; > 12:28:16.538 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; > 12:28:16.541 INFO Funcotator - Executing as xxx on Linux v3.10.0-123.20.1.el7.x86_64 amd64; > 12:28:16.541 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_251-b08; > 12:28:16.542 INFO Funcotator - Start Date/Time: July 21, 2020 12:28:16 PM CEST; > 12:28:16.5",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:76,error,error,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['error'],['error']
Availability,"I have uploaded the bam files for the region; normal bam; [https://drive.google.com/open?id=1z0HcTzSoWXfiTw_FXs0m-nh7ORSFoAFl](https://drive.google.com/open?id=1z0HcTzSoWXfiTw_FXs0m-nh7ORSFoAFl); tumour bam ; [https://drive.google.com/open?id=1sBN3-QBuE6sxhnya1bLLJG600RIth9TH](https://drive.google.com/open?id=1sBN3-QBuE6sxhnya1bLLJG600RIth9TH); mutect2 bamout tumour bam; [https://drive.google.com/open?id=1fjkteAPlSAmykHFO2DKUIjqMN8SgGCi8](https://drive.google.com/open?id=1fjkteAPlSAmykHFO2DKUIjqMN8SgGCi8). It doesn't look like there are any alignment issues. It is a very cleanly aligned region. The allele frequency at position 4,317,584 is 38% in the normal and 51% in the tumour. This is not a few sequencing errors or alignment issue. Mutect should be calling this as an SNV at position 4,317,583 C>T and nothing somatic at 4,317,584 because of the evidence in the normal at this position.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6476#issuecomment-597727920:718,error,errors,718,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6476#issuecomment-597727920,1,['error'],['errors']
Availability,"I have used Haplotype Caller to get husheep_reseq.g.vcf file and run ""./share/nas1/comput5/Tools/GATK/gatk-4.2.0.0/gatk --java-options -Xmx80G GenotypeGVCFs -R /share/nas1/comput5/ref_genome/husheep_ref/GCA_011170295.1_ASM1117029v1_genomic.fna -V husheep_reseq.g.vcf -O husheep_reseq.vcf “ to generate husheep_reseq.vcf file, but I got an error ""The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 100; please use the Haplotype Caller with gVCF output to generate appropriate records"". Seems similar in your case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7147#issuecomment-939198946:339,error,error,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7147#issuecomment-939198946,1,['error'],['error']
Availability,"I hit this error as well when I was doing a ""plumbing"" test for the new exome joint calling (I meant to merge the intervals like you suggested, but I made a mistake). That was just one sample over half of chr20. I can email you the inputs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5300#issuecomment-437364345:11,error,error,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300#issuecomment-437364345,1,['error'],['error']
Availability,"I hit this same error attempting a 156K callset. I'm rerunning with more memory and also downloading the GDB for debugging, which is 46GB. :-/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3412#issuecomment-446736619:16,error,error,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3412#issuecomment-446736619,2,"['down', 'error']","['downloading', 'error']"
Availability,"I hit this same segmentation violation issue on 4 separate branches on travis today (I believe in each case only the Java 11 unit test job failed - the rest of the matrix succeeded). It seems to be intermittent since, so far rerunning the job seems to make it go away. . ```; Finished 210000 tests; Finished 220000 tests; Finished 230000 tests; Finished 240000 tests; Finished 250000 tests; Finished 260000 tests; Finished 270000 tests; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f2bcaefd0f2, pid=10075, tid=10100; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P %E"" (or dumping to /home/travis/build/broadinstitute/gatk/core.10075); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid10075.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest > testLikelihoodsFromHaplotypesForAvailableImplementations SKIPPED; Results: SUCCESS (276386 tests, 276385 successes, 0 failures, 1 skipped). > Task :test FAILED; ```. Entire log is attached. ; [java11segv.txt](https://github.com/broadinstitute/gatk/files/4747769/java11segv.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6649:450,error,error,450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6649,3,"['error', 'failure']","['error', 'failures']"
Availability,"I just got the same error:. ```; Using GATK jar /software/anaconda2/share/gatk4-4.0.12.0-0/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /software/anaconda2/share/gatk4-4.0.12.0-0/gatk-package-4.0.12.0-local.jar FilterMutectCalls -V tumor-vs-normal.mutect.temp1.vcf -O tumor-vs-normal.mutect.temp2.vcf; 22:58:25.052 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/software/anaconda2/share/gatk4-4.0.12.0-0/gatk-package-4.0.12.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:58:26.911 INFO FilterMutectCalls - ------------------------------------------------------------; 22:58:26.912 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.0.12.0; 22:58:26.912 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:58:26.912 INFO FilterMutectCalls - Executing as www-data@SpongeBob on Linux v4.15.0-39-generic amd64; 22:58:26.912 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 22:58:26.912 INFO FilterMutectCalls - Start Date/Time: January 6, 2019 10:58:24 PM SGT; 22:58:26.913 INFO FilterMutectCalls - ------------------------------------------------------------; 22:58:26.913 INFO FilterMutectCalls - ------------------------------------------------------------; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Version: 2.18.1; 22:58:26.913 INFO FilterMutectCalls - Picard Version: 2.18.16; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:58:26.914 INFO FilterMutectCalls - De",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5553#issuecomment-451749085:20,error,error,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553#issuecomment-451749085,1,['error'],['error']
Availability,I just gotten the same issue with gatk-package-4.1.1.0-local.jar. ```; 19/04/05 14:11:45 ERROR Executor: Exception in task 75.0 in stage 5.0 (TID 5210); java.lang.IllegalArgumentException: provided start is negative: -43; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685#issuecomment-480374611:89,ERROR,ERROR,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685#issuecomment-480374611,1,['ERROR'],['ERROR']
Availability,"I just had a similar issue and debugged it using the info above. The error seems to occur when the number of ""ID"" tags and ""PU"" tags are not the same. In my case, the error was ""4,3 not equal to 12,3"" because of a malformed ""PU"" tag where there were only 4 unique ""PU"" tags but 12 unique ""ID"" tags. Presumably, the above error by akkellogg was due to 1 unique ""PU"" tag but 88 unique ""ID"" tags. I'm not sure if this is just my user error, or an actual issue with the software requiring both of those tags to uniquely match, but hopefully this will help anyone else with the issue to fix it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854#issuecomment-496577052:69,error,error,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854#issuecomment-496577052,4,['error'],['error']
Availability,I just noticed this but wanted to pass along the tidbit that with the SV tools we moved to Dataproc 1.3 (and Spark 2.3.0) because we started hitting errors like this:. `java.io.IOException: Failed to create local dir in /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535480872324_0648/blockmgr-1193ec43-c523-423b-bd88-e40ee0102cca/24`. In large jobs running on with the Dataproc 1.2 image after Google released Dataproc 1.3. I also hit the error trying to run Hail the other day and filed an issue with Google here:. https://issuetracker.google.com/issues/113360059,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5125#issuecomment-417345290:149,error,errors,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5125#issuecomment-417345290,2,['error'],"['error', 'errors']"
Availability,"I just pulled to the latest version and was surprised to see `gradlew clean` not work!; ```; $ ./gradlew clean; (...); Could not find org.broadinstitute:barclay:1.0.0-24-g87c3fa2-SNAPSHOT; ```. Reverting to 1.0.0-17-g30db73c-SNAPSHOT didn't work (same error).; Reverting to 1.0.0 made it fail somewhere else, with:; Could not resolve org.broadinstitute:gatk-bwamem-jni:1.0.0-rc1-SNAPSHOT. What's going on? Is there something wrong with my configuration?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2579:252,error,error,252,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579,1,['error'],['error']
Availability,"I just ran into this issue. I get the same error using my account and a service account. I can work around this by adding the following Java lines in runTool():. `; ctx.hadoopConfiguration().set(""fs.gs.project.id"", ""<PROJECT>"");; ctx.hadoopConfiguration().set(""google.cloud.auth.service.account.json.keyfile"", ""<KEYFILE>"");; `",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-385198863:43,error,error,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-385198863,1,['error'],['error']
Availability,"I just ran our pipeline with `--conf spark.driver.userClassPathFirst=false`. It failed near the end with an error that I think is unrelated to the parameter (looks like a regression bug in our logic introduced recently), so I'm inclined to believe changing this setting is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3946#issuecomment-357348189:108,error,error,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3946#issuecomment-357348189,1,['error'],['error']
Availability,"I just tested your branch @kvinter1 with some data at hand with the following command:; ```; ./gatk SelectVariants \; -V /Users/shlee/Downloads/gatk_bundle_1807/2-germline/input_vcfs/trio.vcf.gz \; -sn NA12878 \; --exclude-non-variants ; --remove-unused-alternates \; -O trio_excludeNVrmvUA.vcf.gz; ```. And when I grep for the spanning deletion with `gzcat trio_excludeNVrmvUA.vcf.gz | grep -v '##' | grep '*' | awk '$5=""*""' | wc -l`, I see two records pop up:; ```; 20 19013133 . C * 2084.57 . AC=1,1;AF=0.500,0.500;AN=2;BaseQRankSum=1.46;ClippingRankSum=0.00;DP=48;ExcessHet=3.9794;FS=0.000;MQ=44.84;MQRankSum=-4.256e+00;QD=18.29;ReadPosRankSum=-3.990e-01;SOR=0.672 GT:AD:DP:GQ:PL 1/2:0,0,18:48:1:732,732,732,1,0,178; 20 25939208 . A * 352.14 . AC=1,1;AF=0.500,0.500;AN=2;BaseQRankSum=1.14;ClippingRankSum=0.00;DP=12;ExcessHet=3.0103;FS=21.733;MQ=31.92;MQRankSum=-3.331e+00;QD=11.74;ReadPosRankSum=1.99;SOR=1.570 GT:AD:DP:GQ:PL 1/2:1,0,6:12:15:195,156,177,15,0,923; ```. I assume testing functionality is what you meant for review? In terms of updating the JavaDoc portion, is this something you can write a draft of, e.g. a summary of the functionality implemented. I noticed there were no changes to the doc portions. Your synopsis is something I can then review for clarity and style. P.S. I should mention otherwise your branch removed 511 such unwanted records. Nice.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5129#issuecomment-417095082:134,Down,Downloads,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5129#issuecomment-417095082,1,['Down'],['Downloads']
Availability,"I just tried Mutect2 from GATK 4.1.1.0 and got an error:; ```; A USER ERROR has occurred: standard-min-confidence-threshold-for-calling is not a recognized option; ```. From the [online documentation](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/org_broadinstitute_hellbender_tools_walkers_mutect_Mutect2.php):. > Note that the default was changed from 10.0 to 30.0 in version 4.1.0.0 to accompany the switch to use the the new quality score by default. Thus, it was still maintained in 4.1.0.0. Based on that, I am surprised that it was removed. There are actually a few other parameters that got dropped. For example, `normal-artifact-lod` from `FilterMutectCalls`. Are these changes explained somewhere?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5845:50,error,error,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5845,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I just try to run PrintReadsSpark on cloudera cluster and meet this error. Command:; ```; $ ./gatk-launch PrintReadsSpark -I NA12878.chr17_69k_70k.dictFix.bam -O /user/yaron/output.bam -- --sparkRunner SPARK --sparkMaster yarn --num-executors 5 --executor-cores 2 --executor-memory 1g; ```; Results:; ```; Using GATK jar /home/yaron/gatk/build/libs/gatk-spark.jar; Running:; spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --num-executors 5 --executor-cores 2 --executor-memory 1g /home/yaron/gatk/build/libs/gatk-spark.jar PrintReadsSpark -I NA12878.chr17_69k_70k.dictFix.bam -O /user/yaron/output.bam --sparkMaster yarn; 09:14:13.551 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yaron/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [June 8, 2017 9:14:13 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /user/yaron/output.bam --input NA12878.chr17_69k_70k.dictFix.bam --sparkMaster yarn --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:68,error,error,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['error'],['error']
Availability,"I kicked off the runs now localizing the dbsnp file. Of the 10 runs that had failed before, 5 of them had the dbsnp error and the other 5 had that expected non-negative error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317743568:116,error,error,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317743568,2,['error'],['error']
Availability,"I know the usage in the `Pileup`, because I added it when I ported it: I've just tried to mimic the samtools mpileup default filter flags. In samtools, the help of mpileup said regarding the filters: . ```; --ff, --excl-flags STR|INT filter flags: skip reads with mask bits set; [UNMAP,SECONDARY,QCFAIL,DUP]; ```. Thus I guess that in the `Pileup` the usage is correct.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2165#issuecomment-247377434:264,mask,mask,264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2165#issuecomment-247377434,1,['mask'],['mask']
Availability,"I like option 2. If one of the other options is specified, use the specified value. For example, the following would use 0.1 for interval-psi-scale:. --set-defaults-for-data-type WES. --interval-psi-scale 0.1. On Mon, Apr 30, 2018 at 10:27 PM, samuelklee <notifications@github.com>; wrote:. > Thanks for bringing this up! I actually think that I prefer option 1,; > although not ideal (since, as you say, it places more burden on the user).; > The whole point of having generically parameterized models is that we can; > apply them to many data types. To single out a few with hardcoded sets of; > defaults seems like a slippery slope to me. (Of course, we should; > definitely provide defaults for typical data types in *documentation*.); > And in the end, I think it is beneficial for users that wish to tweak knobs; > to do some work to understand what those knobs actually do (even if just at; > a basic level).; >; > The other downside of option 2 is that it might not be immediately obvious; > from the command line what parameters are being used. For example, if a; > user chooses a set of defaults but then overrides some of them, we should; > make it so they don't have to go digging through the logs to see what; > parameters are actually used in the end. Nor should they have to go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:932,down,downside,932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379,1,['down'],['downside']
Availability,I loaded the docker repo GATK v4.1.4.0 and had the same (or similar) error result. ```; 2019-10-30T13:35:51.791637449Z java.lang.IllegalArgumentException: log10 p: Values must be non-infinite and non-NAN; 2019-10-30T13:35:51.792001654Z 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); 2019-10-30T13:35:51.792175325Z 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); 2019-10-30T13:35:51.792358868Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.clusterProbabilities(SomaticClusteringModel.java:203); 2019-10-30T13:35:51.792559803Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:96); 2019-10-30T13:35:51.792736667Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.TumorEvidenceFilter.calculateErrorProbability(TumorEvidenceFilter.java:27); 2019-10-30T13:35:51.792905235Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 2019-10-30T13:35:51.793072365Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 2019-10-30T13:35:51.793261944Z 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 2019-10-30T13:35:51.793456807Z 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 2019-10-30T13:35:51.793619935Z 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 2019-10-30T13:35:51.793810301Z 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 2019-10-30T13:35:51.794006885Z 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 2019-10-30T13:35:51.794191116Z 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 2019-10-30T13:35:51.794367593Z 	at java.util.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-547909227:69,error,error,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-547909227,1,['error'],['error']
Availability,"I love the improved economy of expression, but I'm not a fan of this kind of thing myself. In addition to the reasons @lbergelson mentioned, its hard to implement these without introducing subtle shape/type-variance issues, i.e., this silently turns a Set into a List, which can manifest downstream in surprising ways. It would be nice though. If this were Scala, we wouldn't have to do this at all...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2369#issuecomment-275775230:288,down,downstream,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2369#issuecomment-275775230,1,['down'],['downstream']
Availability,"I may have a lead. The error occurs here (no insight so far, just looking up the line from the stack trace):; ```; final Object2IntMap<EVIDENCE> evidenceIndexes = evidenceIndexBySampleIndex(sampleIndex);; final int[] indexesToRemove = evidences.stream().mapToInt(e -> {; final int index = evidenceIndexes.getInt(e);; if (index == MISSING_INDEX) {; throw new IllegalArgumentException(""evidence provided is not in sample"");; }; ```; We get an error when `evidenceIndexBySampleIndex(sampleIndex)` yields a `Map` that for some reason doesn't contain a read that it should. So let's investigate `evidenceIndexBySampleIndex()`. This method returns the `evidenceIndexBySampleIndex.get(sampleIndex)` field if it is not `null` (ie uninitialized); otherwise it fills it and then returns it. The code for filling it seems fine, and it explicitly loops over every sample read, so it's hard to see that the error could come from there. It seems rather that the problem is in returning the cached value whenever it is not `null`. The cached value of `evidenceIndexBySampleIndex.get(sampleIndex)` becomes invalid whenever reads are added or removed. However, you can check all the accesses of `evidenceIndexBySampleIndex` (there are only six) and verify that the class never accounts for this. So, suppose that an `AlleleLikelihoods` object invokes `evidenceIndexBySampleIndex(sampleIndex)` more than once and adds or removes reads between these. The second call returns the cached map from the first call, which is bogus. Even if it doesn't explain this issue, it is a bug. Now let's think about which public methods `evidenceIndexBySampleIndex(sampleIndex)` is called in and where this occurs in HaplotypeCaller:. * `addEvidence` (in HC this happens only in the likelihoods for annotations, downstream of our issue, so this is not the culprit).; * `filterPoorlyModeledEvidence` (this happens after Pair-HMM to the haplotype likelihoods, so not the culprit either); * `contaminationDownsampling`; * `retainEvidence`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336:23,error,error,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336,3,['error'],['error']
Availability,"I meet a same error, but the result is still exist.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-902566891:14,error,error,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-902566891,1,['error'],['error']
Availability,I met the same error when trying to create vcf files for PoN.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6529#issuecomment-634382427:15,error,error,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6529#issuecomment-634382427,1,['error'],['error']
Availability,"I met this exception when I was trying to run spark commands on a standalone Spark server. I searched for possible causes, and I found two results. One is the spark lacks some jar files to handle the file system. The other says the version of spark on the server is not the same as the one codes are compiled with. ; So for the first one, I tried downloading jars from Zookeeper, Hive and Hbase, and implemented them as said in ""https://stackoverflow.com/questions/34901331/spark-hbase-error-java-lang-illegalstateexception-unread-block-data"", but it doesn't really change anything. ; And for the other one, I tried spark-2.0.0-hadoop-2.6, spark-2.0.0-hadoop-2.7, spark-2.1.1-hadoop-2.7 and spark-1.6.1-hadoop-2.6. But none of them changed the error message. **So I want to ask what version of Spark should I use actually?**. And I will put the error message here:; Using GATK jar /curr/tianj/gatk/build/libs/gatk-spark.jar; Running:; ```; spark-submit --master spark://ip-xxx-xx-xx-xxx:xxxx --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true /curr/tianj/gatk/build/libs/gatk-spark.jar MarkDuplicatesSpark -I /curr/tianj/data/sortedbam/xx_sort.bam -M xx.m -O xx_markduplicatespark.bam --TMP_DIR tmp --sparkMaster spark://ip-xxx-xx-xx-xxx:xxxx; 00:48:13.577 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:347,down,downloading,347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,4,"['down', 'error']","['downloading', 'error', 'error-java-lang-illegalstateexception-unread-block-data']"
Availability,"I noticed, while looking at https://github.com/broadinstitute/gatk/issues/2713, that the main `GenomicsDBImporter` constructor calls `GenomicsDBImporter.generateSortedCallSetMap()` to download all VCF headers to get the sample names, but it already has the sample names passed in to it (as `variants.keySet()`). This sort of defeats the purpose of the `--sampleNameMap` argument in the `GenomicsDBImport` tool...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2714:184,down,download,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2714,1,['down'],['download']
Availability,"I obtain this reproducible issue with gatk 4.1.2.0:. Using the following code:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walker",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6037:140,down,download,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037,6,"['down', 'echo']","['download', 'echo']"
Availability,"I obtain this reproducible issue with gatk 4.1.3.0:. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/78rz5wrhu9zkfzh/AACW9ZPhl4WnD-wmAkKcdHT3a?dl=0. Then setup a GATK working environment:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.3.0/gatk-4.1.3.0.zip; unzip gatk-4.1.3.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405\; .15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict; ```. Now if I run Mutect2:; ```; gatk-4.1.3.0/gatk \; Mutect2 \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; -L chr1:233443225-233443225; ```. This will generate a VCF file with one variant:; ```; GT:AD:AF:DP:F1R2:F2R1:SB; 0/1:6,21:0.778:27:4,8:0,11:2,4,12,9; ```; With an allelic depth of six supporting the reference. However, there are only four fragments supporting the reference. If I remove those for fragments from the BAM file:; ```; samtools view -h input.bam | \; grep -v "":6112\|:10233\|:18618\|:20229"" | \; samtools view -Sb -o input2.bam && \; samtools index input2.bam; ```. And I run Mutect2 again:; ```; gatk-4.1.3.0/gatk \; Mutect2 \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input2.bam \; -O output.vcf.gz \; -L chr1:233443225-233443225; ```. It will generate a VCF with the same variant:; ```; GT:AD:AF:DP:F1R2:F2R1:SB; 0/1:0,20:0.954:20:0,7:0,11:0,0,11,9; ```; With an allelic depth of zero supporting the reference. The same problem exists with the HaplotypeCaller. I believe this was not the intended ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6096:79,down,download,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6096,3,['down'],['download']
Availability,I opened #6511 for SplitNCigarReads and https://github.com/broadinstitute/picard/issues/1488 for MarkDuplicates. They should both be easy to address once we decide on warning or error.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6501#issuecomment-601410454:178,error,error,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6501#issuecomment-601410454,1,['error'],['error']
Availability,"I prefer to keep out of this user exceptions the arguments from the wrapper script due to downstream projects including tools from GATK (and using the `UserException` classes). Thus, I vote for refere to the `--TMP_DIR`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4709#issuecomment-384568889:90,down,downstream,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4709#issuecomment-384568889,1,['down'],['downstream']
Availability,"I prepared a clean Bam file following GATK Best Practice and used GATK4 HaplotypeCaller to create a gvcf with ploidy1 option:. '''; gatk-4.0.2.1/gatk HaplotypeCaller --native-pair-hmm-threads 24 -I KU_filtered_sorted_mdup.bam -O HC.KU.raw.snps.indels.g.vcf -R ref.fasta -ploidy 1 --emit-ref-confidence GVCF; '''. When I validated the gvcf, ValidateVariants threw errors at the end:. '''; <br />11:27:55.681 INFO ProgressMeter - Traversal complete. Processed 124689522 total variants in 3.8 minutes.; 11:27:55.681 INFO ValidateVariants - Shutting down engine; [April 10, 2018 11:27:55 AM JST] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 3.82 minutes.; Runtime.totalMemory()=4682940416; java.lang.IllegalArgumentException: Illegal character in path at index 15:HC.KU.raw.snps.indels.g.vcf; at java.net.URI.create(URI.java:852); at org.broadinstitute.hellbender.engine.FeatureInput.makeIntoAbsolutePath(FeatureInput.java:242); at org.broadinstitute.hellbender.engine.FeatureInput.toString(FeatureInput.java:314); at java.util.Formatter$FormatSpecifier.printString(Formatter.java:2886); at java.util.Formatter$FormatSpecifier.print(Formatter.java:2763); at java.util.Formatter.format(Formatter.java:2520); at java.util.Formatter.format(Formatter.java:2455); at java.lang.String.format(String.java:2940); at org.broadinstitute.hellbender.engine.FeatureDataSource.close(FeatureDataSource.java:589); at org.broadinstitute.hellbender.engine.FeatureManager.lambda$close$9(FeatureManager.java:505); at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608); at org.broadinstitute.hellbender.engine.FeatureManager.close(FeatureManager.java:505); at org.broadinstitute.hellbender.engine.GATKTool.onShutdown(GATKTool.java:857); at org.broadinstitute.hellbender.engine.VariantWalker.onShutdown(VariantWalker.java:95); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4657:363,error,errors,363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4657,2,"['down', 'error']","['down', 'errors']"
Availability,I push the wrong local clone by mistake... this should fix the test errors.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3676#issuecomment-334862846:68,error,errors,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3676#issuecomment-334862846,1,['error'],['errors']
Availability,I put this into a different branch because I upgraded GDB to fix the weird error. I don't want this feature to go into the 4.0.9.0 release so I'll do a PR of the new branch after.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4969#issuecomment-422509124:75,error,error,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4969#issuecomment-422509124,1,['error'],['error']
Availability,"I ran 408 invocations of an nio using command for BQSR using gatk4 and got 2 failures that looked pretty similar. Is there something I might be doing wrong? The two failures were also on different shards. . I cant remember exactly when I built this jar but it was after this commit - https://github.com/broadinstitute/gatk/commit/4df1d16518cbd3a05a45a070d682446878ec4eaa less than a week ago. If you need any more info let me know, thanks. ```; Using GATK jar /usr/gitc/gatk4/gatk-package-4.beta.3-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -XX:+PrintFlagsFinal -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:gc_log.log -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -Xms3000m -jar /usr/gitc/gatk4/gatk-package-4.beta.3-local.jar ApplyBQSR --createOutputBamMD5 --addOutputSAMProgramRecord -R /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/4a87f12f-014e-438a-9a10-260c70bf3584/call-SortSampleBam/attempt-4/NA12878.aligned.duplicate_marked.sorted.bam --useOriginalQualities -O NA12878.aligned.duplicates_marked.recalibrated.bam -bqsr /cromwell_root/broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/4a87f12f-014e-438a-9a10-260c70bf3584/call-GatherBqsrReports/NA12878.recal_data.csv -SQQ 10 -SQQ 20 -SQQ 30 -L chr5:1+; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.Ni4zSL; [August 22, 2017 2:52:59 PM UTC] ApplyBQSR --output NA12878.aligned.duplicates_marked.recalibrated.bam --bqsr_recal_file /cromwell_root/broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/4a87f12f-014e-438a-9a10-260c70bf3584/call-GatherBqsrReports/NA12878.recal_data.csv --useOriginalQualities true --static_quantized_quals 10 --static_quantized_quals 20 --static_quantized_quals 30 --intervals chr5:1+ --input gs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3481:77,failure,failures,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3481,2,['failure'],['failures']
Availability,"I ran CalibrateDragstrModel on one of the NYGC 1000G crams (which should be Functionally Equivalent with ours) and got the error: `A reference must be supplied that includes the reference sequence for chr12` I did pass a reference to the tool, but couldn't get it to run until I set the samjdk.reference_fasta black magic (at @droazen 's suggestion) in the java invocation. Huge stack trace:; java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1005); 	at org.broadinstitute.hellbender.utils.Utils.runInParallel(Utils.java:1479); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsParallel(CalibrateDragstrModel.java:473); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:152); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1057); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caus",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7060:123,error,error,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060,1,['error'],['error']
Availability,"I ran GATK 4.1.0.0 Mutect2 on a small (~1Mb) targeted panel. I am using a normal control that is not the same individual (basically to exclude technical artifacts), so I do expect to see more variants than with a proper matched normal. I was getting around 100-300 variants per sample with GATK 4.0.6.0. I am still roughly in the same range for some samples GATK 4.1.0.0, but I am getting 0 for some. The problem seems to be at the FilterMutectCalls stage where I am seeing the following error:; ```; [March 19, 2019 10:43:17 PM EDT] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8851030016; java.lang.IllegalArgumentException: errorRate must be good probability but got NaN; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:227); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:211); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyContaminationFilter(Mutect2FilteringEngine.java:79); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:518); at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$For",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5821:488,error,error,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821,4,['error'],"['error', 'errorProbToQual', 'errorRate']"
Availability,"I ran GATK4 HC on /seq/picard_aggregation/C1827/MITO64/current/MITO64.bam, which has >=10,000X coverage over the mitochondrial genes. With Xmx4g Xms4g I had no problems. One called variant had DP=3719, which must be the downsampled coverage? IGV reports about 7200 coverage:; ![image](https://user-images.githubusercontent.com/6578548/39382302-0a284436-4a33-11e8-9628-47dc01dab44e.png); That same bam also ran fine with 2g and even 1g. I tried ploidy 20 ERC GVCF at 1G and that was slow, but also successful. Ploidy 100 revealed that there are some sites with two alts because then one gets dropped. That was also super slow, but successful at 1G. I wouldn't recommend 1G for general purpose, though. There are some Jira tickets complaining about running out of memory with GATK4 on Firehose with 1G (https://broadinstitute.atlassian.net/browse/PO-12076?jql=project%20in%20(PO%2C%20DSDEGP)%20AND%20text%20~%20memory). Mark's contaminated bam above is running now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4272#issuecomment-385084409:220,down,downsampled,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4272#issuecomment-385084409,1,['down'],['downsampled']
Availability,"I ran IndexFeatureFile on a VCF with a valid header but no variant features. IndexFeatureFile crashes due to something regarding the progress meter. This might be a good place to output one of your helpful `USER ERROR` messages. . Thanks!. ```; acesnik@DESKTOP$ gatk/gatk IndexFeatureFile --feature-file bad.vcf; Using GATK jar gatk/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar gatk/gatk-package-4.0.0.0-local.jar IndexFeatureFile --feature-file bad.vcf; 00:17:06.701 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:gatk/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 00:17:06.843 INFO IndexFeatureFile - ------------------------------------------------------------; 00:17:06.843 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.0.0.0; 00:17:06.844 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:17:06.845 INFO IndexFeatureFile - Executing as acesnik@DESKTOP-NTA5PMC on Linux v4.4.0-43-Microsoft amd64; 00:17:06.845 INFO IndexFeatureFile - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 00:17:06.846 INFO IndexFeatureFile - Start Date/Time: January 26, 2018 12:17:06 AM GMT; 00:17:06.846 INFO IndexFeatureFile - ------------------------------------------------------------; 00:17:06.846 INFO IndexFeatureFile - ------------------------------------------------------------; 00:17:06.847 INFO IndexFeatureFile - HTSJDK Version: 2.13.2; 00:17:06.847 INFO IndexFeatureFile - Picard Version: 2.17.2; 00:17:06.848 INFO IndexFeatureFile - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:17:06.849 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:17:06.849 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 00:17:06.850 INFO IndexFeatureFile - HTSJ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4269:212,ERROR,ERROR,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4269,1,['ERROR'],['ERROR']
Availability,"I ran the following command:. ```; gatk --java-options ""-Xmx96g"" SelectVariants -R genome.fasta -V gendb://test_database -O hctest.combinedvariants.chrom2.g.vcf.gz; ```. `SelectVariants` also failed in the same problem region. It started running, got stuck for about 50 minutes and failed:. ```; 14:07:45.004 INFO ProgressMeter - chrom2:4323705 0.3 2000 6209.6; 14:08:00.372 INFO ProgressMeter - chrom2:4327258 0.6 4000 6916.8; 14:08:16.027 INFO ProgressMeter - chrom2:4333144 0.8 7000 8341.1; 14:56:17.993 INFO SelectVariants - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),2722.5394666619995,Cpu time(s),2710.540034284; [January 16, 2020 2:56:22 PM BRT] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 48.97 minutes.; Runtime.totalMemory()=39383990272; Exception in thread ""main"" java.lang.OutOfMemoryError; at java.lang.AbstractStringBuilder.hugeCapacity(AbstractStringBuilder.java:161); at java.lang.AbstractStringBuilder.newCapacity(AbstractStringBuilder.java:155); at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:125); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:596); at java.lang.StringBuilder.append(StringBuilder.java:190); at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:340); at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:356); at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLineReader.java:51); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); at htsjdk.samtools.util.AbstractIterator.next(AbstractIterator.java:57); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:70); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:37); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatur",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-575277936:538,down,down,538,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-575277936,1,['down'],['down']
Availability,"I ran the full test suite using a [branch](https://github.com/broadinstitute/gatk/tree/cn_check_cache_thrash) that throws if a tool ever tries to query the FeatureCache using a query interval that is earlier than, but on the same contig as, the one currently cached. Several tests failed, including a few of the Mutect2/HC ones:. Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly ; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode. The FeatureCache assumes that queries are always increasing along a contig; the failures in this branch indicate that the caller is attempting to back up and re-query territory that has already been cached and then trimmed. I didn't track down all of these cases, but the general pattern appears to be that active region determination results in initial caching and trimming, and then the same/similar territory is traversed again during calling, resulting in cache misses. It happens pretty frequently when running M2 tests, at least for pon and germline resource inputs; we should investigate how much a better caching strategy would help performance. If it would, we'd need https://github.com/broadinstitute/gatk/pull/4902 at a mimimum in order to use a alternate cache strategy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5148:680,failure,failures,680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5148,2,"['down', 'failure']","['down', 'failures']"
Availability,"I read the [CNNScoreVariants documentation](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_walkers_vqsr_CNNScoreVariants.php#--info-annotation-keys) and ran the ```--help``` option, and both state the following. ```; --info-annotation-keys,-info-annotation-keys:String; 		The VCF info fields to send to python. This argument may be specified 0 or more times.; 		Default value: [MQ, DP, SOR, FS, QD, MQRankSum, ReadPosRankSum]. ; ``` . I successfully executed the CNNScoreVariants command with the default value of the ```--info-annotation-keys``` argument in the following way. ```; --info-annotation-keys '[MQ, DP, SOR, FS, QD, MQRankSum, ReadPosRankSum]' ; ```. However, when I try to change the number of fields, for example like. ```; --info-annotation-keys '[MQ, DP, SOR, FS, QD, MQRankSum]' ; ```. or anything more or less than seven fields I get an error like the following one. ```; Traceback (most recent call last):; 		File ""<stdin>"", line 1, in <module>; 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 127, in score_and_write_batch; 		batch_size=python_batch_size); 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/training.py"", line 1152, in predict; 		x, _, _ = self._standardize_user_data(x); 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/training.py"", line 754, in _standardize_user_data; 		exception_prefix='input'); 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/training_utils.py"", line 136, in standardize_input_data; 		str(data_shape)); 	ValueError: Error when checking input: expected annotations to have shape (7,) but got array with shape (6,); ```. According to the documentation, I should be able to use the argument with an arbitrary number of fields. Is this a bug, or am I using the ```--info-annotation-keys``` argument incorrectly?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5939:917,error,error,917,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5939,2,"['Error', 'error']","['Error', 'error']"
Availability,I rebuilt the new branch and got the same `OutOfMemory` error with `genomicsdb_120_6275_fix`. . Should I still see v4.1.4.1-7-gdb054ab-SNAPSHOT for this fix?. Please let me know what data I should provide for you to reproduce this on your side. Thanks a lot for your help.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-575362636:56,error,error,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-575362636,1,['error'],['error']
Availability,"I recently noticed a series of what were evidently memory failures when running HaplotypeCaller on some standard test WGS data when using the exact task used in the warp pipeline here: https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/variant_calling/VariantCalling.wdl. I found that running that wdl with otherwise default inputs except for `haplotype_scatter_count` being set to 10 (so each node doing approximately 5x as much work as when the default, 50, is set) I would get repeated HaplotypeCaller job failures after a few hours that had the pattern of memory failures. The errors tend to involve HaplotypeCaller abruptly ending without any sort of error message or exception at all (which could indicate the vm is dying):; ```; 03:22:15.993 INFO ProgressMeter - chr13:18173014 378.6 1419490 3749.0; 03:22:26.338 INFO ProgressMeter - chr13:18177988 378.8 1419530 3747.4; 03:22:36.801 INFO ProgressMeter - chr13:18203610 379.0 1419700 3746.1; (END); ```; Or alternatively it seems to end without the end-of-run messages being output:; ```; 23:05:30.662 INFO ProgressMeter - chr2:47207099 428.8 1372310 3200.4; 23:05:40.859 INFO ProgressMeter - chr2:47323745 429.0 1372960 3200.7; 23:05:50.896 INFO ProgressMeter - chr2:47476709 429.1 1373720 3201.2; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6933m -Xms6933m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller [INPUTS]; 2022/02/10 23:06:52 Starting delocalization.; 2022/02/10 23:06:53 Delocalization script execution started...; ```. These failures appear to be reproducible and happen at about the same point in every run. The fact that increasing the memory or decreasing the interval per shard seems to remove the issue it makes me suspect there might be an issue where Hapl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7693:58,failure,failures,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7693,5,"['error', 'failure']","['error', 'errors', 'failures']"
Availability,I recommend we error out when provided with `--validation-type-to-exclude ALL`. It doesn't make sense - why call the validator if you're not going to validate?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5862#issuecomment-485714312:15,error,error,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862#issuecomment-485714312,1,['error'],['error']
Availability,"I reproduced various out of memory errors in a Linux VM with 4G of RAM, both with the `IntelInflaterDeflaterIntegrationTest` enabled and disabled. Most resulted in the kernel killing the Java process, like this one (from `dmesg`):; ```; [38425.759992] Out of memory: Kill process 10295 (java) score 747 or sacrifice child; [38425.759998] Killed process 10295 (java) total-vm:7885212kB, anon-rss:3250892kB, file-rss:0kB; ```. Some were caught by the JVM, like this one:; ```; #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 90177536 bytes for committing reserved memory.; # Possible reasons:; # The system is out of physical RAM or swap space; # In 32 bit mode, the process size limit was hit; # Possible solutions:; # Reduce memory load on the system; # Increase physical memory or swap space; # Check if swap backing store is full; # Use 64 bit Java on a 64 bit OS; # Decrease Java heap size (-Xmx/-Xms); # Decrease number of Java threads; # Decrease Java thread stack sizes (-Xss); # Set larger code cache with -XX:ReservedCodeCacheSize=; # This output file may be truncated or incomplete.; #; # Out of Memory Error (os_linux.cpp:2627), pid=20484, tid=139679452493568; #; # JRE version: Java(TM) SE Runtime Environment (8.0_72-b15) (build 1.8.0_72-b15); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.72-b15 mixed mode linux-amd64 compressed oops); # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; ```. Here's my theory of what's happening. The `maxHeapSize` for test JVMs is set to 4G in `build.gradle`:; ```; maxHeapSize = ""4G""; ```. A 4G max heap size is too high for systems with 4G of RAM, because the Java heap grows until the system runs out of memory. If we decrease `maxHeapSize`, the GC should prevent the Java heap from growing too large, with the trade-off of more GC calls. I changed the `maxHeapSize` to `2G` a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-288423316:35,error,errors,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-288423316,1,['error'],['errors']
Availability,"I reran option 2 with latest gatk4/master. I ran the BaseRecalibrator step (scattered about 20 ways) 10 times each. All 10 runs failed. I haven't checked all of the error messages yet, but I did see a couple with a new error message about the dbsnp vcf, which I was also streaming with NIO. I'm going to change that back to localizing the dbsnp vcf to see if those go away. Here's that message:. ```; Using GATK jar /usr/gitc/gatk-package-4.beta.2-11-g1b884aa-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:gc_log.log -Xms4000m -jar /usr/gitc/gatk-package-4.beta.2-11-g1b884aa-SNAPSHOT-local.jar BaseRecalibrator -R /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-960e-17578f6b382c/call-SortAndFixSampleBam/CHMI_CHMI3_WGS2.aligned.duplicate_marked.sorted.bam --useOriginalQualities -O CHIM.recal_data.csv -knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf -knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz -L chr8:1+ --cloudPrefetchBuffer 0 --cloudIndexPrefetchBuffer 0; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.DoXBYr; [July 24, 2017 9:31:25 PM UTC] BaseRecalibrator --useOriginalQualities true --knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf --knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz --output CHIM.re",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317736963:165,error,error,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317736963,2,['error'],['error']
Availability,"I reran the BQSR step 5 times using the exact same input bam as before. One has succeeded, 3 are still running and I got a new yet similar error message on a different shard this time:. ```; Using GATK jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:gc_log.log -Xms4000m -jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar BaseRecalibrator -R /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-960e-17578f6b382c/call-SortAndFixSampleBam/CHMI_CHMI3_WGS2.aligned.duplicate_marked.sorted.bam --useOriginalQualities -O CHIM.recal_data.csv -knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf -knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz -L chr12:1+ -L chr13:1+; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.KnjoXJ; [July 21, 2017 2:50:20 PM UTC] BaseRecalibrator --useOriginalQualities true --knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf --knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz --output CHIM.recal_data.csv --intervals chr12:1+ --intervals chr13:1+ --input gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-960e-17578f6b382c/call-SortAndFixSampleBam/CHMI_CHMI3_WGS2.aligned.duplicate_marked.sorted.bam --reference /cromwell_root/broad-r",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317028955:139,error,error,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317028955,1,['error'],['error']
Availability,"I reran the data with the new branch, but forgot to enable the sites-only mode. :-( I'm going to do it right this time and I'll get you results (and possibly an associated thumbs up or thumbs down) soon @kgururaj .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377567285:192,down,down,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377567285,1,['down'],['down']
Availability,"I reran the workflow, this time allocating 200GB RAM to tmp in the slurm job, everything else exactly the same, and got the ""terminate called without an active exception"" failure again, so that error was not due to the gvcf that failed VCF validation as it was not included. This time, shard 9 succeeded in ImportGvcfs and also successfully completed GenotypeGVCFs. I have attached the top level stdout and stderr logs for the slurm job, and the stdout.background and stderr.background logs from shard 3 of ImportGvcfs. No java error log was present on any of the ImportGvcfs shards' execution directories, and all except shard 9 had rc=250.; [ImportGvcfsWithTmpError.tar.gz](https://github.com/broadinstitute/gatk/files/9911311/ImportGvcfsWithTmpError.tar.gz)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1298661994:171,failure,failure,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1298661994,3,"['error', 'failure']","['error', 'failure']"
Availability,"I reran this with retry enabled. It says it finished successfully. There was an error in the middle but it continued anyways (this isn't shown in the final output, but there was a second bar).; I'm not sure about the output, though. Where is this command supposed to leave `output.bam`? It's not on my desktop. ```; [Stage 1:=====================================> (375 + 2) / 553]17/03/30 18:18:46 WARN org.apache.hadoop.hdfs.DFSClient: DFSOutputStream ResponseProcessor exception for block BP-369249695-10.240.0.8-1490738675068:blk_1073745922_5098; java.io.EOFException: Premature EOF: no length prefix available; 	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2282); 	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:244); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:733); 17/03/30 18:18:46 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block BP-369249695-10.240.0.8-1490738675068:blk_1073745922_5098 in pipeline DatanodeInfoWithStorage[10.240.0.4:50010,DS-5596b1b5-b89c-4c39-bbd8-7423614eae0e,DISK], DatanodeInfoWithStorage[10.240.0.3:50010,DS-a0c20806-0af3-4679-b8cd-9cae6ca25071,DISK]: bad datanode DatanodeInfoWithStorage[10.240.0.4:50010,DS-5596b1b5-b89c-4c39-bbd8-7423614eae0e,DISK]; 17/03/30 20:00:21 INFO org.spark_project.jetty.server.ServerConnector: Stopped ServerConnector@61cda923{HTTP/1.1}{0.0.0.0:4040}; 20:00:21.366 INFO MarkDuplicatesSpark - Shutting down engine; [March 30, 2017 8:00:21 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 165.11 minutes.; Runtime.totalMemory()=1222115328; Job [ac3f4131-f19f-47db-8cc3-82b243ad4b72] finished successfully.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2517#issuecomment-290541369:80,error,error,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517#issuecomment-290541369,5,"['Error', 'Recover', 'avail', 'down', 'error']","['Error', 'Recovery', 'available', 'down', 'error']"
Availability,"I resolved the conflicts, @cmnbroad. The only changes for Barclay were javadoc improvements related to the usage of `CommandLineException` after parsing, because I had some issues thinking that any `UserException` will be catch and exit with an error message, but now that it is decoupled into its own class I don't think that it is necessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2282#issuecomment-267000555:245,error,error,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2282#issuecomment-267000555,1,['error'],['error']
Availability,I restarted the travis build since the one failure seems to be an unrelated transient issue.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5998#issuecomment-502238127:43,failure,failure,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5998#issuecomment-502238127,1,['failure'],['failure']
Availability,"I retried once. Failure doesn't look related to this change, but I can't be certain:. ```; The command ./gradlew -Dscala.version=2.12 jacocoTestReport exited with 1.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7642#issuecomment-1017007146:16,Failure,Failure,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7642#issuecomment-1017007146,1,['Failure'],['Failure']
Availability,"I run HaplotypeCaller twice , the former one was stopped because of unexpected power outages. I check the LOG and found the chromosome where HaplotypeCaller stopped. So i star another HaplotypeCaller(later one) with the ""-L *.intervals"", it begin from the chromosome where former HaplotypeCaller stopped.The ref genome and the parameters were all the same. However, HaplotypeCaller give different results. Note: the ref genome has 26 chromosomes :A01-A13;D01-D13. **_The former LOG:_**. nohup: ignoring input and appending output to ‘nohup.out’; 09:04:49.857 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:05:02.971 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.971 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.10.1; 09:05:02.971 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:05:02.972 INFO HaplotypeCaller - Executing as chenwei@localhost.localdomain on Linux v3.10.0-1160.31.1.el7.x86_64 amd64; 09:05:02.972 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_292-b10; 09:05:02.973 INFO HaplotypeCaller - Start Date/Time: August 22, 2021 9:04:49 AM CST; 09:05:02.973 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.973 INFO HaplotypeCaller - ------------------------------------------------------------; 09:05:02.974 INFO HaplotypeCaller - HTSJDK Version: 2.16.1; 09:05:02.974 INFO HaplotypeCaller - Picard Version: 2.18.13; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:05:02.975 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7454:85,outage,outages,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454,1,['outage'],['outages']
Availability,I run `build/install/hellbender/bin/hellbender BaseRecalibrator`; and I get this scary message (notice the three colons (`:`) on that line and lots of UPPERCASE). ```; ***********************************************************************. A USER ERROR has occurred: Invalid command line: Argument RECAL_TABLE_FILE was missing: Argument 'RECAL_TABLE_FILE' is required. ***********************************************************************; ```. I think it should say something like this:. ```; ***********************************************************************. Invalid command line for BaseRecalibrator: Required argument RECAL_TABLE_FILE was missing. Run BaseRecalibrator -h to see all arguments. ***********************************************************************; ```. @vdauwera please weigh in on what's most useful,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/418:248,ERROR,ERROR,248,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/418,1,['ERROR'],['ERROR']
Availability,"I run the BaseRecalibrator,and at fisrt it can good running,after a time,I got this error：; htsjdk.samtools.SAMFormatException: Invalid GZIP header; This is the log:; Using GATK jar /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx20G -Djava.io.tmpdir=./; -jar /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar BaseRecalibrator -R /data/home/wuly/source/Homo_sapiens_assembly38.fasta -I M1.bam --known-sites /data/home/wuly/source/dbsnp_146.hg38.vcf.gz --known-sites /data/home/wuly/source/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites /data/home/wuly/source/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/home/wuly/source/hapmap_3.3.hg38.vcf.gz -O M1_recal.table17:55:54.326 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compre; ssion.soMay 24, 2019 5:55:56 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:55:56.095 INFO BaseRecalibrator - ------------------------------------------------------------; 17:55:56.096 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.2.0; 17:55:56.096 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:55:56.096 INFO BaseRecalibrator - Executing as wuly@localhost.localdomain on Linux v3.10.0-957.10.1.el7.x86_64 amd64; 17:55:56.096 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 17:55:56.096 INFO BaseRecalibrator - Start Date/Time: May 24, 2019 5:55:54 PM EDT; 17:55:56.096 INFO BaseRecalibrator - ------------------------------------------------------------; 17:55:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5968:84,error,error,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5968,1,['error'],['error']
Availability,"I run the GATK MarkDuplicates in Spark mode and it throws an; **NoClassDefFoundError: scala/Product$class**. The GATK version is **4.1.7** and; **4.0.0**,the environment is: **spark-3.0.0**, **scala-2.12.11**. **GATK commands:**. ```; gatk MarkDuplicatesSpark \; -I hdfs://master2:9000/Drosophila/output/Drosophila.sorted.bam \; -O hdfs://master2:9000/Drosophila/output/Drosophila.sorted.markdup.bam \; -M; hdfs://master2:9000/Drosophila/output/Drosophila.sorted.markdup_metrics.txt; \; -- \; --spark-runner SPARK --spark-master spark://master2:7077; ```; **error logs:**. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError:; scala/Product$class; at; org.bdgenomics.adam.serialization.InputStreamWithDecoder.<init>(ADAMKryoRegistrator.scala:35); at; org.bdgenomics.adam.serialization.AvroSerializer.<init>(ADAMKryoRegistrator.scala:45); at; org.bdgenomics.adam.models.VariantContextSerializer.<init>(VariantContext.scala:94); at; org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:179); at; org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:78); at; org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$8(KryoSerializer.scala:170); at; org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$8$adapted(KryoSerializer.scala:170); at scala.Option.foreach(Option.scala:407); at; org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:170); at; scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); at; org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:221); at; org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:161); at; org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:102); at; com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48); at; org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:109); at; org.apache.spark",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6644:558,error,error,558,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6644,1,['error'],['error']
Availability,"I saw your ""whoops"" commit -- I have actually merged commits that turn off tests. James is trying to figure out if we can have codecov check the absolute number of tests since that should almost never go down. He claims you can run select tests from the data provider with IntelliJ, but it seems... cumbersome.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8717#issuecomment-2008200128:204,down,down,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8717#issuecomment-2008200128,1,['down'],['down']
Availability,"I see that argument, but I think one could reasonably argue either way here. I'm really not that invested in this particular test, so I'm happy to check in the files. Thanks for approving the changes. As you might have seen we got a docker pull limit failure: https://travis-ci.com/github/broadinstitute/gatk/jobs/502703296",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7238#issuecomment-831482999:251,failure,failure,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7238#issuecomment-831482999,1,['failure'],['failure']
Availability,"I see your point, but in both Mutect2 and HaplotypeCaller the STR label strictly applies to indels in STRS, not adjacent substitutions. We have found this to be a reasonable way to flag errors due to polymerase slippage.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8340#issuecomment-1613450040:186,error,errors,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8340#issuecomment-1613450040,1,['error'],['errors']
Availability,"I see. . Yes. That's what I'm planning on (except that `AssemblyContigAlignmentsConfigPicker` is upstream of this unit), and here's the thought for why:; * I'd try to place the alignment picking step in a single place as much as possible, this makes improvements to the alignment picking/filtering step easier; * the size-based filter can be tuned, even by an CLI argument, this would affect the number of segments in the CPX logic, and the alt_arrangment annotations, and the simple variants re-interpreted by `CpxVariantReInterpreterSpark`, but it won't affect the alt haplotype sequence, which IMO is what really is important. ; * I'm developing a downstream variant filter, which hopefully can cut down the false-positives. And for the question of ""why 2 instead of 1"", I think what you are suggesting is to change; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 2;; if (one.getSizeOnRead() >= MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; to; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 1;; if (one.getSizeOnRead() > MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; Am i right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353:651,down,downstream,651,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353,4,['down'],"['down', 'downstream']"
Availability,"I set TEST_TYPE to ""all"" and was able to run this test without failure. The command I used is:; ```; ./gradlew test --tests org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile; ```; I ran it 10 times and got the same result every time:; `BUILD SUCCESSFUL`. It looks like this was a transient problem: either the internet connection was stalled or the authentication server was down temporarily. As this happens at the very beginning of the execution, it's probably not a very big deal: the user can just retry. Incidentally, PR #2506 that is under review lengthens the connection timeouts, if I am not mistaken. This will probably make the problem less likely to reoccur.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2514#issuecomment-288852260:63,failure,failure,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514#issuecomment-288852260,2,"['down', 'failure']","['down', 'failure']"
Availability,I spent a long time struggling to install the environment as it hasn't been updated to the new tensorflow and keras versions which changed syntax in the newer versions which cause a lot of the errors you see here. I managed to get it all working by fixing the versions in the yaml but conda takes a loooooong time to solve the environment so I would highly recommend using mamba or micromamba!; I'm attaching the yaml I used to get CNNScoreVariants to work here (renamed as .txt as it won't attach as a yml). [gatkcondaenv_fixed.yml.txt](https://github.com/broadinstitute/gatk/files/12557096/gatkcondaenv_fixed.yml.txt),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1711257642:193,error,errors,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1711257642,1,['error'],['errors']
Availability,"I spoke too soon. No matter how I define the type, whether String or File, when I run womtools validate I get the error:. ```; ERROR: Value for this attribute is expected to be a string:. bam: {; ```. I added the following parameter_meta field to the task:. ![screenshot 2018-11-08 18 02 03](https://user-images.githubusercontent.com/11543866/48232693-7569ff00-e380-11e8-9dad-2eed3ca68118.png). How to correct this @cjllanwarne? Here's the relevant WDL: https://github.com/broadinstitute/gatk/blob/4.0.11.0/scripts/cnv_wdl/cnv_common_tasks.wdl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437188313:114,error,error,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437188313,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,I still got the problem using cromwell + singularity and a component complained with the following error:; GATK_LOCAL_JAR was set to: /root/gatk.jar but this file doesn't exist. Please fix your environment.; The pipeline I used is . cromwell-executions/GATKSVPipelineSingleSample/87d715e3-7dd9-4079-9109-adf536d99400/call-GatherSampleEvidence/GatherSampleEvidence/181ae713-8db5-4a5c-b709-ad5b73a544b8/call-CollectCounts/attempt-2/execution/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6525#issuecomment-955922376:99,error,error,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6525#issuecomment-955922376,1,['error'],['error']
Availability,"I still got the same error with version 4.1.9.0. . > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/orange/reed/nhouse/Raw_seqs/SEQ9_samples/tmp; 11:30:50.248 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:30:50.478 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 26, 2020 11:30:50 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:30:50.791 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.791 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:30:50.792 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:30:50.792 INFO CombineGVCFs - Executing as nwijewardena@c3a-s8.ufhpc on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 11:30:50.792 INFO CombineGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_31-b13; 11:30:50.792 INFO CombineGVCFs - Start Date/Time: October 26, 2020 11:30:50 AM EDT; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.794 INFO CombineGVCFs - HTSJDK Version: 2.23.0; 11:30:50.794 INFO CombineGVCFs - Picard Version: 2.23.3; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:30:50.795 INFO CombineGVCFs - Deflater: IntelDeflater; 11:30:50.795 INFO CombineGVCFs - Inflater: IntelInflater; 11:30:50",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444:21,error,error,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444,2,"['Redundant', 'error']","['Redundant', 'error']"
Availability,"I still want @davidbenjamin to double check the * handling in his code (i.e. we shouldn't count it as a real allele since it's echoing an upstream event that's already been evaluated -- otherwise we could have a lot of high quality * only variants where the triggering SNP is too low quality, which would be silly). @skwalker I would like to do one more thing to check on the allele-specific annotations. Can you genotype all David B.'s GATK4 GVCFs together using GenomicsDBImport? We can use this to test #3707 on a larger callset and then compare the AS vs. non-AS output at biallelic sites.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-385668755:127,echo,echoing,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-385668755,1,['echo'],['echoing']
Availability,"I suggest making composite quadratures by stacking multiple copies of low-order quadratures. Also, note that high-order Gaussian quadratures only provide reliable error guarantees for smooth integrands. Composite quadratures with the same number of grid points often give better estimates than a high-order quadrature with the same number of points.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3318#issuecomment-316889711:154,reliab,reliable,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3318#issuecomment-316889711,2,"['error', 'reliab']","['error', 'reliable']"
Availability,"I suppose we can wait, but I still think there is some inherent instability that is transient that we're going to have to deal with at some point. It looks like the last time I ran the CI tests on the Java 17 branch, these tests were failing on the docker job but passing on the the non-docker job, both with (probably different versions) of Java 17. I was hoping that narrowing it down to a small range of Java versions might help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8111#issuecomment-1331373214:382,down,down,382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8111#issuecomment-1331373214,1,['down'],['down']
Availability,"I suspect it's a different issue than #4062, but probably some similar library incompatibility issue. Unfortunately the stack trace doesn't have enough information in it. We should consider rewriting the error message for this so that we are sure to have the first cause reported, not just the `Could not load genomicsdb native library` message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-356984584:204,error,error,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-356984584,1,['error'],['error']
Availability,"I suspect the issue is that some lines have a very large number of genotypes (due to some combination of a large number of alleles and ploidy). This causes all sorts of failures in the htslib library because of integer (32-bit) overflow of length fields.; A quick check would be to re-run GenotypeGVCFs with the argument --max-genotype-count=10 (small number that should succeed for 150K samples). If that succeeds, we can try to figure out what's a good number for the argument (the default value is 1000).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6742#issuecomment-675545067:169,failure,failures,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6742#issuecomment-675545067,1,['failure'],['failures']
Availability,"I suspect this may be a somewhat controversial change. One of the issues we face for downsampling in spark is that we need to be able to reproduce the same downsampling behavior at a given site across different partitions/environments. To this end I have implemented the ability to reset the random generator used in the ReservoirDownsampler based on the start position of the next reservoir of reads and the gatk default random seed. I'm interested to know what peoples thoughts are on this change, as theoretically it will make the gatk downsampling the same for each start position in the genome. . Fixes #5437",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5448:85,down,downsampling,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5448,3,['down'],['downsampling']
Availability,"I tested this on a collection of 32 tandem duplications (~10kbp-50kbp events, 5kbp padding, 100bp bins) from an HGSV proband case. Using the latest release docker it nailed almost all of them (hooray!) but it hardly called any dups when I tried to do the calling/postprocessing locally using this branch (log reports successful convergence). This could just be an error on my end but we should check before merging.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4720#issuecomment-386454276:364,error,error,364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720#issuecomment-386454276,1,['error'],['error']
Availability,"I tested this using the following sketchy procedure: I temporarily reverted https://github.com/broadinstitute/gatk/pull/5936 on this branch, thereby re-introducing non-ASCII characters into the source. That builds without error, as it should. Then I temporarily changed the newly added encoding declarations included this PR in build.gradle from ""UTF-8"" to ""US-ASCII"", after which I was able to reproduce exactly the same errors as reported in https://github.com/broadinstitute/gatk/issues/5934, for both compile and gatkDoc tasks. So I think these changes achieve the desired result (accept UTF-8 in source).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5946:222,error,error,222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5946,2,['error'],"['error', 'errors']"
Availability,"I think CollectSequencingArtifactMetrics requires a reference, but the documentation lists it as optional. Without reference, I get an instant crash. I also get NullPointer exceptions when proving --DB_SNP. . ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /home/riestma1/gatk-4.beta.3/gatk-package-4.beta.3-local.jar CollectSequencingArtifactMetrics --input sample1.bam --output sample1_pre_adapter_detail_metrics; ...; 18:06:26.220 INFO CollectSequencingArtifactMetrics - Shutting down engine; [July 26, 2017 6:06:26 PM EDT] org.broadinstitute.hellbender.tools.picard.analysis.artifacts.CollectSequencingArtifactMetrics done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=1517289472; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.picard.analysis.artifacts.CollectSequencingArtifactMetrics.acceptRead(CollectSequencingArtifactMetrics.java:214); 	at org.broadinstitute.hellbender.tools.picard.analysis.SinglePassSamProgram.makeItSo(SinglePassSamProgram.java:114); 	at org.broadinstitute.hellbender.tools.picard.analysis.SinglePassSamProgram.doWork(SinglePassSamProgram.java:53); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:173); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3362:637,down,down,637,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3362,1,['down'],['down']
Availability,I think I got most of the WDL changes down. Adding the apply step is waiting on the new tranche filtering PR: https://github.com/broadinstitute/gatk/pull/4800.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4774#issuecomment-391849356:38,down,down,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4774#issuecomment-391849356,1,['down'],['down']
Availability,"I think a good test would be to add a new test vcf datasource with a number of fields, and the values of each field are just the field header. It's easy to assert that and obvious when there's an error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6178#issuecomment-534738783:196,error,error,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6178#issuecomment-534738783,1,['error'],['error']
Availability,"I think even more important is that they share the reference genome that they are using. Get Outlook for Android<https://aka.ms/AAb9ysg>; ________________________________; From: Gökalp Çelik ***@***.***>; Sent: Wednesday, April 24, 2024 12:28:39 AM; To: broadinstitute/gatk ***@***.***>; Cc: Ilya Soifer ***@***.***>; Assign ***@***.***>; Subject: Re: [broadinstitute/gatk] Prevent users enabling annotations with mismatching data type (flow etc) (Issue #8788). Assigned #8788<https://github.com/broadinstitute/gatk/issues/8788> to @ilyasoifer<https://github.com/ilyasoifer>. —; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/8788#event-12581899218>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AGDPRCP66IKPOBF2GPENP6LY63HAPAVCNFSM6AAAAABGTGMBPWVHI2DSMVQWIX3LMV45UABCJFZXG5LFIV3GK3TUJZXXI2LGNFRWC5DJN5XDWMJSGU4DCOBZHEZDCOA>.; You are receiving this because you were assigned.Message ID: ***@***.***>. ________________________________. CONFIDENTIALITY NOTICE: This message (including any attachments) should be presumed to contain confidential, proprietary, privileged and/or private information. Information contained in this message is intended only for the recipient(s) named above. Any disclosure, reproduction, distribution or other use of this message or any attachments by any unauthorized individual or entity is strictly prohibited. If you have received this message in error, please notify the sender immediately, and delete the message and any attachments. Learn more about Ultima Genomics’ Privacy Policy<https://www.ultimagenomics.com/privacy-policy>.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8788#issuecomment-2074020625:1446,error,error,1446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8788#issuecomment-2074020625,1,['error'],['error']
Availability,"I think it must be, but I don't know what the upstream error is until the user tries with the fix. I suspect something along the lines of ""file not found""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6435#issuecomment-581540482:55,error,error,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6435#issuecomment-581540482,1,['error'],['error']
Availability,"I think it triggers in certain situations where a firewall is blocking the connection. If the internet is simply unreachable it doesn't happen, so I don't know what the exact error case is. It happened consistently for people inside Intel's firewall or vpn. . An option to disable gcs support isn't a bad idea, it's kind of a hack though, it would be better if we could understand and avoid triggering the problem. If we could only initialize GCS support when we are sure that we actually are accessing files from google that could be a useful, but it doesn't seem like there's any single point we can plug into to detect that, it would have to be spread over everything that uses paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141:175,error,error,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141,2,['error'],['error']
Availability,I think it's probably better to require it be passed in than give N's. I think N's will potentially cause a lot of chaos in downstream tools,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2624#issuecomment-297823251:124,down,downstream,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2624#issuecomment-297823251,1,['down'],['downstream']
Availability,"I think that a proper example would be the one in the tutorial from @sooheelee (https://software.broadinstitute.org/gatk/blog?id=7847, see also https://github.com/broadinstitute/gatk/issues/3104#issuecomment-314886000). I divided the PRs for the `IndelRealignment` into 4 different sections for better review (2 components of indel-realignment, `RealignerTargetCreator`and `IndelRealignment`). This strategy is because I dissected the pipeline into the easy `RealignerTargetCreator` to found regions worth to look at (this could be marked as experimental/beta before the indel-realignment is in) and the more complicated and component-based `IndelRealigner` (the same as with other tools, this can be marked as experimental/beta until a really good coverage is achieved - in the meantime, I have some test with the current data in the repository and the GATK3 counterpart). There are two parts that are usable outside `IndelRealigner` that are worthy to separate into two commits, and might be useful for other tools/downstream projects: `ConstrainedMateFixingManager` and `NWaySAMFileWriter`. That's the reason of making the port in split PRs. One option can be to have the PRs open, and reviewed independently without acceptance until every component is ready. Otherwise, I think that an experimental tag would be good until we find a good set of tests for edge cases. Does this approach make sense for you, @cmnbroad?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366187605:1017,down,downstream,1017,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366187605,2,['down'],['downstream']
Availability,"I think that each tool should either emit proper error messages, or deal with the data it's given. currently BQSR emits a cryptic error message so I think that this PR is an improvement regardless of whether the pipeline is sanitized. . That said, I think that it might be a good idea to make ValidateSamFile break on non-ACGTN bases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6625#issuecomment-642719900:49,error,error,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6625#issuecomment-642719900,2,['error'],['error']
Availability,"I think that it is necessary to have a way for downstream projects to override some of the top-level arguments in the base CLP class. For example, the config file is for documentation purposes, but I don't want to expose users to that argument because I will set the defaults programmatically. Another example is the GCS retries, which might not be useful for a software that is not planning to support GCS even if it is already implemented (or does not want to expose). As a downstream developer, for me it is important to being able to configure arguments and expose/hide them to my final users; with the current implementation, my main issue is to have an argument that are irrelevant for the toolkit user and that I get questions about why and how to use them (the most clear example, the config file). If the main problem is to change an interface, a default value for new methods can be added to keep the same behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183:47,down,downstream,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183,4,['down'],['downstream']
Availability,"I think that there's too much flexibility to choose arguments that will result in a corrupt product in ReadsPipelineSpark:. Whether or not to align, and whether to align in single-ended or paired mode ought to be dictated by the input data, not by some data-jockey's choice of arguments. I'm prepared to be corrected on this, but I think that paired alignment will only work on queryname sorted, paired input reads. If the data has already been aligned and coordinate sorted, I'd expect things to blow up because the pairs won't be adjacent, and I don't think we do any input sorting. I think we ought to be inferring whether to align, and whether to align singly or paired, from the input. If we want to force realignment of an already-aligned input, we should just have the user run a tool that strips alignment info and produces unaligned, queryname-sorted input to this tool. (Or we could incorporate that functionality into this tool, I suppose.). Second crazy obscure issue: if the input file hasn't been aligned, and therefore doesn't contain a dictionary, we're going to try to borrow one from the reference when we do alignment. That's going to lead to insanity if the reference happens to be a 2bit file in which the order of the contigs is scrambled. We can use such a source for the contig metadata, but the order of the contigs must be prescribed by the reference image file. (The list of reference contig names in correct order is available from the bwa image.). Third issue maybe isn't really an issue: the BwaSparkEngine code doesn't copy tags from input to output, except for read group. (That's because in the case that you're realigning, you'd want to strip all tags associated with the previous alignment, but there's no comprehensive list of tags that are associated with alignment vs. those that have to do with read itself, and there can't be due to the possibility of custom tags that might go into either category.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3666#issuecomment-335572230:1445,avail,available,1445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3666#issuecomment-335572230,1,['avail'],['available']
Availability,"I think that they should be definitely mutex, but also it should be a way to easily disable the defaults and get the ones requested by the user alone. Renaming the argument sounds good to me, but I don't know if it could have downstream problems for Broad or other users...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2360#issuecomment-275452478:226,down,downstream,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2360#issuecomment-275452478,1,['down'],['downstream']
Availability,"I think that this is because in Picard tools the version is populated from the manifest `Version` attribute, and thus it prints the GATK version. @droazen and @cmnbroad - this shows that https://github.com/broadinstitute/gatk/issues/4101 in combination with a common CLP barclay class is really needed to set versions properly to combine toolkits in one (GATK/Picard, integrate common tools into a downstream project, etc.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386994409:398,down,downstream,398,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386994409,1,['down'],['downstream']
Availability,I think the most likely explanation for this behavior would be trying to use an unmapped source of reads. Has your input BAM been aligned to reference?; I apologize that this code isn't more robust to edge conditions.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7710#issuecomment-1064539438:191,robust,robust,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7710#issuecomment-1064539438,1,['robust'],['robust']
Availability,I think the only failure left is a Travis thing. Back to @vruano after heavy refactoring.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5831#issuecomment-519630216:17,failure,failure,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5831#issuecomment-519630216,1,['failure'],['failure']
Availability,"I think there probably needs to be a mechanism in the code to control whether each transformer is applied before or after filtering. Transformers meant to repair bad input need to go before, and those meant to process it in some other way probably need to go after. @lbergelson @cmnbroad @magicDGS suggestions on what form this should take?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-245993455:155,repair,repair,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-245993455,1,['repair'],['repair']
Availability,"I think this is a duplicate of https://github.com/broadinstitute/gatk/issues/2219. Based on that ticket, it appears that this error occurs when trying to write an empty file -- which is why I was hoping that https://github.com/broadinstitute/gatk/pull/2494 would fix this by adjusting the read filtering. @tushu1232 Can you test with https://github.com/broadinstitute/gatk/pull/2494 and tell us whether you still see the error?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2503#issuecomment-288545709:126,error,error,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2503#issuecomment-288545709,2,['error'],['error']
Availability,"I think this is just expected numerical fluctuation within tolerance -- the no-call is a 0,0,0 and the 0,0,1 is very nearly the same.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2712#issuecomment-427382324:59,toler,tolerance,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2712#issuecomment-427382324,1,['toler'],['tolerance']
Availability,"I think this user report sums it up nicely. ----; User Report; ----. In my BASH scripts I often use ""$?"" to monitor the exit status of a process and normally stop if there is an error. However, I am using the latest version of GATK (4.0.0.0) and some tools return 0 exit status even if they fail. Instead, they display the following message to STDOUT:; ; Tool returned:; 1. Though inconvenient for error handling in BASH scripts, this might be an intended behaviour, but not all tools exhibit it. To mention a few, MarkDuplicates, CollectMultipleMetrics, CollectGcBiasMetrics always have a 0 exit status, whereas VariantsToTable or CountVariants do return 1 if they encounter an error. . A similar issue had been reported in the past for previous versions of GATK (https://gatkforums.broadinstitute.org/gatk/discussion/8618/error-handling-end-exit-codes-in-gatk). Best regards,. Roger. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11414/exit-codes-in-gatk-4-0/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4433:178,error,error,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4433,4,['error'],"['error', 'error-handling-end-exit-codes-in-gatk']"
Availability,"I think this user sums it up nicely:. ""Unless I am missing something, the current set up in GATK 4.0 is not ideal for routine sequencing. First, I need to combine all animals every time a new batch of data is added (rather than adding a batch to existing database). Second, if I decide to use combineGVCFs in GATK 4.0, I have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. . As such I am now reverting back to GATK 3.6. It would be very nice if genomicDBimport allowed addition of new data to existing database, and/or genotypegVCF allowed multiple gVCFs."". ----; User Report; ----. This is the error message I am getting but it doesn't make much sense, given that I got it also on smaller datasets with large amounts of memory (larger than some of the specifications listed on this forum for genomicDBimport). Our IT people, after observing the job, seem to think this is a java-related bug as the process itself doesn't use anywhere near the memory specified. We have installed a new version of java and I will be re-running the analysis to see if this solved the issues. . I'm not sure if I should be creating a new thread for this, but I do have a general comment about genomicDBimport. The project I am involved with is in partnership with an industrial partner, who sequences a number of animals every few weeks. In the pipeline using GATK 3.6, the newly sequenced animals were combined using combinegVCF and multiple gVCFs were then fed into genotypeGVCFs. . Unless I am missing something, the current set up in GATK 4.0 is not ideal for routine sequencing. First, I need to combine all animals every time a new batch of data is added (rather than adding a batch to existing database). Second, if I decide to use combineGVCFs in GATK 4.0, I have to run it twice, first to combine the new cohort, then to combine the new cohort with older animals so that I have 1 file to feed to genotypegVCF. .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4667:695,error,error,695,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4667,1,['error'],['error']
Availability,"I think we can start by looking at the coverage files. We don't typically use alt/decoy contigs, so I wonder if unusual coverage on them is causing the model to misbehave. We should take a look at the model and call directories for the shard that caused the failure as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4840#issuecomment-394395632:258,failure,failure,258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840#issuecomment-394395632,1,['failure'],['failure']
Availability,"I think we just don't have any borderline cases in our integration tests. We have a lot of cases where VQSR passes and one with artificial data where it's obviously going to fail. I couldn't reproduce the particular failure they saw in production, so I didn't want to add that to Travis if it's persnickety.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6534#issuecomment-617341642:216,failure,failure,216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6534#issuecomment-617341642,1,['failure'],['failure']
Availability,"I think we should do this in 2 PRs. First @asmirnov239 can first add his modification to run multiple samples in case mode. I can do a subsequent PR for ""double-chunking"" intervals to avoid PAPI/quota errors for high-resolution WGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5054#issuecomment-408629583:201,error,errors,201,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5054#issuecomment-408629583,1,['error'],['errors']
Availability,I thought that at first too. I re-downloaded and got the same error. The md5sum is (I dont know if filename matters for this):. ca9b358340abbe146ecdd1e83c39cad4 ../bin/GenomeAnalysisTK4.jar,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1040864038:34,down,downloaded,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1040864038,2,"['down', 'error']","['downloaded', 'error']"
Availability,I took my bam files that were giving this error and removed the secondary alignments (samtools view -F 256) and ran them again through Mutect2 4.1.4. I didn't get this error now. Does that mean that Mutect2 uses secondary alignments in its calling algorithm normally? BWA generates secondary alignments by default and doesn't have an option to turn them off. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/61300#Comment_61300,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6230:42,error,error,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6230,2,['error'],['error']
Availability,I took the 2022-03-10-4.2.5.0-13-g1c749b37f-NIGHTLY-SNAPSHOT from 18hours ago and got what looks to be the same error message. [mutect2.log](https://github.com/broadinstitute/gatk/files/8226353/mutect2.log),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064400690:112,error,error,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064400690,1,['error'],['error']
Availability,"I tried 4.1.1.0. Although that error is fixed, now I am getting a new one:. ```; java.lang.IllegalArgumentException: log10 p: Values must be non-infinite and non-NAN; 	at org.broadinstitute.hellbender.utils.MathUtils.log10SumLog10(MathUtils.java:1023); 	at org.broadinstitute.hellbender.utils.MathUtils.log10SumLog10(MathUtils.java:995); 	at org.broadinstitute.hellbender.utils.MathUtils.log10SumLog10(MathUtils.java:999); 	at org.broadinstitute.hellbender.utils.MathUtils.normalizeLog10(MathUtils.java:1098); 	at org.broadinstitute.hellbender.utils.MathUtils.normalizeFromLog10ToLinearSpace(MathUtils.java:1074); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.posteriorProbabilityOfError(Mutect2FilteringEngine.java:91); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.posteriorProbabilityOfError(Mutect2FilteringEngine.java:76); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:60); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorPr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-478026887:31,error,error,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-478026887,1,['error'],['error']
Availability,"I tried a quick upgrade of our guava dependency from 18 -> 22, but it ends in test failures. It looks like at least one of our hadoop dependencies requires guava <= 18. I'm not totally clear if it's an issue for hadoop-core or only in hadoop-minicluster which is a library we use for running tests. If you're not using hdfs I think you won't have any problems including 22, but I'm afraid we can't upgrade our default version without some work. . Hopefully hadoop 3.x will solve the problem in general by shading their internal version of guava. ; https://issues.apache.org/jira/browse/HADOOP-14284, https://issues.apache.org/jira/browse/HADOOP-10101. It sounds like you have a reasonable workaround, let us know if you have further issues with it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516:83,failure,failures,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308181516,2,['failure'],['failures']
Availability,I tried again just now with the latest `google-cloud-java` (`0.47.0-alpha:shaded`) and the latest Dataproc image (`1.2.34`) and got the same error. I've asked Google to reopen https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2453,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-388114485:141,error,error,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-388114485,1,['error'],['error']
Availability,"I tried clearing my caches and rebuilding, but I resolve everything. I noticed that our artifactory website looks much different today than it did yesterday. I wonder if it was down temporarily for an update. Maybe try again now? Unless they put it behind the firewall which would be a disaster... can you access https://artifactory.broadinstitute.org/?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641:177,down,down,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641,2,['down'],['down']
Availability,"I tried removing the workspace I had and running GenomicsDBImport and GenotypeGVCFs with the same GATK version (4.0.2.1 and GenomicsDB version 0.9.2), and unfortunately I still get the same error when I run the GenotypeGVCFs command (the GenomicsDBImport command finishes correctly). I will check the instructions to share data and generate a smaller example with random data that can resproduce the error, since the files I'm using are private. ; Thanks again!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4514#issuecomment-372380414:190,error,error,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514#issuecomment-372380414,2,['error'],['error']
Availability,"I tried running MarkDuplicatesSpark with multiple inputs like it is run in production and got this user error. ```; A USER ERROR has occurred: Sorry, we only support a single reads input for spark tools for now.; ```. For this to go into production it would need to have the ability to take in multiple inputs (I'm currently trying to make a ""fast"" version of the production germline pipeline and it would be great to have this tool included in that pipeline). @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5398:104,error,error,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5398,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I tried running gatk version 4.0.7.0 with the environment variable: 'TILEDB_DISABLE_FILE_LOCKING=YES' to see if that would fix the issue, but I still get the same error. I am not sure that the patch that enable that was actually in the 4.0.7.0 release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215:163,error,error,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215,1,['error'],['error']
Availability,"I tried the FilterSamReads command using the Picard jar file and the output bam file to stdout had no issues; Command:; `java -jar ~/picard/build/libs/picard.jar FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET > picard_stdoutbam.bam`; Log:; ```; 10:33:09.327 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/gbrandt/picard/build/libs/picard.jar!/com/intel/gkl/native/libgkl_compression.dylib; INFO	2021-02-23 10:33:09	FilterSamReads	Filtering [presorted=true] subsampled.bam -> OUTPUT=stdout [sortorder=coordinate]; INFO	2021-02-23 10:33:09	SAMFileWriterFactory	Unknown file extension, assuming BAM format when writing file: file:///dev/stdout; INFO	2021-02-23 10:33:09	FilterSamReads	6 SAMRecords written to stdout; ```; Checking the file:; `gunzip -c -d -f picard_stdoutbam.bam | head -n 5`; No issues:; ```; BAM?2@HD	VN:1.6	SO:coordinate; @SQ	SN:1	LN:249250621	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:2	LN:243199373	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:3	LN:198022430	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:4	LN:191154276	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7080#issuecomment-784427228:1048,down,downloads,1048,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7080#issuecomment-784427228,4,['down'],['downloads']
Availability,I tried the latest GATK release and also reported errors.; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR7_gvcf_database -G StandardAnnotation --genomicsdb-shared-posixfs-optimizations true -O new_ALL_MATERIALS_chr7.g.vcf.gz; 17:49:50.404 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 17:49:50.653 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:49:51.271 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.273 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 17:49:51.273 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:51.273 INFO GenotypeGVCFs - Executing as gaoshibin@comput6 on Linux v3.10.0-693.el7.x86_64 amd64; 17:49:51.274 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 17:49:51.274 INFO GenotypeGVCFs - Start Date/Time: 2022年5月22日 下午05时49分50秒; 17:49:51.274 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.275 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.276 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 17:49:51.276 INFO GenotypeGVCFs - Picard Version: 2.27.1; 17:49:51.276 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_S,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135301848:50,error,errors,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135301848,2,"['Redundant', 'error']","['Redundant', 'errors']"
Availability,"I tried this again carefully (12 runs) and only one failed, due to a ""remote server unavailable"" error. This is the same we see before this PR (we need to retry more aggressively) so I think we can merge safely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-281430631:97,error,error,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-281430631,1,['error'],['error']
Availability,"I tried this branch out and got the dreaded 404 error, unfortunately:. ```; $ ./gatk-launch CountReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -- --sparkRunner GCS --cluster droazen-test-cluster --executor-cores 2 --num-executors 2; Using GATK jar /Users/droazen/src/hellbender/build/libs/gatk-package-4.beta.6-54-g0ee99da-SNAPSHOT-spark.jar; jar caching is disabled because GATK_GCS_STAGING is not set. please set GATK_GCS_STAGING to a bucket you have write access too in order to enable jar caching; add the following line to you .bashrc or equivalent startup script. export GATK_GCS_STAGING=gs://<my_bucket>/. Replacing spark-submit style args with dataproc style args. --cluster droazen-test-cluster --executor-cores 2 --num-executors 2 -> --cluster droazen-test-cluster --properties spark.driver.userClassPathFirst=true,spark.io.compression.codec=lzf,spark.driver.maxResultSize=0,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600,spark.executor.cores=2,spark.executor.instances=2. Running:; gcloud dataproc jobs submit spark --cluster droazen-test-cluster --properties spark.driver.userClassPathFirst=true,spark.io.compression.codec=lzf,spark.driver.maxResultSize=0,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994:48,error,error,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994,1,['error'],['error']
Availability,"I tried to reproduce this error, but failed. In fact, we have CI tests that exercise this case. Could you describe the case in more detail? Perhaps I misunderstood. * VCF A (single sample VCF) has a long deletion and overlapping spanning deletions in subsequent lines; * It gets imported into GenomicsDB along with other files; * Querying a position that overlaps the long deletion causes a crash",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5449#issuecomment-445910647:26,error,error,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5449#issuecomment-445910647,1,['error'],['error']
Availability,"I tried to run VariantRecalibrator using the args echoed from an integration test and found that the resource files weren't listed properly. The command in the test was ` "" --resource known,known=true,prior=10.0:"" + getLargeVQSRTestDataDir() + ""dbsnp_132_b37.leftAligned.20.1M-10M.vcf""` and what came out of the engine was `--resource known:/Users/gauthier/workspaces/gatk/src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf`, so it lost the known=true and the prior which makes the command line unrunnable. Probably affects #2269 too. This behavior can be replicated by running any of the VariantRecalibration integration tests and checking the console output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3247:50,echo,echoed,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3247,1,['echo'],['echoed']
Availability,"I uploaded a slice of a cram file with a script that triggers the error.; The name of the file is issue.3845.tar.gz. On Wed, Jan 17, 2018 at 4:09 PM, Louis Bergelson <notifications@github.com>; wrote:. > @jjfarrell <https://github.com/jjfarrell> Yes, the fix was included in; > 4.0.0.0, if you're still encountering the problem then the fix must be; > incomplete. Would it be possible to provide a test case that triggers the; > bug in 4.0.0.0?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/3845#issuecomment-358446091>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AB3rDZRttq4wA6-k9ri1dJlN4NaaPamyks5tLmGdgaJpZM4QgCf5>; > .; >. -- ; John Farrell, Ph.D.; Biomedical Genetics-Evans 218; Boston University Medical School; 72 East Concord Street; Boston, MA. ph: 617-638-5491",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3845#issuecomment-358517044:66,error,error,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3845#issuecomment-358517044,1,['error'],['error']
Availability,I uploaded the files to reproduce the error following the instructions in: https://software.broadinstitute.org/gatk/documentation/article.php?id=1894. The file I uploaded to the ftp is called issue4514.tar.gz.; Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4514#issuecomment-372703813:38,error,error,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514#issuecomment-372703813,1,['error'],['error']
Availability,I use these steps to produce variant calls for haploid data:; HaplotypeCaller in GVCF mode -> CombineGVCFs -> GenotypeGVCFs; and I then create a snpmask file by adding to it any sites I don't want and keep all called SNPs and INDELs inside per sample vcf files. I then try to create a consensus sequence by running in GATK v 4.2.1.0:. ```; gatk FastaAlternateReferenceMaker \; -R myref.fasta \; -V persamplevariants.vcf \; -O new.fasta \; --line-width 80 \; --snp-mask mask.vcf \; --snp-mask-priority ; ```. But I get this error:. ```; java.lang.IllegalArgumentException: Illegal base [ ] seen in the allele; 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:251); 	at htsjdk.variant.variantcontext.Allele.create(Allele.java:402); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.lambda$handlePosition$0(FastaAlternateReferenceMaker.java:176); 	at java.util.Optional.orElseGet(Optional.java:267); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.handlePosition(FastaAlternateReferenceMaker.java:176); 	at org.broadinstitute.hellbender.tools.walkers.fasta.FastaAlternateReferenceMaker.apply(FastaAlternateReferenceMaker.java:141); 	at org.broadinstitute.hellbender.engine.ReferenceWalker.traverse(ReferenceWalker.java:55); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. By narrowing down on where this happens I find it happens here:. ```; chrom	16798	.	TAGC	*	41.94,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7433:464,mask,mask,464,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7433,4,"['error', 'mask']","['error', 'mask', 'mask-priority']"
Availability,"I used HaplotypeCaller (-ERC GVCF) to generate a few hundred GVCF files (see a section below, Fig. 1; ![Fig1](https://user-images.githubusercontent.com/15146751/85318178-27d2d500-b485-11ea-8efb-a92bdad2cc1d.png); ). Then I was trying to use GenomicsDBImport to generate the datastore to be further processed by GenotypeGVCFs:; `gatk --java-options ""-Xmx10g -Xms10g"" GenomicsDBImport \; --genomicsdb-workspace-path /home/zhen.fu/fu_scratch/Helico/genotye/chr1 \; --batch-size 30 \; -L HaChr01 \; --sample-name-map sample_map_file \; --tmp-dir=/home/zhen.fu/fu_scratch/Helico/genotye/tmp \; --reader-threads 2; `. However, I realized that GenomicsDBImport seemed to only recognize ""GT:DP:GQ:MIN_DP:PL"", where ALT field is <NON_REF> for these sites. Conversely, GenomicsDBImport did not recognize the features in the true variant sites, where a true variant and <NON_REF> coexist, such as ""GT:AD:DP:GQ:PL:SB"". As GVCF records all sites, including the variant and non-variant sites. . The error I got was:; ![Fig2](https://user-images.githubusercontent.com/15146751/85318248-3f11c280-b485-11ea-9c5e-71f01b8db1d9.png). The GenomicsDBImport would only process the first batch (30 samples) and not going further. ; The version I am using is GATK 4.15",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6670:985,error,error,985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6670,1,['error'],['error']
Availability,"I used newly GATK4.1 version, and my scripts are:. java -Xmx300g -jar $GATK Mutect2 \; --dont-use-soft-clipped-bases true \; --tmp-dir $cw/$i/tmp \; --input $DNAbam/ADAR16-DNA-2_NKD180600323/ADAR16-DNA-2_NKD180600323.best.uniq.pair.sort.markdup.bam \; --input $RNAbam/$i/$i.merge.markdup.reheader.bam \; --reference $genome\; --output $cw/$i/$i.dna.rna.vcf \; --normal-sample ADAR16-DNA-2_NKD180600323 \; --tumor-sample $i \; -bamout $cw/$i/$i.support.bam. and tail of error log are:. 12:05:06.287 INFO ProgressMeter - scaffold23905:111448 948.1 636040 670.9; 12:05:30.519 INFO ProgressMeter - scaffold23905:133852 948.5 636120 670.7; 12:05:57.277 INFO ProgressMeter - scaffold23905:147186 949.0 636170 670.4; 12:24:34.669 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 31261.455155273; 12:24:34.670 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 14618.28 sec; INFO	2019-04-13 12:45:11	SortingCollection	Creating merging iterator from 2 files; 13:30:49.708 INFO Mutect2 - Shutting down engine; [April 13, 2019 1:30:49 PM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 1,035.35 minutes.; Runtime.totalMemory()=238653800448; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3332); 	at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:137); 	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:121); 	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:421); 	at java.lang.StringBuilder.append(StringBuilder.java:136); 	at htsjdk.samtools.SAMTextHeaderCodec.advanceLine(SAMTextHeaderCodec.java:142); 	at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:97); 	at htsjdk.samtools.reference.ReferenceSequenceFileFactory.loadDictionary(ReferenceSequenceFileFactory.java:235); 	at htsjdk.samtools.reference.AbstractFastaSequenceFile.(AbstractFastaSequenceFile.java:68)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5900:469,error,error,469,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5900,1,['error'],['error']
Availability,"I used to have maven projects depending on GATK4, but I ported them all to gradle. Nevertheless, I haven't seen this error before...maybe it came after all my code has being ported. Sorry for not being useful here...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-339059469:117,error,error,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3724#issuecomment-339059469,1,['error'],['error']
Availability,"I utilized Mutect2 during clinical tumor testing, and the example I provided earlier clearly represents a false positive site. Regrettably, Mutect2 failed to accurately identify it, thereby leading to the inclusion of such false positive sites in clinical medical reports. This outcome is entirely unacceptable. If it is inappropriate to categorize these false positives as STRs, are there alternative methods available for determining these erroneous sites?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8340#issuecomment-1613985690:410,avail,available,410,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8340#issuecomment-1613985690,2,['avail'],['available']
Availability,"I want to filter SNP, but encountered this error. Can anyone help me resolve this error. thanks .; zhang. `##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A USER ERROR has occurred (version 3.8-1-0-gf15c1c3ef): ; ##### ERROR; ##### ERROR This means that one or more arguments or inputs in your command are incorrect.; ##### ERROR The error message below tells you what is the problem.; ##### ERROR; ##### ERROR If the problem is an invalid argument, please check the online documentation guide; ##### ERROR (or rerun your command with --help) to view allowable command-line arguments for this tool.; ##### ERROR; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR Please do NOT post this error to the GATK forum unless you have really tried to fix it yourself.; ##### ERROR; ##### ERROR **MESSAGE: Bad input: The clustered SNPs filter does not work in the presence of non-variant records; see the** documentation for more details; ##### ERROR ------------------------------------------------------------------------------------------; `",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7042:43,error,error,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7042,21,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I wanted to detect variants with HaplotypeCaller（GATK version 4.1.1.0）, i run the program twice with two reference.fa, and i got so different results. The content of this two reference.fa is same and they are in the same version of soybean (Gmax_275_v2.0.fa). The differences of this two reference.fa are as below:; 1.This two reference.fa were downloaded from different databases;; 2.the order of scaffold is different, one is in the number order (just like scaffold_1, scaffold_2, scaffold_ 3, scaffold_4...),and the other one is in the dictionary order(just like scaffold_1002, scaffold_1005, scaffold_101, scaffold_1010...);; 3.the coding method is different, one was coded with upper and lower letters (like ATGGccatgataGGTCaatgca), and the other one was coded only with upper words (like ATGGCCATGATAGGTCAATGCA). . I compared the coding bases of this two reference.fa, totally same, but when i run HaplotypeCaller with this two reference.fa, i got a very different result, so i am wondering, if results of HaplotypeCaller can be affected by the the scaffold order and the lower or upper letters in the reference.fa file?. ### Instructions. The github issue tracker is for bug reports, feature requests, and API documentation requests. General questions about how to use the GATK, how to interpret the output, etc. should be asked on the [official support forum](http://gatkforums.broadinstitute.org/gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any qu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6825:345,down,downloaded,345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6825,1,['down'],['downloaded']
Availability,"I was able to download the gnomAD VCFs with a gsutil cp command within a; couple hours of the remote funcotator failures. --; Alan Hoyle - alan@alanhoyle.com - alanhoyle.com; ------------------------------; *From:* Jonn Smith <notifications@github.com>; *Sent:* Tuesday, November 10, 2020 9:59:06 PM; *To:* broadinstitute/gatk <gatk@noreply.github.com>; *Cc:* Alan Hoyle <alan@alanhoyle.com>; Mention <mention@noreply.github.com>; *Subject:* Re: [broadinstitute/gatk] Funcotator with gnomAD enabled crashes; with Bad Request (#6926). I have tested this with a fresh gcloud client and have not been able to; reproduce the error. I did find an article from someone else who got the 400: invalid_grant; error:; https://blog.timekit.io/google-oauth-invalid-grant-nightmare-and-how-to-fix-it-9f4efaf1da35; <https://www.google.com/url?q=https://blog.timekit.io/google-oauth-invalid-grant-nightmare-and-how-to-fix-it-9f4efaf1da35&source=gmail-imap&ust=1605668351000000&usg=AOvVaw03ZXI9QiPy1AFI5zfsFIjB>. The long and short of it is that it's an authentication issue. Can you; verify that the authentication you're using on the terminal is valid? That; is, can you get at other public resources on gcloud?. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub; <https://www.google.com/url?q=https://github.com/broadinstitute/gatk/issues/6926%23issuecomment-725097056&source=gmail-imap&ust=1605668351000000&usg=AOvVaw0sZboplCCqclv3xBdQk3Fb>,; or unsubscribe; <https://www.google.com/url?q=https://github.com/notifications/unsubscribe-auth/AACGX433BU42UPHZTKQLTBTSPH4XVANCNFSM4TD2FDGA&source=gmail-imap&ust=1605668351000000&usg=AOvVaw0n415Vb2d-0gOnEk9wramu>; .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-728261716:14,down,download,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-728261716,4,"['down', 'error', 'failure']","['download', 'error', 'failures']"
Availability,"I was able to replicate the users error with GATK4.1.1.0 and the latest build on Nov21. The users data is on the FTP as livinlrg_Problem_Interval. . Running SelectVariants on the same data generates the same error as GenomicsDBImport. ```; /home/tools/gatk/gatk SelectVariants --java-options ""-Xmx20g"" -R /home/test/livinlrg_Problem_Interval/Reference/sacCer3_2micron.fasta -V gendb:///home/test/livinlrg_Problem_Interval/GenomicsDB_ProblemInterval_Test -O /home/test/livinlrg_Problem_Interval/work_dir/selectvariantsout.vcf. WARNING: No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 14:28:46.814 INFO ProgressMeter - chrI:2000 0.2 2000 10258.2; 14:29:00.359 INFO ProgressMeter - chrI:6003 0.4 6000 14261.4; 14:29:28.258 INFO SelectVariants - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),37.44650732699998,Cpu time(s),37.414083634000015; [November 21, 2019 2:29:29 PM UTC] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 0.92 minutes.; Runtime.totalMemory()=1783103488; htsjdk.tribble.TribbleException: Invalid block size -1539959833; at htsjdk.variant.bcf2.BCF2Decoder.readNextBlock(BCF2Decoder.java:66); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:134); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:181); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:49); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$F",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275:34,error,error,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275,3,"['down', 'error']","['down', 'error']"
Availability,"I was developing a `LocusWalker` (#1707) when I found that if several BAM files are provided, the `LocusIteratorByState` (LIBS) returns only a `AlignmentContext` with associated `ReadPileup` with only one sample. I realized that in the LIBS there is a commented exception thrown about that multi-sample is not supported. Because it is commented, the LIBS is providing an `AlignmentContext` for the next sample if the first of them does not have coverage. This is misleading for an API user (it took me some time to understand where the error comes from). I was thinking to do a pull request (or include this in #1707) to solve the issue. There are two ways of doing this:; - As in GATK3, implement an internal `PerSampleReadPileup` that extends the `ReadPileup` and provides an efficient way of separate sample-specific pileups.; - If there is no plan to support multi-sample pileups (I'm worried about this, because I will need it), construct the `AlignmentContext` in the LIBS from all samples. Then, the method `makeFilteredPileup` could be used to extract (in a complicated way) a per-sample pileup by the user side. Because the current implementation was done by @akiezun, could you please give me some feedback? I will need it for my stuff, and I will be very grateful if I can solve this as soon as possible...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1752:536,error,error,536,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1752,1,['error'],['error']
Availability,I was getting a SQL syntax error before making this change.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7050:27,error,error,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7050,1,['error'],['error']
Availability,"I was getting the same error with GATK version 4.2.0.0 for VCFs generated by GenotypeGVCFs, or by other tools. ```A USER ERROR has occurred: More then one variant context at position: chr1:969780```. It turns out it's caused either by the input VCF having 2 SNPs on separate rows but with the same position - for example:; ```; chr1	969780	.	C	T	... etc.; chr1	969780	.	C	A	... etc.; ```; or by an INDEL that overlaps a SNP - for example:; ```; chr1	969778	.	TGC	T	199.60	.	.	GT:AD:DP:GQ:PL	0/1:14,7:21:99:207,0,475; chr1	969780	.	C	T	467.64	.	.	GT:AD:DP:GQ:PL	0/1:7,14:21:99:475,0,207; ```; The solution was to filter out INDELs and multiallelic sites before running ASEReadCounter.; This is a reasonable thing to do regardless since INDELs and multiallelic sites are much more likely to yield biased ASE counts due to worse mapping and variant quality. . One way to do this filtering is with . ```; gatk SelectVariants -R hg38.fa. --variant dna_variants.vcf.bgz --restrict-alleles-to BIALLELIC -select 'vc.getHetCount()==1' --select-type-to-include SNP -O dna_variants.selected.vcf.bgz. bcftools norm --rm-dup all dna_variants.selected.vcf.bgz | bgzip > out.vcf.gz; ```. It would be nice if ASEReadCounter could do this automatically instead of exiting with an error, or print an error message that mentions the solution.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7249#issuecomment-1091075626:23,error,error,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7249#issuecomment-1091075626,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,I was just talking with @vruano. The error might come from improper permissions/roles being set up on the cluster by default.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330939236:37,error,error,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330939236,1,['error'],['error']
Availability,"I was outputting to .vcf.gz. . I reran the command to output to just. vcf and it runs without error:; ```; /gatk-launch FilterByOrientationBias --artifactModes 'G/T' -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P ~/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk FilterByOrientationBias --artifactModes G/T -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; 01:16:16.916 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.4.3.jar!/com/intel/gkl/native/libgkl_compression.dylib; [June 6, 2017 1:16:16 AM EDT] FilterByOrientationBias --output test_filterbyorientationbias.vcf --preAdapterDetailFile /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics --artifactModes G/T --variant /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891:94,error,error,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891,2,['error'],['error']
Availability,"I was rerun HaplotypeCallerSpark. gatk --java-options ""-Xmx120g"" HaplotypeCallerSpark \; --reference $path_ref/$assembly.fa \; --input $SCRATCHDIR/$sample.recal.bam \; --output $SCRATCHDIR/$sample.g.vcf.gz \; **--max-mnp-distance 0 \**; --emit-ref-confidence GVCF \; --sample-ploidy $ploidy \; --intervals $chromosome:1+ \; --native-pair-hmm-threads 12 \; -- \; --spark-master local[*] . and resulted with the same issue. gatk --java-options ""-Xmx30g"" CombineGVCFs. Runtime.totalMemory()=2041511936; ***********************************************************************; A USER ERROR has occurred: Bad input: Combining gVCFs containing MNPs is not supported. Unknown contained a MNP at 1:560576. :(",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-671785953:580,ERROR,ERROR,580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-671785953,1,['ERROR'],['ERROR']
Availability,I was running : java -jar /Users/mac/Downloads/picard-2.jar AddOrReplaceReadGroups I=/Users/mac/Desktop/NGS-/SRR6369642-pe.bam O=/Users/mac/Desktop/NGS-/SRR6369642-pe-RG.bam RGID=C7BDWACXX.5 RGLB=Lmj_A445_EP+3.2_run1 RGPL=Illumina RGPU=C7BDWACXX.5 RGSM=NO8162944 . I get :00:13:06.733 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/mac/Downloads/picard-2.jar!/com/intel/gkl/native/libgkl_compression.dylib; [Tue Sep 08 00:13:07 WEST 2020] AddOrReplaceReadGroups INPUT=/Users/mac/Desktop/NGS-/SRR6369642-pe.bam OUTPUT=/Users/mac/Desktop/NGS-/SRR6369642-pe-RG.bam RGID=C7BDWACXX.5 RGLB=Lmj_A445_EP+3.2_run1 RGPL=Illumina RGPU=C7BDWACXX.5 RGSM=NO8162944 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; [Tue Sep 08 00:13:07 WEST 2020] Executing as mac@MacBook-Air-de-mac.local on Mac OS X 10.15.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_65-b17; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.23.0; INFO	2020-09-08 00:13:08	AddOrReplaceReadGroups	Created read-group ID=C7BDWACXX.5 PL=Illumina LB=Lmj_A445_EP+3.2_run1 SM=NO8162944. INFO	2020-09-08 00:13:08	AddOrReplaceReadGroups	Seen many non-increasing record positions. Printing Read-names as well.; fatal error . the first time is was sorting ana indexing when I do it again I get this error how should I fix it !!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6797:37,Down,Downloads,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6797,5,"['Down', 'avail', 'error']","['Downloads', 'available', 'error']"
Availability,"I was running BaseRecalibrator with an --interval of GL00207.1:1+ and received this error:. A USER ERROR has occurred: The file GL000207.1:1+ does not exist. It seems like if the contig name has a '.' in it, it thinks it should be looking for a file. This worked for all other contigs that did not have a '.' in the name and it didn't complain about the Y or MT contigs that come before GL00207.1 in the full command:. org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator --useOriginalQualities true --knownSites /cromwell_root/broad-gitc-reference/hg19/Homo_sapiens_assembly19.dbsnp138.vcf --knownSites /cromwell_root/broad-gitc-reference/hg19/Mills_and_1000G_gold_standard.indels.b37.sites.vcf --knownSites /cromwell_root/broad-gitc-reference/hg19/Homo_sapiens_assembly19.known_indels_20120518.vcf --output NA12878.recal_data.csv --intervals Y:1+ --intervals MT:1+ --intervals GL000207.1:1+ --intervals GL000226.1:1+ --intervals GL000229.1:1+ --intervals GL000231.1:1+ --intervals GL000210.1:1+ --intervals GL000239.1:1+ --intervals GL000235.1:1+ --intervals GL000201.1:1+ --intervals GL000247.1:1+ --intervals GL000245.1:1+ --intervals GL000197.1:1+ --intervals GL000203.1:1+ --intervals GL000246.1:1+ --intervals GL000249.1:1+ --intervals GL000196.1:1+ --intervals GL000248.1:1+ --intervals GL000244.1:1+ --intervals GL000238.1:1+ --intervals GL000202.1:1+ --intervals GL000234.1:1+ --intervals GL000232.1:1+ --intervals GL000206.1:1+ --intervals GL000240.1:1+ --intervals GL000236.1:1+ --intervals GL000241.1:1+ --intervals GL000243.1:1+ --intervals GL000242.1:1+ --intervals GL000230.1:1+ --intervals GL000237.1:1+ --intervals GL000233.1:1+ --intervals GL000204.1:1+ --intervals GL000198.1:1+ --intervals GL000208.1:1+ --intervals GL000191.1:1+ --intervals GL000227.1:1+ --intervals GL000228.1:1+ --intervals GL000214.1:1+ --intervals GL000221.1:1+ --intervals GL000209.1:1+ --intervals GL000218.1:1+ --intervals GL000220.1:1+ --intervals GL000213.1:1+ --intervals GL000211.1:1+ --i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1626:84,error,error,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1626,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I was running this cmd : ; I get : java -jar /Users/mac/Downloads/picard-2.jar AddOrReplaceReadGroups I=/Users/mac/Desktop/NGS-/SRR6369642-pe.bam O=/Users/mac/Desktop/NGS-/SRR6369642-pe-RG.bam RGID=C7BDWACXX.5 RGLB=Lmj_A445_EP+3.2run1 RGPL=Illumina RGPU=C7BDWACXX.5 RGSM=NO8162944. record positions. Printing Read-names as well.; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010d32bea7, pid=1681, tid=6403; #; # JRE version: Java(TM) SE Runtime Environment (8.0_65-b17) (build 1.8.0_65-b17); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.65-b01 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression7875913179822684367.dylib+0x6ea7] deflate_medium+0x867; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/mac/Desktop/NGS-/hs_err_pid1681.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6. how can I fix it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6796:56,Down,Downloads,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6796,3,"['Down', 'error']","['Downloads', 'error']"
Availability,"I was running this following CML :; java -Xmx8G -jar /Users/mac/Downloads/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar VariantFiltration \ -R /Users/mac/Desktop/LmjFwholegenome_20070731_V5.2.fasta \ -V /Users/mac/Desktop/NGS-/57variants.vcf \ -o /Users/mac/Desktop/NGS-/59varians_filt.vcf \ --filter-expression ""QD < 2.0 || MQ > 50"" \ --filter-name ""hard_filtering_snp and I get : A USER ERROR has occurred: Illegal argument value: Positional arguments were provided ', -R{/Users/mac/Desktop/LmjFwholegenome_20070731_V5.2.fasta{ -V{/Users/mac/Desktop/NGS-/57variants.vcf{ -o{/Users/mac/Desktop/NGS-/59varians_filt.vcf{ --filter-expression{QD < 2.0 || MQ > 50{ }' but no positional argument is defined for this tool; how should I fix it and get a filter files ?!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6789:64,Down,Downloads,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6789,2,"['Down', 'ERROR']","['Downloads', 'ERROR']"
Availability,"I was trying to use SVAnnotate to generate annotation for my own `vcf` file:; ```; $java17 -jar $gatkjar SVAnnotate -V 31354420/250000.vcf --protein-coding-gtf $newgtf_path -O 31354420/annotated_250000.vcf; ```; However, I got the following error message:; ```bash; java.lang.NullPointerException: Cannot invoke ""org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.addTranscript(org.banscriptFeature)"" because ""gene"" is null; at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.aggregateRecordsIntoGeneFeature(AbstractGtfCodec.java:339); at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.decode(AbstractGtfCodec.java:170); at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.decode(AbstractGtfCodec.java:23); at htsjdk.tribble.TribbleIndexedFeatureReader$WFIterator.readNextRecord(TribbleIndexedFeatureReader.java:376); at htsjdk.tribble.TribbleIndexedFeatureReader$WFIterator.<init>(TribbleIndexedFeatureReader.java:343); at htsjdk.tribble.TribbleIndexedFeatureReader.iterator(TribbleIndexedFeatureReader.java:310); at org.broadinstitute.hellbender.engine.FeatureDataSource.iterator(FeatureDataSource.java:531); at org.broadinstitute.hellbender.tools.walkers.sv.SVAnnotate.buildIntervalTreesFromGTF(SVAnnotate.java:297); at org.broadinstitute.hellbender.tools.walkers.sv.SVAnnotate.onTraversalStart(SVAnnotate.java:227); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1096); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Seems like these ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8394:241,error,error,241,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8394,1,['error'],['error']
Availability,I was wondering what version of Picard is bundled with GATK. I assume it's the latest available (2.18.4 at this time) but apparently there's no way to tell:; `$ gatk MergeSamFiles -version` output `Version:4.0.4.0`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4733:86,avail,available,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733,1,['avail'],['available']
Availability,I wasn't able to reproduce this exact issue. However at least I can make the error message a bit more user-friendly (submitting #2417 for review). Crucially this will now give the name of the file we're having issues with (thus removing the uncertainty about whether it's the data or the index file we cannot access).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286:77,error,error,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281515286,2,['error'],['error']
Availability,I will also put the topic link for reference.; https://gatk.broadinstitute.org/hc/en-us/community/posts/18885002525467-Error-in-running-SplitNCigarReads-Attempt-to-add-record-to-closed-writer,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8522#issuecomment-1745224697:119,Error,Error-in-running-SplitNCigarReads-Attempt-to-add-record-to-closed-writer,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522#issuecomment-1745224697,1,['Error'],['Error-in-running-SplitNCigarReads-Attempt-to-add-record-to-closed-writer']
Availability,"I will start working on this off of the work that currently resides in #6034. The proposal will be to perform KBestHaplotype finding for multiple source/sink vertexes and then perform smith waterman on the resulting ""dangling"" haplotypes that are created in order to recover the probable dangling sequence. Hopefully the number of haplotypes will have been brought down by enough that this operation will be tolerable in terms of cost.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5957#issuecomment-511024376:267,recover,recover,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5957#issuecomment-511024376,3,"['down', 'recover', 'toler']","['down', 'recover', 'tolerable']"
Availability,"I will try that as well. I just finished building a PoN at 250bp bin size with 1k intervals per block. This produces ~10k models and the PostprocessGermlineCNVCalls WDL task gets us the following error from Cromwell:. > The task run request has exceeded the maximum PAPI request size.If you have a task with a very large number of inputs and / or outputs in your workflow you should try; > to reduce it. Depending on your case you could: 1) Zip your input files together and unzip them in the command. 2) Use a file of file names and localize the files yourself. Who knew? So, we are also going to have to modify PostprocessGermlineCNVCalls and the case mode calling task to accept a tar archive containing all the models. @samuelklee @mbabadi Let me know if you have any opinions on this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-392119427:196,error,error,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-392119427,1,['error'],['error']
Availability,"I wonder if we could factor out a common base interface to cut down on code duplication, and ensure a more consistent API across `SimpleInterval` and `SVInterval`?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5154#issuecomment-418501188:63,down,down,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5154#issuecomment-418501188,1,['down'],['down']
Availability,"I would be useful to be able to explicitly indicate the Codec class for a FeatureInputs perhaps using an annotation. . Currently the feature manager tries every possible codec hoping to find one and only one that answers yes to the canDecode(FileName) method call. If none does execution fails saying that there is no code available to deal with the input file; if more than one codec returns true then is supposed to throw another error indicating the ambiguity. The former is likely an user cased error whereas the later is rather a bug as Codec developers seems to be responsible to make sure that such a collision never happens... This has a few draw backs:; - Seems to quasi-force to establish a 1-to-1 assignation of Codecs and file extension names; canDecode documentation encourages use the file name as the way to determine whether the codec can decode or not the file. What if the file is a simple tab separated value file (with some column count and format constrains) and general extensions such as .tab or .tsv seem acceptable names in practice?; - The error message when there is no supporting code does not tell what the problem is; whether the extension of the file (due to the the 1-to-1 name to type quasi-restriction above) or a more complex formatting issue in the file (e.g. required header missing, version not supported ... blah blah). ; - All codecs are tried out even when most won't ever apply. Even if the performance impact should in practice be minimal still may cause several file IO open operations as several Codec do actually peek into the file (e.g. BCF and VCF codecs). ; - Codec developers have to make sure their new codec does not collides with others; it would be better if codec development can be totally independent.; - General file extensions such as .tab , .tsv cannot be used by codecs due to possible collisions constraining users to name their files the way GATK needs them to; ""I don't like people telling what file names a have to use... I'm already pl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1184:323,avail,available,323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184,3,"['avail', 'error']","['available', 'error']"
Availability,"I would like to have a look to #3447 and review it because it is quite important for downstream projects. But GitHub just show a fancy unicorn saying that ""This page is taking way too long to load"". . I wonder if this is a problem only for me, and if something can be done to be able to review before accepting it. Do you have any idea of why this is happening? @droazen or @jonn-smith, could you help me here? I would like to review before it gets in, for have minimal effects in downstream projects like mine...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3945:85,down,downstream,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3945,2,['down'],['downstream']
Availability,"I would like to point out that aside from breaking tools and making more work for the many users who want to use the GT field and who will _actually be aware_ of this change and its implications, many users are not particularly sophisticated (no knock on them, in the immortal words of Tim Robinson, ""not everybody knows how to do everything"") and so problems arising from this are likely to be caught, _if at all_ at the manuscript review level. Reviewer comments to the effect of ""go back to your genotype calls, fix them, and then redo everything downstream"" are going to be awful for everyone involved to deal with. And this is going to happen _a lot_.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1932251749:550,down,downstream,550,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1932251749,1,['down'],['downstream']
Availability,"I would like to point out that my opinion is still the same: I prefer camel-case instead of kebab-case; anyway, the only requirement for me is to be able to turn off the requirement in downstream projects...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-341378569:185,down,downstream,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-341378569,1,['down'],['downstream']
Availability,"I would suggest adding the --use-jdk-deflater & --use-jdk-inflater options in all the steps to avoid this kind of error, which seems to be random.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582#issuecomment-991514366:114,error,error,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582#issuecomment-991514366,1,['error'],['error']
Availability,I would support pre-installing the R libraries into the base image. Installing R libraries is slow and tend to fail at random because CRAN isn't as reliable as our other dependencies. We just need keep the docker file for the base image around so we can rebuild it with new libraries if we need to.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2699#issuecomment-300576630:148,reliab,reliable,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2699#issuecomment-300576630,1,['reliab'],['reliable']
Availability,"I wrote an alternative to `CramContainerIterator` called `CramContainerHeaderIterator` that doesn't decompress blocks: . https://github.com/tomwhite/Hadoop-BAM/blob/new-bam-other-formats/src/main/java/org/seqdoop/hadoop_bam/spark/CramContainerHeaderIterator.java. On a 6GB CRAM file, `CramContainerIterator` took around 10 minutes to read all the offsets, while `CramContainerHeaderIterator` took less than a minute. Using the `.crai` file took less than a minute too. This was on HDFS; I haven't tried on cloud yet. So I think we should use the `.crai` file if it's available, and fallback to `CramContainerHeaderIterator`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-378888320:567,avail,available,567,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-378888320,1,['avail'],['available']
Availability,"I'd also be hesitant to break the previous expectation that IntervalArgumentCollection contains a non-empty list of intervals. If I understand correctly (and apologies if not, I'm glancing at the repo between paternity-leave duties and am quite sleep deprived!), all calling code would have to add an explicit check that the new option isn't enabled or risk failing ungracefully downstream. For CNV code, this might be as simple as changing the validation method `CopyNumberArgumentValidationUtils.validateIntervalArgumentCollection`, but I wouldn't generally expect it to be so straightforward to add such checks throughout the codebase. I also agree with @lbergelson that the expected behavior might not be immediately clear and that perhaps this could be addressed in the scattering step---seems like shards could just be limited to regions that cover the resource at the outset. Consider also an older comment at https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435588845 about whether or not we should just use the equivalent Picard tool (horrible glob aside).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540740687:379,down,downstream,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540740687,2,['down'],['downstream']
Availability,I'd feel much more comfortable if there was a check in GenotypeGVCFs. Can you add something to look for MNPs in the ReferenceConfidenceVariantContextMerger.merge() loop over the input vcs and error out if it finds one (and there's more than one input)?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5182#issuecomment-421012306:192,error,error,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5182#issuecomment-421012306,1,['error'],['error']
Availability,I'd rather be rigorous and error out. If we pad the missing values there could still be other repercussions downstream.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413667356:27,error,error,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413667356,2,"['down', 'error']","['downstream', 'error']"
Availability,"I'll be adding documented feature tags to the 57 annotation modules. Before I get to these, I need a new ANNOTATORS category to exist in HelpConstants.java. . I've taken the liberty to name the category ANNOTATORS. Here is the relevant info I added to HelpConstants.java:. - group name variable and descriptor: DOC_CAT_ANNOTATORS = ""Annotation Modules""; - group summary variable and descriptor: DOC_CAT_ANNOTATORS_SUMMARY = ""Annotations available to HaplotypeCaller, Mutect2 and VariantAnnotator""; - super category: Utilities (same group as read filters)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3835:437,avail,available,437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3835,1,['avail'],['available']
Availability,"I'll check this out. I dont see local test failures, so i'm not sure what 7 you're referring to, but I agree callRegion() is worth checking out. I am initially seeing this causing some other GenotypeGVCFsIntegrationTest test failures, but need to look more closely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-582244533:43,failure,failures,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-582244533,2,['failure'],['failures']
Availability,I'm able to create a dummy test data that reproduces the same error messages.; On it to fix now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458#issuecomment-368541002:62,error,error,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458#issuecomment-368541002,1,['error'],['error']
Availability,"I'm almost certain this used to work. ```; ./bin/gatk/gatk-launch FlagStatSpark -I file:///local/dev/akiezun/bin/gatk/src/test/resources/org/broadinstitute/hellbender/tools/valid.bam -- --sparkRunner SPARK --sparkMaster yarn-client; ```. the error is . ```; java.lang.IllegalArgumentException: Wrong FS: file:/local/dev/akiezun/bin/gatk/src/test/resources/org/broadinstitute/hellbender/tools/valid.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:474); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:181); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); ```. It's fine when running a LOCAL runner, or when the file is on HDFS. . When resolving the ticket, make sure to devise a way (or at least enter a ticket) to prevent this from happening again - ie some way to discover this kind of problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1417:242,error,error,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1417,1,['error'],['error']
Availability,I'm also having a problem with this bug breaking downstream analysis. Using bcftools to index the vcf containing `MPOS=-2147483648` converts it to `MPOS=.`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6342#issuecomment-584974479:49,down,downstream,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6342#issuecomment-584974479,1,['down'],['downstream']
Availability,"I'm also seeing this more often during the Docker build, not sure if it is related:. ````; Step 5/27 : RUN /gatk/gradlew clean compileTestJava installAll localJar createPythonPackageArchive -Drelease=$DRELEASE; ---> Running in d08cd7336c45; Downloading https://services.gradle.org/distributions/gradle-3.1-bin.zip; .......................................; Exception in thread ""main"" javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at org.gradle.wrapper.Download.downloadInternal(Download.java:77); 	at org.gradle.wrapper.Download.download(Download.java:44); 	at org.gradle.wrapper.Install$1.call(Install.java:61); 	at org.gradle.wrapper.Install$1.call(Install.java:48); 	at org.gradle.wrapper.ExclusiveFileAccessManager.access(ExclusiveFileAccessManager.java:69); 	at org.gradle.wrapper.Install.createDist(Install.java:48); 	at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:107); 	at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401:241,Down,Downloading,241,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4194#issuecomment-358498401,4,"['Down', 'avail']","['Downloading', 'available']"
Availability,I'm confused. Nowhere in the commit above did I disable physical phasing for non-ERC modes. The only line I touched was to change the error message to use the proper argument (GENOTYPE_GIVEN_ALLELES vs. Genotyping Giving Alleles). I have zero recollection of why physical phasing wouldn't be okay for standard HaplotypeCaller.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470950825:134,error,error,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470950825,1,['error'],['error']
Availability,"I'm getting a similar error. any solutions?; ```; 17:14:13.170 INFO FuncotateSegments - The following datasources support funcotation on segments:; 17:14:13.171 INFO FuncotateSegments - Gencode 34 CANONICAL; 17:14:13.209 INFO FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode. Performing conversion.; 17:14:13.209 WARN FuncotatorEngine - WARNING: You are using B37 as a reference. Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases. There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references.; 17:14:13.411 INFO ProgressMeter - Starting traversal; 17:14:13.412 INFO ProgressMeter - Current Locus Elapsed Minutes Features Processed Features/Minute; 17:14:15.391 INFO FuncotateSegments - Shutting down engine; [September 11, 2022 5:14:15 PM GMT] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.30 minutes.; Runtime.totalMemory()=1752170496; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:917445 end:911649; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2939); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914); at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-1243013314:22,error,error,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-1243013314,3,"['down', 'error']","['down', 'error', 'errors']"
Availability,I'm getting a type error here because L278 expects a string for `POSSIBLE_GERMLINE` but `pandas.read_csv` returns a `float64` for this column.; https://github.com/broadinstitute/gatk/blob/d4db277dfa1a9c13188644fd28616249061f8704/scripts/unsupported/combine_tracks_postprocessing_cnv/combine_tracks.wdl#L277-L278,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5360:19,error,error,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5360,1,['error'],['error']
Availability,"I'm getting the same issue on GATK 4.1.9.0 FilterAlignmentArtifacts. This bug has been present for 1 year. Has this been fixed?; Note: There is no work-around because FilterAlignmentArtifacts does not have a --smith-waterman option. Here is my error:; ```; 20:12:42.724 WARN FilterAlignmentArtifacts - . [1m[31m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 20:12:42.725 INFO FilterAlignmentArtifacts - Initializing engine; 20:12:48.403 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-secure-024a1aae-a4f9-4025-aa93-f759f93a8203/50383670-4607-4e59-9bfc-4db970980f0e/Mutect2/773a91ea-25be-4d49-b97c-16527076250c/call-Filter/cacheCopy/TN-20-36-filtered.vcf; 20:12:50.117 INFO FilterAlignmentArtifacts - Done initializing engine; 20:12:51.042 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 20:12:51.099 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 20:12:51.100 INFO IntelPairHmm - Available threads: 14; 20:12:51.100 INFO IntelPairHmm - Requested threads: 4; 20:12:51.100 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 20:12:51.100 INFO ProgressMeter - Starting traversal; 20:12:51.100 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 20:20:25.766 INFO ProgressMeter - chr3:104142090 7.6 1000 132.0; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007efc9818177e, pid=24, tid=0x00007f13b3c76700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # C [libgkl_smithwaterman18",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098:244,error,error,244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098,1,['error'],['error']
Availability,"I'm going to close this issue because it's not a bug. Several things in the code of Mutect2 and FilterMutectCalls adapt as they traverse the genome and it's possible that some learned parameter shifts minutely. For example, the assembly graph pruning algorithm uses knowledge of previously assembled regions to better distinguish between errors and somatic variation. It's also possible that somewhere we forgot to give something a fixed random seed. In full honesty, I _wish_ that I knew exactly what causes the 3142 to become 3143, and I regret that I don't have time for it. Nonetheless, in principle it is not cause for alarm.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8152#issuecomment-1983783338:338,error,errors,338,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8152#issuecomment-1983783338,2,['error'],['errors']
Availability,I'm guessing if you pass the VCFCodec to the GenomicsDBFeatureReader the error will go away. The BCF2Codec gives the correct types if I remember.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2632#issuecomment-297823266:73,error,error,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2632#issuecomment-297823266,1,['error'],['error']
Availability,"I'm having a similar issue on gatk 4.2.6.1; ```; 17:14:13.170 INFO FuncotateSegments - The following datasources support funcotation on segments:; 17:14:13.171 INFO FuncotateSegments - Gencode 34 CANONICAL; 17:14:13.209 INFO FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode. Performing conversion.; 17:14:13.209 WARN FuncotatorEngine - WARNING: You are using B37 as a reference. Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases. There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references.; 17:14:13.411 INFO ProgressMeter - Starting traversal; 17:14:13.412 INFO ProgressMeter - Current Locus Elapsed Minutes Features Processed Features/Minute; 17:14:15.391 INFO FuncotateSegments - Shutting down engine; [September 11, 2022 5:14:15 PM GMT] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.30 minutes.; Runtime.totalMemory()=1752170496; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:917445 end:911649; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2939); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914); at o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7676#issuecomment-1252518062:533,error,errors,533,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676#issuecomment-1252518062,2,"['down', 'error']","['down', 'errors']"
Availability,"I'm honestly not sure what's going on here. The picard/gatk differences will be resolved soon and then the sed command can go away. I don't think it's worth doing anything until then. We should be able to write vcf.gz so if there's an issue there we want to know about it. There could either be an issue in the TabixIndex writer (which we know for sure has issues, one is a blocker on the 11k project at the moment. ) Alternatively the input file could just be invalid, in which case our vcf writer should be detecting the error and is not.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306632877:523,error,error,523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306632877,1,['error'],['error']
Availability,I'm informed that I have access to the broad-firecloud-dsde billing account and it appears whatever error we encountered the other day was something transient.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437409513:100,error,error,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437409513,1,['error'],['error']
Availability,"I'm missing the possibility of tuning the logging to produce TRACE level log messages. As it is stands the users only can choose down to DEBUG. . It seems that this is due to the integration of several logging systems from old GATK, htjsdk and picard where DEBUG the lowest common level. . It would be great to have the ability to produce TRACE level log messages allowing the user to have control on whether these are output or not. . In this case DEBUG log messages that are going to be produced in big numbers should be TRACE whereas unfrequent (one very 5 second or more) would stay as DEBUG.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6378:129,down,down,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6378,1,['down'],['down']
Availability,"I'm new at this, so I may be missing something, but it looks to me like there is an issue with the conversion code in GenomicsConverter.makeSAMRecord() in com.google.cloud.genomics.gatk.common. I've been working on this bam file issue, correcting errors in the files used for tests. Many of the errors involve reads with FLAGs that indicate that they are in pairs, but the mate is not extant in the file, causing the error. A way to fix this without deleting the offending reads is to set the FLAG to zero and also modify the RNEXT, PNEXT, and TLEN fields, if necessary, so that the read becomes single (provided that the values of all of these fields are not important for the tests). However, when I do this, I find that tests that write and then read bam files fail, because when the just-written file is read back, SAM validation complains that the mate unmapped FLAG is set for an unpaired read. It turns out that the copy of the file written by the test substitutes the value '8' for '0' as the FLAG for the modified reads. The relevant code in GenomicsConvertermakeSamRecord() (line 170) is:. flags += ((read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The effect of this line is that all reads which have null mate positions, even those which the FLAG specifies as unpaired, get the mate unmapped FLAG set, causing the validation errors that i'm seeing. The reason the tests have not failed before is apparently that the existing test files do not contain any reads with FLAGs that specify them as unpaired. A simple fix for this would be to convert the line above to:. flags += ( paired && (read.getNextMatePosition() == null || read.getNextMatePosition.getPosition() == null)) ? 8 : 0;. The redundant parens in the original code suggest that something like this may have been intended,but the google genomics documentation at http://google-genomics.readthedocs.org/en/latest/migrating_tips.html gives the following pseudocode:. flags += read.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033:247,error,errors,247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/569#issuecomment-114101033,3,['error'],"['error', 'errors']"
Availability,"I'm not sure exactly what's happening but I suspect it has something to do with the way the files are mounted. My guess is that there is some sort of transient interruption happening in the connection between the EC2 instance and the file server, and it's causing an error in gatk. When reading from a local file GATK does not expect any errors since errors in local files are usually fatal problems caused by a broken disk. Its probably some sort of bug in amazon's fuse implementation which isn't properly hiding network problems from the software. . I expect that your output is truncated at the point the error occured, and you probably need to rerun those shards. Instead of mounting them with amazon's fuse, you could try to either copy the files to a local disc, or access them using an NIO filesystem plugins like this plugin https://github.com/awslabs/aws-java-nio-spi-for-s3 or as signed URLs using https://github.com/broadinstitute/http-nio/ (included in gatk 4.6).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8735#issuecomment-2214915942:267,error,error,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735#issuecomment-2214915942,4,['error'],"['error', 'errors']"
Availability,"I'm not sure if there is another way to see the reports on travis without going through the PR link. Looks like tests are passing now, though. There is quite a bot of code here - I'll start going through it in the next day or so and see where we stand. Also, thx for reducing those files down...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-432388063:288,down,down,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-432388063,1,['down'],['down']
Availability,I'm not sure that we want to go down this path of having to actually change an interface every time we want to add a new top-level argument to the GATK. Let me discuss this PR with the other GATK devs to get their thoughts -- we'll get back to you on this soon.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361625934:32,down,down,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361625934,1,['down'],['down']
Availability,"I'm not sure this is the right place, but I saw that gencode V34 was tested on hg19, I have downloaded the funcotator datasource files V1.7 but it raised an error for gencode, I made a topic of it on the GATK forum, but it might be relevant for you to look at. ; I use Funcotator V4.1.7; https://gatk.broadinstitute.org/hc/en-us/community/posts/360072132411-Funcotator-datasources-v1-7-gencode-raise-error. Further I think Cosmic.db is not working correctly, I don't see any cosmic fields in my vcf output, while I know some of the mutations are in the Cosmic db and also in the Cosmic.db file provided.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6660#issuecomment-673295906:92,down,downloaded,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6660#issuecomment-673295906,3,"['down', 'error']","['downloaded', 'error']"
Availability,"I'm not sure what proportion of users leverage the incremental import functionality...it wasn't available when GenomicsDBImport was first made available, but has been around for ~3 years now. As for workspaces with whole chromosomes -- there is no requirement or performance benefits to using whole chromosomes. As you say, subsetting a chromosome to smaller regions will work and make the import and query parallelizable. (if you remember where the advice about whole chromosomes came from, let us know. That might be something that needs to be updated/clarified). Many small contigs does add overhead to import though and, till recently, multiple contigs couldn't be imported together (i.e., each contig would have it's own folder under the GenomicsDB workspace - which gets inefficient with many small contigs). For WGS, probably the best way to create the GenomicsDBImport interval list is to split based on where there are consecutive N's in the reference genome (maybe using [Picard](https://broadinstitute.github.io/picard/command-line-overview.html#ScatterIntervalsByNs)) and/or regions that you are blacklisting. I think you suggested that some of the blacklisted regions were especially gnarly - maybe ploidy or high alternate allele count? - depending on the frequency of those, we may save a bit on space/memory requirements. That may address your concern about overlap between variants and import intervals. In general, any variant that starts in a specified import interval will show up in a query to that workspace. I'm not sure if the blacklist regions contain any variants that start within but extend beyond the blacklist -- those may not show up if the regions are split up in this way.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1212486548:96,avail,available,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1212486548,2,['avail'],['available']
Availability,"I'm not sure whether this pertains to issue #6510 or #6473 but I am experiencing this issue, which I thought was solved. I have used version 4.1.7.0 haplotype caller, then when I use combineGVCFs I receive the error: A USER ERROR has occurred: Bad input: Combining gVCFs containing MNPs is not supported. Unknown contained a MNP at A1:684599. I didn't set any value for -max-mnp-dist as the docs say that default is already set to 0. Any idea how to get around this error?. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-646955798:210,error,error,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-646955798,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I'm opposed to including 2 entire references since it will raise our git lfs files to somewhere around 5gb. This is a significant drag on downloading / building / testing gatk and should be avoided if possible. I understand that I may be overruled here, but keeping the test files to a reasonable size was and should remain an important goal of gatk4. . It looks like there may be some options to slim down the existing test files that we should take advantage of if possible. There are a number of large vcfs and fasta files which are NOT currently compressed in our large files. We should compress them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5111#issuecomment-423617603:138,down,downloading,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5111#issuecomment-423617603,2,['down'],"['down', 'downloading']"
Availability,"I'm pretty sure this is a hadoop-bam issue, but I'm finding that any BAM produced by bwa (VN 0.7.16a-r1181) will not load in Spark. The BAM loads successfully in ValidateSamFile (although it throws errors because there are no RGs). Running it through AddOrReplaceReadGroups makes the error go away. Attempting to load from local disk gives the following error:. `htsjdk.samtools.SAMFormatException: Does not seem like a BAM file; 	at org.seqdoop.hadoop_bam.BAMSplitGuesser.<init>(BAMSplitGuesser.java:88); 	at org.seqdoop.hadoop_bam.BAMInputFormat.addProbabilisticSplits(BAMInputFormat.java:228); 	at org.seqdoop.hadoop_bam.BAMInputFormat.getSplits(BAMInputFormat.java:155); 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.getSplits(AnySAMInputFormat.java:252); 	at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:121); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	at scala.Option.getOrE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3488:198,error,errors,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3488,3,['error'],"['error', 'errors']"
Availability,"I'm receiving the following error on Mac OS X El Capitan when trying to run gcnv:; ```; 13:32:10.054 DEBUG ScriptExecutor - Executing:; 13:32:10.054 DEBUG ScriptExecutor - python; 13:32:10.054 DEBUG ScriptExecutor - -c; 13:32:10.054 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/__init__.py"", line 12, in <module>; from .sampling import *; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/sampling.py"", line 14, in <module>; from .plots.traceplot import traceplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/__init__.py"", line 1, in <module>; from .autocorrplot import autocorrplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/autocorrplot.py"", line 2, in <module>; import matplotlib.pyplot as plt; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/pyplot.py"", line 113, in <module>; _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup(); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/__init__.py"", line 60, in pylab_setup; [backend_name], 0); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/backend_macosx.py"", line 19, in <module>; from matplotlib.backends import _macosx; RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4743:28,error,error,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4743,1,['error'],['error']
Availability,I'm rerunning them. There is some nasty failure happening but it's not due to your doc changes.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5969#issuecomment-497040300:40,failure,failure,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5969#issuecomment-497040300,1,['failure'],['failure']
Availability,I'm running in intermittent issues when running HaplotypeCallerSpark with GATK 4.0.3.0 and was hoping to generate ideas to debug further. The underlying error is an index error when calculating likelihoods:; ```; java.lang.ArrayIndexOutOfBoundsException: 4; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); ```; I've been unable to generate a reproducible test case. Re-running on the same machine (Amazon m4.4xlarge instances with 16 cores and 64Gb of memory) works. I've seen the error on two different datasets but it happens infrequently as I've also run hundreds using the same setup without any exceptions. The only other thing I spot when looking through the traceback is block issues about the RDDs but I'm not sure if these are a symptom of the failure or a cause:; ```; 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; ```; Here's the full traceback of the failure:; ```; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Putting block rdd_18_12 failed due to an exception; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 WARN BlockManager: Block rdd_18_12 could not be removed as it was not found on disk or in memory; [2018-04-15T03:55Z] ip-10-0-0-57: 18/04/15 03:55:19 ERROR Executor: Exception in task 12.0 in stage 7.0 (TID 828); [2018-04-15T03:55Z] ip-10-0-0-57: java.lang.ArrayIndexOutOfBoundsException: 4; [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables(GenotypeLikelihoodCalculators.java:388); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:263); [2018-04-1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4661:153,error,error,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661,3,['error'],['error']
Availability,"I'm seeing this type of problem with GATK 4.1.2 for a massive WGS sample (> 2 Billion reads); Note that although the error log below says exome, this is a genome sample. I tried raising the ulimit to the same number this other user tried, but I still can get through BaseRecalibratorSpark. I've been trying for awhile to push this data through MarkDuplicatesSpark and finally gave up and switched back to the Picard MarkDuplicates. But even though I got through Picard MarkDuplicates I'm having problems with BaseRecalibratorSpark. I'm running in local mode with 250gb of memory and 128 cores available. . Is there something else for Spark that I can modify to prevent so many tasks getting created in stage 0.0? I'm running on a shared computing system, so it's quite possible that I can't change the ulimit number of open files. I am able to set ulimit -s unlimited, but I might not be able to set ulimit -n beyond a particular threshold on this system. I'll continue to explore the ulimit setting but I'd like to know if there's something else I could try instead. The relevant portion of the error logs are below... 20/01/05 17:43:23 INFO TaskSetManager: Starting task 4991.0 in stage 0.0 (TID 49; 91, localhost, executor driver, partition 4991, PROCESS_LOCAL, 4959 bytes); 20/01/05 17:43:23 INFO Executor: Running task 4991.0 in stage 0.0 (TID 4991); 20/01/05 17:43:23 INFO TaskSetManager: Finished task 4843.0 in stage 0.0 (TID 48; 43) in 74817 ms on localhost (executor driver) (4864/5114); 20/01/05 17:43:23 ERROR Executor: Exception in task 4876.0 in stage 0.0 (TID 487; 6); org.broadinstitute.hellbender.exceptions.UserException$NoSuitableCodecs: Cannot ; read file:///scratch/tmp/spark-ecd63991-68be-4879-b481-68e6789a2004/userFiles-b7; 2d4821-5e36-4d36-aa79-aa6263768669/dbsnp_138.hg19.vcf because no suitable codecs; found; at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(F; eatureManager.java:468); at org.broadinstitute.hellbender.engine.FeatureDataSource.getCode",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-570992855:117,error,error,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-570992855,2,"['avail', 'error']","['available', 'error']"
Availability,I'm sick of scrolling to the bottom to find the test report uri.; This saves the pain of having to scroll all the way down.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3892:118,down,down,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3892,1,['down'],['down']
Availability,"I'm somewhat at a loss here. I make a clean enlistment (on the theory maybe something about roundtripping the VCFs to/from github matters), remade the indexes, and same problem. . Next, I copied the VCFs to a linux machine, ran GATK4 IndexFeatureFile there, copied the indexes back to my windows laptop and checked in. Same travis errors. . Interestingly, this might only be a problem for VCFs I added to the main source tree, not gitlfs. For example, VariantEvalIntegrationTest.testFunctionClassWithSnpeff isnt erroring (i dont believe), and its VCF file lives in gitlfs, not the main repo. I do not know why this would matter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431470219:331,error,errors,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431470219,2,['error'],"['erroring', 'errors']"
Availability,I'm sorry.; I forgot link for bam file.; Please download bam file from following URL. https://pezycomputing-my.sharepoint.com/:f:/g/personal/sakai_pezy_co_jp/Eo5Gvfau1BpMszGCcfDrD14BOfMgxvk7Mt2JCFqcDfgItQ?e=wzZbpL,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1558623547:48,down,download,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1558623547,1,['down'],['download']
Availability,"I'm stepping into the 7 remaining test failures in the debugger, and I think they may just be fixed by overwriting the actual output of `GenotypeGVCFsIntegrationTest.testEntireVariant` onto the expected output for all the forced output test cases. However, this exercise has revealed some other changes:. `GenotypeGVCFsEngine.callRegion` contains the line `final VariantContext mergedVC = merger.merge(variantsToProcess, loc, ref.getBase(), !outputNonVariants, false)`. Since we want to remove the non-ref allele regardless, we should replace `!outputNonVariants` with `true`. It is important to do this here, because only then does `regenotypeVC` correctly restrict the `AD` to the emitted alleles. Also, doing this strips the variant QUAL from the output if it's monomorphic, which is desired (the `RGQ` field is in the output, so there remains a sense of the quality of the call). Once you do that, could you check whether `removeNonRefAndUnusedAltAlleles` is still necessary? In the integration tests, at least, I didn't see any alt alleles output except those called in a genotype.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-582242487:39,failure,failures,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-582242487,1,['failure'],['failures']
Availability,"I'm trying this, but it breaks the test, because the test relies on being; able to set a not-actually-buffering wrapper. As a compromise I'd adding; both methods, so the convenience method is available but the tests can also; do their thing. On Tue, Jan 24, 2017 at 8:48 AM, droazen <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/engine/GATKTool.java; > <https://github.com/broadinstitute/gatk/pull/2331>:; >; > > @@ -137,6 +157,16 @@ void initializeReference() {; > */; > void initializeReads() {; > if (! readArguments.getReadFiles().isEmpty()) {; > + // Prefetcher is useful for cloud files because of the latencies involved.; > + Function<SeekableByteChannel, SeekableByteChannel> wrapper = Function.identity();; > + Function<SeekableByteChannel, SeekableByteChannel> indexWrapper = Function.identity();; > +; > + if (cloudPrefetchBuffer > 0) {; > + wrapper = is -> GATKTool.addPrefetcher(cloudPrefetchBuffer, is);; > + }; > + if (cloudIndexPrefetchBuffer > 0) {; > + indexWrapper = is -> GATKTool.addPrefetcher(cloudIndexPrefetchBuffer, is);; > + }; >; > Yes, please move it all down into ReadsDataSource, and have the; > ReadsDataSource constructors take the buffer sizes as ints. There are; > clients in gatk-protected that use ReadsDataSource directly and would; > benefit, and in general we want to encapsulate these sort of low-level; > details in the data source classes as much as possible, and not bleed into; > the tool hierarchy.; >; > —; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/2331>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AJ-XRcG2t6_144OndJhGaT5zdw9TSWRIks5rVit7gaJpZM4LdCht>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2331#issuecomment-274949252:192,avail,available,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2331#issuecomment-274949252,2,"['avail', 'down']","['available', 'down']"
Availability,"I'm trying to get the container/image from spacecade7/tutorial_11682_11683/ to use the copy number alteration tutorial.; The following error comes up on both my macbook, and on a linux system. . $ docker pull spacecade7/tutorial_11682_11683; Using default tag: latest; Error response from daemon: manifest for spacecade7/tutorial_11682_11683:latest not found: manifest unknown: manifest unknown",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6836:135,error,error,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6836,2,"['Error', 'error']","['Error', 'error']"
Availability,"I'm trying to run Mutect2 in tumor-only mode, for a small panel, and I get this errors at the FilterMutectCalls step. ```bash; [July 26, 2019 9:34:50 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=2129657856; java.lang.IllegalArgumentException: errorRate must be good probability but got NaN; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:225); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:209); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.lambda$applyFiltersAndAccumulateOutputStats$13(Mutect2FilteringEngine.java:176); at java.util.Optional.ifPresent(Optional.java:159); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.applyFiltersAndAccumulateOutputStats(Mutect2FilteringEngine.java:174); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:142); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6058:80,error,errors,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058,4,['error'],"['errorProbToQual', 'errorRate', 'errors']"
Availability,"I'm using GATK 4.1.8.1. And I found a similar error when trying to use GenotypeGVCFs to consolidate genotyping variants on chrX. It does not give any error message. It silently fails. BTW, I'm dealing with WES data. This is the code I used:; # For GenomicDBImport, I randomly select 50 samples from our history samples(using the same probe set) along with the current batch.; time ${gatk} --java-options ""-Xmx8g -Xms2g"" GenomicsDBImport \; --tmp-dir /paedyl01/disk1/yangyxt/test_tmp \; --genomicsdb-update-workspace-path ${vcf_dir}/genomicdbimport_chr${1} \; -R ${ref_gen}/ucsc.hg19.fasta \; --batch-size 0 \; --sample-name-map ${gvcf}/batch_cohort.sample_map \; --reader-threads 5; check_return_code. # For GenotypeGVCFs; time ${gatk} --java-options ""-Xmx8g -Xms2g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" GenotypeGVCFs \; -R ${ref_gen}/ucsc.hg19.fasta \; -V gendb://${vcf_dir}/genomicdbimport_chr${1} \; -G StandardAnnotation \; -G AS_StandardAnnotation \; -L chr${1} \; -O ${bgvcf}/all_${seq_type}_samples_plus_${sample_batch}.chr${1}.HC.vcf. # These are log records:; 02:07:51.286 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 02:07:51.321 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yangyxt/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl; Nov 06, 2020 2:07:56 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 02:07:56.529 INFO GenotypeGVCFs - ------------------------------------------------------------; 02:07:56.529 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 02:07:56.530 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:08:01.543 INFO GenotypeGVCFs - Executing as yangyxt@paedyl01 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 02:08:01.543 INFO Ge",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059:46,error,error,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059,2,['error'],['error']
Availability,"I'm using GATK 4.2.1.0-0 tool `Mutect2` to call mutations in a mitochondrion genome, and later processing the VCFs with `FilterMutectCalls` enabling as well the mitochondria mode (`--mitochondria-mode true`). For some reason, this results in **some** of the VCFs to return the following error:. > java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; > 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:798); > 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:646); > 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:639); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.lambda$calculateQuantileBackgroundResponsibilities$10(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.utils.MathUtils.applyToArray(MathUtils.java:1035); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.calculateQuantileBackgroundResponsibilities(SomaticClusteringModel.java:271); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.initializeClusters(SomaticClusteringModel.java:165); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:325); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); > 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); > 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); > 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8455:287,error,error,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8455,1,['error'],['error']
Availability,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2033:474,error,error,474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033,1,['error'],['error']
Availability,"I'm using HaplotypeCaller (`gatk-package-4.beta.5`) to analyse the control medulloblastoma sample from the ICGC benchmark https://www.nature.com/articles/ncomms10001. Reads are aligned to `GRCh37` (from the Broad bundle, without decoy sequences) using `bwa mem`. Analysis is performed within the [bcbio-nextgen](https://github.com/chapmanb/bcbio-nextgen), which splits input into chunks by chromosome for parallelisation. For some reason, the chunk corresponding to the chromosome `GL000216.1` makes HaplotypeCaller crash with the error `IllegalArgumentException: contig must be non-null and not equal to *, and start must be >= 1`. I isolated the `gatk-launch` command, and narrowed down the reproducible example to these 2 reads:; ```; H239:179:C1K3VACXX:8:2116:11771:72429 161 GL000216.1 19 23 70S31M 4 49141708 0 ATTCCCTTACATTCGGATTGATACTATTAAAATCACTTACTCTTCCTTACATTCCATTCCATCCGGGCTGTTCCATTTCATTCTATTACACTCCACTCAAT ?1:=D>?B?CC:?A,<C;:AEGC<+AC+++2+:3*1:*11999*:099?<?0?99BBG*9?D*?##################################### NM:i:2 MD:Z:25T2C2 AS:i:25 XS:i:20 RG:Z:MB_normal_50x MQ:i:0 ms:i:1911 mc:i:49141802 MC:Z:11S30M5D60M; HWI-7001436:66:C3FYFACXX:5:1216:4411:82080 65 GL000216.1 27 57 101M 9 72653232 0 CATTCTATTACACTCCATTCCATTTCTATCCATTCCATTCCATTCTATTCCATTCCACTTGGGTCGATTCAATTCCATTCCATTCTATCCCTTCCATTCCA CCCFFFFFHHHHHJJJJJIJJJJJJIJJJJHIJJJJJJJJJJJJJJJJJJJJJJJJJIJIJIJGGIJIJJJJJJJJJJJJJJJJJJGJHHHHHHFFFFFFD NM:i:7 MD:Z:24C2T14A16C1T21C4T12 AS:i:66 XS:i:36 RG:Z:MB_normal_50x MQ:i:0 ms:i:3858 mc:i:72653218 MC:Z:14S45M1D33M9; ```; And one nucleotide target region:; ```; GL000216.1 87 88; ```. These BAM and BED files are attached here: [GL000216.1_87_gatk_debug.zip](https://github.com/broadinstitute/gatk/files/1477532/GL000216.1_87_gatk_debug.zip). Running command below:; ```; gatk-launch HaplotypeCaller \; -R /data/projects/punim0010/local/share/bcbio/genomes/Hsapiens/GRCh37/seq/GRCh37.fa \ ; -I GL000216.1_start_49__read_84_88.bam \; -L GL000216.1_87-88.bed \; --output out.vcf.gz; ```. Gives",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3845:531,error,error,531,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3845,2,"['down', 'error']","['down', 'error']"
Availability,"I'm with @davidbenjamin that a camel-case looks clearer, because there are very long names in the GATK-framework that may involve a lot of dashes. Even if the bash-completion will help on this, for downstream projects it can be a nightmare to change this. For instance, I'm not planning to add the bash-completion generation to my toolkit, and I personally find difficult to read long arguments with tons of dashes...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-323703013:198,down,downstream,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-323703013,2,['down'],['downstream']
Availability,"I'm wondering if you have a recommended way of detecting misencoded base quality reads. If I run FixMisencodedBaseQualityReads, I do get a USER ERROR message that I could possibly detect by checking the logs, but I would rather use another tool to check for the error in a script, or store an emitted value from the tool to check for the error. As far as I can tell, I don't see such a tool or value. I have tried the following in a bash commandline interface:; ```; acesnik@DESKTOP$ var=$(gatk FixMisencodedBaseQualityReads -I input.bam -O output.bam); acesnik@DESKTOP$ echo $var # this echos nothing, indicating there's no emitted value; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4242:144,ERROR,ERROR,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4242,5,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'echos', 'error']"
Availability,I'm working on some Plasmodium falciparum callsets in GATK and I have come across a curious error:. ```; Using GATK wrapper script /juffowup/gatk/build/install/gatk/bin/gatk; Running:; /juffowup/gatk/build/install/gatk/bin/gatk HaplotypeCaller -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:92,error,error,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,1,['error'],['error']
Availability,"I'm worried about relaxing the errors to warnings. We previously had a similar round of issues where installation was failing and producing a warning but no error and we ended up with corrupt docker images. So we added code to convert warnings to errors. What we need is to not throw a warning when a remote is unavailable if the other remotes are available, which seems like the behavior I would expect from specifying multiple remotes...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5602#issuecomment-457246918:31,error,errors,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5602#issuecomment-457246918,4,"['avail', 'error']","['available', 'error', 'errors']"
Availability,"I've actually been lobbying to get rid of IntegrationTestSpec completely. Although it probably once added value, it has some [problems](https://github.com/broadinstitute/gatk/issues/1562) that limit its usefulness, and the remaining functionality (like expected exception checking) is available directly in the test framework. We have quite a few tests that don't use it at all, and it would be nice to only support one style, so I'd be inclined to discourage rather than encourage it, and focus on resolving the BaseTest issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243120257:285,avail,available,285,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243120257,1,['avail'],['available']
Availability,I've added back in `ReadTransformer` and `ReadFilter`; I moved the all the default ReadFilters into `ReadFilter` instead of the now redundant `ReadFilters`; The `ReadFilter`s now come pre-negated.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/179:132,redundant,redundant,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/179,1,['redundant'],['redundant']
Availability,"I've already suggested in https://github.com/broadinstitute/barclay/pull/28#discussion_r98629000), when the feature was implemented, that this can be an issue in downstream projects. Maybe it is still not late to change the extension to "".arg_list"" to be sure that it is what the user requested.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3555#issuecomment-327843256:162,down,downstream,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3555#issuecomment-327843256,1,['down'],['downstream']
Availability,"I've also seen UnknownHostException: metadata, which seems like it's probably related. My favorite part about that exception is `This is likely because code is not running on Google Compute Engine.`. ```; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleT",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:345,Error,Error,345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,2,"['Error', 'Failure']","['Error', 'Failure']"
Availability,"I've asked and they seeming do have success with other files. As for Java, we use OpenJDK downloaded from https://jdk.java.net/; ```; openjdk version ""17.0.2"" 2022-01-18; OpenJDK Runtime Environment (build 17.0.2+8-86); OpenJDK 64-Bit Server VM (build 17.0.2+8-86, mixed mode, sharing); ```. Because it's a shared cluster, we aren't able to run Docker directly. But I attempted converting it in to a Singularity container and it didn't crash in the same way, but the job did end up failing. Logs are as follows -. For the ""bare metal"" known-crashing conditions (AMD-based machine), the final lines of the output are:; ```; 22:47:45.999 INFO ProgressMeter - Scaffold_1:21181812 551.0 125350 227.5; 22:47:56.192 INFO ProgressMeter - Scaffold_1:21203869 551.1 125450 227.6; 22:48:06.937 INFO ProgressMeter - Scaffold_1:21251889 551.3 125650 227.9; 22:48:18.177 INFO ProgressMeter - Scaffold_1:21271601 551.5 125750 228.0; 22:48:29.896 INFO ProgressMeter - Scaffold_1:21281660 551.7 125810 228.0; 22:48:40.223 INFO ProgressMeter - Scaffold_1:21284898 551.9 125830 228.0; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f889b5be310, pid=1422929, tid=1422930; #; # JRE version: OpenJDK Runtime Environment (17.0.2+8) (build 17.0.2+8-86); # Java VM: OpenJDK 64-Bit Server VM (17.0.2+8-86, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xcf310] __memset_avx2_unaligned_erms+0x60; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e"" (or dumping to /bigdata/operations/ejaco020/gatk/core.1422929); #; # An error report file with more information is saved as:; # /bigdata/operations/ejaco020/gatk/hs_err_pid1422929.log; #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8988#issuecomment-2386154680:90,down,downloaded,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8988#issuecomment-2386154680,1,['down'],['downloaded']
Availability,"I've attached the java error log from one of the ImportGvcfs shards as well as the top level stdout and stderr logs for the slurm job that was running the workflow. The JRE error is a SIGBUS (!) which seems to be occurring at the point where the C++ native code is trying to hand off execution to the JRE; it gets into uncompiled Java code, switches to the Java interpreter, and throws a ClassNotFound exception. ; [ImportGvcfsError.tar.gz](https://github.com/broadinstitute/gatk/files/9902359/ImportGvcfsError.tar.gz)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1297238255:23,error,error,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1297238255,2,['error'],['error']
Availability,I've been taking a look this morning but am so far unable to reproduce the problem. I tried adding lines to the integration test file to mimic having a SNP at the last base of a spanning deletion as in the bug report:. ```; 20 10001300 . GGG G . . .; 20 10001302 . G C . . .; ```. But I'm not hitting this error. . @gmagoon any chance you could provide any more information that would help us reproduce this? The exact command line you're using might help. Do you still get the error if you run with an alleles file consisting _only_ of one of the pairs of given allele lines listed above?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5336#issuecomment-431865967:306,error,error,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336#issuecomment-431865967,2,['error'],['error']
Availability,"I've been trying repeatedly to run SplitNCigarReads on a mapped RNA-seq bam file (from STAR), but receive the following error:. [March 2, 2023 at 8:40:36 AM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 1.23 minutes.; Runtime.totalMemory()=5184159744; htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 	at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 	at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); 	at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SAMFileGATKReadWriter.java:21); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.writeReads(OverhangFixingManager.java:358); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.flush(OverhangFixingManager.java:338); 	at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:192); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1101); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). I've run the following command:. /cold/gatk-4.3.0.0/./gatk	 --java-options ""-Xmx25g"" SplitNCigarReads \; 	 -R /cold/base/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam \; 	 --tmp-dir /thing -O thing.bam. I've tried setting --tmp-dir, as well as the system varliable TEMP_DIR, but all to no avail. Any suggestions/work-arounds?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232:120,error,error,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232,2,"['avail', 'error']","['avail', 'error']"
Availability,I've created a new issue to make sure the error message for this is better in the future (#6712). This will be included in 4.1.9.0.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661179629:42,error,error,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661179629,1,['error'],['error']
Availability,"I've got this error. java.lang.IllegalArgumentException: Invalid interval. Contig:ENST00000342066.7|ENSG00000187634.11|OTTHUMG00000040719.10|OTTHUMT00000276866.2|SAMD11-202|SAMD11|2551|protein_coding| start:0 end:0. I'm working on hg38. The VCF input file was produced from Mutect2. First, I've the error ""java.lang.NullPointerException"". So I put only ""gencode"" folder in the data-source folder. Then, I've got the error message above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712:14,error,error,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712,3,['error'],['error']
Availability,"I've had several Travis test failures (on my picard removal branch) that appear to be failures during kryo serialization of a mocked ReferenceMultiSource object (based on the failing class name, (org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource$$EnhancerByMockitoWithCGLIB$$b0dc631f, which looks like the CGLIB names mentioned [here](https://github.com/mockito/mockito/issues/319)). We're on an ancient version of mockito anyway, and newer versions no longer use cglib, so it seemed like a good time to upgrade. To do so I also had to replace usage of the method getArgumentAt, which has been [deprecated](https://github.com/mockito/mockito/pull/373) in favor of getArgument.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3581:29,failure,failures,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3581,2,['failure'],['failures']
Availability,"I've hit the same issue. I can't say for certain the files I'm using run cleanly, because I had been getting the error associated with 'AF=.' #5442. . ```; 2019-01-04T14:02:27.214488889Z [January 4, 2019 2:02:27 PM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.05 minutes.; 2019-01-04T14:02:27.214810448Z Runtime.totalMemory()=407896064; 2019-01-04T14:02:27.215613324Z java.lang.IllegalArgumentException: errorRateLog10 must be good probability but got NaN; 2019-01-04T14:02:27.216082142Z 	at org.broadinstitute.hellbender.utils.QualityUtils.phredScaleLog10ErrorRate(QualityUtils.java:321); 2019-01-04T14:02:27.216288993Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.lambda$applyGermlineVariantFilter$10(Mutect2FilteringEngine.java:207); 2019-01-04T14:02:27.216482786Z 	at java.util.stream.DoublePipeline$3$1.accept(DoublePipeline.java:231); 2019-01-04T14:02:27.216675396Z 	at java.util.Spliterators$DoubleArraySpliterator.forEachRemaining(Spliterators.java:1198); 2019-01-04T14:02:27.216858104Z 	at java.util.Spliterator$OfDouble.forEachRemaining(Spliterator.java:822); 2019-01-04T14:02:27.217027544Z 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 2019-01-04T14:02:27.234512924Z 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 2019-01-04T14:02:27.234886181Z 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); 2019-01-04T14:02:27.235138770Z 	at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); 2019-01-04T14:02:27.235509109Z 	at java.util.stream.IntPipeline.toArray(IntPipeline.java:502); 2019-01-04T14:02:27.235661751Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyGermlineVariantFilter(Mutect2FilteringEngine.java:207); 2019-01-04T14:02:27.235931663Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2Filteri",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5553#issuecomment-451464446:113,error,error,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553#issuecomment-451464446,1,['error'],['error']
Availability,"I've implemented the Gaussian-kernel binary-segmentation algorithm from this paper: https://hal.inria.fr/hal-01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:696,recover,recover,696,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,2,['recover'],['recover']
Availability,I've just seen this error too when running `genome_reads-pipeline_hdfs.sh` from https://github.com/broadinstitute/gatk/tree/master/scripts/spark_eval. It's odd that we get an `ArrayIndexOutOfBoundsException` even though there's a `ensureCapacity` call in the previous block...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408867631:20,error,error,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408867631,1,['error'],['error']
Availability,"I've made some improvements to this PR, including:; - Made it easier to use the `joinOverlapping` method by making the function you supply only have to worry about one interval (shard) at a time. This simplifies the callers code, so PileupSpark (for example) is now shorter.; - Added some documentation. I've also used the same technique to improve `AddContextDataToReadSpark` so that references are filled in on a per shard basis, rather than per read. In tests on a 6.6GB file I managed to get BaseRecalibratorSpark's runtime down from 10.61 minutes to 3.73 minutes, which is over 60% faster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2190#issuecomment-250750843:528,down,down,528,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2190#issuecomment-250750843,2,['down'],['down']
Availability,"I've never seen it do anything else. > On Mar 21, 2017, at 4:50 PM, droazen <notifications@github.com> wrote:; > ; > @tedsharpe Have you seen errors like this before with BwaSpark?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2503#issuecomment-288408300:142,error,errors,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2503#issuecomment-288408300,1,['error'],['errors']
Availability,"I've noticed that HDFUtils.writeIntervals seems a bit more memory intensive than it should be. I'm getting OOM errors for bin sizes of 250bp with -Xmx12G, which seems like it should be more than enough. Let's remember to investigate before merging.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3349#issuecomment-324933789:111,error,errors,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3349#issuecomment-324933789,1,['error'],['errors']
Availability,"I've now fixed this PR to build on Java 8 as usual. For Java 11 testing, I've suppressed the warnings for using the com.sun.javadoc classes (they are still available in Java 11, it's just that they are deprecated - or actually marked for removal). Ready for review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-530296978:156,avail,available,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-530296978,1,['avail'],['available']
Availability,I've opened a ticket (https://github.com/samtools/htsjdk/issues/1651) to improve this error message and make it less confusing,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8192#issuecomment-1428469509:86,error,error,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192#issuecomment-1428469509,1,['error'],['error']
Availability,"I've pulled the problem VCF and a couple of successful ones locally and I can confirm that when running with 4 VCFs:. - VCFs that succeeded through JointGermlineCNVSegmentation in our pipeline succeeded for me locally; - The VCF that was flagged in the error message `Exception thrown at chrX:6383391 [VC SAMPLE_ID.segments.vcf.gz ...` completes just fine with other successful partners; - The VCF that was not identified in the error message, but was inferred to be a sex chromosome aneuploidy causes a failure with any combination of other VCFs; - If there are more than 2 VCFs run together, including the failing VCF/aneuploid sample, the error message indicates the problem originates in a non-aneuploid VCF, which misleading and makes this hard to treat. This behaviour was consistent in `4.5.0.0`. Command used in my toy dataset:. ```; gatk --java-options ""-Xms4000M -Xmx6000M"" JointGermlineCNVSegmentation -R /data/Homo_sapiens_assembly38_masked.fasta -O /data/out.vcf.gz -V /data/SAM1.segments.vcf.gz -V /data/SAM2.segments.vcf.gz -V /data/SAM3.segments.vcf.gz -V /data/SAM4.segments.vcf.gz --model-call-intervals /data/preprocessed.interval_list -ped /data/inferred_sex_pedigree.ped; ```. - In this configuration, `SAM4` is aneuploid, and `SAM1` is always the flagged VCF; - If I remove `SAM1` and re-run with 3 VCFs, `SAM3` is mentioned in the error message. It's not derived from alphabetical order, first argument specified with `-V`, or first in the PED file",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8834#issuecomment-2123897736:253,error,error,253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8834#issuecomment-2123897736,5,"['error', 'failure']","['error', 'failure']"
Availability,"I've run into an error using a certain BAM file I created for testing. Possibly relevant: I also tried running it through PrintReads - all reads were filtered out by the WellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2219:17,error,error,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219,1,['error'],['error']
Availability,"I've run into this error in the past as well. As far as I can tell, the SparkGenomeReadCounts code is not trying to do anything too funky, so I wonder if this could be a more general htsjdk/engine issue. But I could be wrong. @droazen could you assign someone to help me look into it? Thanks!. org.apache.spark.SparkException: Job aborted due to stage failure: Task 137 in stage 0.0 failed 1 times, most recent failure: Lost task 137.0 in stage 0.0 (TID 137, localhost): java.lang.IllegalArgumentException: Reference name for '858929714' not found in sequence dictionary.; at htsjdk.samtools.SAMRecord.resolveNameFromIndex(SAMRecord.java:569); at htsjdk.samtools.SAMRecord.setReferenceIndex(SAMRecord.java:422); at htsjdk.samtools.BAMRecord.<init>(BAMRecord.java:87); at htsjdk.samtools.DefaultSAMRecordFactory.createBAMRecord(DefaultSAMRecordFactory.java:42); at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:210); at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.<init>(BAMFileReader.java:1003); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:944); at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:174); at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:226); at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:170); at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoop",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3679:19,error,error,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3679,3,"['error', 'failure']","['error', 'failure']"
Availability,"I've seen this a few times on two different Mac laptops (both with 16G), primarily while running the IntelInflaterDeflaterIntegrationTest from within IntelliJ, but a couple of times I've seen it while running the full test suite from gradle. I saw these while trying to narrow down https://github.com/broadinstitute/gatk/issues/2490 - its probably related. This one happened while several times when running just the IntelInflaterDeflaterIntegrationTest from (on one of the PrintReads tests) from within IntelliJ:. [TestNG] Running:; /Users/cnorman/Library/Caches/IntelliJIdea2016.3/temp-testng-customsuite.xml; java(79316,0x700000d3b000) malloc: *** error for object 0x7f9543bf1000: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Process finished with exit code 134 (interrupted by signal 6: SIGABRT). This one happened while running the full gatk test suite from gradle (note that this one appears to occur during VariantsSparkSinkUnitTest, but in this case the IntelInflaterDeflaterIntegrationTest was the test that had been run immediately previously):. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest.testWritingToFileURL[0](/Users/cmn/projects/hellbender/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf) STANDARD_OUT; 23:02 DEBUG: [kryo] Write: SerializableConfiguration; java(51936,0x119471000) malloc: *** error for object 0x7fd0b7a1d600: pointer being freed was not allocated; *** set a breakpoint in malloc_error_break to debug; Results: SUCCESS (0 tests, 0 successes, 0 failures, 0 skipped)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2535:277,down,down,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535,4,"['down', 'error', 'failure']","['down', 'error', 'failures']"
Availability,"I've tentatively categorized the tools and they are listed in speadsheet format at:; ### https://docs.google.com/a/broadinstitute.org/spreadsheets/d/19SvP6DHyXewm8Cd47WsM3NUku_czP2rkh4L_6fd-Nac/edit?usp=sharing. - [1] GATK4 and Picard tool categories are up for discussion. They are meant to be functional and will be used at <https://software.broadinstitute.org/gatk/documentation/tooldocs/current/>. First pass by Soo Hee. If you have a better idea, please write to this issue ticket.; - [2] We can do better than minimum. At minimum, each tool has a summary description and example command. ; - Authorship should not be picked up by gatkDocs (but can remain in javaDoc portion of code so long as masked). If `* @author Valentin Ruano-Rubio &lt;valentin@broadinstitute.org&gt;` is placed at top of doc, causes javaDoc to not show. Such lines should be at the end of the javaDoc portion. @vdauwera prefers all author annotations be removed.; - Tool commands should use `gatk` to invoke the launch script, not `gatk-launch`. Engine team tells me this change will be effective end of this month.; - A number of tools need `-Xmx` to be defined and this should be reflected in the example command(s). Hopefully, if your tool needs it, you already know it. Otherwise, see <https://github.com/broadinstitute/gatk/issues/3137>.; - [3] `**AMENDED**` Documentation of Picard tools in the Best Practices are a priority as is categorization of Picard tools. In the forum tool list, Picard tools will be mixed with GATK tools alphabetically, with the PICARD label coming after the tool name. To view docs, build with `./gradlew clean gatkDoc`, then view local index in browser.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-345867598:699,mask,masked,699,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-345867598,1,['mask'],['masked']
Availability,"I've tried the latest GATK today. The error message changed. Please help. Thanks. Using GATK jar /omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --output-file-format VCF --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 10:24:13.971 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:14.182 INFO Funcotator - ------------------------------------------------------------; 10:24:14.183 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.4.0-0.0.2; 10:24:14.183 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:24:14.183 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 10:24:14.184 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 10:24:14.184 INFO Funcotator - Start Date/Time: April 28, 2018 10:24:13 AM ICT; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.185 INFO Funcotator - HTSJDK Version: 2.14.3; 10:24:14.185 INFO Funcotator - Picard Version: 2.18.2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:24:14.186 INFO Fu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363:38,error,error,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363,1,['error'],['error']
Availability,"ID 1976). 667 bytes result sent to driver; 23/05/23 13:20:19 INFO TaskSetManager: Finished task 66.0 in stage 31.0 (TID 2040) in 160 ms on localhost (executor driver) (1/128); 23/05/23 13:20:19 INFO TaskSetManager: Finished task 2.0 in stage 31.0 (TID 1976) in 330 ms on localhost (executor driver) (2/128); 23/05/23 13:20:19 INFO Executor: Finished task 3.0 in stage 31.0 (TID 1977). 667 bytes result sent to driver; ...; 23/05/23 13:20:19 INFO TaskSetManager: Finished task 97.0 in stage 31.0 (TID 2071) in 123 ms on localhost (executor driver) (127/128); 23/05/23 13:20:19 INFO TaskSetManager: Finished task 112.0 in stage 31.0 (TID 2086) in 88 ms on localhost (executor driver) (128/128); 23/05/23 13:20:19 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool ; 23/05/23 13:20:19 INFO DAGScheduler: ResultStage 31 (foreach at BwaMemIndexCache.java:84) finished in 0.389 s; 23/05/23 13:20:19 INFO DAGScheduler: Job 7 finished: foreach at BwaMemIndexCache.java:84, took 0.392269 s; 23/05/23 13:20:19 INFO SparkUI: Stopped Spark web UI at http://d01.capitalbiotech.local:4040; 23/05/23 13:20:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 23/05/23 13:20:26 INFO MemoryStore: MemoryStore cleared; 23/05/23 13:20:26 INFO BlockManager: BlockManager stopped; 23/05/23 13:20:26 INFO BlockManagerMaster: BlockManagerMaster stopped; 23/05/23 13:20:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 23/05/23 13:20:26 INFO SparkContext: Successfully stopped SparkContext; 13:20:26.099 INFO PathSeqPipelineSpark - Shutting down engine; [May 23, 2023 1:20:26 PM CST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 1.04 minutes.; Runtime.totalMemory()=156475326464; 23/05/23 13:20:26 INFO ShutdownHookManager: Shutdown hook called; 23/05/23 13:20:26 INFO ShutdownHookManager: Deleting directory pathseq/tmp/spark-2042a18b-a4af-4a86-a236-c4914f0407a1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8339:58018,down,down,58018,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339,1,['down'],['down']
Availability,"ID 6, xx.xx.xx.16, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO TaskSetManager: Lost task 0.1 in stage 2.0 (TID 6) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:55 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.23, executor 5, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:55:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.23:42535 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:55:55 INFO TaskSetManager: Lost task 0.2 in stage 2.0 (TID 7) on xx.xx.xx.23, executor 5: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 3]; 01:00 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:55 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.24, executor 4, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:56:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:49966 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.24:49966 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:56:07 WARN TaskSetManager: Lost task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:28628,Error,Error,28628,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Error'],['Error']
Availability,"IMO we probably should accept any valid VCFs but I can't say it's a high priority to me when our provided resource file works and the user has zero reason not to use it. I mean, if users want to shoot themselves in the foot, and the GATK's downward aim is buggy, that's kind of a feature, isn't it?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5391#issuecomment-435634331:240,down,downward,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5391#issuecomment-435634331,1,['down'],['downward']
Availability,"INDEL alleles=[ACAGTGGGGGTCATTCCCCCTGCAGTGTGTTGGGAGGAGGAGG*, A] attr={AS_FilterStatus=SITE, AS_SB_TABLE=[43, 26|2, 2], DP=94, ECNT=1, GERMQ=93, MBQ=[31, 20], MFRL=[288, 110], MMQ=[60, 60], MPOS=56, NALOD=1.37, NLOD=6.17, POPAF=4.6, ROQ=93, TLOD=10.97} GT=GT:AD:AF:DP:F1R2:F2R1:SB 0/1:46,4:0.07:50:14,3:10,0:28,18,2,2 0/0:23,0:0.041:23:8,0:5,0:15,8,0,0 filters=; 11:43:25.661 WARN GencodeFuncotationFactory - Creating default GencodeFuncotation on transcript ENST00000441716.2 for problem variant: chr6:167976552-167976594(ACAGTGGGGGTCATTCCCCCTGCAGTGTGTTGGGAGGAGGAGG* -> A); 11:44:04.904 INFO ProgressMeter - chr8:677091 4.5 3000 666.0; 11:45:35.226 INFO ProgressMeter - chr11:62279639 6.0 4000 665.6; 11:46:54.284 INFO ProgressMeter - chr15:19905537 7.3 5000 682.4; 11:48:12.767 WARN FuncotatorUtils - createAminoAcidSequence given a coding sequence of length not divisible by 3. Dropping bases from the end: 2 (size=293, ref allele: G); 11:48:16.949 ERROR GencodeFuncotationFactory - Problem creating a GencodeFuncotation on transcript ENST00000379751.5 for variant: chr20:3786474-3786537(TGGGGCCCATCCCGGCGCGCCCCCCGCCCCGGGGCCCGGCGCCGCCGCCGCCGCCCCGGGGCGG* -> T): Cannot yet handle indels starting outside an exon and ending within an exon.; 11:48:16.949 WARN GencodeFuncotationFactory - Creating default GencodeFuncotation on transcript ENST00000379751.5 for problem variant: chr20:3786474-3786537(TGGGGCCCATCCCGGCGCGCCCCCCGCCCCGGGGCCCGGCGCCGCCGCCGCCGCCCCGGGGCGG* -> T); 11:48:31.506 INFO ProgressMeter - chr21:18282114 8.9 6000 670.6; 11:49:08.210 INFO ProgressMeter - chr21:18282114 9.6 6888 720.6; 11:49:08.210 INFO ProgressMeter - Traversal complete. Processed 6888 total variants in 9.6 minutes.; 11:49:08.210 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/2; 11:49:08.211 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/4781; 11:49:08.230 INFO Funcotator - Shutting down engine; [July 7, 2021 11:49:08 AM GMT] org.broadinstitute.hellbender.tools.fun",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-887961422:1487,ERROR,ERROR,1487,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-887961422,1,['ERROR'],['ERROR']
Availability,"INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:35903 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:56:39 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 10) on xx.xx.xx.16, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 18/04/24 17:56:39 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:56:39 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:56:39 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 45.219 s due to Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:35222,Error,Error,35222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Error'],['Error']
Availability,"INFO BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up; 10:33:06.831 INFO SparkEnv - Registering BlockManagerMasterHeartbeat; 10:33:06.846 INFO DiskBlockManager - Created local directory at /raid/tmp/d6/c66ba827e22dbc38625af1cbc85adc/tmp/blockmgr-8dc41ac8-6cf4-4424-9b15-7e2cbfc9e538; 10:33:06.872 INFO MemoryStore - MemoryStore started with capacity 1076.2 GiB; 10:33:06.886 INFO SparkEnv - Registering OutputCommitCoordinator; 10:33:06.916 INFO log - Logging initialized @3948ms to org.sparkproject.jetty.util.log.Slf4jLog; 10:33:06.992 INFO Server - jetty-9.4.46.v20220331; built: 2022-03-31T16:38:08.030Z; git: bc17a0369a11ecf40bb92c839b9ef0a8ac50ea18; jvm 17.0.9+8-LTS; 10:33:07.009 INFO Server - Started @4042ms; 10:33:07.080 INFO AbstractConnector - Started ServerConnector@2f829853{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}; 10:33:07.081 INFO Utils - Successfully started service 'SparkUI' on port 4040.; 10:33:07.116 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@7074da1d{/,null,AVAILABLE,@Spark}; 10:33:07.182 INFO Executor - Starting executor ID driver on host 172.20.19.130; 10:33:07.189 INFO Executor - Starting executor with user classpath (userClassPathFirst = false): ''; 10:33:07.208 INFO Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43279.; 10:33:07.208 INFO NettyBlockTransferService - Server created on 172.20.19.130:43279; 10:33:07.210 INFO BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 10:33:07.214 INFO BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.221 INFO BlockManagerMasterEndpoint - Registering block manager 172.20.19.130:43279 with 1076.2 GiB RAM, BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.225 INFO BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 172.20.19.130, 43279, None); 10:33:07.226 INFO BlockManager - Initialized BlockManager: B",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:43641,AVAIL,AVAILABLE,43641,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,INFO CombineGVCFs - Shutting down engine,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947:29,down,down,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947,1,['down'],['down']
Availability,"INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Version: 2.21.2; > 21:14:29.496 INFO GenotypeGVCFs - Picard Version: 2.21.9; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:14:29.496 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:14:29.496 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:14:29.496 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:14:29.496 INFO GenotypeGVCFs - Requester pays: disabled; > 21:14:29.496 INFO GenotypeGVCFs - Initializing engine; > **[TileDB::StorageManager] Error: Cannot lock consolidation filelock; Cannot lock.**; > 21:14:30.336 INFO GenotypeGVCFs - Shutting down engine; > [May 25, 2020 9:14:30 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.02 minutes.; > Runtime.totalMemory()=1199570944; > ***********************************************************************; > ; > A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader; > ; > ***********************************************************************; > Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace. and last GATK commands used:. ```; gatk GenomicsDBImport \; -V SRR630496.erc.g.vcf \; -V SRR630877.erc.g.vcf \; --genomicsdb-workspace-path mydatabase \; --intervals chr22; ```. > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/miniconda3/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar GenomicsDBImport -V SRR630496.erc.g.vcf -V SR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6627:2933,down,down,2933,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6627,1,['down'],['down']
Availability,"INFO SelectVariants - ------------------------------------------------------------; 10:52:41.265 INFO SelectVariants - ------------------------------------------------------------; 10:52:41.266 INFO SelectVariants - HTSJDK Version: 2.23.0; 10:52:41.266 INFO SelectVariants - Picard Version: 2.22.8; 10:52:41.266 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:52:41.266 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:52:41.267 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:52:41.267 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:52:41.267 INFO SelectVariants - Deflater: IntelDeflater; 10:52:41.267 INFO SelectVariants - Inflater: IntelInflater; 10:52:41.267 INFO SelectVariants - GCS max retries/reopens: 20; 10:52:41.267 INFO SelectVariants - Requester pays: disabled; 10:52:41.267 INFO SelectVariants - Initializing engine; 10:52:41.269 INFO SelectVariants - Shutting down engine; [September 2, 2020 10:52:41 AM GMT] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2076049408; ***********************************************************************. **A USER ERROR has occurred: The specified fasta file (file:///scratch/DBC/BCRBIOIN/SHARED/genomes/homo_sapiens/GRCh38/dna/GRCh38.d1.vd1.fa) does not exist.**. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MissingReference: The specified fasta file (file:///scratch/DBC/BCRBIOIN/SHARED/genomes/homo_sapiens/GRCh38/dna/GRCh38.d1.vd1.fa) does not exist.; 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.checkFastaPath(CachingIndexedFastaSequenceFile.java:173); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:143); 	at org.broadinstitute.hellbender.utils.fasta.C",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685695328:3879,down,down,3879,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685695328,1,['down'],['down']
Availability,"INFO field FS - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.524 info NativeGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.524 info NativeGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field QD - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.524 info NativeGenomicsDB - pid=1332903 tid=1332904 No valid combination operation found for INFO field SOR - the field will NOT be part of INFO fields in the generated VCF records; 20:09:23.528 INFO GenotypeGVCFs - Shutting down engine; [September 23, 2023 at 8:09:23 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2801795072; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:463); at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:365); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:319); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:291); at org.broadinstitute.hellbender.engine.VariantLocusWalker.initialize at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFava:726); at org.broadinstitute.hellbender.engine.VariantLocusWalker.onStartup(VariantLocusWalker.java:63); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.Com",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8527:5989,ERROR,ERROR,5989,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8527,1,['ERROR'],['ERROR']
Availability,"INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@418f0534{/jobs,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@134a8ead{/jobs/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54247647{/jobs/job,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5463f035{/jobs/job/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44fd7ba4{/stages,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69d103f0{/stages/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74fb5b59{/stages/stage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26fadd98{/stages/stage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3db6dd52{/stages/pool,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ef4cbe1{/stages/pool/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2baac4a7{/storage,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6bce4140{/storage/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5882b202{/storage/rdd,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b506ed0{/storage/rdd/json,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65f3e805{/environment,null,AVAILABLE,@Spark}; 18/01/09 18:30:55 INFO handler.ContextHandler: Started o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:8083,AVAIL,AVAILABLE,8083,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['AVAIL'],['AVAILABLE']
Availability,"INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@710ae6a7{/jobs,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b211077{/jobs/json,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62b0bf85{/jobs/job,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f07d414{/jobs/job/json,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40faff12{/stages,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@223967ea{/stages/json,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d7f1e59{/stages/stage,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68e47e7{/stages/stage/json,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16ac4d3d{/stages/pool,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@719c1faf{/stages/pool/json,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f172892{/storage,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45f9d394{/storage/json,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a588b5f{/storage/rdd,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2bdb5e0f{/storage/rdd/json,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2262f0d8{/environment,null,AVAILABLE,@Spark}; 17/10/13 18:11:34 INFO handler.ContextHandler: Started ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:6671,AVAIL,AVAILABLE,6671,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['AVAIL'],['AVAILABLE']
Availability,"IO_READ_FOR_SAMTOOLS : false; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:14:29.496 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:14:29.496 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:14:29.496 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:14:29.496 INFO GenotypeGVCFs - Requester pays: disabled; > 21:14:29.496 INFO GenotypeGVCFs - Initializing engine; > **[TileDB::StorageManager] Error: Cannot lock consolidation filelock; Cannot lock.**; > 21:14:30.336 INFO GenotypeGVCFs - Shutting down engine; > [May 25, 2020 9:14:30 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.02 minutes.; > Runtime.totalMemory()=1199570944; > ***********************************************************************; > ; > A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader; > ; > ***********************************************************************; > Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace. and last GATK commands used:. ```; gatk GenomicsDBImport \; -V SRR630496.erc.g.vcf \; -V SRR630877.erc.g.vcf \; --genomicsdb-workspace-path mydatabase \; --intervals chr22; ```. > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/miniconda3/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar GenomicsDBImport -V SRR630496.erc.g.vcf -V SRR630877.erc.g.vcf --genomicsdb-workspace-path mydatabase -L chr22; > 21:21:17.641 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/miniconda3/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; > May 25, 2020 9:21:17 PM shaded.cloud_nio.com.google.auth.oauth2.Comput",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6627:3195,ERROR,ERROR,3195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6627,1,['ERROR'],['ERROR']
Availability,"IT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global paramete",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:4820,error,error,4820,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,4,['error'],['error']
Availability,"If I clone GATK with the ssh URL (`git@github.com:broadinstitute/gatk.git`), and then run a `docker build` command from the root of that clone, I get ssh authentication errors at the `git lfs pull` step:. ```; Step 9/36 : RUN git lfs pull; ---> Running in 1f415556efd2; Git LFS: (0 of 104 files) 0 B / 1.28 GB ; batch request: Host key verification failed.: exit status 255; batch request: Host key verification failed.: exit status 255; error: failed to fetch some objects from 'https://github.com/broadinstitute/gatk.git/info/lfs'; The command '/bin/sh -c git lfs pull' returned a non-zero code: 2; ```. If I do the same thing from a GATK clone created using the https URL (`https://github.com/broadinstitute/gatk.git`), I get no lfs error. This also raises the larger question of whether we are authenticating with github before doing `git lfs pull` during the docker build, as I believe that the quotas for unauthenticated `git lfs` operations are much smaller than for authenticated operations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7077:169,error,errors,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7077,3,['error'],"['error', 'errors']"
Availability,"If I make a tool fail, e.g. a bad argument. the process exit status is 0 making difficult to track failure in including scripts, SGE and (perhaps?) Queue?. Failure should result in a non-zero exit status.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/342:99,failure,failure,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/342,2,"['Failure', 'failure']","['Failure', 'failure']"
Availability,"If I run BaseRecalibrator on a reference with contigs [20,21] and knownSites only has only sites from 17, then GATK3 blows up:. ```; ##### ERROR MESSAGE: Input files knownSites and reference have incompatible contigs: No overlapping contigs found.; ##### ERROR knownSites contigs = [17]; ##### ERROR reference contigs = [20, 21]; ```. but gatk4 does not (and it should). This is the cause of the bogus tests in #1017 (they should have never been allowed to exist). The commandline for GATK3 is; (the VCF has no sequence dictionary). ```; -T BaseRecalibrator -R src/test/resources/large/human_g1k_v37.20.21.fasta -I src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.bam --out gatk3.3.recal.txt --knownSites src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1029:139,ERROR,ERROR,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1029,3,['ERROR'],['ERROR']
Availability,If I try to navigate to https://gatk-jenkins.broadinstitute.org/ I get:. ```; Proxy Error. The proxy server received an invalid response from an upstream server.; The proxy server could not handle the request GET /. Reason: Error reading from remote server. Apache Server at gatk-jenkins.broadinstitute.org Port 443; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3862:84,Error,Error,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3862,2,['Error'],['Error']
Availability,If IndexingVariantContextWriter is meant to create a index alongside /dev/null (... /dev/null.idx?) it isn't surprising that it does not work although the error message could be more informative. Is there a way to ask SelectVariants not to create an index and in that case you still have an exception?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3829#issuecomment-345038335:155,error,error,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3829#issuecomment-345038335,1,['error'],['error']
Availability,"If Louis says this is allowed in our style guide then you can leave them; in. I didn't realize that. Feel free to drop the hammer on us for any style; violations. On Mon, May 1, 2017 at 1:04 PM, tedsharpe <notifications@github.com> wrote:. > *@tedsharpe* commented on this pull request.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/spark/sv/; > BreakpointClusterer.java; > <https://github.com/broadinstitute/gatk/pull/2627#discussion_r114155944>:; >; > > }; >; > - @Override; > - public Iterator<BreakpointEvidence> apply( final BreakpointEvidence evidence ) {; > - if ( evidence.getContigIndex() != currentContig ) {; > - currentContig = evidence.getContigIndex();; > - locMap.clear();; > + public Iterator<BreakpointEvidence> apply( final Iterator<BreakpointEvidence> evidenceItr ) {; > + while ( evidenceItr.hasNext() ) {; > + final BreakpointEvidence evidence = evidenceItr.next();; > + final SVInterval location = evidence.getLocation();; > + final SVIntervalTree.Entry<List<BreakpointEvidence>> entry = evidenceTree.find(location);; > + if ( entry != null ) entry.getValue().add(evidence);; >; > Pretty sure that Louis said that this was one of our departures from; > Google style: single statements following an ""if"", ""else"", or ""else if""; > that fit comfortably on the same line are allowed (but not required) to be; > unbraced.; > Since you prefer braces, I'll change these.; > However, since you've thrown down the gauntlet, I'm going to start nailing; > you guys on very long lines (max line length is supposed to be 100; > characters). So there.; >; > —; > You are receiving this because your review was requested.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/2627#discussion_r114155944>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZZPkQPglpEhCzZqbA17GshZt6t-Dks5r1hCsgaJpZM4NKPYH>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2627#issuecomment-298379400:1464,down,down,1464,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2627#issuecomment-298379400,2,['down'],['down']
Availability,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2081:407,error,error,407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081,3,"['ERROR', 'avail', 'error']","['ERROR', 'available', 'error']"
Availability,"If a core dump is produced while running tests on travis, this will echo the log file to the travis log (ie., it was triggered [here](https://api.travis-ci.com/v3/job/468677651/log.txt) by the pair hmm seg fault) so the java and native thread stacks can be inspected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7020:68,echo,echo,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7020,2,"['echo', 'fault']","['echo', 'fault']"
Availability,"If an invalid path is given to `Pileup`, the exception is not informative for the final user: `A USER ERROR has occurred: Invalid command line: Argument output has a bad value: /some/invalid/path. Problem constructing PrintStream from the string '/some/invalid/path'.`. Although #121 should correct this issue, in the meanwhile it could be better to use `File` in the argument and generate the stream in `onTraversalStart` to throw a more informative `CouldNotCreateOutputFile`. Thanks to @lbergelson for pointing it out when reviewing #1862.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1909:102,ERROR,ERROR,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1909,1,['ERROR'],['ERROR']
Availability,"If anyone wants to learn more about the horrors of HLA (and MHC more generally) naming, ping me elsewhere, probably best at https://github.com/nmdp-bioinformatics/genotype-list.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3360#issuecomment-324732655:88,ping,ping,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360#issuecomment-324732655,2,['ping'],['ping']
Availability,"If consolidate ran without issues, that array/contig folder didn't have any fragments that were incomplete. And yes, the duplicates were consolidated down so everything should be fine. . I should add, my last bullet about sanity checks regarding number of fragments does not apply to consolidated arrays. Those will, of course, only have a single fragment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722733110:150,down,down,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722733110,1,['down'],['down']
Availability,"If gatk-launch is run so that it invokes a local jar directly without the generated launch script, it fails to properly pass system properties. This will cause confusing bugs and performance issues. It was discovered in due to #2300. This effects all users using our packaged jars available on the website since they don't include the gradle generated wrapper script.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316:281,avail,available,281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316,1,['avail'],['available']
Availability,"If it helps, I have seen this error when using local drives exclusively (not attached to a shared file system). . Twice it has manifested as a core dump that points to ` C [libc.so.6+0xaf4f9] malloc+0x169`: . ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000014cfb1d504f9, pid=1182729, tid=1195264; #; # JRE version: OpenJDK Runtime Environment (17.0.3) (build 17.0.3-internal+0-adhoc..src); # Java VM: OpenJDK 64-Bit Server VM (17.0.3-internal+0-adhoc..src, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xaf4f9] malloc+0x169; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h"" (or dumping to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.3/core.1182729); #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; [dalegre@login4601 fdone]$ head -n 20 hs_err_pid1182729.log; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000014cfb1d504f9, pid=1182729, tid=1195264; #; # JRE version: OpenJDK Runtime Environment (17.0.3) (build 17.0.3-internal+0-adhoc..src); # Java VM: OpenJDK 64-Bit Server VM (17.0.3-internal+0-adhoc..src, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xaf4f9] malloc+0x169; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h"" (or dumping to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.3/core.1182729); #; # If you would like to submit a bug report, please v",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1936285520:30,error,error,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1936285520,2,['error'],['error']
Availability,"If not another dev, I am the only one from Comms available currently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3950#issuecomment-351196606:49,avail,available,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3950#issuecomment-351196606,1,['avail'],['available']
Availability,"If one of the block compressed VCFs in the list is empty (i.e. it does have proper header lines but there are no variant records, which is perfectly valid) then the tool fails with an IllegalStateException:. java.lang.IllegalStateException: Could not read available bytes from BlockCompressedInputStream.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:697); at org.broadinstitute.hellbender.tools.GatherVcfs.gatherWithBlockCopying(GatherVcfs.java:354)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3218:256,avail,available,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3218,1,['avail'],['available']
Availability,"If only `HaplotypeCaller` uses `--contamination-fraction-to-filter`/`--contamination-fraction-per-sample-file`, then perhaps those arguments should be moved down into `HaplotypeCallerArgumentCollection`?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5352#issuecomment-432709807:157,down,down,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5352#issuecomment-432709807,1,['down'],['down']
Availability,"If the sample name does not exist in the vid map (JSON file), should we throw an error or print warning and continue?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469787368:81,error,error,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469787368,1,['error'],['error']
Availability,"If the version of the data sources is not up to date, Funcotator should give an error to that effect and halt.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5660:80,error,error,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5660,1,['error'],['error']
Availability,"If these events were indeed not CNLOH, as we discussed, then I don't think we should merge this. Perhaps we should take a step back and answer definitively whether simply blacklisting common germline regions is enough to replicate/obviate most of the postprocessing. Should be straightforward to run an evaluation with and without blacklisting---and hopefully our truth data accurately reflects whether blacklisting is desirable. If tagging/filtering rare germline is still a concern, then I'd say the next step is to see whether simply changing segmentation parameters to artificially decrease resolution and/or simple length-based filtering suffices. Finally, simple filtering based on CR-AF as described above could be implemented. If the normal is available, we can make IS_NORMAL calls simply based on the overlap of the ModelSegments posteriors (with corresponding qualities). If not, then some heuristic determination of the normal state from the tumor alone as in Marton's caller could be performed. This would combine the IS_NORMAL calling and filtering steps into one simple tool. The output could be a tagged/filtered ModelSegments .seg file and the corresponding VCF.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-458551250:752,avail,available,752,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-458551250,2,['avail'],['available']
Availability,"If we believe the reported phasing, there does appear to be a representation problem, but I've never encountered this issue. Likely the same issue exists in CombineGVCFs -- can you confirm, @purod?. The solution would be to look downstream before padding deletion alleles. This becomes a computationally intractable problem with large sample sizes. Rather than implementing more heuristics, the best solution might be a new implementation using a graph representation of variants over all samples for the region of interest, which isn't high priority right now. If you're interested in analyzing the calls from this sample in isolation, you may benefit from running vt-normalize (https://genome.sph.umich.edu/wiki/Vt#Normalization) to remove the erroneous allele padding. SelectVariants with the `-env` argument should work as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5944#issuecomment-493086884:229,down,downstream,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5944#issuecomment-493086884,1,['down'],['downstream']
Availability,"If we let HTSJDK fail, we'll need to wrap the error in a `UserException`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6665#issuecomment-647690180:46,error,error,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6665#issuecomment-647690180,1,['error'],['error']
Availability,"If you click the link for --read-index on this tooldocs page, it redirects to the tools index instead of jumping down to the explanation of the flag https://gatk.broadinstitute.org/hc/en-us/articles/360041416652-AnnotateIntervals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6600:113,down,down,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6600,1,['down'],['down']
Availability,"If you download the pon file from the GATK's buckets, change .vcf.vcf to .vcf.gz and run it again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8477#issuecomment-1710545730:7,down,download,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8477#issuecomment-1710545730,1,['down'],['download']
Availability,"If you get any more of these errors, it's either an argument that never had any effect or something that you 4.1.1 got rid of. In the latter case, you don't need to replace it with anything. In 4.1.1 `FilterMutectCalls` automatically learns a lot of parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5845#issuecomment-478011007:29,error,errors,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5845#issuecomment-478011007,2,['error'],['errors']
Availability,"If you have a barclay `@Argument` field of type `List`, barclay will fail to set the field value properly when the argument is specified if the `List` is initialized using an immutable Collection, such as that returned by `Collections.emptyList()`. Example error:. ```; java.lang.UnsupportedOperationException; 	at java.util.AbstractList.add(AbstractList.java:148); 	at java.util.AbstractList.add(AbstractList.java:108); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:706); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:220); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:194); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Ideally, barclay should detect immutable collections and replace them with mutable ones when necessary.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4702:257,error,error,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4702,1,['error'],['error']
Availability,"If you look at the approximation going from equation 34 to 35 in https://github.com/broadinstitute/gatk/blob/master/docs/mutect/mutect.pdf you will find that we replace f(1 - e) + (1 - f)e by just f(1 - e), where f is the allele fraction and e is the error rate. When f is much bigger than e this is okay but when they are comparable (consider mitochondrial or cfDNA calling with f = 1% and base qualities of 25) the approximation breaks down and we significantly underestimate the log odds, thereby failing to consider a region active. This must be fixed!. @meganshand",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4816:251,error,error,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4816,2,"['down', 'error']","['down', 'error']"
Availability,"If you run AlignAssembledContigsSpark with an incorrect BWA index version (ie. one that was generated with BWA index 0.7.12 or previous), you get executor logs that trace back to an ""IOException: File system is closed"" error, which is very misleading. I believe that this happens because Spark retries the tasks multiple times after they have failed, and in the subsequent tries the filesystem is in a bad state. It would be nice if we could either catch this error earlier, or check to make sure that the reference is compatible before trying to load it somehow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2123:219,error,error,219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123,2,['error'],['error']
Availability,"If you would like the GenomicsDB for chromosome `CM031199.1` (which, by the way, was created with GATK 4.2.4.1) that I used in the above two examples, for your own debugging purposes, you can download it as a .tar archive from:. [https://drive.google.com/file/d/1LzZCkWfmNb8IcZpdreaNIxtJ8GQQ-b7g/view?usp=sharing](https://drive.google.com/file/d/1LzZCkWfmNb8IcZpdreaNIxtJ8GQQ-b7g/view?usp=sharing). It is 385 Mb, and it has an SHA1 hash (from the Unix `shasum` utility) of `d330a28120713fb05c95d1bf54342944f5d741c9`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014196799:192,down,download,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014196799,1,['down'],['download']
Availability,"Implement a `SeqGraph` version of the junction trees described in Kiran's paper. For now we can do something naive about reads with errors corresponding to pruned edges, such as skipping the remainder of the read. In addition to involving a minimal change to the current code, using `SeqGraph`s will make handling read errors a bit simpler and is a much more natural way to handle dangling ends.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5923:132,error,errors,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5923,2,['error'],['errors']
Availability,Implement a convolutional network for the reference context merged with a dense network for annotations. The training data for the classifier consists of low-AF false positives from a normal and true hets with alt alleles downsampled to look like somatic variants. (This is in the db_m2_pon_mode branch).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3239:222,down,downsampled,222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3239,1,['down'],['downsampled']
Availability,Implement a test that catches failures to register new annotations in GATKVCFHeaderLines,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1713:30,failure,failures,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1713,1,['failure'],['failures']
Availability,"Implement https://github.com/broadinstitute/gatk/issues/2147. ```; Change ReadPosRankSumTest.isUsableRead to take deletions into account. Previously, reads were skipped because the variant location was considered downstream from the read. ; ```; The same fix was done to `AS_ReadPosRankSumTest`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2318:213,down,downstream,213,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2318,1,['down'],['downstream']
Availability,"Implements two new tools and updates some methods for a revamp of the `CombineBatches` cross-batch integration module in [gatk-sv](https://github.com/broadinstitute/gatk-sv). - `SVStratify` - tool for splitting out a VCF by variant class. Users pass in a configuration table (see tool documentation for an example) specifying one or more stratification groups classified by SVTYPE, SVLEN range, and reference context(s). The latter are specified as a set of interval lists using `--context-name` and `--context-intervals` arguments. All variants are matched with their respective group which is annotated in the `STRAT` INFO field. Optionally, the output can be split into multiple VCFs by group, which is a very useful functionality that currently can't be done efficiently with common commands/toolkits.; - `GroupedSVCluster` - a hybrid tool combining functionality from `SVStratify` with `SVCluster` to perform intra-stratum clustering. This tool is critical for fine-tuned clustering of specific variants types within certain reference contexts. For example, small variants in simple repeats tend to have lower breakpoint accuracy and are typically ""reclustered"" during call set refinement with looser clustering criteria.; - `SVStratificationEngine` - new class for performing stratification.; - Updates to breakpoint refinement in `CanonicalSVCollapser` that should improve breakpoint accuracy, particularly in larger call sets. Raw evidence support and variant quality are now considered when choosing a representative breakpoint for a group of clustered SVs.; - Added `FlagFieldLogic` type for customizing how `BOTHSIDE_PASS` and `HIGH_SR_BACKGROUND` INFO flags are collapsed during clustering.; - `RD_CN` is now used as a backup if `CN` is not available when determining carrier status for sample overlap.; - Removed no-sort option in favor of spooled sorting.; - Bug fix: support for empty EVIDENCE info fields; - Bug fix: in one of the JointGermlineCnvDefragmenter tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8990:1753,avail,available,1753,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8990,1,['avail'],['available']
Availability,Improve Hadoop-bam error message in createRecordReader,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1452:19,error,error,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1452,1,['error'],['error']
Availability,Improve error message for no-access and disabled-account cases,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2417:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2417,1,['error'],['error']
Availability,Improve error message in GATKRead.setMatePosition,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6779:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6779,1,['error'],['error']
Availability,Improve error message in GenomicsDBImport,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7375:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7375,1,['error'],['error']
Availability,Improve error message in spark tools when trying to access a local file from other nodes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1417:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1417,1,['error'],['error']
Availability,Improve error message when Genotype PL and PP do not have 3 items when calculatePosteriorGLs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3320:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3320,1,['error'],['error']
Availability,Improve failure message in VariantContextTestUtils,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8725:8,failure,failure,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8725,1,['failure'],['failure']
Availability,Improve import error message [VS-437],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7855:15,error,error,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7855,1,['error'],['error']
Availability,Improve indel calls on repeats (review the current PCR error model).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2519:55,error,error,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2519,1,['error'],['error']
Availability,"Improved error message sounds like a good solution, thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5064#issuecomment-439983479:9,error,error,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5064#issuecomment-439983479,1,['error'],['error']
Availability,"In @meganshand's work on mitochondria we found that adjusting `-min-pruning` to be commensurate with the expected error rate per locus (eg minPruning = depth / 500 or something like that) rescued some false negatives and fixed the issue of interval- and padding-dependent calls. In her case, the sheer complexity of the assembly graph caused two undesirable things: 1) the true variant was missing from the best haplotypes and 2) the evidence for the true variant was diluted beyond recognition among many spurious haplotypes. This is not always the answer but for high depths or high error rates it is a strong possibility. I would be curious to see what turning on kmer error correction does, and in the long run I hope that #4868 will help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-399726606:114,error,error,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-399726606,3,['error'],['error']
Availability,"In FindBreakpointEvidenceSpark, KmerCleaner is ugly and seems redundant",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1889:62,redundant,redundant,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1889,1,['redundant'],['redundant']
Availability,"In GATK Office hours we found a change that contributed to this error message. The issue may be a bug or an issue with the data that is showing up with the more strict filters in the latest version. This request was created from a contribution made by Igor Islanov on July 06, 2020 12:11 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360071204731-FilterVariantTranches-brakes-on-new-version-of-Gatk](https://gatk.broadinstitute.org/hc/en-us/community/posts/360071204731-FilterVariantTranches-brakes-on-new-version-of-Gatk). \--. Good day,. While updating gatk from 4.1.4.0 to 4.1.8.0 and after running pipeline it brakes on FilterVariantTranches step with error:. htsjdk.tribble.TribbleException: The provided reference alleles do not appear to represent the same position, C\* vs. T\*. The command line  is  ; ; gatk FilterVariantTranches -I ${R1%%\_\*}-recal.bam -V ${R1%%\_\*}-annotated.vcf -R /mnt/d/GenLab/WES/reference/hg19.fasta --create-output-variant-index true --resource /mnt/d/GenLab/WES/db/00-All.vcf.gz --resource /mnt/d/GenLab/WES/db/00-common\_all.vcf.gz --resource /mnt/d/GenLab/WES/reference/1000G\_phase1.indels.hg19.sites.vcf --resource /mnt/d/GenLab/WES/reference/Mills\_and\_1000G\_gold\_standard.indels.hg19.sites.vcf --snp-tranche 99.9 --snp-tranche 99.95 --indel-tranche 99.0 --indel-tranche 99.4 -O ${R1%%\_\*}-filtered.vcf --tmp-dir /mnt/d/GenLab/WES/output/tmp --java-options ""-Xmx24G"". On 4.1.4.0 no problems whatsoever, on 4.1.8.0 not working at all. Double-confirmed by 2 seperate conda envs. The reference file is unchanged during whole running processes, obviously. Full error log: ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx24G -jar /mnt/d/GenLab/WES/software/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar FilterVariantTranches -I D1394-recal.bam -V D1394-annotated.vcf -R /mnt/d/GenLab/WES/refe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6701:64,error,error,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6701,2,['error'],['error']
Availability,"In GATK3, the VQSR tools have specific expectations for novel Ti/Tv that are based on what was known at the time before the 1000 Genomes project results were added to dbsnp. If you use the corresponding ""old"" dbsnp version, the expectations are fulfilled and your QC plots come out looking shiny. If you use a more recent version, key assumptions break down and it screws up the VQSR's QC routines (which produce the plots). Would be good to be able to handle whatever version of dbsnp users use. . This Issue was generated from your [forums](http://gatkforums.broadinstitute.org/discussion/comment/20493#Comment_20493)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/385:353,down,down,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/385,1,['down'],['down']
Availability,"In GATK4 (specifically 4.0.1.2), you can no longer give SelectVariants a file of variant IDs to keep. There isn't any error message, but the output VCF is empty. (It works in GATK3)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4460:118,error,error,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4460,1,['error'],['error']
Availability,"In IntervalUtils, when Picard intervals are parsed and checked for validity, (line 359 `glParser.isValidGenomeLoc(interval.getContig(), interval.getStart(), interval.getEnd(), true)`), if the contig doesn't match the supplied reference (via -R) then the error produced is `has an invalid interval`. The interval is perfectly valid, especially since the Picard interval_list has a corresponding sequence dictionary. I'm not sure if the preferred behavior here is to validate against the interval_list seqdict and then note that the -R reference doesn't match or to error because the -R ref doesn't match. Maybe if the tool requiresReference() and the -R doesn't match throw an error?. I encountered this in the context of a tool similar to SplitIntervals, which requires a reference even if a Picard interval_list is provided. I see that this is a TODO in GATKTool::getBestAvailableSequenceDictionary.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5410:254,error,error,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5410,3,['error'],['error']
Availability,"In a joint calling run with 11,000 samples, and broken up into over 10,000 scatters, a single one failed with a NPE. I was able to get around it for now by just ignoring that scatter for the output, but that's really not an ideal thing to do for joint calling (and we cannot do that for the CCDG callset). I can't give you the inputs because it was running on so many samples (and via GenomicsDB), but hopefully the stacktrace will help here:. java.lang.NullPointerException; at org.broadinstitute.hellbender.tools.walkers.genotyper.AlleleSubsettingUtils.calculateLikelihoodSums(AlleleSubsettingUtils.java:234); at org.broadinstitute.hellbender.tools.walkers.genotyper.AlleleSubsettingUtils.calculateMostLikelyAlleles(AlleleSubsettingUtils.java:199); at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:241); at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:205); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.calculateGenotypes(GenotypeGVCFs.java:276); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.regenotypeVC(GenotypeGVCFs.java:234); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:213). I'm not sure who now owns this code, so will ping @davidbenjamin, @ldgauthier, @droazen.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3210:1334,ping,ping,1334,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3210,1,['ping'],['ping']
Availability,In case of shut down while updating gvcf file to GenomicDB,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7324:16,down,down,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324,1,['down'],['down']
Availability,"In doing some performance evaluation work for some other HaplotypeCaller work I have noticed that there is apparently a performance regression on the order of perhaps 10-20% of runtime. Running locally I find that running over the same section of a WGS chromosome 15 on the current master 78a9ecd3123fdb77acf3dd7a73b0c12bf4602a1c vs the release 4.1.5.0 i get the following results: . Master: ; real	12m19.765s; user	13m49.276s; sys	0m8.571s; 4.1.5.0: ; real	9m50.558s; user	11m11.924s; sys	0m10.193s. Doing some very cursory digging it would appear that the culprit is in the HMM adjacent code being slowed down. (Note the relative runtime of HMM vs SW) ; Master: ; <img width=""822"" alt=""Screen Shot 2020-04-23 at 1 28 52 PM"" src=""https://user-images.githubusercontent.com/16102845/80130392-80115780-8566-11ea-8f2b-a6978ac71d39.png"">. 4.1.5.0: ; <img width=""850"" alt=""Screen Shot 2020-04-23 at 1 28 33 PM"" src=""https://user-images.githubusercontent.com/16102845/80130396-80115780-8566-11ea-9a1e-1e923bef47a5.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6567:607,down,down,607,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6567,1,['down'],['down']
Availability,In doing some trials with the ReadErrorCorrector I have noticed that it apparently seems to have no impact on my results. Upon closer inspection I have found the offending lines. In `ReadErrorCorrector.correctRead(final GATKRead inputRead)` we compute the `correctedQuals` and `correctedBases` arrays and then neglect to add them to the copied read later. This needs to be fixed. ; Other issues with the class:; - It apparently slows the HaplotypeCaller down 10x; - There is no tiebreaking for hamming distance comparisons; - ^those comparisons are done non-deterministically,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6365:454,down,down,454,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6365,1,['down'],['down']
Availability,"In fact, setting the deploy-mode works with manual jobs as we get logs in our Hadoop monitor ( the tool to monitor the jobs on the spark cluster ) and directly on our console if deploy-mode is not set / set to client. Both `--deploy-mode` and `--conf 'spark.submit.deployMode=cluster'`. But with GATK, logs appear directly on my console and not in the Hadoop monitor even if we set with `--conf 'spark.submit.deployMode=cluster`. The other methods `--deploy-mode` and `-- --deploy-mode` having the said problems.; About the `-- --deploy-mode` and the JNI linkage error, I'm currently checking this.; All our Spark nodes have access to the mapr libraries from `/opt/mapr/...`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350676916:563,error,error,563,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350676916,1,['error'],['error']
Availability,"In general our germline tools are designed for short variants. I don't think any of them will handle a millions long indel well or at all. The SV or CNV tools sound like a better fit although I'm not sure exactly if they cover your use case exactly. Typically we process short variants and long variants like this separately. . We should be detecting this variant up front on when loading into genomicsDB if it's going to be problematic to retrieve it, and we should be giving a better error message. I don't think we'll be able to handle it through GenotypeGVCFs in any helpful way though. (The best I can imagine it doing is passing it through ungenotyped.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1376029770:486,error,error,486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1376029770,1,['error'],['error']
Availability,"In helping @bhanugandham figure out why a particular site was failing it became apparent that merging dangling head code was failing to recover deletions in the dangling head. Furthermore there is some code in the dangling end recovery code that asserts a certain high standard of matching (usually 1 but sometimes dangling branch length/kmersize) `getMaxMismatches(final int lengthOfDanglingBranch)`. Both of these facts seem likely to cause dangling heads to be dropped despite their being still potentially informative, particularly the indel code. . I have added the ability for the index recovery code to account for the cigar string when merging dangling ends. Addtionally rather than counting mismatches to reject the branch it simply requires a minimum matching end (which can be changed, I suspect this is where the lionshare of the differences come from). Unfortunately changing the tests is non-trivial (as this happened to change the integration test results for HaplotypeCaller at a few sites) so I wanted to get this branch up to solicit advice a to whether it is worth pursuing this fix. @davidbenjamin @ldgauthier @droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6113:136,recover,recover,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6113,3,['recover'],"['recover', 'recovery']"
Availability,"In high-depth calling (eg @meganshand's work with mitochondria) it is necessary to tweak the `min-pruning` argument. If it is too low, base errors render the assembly graph nearly dense, causing a loss of sensitivity when the assembly engine essentially chooses random haplotypes. If it is too high, we also lose sensitivity because true variants are pruned. Setting the command line argument differently for each sample is not only cumbersome. It also doesn't solve the problem because depth varies within the same bam. Thus, pruning must adapt to each assembly region.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4867:140,error,errors,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4867,1,['error'],['errors']
Availability,"In keeping concordance with gatk3, when computing the RankSumAnnotation for pileups when the likelihoods map is not available we attempt to match the bases of the reads to the allele in the variant context. This is problematic if the variant is not a snp. For most other annotations we explicitly only compute the annotation for SNPs in this case, but in order to mimic gatk3 behavior it will still attempt to compute the rank sum over indels. This should be evaluated and changed. . Related to #4450, #3803",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4452:116,avail,available,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4452,1,['avail'],['available']
Availability,"In light of the discovery of the (relatively minor) numerical differences caused by changes to non-CNV code outlined in #7649, and because we are still awaiting coverage from pipeline-level/CARROT testing, I decided to go ahead and add these exact-match tests. This essentially freezes current ModelSegments behavior, which has been exactly stable since https://github.com/broadinstitute/gatk/pull/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7652:641,down,downsampled,641,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652,1,['down'],['downsampled']
Availability,"In looking at Mutect2 for clinical applications, one thing that always seems to come up has to do with the big difference between the ref/alt coverage denoted in the VCF file and what is seen in IGV. For clinical reporting, many labs will provide mutant allele depths, along with the VAF estimate. I understand the purpose of downsampling at stages of the m2 workflow, and I also understand this negatively affects amplicon-based studies. How viable is it to provide more exact (include reads that are high quality but not used during variant determination) estimates of coverage at variant loci, while not substantially increasing runtime? It would be great to get some of our analysts away from always feeling as if they need to visualize calls in IGV... Thanks,; John",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3808:326,down,downsampling,326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3808,1,['down'],['downsampling']
Availability,"In looking at his further, the container header contains a stream offset, and each slice header also contains a global record counter. Both of these need to be updated. Its not clear if its possible to repair these without re-encoding the entire container stream, but if so that should probably be done in a method exposed by htsjdk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574:202,repair,repair,202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2201#issuecomment-324756574,2,['repair'],['repair']
Availability,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:447,down,download,447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495,1,['down'],['download']
Availability,"In our Mutect2 workflow, we run a pair of Normal/Tumor through `CalculateContamination` step, the output of which is used in `FilterMutectCalls`. Since upgrading to `4.1.0.0`, `CalculateContamination` is breaking in cases where there're mismatched of N/T samples. . For e.g., `4.0.11.0` generates the following output:; ```; level contamination error; whole_bam 0.5013841326835697 0.0055644124674135865; ```; And `4.1.0.0` gives the following:; ```; sample contamination error; Run06_Pair07_Tumor 1.0 0.03452380752462225; ```. As a result of the above output files, the next step in our pipeline `FilterMutectCall` is failing (issue related to https://github.com/broadinstitute/gatk/issues/5821)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5880:345,error,error,345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5880,2,['error'],['error']
Availability,"In particular, the new default `--mapping-error-rate` for DetermineGermlineContigPloidy looks quite large (it was changed from 0.01 to 0.3)! Would be great to record some info on that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8628#issuecomment-1856582943:42,error,error-rate,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628#issuecomment-1856582943,1,['error'],['error-rate']
Availability,"In preparing the VCF that generated the error, I had applied a hard filter on the excessHets variants before running the VQSR. If the excessHets are not filtered out, the VQSR runs fine. I traced the error to having AS_MQ in the model without the excessHet variants in the vcf. It would be nice to have a better error message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7380#issuecomment-905132194:40,error,error,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380#issuecomment-905132194,3,['error'],['error']
Availability,"In running and re-running GvsPrepareCallset.wdl, one past run did not use compressed references, so that is always used with call caching is turned on (which it is by default), even though the dataset has reingested compressed references since then. This is the exact scenario that GetBQTableLastModifiedDatetime was created for — database-based tasks that we want to be able to call cache accurately. Integration run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ea2ecb01-f35f-441a-ba08-1e7938da2ebe (single failure is for ExtractFilterTask.GvsCreateFilterSet.BigQuery Query Scanned ""The relative difference between these is 0.0507051, which is greater than the allowed tolerance (0.05)"")",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8667:541,failure,failure,541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8667,2,"['failure', 'toler']","['failure', 'tolerance']"
Availability,"In scripts/gatkcondaenv.yml.template, the following line produces an error upon the initial attempt to create the conda environment:. `anaconda::tensorflow=1.12.0=mkl_py36h69b6ba0_0`. It seems that the up to date dependency should be:. `anaconda::tensorflow=1.12.0=mkl_py36h2b2bbaf_0`. To replicate, perform a clean clone, with no conda environment created yet, and then:. `$ ./gradlew localDevCondaEnv`. This will fail with:. `ResolvePackageNotFound: - anaconda::tensorflow==1.12.0=mkl_py36h69b6ba0_0 `",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6369:69,error,error,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6369,1,['error'],['error']
Availability,"In some preliminary testing I've done it looks like using native Hadoop libraries can speed up tools running in Spark local mode. In private Spark tools under development (which travers a WGS BAM and then performing several shuffles) I have seen speedups of up to 40% (~ 46 minutes -> 26 minutes). An initial test of `MarkDuplicatesSpark` using a 30GB bam file gave me a 9% speedup (logs are below). It might be good to investigate making this easier for users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746:464,down,downloaded,464,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746,1,['down'],['downloaded']
Availability,"In the README, the full download size when downloading large files using Git LFS is reported to be 2 GB in one section and several hundred MB in another section, when it is actually ~4.81 GB in size. ![git_lfs](https://user-images.githubusercontent.com/52426291/97712898-5d38a080-1abf-11eb-8070-d71d63d5add1.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6932:24,down,download,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6932,2,['down'],"['download', 'downloading']"
Availability,"In the VAT validation, give clearer error msg about which clinvar classification values are missing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7939:36,error,error,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7939,1,['error'],['error']
Availability,"In the WGS SV pipeline, for deletions and duplications that the pipeline believes to be biallelic we do the following:. - ALT: `<DEL>` or `<DUP>`; - SVTYPE: `DEL` or `DUP`; - GT: `0/0` or `0/1` or `1/1`. We currently report depth based copy number and quality for these variants in custom format fields `RD_CN` and `RD_GQ` if they are available; we could possibly move those values to the standard `CN` and `CNQ`, but there is some complexity in how to handle events detected by paired end and split reads without good read depth support; ie those under 1kb or so depending on our depth binning size and the coverage. Our depth genotyping module makes estimates of copy number for these sites but sometimes these can be very inaccurate so at the moment we prefer not to report total copy number in those fields. Probably what we _should_ do is fill in CN with 0, 1, or 2 based on the genotype we emitted and set CNQ to the value we computed for GQ. For multiallelic CNVs (i.e. sites where our model is not sure that the variant is bi-allelic) we write:. - ALT: `<CNV>`; - SVTYPE: `CNV`; - GT and GQ: `.`; - CN and CNQ: estimate of total (diploid/unphased) copy number and quality of the depth evidence. I think there are some tradeoffs in completely characterizing the evidence for and quality of each call and enabling easy searching across the whole VCF without having to parse and understand the entire record. Older versions of our pipeline used to put the diploid copy number of the event into the GT field, I think similarly to what's being described above. This is incorrect VCF -- GT values should be indices into the allele list for the variant, and should be a list of length equal to the ploidy. . My view is that if you can confidently infer the alleles present at the site in the sample set you should use a GT value of the form `0/1`, and if you don't know or aren't interested in trying to infer them you should use CN for total copy number and CNQ for the quality. CNF is also availabl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6167#issuecomment-622053171:335,avail,available,335,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6167#issuecomment-622053171,1,['avail'],['available']
Availability,"In the branch `dr_intel_deflater_bug_repro`, running `./gradlew clean test -Dtest.single=GatherVcfsIntegrationTest` will trigger the following test failure:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather[14](/Users/droazen/src/hellbender/src/test/resources/large/gvcfs/combined.gatk3.7_30_ga4f720357.expected.vcf, 8536) FAILED; java.lang.AssertionError: different sizes 16940 vs 17070; at org.broadinstitute.hellbender.utils.test.VariantContextTestUtils.assertEqualVariants(VariantContextTestUtils.java:173); at org.broadinstitute.hellbender.tools.GatherVcfsIntegrationTest.testBlockGather(GatherVcfsIntegrationTest.java:103); Results: FAILURE (15 tests, 14 successes, 1 failures, 0 skipped); ```. The tool writes a vcf that, when read back in by GATK, appears to have fewer records than it should. The same test does NOT fail if you do ANY of the following:. * Edit `GatherVcfsIntegrationTest.testBlockGather()` to turn on the JDK deflater by changing `.addBooleanArgument(""use_jdk_deflater"", false);` to `.addBooleanArgument(""use_jdk_deflater"", true);`. * Keep the Intel deflater on, but edit `build.gradle` to change `samjdk.compression_level` to 1 or 2. (You'll also need to change the `Assert.assertEquals(System.getProperty(""samjdk.compression_level""), ""5"");` line in `GatherVcfsIntegrationTest.testBlockGather()` accordingly). * Edit the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest` to change the failing `{LARGE_VCF, 8536}` test case to `{LARGE_VCF, 8535}`. This cuts the number of files that the vcf gets split into in half, and the test passes. * Comment out all but the last test case in the `getVcfsToShard` `DataProvider` in `GatherVcfsIntegrationTest`. This indicates that there is something stateful going on, since the test case does not fail if run in isolation. One additional bit of information: the test fails with the Intel deflater and compression levels 5 and 9, but with compression level 9 GA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3117:148,failure,failure,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3117,3,"['FAILURE', 'failure']","['FAILURE', 'failure', 'failures']"
Availability,"In the course of tinkering with the GATK tutorial, I discovered that FilterVariantTranches errors out if it doesn't find any indels. The new logic isn't bulletproof (e.g. it will still run if it has SNP input and indel training), but it solves the SNP-only (and indel-only) case.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6411:91,error,errors,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6411,1,['error'],['errors']
Availability,In the error handling for `catch ( final FuncotatorUtils.TranscriptCodingSequenceException ex )` in `GencodeFuncotationFactory` we should add an `otherTranscript` to the output annotations with the transcript ID and an ERROR statement if an error occurred during processing. This will make the user more likely to see it if there was a problem creating funcotations for a particular transcript.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4861:7,error,error,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4861,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"In the interest of time, here's a link to download a TAR. it's 101GB, containing the single-contig workspace, two core dumps, and a sample command. . https://prime-seq.ohsu.edu/_webdav/Labs/Bimber/Collaborations/Public/%40files/genomicsDB.tar.gz. Please let me know when/if you get this so I can remove it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-718248354:42,down,download,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-718248354,1,['down'],['download']
Availability,"In the latest master, running for example `java -jar build/libs/gatk.jar FixVcfHead` returns:. ```; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Base Calling: Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters; CheckIlluminaDirectory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run. ...skipped for brevity... VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. Exception in thread ""main"" org.broadinstitute.hellbender.exceptions.UserException: 'FixVcfHead' is not a valid command.; Did you mean this?; FixVcfHeader; 	at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:341); 	at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:172); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:192); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); ```. I expect something without the stack trace and the scary ""Exception"" message. For example:. ```; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Base Calling: Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters; CheckIlluminaDirectory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run. ...skipped for brevity... VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4256:128,Avail,Available,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4256,1,['Avail'],['Available']
Availability,In the other issue I couldn't get any output from GenomicsDB because there was a deletion issue that would cause an error. The PID tag tells us that all those variants are on the same haplotype because they're 0|1. If one of the variants were on the other haplotype it would be 1|0.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5944#issuecomment-493109586:116,error,error,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5944#issuecomment-493109586,1,['error'],['error']
Availability,"In this branch are a number of improvements and changes that form the baseline for the current ongoing evaluation of the DRAGEN/GATK pipeline. This represents the joint work of both msyelf and @vruano. The major improvements in this branch are as follows:; - `EstimateDragstrModelParameters` tool for estimating the per-sample/per-STRType errors for use in the HMM gap open/gap close penalties as well as the necessary changes to the PairHMM loading code in order to adjust the model appropriately.; - Support for using the DragstrParams and flat SNP priors to compute genotype posteriors and the support for using them in the selection of genotypes as well as for computing the QUAL score. ; - Base Quality Dropout (BQD) model which penalizes variants with low average base quality scores among genotyped reads and reads that were otherwise excluded from the genotyper. A number of additional arguments to expose internal behaviors in the readThreadingAssembler and HaplotypeCaller have been made in order to support threading more lowBQ reads through to the genotyper. ; - Foreign Read Detection (FRD) model which uses an adjusted mapping quality score as well as read strandedness information to penalize reads that are likely to have originated from somewhere else on the genome. A number of additional arguments and behaviors have been exposed in order to preserve lower mapping quality reads in the HaplotypeCaller in service.; - Dynamic Read Disqualification, allows for longer/lower base quality reads to be less likely to be rejected by eliminating the hard cap on quality scores and further adjusting the limit based on the average base quality for bases in the read. . Design decisions that I would direct the reviewers attention to as they correspond to potentially dangerous/controversial changes:; - Because FRD/BQD require low quality ends to be included in the models for genotyping, I have added the option to softclipLowQualityEnds (as opposed to their current treatment which involv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6634:339,error,errors,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6634,1,['error'],['errors']
Availability,"In this case, allele-specific annotations are outputted to GVCFs files by 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command. Although some warning message of 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command.; ```; WARN:StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; ```; This warning message does not happen in all of my cases, but the FORMAT/AD error happen in all of my cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4691#issuecomment-383763740:433,error,error,433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4691#issuecomment-383763740,1,['error'],['error']
Availability,"Included a test case generated using the buggy version of GenomicsDBImport in which the sample names declared in the file headers did not always match the samples provided to GenomicsDBImport via the sample name map file, and showed that the tool could repair the callset successfully.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3868:253,repair,repair,253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3868,1,['repair'],['repair']
Availability,"Includes latest Gencode and an implicit fix for #6564. Had to make some code changes for latest liftover Gencode data(v34 -> hg19). . The associated DS test release correctly annotates data on hg19 and hg38. Left to do:. - [x] Update data sources downloader.; - [x] Update data source version validation code. Code updates:; - Now both hg19 and hg38 have the contig names translated to `chr__`; - Added 'lncRNA' to GeneTranscriptType.; - Added ""TAGENE"" gene tag.; - Added the MANE_SELECT tag to FeatureTag.; - Added the STOP_CODON_READTHROUGH tag to FeatureTag.; - Updated the GTF versions that are parseable.; - Fixed a parsing error with new versions of gencode and the remap; positions (for liftover files).; - Added test for indexing new lifted over gencode GTF.; - Added Gencode_34 entries to MAF output map.; - Minor changes to FuncotatorIntegrationTest.java for code syntax.; - Pointed data source downloader at new data sources URL.; - Minor updates to workflows to point at new data sources. Script updates:; - Updated retrieval scripts for dbSNP and Gencode.; - Added required field to gencode config file generation.; - Now gencode retrieval script enforces double hash comments at; top of gencode GTF files. Bug Fixes:; Removing erroneous trailing tab in MAF file output. - Fixes #6693",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6660:247,down,downloader,247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6660,3,"['down', 'error']","['downloader', 'error']"
Availability,"Including the schema in a header in text format is broken, and makes sharding error-prone and inefficient. I'd suggest defining schema using [Apache Avro](https://avro.apache.org/), and storing the data in [Apache Parquet](https://parquet.apache.org/) format on disk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480975105:78,error,error-prone,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480975105,1,['error'],['error-prone']
Availability,"Inconsistencies applies to GATK 4.beta.6 (observed by forum user) and 3.7 (recapitulated by @sooheelee). The inconsistency is namely in the form of BQ-clipping, e.g. from `AAFFFKKKKKKKKKKKKKKKKKKK` to `AAF55!!5!!!5!!!55!!5!!!!` for read and a supplementary read of the mate that is highlighted in red:. <img width=""1281"" alt=""screenshot 2017-11-14 14 00 00"" src=""https://user-images.githubusercontent.com/11543866/32800037-56ea32ba-c947-11e7-9d4d-b33a3ac5030a.png"">. This particular read pair does not contribute to variant calling. However, I assume there could be scenarios in which such BQ-clipped reads end up counting towards variant calling. - original forum user report for v4.beta.6: https://gatkforums.broadinstitute.org/gatk/discussion/comment/43851#Comment_43851; - original user data bundle: /humgen/gsa-scr1/pub/incoming/BasequalityBug.tar.gz; - Comms recapitulation and observations including for v3.7: https://github.com/broadinstitute/dsde-docs/issues/2661. I also request that GATK4 HaplotypeCaller port the `--emitDroppedReads` option that is available in v3 but not so far in v4. This option allows users to understand why a read may be dropped from HaplotypeCaller consideration.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3834:1061,avail,available,1061,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3834,1,['avail'],['available']
Availability,Incorrect error message when trying to read corrupt bam file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2551:10,error,error,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2551,1,['error'],['error']
Availability,Indeed adding ```--createOutputVariantIndex false``` there is no exception any longer.; Probrably the actual base cause is that the user does not have permission to create a file under /dev. Ideally instead of ``` Unable to close index for /dev/null``` the error message should be something like ``` Unable to create index file /dev/null.idx for the output; not enough permissions```. ... and make sure we fail early specially considering large VCF inputs.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3829#issuecomment-345040375:257,error,error,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3829#issuecomment-345040375,1,['error'],['error']
Availability,IndelRealignement error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6798:18,error,error,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6798,1,['error'],['error']
Availability,IndexFeatureFile Error to Run Funcotator with Mouse Ensembl GTF,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7054:17,Error,Error,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7054,1,['Error'],['Error']
Availability,IndexFeatureFile: more informative error message when trying to index a malformed file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4187:35,error,error,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4187,1,['error'],['error']
Availability,Ingest Error Handling Fixes [VS-261],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7787:7,Error,Error,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7787,2,['Error'],['Error']
Availability,Initial check-in to find test failures. . Adresses #8328,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8689:30,failure,failures,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8689,1,['failure'],['failures']
Availability,"Initial signed url support exists as of https://github.com/broadinstitute/gatk/pull/6526 but there are some caveats that may prevent production use. * Some methods in the Path api weren't implemented yet, the noticeable exception is the method used when finding sister paths such as the .bai. This means that indexes must be manually specified or you'll hit a UnimplementedFeatureException. * There isn't retry logic so http failures will cause job failure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6317#issuecomment-801427075:425,failure,failures,425,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6317#issuecomment-801427075,2,['failure'],"['failure', 'failures']"
Availability,"InputStream.readNonProxyDesc(ObjectInputStream.java:1826); 	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000); 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); 	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); 	at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); 	... 20 more; 17/11/15 19:43:35 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@5917b44d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 17/11/15 19:43:35 WARN org.apache.spark.ExecutorAllocationManager: No stages are running, but numRunningTasks != 0; 19:43:35.858 INFO PrintVariantsSpark - Shutting down engine; [November 15, 2017 7:43:35 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVariantsSpark done. Elapsed time: 0.43 minutes.; Runtime.totalMemory()=823132160; org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; Serialization trace:; genotypes (org.seqdoop.hadoop_bam.VariantContextWithHeader); interval (org.broadinstitute.hellbender.engine.spark.SparkSharder$PartitionLocatable); 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAG",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840:8207,failure,failure,8207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840,1,['failure'],['failure']
Availability,Instructions on how to download BQ Metadata and visualize results,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7359:23,down,download,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7359,1,['down'],['download']
Availability,Integer overflow error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6302:17,error,error,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6302,1,['error'],['error']
Availability,Integration run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/76c46310-3c0d-43a8-9fce-072ef7750651). As written the task requires `apt-get`. Converting this to Alpine would be non-trivial and not really worthwhile as it might even take longer to build all the extra things into the `alpine` image that we simply download with the `slim` image.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8065:327,down,download,327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8065,1,['down'],['download']
Availability,"Integration run here: https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%203%20samples/job_history/acb5b878-af45-443d-8139-0f0044cbcb38. The basic problem: https://news.ycombinator.com/item?id=9255830. Repro:. ```; % # make a file shaped like what was failing in ingest; % for i in $(seq 50000); do ; echo foo,${i} >> file.csv; done; % # repro the pipeline that was failing; % set -o pipefail; % cat file.csv | cut -d, -f2 | sort -r -n | head -1; 50000; % echo $?; 141; % # repeat with temp file construct; % head -1 <(cat file.csv | cut -d, -f2 | sort -r -n) ; 50000; % echo $?; 0; %; ```; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8441:315,echo,echo,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8441,3,['echo'],['echo']
Availability,Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/80b6ed98-3b79-40f3-b55b-524b6f63f1e1). Failed one of the four (in Hail VDS tieout) - with a weird python3 error. 99.9994% sure not related to this PR.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8393#issuecomment-1613622838:215,error,error,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8393#issuecomment-1613622838,1,['error'],['error']
Availability,IntegrationTestSpec is hardwired to text files and bam files but compares them byte-by-byte. We need more digested way of comparing files to remove the brittleness of md5 while retaining the ability to notice failures.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/190:209,failure,failures,209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/190,1,['failure'],['failures']
Availability,"IntelliJ seems to be having a bit of difficulting loading GATK these days:; > build.gradle error (413,0); > Received fatal alert: protocol_version",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2642#issuecomment-298437516:91,error,error,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2642#issuecomment-298437516,1,['error'],['error']
Availability,Interesting to see whether it recovers real vairants.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7321:30,recover,recovers,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7321,1,['recover'],['recovers']
Availability,"Interesting, it's definitely possible it's coming from one of the other buckets. I don't think we have fine grained control over WHICH bucket we attempt to read requester pays status from, so it's possible if it's enabled it's necessary to have that permission on every bucket. It's annoying that the error message doesn't say which reader is performing the access. Is there a longer stack trace available?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934908586:301,error,error,301,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934908586,2,"['avail', 'error']","['available', 'error']"
Availability,"Interesting. Sorry this is causing so much trouble. From one of your above comments I wasn't clear if the solution using `--conf 'spark.submit.deployMode=cluster'` work correctly or not. . Is it possible that it's correct behavior for it to fail with the linkage error? According to the [mapr doc](https://maprdocs.mapr.com/52/DevelopmentGuide/c-loading-mapr-native-library.html) that command causes it to expect the application to load the library itself, but GATK by default doesn't have a copy of MAPR and won't load it on it's own. Have you included the mapr library somehow into the gatk jar? Or is it provided to spark some other way? I don't really know how maprfs works and how it interacts with hadoop paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350315653:263,error,error,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350315653,2,['error'],['error']
Availability,"Interesting. The CRAI file itself (when you gunzip it) appears normal although I don't have an md5 from GP for the CRAI (just for the CRAM). I suppose I can't rule out a bitflip somewhere. I don't know which software GP uses to produce CRAI files and since they lack a header, I'm not sure I'll be able to find out. Having reindexed it with `samtools index` (v1.7, htslib 1.7-2), and then diffing my gunzipped crai vs the original crai, I think I see the offending line:. ```; # Original .crai:; -1	0	-2147483647	710543306	480	278484. # samtools-1.7 .crai:; -1	0	1	710543306	480	278484; ```. I'll ping our Broad GP contact to see if they can identify an issue in their pipeline.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7755#issuecomment-1088788251:597,ping,ping,597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7755#issuecomment-1088788251,1,['ping'],['ping']
Availability,Intermittent errors when running on distributed Spark cluster,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1491:13,error,errors,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491,1,['error'],['errors']
Availability,"Intermittent failure at https://travis-ci.com/github/broadinstitute/gatk/jobs/297047618. ```; [TileDB::FileSystem] Error: hdfs: Cannot list contents of dir gs://hellbender-test-logs/staging/703469fc-52fe-441d-b6e0-8092a114fe2c//chr20$17960187$17981445/genomicsdb_meta_dir; hdfsBuilderConnect(forceNewInstance=0, nn=gs://hellbender-test-logs, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:; java.io.IOException: Must supply a value for configuration setting: fs.gs.project.id; 	at com.google.cloud.hadoop.util.ConfigurationUtil.getMandatoryConfig(ConfigurationUtil.java:39); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createOptionsBuilderFromConfig(GoogleHadoopFileSystemBase.java:2185); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1832); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1013); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:976); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2812); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:171); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:168); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:168); 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6522:13,failure,failure,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6522,3,"['Error', 'error', 'failure']","['Error', 'error', 'failure']"
Availability,Intermittent failure of AsynchronousStreamWriterServiceUnitTest.testAsyncWriteInBatches on Travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4024:13,failure,failure,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4024,1,['failure'],['failure']
Availability,Intermittent failure of GenomicsDBImportIntegrationTest.testWriteToAndQueryFromGCS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6522:13,failure,failure,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6522,1,['failure'],['failure']
Availability,"IntervalArgumentCollection - Processing 83257441 bp from intervals; 14:52:21.917 INFO PrintReads - Done initializing engine; 14:52:22.027 INFO ProgressMeter - Starting traversal; 14:52:22.027 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 14:52:32.033 INFO ProgressMeter - chr17:6779805 0.2 494000 2962814.9; 14:52:42.035 INFO ProgressMeter - chr17:18100301 0.3 1275000 3823661.7; 14:52:52.089 INFO ProgressMeter - chr17:32183301 0.5 2017000 4025814.2; 14:53:02.141 INFO ProgressMeter - chr17:38342966 0.7 2500000 3739436.1; 14:53:12.267 INFO ProgressMeter - chr17:46549838 0.8 3360000 4012818.7; 14:53:22.273 INFO ProgressMeter - chr17:63099258 1.0 4210000 4192879.1; 14:53:30.687 INFO PrintReads - No reads filtered by: WellformedReadFilter; 14:53:30.687 INFO ProgressMeter - chr17:83185333 1.1 5250614 4588427.4; 14:53:30.687 INFO ProgressMeter - Traversal complete. Processed 5250614 total reads in 1.1 minutes.; 14:53:33.576 INFO PrintReads - Shutting down engine; [October 5, 2017 2:53:33 PM EDT] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 1.38 minutes.; Runtime.totalMemory()=8385462272; ```. ## Cloud CRAM; Running just PrintReads without `-L` intervals succeeds.; ```; /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-launch PrintReads \; -I gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram \; -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -O HG00190_cram.bam; ```; ```; -bash-4.1$ /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-launch PrintReads \; > -I gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram \; > -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; > -O HG00190_cram.bam; Using GATK jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:4502,down,down,4502,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['down'],['down']
Availability,IntervalUtils::loadIntervals gives bad error on missing .interval_list file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6956:39,error,error,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6956,1,['error'],['error']
Availability,Investigate GATK error message with docker --network none,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7983:17,error,error,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983,1,['error'],['error']
Availability,Investigate NIO retry failures observed in CNV walkers.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631:22,failure,failures,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631,1,['failure'],['failures']
Availability,Investigate Potential Base Recovery Error In Reference Caching Code,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6338:27,Recover,Recovery,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6338,2,"['Error', 'Recover']","['Error', 'Recovery']"
Availability,Investigate dataflow debugging/error logging,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/276:31,error,error,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/276,1,['error'],['error']
Availability,Investigate errors seen in the DRAGEN-GATK GVCF outputs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7108:12,error,errors,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7108,1,['error'],['errors']
Availability,Investigate out of memory error in CalibrateDragstrModel,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7189:26,error,error,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7189,1,['error'],['error']
Availability,Investigating test failures. There might be a better approach.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6769#issuecomment-681107365:19,failure,failures,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6769#issuecomment-681107365,1,['failure'],['failures']
Availability,"Is not a duplicate IMO. Regardless of the bug, make sense to pass down those filtered reads to annotators?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7144#issuecomment-804553263:66,down,down,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7144#issuecomment-804553263,1,['down'],['down']
Availability,Is the Downsampled annotation still a real thing?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5678:7,Down,Downsampled,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5678,1,['Down'],['Downsampled']
Availability,"Is the issue fixed? I ran into the same error and after checking out GATK from GitHub and with the latest build (commit 031c40773d2aefd289005319d6880dbeb3c1dec9, gatk-4.1.4.1-83-g031c407-SNAPSHOT), the problem persisted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-591786301:40,error,error,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-591786301,1,['error'],['error']
Availability,"Is there a script that can be tuned to stop putting the legend of VQSR plots on top of the plot?; It is masking important information as shown below.; When this is using ggplot2, the legend could be captured and plotted as a separate grid object next to the plot (for instance). ![example](https://i.imgur.com/53rbF0c.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6585:104,mask,masking,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6585,1,['mask'],['masking']
Availability,"Is there a way to specify the number of threads or cores to use ? Specifically for HaplotypeCaller ?; In GATK3, we had -nt and -nct (more of less reliable for what I read) but it doesn't seems to be here anymore.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4448:146,reliab,reliable,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4448,1,['reliab'],['reliable']
Availability,Is there any other variants up or downstream within a read length? (try to zoom out a bit). Also when looking into a bamout it helps to color reads and haplotypes by their best haplotype id (HC tag I think).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8238#issuecomment-1479957229:34,down,downstream,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8238#issuecomment-1479957229,1,['down'],['downstream']
Availability,Is this a problem in docker? We control the image so it should have plenty of file handles available.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4409#issuecomment-365705851:91,avail,available,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4409#issuecomment-365705851,1,['avail'],['available']
Availability,Is this actually fixed? Is there a version of GATK tools that incorporates this change so I can stop having the majority of my logs when run locally be filled up by this exact error message?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-472935667:176,error,error,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-472935667,1,['error'],['error']
Availability,"Is this expected? If not, how should we address that? I could think of copying the type determination code from HTSJDK into VariantTypesVariantFilter and accounting for this case, which would fix the problem for this filter. However I could also imagine that always assigning MIXED to every GVCF variant isn't very helpful in other situations, but making a change in HTSJDK's VariantContext would probably have massive downstream effects. Also referenced here: https://gatk.broadinstitute.org/hc/en-us/community/posts/360071943332-SelectVariants-error",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7111#issuecomment-786658747:419,down,downstream,419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7111#issuecomment-786658747,2,"['down', 'error']","['downstream', 'error']"
Availability,"Is this in the gatk repo or a different repo?. On Mon, Mar 22, 2021 at 11:14 AM jamesemery ***@***.***>; wrote:. > @ahaessly <https://github.com/ahaessly> It looks like you are still; > running on a somewhat old snapshot of the DRAGEN code. It might be worth; > updating to master and re-running since we fixed some other errors related; > to this tool since 4.2 that you might want anyway.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/7152#issuecomment-804141441>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFO2VRMVEHQWPJKTJ2E4OITTE5NFLANCNFSM4ZNOBRZQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7152#issuecomment-804163665:322,error,errors,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7152#issuecomment-804163665,1,['error'],['errors']
Availability,Is this issue fixed yet? @TnakaNY I am using `GATK v4.1.7.0` and still getting the error `wrong number of fields in AS_FilterStatus?` while merging Mutect2 VCF files from multiple donors using the `bcftools merge`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6857#issuecomment-892564135:83,error,error,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6857#issuecomment-892564135,1,['error'],['error']
Availability,"Is this issue still open? I'm getting a similar error like this:; ```; .; .; .; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:14:42.635 INFO BaseRecalibrator - ------------------------------------------------------------; 19:14:42.635 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.8.1; 19:14:42.635 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:14:42.638 INFO BaseRecalibrator - Executing as XXX on Linux v3.10.0-957.12.2.el7.x86_64 amd64; 19:14:42.638 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_212-b04; 19:14:42.638 INFO BaseRecalibrator - Start Date/Time: September 12, 2020 7:14:42 PM PDT ; 19:14:42.638 INFO BaseRecalibrator - ------------------------------------------------------------; 19:14:42.638 INFO BaseRecalibrator - ------------------------------------------------------------; 19:14:42.638 INFO BaseRecalibrator - HTSJDK Version: 2.23.0; 19:14:42.638 INFO BaseRecalibrator - Picard Version: 2.22.8; 19:14:42.638 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2 ; 19:14:42.638 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:14:42.639 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:14:42.639 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:14:42.639 INFO BaseRecalibrator - Deflater: IntelDeflater; 19:14:42.639 INFO BaseRecalibrator - Inflater: IntelInflater; 19:14:42.639 INFO BaseRecalibrator - GCS max retries/reopens: 20; 19:14:42.639 INFO BaseRecalibrator - Requester pays: disabled; 19:14:42.639 INFO BaseRecalibrator - Initializing engine; 19:14:43.472 INFO FeatureManager - Using codec BEDCodec to read file XXX; 19:14:43.726 WARN IndexUtils - Feature file XXX appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 19:14:43.755 INFO BaseRecalibrator - Done ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807#issuecomment-691600264:48,error,error,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807#issuecomment-691600264,1,['error'],['error']
Availability,"Is this the only `CommandLineException` which should be an `UserException`? If not, how is going to work the development of new exceptions in Barclay. For instance, in https://github.com/broadinstitute/barclay/pull/11 there is a new exception that I made for values out of range, which extends `BadArgumentValue`. I agree that this errors should be decoupled from the ones that are not, but in this case I think that this is already implemented:. * `UserException` are handled in the `mainEntry()`, distinguishing errors that comes from the user's side regarding some specifications in the tools/framework.; * `CommandLineException` are handled in `parseArgs()`, distinguishing errors that comes from the command line from the user side while parsing. I expect that any command line error that is not `CommandLineParserInternalException ` or `ShouldNeverReachHereException` comes from the user's side. The contract in Barclay says that are `CommandLineException` are _""Exceptions thrown by CommandLineParser implementations.""_, and I think that if other parts of the code (outside arg parsing) is throwing this exception is a bug that does not come from the user. I guess that this is the problematic part.; * Any other `Exception` is thrown in `Main.handleNonUserException()`, which may be caused by non-user exceptions. I propose that `CommandLineException` is handled as currently to separate ""errors that are the user's fault regarding input and/or assumptions"" (`UserException`), ""errors that are the user's fault while providing parameters to the command line"" (`CommandLineException`) and ""errors that are not the user's fault"" (other `Exception`s). Actually, this is reasonable because the exit status is different for any kind of errors in the current `Main`. The only problem that I see with this approach is the silently failing of a ""bug"" in tools/engine code, which can be rethrow easily in `CommandLineProgram.instanceMain`` as following:. ```java; public Object instanceMain(final Strin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268773161:332,error,errors,332,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268773161,4,['error'],"['error', 'errors']"
Availability,"Is your GenomicDB named as your contig ? If so it may produce an error if your working directory is the Genomic DB folder, see my comment here : . https://gatk.broadinstitute.org/hc/en-us/community/posts/26901826831899-GenotypeGVCFs-issue-when-using-include-non-variant-sites-parameter. In addition the option -L needs to be set in in both the GenomicDBimport step and the GenotypeGVCFs step, see below : . `singularity exec --bind /nvme/disk0/lecellier_data:/nvme/disk0/lecellier_data /home/hdenis/gatk_latest.sif gatk --java-options ""-Xmx15g -Xms4g"" GenomicsDBImport --genomicsdb-workspace-path ""${INDIR}GenomicDB/${CONTIG_NAME}"" --batch-size 50 -L $CONTIG --sample-name-map ""${INDIR}aspat_gvcf_clean.sample_map"" --tmp-dir /nvme/disk0/lecellier_data/WGS_GBR_data/tmp --reader-threads 7 --genomicsdb-shared-posixfs-optimizations true --bypass-feature-reader true `. `singularity exec --bind /nvme/disk0/lecellier_data:/nvme/disk0/lecellier_data /home/hdenis/gatk_latest.sif gatk --java-options ""-Xmx58g"" GenotypeGVCFs -R $REF_3 -V ""gendb://${INDIR}GenomicDB/${CONTIG_NAME}"" -O ""${OUTDIR}aspat_clean_${CONTIG_NAME}.vcf.gz"" --tmp-dir /nvme/disk0/lecellier_data/WGS_GBR_data/tmp --include-non-variant-sites true -L $CONTIG --only-output-calls-starting-in-intervals true`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2212079735:65,error,error,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2212079735,1,['error'],['error']
Availability,"Issue from the forum regarding GenotypeGVCFs. User is getting an error with the bcf codec. They are running 4.1.7.0, so I recommended updating to 4.1.8.1 for now to see if it is fixed. This request was created from a contribution made by Brynjar Sigurðsson on August 04, 2020 09:49 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360072049511-Assertion-nps-nps-line-n-sample-n-failed](https://gatk.broadinstitute.org/hc/en-us/community/posts/360072049511-Assertion-nps-nps-line-n-sample-n-failed). \--. Hello,. I am running a variant calling using GenotypeGVCFs. The process first creates a GenomicsDB on 50Kbase regions from 150K GVCFs and then runs GenotypeGVCFs wrapped in GNU parallel (after splitting the region into as many threads as are available). Most regions complete without a problem, but some fail on GenotypeGVCFs with the assertion error. java: /home/vagrant/GenomicsDB/dependencies/htslib/vcf.c:4225: bcf\_update\_format: Assertion \`nps && nps\*line->n\_sample==n' failed. Some of the failing regions I have run with up to 1.5 TB memory (18 threads) but they still fail. **a) GATK version used**. **version 4.1.7.0**. **b) Exact GATK commands used**. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx1290240M -Xms1290240M -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true -jar /nfs/fs1/bioinfo/apps-x86\_64/GATK/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar GenomicsDBImport --genomicsdb-workspace-path /tmp/tmp.ceRdvv/GDB --intervals chr1:5149001-5201000 --tmp-dir /tmp/tmp.ceRdvv/GDB\_tmp --sample-name-map /tmp/tmp.ceRdvv/snmap --batch-size 100 --reader-threads 17. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Djava.io.tmpdir=/tmp/tmp.ceRdvv -Xmx71680M -Xms71680M -jar /nfs/fs1/bioinfo/apps-x86\_64/GATK/gatk",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6742:65,error,error,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6742,3,"['avail', 'error']","['available', 'error']"
Availability,"Issue while Building GATK4. sudo ./gradlew bundle; Starting a Gradle Daemon, 1 incompatible and 1 stopped Daemons could not be reused, use --status for details. FAILURE: Build failed with an exception. * Where:; Build file '/home/rafay/gatk-4.0.2.0/build.gradle' line: 289. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Cannot find '.git' directory. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 24.349 secs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4475:161,FAILURE,FAILURE,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4475,1,['FAILURE'],['FAILURE']
Availability,"Issue: Integer overflow error caused Mutect2 v4.1.4.0 to generate a stats file with a negative number. Solution is to change the int data type to long. User report:. Hello, I've just adapted my pipeline to the new filtering strategies, while looking at the files I noticed that for a WGS run I obtained a stats file with a negative number:; [egrassi@occam biodiversa]>cat mutect/CRC1307LMO.vcf.gz.stats; statistic value; callable -1.538687311E9. Looking around about the meaning of the number I found https://gatkforums.broadinstitute.org/gatk/discussion/24496/regenerating-mutect2-stats-file, so I'm wondering if I should be worried by having a negative number of callable sites :/; What's more puzzling is that FilterMutectCalls after ran without any error. Before running mutect I used the usual best practices pipeline, then:; ; gatk Mutect2 -tumor CRC1307LMO -R /archive/home/egrassi/bit/task/annotations/dataset/gnomad/GRCh38.d1.vd1.fa -I align/realigned_CRC1307LMO.bam -O mutect/CRC1307LMO.vcf.gz --germline-resource /archive/home/egrassi/bit/task/annotations/dataset/gnomad/af-only-gnomad.hg38.vcf.gz --f1r2-tar-gz mutect/CRC1307LMO_f1r2.tar.gz --independent-mates 2> mutect/CRC1307LMO.vcf.gz.log; ; gatk CalculateContamination -I mutect/CRC1307LMO.pileup.table -O mutect/CRC1307LMO.contamination.table --tumor-segmentation mutect/CRC1307LMO.tum.seg 2> mutect/CRC1307LMO.contamination.table.log; ; gatk LearnReadOrientationModel -I mutect/CRC1307LMO_f1r2.tar.gz -O mutect/CRC1307LMO_read-orientation-model.tar.gz 2> mutect/CRC1307LMO_read-orientation-model.tar.gz.log; ; gatk FilterMutectCalls -V mutect/CRC1307LMO.vcf.gz -O mutect/CRC1307LMO.filtered.vcf.gz -R /archive/home/egrassi/bit/task/annotations/dataset/gnomad/GRCh38.d1.vd1.fa --stats mutect/CRC1307LMO.vcf.gz.stats --contamination-table mutect/CRC1307LMO.contamination.table --tumor-segmentation=mutect/CRC1307LMO.tum.seg --filtering-stats mutect/CRC1307LMO_filtering_stats.tsv --ob-priors mutect/CRC1307LMO_read-orientation-model.t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6302:24,error,error,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6302,2,['error'],['error']
Availability,"It also fails in Mac OS X 10.11.6 x86_64. I'm trying to update my project to the latest version of GATK and this dependency throws the following error with some of my gradle tests and while running an uber-jar (using `--use_jdk_deflater false`):. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011d925644, pid=7088, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression8215566221555962564.dylib+0x1644] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid7088.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Find attached the log: [hs_err_pid7088.log.txt](https://github.com/broadinstitute/gatk/files/652421/hs_err_pid7088.log.txt). Should I open a different issue for this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-267103689:145,error,error,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-267103689,3,['error'],['error']
Availability,"It appears for at least the last 6 days that our travis tests for the the CNV and M2 WDLs have been failing generating the following error messages:. `[2017-09-07 10:05:53,75] [warn] BackendPreparationActor_for_0b561ba3:CNVSomaticPanelWorkflow.PadTargets:-1:1 [0b561ba3]: Docker lookup failed:; java.lang.Exception: Docker image broadinstitute/gatk:80d8662d760f451045957080813d3963a1b68cc5 not found; 	at cromwell.engine.workflow.WorkflowDockerLookupActor.cromwell$engine$workflow$WorkflowDockerLookupActor$$handleLookupFailure(WorkflowDockerLookupActor.scala:193); 	at cromwell.engine.workflow.WorkflowDockerLookupActor$$anonfun$3.applyOrElse(WorkflowDockerLookupActor.scala:91); 	at cromwell.engine.workflow.WorkflowDockerLookupActor$$anonfun$3.applyOrElse(WorkflowDockerLookupActor.scala:75); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.WorkflowDockerLookupActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowDockerLookupActor.scala:39); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.WorkflowDockerLookupActor.processEvent(WorkflowDockerLookupActor.scala:39); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at cromwell.docker.DockerClientHelper$$anonfun$dockerResponseReceive$1.applyOrElse(DockerClientHelper.scala:16); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.WorkflowDockerLookupActor.aroundReceive(WorkflowDockerLookupActor.scala:39); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3558:133,error,error,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3558,1,['error'],['error']
Availability,"It appears that the linux version of the packaged libfml was compiled with a different version of gcc than the one on jprnz's machine. (We compile so that we have the right version for a Google dataproc cluster.); I think the best thing to do would be to download git@github.com:broadinstitute/gatk-fermilite-jni.git, and follow the instructions in the last paragraph of the readme for doing a local recompilation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5145#issuecomment-417355592:255,down,download,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145#issuecomment-417355592,1,['down'],['download']
Availability,"It could be a synchronization issue. E.g. if thread A is asking for an alleleCount of 3 and and thread B an alleleCount of 4, then thread A could grow the array to 3 after thread B grows it to 4 (meaning the array is grown to size 4 but then set back to size 3), but before thread B reads position 4. This read will then fail. BTW I was seeing quite a lot of task failures, around 10%.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408872373:364,failure,failures,364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408872373,1,['failure'],['failures']
Availability,"It depends why it fails the filter. One reason is consecutive indel elements, but consider for example:. ref: ACGTTTA; read: AC TTTTA; cigar: 2M1D1I4M. Especially in long-read technologies with a lot of indel errors it seems draconian to throw out reads where this happens once. And okay, I understand that an aligner could represent this as a G->T subsitution, but what about the same thing but with 2D followed by 2I? That strikes me as a much better cigar than calling it a DNP. In general, a bad cigar should mean either that the aligner is bad (in which case why are we filtering isolated reads and not just rejecting the entire BAM?) or the read is malformed. But consecutive indels in a technology with many indel errors is neither of these!. Anyway, I don't think there's a problem with allowing these reads in the GATK, and if there is the refactoring in this #6403 should let us fix any problem easily enough.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6433#issuecomment-583415079:209,error,errors,209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6433#issuecomment-583415079,2,['error'],['errors']
Availability,"It doesn't explode in GATK3 -- I had to drop -stand_call_conf to 10 to get it to output, then I got MQ=NaN. The NaN/explosion is my fault -- pulling the DP from the format field seemed like such a good idea at the time, but it's just causing problems. I had a ticket open for it, but the particular test case got fixed in another way. I'll open another ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2658#issuecomment-299502902:132,fault,fault,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2658#issuecomment-299502902,1,['fault'],['fault']
Availability,"It doesn't seem related to the version of java. I updated my version of java to; ```; javac -version; javac 1.8.0_191. java -version; java version ""1.8.0_191""; Java(TM) SE Runtime Environment (build 1.8.0_191-b12); Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode); ```. The issue remains the same, I pulled the latest commit on master and ran; ```; ./gradlew clean; ./gradlew bundle; ./gradlew test; ...; Results: FAILURE (500075 tests, 500072 successes, 2 failures, 1 skipped). 500075 tests completed, 2 failed, 1 skipped; ```; The behavior is the same, the exact two same tests fail.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5511#issuecomment-451930915:430,FAILURE,FAILURE,430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511#issuecomment-451930915,2,"['FAILURE', 'failure']","['FAILURE', 'failures']"
Availability,"It happened again, on Friday. This was running updated code that [checks the position before calling](https://github.com/broadinstitute/gatk/blob/jp_retry_more_2/src/main/java/org/broadinstitute/hellbender/utils/nio/SeekableByteChannelPrefetcher.java#L119) - so we know that we set the position to a non-negative value before doing the read. The error says that the position was -218103808 -- this is 0xD000000 in hex, a suspiciously round number. The previous times we've seen this, we got:. value seen in error | hex | on 40MB boundary?; ------------ | ------------- | ---; -218103808 | -0xD000000 | no; -285212672 | -0x11000000 | no; -1577058304 | -0x5E000000 | no; -385875968 | -0x17000000 | no. Our prefetch buffer size is 40 MB (0x2800000) so we might think that explains the many leading zeroes (are we just on a negative number of buffer boundaries?) but the error number is not a multiple of this constant, so that's not it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2516#issuecomment-289573036:346,error,error,346,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2516#issuecomment-289573036,3,['error'],['error']
Availability,"It happened multiple times over the course of a couple days. Since I; downloaded the full gnomad exome data locally, I haven't tested again. --; - Alan Hoyle - alan@alanhoyle.com - http://www.alanhoyle.com/ -. On Mon, Nov 9, 2020 at 2:23 PM droazen <notifications@github.com> wrote:. > @alanhoyle <https://github.com/alanhoyle> Can you tell us whether the 400; > Bad Request error is repeatable -- did you see it more than once?; > Oftentimes when accessing cloud data we encounter transient errors like; > this that go away on their own.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/6926#issuecomment-724225557>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AACGX43OY2CFF5KFZXZOH4LSPA6T3ANCNFSM4TD2FDGA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-724267156:70,down,downloaded,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926#issuecomment-724267156,3,"['down', 'error']","['downloaded', 'error', 'errors']"
Availability,"It is not clear to me from the docs whether parent/child pairs are intended to be supported by `CalculateGenotypePosteriors`, but a quick glance at the [mention](https://github.com/broadinstitute/gatk/blob/67f0f0f2e59185b721398b17c24eba487a2ac76c/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/FamilyLikelihoods.java#L210)[s](https://github.com/broadinstitute/gatk/blob/67f0f0f2e59185b721398b17c24eba487a2ac76c/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/FamilyLikelihoods.java#L231) in comments in `FamilyLikelihoods.java` makes me suspect that they are intended to be supported. (In any case, from my perspective, it would be a very nice feature as I have yet to find a tool that will robustly handle this use case.). Here are the main issues that I'm encountering when trying to use `CalculateGenotypePosteriors` for a parent-child pair:; 1) If I supply a ped file with two individuals like the following, [this check](https://github.com/broadinstitute/gatk/blob/1e98c6d02cefefbaa1a15db0aea64ea7518025fa/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/CalculateGenotypePosteriors.java#L260) gets triggered, resulting in printing of the warning and skipping family priors.; ```; FAM	MOM	0	0	2	0; FAM	CHILD	0	MOM	2	0; ```; 2) If I add a father to the ped file to form a trio, like below, `CalculateGenotypePosteriors` proceeds without the warning that occurs in first approach, but the output doesn't appear to make any adjustments to genotypes, posteriors, etc. Note that there is no entry for ""DAD"" in the input VCF.; ```; FAM	MOM	0	0	2	0; FAM	DAD	0	0	1	0; FAM	CHILD	DAD	MOM	2	0; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5409:738,robust,robustly,738,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5409,1,['robust'],['robustly']
Availability,It is publically available in the GATK workshop bundle but here it is for convenience:. [forKatieFromSooHee.zip](https://github.com/broadinstitute/gatk/files/2336419/forKatieFromSooHee.zip),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5129#issuecomment-417326998:17,avail,available,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5129#issuecomment-417326998,1,['avail'],['available']
Availability,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6072:754,Error,Error,754,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072,4,"['Error', 'avail', 'down']","['Error', 'available', 'down']"
Availability,"It looks like all the changes in my original commit with the raw GATK3 code (except for one file) got squashed out somehow, so I can't see just the changes from GATK3 anymore. I'll probably have to go back and re-commit those when you're ready to make this tractable to review. I'll wait to comment on #1 until that happens. As for a default plugin descriptor, I'd prefer not to take one unless its fully implemented, with tests. Plus, although we could develop it here, it should really live in the Barclay repo if its truly generic. More importantly, I'm not sure the plugins in this PR should be plugins at all. Historically, plugins have required a lot of test development and iteration because they have command line arguments (the plugin system is for extending the command line parser with discoverable, re-useable components that are shared across multiple tools, and need shared command line arguments). I haven't looked at the new ones closely, but I'm not sure they're a good fit. As for the files, it look like about 400MB (?) Thats pretty big - you should try to squeeze them down or target some existing files if you can.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431839008:1089,down,down,1089,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431839008,1,['down'],['down']
Availability,"It looks like at one point GATK 3.8 was available via the [homebrew science tap](https://github.com/ilovezfs/homebrew-science/blob/master/gatk.rb). I've tried adding their formula to my homebrew formula folder and installing via ``` brew install gatk.rb``` but there's a ton of errors. ```Updating Homebrew...; ==> Downloading https://github.com/broadgsa/gatk-protected/archive/3.8-1.tar.gz; Already downloaded: /Users/timothystiles/Library/Caches/Homebrew/gatk-3.8-1.tar.gz; ==> mvn package -Dmaven.repo.local=${PWD}/repo; Last 15 lines from /Users/timothystiles/Library/Logs/Homebrew/gatk/01.mvn:; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR]; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR]; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR]; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4164#issuecomment-358697586:40,avail,available,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4164#issuecomment-358697586,6,"['Down', 'ERROR', 'avail', 'down', 'error']","['Downloading', 'ERROR', 'available', 'downloaded', 'errors']"
Availability,"It looks like the errors are all of the flavor:. ```; Unable to load Maven meta-data from https://artifactory.broadinstitute.org/artifactory/libs-snapshot/com/github/samtools/htsjdk/2.9.1-34-gd7bae17-SNAPSHOT/maven-metadata.xml.; > Could not GET 'https://artifactory.broadinstitute.org/artifactory/libs-snapshot/com/github/samtools/htsjdk/2.9.1-34-gd7bae17-SNAPSHOT/maven-metadata.xml'. ; Received status code 401 from server: Unauthorized; ```; Perhaps Maven's temporarily in a bad mood, we'll have to try again later.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-306264180:18,error,errors,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-306264180,1,['error'],['errors']
Availability,"It looks like the tests ran on Travis but there is a legitimate failure (testMultipleCompTracks), which fails for me locally as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-432235415:64,failure,failure,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-432235415,1,['failure'],['failure']
Availability,"It looks like the underlying error is ""No default remote / No remotes defined"". What does `git remote show origin` output? If it doesn't show the URL of this repo under ""Fetch URL"" and ""Push URL"", then your git clone was not created correctly. In this case, you should re-clone the repo using a command like `git clone https://github.com/broadinstitute/gatk.git gatk`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383216894:29,error,error,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383216894,1,['error'],['error']
Availability,"It looks like they're using a strange hybrid of Picard and Barclay syntax. If they just use pure Barclay syntax with no ""="" (`--RUN_DATE 2011-04-30T01:00:00+0100`) when running the Picard tool from gatk, the error message should go away. The only part I don't understand is that they say the hybrid syntax works if they leave out `RUN_DATE`. I wouldn't expect that - I'll have to look into it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5189#issuecomment-421191038:208,error,error,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5189#issuecomment-421191038,1,['error'],['error']
Availability,"It looks like this is a bug with 4.2.0.0 because the same Mutect2 output has no issues with FilterMutectCalls 4.1.6.0. . This request was created from a contribution made by Qihan Long on June 04, 2021 03:21 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078236332-The-output-of-Mutect2-cannot-be-accepted-by-FilterMutectCalls-in-GATK-4-2-0-0](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078236332-The-output-of-Mutect2-cannot-be-accepted-by-FilterMutectCalls-in-GATK-4-2-0-0). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0 ; ; b) Exact command used: . gatk FilterMutectCalls \\ ; ; \-R /public1/data/resources/ref\_genome/GRCh38/GRCh38.d1.vd1.fa \\ ; ; \-V somatic\_mutation/Mutect2/test.vcf.gz \\ ; ; \-O somatic\_mutation/FilterMutectCalls/test.vcf.gz. c) Entire error log:. I used the ""--enable-all-annotations"" option within Mutect2 to get a vcf file with abundant information. However, the following FilterMutectCalls step seemed to be intolerant of some information within previous step's vcf file record. The intolerated record within vcf listed below: . chr1 6197724 . C CT,CTT,CTTT . . AC=1,1,1;AF=0.167,0.167,0.167;AN=6;AS\_BaseQRankSum=-6.431;AS\_MQ=60.00,60.00,60.00;AS\_MQRankSum=0.000;AS\_ReadPosRankSum=5.751;AS\_SB\_TABLE=42,880|3,164|3,32|0,14;**AS\_UNIQ\_ALT\_READ\_COUNT=167|35|14**;BQHIST=5,1,0,0,1,11,2,0,0,0,14,2,0,0,1,15,1,0,0,0,16,1,0,0,0,17,0,2,0,0,18,2,0,0,1,19,6,0,1,0,20,25,0,2,2,21,13,0,1,2,22,20,0,3,3,23,2,1,2,1,24,6,0,2,0,25,21,1,4,7,26,33,0,5,6,27,18,0,0,7,28,29,0,0,4,29,26,2,4,8,30,161,4,5,51,31,263,2,3,51,32,129,2,3,22,33,41,0,0,0,34,15,0,0,0,35,20,0,0,0,36,19,0,0,0,37,12,0,0,0,38,1,0,0,0,39,9,0,0,0,41,18,0,0,0,44,26,0,0,0;BaseQRankSum=-6.431;ClippingRankSum=-7.714;DP=1323;ECNT=1;FS=0.000;LikelihoodRankSum=-7.886;MBQ=31,30,26,30;MFRL=6590,6585,4819,6586;MMQ=60,60,60,60;MPOS=16,15,7;MQ=59.98;MQ0=0;MQRankSum=0.000;NALOD=0.569,1.49,1.49;NCC=0;NCount=0;NLOD=27.80,30.51,30",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7298:545,error,error,545,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298,2,['error'],['error']
Availability,"It looks like we've found a bug in GenomicsDB. We had a project with 26 replicates (same sample in there twice) among another ~1000 samples. Therefore we uniquified the names in the sample map that’s input to TileDB for those 26 samples (i.e. converted them to project.sample instead of just sample) -- but obviously the names in the gvcfs remained unaltered. When we look at the output VCF from GenomicsDB, there's definitely a problem. These 52 samples are the first ones in the list and here's what we see:. The first 26 samples (the first occurrence of the replicates) are fine.; Then the next 24 samples (the second occurrence of the replicates) are all “.:0,0” (i.e. empty) for all columns in the VCF.; Then the next 2 samples (also second occurrences of replicates) are fine. Given that our batch size was 50 when importing into GenomicsDB, this looks suspiciously like an error with the batching. So within a batch it looks like it’s not respecting the renaming somehow?. @kgururaj",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3814:880,error,error,880,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3814,1,['error'],['error']
Availability,"It looks like when this was added, a mistake was made between a filter returning test() == true (passing the filter) and test() == false (failing the filter, read removed). Furthermore the invert filter argument in here is now redundant as of #8724 and I will go ahead and remove it from this filter. I have also tweaked the filter arguments slightly to clarify what they do now mean more intuitively. . Fixes #8887",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8888:227,redundant,redundant,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8888,1,['redundant'],['redundant']
Availability,"It reports the argument ranges kind of weirdly, floats fault to -infinity -> infinity but so do ints and probably longs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6565#issuecomment-617912221:55,fault,fault,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6565#issuecomment-617912221,1,['fault'],['fault']
Availability,"It seems a lot of users use bcftools on their VCFs, and it sometimes converts floats to integers. For example MQ=31.0 to MQ=31. This change causes GATK tools to error. Is it possible to relax this validation?. ----; User Report; ----. Hi,. Every time I had this message, this was due to bcftools which can change some float values to an integer representation : (e.g : before bcftools : MQ=31.0; after bcftools : MQ=31). . The fact that GATK is very strict on that subject (40.0 is considered as a float while 40 is not) have some advantages and some drawbacks. I hope this problem will be resolved in GATK4 because bcftools is really useful and widely used when dealing with vcf files. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/43270#Comment_43270",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3734:161,error,error,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3734,1,['error'],['error']
Availability,"It seems like the argument `--force-call-filtered-alleles` is redundant with `--alleles`. Unless I misunderstand it, it looks like force-call-filtered-alleles is just used to decide if we should look at the `--alleles` argument or not, which is only used in conjunction with force calling. It seems redundant. Couldn't we merge them into a single argument that takes an allele list?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6572:62,redundant,redundant,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6572,2,['redundant'],['redundant']
Availability,"It seems like the patch in 4.1.6 didn't go far enough and that exception needs to be replaced with a `continue` in all cases. This seems to be occurring for haplotypes with long indels inside their assembly padding that don't have enough spanning sequence to resolve. Since the variation is inside the padding, it seems safe to ignore. Increasing padding resolves the issue, alhtough this is at the cost of runtime and should not be necessary. For example, suppose we have a ref haplotype ABCDD, where A, B, and C represent sequences of, say, 100 bases and D is a sequence of 50 bases. Suppose further that A and DD are the padding. Then the cigar of an alt haplotype ABCD gets aligned as a 350 base match that doesn't span the full padded reference region, leading to the error. I still need to figure out why this didn't happen in 4.1.4 (my guess is that elsewhere the code effectively skipped these haplotypes before the exception).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-607059533:773,error,error,773,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-607059533,1,['error'],['error']
Availability,"It seems that a [few users](https://gatk.broadinstitute.org/hc/en-us/community/posts/11440622639387-Unable-to-trim-uncertain-bases-without-flow-order-information?page=1#community_comment_12925222020763) have complained when they try to use LongReads that their minimap2 bams are failing with the message: ; `org.broadinstitute.hellbender.exceptions.GATKException: Unable to trim uncertain bases without flow order information`. It looks like in `AssemblyBasedCallerUtils.java:147` we are calling `FlowBasedReadUtils.isFlow(originalRead)` on every read and the presence of the `tp` read tag in reads is considered sufficient to flag reads as being flow based which finally causes them to fail. This check was probably misguided, we should really be checking flow-based identity (at this stage anyway) from the readgroup in the header to prevent any spurious read tags from god knows what aligners don't cause problems like this again. Alternatively we should thread the ""isFlowBased"" check down into this part of the code so its opt-in to treat flow based reads specially when clipping here.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8335:989,down,down,989,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8335,1,['down'],['down']
Availability,"It seems that insQual and delQual score may produce by BQSR or modified by the Indel error model may sum more than 1. Eg. IQ = 1, DQ = 1 so 0.9 prob of an insertion and 0.9 prob of a deletion. This would result Prob > 1 in PairHMM (a different bug from the previously reported to this regard). . The question is how to fix this… must be controlled by BQSR/ index error model… should result in a warning and the qual adjusted to a maximum error probability.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/268:85,error,error,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/268,3,['error'],['error']
Availability,"It seems that the code in SparkSharder responsible of grouping Locatables into sharded reference intervals cannot handle large locatables that would overlap more than a couple of shards... . ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost): org.broadinstitute.hellbender.exceptions.UserException: Max size of locatable exceeded. Max size is 10000, but locatable size is 1157593. Try increasing shard size and/or padding. Locatable: [VC Unknown @ 1:29721370-30878962 Q. of type=SYMBOLIC alleles=[C*, <INV>] attr={ALIGN_LENGTHS=144, ASSEMBLY_IDS=276, CONTIG_IDS=contig-5, END=30878962, HQ_MAPPINGS=1, INSERTED_SEQUENCE=AAACCAGGCCCCAGGGCCCCAGAAAGCAGGTAGTAGGGCCAAGCGAGGGCCGGGGCAGGCTAGCTCCAAGCCCACTGCAGGCCTCAGCTCTGCT, INV55=true, MAPPING_QUALITIES=60, MAX_ALIGN_LENGTH=144, SVLEN=1157592, SVTYPE=INV, TOTAL_MAPPINGS=1} GT=[]; 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$4.computeNext(SparkSharder.java:232); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$4.computeNext(SparkSharder.java:212); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$7.computeNext(Iterators.java:650); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.TransformedIterator.hasNext(TransformedIterator.java:43); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42). ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2554:254,failure,failure,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2554,2,['failure'],['failure']
Availability,It seems that the error goes away when I installed 2.0.2 ..... I had the 2.1.0 that is the only 2.x.x available thru homebrew on Mac. So it might all be due to a version difference.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2545#issuecomment-290556381:18,error,error,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2545#issuecomment-290556381,2,"['avail', 'error']","['available', 'error']"
Availability,"It seems that there are some small numerical differences in apache commons Math3 Hypergeometric distribution. In GATK3 we call the Hypergeometric distribution to get the probability directly, in GATK4 we call the Hypergeometric distribution to get the log of the probability. In these cases there are sometimes some very small rounding errors in the 15th or so least significant digit. The problem is that we either add these numbers to the p-value or not by checking if they are less than or equal to some threshold. In this case these rounding errors can have a large impact (especially when the number of reads is relatively small) on the final p-value. R seems to have more digits and therefore usually reports that they are the same values, so in this case neither GATK3 or GATK4 match R. I haven't tried other implementations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2586#issuecomment-297749496:336,error,errors,336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2586#issuecomment-297749496,2,['error'],['errors']
Availability,"It seems to me the `Header definition line` encompasses the information given by the `VCF Field` so this latter is redundant. . It would definitely be useful to categorize INFO (cohort) versus FORMAT (SAMPLE) level annotations. I'm not clear on the significance of the `Type` nor `Category` fields. `Type` might be the groupings, e.g. HaplotypeCaller standard annotations versus Mutect2 standard annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-344423143:115,redundant,redundant,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-344423143,2,['redundant'],['redundant']
Availability,It seems very likely that we have a file handle leak in our spark code somewhere. Raising the ulimit might help but it's not a great solution. We'll have to hunt down the place that things aren't being closed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-598218588:162,down,down,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-598218588,1,['down'],['down']
Availability,It should detect that java is missing and exit with a clear error message instead.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5993:60,error,error,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5993,1,['error'],['error']
Availability,"It should instead notice that `sampleToVCMap.get()` has returned null, and throw a descriptive error message (including, crucially, the sample name in question) instead of allowing a `NullPointerException` to occur.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2715:95,error,error,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2715,1,['error'],['error']
Availability,It still looks like it's adding a few minutes to the times... Are we still having a lot of a failures? I haven't been noticing them.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6093#issuecomment-521703539:93,failure,failures,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6093#issuecomment-521703539,1,['failure'],['failures']
Availability,"It turns out I was mistaken that setting the environment variables fixes the problem (stupid error on my part). It's possible the BaseTest message is unrelated. I haven't tested this branch out yet, but building from the commit immediately before the update works. I am going to try the next version to see if it helps. Edit: #3594 does not fix the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330896419:93,error,error,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330896419,1,['error'],['error']
Availability,It turns out that using an SSD for `spark.local.dir` results in a massive speedup over using a conventional HDD for our Spark tools. We should find out whether Google dataproc is smart enough to set `spark.local.dir` to an SSD when one is available,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-282809620:239,avail,available,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-282809620,1,['avail'],['available']
Availability,It was noticed while doing #8351 that the `GencodeFunctotation.equals()` method has the following line in it; ``` ; if (geneTranscriptType != that.geneTranscriptType) return false; ; ```. Unfortunately the geneTranscriptType is stored as a Sting and thus this should NOT be expected to succeed in almost any case. As it stands fixing this innocuous oversight seems to break several of the combinatorial funcotator tests and an integration test. Somebody should fix this behavior (easy) and validate that the test changes are within tolerable levels (hard).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8385:532,toler,tolerable,532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8385,1,['toler'],['tolerable']
Availability,It will be very useful to have an abstract class for the plugin arguments (as I did for the read filters plugin in #2355) to be able for downstream projects to change default values or hide arguments to the final user.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3292#issuecomment-316079921:137,down,downstream,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3292#issuecomment-316079921,1,['down'],['downstream']
Availability,"It won't be able to run any faster than BWA mem does with a similar number of cores, since it is essentially just running bwamem. It's potentially faster as part of a spark pipeline so you can load and process data once instead of saving the data to disk and reloading it repeatedly. . The complete list of spark configuration parameters is available on the [spark docs](https://spark.apache.org/docs/3.5.0/configuration.html). Many of them are not relevant in local mode. From what I understand the local mode is going to execute as a single executor with the number of cores specified in the `local[#]` block ( or the total number of system threads if it's set to `*`) It will use the available memory that java is configured with. I'm pretty sure it's ignoring the memory and configuration parameters you've set. Those will be relevant if you configure a stand alone spark cluster (potentially one running exclusively on your local machine). . Our spark tools are not being actively developed for the most part. We've moved away from them to use single threaded tools widely sharded and managed by cromwell. The additional complexity of the spark environment made it hard to see much benefit when most of the tools are embarassingly parallel and easily shardable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8897#issuecomment-2214866066:341,avail,available,341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8897#issuecomment-2214866066,2,['avail'],['available']
Availability,"It's OK, I can still look at this. Sorry for the delay. I don't want @TedBrookings to be distracted from trying to wrap up his change set. If I understand correctly, you'd like to keep the lower threshold of 2 here and then apply the more strict threshold downstream, in `AssemblyContigAlignmentsConfigPicker`? In that case, why make it 2 and not 1, ie. not have a limit at all here?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405610320:256,down,downstream,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405610320,1,['down'],['downstream']
Availability,"It's a little confusing that we use downsampled tumor BAMs for the gCNV WDL tests, for example. Can wait until after release.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4007:36,down,downsampled,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4007,1,['down'],['downsampled']
Availability,"It's a type of quality metrics they designed - quotes below -(https://support.illumina.com/content/dam/illumina-support/documents/documentation/software_documentation/dragen-bio-it/Illumina-DRAGEN-Bio-IT-Platform-User-Guide-1000000141465-00.pdf). . About the chrM, thank you. . Anyway, I don't think the ""VCFAdapterException"" error is related with the chrM, because the JoinGenotype pipeline worked fine with Reblock(snap) + GATK 4.2.5, and they were there (of course we know it was generating wrong data, but we had results to open and check in HAIL). ```; You can use somatic quality (SQ) as the primary metric to describe the confidence with which the caller; made a somatic call. SQ is reported as a format field for the tumor sample. Variants with SQ score; below the SQ filter threshold are filtered out using the weak_evidence tag. To trade off sensitivity; against specificity, adjust the SQ filter threshold. Lower thresholds produce a more sensitive caller and; higher thresholds produce a more conservative caller.; ```. ```; QUAL is not output in the somatic variant records. Instead, the confidence score is FORMAT/SQ.; ##FORMAT=<ID=SQ,Number=1,Type=Float,Description=""Somatic quality"">; The field is specific to the sample. For the tumor samples, it quantifies the evidence that a somatic; variant is present at a given locus.; If a normal sample is also available, the corresponding FORMAT/SQ value quantifies the evidence that; the normal sample is a homozygous reference at a given locus.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7797#issuecomment-1113714714:326,error,error,326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7797#issuecomment-1113714714,2,"['avail', 'error']","['available', 'error']"
Availability,"It's kind of tricky because suppose eg that we have three alt alleles each with an allele qual of 19, so that the overall variant qual is roughly 3x19 = 57. If we filter alleles with a confidence of 20, we get no alleles and the variant qual changes to 0. . Now, if instead of filtering by allele we only filter by overall variant qual we then have to keep an arbitrary number of sketchy alleles. I mean, what if we have 30 alleles each with a qual of 1? The current behavior seems preferable to me because the usual question users would ask downstream is whether some allele is real, not whether some site exhibits variation. As long as we define `-stand-call-conf` to pertain to alleles everything is consistent.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5793#issuecomment-480037602:542,down,downstream,542,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5793#issuecomment-480037602,1,['down'],['downstream']
Availability,"It's likely that this persists in GATK4, but this isn't high priority because in practice we've found that most of our users ignore the spanning deletion alleles or actively dislike them. There are a variety of known issues surrounding spanning deletions, including filtering of * genotypes when the upstream deletion is filtered. I've attached our b37/GRCh37 WGS interval list (no decoy contig), which is split at Ns in the reference. There are 626 intervals. If recovering all the spanning deletion at shard boundaries is important to you, you can use that list to generate your shards and not subdivide further, though I can't guarantee they will be balanced. [wgs_calling_regions.v1.interval_list.txt](https://github.com/broadinstitute/gatk/files/3116941/wgs_calling_regions.v1.interval_list.txt). I had to add a .txt extension for Github, so you'll want to rename it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5905#issuecomment-486663930:464,recover,recovering,464,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5905#issuecomment-486663930,1,['recover'],['recovering']
Availability,"It's on my list. Pretty near the bottom, but it's there. Runtime is probably getting up near an hour for big jobs. The memory requirements are horrific, because we load all the variants into memory and then we don't even use them all! For the biggest cohorts we use 104GB. I wish I was joking. If sklearn can minibatch GMMs then that would be amazing. We use a maximum of 2.5M variants for training and number of annotations/dimensions is O(10). The smallest exome cohort would train with about 80,000 variants with about 3GB of memory. That being said this definitely isn't the biggest cost contributor for joint calling, and hopefully all the sporadic failures have been hammered out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6425#issuecomment-594061198:654,failure,failures,654,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6425#issuecomment-594061198,1,['failure'],['failures']
Availability,It's possible that this is our first example of the much theorized and never before seen splitting failure. Or it could be something much more mundane like an htsjdk bug.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2113#issuecomment-242517867:99,failure,failure,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2113#issuecomment-242517867,1,['failure'],['failure']
Availability,"It's pretty clear at this point that there is a bug in tribble with iteration over block-compressed inputs that lack an index. This is a completely different codepath (and even a different `FeatureReader`) than you get if an index is present. To buy us some time to nail this down, we are going to patch GATK to always require an index for block-compressed tribble files, even if `-L` is not specified. This change will go out in the bug fix release this Friday.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-359855307:276,down,down,276,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-359855307,2,['down'],['down']
Availability,It's really hard to say without data to debug. @Fazulur can you upload a piece of the bam that can reproduce the error? https://gatk.broadinstitute.org/hc/en-us/articles/360035889671,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6552#issuecomment-617199334:113,error,error,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6552#issuecomment-617199334,1,['error'],['error']
Availability,It's up now. Looks like it might have been down for a month! I'll set up an alert on it (we are not actively monitoring non-Prod machines).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3862#issuecomment-346133246:43,down,down,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3862#issuecomment-346133246,1,['down'],['down']
Availability,"It's weird, usually java should output an error message if it runs out of memory. The exception would be when java is assigned so much memory that the SYSTEM kills it instead of java killing itself. ; You could try adding `dmesg | tail -100` to your wdl after m2 runs to see if there are any messages from the OOMkiller. . What's your total available memory on the machine and your -Xmx setting? You typically need to leave some memory room for the OS and other native sofware. (although by default our pipelines SHOULD have that configured correctly.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7494#issuecomment-939070414:42,error,error,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494#issuecomment-939070414,2,"['avail', 'error']","['available', 'error']"
Availability,It's wierd. It should show up as an error. I suspect githubu failed to signal travis or travis was offline.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6782#issuecomment-684043098:36,error,error,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6782#issuecomment-684043098,1,['error'],['error']
Availability,"Its possible to specify CNN inference size argument values that cause the Python process run out of memory, and the failure mode appears to be the java process hangs. Its not clear whether its always possible to recover from this using the global exception handler we currently install on the Python side - we need to explore a bit to see if the handler is being invoked on OOM; whether catching the OOM exception explicitly would help, or if we need an alternative reporting strategy for low-memory conditions. Attached is a log provided by @bhanugandham from a run in a Terra notebook that failed and that exhibited a hang that we assume was due to OOM, and that was resolved by reducing the inference batch size. [gatkStreamingProcessJournal-772629669.txt](https://github.com/broadinstitute/gatk/files/2988819/gatkStreamingProcessJournal-772629669.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5820:116,failure,failure,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5820,2,"['failure', 'recover']","['failure', 'recover']"
Availability,"Its the same optimizations for level as we discussed before, no new optimizations. The previous merge did not include all the optimizations (error on my part)... hence I have to do a new PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4379#issuecomment-364265035:141,error,error,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4379#issuecomment-364265035,1,['error'],['error']
Availability,"I’m sorry I wasn’t really keeping a good trail of the errors and it was a few weeks ago and I have been doing other stuff. . . I have a notation in my log about the run time switch and I seem to have gotten the notion about using it from reading stuff in one of the tutorials or on the forum. I gotta admit I am clueless about what the dictionary validation is actually doing so it wouldn’t have been anything that I conjured up on my own. . . From: Louis Bergelson <notifications@github.com> ; Sent: Wednesday, October 30, 2019 4:13 PM; To: broadinstitute/gatk <gatk@noreply.github.com>; Cc: rdbremel <rdbremel017@gmail.com>; Mention <mention@noreply.github.com>; Subject: Re: [broadinstitute/gatk] Funcotator shuts down (#6182). . Can you point out where in the log you see that? I'm looking at it but I don't see anything about memory in the log you provided. (Except the Runtime.totalMemory()=4523032576 which is just standard output spam from gatk when it shutsdown) Sequence dictionary validation usually happens first, it's strange that a failure in the middle of a run would be effected by it. I'm no very curious what weird thing is happening that's causing this... —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub <https://github.com/broadinstitute/gatk/issues/6182?email_source=notifications&email_token=ANCR2VFFO5775FSQO6EHFX3QRH2HHA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECVZCVA#issuecomment-548114772> , or unsubscribe <https://github.com/notifications/unsubscribe-auth/ANCR2VA3C2XW4BZEI5YNGITQRH2HHANCNFSM4I2MRFQA> . <https://github.com/notifications/beacon/ANCR2VFOYRDVUGP66IIIVHDQRH2HHA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECVZCVA.gif>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548119939:54,error,errors,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548119939,3,"['down', 'error', 'failure']","['down', 'errors', 'failure']"
Availability,"I’ve also revisited this work for MalariaGEN, additionally including further cleanup of the canonical part of the WDLs (mostly low hanging fruit like adding structs, which help a lot for cutting down parameter cruft on Terra). For ease of iteration, this work broke things up into 3 pushes of a button: 1) data collection, 2) preclustering (done in a relatively modular way, so you can swap in whatever clustering script you like, as long as it outputs hard/soft responsibilities) +random selection of training cohorts, and 3) cohort mode + scattered case mode on all clusters. But no reason we couldn’t link some of those up. No problem running 16k samples, with 6 clusters and 300 training samples per, but also note I was only running a single genomic shard containing CNVs of interest for this use case. (I did manage to break Terra for a few days when I tried to attach collected counts to the data model in what I would’ve thought would be a relatively trivial way, but that’s another matter.). I’ve shared some version of these WDLs over Slack previously, but happy to also open up a branch here. I think some of this work may be replicated in GATK-SV and I’m also not sure what we want to make canonical. Surely most users will run only a single cluster. But from the perspective of our MalariaGEN collaborators, the more of what I put together for them being made canonical, the better, as this will ease future maintainability. But will leave it up to other current stakeholders.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5633#issuecomment-894527360:195,down,down,195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5633#issuecomment-894527360,1,['down'],['down']
Availability,JAVA doc error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466:9,error,error,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466,1,['error'],['error']
Availability,"JDK Defaults.COMPRESSION_LEVEL : 2; 23:15:30.439 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:15:30.440 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:15:30.440 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:15:30.440 INFO AnalyzeCovariates - Deflater: IntelDeflater; 23:15:30.440 INFO AnalyzeCovariates - Inflater: IntelInflater; 23:15:30.440 INFO AnalyzeCovariates - GCS max retries/reopens: 20; 23:15:30.440 INFO AnalyzeCovariates - Requester pays: disabled; 23:15:30.440 INFO AnalyzeCovariates - Initializing engine; 23:15:30.440 INFO AnalyzeCovariates - Done initializing engine; 23:15:30.790 INFO AnalyzeCovariates - Generating csv file '/tmp/AnalyzeCovariates6611620304443967041.csv'; 23:15:30.854 INFO AnalyzeCovariates - Generating plots file '/researchers/sebastian.hollizeck/lowcWGS/IN-PM01004/Bam/AnalyzeCovariates.pdf'; 23:15:31.932 INFO AnalyzeCovariates - Shutting down engine; [January 19, 2020 11:15:31 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2161115136; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.2074992987327687075';source('/tmp/BQSR.6874121927957307421.R'); /tmp/AnalyzeCovariates6611620304443967041.csv /researchers/sebastian.hollizeck/lowcWGS/IN-PM01004/Bam/IN-PM01004_rmd.recal.bam.recalTable /researchers/sebastian.hollizeck/lowcWGS/IN-PM01004/Bam/AnalyzeCovariates.pdf; Stdout: WARNING: ignoring environment value of R_HOME. Stderr: During startup - Warning messages:; 1: Setting LC_CTYPE failed, using ""C"" ; 2: Setting LC_COLLATE failed, using ""C"" ; 3: Setting LC_TIME failed, using ""C"" ; 4: Setting LC_MESSAGES failed, using ""C"" ; 5: Setting LC_MONETARY failed, using ""C"" ; 6: Setting LC_PAPER failed, using ""C"" ; 7: Setting LC_MEASUREMENT failed, using ""C"" ; Error in rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6393:3050,down,down,3050,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6393,1,['down'],['down']
Availability,"Java 17.0.12 from [Oracle](https://www.oracle.com/java/technologies/downloads/#java17) seems to display the same behavior. ```; 12:19:27.622 INFO ProgressMeter - Scaffold_1:21175995 247.8 125320 505.8; 12:19:49.612 INFO ProgressMeter - Scaffold_1:21178224 248.1 125330 505.1; 12:20:02.383 INFO ProgressMeter - Scaffold_1:21179909 248.4 125340 504.7; 12:20:14.545 INFO ProgressMeter - Scaffold_1:21183582 248.6 125360 504.4; 12:20:25.422 INFO ProgressMeter - Scaffold_1:21255583 248.7 125670 505.2; 12:20:36.810 INFO ProgressMeter - Scaffold_1:21281660 248.9 125810 505.4; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f4ad4d94291, pid=3638446, tid=3638447; #; # JRE version: Java(TM) SE Runtime Environment (17.0.12+8) (build 17.0.12+8-LTS-286); # Java VM: Java HotSpot(TM) 64-Bit Server VM (17.0.12+8-LTS-286, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xcf291] __memset_avx2_erms+0x11; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e"" (or dumping to /bigdata/operations/ejaco020/gatk/core.3638446); #; # An error report file with more information is saved as:; # /bigdata/operations/ejaco020/gatk/hs_err_pid3638446.log; #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. I also attempted running within a singularity container, allocating 64GB of memory to the job and specifying -Xmx60G. Still seemed to silently ""crash"". Command I ran was:; ```; singularity run gatk_4.6.0.0.sif gatk HaplotypeCaller --java-options -Xmx60G -R /rhome/ejaco020/bigdata/gatk/Cclementina_182_v1_2.fa -I AlignedCalToCcl_Scaffolds_MarkDupOut.bam \ ; -O sing.vcf.gz \ ; -ERC GVCF; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8988#issuecomment-2389450721:68,down,downloads,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8988#issuecomment-2389450721,3,"['down', 'error']","['downloads', 'error']"
Availability,Java heap space error (java.lang.OutOfMemoryError) in mutect2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5900:16,error,error,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5900,1,['error'],['error']
Availability,"Java implementation of segmentation is now in the sl_wgs_segmentation dev branch, with a few simple unit tests. I'll expand on these and add tests for denoising in the future, but for now we have a working revised pipeline up through segmentation. The CLI is simply named ModelSegments (since my thinking is that it could eventually replace ACNV). I ran it on some old denoised exomes. Runtime is <10s, comparable to CBS. Here's a particularly noisy exome:. CBS found 1398 segments:; ![cbs](https://user-images.githubusercontent.com/11076296/30165095-cdf6251a-93ac-11e7-91fb-dcc8f48fe07f.png). Kernel segmentation with a penalty given by a = 1, b = 0 found 1018 segments:; ![kern](https://user-images.githubusercontent.com/11076296/30165106-dbbe0b40-93ac-11e7-99ec-5d58d8417d8b.png). Kernel segmentation with a penalty given by a = b = 1 (which is probably a reasonable default penalty, at least based on asymptotic theoretical arguments) reduced this to 270 segments :; ![kern-smooth](https://user-images.githubusercontent.com/11076296/30165113-e2b545a8-93ac-11e7-97a9-a692e43ebbdf.png). The number of segments can similarly be controlled in WGS. WGS runtime is ~7min for 250bp bins, ~30s of which is TSV reading, and there is one more spot in my implementation that could stand a bit of optimization, which might bring the runtime down. In contrast, I kicked off CBS 45 minutes ago, and it's still running... @LeeTL1220 this is probably ready to hand off to you for some WDL writing and preliminary evaluation. ; Although I can't guarantee that there aren't bugs, I ran about ~80 exomes with no problem. We can talk later today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936:1333,down,down,1333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936,2,['down'],['down']
Availability,Java related error encountered while running gatk PathSeqPipelineSpark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802:13,error,error,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802,1,['error'],['error']
Availability,Javadoc update: minor error in the documentation regarding --genotyping-mode,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5657:22,error,error,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5657,1,['error'],['error']
Availability,Jenkins seems to have gone down completely or me now. I assume it's related?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2180#issuecomment-248319705:27,down,down,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2180#issuecomment-248319705,1,['down'],['down']
Availability,JointDiscovery Workflow Errors due to Java Heap Space?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6165:24,Error,Errors,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6165,1,['Error'],['Errors']
Availability,"Jumping on to report I've also been having issues using GATK 4 genotyping. CombineGVCFs (v4.0.1.1) runs fine no matter how many samples I use, but GenotypeGVCFs chokes at some limit. I iteratively subsetted my sample list to see at what point it begins to choke. I used line count of the final vcf file as an approximation of how far the genotyper got. Even at the 110 samples, though, CombineGVCFs ran for several hours. It just produced a final, genotyped VCF file that was severely truncated with few variants. See the graph at this image attached.; ![iterative memory loss](https://user-images.githubusercontent.com/5849554/37933389-e15935ac-30ff-11e8-91ea-80fffd8deb48.png). As for the errors:. `Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded`; and several; `at java.util....` and `at org.broadinstitute.hellbender.tools.walkers.genotyper....`. I attached three of these error files so you can see the full list of memory problems. `genotype55.e...` is the log file with the most number of samples that worked. `genotype60.e...` ends abruptly. `genotype65.e...` contains the memory errors. Note that these error/log files include both CombineGVCFs and GenotypeGVCFs. [genotype55.e5195822.txt](https://github.com/broadinstitute/gatk/files/1849559/genotype55.e5195822.txt); [genotype60.e5195820.txt](https://github.com/broadinstitute/gatk/files/1849560/genotype60.e5195820.txt); [genotype65.e5195821.txt](https://github.com/broadinstitute/gatk/files/1849561/genotype65.e5195821.txt). For reference, genotyping using GATK 3.8.0 on all 108 of my samples produced a final vcf file 2784 lines long in *36 seconds* with *no issue.* Let me know if you have any other questions!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-376314262:691,error,errors,691,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-376314262,4,['error'],"['error', 'errors']"
Availability,"Just a minor annoyance:. ```/home/slee/.pyenv/versions/anaconda3-5.3.1/envs/gatk/lib/python3.6/site-packages/gcnvkernel/io/io_commons.py:394: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.; mu_slice = mu_all[_get_singleton_slice_along_axis(mu_all, var_sample_axis, sample_index)]; /home/slee/.pyenv/versions/anaconda3-5.3.1/envs/gatk/lib/python3.6/site-packages/gcnvkernel/io/io_commons.py:395: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.; std_slice = std_all[_get_singleton_slice_along_axis(mu_all, var_sample_axis, sample_index)]```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6226:385,error,error,385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6226,2,['error'],['error']
Availability,"Just a minor fix, but could conceivably change results by keeping/dropping samples/intervals on the edge of the filter. See discussion in https://gatk.broadinstitute.org/hc/en-us/community/posts/360057785591-Error-while-running-CreateReadCountPanelOfNormals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6624:208,Error,Error-while-running-CreateReadCountPanelOfNormals,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6624,1,['Error'],['Error-while-running-CreateReadCountPanelOfNormals']
Availability,"Just a quick test of `--splitMultiallelics` looks good:; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250_splitmultiallelics.vcf.gz --splitMultiallelics; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250_splitmultiallelics.vcf.gz --splitMultiallelics; 17:52:19.004 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 05, 2018 5:52:19 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 17:52:19.130 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 17:52:19.131 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-24-gb43bc27-SNAPSHOT; 17:52:19.131 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:52:19.131 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 17:52:19.131 INFO LeftAlignAndTrimVariants ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418893971:170,Down,Downloads,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418893971,2,['Down'],['Downloads']
Availability,"Just another message reiterating that a VCF should follow the specs of a VCF. Default for missing data should be ./. . Having the default as 0/0, apart from being nonsensical, is likely to generate a lot of downstream errors in many papers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1932330312:207,down,downstream,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1932330312,2,"['down', 'error']","['downstream', 'errors']"
Availability,"Just as a note, we included that sample because it reportedly has ~25% contamination. Not sure if the contamination downsampling comes into play at, but it is something that's distinctive about this bam.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-624856521:116,down,downsampling,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-624856521,1,['down'],['downsampling']
Availability,"Just as a note. This change in line 251 of [`gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RMSMappingQuality.java`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RMSMappingQuality.java) of function `parseRawDataString`:. ```; final int squareSum = Integer.parseInt(parsed[SUM_OF_SQUARES_INDEX]);; ```. results in the following type of GVCF/VCF parsing error:. ```; A USER ERROR has occurred: Bad input: malformed RAW_MQ annotation: 3415207168,1749038; ```. when the `RAW_MQ` first index is greater than `Integer.MAX_VALUE`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4969#issuecomment-439642813:452,error,error,452,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4969#issuecomment-439642813,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,Just as feedback we use gcs nio too in Cromwell and have had to add retries around this error as it has popped up every now and then.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300269489:88,error,error,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300269489,2,['error'],['error']
Availability,"Just came across this again -- the gatk wrapper is in the path now and available from wherever, so I think this ticket is satisfied. Feel free to reopen if I'm wrong.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3899#issuecomment-449661421:71,avail,available,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3899#issuecomment-449661421,1,['avail'],['available']
Availability,"Just checked and this is not new behavior. I was afraid this would be an unintended consequence of the DRAGEN branch but that doesn't seem to be the case. Looking a little closer into the code I actually think the only difference this makes is explicitly for the contamination and nothing else. There are a few layers where we filter reads before the annotations are called (filtered by QC before active region determination, MQ/etc-filtered/un-softclipped/overlap-score-adjusted before assembly, filtered based on poor concordance with existing haplotypes, reads are realigned and re-filtered by position, and again filtered due to contamination downsampling). It seems to me that the two likelihoods arrays fed to the `prepareReadAlleleLikelihoodsForAnnotation()` have had all of the above steps applied to them EXCEPT for the contamination downsampling step applied to them looking thorough the code in the HaplotypeCaller. I guess the question is whether there is a strong reason to make the annotations with/without the contamination adjustment... I think the argument `--use-filtered-reads-for-annotations` is misleadingly labeled though the description looks correct to me since it really does only seem to make a difference for the contamination step. . There is another wrinkle to all of this. For DRAGEN-GATK we evaluated calculating the overlaps/annotations exactly how they do it in DRAGEN and decided against it. In DRAGEN they retain the original reads from the bam (i.e. no realignment/no score adjustments/etc...) and use THOSE for annotation and for genotyping. I would have to pick through their debug output to tell just which subsets get used for genotyping (for example they still use non-haplotype-matching reads for FRD and BQD but I don't remember if those are also used for calculating annotations).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7144#issuecomment-800380324:647,down,downsampling,647,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7144#issuecomment-800380324,2,['down'],['downsampling']
Availability,"Just finished switching over all of the CNV tools to fail early if directories are not writeable---or do not exist and cannot be created---only to realize that this behavior is inconsistent with that of Picard IntervalListTools (which is used in the gCNV pipeline). That tool fails early if the output directory is not writeable or does not exist, and although there is a code path later that suggests that output directories should be created, it is not reached due to this early fail. It might be that this inconsistency was introduced in https://github.com/broadinstitute/picard/pull/1208 and I did not catch it in my PR review. @yfarjoun any opinions what the intended behavior should be? Are there any conventions for Picard tools in general?. Perhaps we could enforce this at the engine level (maybe checks that are triggered by annotations such as suggested in https://github.com/broadinstitute/gatk/issues/141, if possible)? But this would only work for GATK tools and would still rely on the diligence of developers. In any case, I'll decide on and document a convention for the CNV tools, but I think it might be a quixotic dream to enforce consistent behavior---especially without breaking things downstream which may rely on existing, inconsistent behavior...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-469067676:1208,down,downstream,1208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-469067676,1,['down'],['downstream']
Availability,"Just for information, when do you think a new release with this corrected bug will be available (possibly also in bioconda) ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-466662922:86,avail,available,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-466662922,1,['avail'],['available']
Availability,"Just for posterity:. jhess 1:55 PM; just to clarify: what is the logic behind approximating σ_(τ/min/maj) ≈ (post90 - post10)/2? (edited) ; 1:55; what is the scale factor of 1/2 for?; 1:58; one other thing — how come σ_(min/maj) is the sum of the total CR segment’s variance (i.e. σ_τ) and the allelic segment’s variance?; 2:00; this would imply that the allelic segments are actually a sum of the random variables corresponding to the allelic and total segmentation, which I’m not sure is the case?. slee 5:32 PM; Sorry, just now seeing your questions!; 5:33; The scale factor of 1/2 is pretty arbitrary. Just trying to give an estimate of posterior width when the credible interval might be skewed. A better approach would be to refit posteriors with Gaussians/Betas as mentioned previously.; 5:35; However, I'm not actually convinced that these credible intervals are what we want to pass to the sigmas. As I also mentioned above, if sigma.tau is supposed to be a global quantity, probably the posterior median of the parameter that controls the global variance (given in the .cr.params file) might be a better thing to use. However, I never got a straight answer from anybody about whether this was a segment-level or global quantity---any idea?. slee 5:41 PM; As for using the sum of the CR variance and the MAF variance, you're right---we should be propagating error for the product of the two random variables. Not sure what I was thinking...probably just a brain fart. Not sure if it will make a difference for ABSOLUTE, but thanks for catching that!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5804#issuecomment-652411494:1367,error,error,1367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5804#issuecomment-652411494,1,['error'],['error']
Availability,"Just for the record, as referenced in Slack, I managed to track down the previous PR in which this code was saved from the chopping block: https://github.com/broadinstitute/gatk/pull/4181 But our current reasons are certainly compelling enough---we'll pour one out for Mehrtash's beloved solvers!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7922#issuecomment-1170120168:64,down,down,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7922#issuecomment-1170120168,1,['down'],['down']
Availability,"Just found the log4j errors could be fixed by editing `gatk-launch` and set `spark.driver.userClassPathFirst` to `false` (was `true` by default), or add `--conf spark.driver.userClassPathFirst=false` to gatk-launch command line.; Not sure if that would have any unexpected effect though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-315969501:21,error,errors,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-315969501,1,['error'],['errors']
Availability,"Just got an update by email from the GP team indicating that their perspective is that these are valid index files. It seems that this issue lives in a liminal space where Mutect2 is crashing due to reasons that are not a GATK bug, but also not due to any formal problem in the GP pipeline either. Still, I'm running Broad-built software on recently sequenced Broad-provided genetic data, so it seems a little odd that these aren't playing well together. Trying to think of a solution so that other people in this situation aren't left confused by the error message, is this error specific enough that an additional statement could be added to the error message, e.g., ""This error may be triggered by CRAM indexes produced by older implementations of `htsjdk`, in which case reindexing with an updated `htsjdk` may resolve this problem.""? Or is this error more general, making such a suggestion inappropriate?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7755#issuecomment-1099607060:552,error,error,552,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7755#issuecomment-1099607060,5,['error'],['error']
Availability,"Just making sure 😛 . I don't think you have to download them again, but I have seen SQLite do some strange things on NFS drives sometimes. When I looked for the issue a [couple](https://stackoverflow.com/questions/29244788/error-disk-i-o-error-on-a-newly-created-database) [StackOverflow posts](https://stackoverflow.com/questions/41744631/sqlite-jdbc-error-sqlite-ioerr-lock) indicated it was a SQLite + NFS issue. If you have a local disk you can store the data sources on that would probably fix the issue immediately. I'm not sure what the exact problem is with NFS + SQLite, unfortunately.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661899426:47,down,download,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661899426,3,"['down', 'error']","['download', 'error-disk-i-o-error-on-a-newly-created-database', 'error-sqlite-ioerr-lock']"
Availability,"Just saw one of these in the log that I'm going to go investigate. The new error message is very helpful, thanks @kgururaj !",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3745#issuecomment-364159215:75,error,error,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3745#issuecomment-364159215,1,['error'],['error']
Availability,"Just some notes before I forget:. Using these test samples, I made some tweaks to the ploidy model that made it more robust to incorrect ploidy calls and added a simple modeling of mosaicism:. ```` ; # per-contig bias; bias_j = Gamma('bias_j',; alpha=100.0,; beta=100.0,; shape=(ploidy_workspace.num_contigs,)); norm_bias_j = bias_j / tt.mean(bias_j). # per-sample depth; depth_s = Uniform('depth_s',; lower=0.0,; upper=10000.0,; shape=(ploidy_workspace.num_samples,)); ; # per-sample probability of mosaicism; pi_mosaicism_s = Beta(name='pi_mosaicism_s',; alpha=1.0,; beta=50.0,; shape=(ploidy_workspace.num_samples,)). # per-sample-and-contig mosaicism factor; f_mosaicism_sj = Beta(name='f_mosaicism_sj',; alpha=10.0,; beta=1.0,; shape=(ploidy_workspace.num_samples, ploidy_workspace.num_contigs,)); norm_f_mosaicism_sj = f_mosaicism_sj / tt.max(f_mosaicism_sj, axis=1).dimshuffle(0, 'x'). # per-contig mapping error; eps_j = HalfNormal('eps_j', sd=0.01, shape=(ploidy_workspace.num_contigs,)). # negative-binomial means; mu_sjk = depth_s.dimshuffle(0, 'x', 'x') * t_j.dimshuffle('x', 0, 'x') * norm_bias_j.dimshuffle('x', 0, 'x') * \; (ploidy_workspace.int_ploidy_values_k.dimshuffle('x', 'x', 0) + eps_j.dimshuffle('x', 0, 'x')); mu_mosaic_sjk = norm_f_mosaicism_sj.dimshuffle(0, 1, 'x') * mu_sjk. # ""unexplained variance""; psi = Uniform(name='psi', upper=10.0). # convert ""unexplained variance"" to negative binomial over-dispersion; alpha = tt.inv((tt.exp(psi) - 1.0)). def _get_logp_sjk(_n_sj):; _logp_sjk = logsumexp([tt.log(1 - pi_mosaicism_s.dimshuffle(0, 'x', 'x')) + commons.negative_binomial_logp(mu_sjk, alpha.dimshuffle('x', 'x', 'x'), _n_sj.dimshuffle(0, 1, 'x')),; tt.log(pi_mosaicism_s.dimshuffle(0, 'x', 'x')) + commons.negative_binomial_logp(mu_mosaic_sjk, alpha.dimshuffle('x', 'x', 'x'), _n_sj.dimshuffle(0, 1, 'x'))],; axis=0)[0]; return _logp_sjk. DensityDist(name='n_sj_obs',; logp=lambda _n_sj: tt.sum(q_ploidy_sjk * _get_logp_sjk(_n_sj), axis=2),; observed=n_sj); ````. Brie",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-371334890:117,robust,robust,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-371334890,4,"['error', 'robust']","['error', 'robust']"
Availability,"Just splitting off a chunk of @vruano's ideas in #264 here:. We start threading at the first unique kmer of each read (sequence). There are at least two problems with this. First, since we track unique kmers as we go the resulting graph may depend on the order in which reads were threaded. Second, we are throwing away information at the beginning of the read before the first unique (and existing) k-mer in each sequence is found. This is only partly fixed when we recover dangling heads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4942:467,recover,recover,467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4942,1,['recover'],['recover']
Availability,"Just to add to Kushal's point:. Let's say there are two partitions:; part_0: chr1:[0, 1M]; part_1: chr1:[1M, 2M]. Suppose the user specifies -L chr1:0.5M-1.5M, should we; (a) Flag an error?; (b) Write to 2 partitions?. It's possible to implement either option. It's still the user's responsibility to ensure that data goes to the 'correct' partition. For example, if the expectation is that each machine hosts a single partition, then option (b) will create both partitions while writing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277386859:183,error,error,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277386859,1,['error'],['error']
Availability,"Just to clarify -- your proposed approach is certainly useful for filtering large germline events, etc, and must be part of the solution (for more robustness). What I'm saying here is that some elementary filtering gives us great mileage, both in terms of our ease of mind in determining the hard-filtering region in the ploidy tool, and for reducing false calls.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375919317:147,robust,robustness,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375919317,1,['robust'],['robustness']
Availability,"Just to clarify, I think argsets without the ability to override individual parameters would be fine. Although it’s probably not difficult to implement, I think allowing overrides introduces unnecessary possibilities for error and confusion. If you are a user that wants to just go with one of the provided argsets, fine; if you’re a user that wants to set all parameters individually, also fine. Will leave it up to engine team and Comms if they want to support overrides, though. In any case, when these defaults represent sweet spots for *Broad-generated data*, let’s communicate that in documentation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-391014975:221,error,error,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-391014975,1,['error'],['error']
Availability,"Just to clarify, the issue described was not in the gVCF produced by HaplotypeCaller. The issue appears to be introduced during the GenotypeGVCF step. Below is our HaplotypeCaller Command:. gatk HaplotypeCaller -I {input_bam} -O {output_gvcf} -R {ref} \; --pair-hmm-implementation AVX_LOGLESS_CACHING --emit-ref-confidence GVCF --max-reads-per-alignment-start 0 \; --dbsnp {dbsnp} -L {region} \; -A AlleleFraction -A AS_BaseQualityRankSumTest -A AS_FisherStrand -A AS_InbreedingCoeff \; -A AS_MappingQualityRankSumTest -A AS_QualByDepth -A AS_ReadPosRankSumTest -A AS_RMSMap\pingQuality \; -A AS_StrandOddsRatio -A BaseQuality -A BaseQualityRankSumTest -A ChromosomeCounts -A ClippingRankSumTest \; -A CountNs -A Coverage -A DepthPerAlleleBySample -A DepthPerSampleHC -A ExcessHet -A FisherStrand \; -A FragmentLength -A GenotypeSummaries -A InbreedingCoeff -A LikelihoodRankSumTest \; -A MappingQuality -A MappingQualityRankSumTest -A MappingQualityZero -A OrientationBiasReadCounts \; -A OriginalAlignment -A PossibleDeNovo -A QualByDepth -A ReadPosition -A ReadPosRankSumTest \; -A ReferenceBases -A RMSMappingQuality -A SampleList -A StrandBiasBySample -A StrandOddsRatio \; -A TandemRepeat -A UniqueAltReadCount",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6938#issuecomment-729104689:575,ping,pingQuality,575,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6938#issuecomment-729104689,1,['ping'],['pingQuality']
Availability,"Just to make sure I understand the issue---will this cause technical problems in the Firecloud environment, or is it more of a style issue?. If the latter, one reason I prefer the use of optional file inputs to trigger tool-level ""modes"" when possible is that it propagates more naturally from the tool level. For example, let's consider a tool that can operate in either tumor-only or matched-pair mode. It is natural at the tool level to make the tumor a required input and the normal optional. The other options are quite awkward: 1) make both inputs required and switch between using the normal or not with a flag (in which case it is very easy for the user to shoot themselves in the foot if they forget to set the flag right, and we'd have to pass a dummy normal every time we want to run tumor only if we don't actually have a pair), 2) leave the normal as optional but add a flag anyway, which would be redundant and require an additional validation (i.e., if the flag is set to matched mode but we don't have a normal, we should fail early), or 3) write separate tools for each mode with the corresponding required inputs. If we accept that optional file input is the way to handle such a scenario at the tool level but not at the workflow level, then we will simply run into the same problems at the workflow level. I'm sure there are more complex scenarios when triggering on file presence/absence doesn't uniquely specify a workflow, in which case flags are a must. But for simple scenarios, I'm not sure why we shouldn't take advantage of the ability to specify optional file inputs in WDL (actually, I'm not sure how else we are supposed to use them?). However, if this is a problem for Firecloud, then I'd like to understand why---and what possible solutions there might be.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334046444:911,redundant,redundant,911,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334046444,2,['redundant'],['redundant']
Availability,"Just to provide additional context, all of the machinery in the AbstractRecordCollection classes for reading/writing CNV input/output TSVs was meant to make passing metadata (dictionaries, sample names, etc.) from tool to tool as automatic and consistent as possible. This avoids having to re-provide sample names, dictionaries, etc. at each tool/step---which often led to dictionary inconsistencies, contig/sample ordering bugs, etc. in older versions of the pipelines---at the cost of 1) redundantly carrying along this metadata in input/output TSVs, and 2) requiring consistent dictionaries in all initial BAM inputs. I think these are small costs to pay.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6957#issuecomment-726973610:490,redundant,redundantly,490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6957#issuecomment-726973610,1,['redundant'],['redundantly']
Availability,"Just wanted to add - I just tried installing the GATK docker as described here: https://gatk.broadinstitute.org/hc/en-us/articles/360035889991--How-to-Run-GATK-in-a-Docker-container. As I'd think that all software dependencies and whatnot should be fine. However, I still get the same error message:. /gatk/./gatk --java-options ""-Xmx25g"" SplitNCigarReads \; > -R Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam \; > --tmp-dir /gatk/my_data/temp -O thing.bam; Using GATK jar /gatk/gatk-package-4.1.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx25g -jar /gatk/gatk-package-4.1.3.0-local.jar SplitNCigarReads -R Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam --tmp-dir /gatk/my_data/temp -O thing.bam. 21:12:14.158 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 02, 2023 9:12:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:12:16.383 INFO SplitNCigarReads - ------------------------------------------------------------; 21:12:16.384 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.3.0; 21:12:16.384 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:12:16.384 INFO SplitNCigarReads - Executing as root@9d399eec0e24 on Linux v5.19.0-32-generic amd64; 21:12:16.384 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 21:12:16.384 INFO SplitNCigarReads - Start Date/Time: March 2, 2023 9:12:14 PM UTC; 21:12:16.385 INFO SplitNCigarReads - ------------------------------------------------------------; 21:12:16.385 INFO SplitNCigarReads - ----------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452564826:285,error,error,285,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452564826,1,['error'],['error']
Availability,"Just wanted to mention, I'm able to get this error when I use --af-of-alleles-not-in-resource 0.00003125. Noticed the edge case just mentioned 0, but if this hasn't already been dealt with in this fix, I can provide more information. . Edit: I've noticed that this happens when I supply -I tumor.bam -I normal.bam -tumor SAMPLE1 and I don't specify a -normal SAMPLE2. If I do include the -normal SAMPLE2, it works fine without error. Not sure if intended or not.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5563#issuecomment-452022519:45,error,error,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5563#issuecomment-452022519,2,['error'],['error']
Availability,"Justin Rhoades of the blood biopsy team noticed a bug in the Mutect2 pipeline: if the number of scatters was sufficiently high, the last chunk of intervals did not contain any autosomal contigs, and therefore `GetPileupSummaries` was run on an empty interval for that scatter, throwing an error. For the pipeline there is no reason not to allow this empty interval and consequent empty output because it gets merged with other output later in the pipeline. It also seems that this is a generic feature of scattered jobs -- empty intersection of intervals need not imply a user error. Therefore, I added an argument to `IntervalArgumentCollection` to allow empty intervals. Since the change to the Mutect2 pipeline is tiny the primary need in code review is to judge whether the changes to `IntervalArgumentCollection` are acceptable. @lbergelson could you look at this PR?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6209:289,error,error,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6209,2,['error'],['error']
Availability,"K Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:16:35.498 INFO GenotypeGVCFs - Deflater: IntelDeflater; 21:16:35.499 INFO GenotypeGVCFs - Inflater: IntelInflater; 21:16:35.499 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 21:16:35.499 INFO GenotypeGVCFs - Requester pays: disabled; 21:16:35.499 INFO GenotypeGVCFs - Initializing engine; 21:16:36.737 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed.; 21:16:38.472 INFO GenotypeGVCFs - Shutting down engine; [January 17, 2021 9:16:38 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=2551709696; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:410); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:326); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:67); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:709); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.onStartup(VariantLocusWalker.java:63); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLine",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839:4849,ERROR,ERROR,4849,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839,1,['ERROR'],['ERROR']
Availability,"K version in the ExcessHet documentation to 4.2.2.0, but we'll see if I need to revisit that.; - [x] Not quite sure about the `ReducibleAnnotation` business. Let me know how to make these changes, or else happy to punt and file an issue.; - [ ] Also not sure I've parsed the results of the Jenkins tests, at least in terms of comparing how many sites get hard filtered with/out the change. Where should I be looking at to see the baseline result for that step? Also looks like a lot of results for https://gotc-jenkins.dsp-techops.broadinstitute.org/job/warp-workflow-tests/11755/ were call-cached, is that to be expected? Haven't looked at these tests before, so maybe you can walk me through them at some point. But I guess we can be sure that the overall results don't change too much (at least for 50 samples), which is a good start.; - [x] Didn't quite get to making those plots of the change in decision boundary, will do that tomorrow or later this week. EDIT: Nevermind, took like 5 minutes to throw them together (albeit using the slow python implementation and some for loops...), see below.; - [x] Hmm, looks like my own PR #6885 might've introduced a few more exact match test failures...grr. Here are some plots for N = 50, 100, and 500 samples showing (in black) those counts that previously fell under the 3E-6 threshold with the mid-p correction but now pass without it. As you can see, not much to sweat from these ""theoretical"" plots, but good to convolve with the actual allele frequency spectrum and get an idea of how many sites occupy these black squares in practice (as well as start us down the road of reexamining the threshold itself):. ![image](https://user-images.githubusercontent.com/11076296/132413689-37f3dfeb-e3f5-4869-a803-fe27f3cd79bd.png); ![image](https://user-images.githubusercontent.com/11076296/132413649-d716ee7d-6763-4275-82de-e0e226dcb1de.png); ![image](https://user-images.githubusercontent.com/11076296/132413673-11f54f3f-975c-46ab-b874-ec37e461694f.png)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-914612907:1319,failure,failures,1319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-914612907,2,"['down', 'failure']","['down', 'failures']"
Availability,"K/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/ngs/; datanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50075. volumes:; datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:2621,failure,failure,2621,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['failure'],['failure']
Availability,"KI270448v1, chrUn_KI270521v1, chrUn_GL000195v1, chrUn_GL000219v1, chrUn_GL000220v1, chrUn_GL000224v1, chrUn_KI270741v1, chrUn_GL000226v1, chrUn_GL000213v1, chrUn_KI270743v1, chrUn_KI270744v1, chrUn_KI270745v1, chrUn_KI270746v1, chrUn_KI270747v1, chrUn_KI270748v1, chrUn_KI270749v1, chrUn_KI270750v1, chrUn_KI270751v1, chrUn_KI270752v1, chrUn_KI270753v1, chrUn_KI270754v1, chrUn_KI270755v1, chrUn_KI270756v1, chrUn_KI270757v1, chrUn_GL000214v1, chrUn_KI270742v1, chrUn_GL000216v2, chrUn_GL000218v1, chrEBV]; features contigs = [X, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]"". The VCF that I have uses just numbers for chromosomes, whereas the reference genome uses chr1, chr2, etc. Both naming conventions are valid. This is a 4.3 VCF. I have read https://gatk.broadinstitute.org/hc/en-us/articles/360035891131-Errors-about-input-files-having-missing-or-incompatible-contigs and this seems to be the same issue, but I believe there should be a translation that happens, e.g. 1 -> chr1 or the reverse as well. #### Steps to reproduce; Ran the following command using a VCF and reference file that I have:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.2.0-local.jar LeftAlignAndTrimVariants --max-indel-length 500 --max-leading-bases 2000 --dont-trim-alleles false --verbosity DEBUG --variant <input vcf file> --output /data/<vcf_output> --reference /data/<reference file> --split-multi-allelics true. #### Expected behavior; I would expect GATK to be able to translate 1 -> chr1, 2 -> chr2, etc. since both naming conventions are valid according to the VCF spec http://samtools.github.io/hts-specs/VCFv4.3.pdf. When running the same exact command on a VCF file that uses chr1, chr2, etc. as the naming convention the command runs successfully. #### Actual behavior; GATK exits and gives error message mentioned in description.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7538:5320,error,error,5320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7538,1,['error'],['error']
Availability,"KTRACE\_ON\_USER\_EXCEPTION=true -jar /nfs/fs1/bioinfo/apps-x86\_64/GATK/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar GenomicsDBImport --genomicsdb-workspace-path /tmp/tmp.ceRdvv/GDB --intervals chr1:5149001-5201000 --tmp-dir /tmp/tmp.ceRdvv/GDB\_tmp --sample-name-map /tmp/tmp.ceRdvv/snmap --batch-size 100 --reader-threads 17. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Djava.io.tmpdir=/tmp/tmp.ceRdvv -Xmx71680M -Xms71680M -jar /nfs/fs1/bioinfo/apps-x86\_64/GATK/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar GenotypeGVCFs --genomicsdb-use-vcf-codec -R /odinn/data/extdata/1000genomes/2019-06-21\_GRCh38/GRCh38\_full\_analysis\_set\_plus\_decoy\_hla.fa -V gendb:///tmp/tmp.ceRdvv/GDB --tmp-dir=/tmp/tmp.ceRdvv --interval-padding 1000 --only-output-calls-starting-in-intervals -L chr1:5161113-5163890 -O /tmp/tmp.ceRdvv/splitdir/reg\_5.padded.vcf.gz. **c) The entire error log if applicable.**. Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx1290240M -Xms1290240M -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true -jar /nfs/fs1/bioinfo/apps-x86\_64/GATK/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar GenomicsDBImport --genomicsdb-workspace-path /tmp/tmp.ceRdvv/GDB --intervals chr1:5149001-5201000 --tmp-dir /tmp/tmp.ceRdvv/GDB\_tmp --sample-name-map /tmp/tmp.ceRdvv/snmap --batch-size 100 --reader-threads 17 ; ; 20:05:36.112 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/nfs/fs1/bioinfo/apps-x86\_64/GATK/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jul 27, 2020 8:05:40 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 20:05:40.627 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6742:2398,error,error,2398,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6742,1,['error'],['error']
Availability,"KTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /gatk/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.0.0-local.jar BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz; ```. I downsampled the fastq files and got similar results.; However, when giving only the reduced known-sites file (`--known-sites snp151flagged_tablebrowser.bed.bgz`) and specifying two intervals (`--intervals chr22 --intervals chrY`), it worked. I attached the downsampled bam file and the reduced known-sites file [here](https://gatkforums.broadinstitute.org/gatk/discussion/comment/57049/#Comment_57049), and the reference file can be found [here](ftp://ftp.ncbi.nlm.nih.gov/genomes/archive/old_genbank/Eukaryotes/vertebrates_mammals/Homo_sapiens/GRCh38/seqs_for_alignment_pipelines/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna). I hope you can help me understanding what is going on and how to fix it. Thank you in advance. Best regards,. Miguel. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/57049#Comment_57049",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807:8860,down,downsampled,8860,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807,2,['down'],['downsampled']
Availability,"Karthik;; Thanks for this, I've done that in bcbio so hopefully will avoid the issue going forward. Feel free to close here unless you want to try and trace down further what is happening. Thanks again for the pointer that led us to the underlying issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407512076:157,down,down,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407512076,1,['down'],['down']
Availability,"K}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see descrip",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:3110,error,error,3110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,1,['error'],['error']
Availability,"LE : false; 10:20:01.719 INFO GermlineCNVCaller - Deflater: IntelDeflater; 10:20:01.719 INFO GermlineCNVCaller - Inflater: IntelInflater; 10:20:01.719 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 10:20:01.719 INFO GermlineCNVCaller - Requester pays: disabled; 10:20:01.720 INFO GermlineCNVCaller - Initializing engine; 10:20:07.111 INFO GermlineCNVCaller - Done initializing engine; 10:20:07.207 INFO GermlineCNVCaller - Running the tool in CASE mode...; 10:20:07.207 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 10:20:07.231 INFO GermlineCNVCaller - Aggregating read-count file /media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_noProbe.hdf5 (1 / 1); log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:20:25.874 INFO GermlineCNVCaller - Shutting down engine; [March 14, 2024 at 10:20:25 AM CET] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=2147483648; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /media/Data/tmp/case_denoising_calling.3564509013495540802.py --ploidy_calls_path=/media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_DGCP_noProbe-calls --output_calls_path=/media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_GCNV_noProbe-calls --output_tracking_path=/media/Ergebnisse/0115-24_Masterpanel_NB501654_0623/0115-24_GCNV_noProbe-tracking --input_model_path=/media/Data/MasterV3/GCNV_noProbe-model --random_seed=1984 --read_count_tsv_files /media/Data/tmp/0115-24.rc16220482177493702615.tsv --psi_s_scale=1.000000e-04 --mapping_error_rate=1.000000e-02 --depth_correction_tau=1.000000e+04 --q_c_expectation_mode=hybrid --num_samples_copy_ratio_approx=200 --p_alt=1.000000e-06 --cnv_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8740:3936,down,down,3936,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740,1,['down'],['down']
Availability,"LEVEL : 5; 18:01:51.712 INFO SortSam - Defaults.CREATE_INDEX : false; 18:01:51.712 INFO SortSam - Defaults.CREATE_MD5 : false; 18:01:51.712 INFO SortSam - Defaults.CUSTOM_READER_FACTORY : ; 18:01:51.712 INFO SortSam - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 18:01:51.712 INFO SortSam - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 18:01:51.712 INFO SortSam - Defaults.REFERENCE_FASTA : null; 18:01:51.712 INFO SortSam - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 18:01:51.713 INFO SortSam - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:01:51.713 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 18:01:51.713 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:01:51.713 INFO SortSam - Defaults.USE_CRAM_REF_DOWNLOAD : false; 18:01:51.713 INFO SortSam - Deflater IntelDeflater; 18:01:51.713 INFO SortSam - Initializing engine; 18:01:51.713 INFO SortSam - Done initializing engine; 18:02:01.512 INFO SortSam - Shutting down engine; [December 7, 2016 6:02:01 PM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=1911029760; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2299#issuecomment-265469924:1564,down,down,1564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299#issuecomment-265469924,1,['down'],['down']
Availability,"Largely taken from Lee's sample code, see JIRA ticket for details. Spins up a Hail cluster and runs a script to extract from a VDS to VCF files on a per-chromosome basis. Includes some refactoring to move some of the workspace-sniffing that was part of bulk ingest into more generic utility code. In terms of cluster tracking:. - Cluster name is calculated in shell script and visible in the logs; - Cluster name is written to a file which is delocalized even if the workload script fails. . Unintended but useful example [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/a96667a7-e08c-43f4-abad-b55fbe7f0c06) where not only is the cluster name logged and written to an output file which is delocalized, but the cluster gets shut down anyway by cleanup code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8525:741,down,down,741,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8525,1,['down'],['down']
Availability,"Last week, spark 2.0.0 is formally released. However, when I tested gatk4 on spark2.0.0, I found they were incompatible. It seems that the interface isn't match. The error log looks like below. Exception in thread ""main"" java.lang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2073:166,error,error,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073,1,['error'],['error']
Availability,"Laura if you don't mind - is `@RG` really necessary for HaplotypeCaller's work? I understand that it is defined in the specs but is it critical for the functioning, if we assume that each sample is one library? ; In my case, I was following a pipeline involving `STAR` mapping -> `MarkDuplicates` -> `SplitNCigarReads` -> `HaplotypeCaller`. As I understand, normally I should have added `@RG`s at the mapping step. Neither `MarkDuplicates` nor `SplitNCigarReads` gave any warning about it, so I only figured the problem at the final step. I have then added totally fake `@RG`s to my files just after SplitNCigarReads step, and it worked - then why HaplotypeCaller can't do something similar in case when RGs are not present?; Thanks! . > ; > ; > Got it. Then I guess I'm arguing against @aushev's expected behavior that; > ; > > HaplotypeCaller should just treat them normally without any error",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6501#issuecomment-599721878:889,error,error,889,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6501#issuecomment-599721878,1,['error'],['error']
Availability,Lessons learned in VDS creation during Echo Scale Testing. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e6aa362-e25b-49d0-83cd-d64e926c6386).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8602:39,Echo,Echo,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8602,1,['Echo'],['Echo']
Availability,Let's do off by default for all modes. Error if MNPs and GVCF mode.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-388080057:39,Error,Error,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-388080057,1,['Error'],['Error']
Availability,"Let's hear what others say, but I think I would strongly prefer to simply take over VariantEval in another repo if this was something you'd consider. I'd likely do much of what you propose anyway (certainly WRT testing); however, perhaps not the microscope we went through with the core GATK changes earlier. On plugins: I like what seems to be shaping up w/ Barclay. I carried over the Stratifier and Evaluator as plugins because it seems like it would make sense to allow tools to provide extensions (VariantEval, our tool, does). If I took this PR a step further, I would have migrated many arguments currently top-level on VariantEval into the plugins themselves (a good feature in Barclay). As an aside: I dont think VariantAnnotator is migrated yet, but we have many GATK3 plugins related to annotation, and hope that tool retains Annotator plugins when it get migrated. My impressions of barclay are probably a little out of date. I agree the main argument parsing framework is pretty robust. Specifically on plugins, it seems a little less so, or at least there are not many tools I visibly see exercising that part of the code. For example, there really should be a default implmentation or base class between Barclay's plugins and ReadFilter plugins. I'm guessing if more tools in GATK4 were using plugins this would have happened. I created something like this for VariantEval, and without a ton of work that could probably get generalized; however, doing so would throw a lot higher bar on me and as noted above I'm trying to take on less, not more at the moment. If we do take over VariantEval, I'm certainly happy to try to contribute code and experiences to improve the core, through more targeted PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407202501:992,robust,robust,992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407202501,2,['robust'],['robust']
Availability,"Let's please repair the branch before merge, rather than risk clobbering master. Squash/rebase does not interact nicely with merge commits in the history, particular if the merge commits contain changes due to conflict resolution.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-341790241:13,repair,repair,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-341790241,1,['repair'],['repair']
Availability,"Let's remove the option from the script, then, if it's causing a failure on our primary benchmarking sample.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141#issuecomment-357355467:65,failure,failure,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141#issuecomment-357355467,1,['failure'],['failure']
Availability,"Let's take the SB field as an example - the header says it's a fixed length field of 4 integers. What should happen if one of the data lines only contains 2 integers?. Similarly, let's say the AD field (length R) in a specific line contains fewer elements than the total number of alleles (alt+1). What should happen? . This is irrespective of the internals of TileDB/GenomicsDB - should I report an error (exception) and exit? Print warning message, (pad if needed) and continue? Both those options are possible, it's a question of what is preferable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413299560:400,error,error,400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413299560,1,['error'],['error']
Availability,Let's use Exceptions instead to indicate error conditions. The main program can return a error status but not internal classes. Return values are too valuable to use them for error codes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/71:41,error,error,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/71,3,['error'],['error']
Availability,Let's use a different downsampling strategy than GATK3 for the `HaplotypeCaller` -- perhaps a `ReservoirDownsampler` whose size is proportional to the size of each region. Need to make sure that whatever strategy we use allows us to deal with high-coverage regions without introducing calling artifacts / missed calls.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1642:22,down,downsampling,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1642,1,['down'],['downsampling']
Availability,"List(),; started=false); 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: Stopped; 18/01/09 18:31:26 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/01/09 18:31:26 INFO memory.MemoryStore: MemoryStore cleared; 18/01/09 18:31:26 INFO storage.BlockManager: BlockManager stopped; 18/01/09 18:31:26 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/01/09 18:31:26 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/01/09 18:31:26 INFO spark.SparkContext: Successfully stopped SparkContext; 18:31:26.896 INFO BwaAndMarkDuplicatesPipelineSpark - Shutting down engine; [January 9, 2018 6:31:26 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark done. Elapsed time: 0.89 minutes.; Runtime.totalMemory()=881328128; ***********************************************************************. A USER ERROR has occurred: Input files reference and reads have incompatible contigs: No overlapping contigs found.; reference contigs = [chrM, chr1, chr2, chr3, chr4, chr5, chr6, chr7, chr8, chr9, chr10, chr11, chr12, chr13, chr14, chr15, chr16, chr17, chr18, chr19, chr20, chr21, chr22, chrX, chrY, chr1_gl000191_random, chr1_gl000192_random, chr4_ctg9_hap1, chr4_gl000193_random, chr4_gl000194_random, chr6_apd_hap1, chr6_cox_hap2, chr6_dbb_hap3, chr6_mann_hap4, chr6_mcf_hap5, chr6_qbl_hap6, chr6_ssto_hap7, chr7_gl000195_random, chr8_gl000196_random, chr8_gl000197_random, chr9_gl000198_random, chr9_gl000199_random, chr9_gl000200_random, chr9_gl000201_random, chr11_gl000202_random, chr17_ctg5_hap1, chr17_gl000203_random, chr17_gl000204_random, chr17_gl000205_random, chr17_gl000206_random, chr18_gl000207_random, chr19_gl000208_random, chr19_gl000209_random, chr21_gl000210_random, chrUn_gl000211, chrUn_gl000212, chrUn_gl000213, chrUn_gl000214, chrUn_gl000215, chrUn_gl000216, chrUn_gl000217, chrUn_gl000218, chrUn_gl000219, chrUn_gl000220, chrUn_g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:31357,ERROR,ERROR,31357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['ERROR'],['ERROR']
Availability,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1320:58,error,error,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320,1,['error'],['error']
Availability,"Looking at the code in both GATK's `GenomicsDBImport` and the GenomicsDB library itself, I don't think the sample name map was ever intended as a mechanism to rename samples. It was just added as a way to avoid the up-front download of all the VCF headers. As evidence for this, we have a couple of asserts like this in the code:. ```; assert sampleName.equals(((VCFHeader) reader.getHeader()).getGenotypeSamples().get(0));; ```. However, using the map file to rename samples is a pretty natural thing for clients to want to do. At a minimum, we need to throw if a rename is attempted until sample renaming via the map file is officially supported and tested.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3814#issuecomment-343261932:224,down,download,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3814#issuecomment-343261932,1,['down'],['download']
Availability,Looking at the error it seems to be failing because the file doesn't have a BGZF magic number. Can you post the first few bytes of the file (via hexdump or similar)?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3488#issuecomment-324579749:15,error,error,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3488#issuecomment-324579749,1,['error'],['error']
Availability,"Looking at the htsjdk code responsible for the original throw (as far as I can see in the stack enclosed in the description) there is a few ""smells"" in the way synchronized is used or not use ReferenceSource.java. It is likely to be the reason behind the error considering that is failing in multi-thread. . Probably adding synchronized to getReferenceBasesByRegion would fix that. Is a htsjdk issue and not a GATK one. Do you want to add a workaround in GATK or press for a fix and update of the htsjdk dependency. @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139#issuecomment-1376298392:255,error,error,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139#issuecomment-1376298392,1,['error'],['error']
Availability,"Looking at the stack trace, the error takes place in a low-level BAM input reading component in a dependency library.... It seems that is trying to read as a number portions of the contig name for a read record in the input... which makes very little sense.... . My guess is that either it is a bug entirely contained in that library (is the name seqdoop?), say it does not tolerate those contig names, or that the user managed to input a bam file that isn't formatted correctly on those records.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3357#issuecomment-317817540:32,error,error,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3357#issuecomment-317817540,2,"['error', 'toler']","['error', 'tolerate']"
Availability,"Looking at the three tools {GetPileupSummaries, CollectAllelicCounts, CollectPerBaseCounts} it definitely seems possible to eliminate a lot of redundancy. I think adding an option to count bases per read as opposed to per fragment, the current default in this tool, would essentially accomplish that---the idea being that you may want to look at read bases for modelling errors. I understand, though, that I might be kibitzing something I know far too little about and there might be many more implications to consider, but those are my general thoughts for the three tools. This seems to agree with the discussion [#4717 (comment)](https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6545#issuecomment-610591143:371,error,errors,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6545#issuecomment-610591143,1,['error'],['errors']
Availability,Looks good to me except for one minor thing. Hopefully this does the trick! Thanks also for cutting down on the test runtime.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3140#issuecomment-309932124:100,down,down,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140#issuecomment-309932124,1,['down'],['down']
Availability,"Looks great!. One quick note: I don't get the idea behind `Poisson` -- shouldn't we simply use negative binomials w/ modeled `mu_sj` and `alpha_sj`, evaluated at observed counts (`tt.arange(min_count, max_count + 1)`), and weighted with the number bins for each count (`_hist_sjm`)? i.e. if one observes an empirical distribution `P_obs(x)` rather than `x` draws, then the appropriate max likelihood objective function is `\sum_x P_obs(x) log P_model(x | \theta)`. Perhaps this is exactly what you've done and I don't get it. Another quick note: what I had in mind was _either_ modeling `mu_sj` at quantized ploidy states, _or_ let the ploidy state be unrestricted w/ a penalty via. a Bernoulli process (possibly w/ different per-contig penalties to account for e.g. higher rate of X/Y loss). We have enough samples in the cohort to select the quantized model (and those samples pin down the per-contig biases `b_j`). The samples that do not conform to quantized ploidy states can then choose whatever (variable) ploidy state they wish by paying a (hefty) price. We would also need to mask contigs that have variable ploidy calls from gCNV.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376286536:883,down,down,883,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376286536,4,"['down', 'mask']","['down', 'mask']"
Availability,Looks like CI is erroring out for other unrelated pull requests also? [https://travis-ci.org/broadinstitute/gatk/builds/453937675]([](https://travis-ci.org/broadinstitute/gatk/builds/453937675)),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-437924447:17,error,erroring,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-437924447,1,['error'],['erroring']
Availability,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:712,error,errors,712,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261,4,['error'],"['error', 'errors']"
Availability,Looks like python-is-python3 is not available to ubuntu bionic :-1: my bad on the bad choice of packages.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8499#issuecomment-1693912537:36,avail,available,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8499#issuecomment-1693912537,1,['avail'],['available']
Availability,Looks like some NIO version after 66 broke this API for some subset of paths?. I don't know how to figure out what the difference is either. @cmnbroad or @lbergelson would you have any ideas about this? (see my comment on this issue above where I narrow this bug down to an nio API call.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5935#issuecomment-491953383:263,down,down,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935#issuecomment-491953383,1,['down'],['down']
Availability,Looks like some failures related to Spark tests and usage of the NIO library in Picard that we'll have to work through...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8352#issuecomment-1581460929:16,failure,failures,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8352#issuecomment-1581460929,1,['failure'],['failures']
Availability,"Looks like the chromosome names are 1, 2, 3, instead of chr1, chr2, chr3 in the supplied clinvar resource. Maybe that is the issue?. Note, I'm finding multiple other issues with the Funcotator 1.8 resources. 1. Separate issue: #8965 ; 2. The Funcotator download tool for 1.8 creates an empty directory clinvar_hgmd. Not sure what that is?; 3. The Funcotator 1.8 somatic extracted folder on the google bucket does not have the dna_repair_genes folder that is created by the Funcotator download tool. Suggests a discrepancy between the extracted folder on the google bucket vs the tar.gz file in the google bucket. I think it might help for someone from the Funcotator team (@jamesemery ?) to review all the resources again, or perhaps there is something I'm doing wrong.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8963#issuecomment-2314225290:253,down,download,253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8963#issuecomment-2314225290,2,['down'],['download']
Availability,Looks like the integration tests failed with an unrelated error -- I'll try re-running them.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7393#issuecomment-953190949:58,error,error,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7393#issuecomment-953190949,1,['error'],['error']
Availability,Looks like there was an R error in Carrot. Fortunately the actual meat of the Carrot test ran to completion (for the CHM) I have also checked that this matches for the exome and the NIST sample. I would say that these samples are exactly matching in terms of their VCFeval output and thus we can be confident that this branch did not break the standard pipeline path and we can call this 👍. ```; Type | Precision | Recall | F1_Score | TP_Eval | TP_Base | FP | FN | Stratifier | IGV_Session | UNK | Name | Truth_Set | Summary_Type | Comparison_Engine; -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --; SNP | 0.9706 | 0.9863 | 0.9784 | 3473278 | 3489284 | 105213 | 48308 | NA | gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/7012fa81-18fd-4c9c-8722-22c9d7fa642d/call-CHMSampleHeadToHead/BenchmarkComparison/ae3dcac7-7105-4847-ac07-f9f64a43c4c8/call-BenchmarkVCFControlSample/Benchmark/67f50e9a-e7a3-4b31-9d14-d700b46ddfa5/call-VcfEvalWriteXMLfile/shard-0/CONTROLSNAPSHOT2018HG002_CHM_GRCh38_SYNDIPv20180222_vcfeval.xml | 539662 | CONTROLSNAPSHOT2018HG002 | CHM_GRCh38_SYNDIPv20180222 | summary | VcfEval; INDEL | 0.8814 | 0.8636 | 0.8724 | 485076 | 465627 | 65264 | 73548 | NA | gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/7012fa81-18fd-4c9c-8722-22c9d7fa642d/call-CHMSampleHeadToHead/BenchmarkComparison/ae3dcac7-7105-4847-ac07-f9f64a43c4c8/call-BenchmarkVCFControlSample/Benchmark/67f50e9a-e7a3-4b31-9d14-d700b46ddfa5/call-VcfEvalWriteXMLfile/shard-0/CONTROLSNAPSHOT2018HG002_CHM_GRCh38_SYNDIPv20180222_vcfeval.xml | 429205 | CONTROLSNAPSHOT2018HG002 | CHM_GRCh38_SYNDIPv20180222 | summary | VcfEval; Type | Precision | Recall | F1_Score | TP_Eval | TP_Base | FP | FN | Stratifier | IGV_Session | UNK | Name | Truth_Set | Summary_Type | Comparison_Engine; SNP | 0.9743 | 0.99 | 0.9821 | 3384890 | 3391796 | 89358 | 34303 | HCR | gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/7012fa81-18fd-,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1850884297:26,error,error,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1850884297,1,['error'],['error']
Availability,Looks like there were some test failures @lbergelson ?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7149#issuecomment-801356977:32,failure,failures,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7149#issuecomment-801356977,1,['failure'],['failures']
Availability,"Looks like this is my fault... I didn't realize BWA produces SAM output and the non-spark tool was correcting my mistake automatically (by checking for a magic number). Can we make the error message more informative like: ""BAM file must start with BGZF magic number""? . It would be great to detect whether it's SAM or BAM by checking the file contents, as in non-spark tools that use htsjdk, rather than the extension. Is this easily done?. @lbergelson To clarify I was using the regular BWA binaries not the GATK BWA tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3488#issuecomment-324959378:22,fault,fault,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3488#issuecomment-324959378,2,"['error', 'fault']","['error', 'fault']"
Availability,"Looks like this java.lang.NullPointerException is from an environment set up issue. . This request was created from a contribution made by Jordi Maggi on April 25, 2022 09:25 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/5574426055963-CNNScoreVariants-crashes-with-java-lang-NullPointerException](https://gatk.broadinstitute.org/hc/en-us/community/posts/5574426055963-CNNScoreVariants-crashes-with-java-lang-NullPointerException). \--. Hi,. I created a conda environment and installed gatk4 through `conda install -c bioconda gatk4`. I have been using this environment to run all steps of the single sample germline variant calling best practices workflow (both gatk and picard). However, I have never been able to run CNNScoreVariants with this setup, as it always results in a java.lang.NullPointerException error. The only way I am able to run this step is by running it through the docker image you provide. That, however, is not ideal for our setup. Any idea as to what I may try to be able to run it directly?. GATK version:. Using GATK jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar --version ; ; The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; HTSJDK Version: 2.24.1 ; ; Picard Version: 2.25.4. Exact command:. gatk CNNScoreVariants -I 73318\_WES\_hg19\_recalibrated.sorted.bam -V 73318\_80\_IDTv1.vcf.gz -R /media/analyst/Data/Reference\_data/hg19.fa -O /media/analyst/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor > /media/analyst/Data/CNNScoreVariants.log. Entire console output:. Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811:833,error,error,833,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811,1,['error'],['error']
Availability,"Looks like travis failed with an out-of-memory error (exit code 137) in gradle. @lbergelson was investigating this error earlier, and we thought we'd fixed it...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6075#issuecomment-518705863:47,error,error,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6075#issuecomment-518705863,2,['error'],['error']
Availability,"Looks like we're mostly there, except for adding a model version, which @lucidtronix is working on, and java downsampling, which probably won't make it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5548#issuecomment-458580273:109,down,downsampling,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5548#issuecomment-458580273,1,['down'],['downsampling']
Availability,Lower error rate in SGA overlap and better error handling,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1997:6,error,error,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1997,2,['error'],['error']
Availability,"M error are related. The only difference in invocation was that with the OOM failure, I was running with the default for `--max-reads-per-alignment-start` (`50`). This also works just fine with that setting at 15. The failure seems to occur around the same place in the data each time (the end of `chr13`). At that point in the data, there is a very large pileup which is probably instigating this. Additionally, if I remove the `--linked-de-bruijn-graph` argument, this runs just fine with the default setting of `--max-reads-per-alignment-start`. I have a minimally reproductive dataset that I can share which reproduces the OOM error for sure (I'm 99% sure it reproduces this one as well). For the OOM failures, the final logs from HaplotypeCaller look like this:. ```; ./gatk HaplotypeCaller ...; ...; 15:56:23.205 INFO ProgressMeter - Pf3D7_13_v3:2603234 100.5 114070 1134.5; 15:56:33.443 INFO ProgressMeter - Pf3D7_13_v3:2661462 100.7 114420 1136.1; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:56:43.998 INFO ProgressMeter - Pf3D7_13_v3:2730055 100.9 114840 1138.3; 15:56:59.911 INFO ProgressMeter - Pf3D7_13_v3:2798281 101.2 115210 1139.0; 15:59:27.062 INFO ProgressMeter - Pf3D7_13_v3:2861780 103.6 115460 1114.4; Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); Dangling End recovery killed because of a loop (getReferencePathForwardFromKmer); 15:59:37.457 INFO ProgressMeter - Pf3D7_13_v3:2869697 103.8 115500 1112.9. real 671m24.770s; user 777m30.923s; sys 6m13.682s. $ echo $?; 247; ```. Here is my command-line invocation:; ```; ./gatk --java-options ""-Xmx100000m -Xms25000m"" \; HaplotypeCaller \; -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta \; -I ${WORKING_DIR}/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam \; -O ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz \; --bam-output ${WORKING_DIR}/PG0004-CW.haplotype_caller.fixed_bam_fil",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:5294,recover,recovery,5294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,1,['recover'],['recovery']
Availability,"M2 WDL tests failed due to a transient 429 ""too many requests"" error -- re-running them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8715#issuecomment-1984346355:63,error,error,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8715#issuecomment-1984346355,1,['error'],['error']
Availability,M2 WDL tests had a transient failure on this branch -- re-running them.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3140#issuecomment-311466607:29,failure,failure,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140#issuecomment-311466607,1,['failure'],['failure']
Availability,M2 error with canine germline resource and variants_for_contamination files,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098:3,error,error,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098,1,['error'],['error']
Availability,"M2 wdl doesn't emit unfiltered vcf, which is redundant",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5076:45,redundant,redundant,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5076,1,['redundant'],['redundant']
Availability,MARK DUPLICATE PROCESS ERROR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8596:23,ERROR,ERROR,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8596,1,['ERROR'],['ERROR']
Availability,"MIGRATED FROM GATK3. @ldgauthier commented on [Thu Mar 12 2015](https://github.com/broadinstitute/gsa-unstable/issues/868). From Grace Tiao:. We found a large number of PASS frameshift indels that looked suspicious in our pan-cancer germline callset (~37k samples) that are driving the top genes in some of our case-control associations. These indels all have the following properties:; 1. They are flagged as PASS by VQSR and are PASS in ExAC; but examination in IGV shows that these variants are all present at extremely low allelic fractions. (These are all normal samples.); 2. These variants occur in the specific context of 7-base (and in one case, 8-base) homopolymer runs. The variant we looked at in the meeting had a very low QD score, and Eric Banks's suggestion for the short-term is to filter our callset for low QD indels (i.e., remove all indels with QD<1). The hard filter will take care of most of these faulty variants, but at least two of the suspicious indels have QD scores > 1 (see VCF columns below). In the long run, it might be beneficial to catch these cases in the VQSR indel modeling. 1 33745932 . G GC 2175.55 PASS AC=131;AF=1.743e-03;AN=75168;BaseQRankSum=-1.540e-01;CCC=75168;ClippingRankSum=0.306;DP=886479;FS=0.000;GQ_MEAN=58.06;GQ_STDDEV=13.70;HWP=1.0000;InbreedingCoeff=-0.0041;MLEAC=86;MLEAF=1.144e-03;MQ=59.62;MQ0=0;MQRankSum=0.457;NCC=23;QD=0.55;ReadPosRankSum=-2.130e-01;VQSLOD=2.00;culprit=QD; 1 40028015 . T TG 3661.09 PASS AC=143;AF=1.902e-03;AN=75202;BaseQRankSum=0.169;CCC=75202;ClippingRankSum=0.331;DP=1059106;FS=0.000;GQ_MEAN=61.43;GQ_STDDEV=19.66;HWP=1.0000;InbreedingCoeff=-0.0032;MLEAC=93;MLEAF=1.237e-03;MQ=59.79;MQ0=0;MQRankSum=0.331;NCC=6;QD=0.53;ReadPosRankSum=-1.760e-01;VQSLOD=2.03;culprit=QD; 2 220504281 . T TG 2883 PASS AC=98;AF=1.303e-03;AN=75214;BaseQRankSum=0.095;CCC=75214;ClippingRankSum=0.040;DP=1139600;FS=0.000;GQ_MEAN=68.44;GQ_STDDEV=17.59;HWP=1.0000;InbreedingCoeff=-0.0022;MLEAC=57;MLEAF=7.578e-04;MQ=59.42;MQ0=0;MQRankSum=0.401;NC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2508:921,fault,faulty,921,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2508,1,['fault'],['faulty']
Availability,"MPRESSION_LEVEL : 2; 12:57:16.776 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:57:16.776 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:57:16.776 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:57:16.776 INFO AnalyzeCovariates - Deflater: IntelDeflater; 12:57:16.776 INFO AnalyzeCovariates - Inflater: IntelInflater; 12:57:16.776 INFO AnalyzeCovariates - GCS max retries/reopens: 20; 12:57:16.776 INFO AnalyzeCovariates - Requester pays: disabled; 12:57:16.776 INFO AnalyzeCovariates - Initializing engine; 12:57:16.776 INFO AnalyzeCovariates - Done initializing engine; 12:57:17.333 INFO AnalyzeCovariates - Generating csv file '/tmp/AnalyzeCovariates17353441228865531235.csv'; 12:57:17.414 INFO AnalyzeCovariates - Generating plots file '/home/detagen/Desktop/pipeline/playground/NECESSARY/FMF-248/AnalyzeCovariates.FMF-248.pdf'; 12:57:17.829 INFO AnalyzeCovariates - Shutting down engine; [December 17, 2020 at 12:57:17 PM TRT] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=633339904; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.10272183847736955081';source('/tmp/BQSR.16251220439562120273.R'); /tmp/AnalyzeCovariates17353441228865531235.csv /home/detagen/Desktop/pipeline/playground/BACKUP/FMF-248_Backup/before.recal.FMF-248.table /home/detagen/Desktop/pipeline/playground/NECESSARY/FMF-248/AnalyzeCovariates.FMF-248.pdf; Stdout: ; Stderr: Error in library(gplots) : there is no package called ‘gplots’; Calls: source -> withVisible -> eval -> eval -> library; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:80); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:19); 	at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7006:3474,down,down,3474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7006,1,['down'],['down']
Availability,"MRecordToReadIterator.java:13); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: java.nio.channels.ClosedChannelException; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.position(SeekableByteChannelPrefetcher.java:416); 	at htsjdk.samtools.seekablestream.SeekablePathStream.seek(SeekablePathStream.java:63); 	at htsjdk.samtools.CRAMFileReader.queryUnmapped(CRAMFileReader.java:402); 	... 23 more; ```. #### Steps to reproduce; It looks to me like running with the two -L args (`-L loci.interval_list -L UNMAPPED`) causes this. ; Removing either one of them prevents the error, so my current work-around is to run ; PrintReads separately - once with `-L loci.interval_list` and once with `-L UNMAPPED`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6475:3373,error,error,3373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6475,1,['error'],['error']
Availability,"MT; 22:19:48.307 INFO FixMisencodedBaseQualityReads - ------------------------------------------------------------; 22:19:48.307 INFO FixMisencodedBaseQualityReads - ------------------------------------------------------------; 22:19:48.309 INFO FixMisencodedBaseQualityReads - HTSJDK Version: 2.13.2; 22:19:48.309 INFO FixMisencodedBaseQualityReads - Picard Version: 2.17.2; 22:19:48.310 INFO FixMisencodedBaseQualityReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 22:19:48.314 INFO FixMisencodedBaseQualityReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:19:48.318 INFO FixMisencodedBaseQualityReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:19:48.319 INFO FixMisencodedBaseQualityReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:19:48.325 INFO FixMisencodedBaseQualityReads - Deflater: IntelDeflater; 22:19:48.326 INFO FixMisencodedBaseQualityReads - Inflater: IntelInflater; 22:19:48.330 INFO FixMisencodedBaseQualityReads - GCS max retries/reopens: 20; 22:19:48.330 INFO FixMisencodedBaseQualityReads - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 22:19:48.331 INFO FixMisencodedBaseQualityReads - Initializing engine; 22:19:48.861 INFO FixMisencodedBaseQualityReads - Done initializing engine; 22:19:48.917 INFO ProgressMeter - Starting traversal; 22:19:48.917 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 22:19:49.026 INFO FixMisencodedBaseQualityReads - 196 read(s) filtered by: WellformedReadFilter. 22:19:49.029 INFO ProgressMeter - unmapped 0.0 918 505321.1; 22:19:49.030 INFO ProgressMeter - Traversal complete. Processed 918 total reads in 0.0 minutes.; 22:19:49.079 INFO FixMisencodedBaseQualityReads - Shutting down engine; [January 23, 2018 10:19:49 PM GMT] org.broadinstitute.hellbender.tools.FixMisencodedBaseQualityReads done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=580386816; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4241:2992,down,down,2992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4241,1,['down'],['down']
Availability,"MTOOLS : false; 14:59:15.873 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:59:15.873 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:59:15.873 INFO PrintReads - Deflater: IntelDeflater; 14:59:15.873 INFO PrintReads - Inflater: IntelInflater; 14:59:15.873 INFO PrintReads - GCS max retries/reopens: 20; 14:59:15.873 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:59:15.873 INFO PrintReads - Initializing engine; 14:59:21.404 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 14:59:21.421 INFO PrintReads - Shutting down engine; [October 5, 2017 2:59:22 PM EDT] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 0.11 minutes.; Runtime.totalMemory()=2129133568; ***********************************************************************. A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--javaOptions '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; ```. Still fails with `-readIndex` specified (.cram.crai OR .crai):; ```; -bash-4.1$ /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-launch PrintReads \; > -I gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram \; > -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; > -readIndex gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram.crai \; > -O HG00190_cram.bam \; > ; Using GATK jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar; Running:; ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:12185,ERROR,ERROR,12185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['ERROR'],['ERROR']
Availability,"MTOOLS : false; 15:00:08.250 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:00:08.250 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:00:08.250 INFO PrintReads - Deflater: IntelDeflater; 15:00:08.250 INFO PrintReads - Inflater: IntelInflater; 15:00:08.250 INFO PrintReads - GCS max retries/reopens: 20; 15:00:08.250 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 15:00:08.250 INFO PrintReads - Initializing engine; 15:00:13.258 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 15:00:13.275 INFO PrintReads - Shutting down engine; [October 5, 2017 3:00:14 PM EDT] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=2233466880; ***********************************************************************. A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--javaOptions '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; ```; ```; -bash-4.1$ /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-launch PrintReads \; > -I gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram \; > -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; > -readIndex gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.crai \; > -O HG00190_cram.bam \; > -L chr17; Using GATK jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_asy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:19749,ERROR,ERROR,19749,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['ERROR'],['ERROR']
Availability,"MTOOLS : false; 17:19:12.935 INFO CreateSomaticPanelOfNormals - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:19:12.935 INFO CreateSomaticPanelOfNormals - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:19:12.935 INFO CreateSomaticPanelOfNormals - Deflater: IntelDeflater; 17:19:12.935 INFO CreateSomaticPanelOfNormals - Inflater: IntelInflater; 17:19:12.935 INFO CreateSomaticPanelOfNormals - GCS max retries/reopens: 20; 17:19:12.935 INFO CreateSomaticPanelOfNormals - Requester pays: disabled; 17:19:12.935 WARN CreateSomaticPanelOfNormals - . [1m[31m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CreateSomaticPanelOfNormals is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 17:19:12.935 INFO CreateSomaticPanelOfNormals - Initializing engine; 17:19:13.291 INFO CreateSomaticPanelOfNormals - Shutting down engine; [January 16, 2021 5:19:13 PM IST] org.broadinstitute.hellbender.tools.walkers.mutect.CreateSomaticPanelOfNormals done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2132803584; ***********************************************************************. A USER ERROR has occurred: Couldn't read file file:///home/akansha/vivekruhela/pon_db/. Error was: It isn't a regular file. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; Using GATK jar /home/akansha/vivekruhela/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/akansha/vivekruhela/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar CreateSomaticPanelOfNormals -R /home/akansha/vivekruhela/hg19/ucsc.hg19.fasta -V pon_db -O /home/akansha/vivekruhela/pon1.vcf.gz",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811:15896,ERROR,ERROR,15896,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: java.io.FileNotFoundException: /tmp/test%20a/data/calling/a.vcf.gz (No such file or directory); at java.io.RandomAccessFile.open0(Native Method); at java.io.RandomAccessFile.open(RandomAccessFile.java:316); at java.io.RandomAccessFile.<init>(RandomAccessFile.java:243); at htsjdk.samtools.seekablestream.SeekableFileStream.<init>(SeekableFileStream.java:47); at htsjdk.samtools.seekablestream.SeekableStreamFactory$DefaultSeekableStreamFactory.getStreamFor(SeekableStreamFactory.java:99); at htsjdk.tribble.readers.TabixReader.<init>(TabixReader.java:129); at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:80); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:117); ... 9 more; ```. #### Steps to reproduce; Below few steps to reproduce the bug and the specificities mentioned above. ```bash; # Create test directory without whitespace; cd /tmp; mkdir -p test-a/data/calling/; cd test-a. # Upload appropriate VCFs in data/calling. # Run MergeVcfs; gatk MergeVcfs -I data/calling/a.vcf.gz -I data/calling/b.vcf.gz -O c.vcf.gz ## It runs as expected. # Introduce a whitespace in the directory name and move into the directory again; cd ..; mv test-a ""test a""; cd ""test a"". # Run MergeVcfs; gatk MergeVcfs -I data/calling/a.vcf.gz -I data/calling/b.vcf.gz -O c.vcf.gz ## It throws an error. # Introduce withespace in the VCFs; mv data/calling/a.vcf.gz -I data/calling/a\ 1.vcf.gz; mv data/calling/b.vcf.gz -I data/calling/b\ 1.vcf.gz. # Run MergeVcfs; gatk MergeVcfs -I data/calling/a\ 1.vcf.gz -I data/calling/b\ 1.vcf.gz -O c.vcf.gz ## It runs as expected. # If VCFs without whitespace in their names are moved into data or in the current working directory (""test a""), merging works as expected.; ```. #### Expected behavior; MergeVcfs should be able to handle whitespace when present anywhere in the file path. #### Actual behavior; It does not.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664:4888,error,error,4888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664,1,['error'],['error']
Availability,Make Google NIO provider for GCS public and available via maven,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1547:44,avail,available,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1547,1,['avail'],['available']
Availability,Make ReadsSparkSource.putPairsInSamePartition() more robust,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2442:53,robust,robust,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2442,1,['robust'],['robust']
Availability,Make RobustBrentSolver more flexible,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2971:5,Robust,RobustBrentSolver,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2971,1,['Robust'],['RobustBrentSolver']
Availability,Make error informative for non-diploid family likelihoods,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3329:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3329,1,['error'],['error']
Availability,"Make the logging frequency used by the ProgressLogger available as an input. If not used, sets the default value. Variants team is using a branch of gatk and have made this change there, so pulling this change into master to simplify future merges / branch updates.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8662:54,avail,available,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8662,1,['avail'],['available']
Availability,Make warnings into errors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/368:19,error,errors,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/368,1,['error'],['errors']
Availability,"Makes CreateVariantIngestFiles robust to partially or fully loaded samples. Commit 21828af8f5a925cc331dce6093c0d510042d7b64 is what I actually propose to merge, while commit de673204183a4c45059dc9ea4e05868e2ea6ae59 randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7843:31,robust,robust,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7843,3,"['failure', 'robust']","['failure', 'failures', 'robust']"
Availability,"Makes `CreateVariantIngestFiles` robust to partially or fully loaded samples. Commit a8dc5ea89653a7f94588aa040b49d0264d17f72d is what I actually propose to merge, while commit 118a44604343e8f77d53bcc6545b2360fefbe1cc randomly injects failures covering all the known failure modes. I tested these changes using both commits and was able to verify that partially loaded samples were handled correctly on subsequent attempts to load the sample (unfortunately we can't actually prevent these partial loadings from happening in the first place because preemptions, among other possible reasons).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7831:33,robust,robust,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7831,3,"['failure', 'robust']","['failure', 'failures', 'robust']"
Availability,Many of the Picard tools we'd like to port have no existing unit tests. We should write them and then backport them to Picard. Let's keep track of them here:. picard.sam:; AddOrReplaceReadGroups; BamIndexStats; BuildBamIndex; CalculateReadGroupChecksum; CheckTerminatorBlock; DownsampleSam; EstimateLibraryComplexity?; FilterReads; FixMateInformation; ReorderSam; ReplaceSamHeader; RevertOriginalBaseQualitiesAndAddMateCigar; SortSam. TODO other packages,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/144:276,Down,DownsampleSam,276,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/144,1,['Down'],['DownsampleSam']
Availability,"Marissa Powers here -- I'm an Intel engineer on the same team as Ed. It sounds like we all agree on having Intel-optimized TF as the default and figuring out the best intervention for older machines from there. We can add the AVX flag within CNNScoreVariant (and any other AI tool). From there, we can (1) provide a detailed error output describing the issue, (2) provide a non-AVX TF build, and (3) automatically roll back TF to the provided version. @EdwardDixon, it sounds like @cmnbroad is suggesting is options (1), (2), and (3), while you're suggesting just (1). Sound right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429409667:325,error,error,325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429409667,1,['error'],['error']
Availability,MarkDuplicates Spark output needs to tested against the version of picard they use in production to ensure that it produces identical output and is reasonably robust to pathological files. This requires that the following issues have been resolved:; #3705 ; #3706,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675:159,robust,robust,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675,1,['robust'],['robust']
Availability,MarkDuplicatesSpark error when FASTQ headers contain another @ in string,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8134:20,error,error,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8134,1,['error'],['error']
Availability,MarkDuplicatesSpark improvements checkpoint,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4656:33,checkpoint,checkpoint,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4656,1,['checkpoint'],['checkpoint']
Availability,MarkDuplicatesSpark throw an error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7035:29,error,error,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7035,1,['error'],['error']
Availability,Marking `AuthService` as `optional` in gcloud-nio's pom fixes it:. ```; <dependency>; <groupId>com.google.auto.service</groupId>; <artifactId>auto-service</artifactId>; <version>1.0-rc3</version>; <optional>true</optional>; <scope>provided</scope> <!-- to leave out of the all-deps jar -->; </dependency>; ```. $ ./gradlew sparkJar; BUILD SUCCESSFUL. $ ./gatk-launch ExampleNioCountReads (...); (starts without error),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3241#issuecomment-314265654:411,error,error,411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3241#issuecomment-314265654,1,['error'],['error']
Availability,Mask duplicates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8043:0,Mask,Mask,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8043,1,['Mask'],['Mask']
Availability,Master is broken. [E.g.](https://travis-ci.com/broadinstitute/gatk/jobs/227807624). ```; Fetched 217 kB in 1s (163 kB/s); Reading package lists...; W: http://ppa.launchpad.net/couchdb/stable/ubuntu/dists/trusty/Release.gpg: Signature by key 15866BAFD9BCC4F3C1E0DFC7D69548E1C17EAB57 uses weak digest algorithm (SHA1); W: GPG error: https://packagecloud.io/github/git-lfs/ubuntu trusty InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6B05F25D762E3157; W: The repository 'https://packagecloud.io/github/git-lfs/ubuntu trusty InRelease' is not signed.; W: There is no public key available for the following key IDs:; 6B05F25D762E3157 . ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6116:324,error,error,324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6116,3,"['avail', 'error']","['available', 'error']"
Availability,Mention acceptable compressed VCF file extension in GenomicsDBImport error message,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7692:69,error,error,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7692,1,['error'],['error']
Availability,Merge changes from the EchoCallset branch back into our main branch ('ah_var_store'). Most of these changes are VDS creation related. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f5cb7a2d-b224-4b8e-8daf-2d22939a1d96),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8993:23,Echo,EchoCallset,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8993,1,['Echo'],['EchoCallset']
Availability,Merging VS-1379 code into Echo Callset branch now. Need some thumbs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8879:26,Echo,Echo,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8879,1,['Echo'],['Echo']
Availability,"Merging this despite the unrelated ""No space left on device"" error in the M2 WDL tests, which is definitely unrelated to this change.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3140#issuecomment-311484682:61,error,error,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140#issuecomment-311484682,1,['error'],['error']
Availability,"Merging this one despite the ongoing failure in the cloud tests, which is understood and known to be unrelated.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8318#issuecomment-1551795859:37,failure,failure,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8318#issuecomment-1551795859,1,['failure'],['failure']
Availability,Migrate read arguments and downstream code to GATKPathSpecifier,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6561:27,down,downstream,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6561,1,['down'],['downstream']
Availability,Migrate reference arguments and downstream code to GATKPathSpecifier.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6524:32,down,downstream,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6524,1,['down'],['downstream']
Availability,"Minor mystery solved. I somehow overlooked the corresponding .vcf.idx files, which were over or at the 144 byte limit, so the errors are less surprising. PR made.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4718#issuecomment-386327818:126,error,errors,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4718#issuecomment-386327818,1,['error'],['errors']
Availability,Misleading error message about multi-sample when no read groups are present,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6501:11,error,error,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6501,1,['error'],['error']
Availability,ModelSegments command memory error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5948:29,error,error,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5948,1,['error'],['error']
Availability,ModelSegments integration test failures on newer Java 11 releases,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8107:31,failure,failures,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8107,1,['failure'],['failures']
Availability,Modern versions of womtool don't tolerate task inputs with the same name as task outputs and produce baffling error messages like [this](https://broadinstitute.slack.com/archives/C4GSMFXS9/p1654807836682679).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8265:33,toler,tolerate,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8265,2,"['error', 'toler']","['error', 'tolerate']"
Availability,"More info from @ldgauthier:. ```; I’ve only been trying the same GenomicsDBImport over and over again. I estimate it to take about; 30 hours if it’s ever successful. The exception happens in different batches every time. Sam F. ; said he saw the exception too but he could eventually resubmit his way through it. His jobs are; shorter running.; ```. So it seems like the error is nondeterministic, but can't be recovered from within the same VM instance / process.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412917164:371,error,error,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412917164,2,"['error', 'recover']","['error', 'recovered']"
Availability,More robust parsing of the flow based read - determines the maximal possible quality automatically,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8642:5,robust,robust,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8642,1,['robust'],['robust']
Availability,"Most of the scaling issues in Cromwell/Terra have been resolved. Terra still has limitations on workflow metadata size, and passing long file arrays to ever task in large scatters (i.e. the full list of counts files is passed into every gCNV shard) can limit our batch sizes for workflows that embed gCNV (e.g. GatherBatchEvidence in gatk-sv). gCNV workflows also don't call cache reliably (presumably due to timeouts) probably again due to the large file arrays, including 2D arrays.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-928168236:381,reliab,reliably,381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-928168236,1,['reliab'],['reliably']
Availability,Move NativeUtils class to gatk-native-bindings once it's available via maven,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1848:57,avail,available,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1848,1,['avail'],['available']
Availability,"Move to google-cloud-java snapshot with more robust retries, and set number of retries/reopens globally",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3295:45,robust,robust,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3295,1,['robust'],['robust']
Availability,Moving classes that tests depend on from the test folders into the src folders in the utils.test package. This way they will be available to projects that depend on hellbender. Fixes #525 . Updating to the newest testng release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/527:128,avail,available,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/527,1,['avail'],['available']
Availability,MuTect2 Should Error if Almost All Somatic Variants Supported by Few Reads,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6674:15,Error,Error,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6674,1,['Error'],['Error']
Availability,Multithreading is not worth the errors and must be removed from hellbender code. GATK4 is a single-thread toolkit.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/402:32,error,errors,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/402,1,['error'],['errors']
Availability,Mutect downsampler that recognizes and skips bad mapping regions,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3988:7,down,downsampler,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3988,1,['down'],['downsampler']
Availability,Mutect isActive loses sensitivity when allele fraction is comparable to base error rate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4816:77,error,error,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4816,1,['error'],['error']
Availability,Mutect2 (GATK 4.1.0.0) fails occasionally in smith waterman native library as below. stderr is attached. I can also provide core dump if necessary. [stderr.tar.gz](https://github.com/broadinstitute/gatk/files/2880800/stderr.tar.gz). ```; 07:30:59.335 INFO ProgressMeter - 17:78451657 627.7 1223980 1950.0; *** Error in `java': munmap_chunk(): invalid pointer: 0x00002ba8e50b7740 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x7ab54)[0x2ba8df926b54]; /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/2cebc7be-fe23-4787-9095-9b91227c6526/call-M2/shard-13/attempt-2/tmp.945f1f83/libgkl_smithwaterman5575294852416409537.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x2ba9aee21fa8]; /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/2cebc7be-fe23-4787-9095-9b91227c6526/call-M2/shard-13/attempt-2/tmp.945f1f83/libgkl_smithwaterman5575294852416409537.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x2ba9aee21bf8]; [0x2ba8e8f6675a]; ======= Memory map: ========; 00400000-00401000 r-xp 00000000 08:03 5769910 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-1.b15.el7_2.x86_64/jre/bin/java; 00600000-00601000 r--p 00000000 08:03 5769910 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-1.b15.el7_2.x86_64/jre/bin/java; ...; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690:310,Error,Error,310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690,1,['Error'],['Error']
Availability,Mutect2 error ComparableSamRecordIterator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7872:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7872,1,['error'],['error']
Availability,Mutect2 error getNumTandemRepeatUnits String index out of range,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6516:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6516,1,['error'],['error']
Availability,Mutect2 error when running inside a pipeline in parallel with intervals,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7059:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7059,1,['error'],['error']
Availability,Mutect2 error when trying to create fragment with no read support,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6310:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6310,1,['error'],['error']
Availability,Mutect2 gatk 4.1.6.0: java.lang.IllegalStateException: Smith-Waterman alignment failure,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6529:80,failure,failure,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6529,1,['failure'],['failure']
Availability,"Mutect2 matches called variants against known variants retrieved from the germline resource VCF (if available) for the POPAF annotation. While comparing the called allele to the germline resource variants, Mutect2 only takes into account the sequence of the alternate allele(s) while ignoring the reference allele sequence. This can cause incorrect annotations at sites with multiple alternate alleles (e.g. CT -> C/CTT in the germline resource while M2 calls C -> CT). This PR is a proposed fix along with some unit tests that demonstrate the issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6999:100,avail,available,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6999,1,['avail'],['available']
Availability,"Mutect3 dataset enhancements: optional truth VCF for labels, seq error likelihood annotation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7975:65,error,error,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7975,1,['error'],['error']
Availability,"My GenotypeGVCFs run for a single chromosome returned the following completion statement:; 18:54:40.516 INFO ProgressMeter - Traversal complete. Processed 606308 total variants in 75.2 minutes. However, there are only 46814 variant rows (excluding 52 header rows) in the corresponding vcf file. Does the above figure of 606308 correspond to a multiple of 'variants x number of samples'?. Also, there are only 16863 lines in my log file, does this mean that the 'Current Locus' column in the log file doesn't correspond to a single genomic location (bp) in the fasta file?. I am curious to know what is the relation between all these figures to fully understand what is happening while processing the gCVF files. Also, on the inbreeding coefficient warning issue, I understand from your @Neato-Nick feedback that the variants with these warnings may still be fine and can be retained. However, this still leaves me worrying that out of 384 samples the locus doesn't even have 10 samples for generating the required metrics. Such variants won't be of any use for downstream analyses anyway where any variants with more than 80% missing samples will be removed. Therefore, I wish to seek some more information about this 10 sample thing - does it have some other context or does it literally mean that there are only less than 10 samples carrying that variant?. Regards,; Sanjeev",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-409255344:1061,down,downstream,1061,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-409255344,2,['down'],['downstream']
Availability,"My concern is that this Annotation only works if a pedigree file is supplied, not if the user supplies FounderIDs but i'm not sure the best way to handle that (with an error or a warning or some such).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-462836865:168,error,error,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-462836865,1,['error'],['error']
Availability,My current workaround is to downgrade GATK to 4.1.3.0,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-789572872:28,down,downgrade,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-789572872,1,['down'],['downgrade']
Availability,My fault for not telling @vruano. I will make the changes to `MathUtils` ASAP.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2235#issuecomment-257472282:3,fault,fault,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2235#issuecomment-257472282,1,['fault'],['fault']
Availability,"My problem is fixed with the first commit, because if `customCommandLineValidation()` throws an `UserException.CommandLineException` it is catched and printed (otherwise, it is ignore in `Main`, except for the returning status). Anyway, I added a commit with the implementation of the void method for all the tools. I guess that it is not a good idea after all, because it could help to print several errors. My first commit deal with the two situations, printing one/several errors if the `String[]` is not null, and if the validation throws an error catching it and handle in the same way as the ones in `CommandLineProgram`. I will rather go for the first and changing the printing of the errors in the array to the same place as the catched exceptions, and decorate it in the same way for not confusing the software user. What do you thing, @lbergelson?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255856456:401,error,errors,401,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255856456,4,['error'],"['error', 'errors']"
Availability,"My struggles with indexes in GATK4 continue. I forgot to pull down the corresponding *.tbi index for my .vcf.gz, but SelectVariants just toodled along until chr14:; ```; htsjdk.tribble.TribbleException: Line 3889836: there aren't enough columns for line chr14 24737838 rs1101636 C T . PASS AC=2;AF=1.00;AN=2;DB;DP=38;ExcessHet=0.7420;FS=0.000;InbreedingCoeff=0.0357;MQ=59.98;MQRankSum=0.026;MQ_DP=24332;POSITIVE_TRAIN_SITE;QD=22.41;QUALapprox=537 (we expected 9 tokens, and saw 8 ), for input source: file:///humgen/gsa-hpprojects/dev/gauthier/reblockGVCF/gnomADaccuracyTest.noMQinSNPVQSR.SynDip.vcf.gz; at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:281); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:262); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:64); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:70); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:37); at htsjdk.tribble.TribbleIndexedFeatureReader$WFIterator.readNextRecord(TribbleIndexedFeatureReader.java:365); at htsjdk.tribble.TribbleIndexedFeatureReader$WFIterator.next(TribbleIndexedFeatureReader.java:346); at htsjdk.tribble.TribbleIndexedFeatureReader$WFIterator.next(TribbleIndexedFeatureReader.java:307); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4224:62,down,down,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4224,1,['down'],['down']
Availability,"My suspicion was wrong. We also include a safety check which cause us to correctly reject most accidental matches. If we detect 2 chromosomes with the same name but different lengths we fail even if we detect otherwise matching chromosomes. I've run all the dictionaries I could find in the gatk bundle against each and only b37 and b37_decoy are compatible with each other which is the desired behavior I believe. | | hg18 | hg19 | b37 | b37_decoy | hg38 |; | -- |------|-----|------|-----------|-------|; | hg18 | ✅ | | | | |; | hg19 | | ✅ | | | |; | b37 | | | ✅ | ✅ | |; | b37_decoy | | | ✅ | ✅ | |; | hg38 | | | | | ✅ |. ```; @DataProvider; public Iterator<Object[]> getComparisons(){; final ArrayList<Object[]> comparisons = new ArrayList<>();; final List<String> dicts = Arrays.asList(""Homo_sapiens_assembly18.dict"",; ""ucsc.hg19.dict"",; ""human_b36_both.dict"",; ""human_g1k_v37.dict"",; ""human_g1k_v37_decoy.dict"",; ""Homo_sapiens_assembly38.dict"");; for( String left : dicts) {; for (String right: dicts){; Path leftDict =Paths.get(""/Users/louisb/Downloads/dicts"", left);; Path rightDict = Paths.get(""/Users/louisb/Downloads/dicts"", right);. comparisons.add( new Object[] {leftDict, rightDict});; }; }; return comparisons.iterator();; }. @Test(dataProvider = ""getComparisons""); public void testSequenceDictionariesAgainstEachother(Path left, Path right){; String leftName = left.getFileName().toString();; String rightName = right.getFileName().toString();; SequenceDictionaryUtils.validateDictionaries(leftName,; SAMSequenceDictionaryExtractor.extractDictionary(left),; rightName,; SAMSequenceDictionaryExtractor.extractDictionary(right));; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3754#issuecomment-494924193:1050,Down,Downloads,1050,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3754#issuecomment-494924193,2,['Down'],['Downloads']
Availability,N StrandBiasBySample - Annotation will not be calculated at position chr6:67407399 and possibly subsequent; genotype for sample 8939{JXM}-3 is not called ; ; 23:44:10.556 WARN DepthPerSampleHC - Annotation will not be calculated at position chr6:67407415 and possibly subsequent; genotype for sample 8939{JXM}-3 is not called ; ; 23:44:10.556 WARN StrandBiasBySample - Annotation will not be calculated at position chr6:67407415 and possibly subsequent; genotype for sample 8939{JXM}-3 is not called ; ; 23:44:14.224 INFO ProgressMeter - chr6:67607778 569.6 5544800 9734.3 ; ; 23:44:24.280 INFO ProgressMeter - chr6:68147283 569.8 5547230 9735.7 ; ; 23:44:30.026 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 27.307544954 ; ; 23:44:30.027 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 4768.198119518001 ; ; 23:44:30.027 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 4695.36 sec ; ; 23:44:30.027 INFO HaplotypeCaller - Shutting down engine ; ; \[2021年11月1日 下午11时44分30秒\] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 569.97 minutes. ; ; Runtime.totalMemory()=742916096 ; ; htsjdk.samtools.SAMFormatException: Did not inflate expected amount ; ; at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:147) ; ; at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196) ; ; at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockC,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582:7953,down,down,7953,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582,1,['down'],['down']
Availability,"N; Y 59252232 59252800 target_189882_VAMP7 NaN; Y 59272120 59272713 target_189883_VAMP7 NaN; Y 59274302 59274871 target_189884_VAMP7 NaN; Y 59330180 59330708 target_189885_IL9R NaN; Y 59333828 59334429 target_189886_IL9R NaN; Y 59335302 59335904 target_189887_IL9R NaN; Y 59335905 59336289 target_189888_IL9R NaN; Y 59336290 59336776 target_189889_IL9R NaN; Y 59336840 59337486 target_189890_IL9R NaN; Y 59337698 59338400 target_189891_IL9R NaN; Y 59338503 59339109 target_189892_IL9R NaN; Y 59339943 59340528 target_189893_IL9R NaN; Y 59342236 59343330 target_189894_IL9R NaN. ---. @mbabadi commented on [Fri Aug 26 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-242767764). @davidbenjamin could you please take a look? it sounds like it could be a problem with the reference missing these regions. ---. @mbabadi commented on [Fri Aug 26 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-242774747). OK it turns out that the reference is hard masked and has ""N"" in that region. Nevertheless, we shouldn't get NaNs. In my opinion, the correct behavior is to drop targets on which GC percentage can not be defined + emit informative error messages. ---. @davidbenjamin commented on [Sun Sep 11 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-246177779). I will address this. ---. @mbabadi commented on [Tue Sep 27 2016](https://github.com/broadinstitute/gatk-protected/issues/651#issuecomment-250018497). @davidbenjamin also, CorrectGCBias produces NaNs when a sample has very low coverage. I think the correct behavior is this:. (1) when annotating targets, it is OK to produce NaNs on targets whose GC bias can not be determined. When correcting for GC bias, those targets must be removed altogether. (2) if the bias curve can not be determined (let's say because of low coverage), the tool should remove that sample from the collection and emit appropriate warning messages. If all samples are removed, the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2882:3808,mask,masked,3808,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2882,1,['mask'],['masked']
Availability,"NCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.419 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 18:53:59.422 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; 18:53:59.433 INFO ProgressMeter - Starting traversal; 18:53:59.433 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 18:54:01.952 INFO IndexFeatureFile - Shutting down engine; [March 8, 2021 at 6:54:01 PM CET] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=473956352; java.lang.IllegalArgumentException: Unexpected value: MANE_Plus_Clinical; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureTag.getEnum(GencodeGtfFeature.java:1388); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(GencodeGtfFeature.java:197); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfTranscriptFeature.<init>(GencodeGtfTranscriptFeature.java:19); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfTranscriptFeature.create(GencodeGtfTranscriptFeature.java:23); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureType$2.create(GencodeGtfFeature.java:768); at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.create(GencodeGtfFeature.java:327); at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCodec.decode(Ab",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7134:2738,down,down,2738,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7134,1,['down'],['down']
Availability,"NC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 04:33:13.196 INFO IndexFeatureFile - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 04:33:13.196 INFO IndexFeatureFile - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 04:33:13.196 INFO IndexFeatureFile - Deflater: IntelDeflater ; ; 04:33:13.196 INFO IndexFeatureFile - Inflater: IntelInflater ; ; 04:33:13.196 INFO IndexFeatureFile - GCS max retries/reopens: 20 ; ; 04:33:13.196 INFO IndexFeatureFile - Requester pays: disabled ; ; 04:33:13.196 INFO IndexFeatureFile - Initializing engine ; ; 04:33:13.196 INFO IndexFeatureFile - Done initializing engine ; ; 04:33:13.396 INFO FeatureManager - Using codec EnsemblGtfCodec to read file file:///gatk/funcotator-scripts/gencode/mm10/gencode.vM25.annotation.gtf ; ; 04:33:13.400 INFO ProgressMeter - Starting traversal ; ; 04:33:13.400 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute ; ; 04:33:21.040 INFO IndexFeatureFile - Shutting down engine ; ; \[January 25, 2021 4:33:21 AM GMT\] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.13 minutes. ; ; Runtime.totalMemory()=1835532288 ; ; java.lang.IllegalArgumentException: Unexpected value: IG\_D\_pseudogene ; ; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$GeneTranscriptType.getEnum(GencodeGtfFeature.java:1060) ; ; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(GencodeGtfFeature.java:158) ; ; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.<init>(GencodeGtfGeneFeature.java:19) ; ; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.create(GencodeGtfGeneFeature.java:23) ; ; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureType$1.create(GencodeGtfFeature.java:760) ; ; at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.create(GencodeGtfFeature.java:327) ; ; at org.broadinstitute.hellbender.utils.codecs.gtf.AbstractGtfCo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7054:3604,down,down,3604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7054,1,['down'],['down']
Availability,NDArray error from GermlineCNVCaller,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3996:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3996,1,['error'],['error']
Availability,"NFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5a39e554{/jobs,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@67941d{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2ad2b274{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7114e780{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4def42c3{/stages,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@72110818{/stages/json,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6eabe718{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7530090a{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4492b393{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@55fb36de{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@63a7781{/storage,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@73b74615{/storage/json,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@22686ddb{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2e29f28e{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7bfa1eb5{/environment,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContex",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:7814,AVAIL,AVAILABLE,7814,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['AVAIL'],['AVAILABLE']
Availability,"NFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode_xrefseq_v90_38.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg38/gencode_xrefseq_v90_38.tsv; > 15:16:43.878 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/cosmic_tissue.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/cosmic_tissue/hg38/cosmic_tissue.tsv; > 15:16:43.926 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode.v34.annotation.REORDERED.gtf -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; > 15:16:43.926 INFO DataSourceUtils - Setting lookahead cache for data source: Gencode : 100000; > 15:16:43.937 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; > 15:16:43.938 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; > 15:16:43.939 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; > 15:16:43.946 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; > 15:16:44.093 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode.v3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:13791,error,errors,13791,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['error'],['errors']
Availability,"NFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGermlineContigPloidy - Initializing engine; 08:48:45.931 DEBUG ScriptExecutor - Executing:; 08:48:45.931 DEBUG ScriptExecutor - python; 08:48:45.932 DEBUG ScriptExecutor - -c; 08:48:45.932 DEBUG ScriptExecutor - import gcnvkernel. WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.; /home/ec2-user/miniconda3/envs/gatk/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; 08:48:50.351 DEBUG ScriptExecutor - Result: 0; 08:48:50.351 INFO DetermineGermlineContigPloidy - Done initializing engine; 08:48:50.352 INFO DetermineGermlineContigPloidy - Shutting down engine; [October 17, 2019 8:48:50 AM UTC] org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=597164032; java.lang.IllegalArgumentException: List of input read-count files cannot contain duplicates.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:725); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.validateArguments(DetermineGermlineContigPloidy.java:304); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.doWork(DetermineGermlineContigPloidy.java:277); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProg",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:6756,down,down,6756,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['down'],['down']
Availability,"NFO GenomicsDBImport - Requester pays: disabled; 12:52:37.524 INFO GenomicsDBImport - Initializing engine; 12:52:38.096 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:43.641 INFO IntervalArgumentCollection - Processing 134492644 bp from intervals; 12:52:43.720 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 12:52:43.722 INFO GenomicsDBImport - Done initializing engine; 12:52:44.113 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vidmap.json; 12:52:44.113 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/callset.json; 12:52:44.114 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vcfheader.vcf; 12:52:44.114 INFO GenomicsDBImport - Importing to array - /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/genomicsdb_array; 12:52:44.114 INFO ProgressMeter - Starting traversal; 12:52:44.115 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 13:03:44.100 INFO GenomicsDBImport - Importing batch 1 with 2 samples; [TileDB::FileSystem] Error: (sync_path) Cannot sync file; File syncing error; path=/mnt/fargen/experiments/joint_call/data/genomicsdb/run1/chr1$11981$12351/.__cd28ac27-6a06-422f-a674-2acfbdb072d1140406632785664_1551359040166; errno=22(Invalid argument); terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while syncing array chr1$11981$12351 to disk; TileDB error message :",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5740:4624,Error,Error,4624,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740,4,"['Error', 'error']","['Error', 'error']"
Availability,NFO GenotypeGVCFs - Picard Version: 2.21.9; 09:48:14.873 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:48:14.874 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:48:14.874 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:48:14.874 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:48:14.874 INFO GenotypeGVCFs - Deflater: IntelDeflater; 09:48:14.874 INFO GenotypeGVCFs - Inflater: IntelInflater; 09:48:14.874 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 09:48:14.874 INFO GenotypeGVCFs - Requester pays: disabled; 09:48:14.874 INFO GenotypeGVCFs - Initializing engine; 09:48:16.015 INFO GenotypeGVCFs - Shutting down engine; [27 May 2020 09:48:16 CEST] oAB.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2301100032; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; oAB.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; 	at oAB.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:410); 	at oAB.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:326); 	at oAB.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at oAB.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76); 	at oAB.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:67); 	at oAB.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:706); 	at oAB.broadinstitute.hellbender.engine.VariantLocusWalker.onStartup(VariantLocusWalker.java:63); 	at oAB.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLine,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6616:10469,ERROR,ERROR,10469,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6616,1,['ERROR'],['ERROR']
Availability,"NFO LeftAlignAndTrimVariants - 10 variants skipped because the reference allele was too long. The longest had a reference allele length of 245. To not skip these variants set --max-indel-length >= 245; 12:55:32.536 INFO LeftAlignAndTrimVariants - 0 variants left aligned; 12:55:32.542 INFO LeftAlignAndTrimVariants - Shutting down engine; [September 6, 2018 12:55:32 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariants done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=249036800; ```; Multiple changes to messages in stdout. Includes # total records, number of records that were trimmed, # variant records skipped due to ref allele being too long and finally the max-indel-length value that needs to be set to include these in the leftalignandtrim. This is an improvement to previous stdout messaging. Upping max-indel-length; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --max-indel-length 250 -O zeta_snippet_leftalign_250_96branch.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --max-indel-length 250 -O zeta_snippet_leftalign_250_96branch.vcf.gz; 14:03:44.243 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 06, 2018 2:03:44 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accou",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326:6205,Down,Downloads,6205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326,1,['Down'],['Downloads']
Availability,"NFO ProgressMeter - chr6:7646289 1.0 171000 166913.4; 23:23:03.973 INFO ProgressMeter - chr6:9029926 1.2 200000 167553.3; 23:23:14.220 INFO ProgressMeter - chr6:10374988 1.4 229000 167835.2; 23:23:24.322 INFO ProgressMeter - chr6:11782077 1.5 259000 168971.8; 23:23:34.465 INFO ProgressMeter - chr6:13360174 1.7 290000 170404.5; 23:23:44.556 INFO ProgressMeter - chr6:14757971 1.9 319000 170585.2; 23:23:54.657 INFO ProgressMeter - chr6:16217652 2.0 350000 171704.7; 23:24:04.905 INFO ProgressMeter - chr6:17737681 2.2 381000 172461.9; 23:24:15.102 INFO ProgressMeter - chr6:19070725 2.4 409000 171911.3; 23:24:25.321 INFO ProgressMeter - chr6:20580950 2.5 441000 172978.5; 23:24:36.315 INFO ProgressMeter - chr6:22100346 2.7 472000 172724.0; 23:24:46.346 INFO ProgressMeter - chr6:23531348 2.9 502000 173112.4; 23:24:56.432 INFO ProgressMeter - chr6:24734131 3.1 531000 173078.8; 23:25:06.662 INFO ProgressMeter - chr6:26183595 3.2 559000 172612.6; 23:25:11.087 INFO ApplyVQSR - Shutting down engine; [October 12, 2022 11:25:11 PM EDT] org.broadinstitute.hellbender.tools.walkers.vqsr.ApplyVQSR done. Elapsed time: 3.33 minutes.; Runtime.totalMemory()=8242331648; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chr6:26914009 [VC chr6.raw.excessHet.vcf.gz @ chr6:26914009 Q276902.75 of type=INDEL alleles=[G*, GTGTA, GTGTATA, GTGTGTA] attr={AC=[4269, 29, 5], AF=[0.620, 4.209e-03, 7.257e-04], AN=6890, AS_BaseQRankSum=[0.500, 0.500, 0.500], AS_FS=[0.544, 0.000, 0.000], AS_InbreedingCoeff=[0.0312, 0.0151, 0.0858], AS_MQ=[59.29, 57.89, 58.81], AS_MQRankSum=[0.000, -3.800, -3.800], AS_QD=[3.94, 0.05, 0.01], AS_ReadPosRankSum=[0.200, 0.300, 0.300], AS_SOR=[0.747, 0.705, 0.739], BaseQRankSum=0.515, DP=121924, ExcessHet=0.1315, FS=0.552, InbreedingCoeff=0.0304, MLEAC=[4273, 28, 5], MLEAF=[0.620, 4.064e-03, 7.257e-04], MQ=57.54, MQRankSum=-1.059e+00, QD=3.98, ReadPosRankSum=0.244, SOR=0.780} GT=GT:AD:DP:GQ:PGT:PID:PL:PS31/1:0,15,0,0:18:45:.:.:106,45,0,569,45,106,569,4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8054:3634,down,down,3634,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8054,1,['down'],['down']
Availability,"NFO VariantAnnotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:02:45.348 INFO VariantAnnotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:02:45.349 INFO VariantAnnotator - Deflater: JdkDeflater; 14:02:45.349 INFO VariantAnnotator - Inflater: JdkInflater; 14:02:45.349 INFO VariantAnnotator - GCS max retries/reopens: 20; 14:02:45.349 INFO VariantAnnotator - Requester pays: disabled; 14:02:45.349 INFO VariantAnnotator - Initializing engine; 14:02:45.425 INFO FeatureManager - Using codec VCFCodec to read file file:///directory_masked/test.vcf; 14:02:45.436 INFO VariantAnnotator - Done initializing engine; 14:02:45.459 INFO ProgressMeter - Starting traversal; 14:02:45.459 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:02:45.498 WARN VariantAnnotatorEngine - Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given.; 14:02:45.505 INFO VariantAnnotator - Shutting down engine; [April 30, 2024 at 2:02:45 PM HKT] org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotator done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=285212672; java.lang.IndexOutOfBoundsException: Index 1 out of bounds for length 1; at java.base/jdk.internal.util.Preconditions.outOfBounds(Preconditions.java:64); at java.base/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Preconditions.java:70); at java.base/jdk.internal.util.Preconditions.checkIndex(Preconditions.java:266); at java.base/java.util.Objects.checkIndex(Objects.java:361); at java.base/java.util.ArrayList.get(ArrayList.java:427); at java.base/java.util.Collections$UnmodifiableList.get(Collections.java:1347); at org.broadinstitute.hellbender.tools.walkers.annotator.AllelePseudoDepth$1.visit(AllelePseudoDepth.java:119); at org.apache.commons.math3.linear.Array2DRowRealMatrix.walkInRowOrder(Array2DRowRealMatrix.java:400); at org.apache.commons.math3.linear.AbstractRealMatrix.walkInOptimizedOrder(Abstra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8800:2896,down,down,2896,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8800,1,['down'],['down']
Availability,"NIENT \ ; --nativePairHmmThreads 32 \; --createOutputVariantIndex true \; --output NA12892.raw.snps.indels.g.vcf_. **This execution time for GATK 4 Beta2 is: 51 Hours, 32 min**. Alternatively, I was running the same sample (NA12892) using GATK 3.7 using the following command: . _time -p java -XX:+UseParallelGC -XX:ParallelGCThreads=32 -Xmx128g \; -jar /gpfs/software/genomics/GATK/3.7/base/GenomeAnalysisTK.jar -T HaplotypeCaller \; -nct 8 -pairHMM VECTOR_LOGLESS_CACHING \ ; -R /gpfs/data_jrnas1/ref_data/Hsapiens/hs37d5/hs37d5.fa \; -I NA12892.realigned.recal.bam -\ ; -emitRefConfidence GVCF \; --variant_index_type LINEAR \; --variant_index_parameter 128000 \; --dbsnp /gpfs/data_jrnas1/ref_data/Hsapiens/GRCh37/variation/dbsnp_138.vcf.gz \; -o NA12892.raw.snps.indels.g.vcf _. **This execution time for GATK 3.7 is: 18 Hours, 12 min**. I don't know, how to use multithreads (e.g. -nct) for GATK 4 version to reduce the execution time on the single node. Because, we have 32 cores per node with 512GB memory available for benchmarking. To parallelize the GATK 4 workload, I used the Spark version also. . I used **GATK 4 Beta2 Spark job on the cluster of 32 nodes** (32 nodes x 32 cores, totaling 1024 cores). The execution time is almost same as GATK 4 Beta2 ( 50 Hours, 21 min). Please help me, how to reduce the execution time for GATK 4 Beta2 HaplotypeCaller? . Please see this below Spark logs:. + /gpfs/software/spark/spark-2.1.0-bin-hadoop2.7//bin/spark-submit --master spark://nsnode11:6311 --driver-java-options -Dsamjdk.use_async_io_read_samtools=false,-Dsamjdk.use_async_io_write_samtools=true,-Dsamjdk.use_async_io_write_tribble=false,-Dsamjdk.compression_level=1 --conf spark.io.compression.codec=snappy --conf spark.yarn.executor.memoryOverhead=6000 --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.userClassPathFirst=true --conf spark.driver.maxResultSize=0 --conf spark.executor.cores=1024 --conf spark.reducer.maxSizeInFlight=100m --conf spark.shuffle.file.buffer",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631:1574,avail,available,1574,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631,1,['avail'],['available']
Availability,"NIO error: ""position should be non-negative""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2516:4,error,error,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2516,1,['error'],['error']
Availability,"NVAR, or Number of genotyped VARiants. Description:. [minor/major allele] A minor (respectively major) allele is an allele present in less than (respectively, in at least) half the alleles from all the genotyped samples. Description:. [NMIN] NMIN is the number of variants of the given sample with a genotype that is not major/major. Influenced by:. Ethnicity (the distribution of NMIN is usually bimodal with Africans in one mode in the higher values). Remark:. Since this metric focuses of minor alleles, the proportion of rare variants considered (where more errors are expected) is higher. This makes NMIN more sensitive to errors than NALT, which is a desirable feature. Calculated by:. pseq i-stats (NMIN)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/536:562,error,errors,562,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/536,2,['error'],['errors']
Availability,"NVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gatk_intervals.interval_list.annotated.tsv --contig-ploidy-calls /outputs/COHORT_runDir/COHORT-calls --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5 --output /outputs/COHORT_runDir --output-prefix COHORT; ```. We used data from `PRJNA399748` project to test. #### Expected behavior. - `test_gatkgermlinecnvcaller_genotyped-intervals-cohort_0.woTimestamp.vcf` (`##contig` cut from header and only first 5 `chr22` CNVs present). ```; ##fileformat=VCFv4.2; ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Copy number maximum a posteriori value"">; ##FORMAT=<ID=CNLP,Number=.,Type=Integer,Description=""Copy number log posterior (in Phred-scale) rounded down"">; ##FORMAT=<ID=CNQ,Number=1,Type=Integer,Description=""Genotype call quality as the difference between the best and second best phred-scaled log posterior scores"">; ##FORMAT=<ID=GT,Number=1,Type=Integer,Description=""Genotype"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38.d1.vd1>; ...; ##contig=<ID=HPV-mSD2,length=7300,assembly=GRCh38.d1.vd1>; ##source=PostprocessGermlineCNVCalls; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	E07002_normal; chr1	10000	CNV_chr1_10000_10999	N	<DEL>,<DUP>	.	.	END=10999	GT:CN:CNLP:CNQ	1:0:0,80,90,100,108,116:80; chr1	11000	CNV_chr1_11000_11999	N	<DEL>,<DUP>	.	.	END=11999	GT:CN:CNLP:CNQ	1:0:0,81,86,89,92,95:81; chr1	12000	CNV_chr1_12000_12999	N	<DEL>,<DUP>	.	.	END=12999	GT:CN:CNLP:CNQ	1:0:0,93,107,119,129,137:93; chr1	13000	CNV_chr1_13000_13999	N	<DEL>,<DUP>	.	.	END=13999	GT:CN:CNLP:CNQ	1:0:0,89,95,99,102,104:89; chr1	14000	CNV_chr1_14000_14999	N	<DEL>,<DUP>	.	.	END=14999	GT:CN:CNLP:CNQ	1:0:0,86,91,93,96,97:86; chr1	15000	CNV_chr1_15000_15999	N	<DEL>,<DUP>	.	.	END=15999	GT:CN:CNLP:CNQ	1:0:0,82,88,92,97,101:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8619:2714,down,down,2714,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8619,1,['down'],['down']
Availability,"Naive subsampling of the data in the ModelSegments MCMC (i.e., limiting per-segment and global quantities to 1000 points randomly sampled each MCMC iteration and rescaling likelihoods accordingly) was implemented in #3913 to bring WGS runtime down to reasonable levels. However, this sort of naive subsampling does not accurately preserve the posterior, which leads to some artifacts in posterior estimation. @MartonKN suspected that this negatively affected downstream performance in his caller, since weights of larger segments were underestimated. . For example, the copy-ratio posterior widths should scale with the inverse square root of the number of copy-ratio bins in each segment. However, subsampling yields an artificial break at 1000 bins and screws up the scaling:. ![cr-ss](https://user-images.githubusercontent.com/11076296/51122629-417be180-17e8-11e9-9a8f-e17a5d0563f5.png). To fix this, I implemented minibatch slice sampling as described in http://proceedings.mlr.press/v33/dubois14.pdf. This uses early stopping of sampling as determined by a simple statistical test to perform approximate sampling of the posterior in a way that is more well behaved:. ![cr-mb](https://user-images.githubusercontent.com/11076296/51122680-61aba080-17e8-11e9-992a-f756a267d0ce.png). Note that the scaling levels off for larger segments, but the approximation can be made exact by taking the appropriate parameter to zero (here, this parameter is set to 0.1). However, since subsampling parameters were not exposed in the old code, I have not exposed the parameters for the approximation here. We can do this in a future PR if desired. Changing these parameters can affect runtime and results, but I've set them to reasonable values for now. The implementation involved 1) creating an abstract class to extract some common functionality shared with the old batch SliceSampler (which is now no longer used in production code), 2) implementing the MinibatchSliceSampler as described in the above referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5575:243,down,down,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5575,2,['down'],"['down', 'downstream']"
Availability,Nalini pinged me about this again. Any idea when someone will have the time?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4331#issuecomment-371210732:7,ping,pinged,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4331#issuecomment-371210732,1,['ping'],['pinged']
Availability,Native libraries should be downloadable as dependencies via gradle,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1654:27,down,downloadable,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1654,1,['down'],['downloadable']
Availability,Need better CreatePanelOfNormals error catching and messages.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2901:33,error,error,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2901,1,['error'],['error']
Availability,"Needs better error message: Using --all-sites in GATK4.1.1.0 GenotypeGVCFs throws the IllegalStateException ""There are no sources based on those query parameters""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5865:13,error,error,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865,1,['error'],['error']
Availability,"New functionality to limit the amount of memory needed to read in all the data, intended for use with large WGS callsets. (VQSR downsamples training data if there are more than 2.5M variants anyway.). Also contains port of broadinstitute/gsa-unstable#1608 and broadinstitute/gsa-unstable#1575. Addresses #3230",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3255:128,down,downsamples,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3255,1,['down'],['downsamples']
Availability,"New implementation of `SlidingWindowWalker` with some ideas from the discussion in #1528. The thinks that are requested in #1198 still holds, but now it is more general: padding option is added and construction of windows are done by interval. The code contain a lot of TODO because it relies on changes implemented in #1567, and because it is suppose to be a walker over `ReadWindow` instead of `SimpleInterval`+`ReadsContext` if reads are available. I think that with these changes it could be general to be extended by `ReadWindowWalker` and by users that needs a different way of ""slide"" over intervals.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708:441,avail,available,441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708,1,['avail'],['available']
Availability,New qual parameter errors GenotypeGVCFs in v4.0.5.0.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4975:19,error,errors,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975,1,['error'],['errors']
Availability,"New tool aiming to call all types of precise variants detectable by long read alignments (not fully functioning yet in the sense that not all types of variants are detected yet&mdash;to be handled by later PRs in this series).; This new tool splits the input long reads by scanning their alignment characteristics (number of alignments, if strand switch is involved, if mapped to the same chromosome, if have equally good alignment configurations based on the scoring tool, etc), and send them down different code path/logic units for variant type inference and VCF output.; This PR would only deal with simple INSDEL, for long reads having exactly 2 alignments (no other equally good alignment configuration) mapped to the same chromosome without strand switch or order switch (translocation or large tandem duplications), because we already have this type of variant covered in master. __UPDATE__; See updated roadmap in #2703. NEEDS TO WAIT UNTIL PART 1 IS IN.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3456:494,down,down,494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3456,1,['down'],['down']
Availability,"Next I set out to determine whether hellbender is slowing down on the larger interval simply because there is more data / a longer traversal, or because it's slower at processing the `1:1-10000000` interval than the `1:10000000-20000000` interval. And surprisingly, it appears that the latter is the case:. Time to process the `1:1-10000000` interval across two runs:. ```; GATK3: 5m25.983s 5m31.913s; HB: 6m2.156s 5m59.804s; ```. (Recall that HB was ~5% faster than GATK3 at processing the `1:10000000-20000000` interval). Moreover, our newly-installed progress meter shows that the rate at which we process records is unusually low at the start of the `1:1-10000000` interval, but is consistent throughout the processing of the `1:10000000-20000000` interval:. HB processing rate over 1:1-10000000:. ```; 14:22:19.520 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 14:22:29.522 INFO ProgressMeter - 1:769026 0.2 133000 797920.2; 14:22:39.531 INFO ProgressMeter - 1:1066133 0.3 298000 893553.2; 14:22:49.544 INFO ProgressMeter - 1:1389358 0.5 471000 941247.0; 14:22:59.572 INFO ProgressMeter - 1:1695902 0.7 636000 952785.2; 14:23:09.601 INFO ProgressMeter - 1:1961884 0.8 808000 968031.8; 14:23:19.636 INFO ProgressMeter - 1:2264803 1.0 985000 983099.3; 14:23:29.637 INFO ProgressMeter - 1:2583326 1.2 1162000 994352.2; 14:23:39.694 INFO ProgressMeter - 1:2817177 1.3 1297000 970638.9; 14:23:49.705 INFO ProgressMeter - 1:3095124 1.5 1467000 975993.8; 14:23:59.726 INFO ProgressMeter - 1:3372416 1.7 1637000 980190.6; 14:24:09.734 INFO ProgressMeter - 1:3678706 1.8 1810000 985355.8; 14:24:19.777 INFO ProgressMeter - 1:4087198 2.0 1984000 989880.0; 14:24:29.813 INFO ProgressMeter - 1:4341518 2.2 2165000 996983.7; 14:24:39.822 INFO ProgressMeter - 1:4598153 2.3 2350000 1004975.0; 14:24:49.834 INFO ProgressMeter - 1:4859664 2.5 2530000 1009892.7; 14:24:59.838 INFO ProgressMeter - 1:5103960 2.7 2712000 1014982.7; 14:25:09.887 INFO ProgressMeter - 1:5341742 ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1032#issuecomment-150660236:58,down,down,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1032#issuecomment-150660236,1,['down'],['down']
Availability,"Nice cleanup, thanks!. Also, yesterday I verified that passing in -V <hg38VCF> -supporting <b37VCF> does throw a user error:; `A USER ERROR has occurred: Badly formed genome unclippedLoc: Contig '1' does not match any contig in the GATK sequence dictionary derived from the reference; are you sure you are using the correct reference fasta file?`; I don't feel like the error text is entirely accurate, but at least no one is able to anything too crazy.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4431#issuecomment-367342846:118,error,error,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4431#issuecomment-367342846,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"NioByteUnsafe.read(AbstractNioByteChannel.java:131); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111); at java.lang.Thread.run(Thread.java:744); ```. And then warnings about lost tasks:. ```; 16/02/16 11:45:59 WARN TaskSetManager: Lost task 42.1 in stage 0.0 (TID 364, dataflow03.broadinstitute.org): java.io.IOException: Connection from /69.173.65.227:56014 closed; ```. Then errors like this:. ```; 16/02/16 11:47:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@69.173.65.227:47043] -> [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]: Error [Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]] [; ```. akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]; Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: dataflow05.broadinstitute.org/69.173.65.230:36695; ]; akka.event.Logging$Error$NoCause$. ```; 16/02/16 11:47:39 ERROR YarnScheduler: Lost executor 37 on dataflow02.broadinstitute.org: remote Rpc client disassociated; ```. This seems to be causing tasks to be re-queued and executed, which hurts performance. The command line I'm using is:. ```; gatk-launch FindBadGenomicKmersSpark --reference hdfs:///user/cwhelan/reference/Homo_sapiens_assembly19.2bit --output bad_kmers_v5_cluster.txt -- --sparkRunner SPARK --sparkMaster yarn-client --executor-memory 8g --driver-memory 8g --conf spark.broadcast.blockSize=1g; ```. Running against commit f2b3bae of branch https://github.com/broadinstitute/gatk/tree/cw_clusterize_sv_spark_tools",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1491:6062,Error,Error,6062,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"No AVX = it'll crash... sounds bad, I admit. But remember we are talking about running a deep neural network over a large dataset: is someone really going to want to do that on hardware that is 8 years old (pre-AVX)? This version of TensorFlow is now the _default_ for all Anaconda users, which in practice probably means a sizeable fraction of the machine learning community, and so having minimum hardware requirements in line with theirs is perhaps not so unreasonable?. Another option would be to change the default: have the gatk enviroment use the accelerated TensorFlow (since almost everyone has AVX, and they can get a 10X or so speedup), but make a second environment available for people that want to try to run a deep neural network on very old hardware - gatk-old?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5142#issuecomment-417041021:678,avail,available,678,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5142#issuecomment-417041021,2,['avail'],['available']
Availability,No failures so far on the latest run -- think this may have done the trick :),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3072#issuecomment-307499683:3,failure,failures,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3072#issuecomment-307499683,1,['failure'],['failures']
Availability,No prob. It was reasonably straight-forward - just a little more work than I expected to bubble down the command-line parameter. Just addressed your comments. When it's done passing tests I'll merge it in.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6577#issuecomment-621380344:96,down,down,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6577#issuecomment-621380344,1,['down'],['down']
Availability,"No problem adding hasEnd()... but can you break down what SV types have it which don't .... . *BUT* I think is important to consider here what ""end"" means in reality.. I think that ""end"" here should be the last position continuously overlapped by the variant from its start position. So for insertions, translocations and bnds, typically it would be set equal to start. . Think about ""start"" itself.... it does not make reference to the first overlapped based but the ones before it. If a BND would not have an ""end"" why should it have an ""start""?. I think start-end is just defined to what is practical for the sake of working with VCFs. What do you think? @SHuang-Broad",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3476#issuecomment-325025262:48,down,down,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3476#issuecomment-325025262,1,['down'],['down']
Availability,"No problem – I can see how this would be challenging to review – maybe it’s not even practical – if you decide it’s best just not to use it that’s OK - working on these issues has been a valuable learning experience for me. . I sent an archive with the new validations and the diffs between the old and new bams to akiezun. Although in many cases the files shared types of errors, I had to look at each file individually to take into account the particular errors in each file and how to fix them without (to the best of my knowledge) interfering with the purpose of the test. I did write a python script to use where necessary for converting multiple unpaired reads in a file to single reads, and I used bash scripts to call the picard tools to convert multiple files at a time from bam to sam for editing and then back again after they were modified. In some cases I had to modify the values in test output files to match the values produced by the test using the modified bams/sams, or just capture the new output files and use them to replace the old with the new. In two cases where file format errors appeared to be necessary but the filename did not indicate this, I renamed the files to make this clear. From: Louis Bergelson ; Sent: Thursday, August 20, 2015 2:13 PM; To: broadinstitute/hellbender ; Cc: nenewell ; Subject: Re: [hellbender] Issue 569 - bam and sam file cleanup. (#809). @nenewell Sorry this has been sitting. We've been trying to figure out how to review this one. Could you describe how you made the changes? Did you script it or go through by hand and modify them all?. —; Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051:373,error,errors,373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809#issuecomment-133123051,3,['error'],['errors']
Availability,No space left on device M2 WDL Travis failure.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3572:38,failure,failure,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3572,1,['failure'],['failure']
Availability,"No update, sorry. The PRs are pending of review, and the data is still not available for proper testing...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-361892345:75,avail,available,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-361892345,1,['avail'],['available']
Availability,No validation here. I was satisfied with the validation from the Palantir report and using this as a robustness test to show that GATK4 HC isn't going to fall over. I have a matched list of GVCFs here: /humgen/gsa-hpprojects/dev/gauthier/scratch/newQualHC/check.list @skwalker could you adapt your analysis to run with this list? I'll need to give you a different jar for the GenotypeGVCFs step on my GVCFs since the annotation format is outdated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981:101,robust,robustness,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981,1,['robust'],['robustness']
Availability,Non-informative error message when the reference dictionary (.dict) file is missing,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3492:16,error,error,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3492,1,['error'],['error']
Availability,"Nope, just oncotator. Not only that, only the CNV oncotator really needs it, since it uses a big docker image with some datasources baked-in. We could probably remove it from the M2 WDL, since that uses the typical oncotator docker image and downloads the datasources on the fly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4045#issuecomment-355373868:242,down,downloads,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4045#issuecomment-355373868,1,['down'],['downloads']
Availability,"Not a bad idea, will look into that tomorrow. Note that you are using Tensorflow 1.4 or 1.5 and that from v1.6 even the; non-Intel optimized build supports only AVX capable machines. On Thu 11 Oct 2018, 21:07 droazen, <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/CNNScoreVariants.java; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>:; >; > > @@ -198,6 +200,13 @@; > return new String[]{""No default architecture for tensor type:"" + tensorType.name()};; > }; > }; > +; > + IntelGKLUtils utils = new IntelGKLUtils();; > + if (utils.isAvxSupported() == false); > + {; > + return new String[]{CNNScoreVariants.AVXREQUIRED_ERROR};; >; > Maybe the answer is for the conda environments to set an extra environment; > variable that would allow GATK to detect which conda environment it's in.; > Then you could have a check in CNNScoreVariants that aborts the tool only; > if AVX is not present AND you're running in the Intel conda environment,; > and point the user to the non-Intel conda environment in the error message.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG6lr8HM6ItLWqfSaTKeVY4yCp07il29ks5uj6TugaJpZM4XNHdi>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651:1172,error,error,1172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651,1,['error'],['error']
Availability,"Not really... after further thought I think it would be cleaner to break down the main readme into two docs -- one quickstart doc for end users with usage instructions etc that we can include in the distro, and one for developers with compilation instructions etc that is repo only. I know you didn't want to move anything to the wiki in order to keep everything versioned, which I agree makes sense -- so this would preserve that. . Either one could be the ""main"" README, and each would have a preface pointing at the other, or we could have a very short master README pointing to both.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3199#issuecomment-312009716:73,down,down,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3199#issuecomment-312009716,1,['down'],['down']
Availability,"Not sure how java could cause this, but what version of java are you running. Above you said you ran it locally without submitting, but you didn't say whether or not that worked. Does it work locally for you ? When you use `lt` instead of `<`, is the error message exactly the same except for the `lt` ? (i.e., is it `A USER ERROR has occurred: Invalid argument 'lt'`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6241#issuecomment-548891872:251,error,error,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6241#issuecomment-548891872,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,Not sure if there is a better way to do this. UserException seems wrong since its displays USER Error.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3057:96,Error,Error,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3057,1,['Error'],['Error']
Availability,"Not sure if this is the right change. We are seeing `Unrecognized name: CALL_PS at [4:136]` error in AoU genomic workflow. So make this optional, when column is missing, return null",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9025:92,error,error,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9025,1,['error'],['error']
Availability,"Not sure if this should be a separate tickets... It appears that `SelectVariants` duplicates the INFO/AF tag in the header, this results in an invalid vcf. At least according to some tools like [rtg](https://github.com/RealTimeGenomics/rtg-tools). This is with gatk 4.0.1.0. Example:. * Input has correct header:. ```; zcat gnomad.genomes.r2.0.2.sites.hg38.chr18.vcf.gz | grep 'ID=AF,'; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency among genotypes, for each ALT allele, in the same order as listed"">; ```. * Run SelectVariants. Header has now two entries for INFO/AF. ```; gatk SelectVariants -V gnomad.genomes.r2.0.2.sites.hg38.chr18.vcf.gz -O gnomad.r2.0.2.biallelic.hg38.chr18.vcf.gz --restrict-alleles-to BIALLELIC. zcat gnomad.r2.0.2.biallelic.hg38.chr18.vcf.gz | grep 'ID=AF,'; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency among genotypes, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed"">; ```. * rtg complains:. ```; rtg vcfsubset -i gnomad.r2.0.2.biallelic.hg38.chr18.vcf.gz --keep-info AF -o - ; Error: Invalid VCF header. VCF header contains multiple field declarations with the same ID=AF; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency among genotypes, for each ALT allele, in the same order as listed"">; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed""> on line:##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed"">; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-365616512:1162,Error,Error,1162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-365616512,1,['Error'],['Error']
Availability,"Not sure if/how `GATKPathSpecifier` could cause this. @riederd Have you ever run this same command in a previous GATK version (ie., 4.1.6.0). It would be super helpful to know if 4.1.6.0 doesn't have this problem. Also, the only `GATKPathSpecifier` changes in 4.1.7.0 were for reference files. . I've long suspected that we have a file handle leak somewhere, since I encounter it when running tests locally, but have never been able to track it down.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6578#issuecomment-623454830:445,down,down,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6578#issuecomment-623454830,1,['down'],['down']
Availability,"Not sure they are related but I noted a couple of other mysteries. . . I am running the Docker version of GATK on a high end windows workstation and have allocated about 30GB to Docker. . Mystery 1: I get a warning on some commands that it is unable to determine whether it is running on google. Related to the Funcotator issue perhaps if it can’t determine where it is running it crashes out?. . Mystery 2: CollectAllelicCounts crashes with a java memory error. The -Xmx5g is several multiples of the recommendation. . gatk --java-options ""-Xmx5g"" CollectAllelicCounts -L mydata/refs/hg19_intervals.interval_list -I mydata/P50513/Tumor_P50513_2.bam -R mydata/refs/Homo_sapiens_assembly19.fasta -O mydata/P50513/P50513_Tumor.allelicCounts.tsv . . 20:31:39.543 INFO ProgressMeter - 1:169308662 59.1 85227000 1443218.8. 20:32:01.576 INFO ProgressMeter - 1:169321662 59.4 85240000 1434518.9. 20:32:22.203 INFO ProgressMeter - 1:169334662 59.8 85253000 1426484.3. 20:32:43.007 INFO ProgressMeter - 1:169341665 60.1 85260000 1418372.5. 20:33:04.435 INFO ProgressMeter - 1:169350665 60.5 85269000 1410144.2. 20:33:29.473 INFO CollectAllelicCounts - Shutting down engine. [October 5, 2019 8:33:29 PM UTC] org.broadinstitute.hellbender.tools.copynumber.CollectAllelicCounts done. Elapsed time: 60.94 minutes. Runtime.totalMemory()=5,285,347,328. . . Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded. . . at java.util.Collections.unmodifiableList(Collections.java:1287). at htsjdk.samtools.Cigar.getCigarElements(Cigar.java:54). at org.broadinstitute.hellbender.utils.read.SAMRecordToGATKReadAdapter.getCigarElements(SAMRecordToGATKReadAdapter.java:336). at org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary$ReadLengthEqualsCigarLengthReadFilter.test(ReadFilterLibrary.java:217). at org.broadinstitute.hellbender.engine.filters.ReadFilter$ReadFilterAnd.test(ReadFilter.java:70). at org.broadinstitute.hellbender.engine.filters.ReadFilter$ReadFilterAnd.test(ReadFil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548929777:456,error,error,456,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548929777,1,['error'],['error']
Availability,"Not sure what your `GenotypeGVCFs` command was, but did you use the `--genomicsdb-shared-posixfs-optimizations` option? This option is available for the import too and may improve your performance.; ```; --genomicsdb-shared-posixfs-optimizations <Boolean>; Allow for optimizations to improve the usability and performance for shared Posix; Filesystems(e.g. NFS, Lustre). If set, file level locking is disabled and file system; writes are minimized. Default value: false. Possible values: {true, false} ; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879779166:135,avail,available,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879779166,2,['avail'],['available']
Availability,"Not surprised that tests didn't change -- we only use the `PositionalDownsampler` inside a downsampling iterator for HC/M2, never a `ReservoirDownsampler` directly. I believe that the CNN tool is the first case where we wanted to use a `ReservoirDownsampler` directly inside an iterator, and ran into this bug.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-456980817:91,down,downsampling,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-456980817,1,['down'],['downsampling']
Availability,"Not the newest version of java, the newest version of GATK. Use java 8 and [Gatk 4.1.4.1](https://github.com/broadinstitute/gatk/releases/download/4.1.4.1/gatk-4.1.4.1.zip).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6384#issuecomment-575718095:138,down,download,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6384#issuecomment-575718095,1,['down'],['download']
Availability,Note that I restarted the original failing build to see if an intermittent Travis issue was to blame. But it appears that the current failure is different from the original one...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2813#issuecomment-306857194:134,failure,failure,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2813#issuecomment-306857194,1,['failure'],['failure']
Availability,"Note that no segmentation or resolution parameters have been tuned yet for either performance or runtime. Some of these are very easy wins. For example, `kernel-approximation-dimension` is set to a default of 100, and the time for segmentation scales roughly linearly with this (documentation erroneously states that the scaling is quadratic, this should be fixed---my bad). In practice, setting this to as little as 2 seems to work OK for some cases, so we should evaluate this more rigorously. This can cut WGS segmentation down from ~10 minutes (out of the total ~60 minutes for 250bp bins, typically) to ~1 minute.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-461607380:526,down,down,526,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-461607380,1,['down'],['down']
Availability,"Note that the error message says ""in the input callset"", which refers to your -V ${inputfile}. It means that for the sites in your callset that correspond with the sites in the training resources, your callset does not have any variants with FS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6715#issuecomment-663012883:14,error,error,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6715#issuecomment-663012883,1,['error'],['error']
Availability,Note that we got a second report for this error here: https://github.com/broadinstitute/gatk/issues/6790,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-686561623:42,error,error,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-686561623,1,['error'],['error']
Availability,"Note: the test failures seem to have a lot of docker ""toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading"" and are probably not due to these changes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7219#issuecomment-824459892:15,failure,failures,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7219#issuecomment-824459892,1,['failure'],['failures']
Availability,"Notes from @kvg :. Manuscript on LdBG details: https://academic.oup.com/bioinformatics/article/34/15/2556/4938484. Reference implementation of the assembler in C: https://github.com/mcveanlab/mccortex; mcveanlab/mccortex. My Java library for accessing and manipulating LdBGs and associated formats: https://github.com/mcveanlab/CortexJDK. A useful starting point is the tests I've written to produce Figure 1 (the one with the pentagram cycle) from the manuscript. Without links: https://github.com/mcveanlab/CortexJDK/blob/4ba64ee268314729c871916dfc9ee7c9c422c5cb/public/java/tests/uk/ac/ox/well/cortexjdk/utils/traversal/TraversalEngineTest.java#L210. With links: https://github.com/mcveanlab/CortexJDK/blob/4ba64ee268314729c871916dfc9ee7c9c422c5cb/public/java/tests/uk/ac/ox/well/cortexjdk/utils/traversal/TraversalEngineTest.java#L229. (Oh and FYI, there's one place where my implementation of the read threading currently doesn't match the McCortex C reference implementation - the handling of sequencing errors and replacing the errorful kmers with kmers from the graph. It's an easy thing to add; I just hadn't gotten around to it because I didn't have the need to do that alignment myself given that all my graphs and links were constructed with McCortex anyway.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5828#issuecomment-475750843:1010,error,errors,1010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5828#issuecomment-475750843,2,['error'],"['errorful', 'errors']"
Availability,"Notes:. - Classes in hellbender/tools/picard/analysis/artifacts are removed and replaced with Picard versions (except Transition, which is not public in Picard).; - GATK version of GatherVcfs is retained, and the Picard version is masked out - is this what we want ?; - The non-Spark GATK metrics tools have been removed and replaced with the Picard versions. The test data is retained (but moved) since its used by the Spark metrics tool tests. Additional changes we'll want to make separately to minimize the complexity of this PR:; - Eliminate the download of picard.jar from the GATK WDL tests and update the WDL to run Picard tools through GATK.; - Unify and merge the Picard and GATK program groups. These are similar, but not identical, and the combined result has artificial/duplicate groups.; - Normalize the confusing mix of Alpha/Beta/Experimental tags and comments.; - Add unified doc and tab-completion tasks that include Picard.; - Remove and replace SamComparison and Transition classes with the Picard versions.; - Fix GATK CompareBaseQualities (its a PicardCommandLineProgram).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3620:231,mask,masked,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3620,2,"['down', 'mask']","['download', 'masked']"
Availability,"Noticed some compile warnings in the Travis logs, looks like a funky apostrophe is to blame?. :compileJava/gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/ReferenceConfidenceVariantContextMerger.java:511: error: unmappable character for encoding ASCII; // if there???s more than 1 DEL allele then we need to use the best one; ^",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3748:220,error,error,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3748,1,['error'],['error']
Availability,Now can create annotations for symbollic alleles and masked alleles.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5406:53,mask,masked,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5406,1,['mask'],['masked']
Availability,"Now run with `--maxIndelSize 250`.; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250.vcf.gz; 17:24:16.345 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 05, 2018 5:24:16 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 17:24:16.502 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 17:24:16.502 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-24-gb43bc27-SNAPSHOT; 17:24:16.502 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:24:16.502 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 17:24:16.502 INFO LeftAlignAndTrimVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 17:24:16.503 INFO LeftAlignAndTrimV",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418887543:149,Down,Downloads,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418887543,2,['Down'],['Downloads']
Availability,"Now that UserException has been ported, we should eventually make an effort to wrap exceptions in htsjdk that are the result of user error in UserExceptions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/86:133,error,error,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/86,1,['error'],['error']
Availability,"Now that we've added the complete B37 and HG38 references to our test data (https://github.com/broadinstitute/gatk/pull/5309), we should remove redundant snippets of these references to save space, and replace usages of the snippets with usages of the full-sized references.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5313:144,redundant,redundant,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5313,1,['redundant'],['redundant']
Availability,Now throws a user error for an AD field with only 1 value in MAF mode.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5860:18,error,error,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5860,1,['error'],['error']
Availability,NullPointerException error while running Mutect2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6142:21,error,error,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6142,1,['error'],['error']
Availability,NullPointerException error while running VariantEval,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6212:21,error,error,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6212,1,['error'],['error']
Availability,"O ContextHandler:781 - Started o.s.j.s.ServletContextHandler@44084713{/jobs,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@43c0c13a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@731db93f{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2ad2b274{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6fde6f05{/stages,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7114e780{/stages/json,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4def42c3{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@38d525aa{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@f9b8129{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7530090a{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4492b393{/storage,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@55fb36de{/storage/json,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@63a7781{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@73b74615{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@22686ddb{/environment,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContext",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:7554,AVAIL,AVAILABLE,7554,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['AVAIL'],['AVAILABLE']
Availability,"O FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:30:15.674 INFO FilterMutectCalls - Deflater: IntelDeflater; 11:30:15.674 INFO FilterMutectCalls - Inflater: IntelInflater; 11:30:15.674 INFO FilterMutectCalls - GCS max retries/reopens: 20; 11:30:15.675 INFO FilterMutectCalls - Requester pays: disabled; 11:30:15.675 INFO FilterMutectCalls - Initializing engine; 11:30:15.870 INFO FeatureManager - Using codec VCFCodec to read file file:///tmp/tmp.8lRGFREUhm/mu.2.vcf; 11:30:15.883 INFO FilterMutectCalls - Done initializing engine; 11:30:15.929 INFO ProgressMeter - Starting traversal; 11:30:15.929 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 11:30:15.930 INFO FilterMutectCalls - Starting pass 0 through the variants; 11:30:15.958 INFO FilterMutectCalls - Finished pass 0 through the variants; 11:30:15.970 INFO FilterMutectCalls - Starting pass 1 through the variants; 11:30:15.974 INFO FilterMutectCalls - Shutting down engine; [November 7, 2019 11:30:15 AM CET] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2252865536; java.lang.IllegalArgumentException: log10 p: Values must be non-infinite and non-NAN; 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.clusterProbabilities(SomaticClusteringModel.java:203); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:96); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.TumorEvidenceFilter.calculateErrorProbability(TumorEvidenceFilter.java:27); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6255:9463,down,down,9463,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6255,1,['down'],['down']
Availability,"O MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 679.0 B, free 399.8 GB); 23/05/23 13:20:18 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on d01.capitalbiotech.local:41352 (size: 679.0 B, free: 399.8 GB); 23/05/23 13:20:18 INFO SparkContext: Created broadcast 17 from broadcast at ReadsSparkSink.java:146; 23/05/23 13:20:18 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 7.3 KB, free 399.8 GB); 23/05/23 13:20:18 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 679.0 B, free 399.8 GB); 23/05/23 13:20:18 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on d01.capitalbiotech.local:41352 (size: 679.0 B, free: 399.8 GB); 23/05/23 13:20:18 INFO SparkContext: Created broadcast 18 from broadcast at BamSink.java:76; 23/05/23 13:20:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2; 23/05/23 13:20:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 23/05/23 13:20:18 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78; 23/05/23 13:20:18 INFO DAGScheduler: Registering RDD 68 (mapToPair at SparkUtils.java:161) as input to shuffle 7; 23/05/23 13:20:18 INFO DAGScheduler: Got job 6 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions; 23/05/23 13:20:18 INFO DAGScheduler: Final stage: ResultStage 30 (runJob at SparkHadoopWriter.scala:78); 23/05/23 13:20:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 29); 23/05/23 13:20:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 29); 23/05/23 13:20:18 INFO DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[68] at mapToPair at SparkUtils.java:161), which has no missing parents; 23/05/23 13:20:18 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 14.8 KB, free 399.8 GB); 23/05/23 13:20:18 INFO MemoryStore: Block br",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8339:48305,failure,failures,48305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339,1,['failure'],['failures']
Availability,"O ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 23:40:38.767 INFO BaseRecalibrator - 62393454 read(s) filtered by: MappingQualityNotZeroReadFilter; 0 read(s) filtered by: MappingQualityAvailableReadFilter; 0 read(s) filtered by: MappedReadFilter; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter; 0 read(s) filtered by: NotDuplicateReadFilter; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter; 0 read(s) filtered by: WellformedReadFilter; 62393454 total reads filtered; 23:40:38.769 INFO ProgressMeter - unmapped 1.0 0 0.0; 23:40:38.769 INFO ProgressMeter - Traversal complete. Processed 0 total reads in 1.0 minutes.; 23:40:38.770 INFO BaseRecalibrator - Calculating quantized quality scores...; 23:40:38.784 INFO BaseRecalibrator - Writing recalibration report...; 23:40:38.821 INFO BaseRecalibrator - ...done!; 23:40:38.822 INFO BaseRecalibrator - BaseRecalibrator was able to recalibrate 0 reads; 23:40:38.822 INFO BaseRecalibrator - Shutting down engine; [February 26, 2020 11:40:38 PM EST] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 1.07 minutes.; Runtime.totalMemory()=3622305792; Tool returned:; SUCCESS; ```; ApplyBQSR; ```; 23:41:16.951 INFO ApplyBQSR - The Genome Analysis Toolkit (GATK) v4.1.4.1-83-g031c407-SNAPSHOT; 23:41:16.952 INFO ApplyBQSR - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:41:16.952 INFO ApplyBQSR - Executing as <XXX@XXX> on Linux v3.10.0-957.12.1.el7.x86_64 amd64; 23:41:16.952 INFO ApplyBQSR - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-b08; 23:41:16.952 INFO ApplyBQSR - Start Date/Time: February 26, 2020 11:41:16 PM EST; 23:41:16.952 INFO ApplyBQSR - ------------------------------------------------------------; 23:41:16.952 INFO ApplyBQSR - ------------------------------------------------------------; 23:41:16.953 INFO ApplyBQSR - HTSJDK Version: 2.21.2; 23:41:16.953 INFO ApplyBQSR - Picard Version: 2.21.9; 23:41:16.953 INFO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237:5489,down,down,5489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237,1,['down'],['down']
Availability,"O ProgressMeter - unmapped 3.7 5276000 1415236.1; > 15:38:44.689 WARN IntelInflater - Zero Bytes Written : 0; > 15:38:44.691 INFO SplitNCigarReads - 0 read(s) filtered by:; > AllowAllReadsReadFilter; >; > 15:38:45.179 INFO OverhangFixingManager - Overhang Fixing Manager saved; > 434 reads in the first pass; > 15:38:45.182 INFO SplitNCigarReads - Starting traversal pass 2; > 15:38:54.809 INFO ProgressMeter - 1:67424901 3.9 5368000 1376916.3; > 15:39:05.144 INFO ProgressMeter - 1:153935839 4.1 5421000 1331662.9; > 15:39:15.368 INFO ProgressMeter - 1:210675831 4.2 5438000 1282169.2; > 15:39:25.463 INFO ProgressMeter - 10:119579965 4.4 5479000 1242549.2; > 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; > 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; > 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at h",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:5956,down,down,5956,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['down'],['down']
Availability,"O ReblockGVCF - Inflater: IntelInflater; 11:25:55.711 INFO ReblockGVCF - GCS max retries/reopens: 20; 11:25:55.711 INFO ReblockGVCF - Requester pays: disabled; 11:25:55.711 WARN ReblockGVCF - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: ReblockGVCF is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 11:25:55.711 INFO ReblockGVCF - Initializing engine; 11:25:56.290 INFO FeatureManager - Using codec VCFCodec to read file file:///rprojectnb2/kageproj/gatk/gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz; 11:25:56.569 INFO ReblockGVCF - Done initializing engine; 11:25:56.690 INFO ProgressMeter - Starting traversal; 11:25:56.690 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 11:26:06.694 INFO ProgressMeter - chr1:5066659 0.2 771000 4624612.6; 11:26:16.711 INFO ProgressMeter - chr1:12628456 0.3 1886000 5652065.3; 11:26:26.103 INFO ReblockGVCF - Shutting down engine; [June 30, 2021 11:26:26 AM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ReblockGVCF done. Elapsed time: 0.51 minutes.; Runtime.totalMemory()=3303538688; java.lang.IllegalArgumentException: cannot add a genotype with GQ=-1 because it's not within bounds [0,20); 	at org.broadinstitute.hellbender.utils.variant.writers.HomRefBlock.add(HomRefBlock.java:99); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFBlockCombiner.createNewBlock(GVCFBlockCombiner.java:168); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFBlockCombiner.addHomRefSite(GVCFBlockCombiner.java:137); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFBlockCombiner.submit(GVCFBlockCombiner.java:200); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFWriter.add(GVCFWriter.java:91); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ReblockGVCF.apply(ReblockGVCF.java:229); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalke",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7334:4168,down,down,4168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334,1,['down'],['down']
Availability,"OK - PedigreeValidationType is now set in the constructor and is final. This does not separate the two intertwined codepaths around PedigreeFile vs. FounderIds, but that was a pre-existing problem. It doesnt doesnt change the pre-existing weirdness around the timing of setting pedigreeFile and/or founderIds within GATKAnnotationPluginDescriptor, where PedigreeAnnotation gets special treatment. I dont think this makes that situation any worse. if you still have concerns on this proposal, I actually think I could make our code work if you simply exposed a protected getPedigreeFile() method on PedigreeAnnotation. I can make the SampleDB instance in my code without needed to share code here. It seemed useful to expose some of that code to avoid duplication, but if it's going to over-complicate we can remove it. Also: that one test failure seems potentially unrelated (https://travis-ci.com/github/broadinstitute/gatk/jobs/510624560)? A compile issue with javadoc?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853986169:839,failure,failure,839,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853986169,2,['failure'],['failure']
Availability,"OK I did some more testing, turns out I was wrong (the forum post was also misleading). ; The problems in the comparison here are not between the Input VCF and the dbSNP, but between the dbSNP and the Reference FASTA file. . I.e. I was using the dbSNP file where the order of chromosomes was like this; `1,2,3...` (downloaded from BROAD); But the Reference FASTA had the order of chromosomes like this; `1,10,11...` (downloaded from ENSEMBL). So yeah... if there's an option for the comparator not to look at the order of chromosomes, cool, and if not, then sorting the dbSNP according to the Reference FASTA is the way to go",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6855#issuecomment-712495203:315,down,downloaded,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6855#issuecomment-712495203,2,['down'],['downloaded']
Availability,"OK, I think I've narrowed down the problem. Curiously, it seems to be related to the inclusion of genotypes in the `--alleles` input. My original alleles input included a number of phased genotypes from 1000 Genomes Project. When I cut down the `--alleles` VCF input to just those two sites while retaining the genotypes (as follows) the crash is reproduced:; ```; ##fileformat=VCFv4.1; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	HG00096	HG00097	HG00099	HG00100	HG00101	HG00102	HG00103	HG00105	HG00106	HG00107	HG00108	HG00109	HG00110	HG00111	HG00112	HG00113	HG00114	HG00115	HG00116	HG00117	HG00118	HG00119	HG00120	HG00121	HG00122	HG00123	HG00125	HG00126	HG00127	HG00128	HG00129	HG00130	HG00131	HG00132	HG00133	HG00136	HG00137	HG00138	HG00139	HG00140	HG00141	HG00142	HG00143	HG00145	HG00146	HG00148	HG00149	HG00150	HG00151	HG00154	HG00155	HG00157	HG00158	HG00159	HG00160	HG00171	HG00173	HG00174	HG00176	HG00177	HG00178	HG00179	HG00180	HG00181	HG00182	HG00183	HG00185	HG00186	HG00187	HG00188	HG00189	HG00190	HG00231	HG00232	HG00233	HG00234	HG00235	HG00236	HG00237	HG00238	HG00239	HG00240	HG00242	HG00243	HG00244	HG00245	HG00246	HG00250	HG00251	HG00252	HG00253	HG00254	HG00255	HG00256	HG00257	HG00258	HG00259	HG00260	HG00261	HG00262	HG00263	HG00264	HG00265	HG00266	HG00267	HG00268	HG00269	HG00271	HG00272	HG00273	HG00274	HG00275	HG00276	HG00277	HG00278	HG00280	HG00281	HG00282	HG00284	HG00285	HG00288	HG00290	HG00304	HG00306	HG00308	HG00309	HG00310	HG00311	HG00313	HG00315	HG00318	HG00319	HG00320	HG00321	HG00323	HG00324	HG00325	HG00326	HG00327	HG00328	HG00329	HG00330	HG00331	HG00332	HG00334	HG00335	HG00336	HG00337	HG00338	HG00339	HG00341	HG00342	HG00343	HG00344	HG00345	HG00346	HG00349	HG00350	HG00351	HG00353	HG00355	HG00356	HG00357	HG00358	HG00360	HG00361	HG00362	HG00364	HG00365	HG00366	HG00367	HG00368	HG00369	HG00371	HG00372	HG00373	HG00375	HG00376	HG00378	HG00379	HG00380	HG00381	HG00382	HG00383	HG00384	HG00403	HG00404	HG00406	HG",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5336#issuecomment-431890629:26,down,down,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5336#issuecomment-431890629,2,['down'],['down']
Availability,"OK, I think things are looking good! Updated a bunch of things, including the following:. - conda 23.1.0 -> 23.10.0; in the base Docker, also disabled conda auto-updating and set the solver to the much faster libmamba (NOTE: before this PR went in, this change was actually made in https://github.com/broadinstitute/gatk/pull/8610); - python 3.6.10 -> 3.10.13; - pymc 3.1 -> 5.10.0; - theano 1.0.4 -> pytensor 2.18.1; - added pytorch 2.1.0; - removed tensorflow 1.15.0 and other CNN dependencies; - added libblas-dev to the base Docker; I think MKL versions of all packages are being used, but we should verify!. These and other packages (numpy, scipy, etc.) are all pretty much at the latest available versions for python 3.10. I've also bumped version numbers for our internal python packages. I also made all of the changes to the gCNV code to accommodate any changes introduced by PyMC/Pytensor. For the most part, these were minor renamings of `theano`/`tt`/etc. to `pytensor`/`pt`/etc. However, there were some more nontrivial changes, including to 1) model priors (since some of the distributions previously used were removed or are now supported differently), 2) the implementation of posterior sampling, 3) some shape/dimshuffle operations, and other things along these lines. Using a single test shard of 20 1kGP WES samples x 1000 intervals, I have verified determinism/reproducibility for DetermineGermlineContigPloidy COHORT/CASE modes, GermlineCNVCaller COHORT/CASE modes, and PostprocessGermlineCNVCalls. Numerical results are also relatively close to those from 4.4.0.0 for all identifiable call and model quantities (albeit far outside any reasonable exact-match thresholds, most likely due to differences in RNG, sampling, and the aforementioned priors). Some remaining TODOs:. - [x] Rebuild and push the base Docker. EDIT: Mostly covered by #8610, but this also includes an addition of `libblas-dev`.; - [x] Update expected results for integration tests, perhaps add any that might ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285:693,avail,available,693,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285,1,['avail'],['available']
Availability,"OK, finally tracked down that original issue from Mehrtash concerning the bundling: https://github.com/broadinstitute/gatk/issues/4397 As we discussed, there was a lot of back and forth to try to resolve this issue, and it was confounded by a lexicographical bug (which may have been reintroduced here). The last chapter in this saga was https://github.com/broadinstitute/gatk/pull/5490. If the matrix transpose is still troublesome and we can avoid it by being more clever with WDL indexing, then maybe we can explore that. Or we can just see if there are analogous existing WDLs and borrow their solution. However, note that @mwalker174 indicated that the *creation* of the matrix itself is troublesome for call caching. If bundling is the only answer and we are willing to pay the cost of localizing all gCNV results to all shards, it might make things easier to first bundle everything up at the end of each gCNV task. Also, would Cromwell be able to handle things if we change the bundling from a) *all* gCNV results (i.e., across all samples and shards) to b) a single bundled global quantity (model + interval lists) + calls bundled (across shards) per sample? Each postprocessing task would then take the global bundle + the bundle containing calls for that sample. That seems like it would resolve Mehrtash's original complaint, while still minimizing the number of files whizzing around. We also discussed batching by sample at the postprocessing task level, but I think we want to keep this task at the per-sample level for parallelism.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-632303744:20,down,down,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-632303744,1,['down'],['down']
Availability,"OK, relaxed the exact match to a delta of 1E-6 (chosen because doubles are formatted in somatic CNV outputs as `""%.6f""`) and tests pass on Travis (modulo an unrelated intermittent timeout failure). Note also that I was also able to reproduce locally by switching between Java 8 and 11. Had to add some quick test code for doing the comparisons; not actually sure if we have other utility methods to do so somewhere in the codebase. Another interesting note: I tried to clean up the offending use of log10factorial in AlleleFractionLikelihoods, but this introduced numerical differences at the ~1E-3 level. I think all of the round tripping between log and log10 actually adds up. Some digging revealed that this was introduced way back in gatk-protected in https://github.com/broadinstitute/gatk-protected/commit/aeec297e104db9f5196cb8f8e6691133302474bc#diff-34bd76cb2a416a212e25cbfb11298207265fb9cced775918aefcdb6b91ebc247. Despite the fact that we could easily replace the use of log10factorial with a private logGamma cache, at this point I think it makes more sense to freeze the current behavior. But if similar numerical changes are introduced to ModelSegments in the future, then it might make sense to clean this up at that point as well. Anyway, changed the title of the PR to reflect this update. Should be ready to go!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1023793014:188,failure,failure,188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1023793014,1,['failure'],['failure']
Availability,"OK, that's reasonable. I'll dig into the other test changes. I can answer a few:. - Regarding passing the VariantWalker: I agree that's not an improvement by itself, but I would argue it's not that much different than it was. My plan is to pass a VariantEvalContext object, which would obscure any need to have knowledge of the walker. In an attempt to keep this PR simpler, I didnt complete that work. I do expect to make a second PR in relatively short order, once we get this resolved. - With respect to testEvalTrackWithoutGenotypesWithSampleFields and the different reference: I think the issue is that the old version (master GATK branch) didnt validate as strictly. When switching to MultiVariantWalkerGroupedOnStart, the reference is required, and the tool will error if the contigs dont match. VariantEval on the master branch didnt really need the reference for anything, and was apparently more permissive if it didnt line up. It probably preferentially grabbed the dictionary from the VCF header. I will look into those other questions",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-744698072:770,error,error,770,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-744698072,2,['error'],['error']
Availability,"OK, the first test run I tried was with 1kb bins and *no additional normals*. Coverage takes about an hour to collect per BAM and ploidy inference takes about 10 minutes. A few things:. 1) Looks like we are concordant with the truth CN on X for all but 3/40 of the samples. The GQs for these discordant calls are low (~3, 23, and 25 compared with ~400 for most of the others). 2) However, we are striking out on over half of the samples on Y. We mostly call 1 copy when the truth calls 0. Mehrtash thinks this is because a) I didn't mask out any PARs or otherwise troublesome regions on Y and b) I didn't include any other normals. I'll try rerunning with a mask first, then with other normals, and then with both. Hopefully this should clear up with just the mask. 3) There are a few samples where we strike out because the truth calls 2 copies on Y and we call 1. Mehrtash pointed out that this is most likely because the prior table we put together assumes Y can have at most 1 copy. So hopefully these are trivially recovered once we relax this. 4) The GQs are weirdly high on 1, X, and Y compared to the rest of the autosomes. @ldgauthier any idea why this might be? If there's no reason, then something funny is going on within the tool. I haven't gotten a chance to plot any of the counts data yet, either, which may make things more obvious. I'll do this today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-364234449:533,mask,mask,533,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-364234449,8,"['mask', 'recover']","['mask', 'recovered']"
Availability,"OK. @LeeTL1220 How do you want to handle this? I'd strongly prefer to stick with data.table despite the GitHub issue above, since it's much faster than the usual read.table (e.g., 4 seconds vs. 45 seconds for your ~9.7M row WGS copy-ratio TSV that originally caused the error). Is there any other way we can increase /dev/shm size? @droazen @jamesemery @lbergelson Any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357324264:270,error,error,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357324264,1,['error'],['error']
Availability,"OK. However, don't forget that the denoising model is fit independently in each block. So introducing too many blocks could cause overfitting, in a sense. Also, you want to make sure that you have enough bins in each block to learn the model. 10k seems safe, but I'd spot check results first if you want to go down to 1k.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-391071615:310,down,down,310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-391071615,2,['down'],['down']
Availability,"OK. In GATK3, the sharding size is calculated in `GenomeAnalysisEngine.getShardStrategy`. Since `GenotypeGVCFs` is a RodWalker and does not use input reads (BAM file), the shard size is `1,000,000`. ; This might be more than a thread safety bug (which is easy to fix, by making `GenotypingEngine.calculateOutputAlleleSubset() ` `synchronized`). What worries me is If the cache of upstream deletions spans intervals, this code will not work since the processing is asynchronous. For example, if there are 2 threads and the removed deletion crosses the shard barrier and the downstream interval thread is first to process, it will not see the upstream removed deletion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270467197:573,down,downstream,573,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270467197,1,['down'],['downstream']
Availability,"OLS : false; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" v",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:4848,ERROR,ERROR,4848,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['ERROR'],['ERROR']
Availability,"OMPRESSION_LEVEL : 2; 15:38:00.019 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:38:00.020 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:38:00.020 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:38:00.020 INFO GenotypeGVCFs - Deflater: IntelDeflater; 15:38:00.020 INFO GenotypeGVCFs - Inflater: IntelInflater; 15:38:00.020 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 15:38:00.020 INFO GenotypeGVCFs - Requester pays: disabled; 15:38:00.020 INFO GenotypeGVCFs - Initializing engine; 15:38:00.590 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.3-6069e4a; 15:38:00.652 INFO GenotypeGVCFs - Shutting down engine; [March 14, 2023 3:38:00 PM CET] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2326265856; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:463); at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:365); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:319); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:291); at org.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76); at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:67); at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:726); at org.broadinstitute.hellbender.engine.VariantLocusWalker.onStartup(VariantLocusWalker.jav",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1468228918:3165,ERROR,ERROR,3165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1468228918,1,['ERROR'],['ERROR']
Availability,"OOLS : true; 16:11:10.076 INFO SVAnnotate - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:11:10.076 INFO SVAnnotate - Deflater: IntelDeflater; 16:11:10.077 INFO SVAnnotate - Inflater: IntelInflater; 16:11:10.077 INFO SVAnnotate - GCS max retries/reopens: 20; 16:11:10.077 INFO SVAnnotate - Requester pays: disabled; 16:11:10.077 INFO SVAnnotate - Initializing engine; 16:11:10.152 INFO FeatureManager - Using codec VCFCodec to read file file:///home/Division/user/2_Exome/snv_GWAS_data/disease_related_SV/test/test.vcf; 16:11:10.251 INFO SVAnnotate - Done initializing engine; 16:11:10.260 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 43): ##description: evidence-based annotation of the human genome (GRCh38), version 43 (Ensembl 109) Continuing, but errors may occur.; 16:11:10.260 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 43): ##description: evidence-based annotation of the human genome (GRCh38), version 43 (Ensembl 109) Continuing, but errors may occur.; 16:11:10.261 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///home/Division/user/2_Exome/snv_GWAS_data/disease_related_SV/gencode.v43.basic.modified_annotation.gtf; 16:11:10.261 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 43): ##description: evidence-based annotation of the human genome (GRCh38), version 43 (Ensembl 109) Continuing, but errors may occur.; 16:11:21.599 INFO ProgressMeter - Starting traversal; 16:11:21.600 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 16:11:21.632 INFO ProgressMeter - unmapped 0.0 53 99375.0; 16:11:21.632 INFO ProgressMeter - Traversal complete. Processed 53 total variants in 0.0 minutes.; 16:11:21.674 INFO SVAnnotate - Shutting down engine; [2023年7月5日 CST 下午4:11:21] org.broadinstitute.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8394#issuecomment-1621377138:3244,error,errors,3244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8394#issuecomment-1621377138,1,['error'],['errors']
Availability,"OR_SAMTOOLS : false; 09:54:28.494 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:54:28.495 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:54:28.495 INFO FilterMutectCalls - Deflater: IntelDeflater; 09:54:28.495 INFO FilterMutectCalls - Inflater: IntelInflater; 09:54:28.495 INFO FilterMutectCalls - GCS max retries/reopens: 20; 09:54:28.495 INFO FilterMutectCalls - Requester pays: disabled; 09:54:28.495 INFO FilterMutectCalls - Initializing engine; 09:54:28.840 INFO FeatureManager - Using codec VCFCodec to read file file:///mnt/md0/DataProcess/Ranshi/Mutect2/Try.vcf.gz; 09:54:28.923 INFO FilterMutectCalls - Done initializing engine; 09:54:28.975 INFO ProgressMeter - Starting traversal; 09:54:28.975 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 09:54:28.976 INFO FilterMutectCalls - Starting first pass through the variants; 09:54:29.139 INFO FilterMutectCalls - Shutting down engine; [August 20, 2019 9:54:29 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=1809317888; java.lang.ArrayIndexOutOfBoundsException: 2; 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyContaminationFilter(Mutect2FilteringEngine.java:64); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:518); 	at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); 	at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemain",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6102:2866,down,down,2866,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6102,1,['down'],['down']
Availability,O] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Pa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4686:1050,FAILURE,FAILURE,1050,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686,1,['FAILURE'],['FAILURE']
Availability,Of the three Joint Callings workflow I resubmitted yesterday 2 failed with this error.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3735#issuecomment-339006230:80,error,error,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3735#issuecomment-339006230,1,['error'],['error']
Availability,"Oh I'm so sorry about that. I ran ValidateVariants on a few samples (running the whole batch now). This is the error I get when I don't specify the --validate-GVCF or --validation-type-to-exclude ALLELES . > A USER ERROR has occurred: Input renamed_seq1trimq10_LHA_AS02_1.raw_variants.g.vcf fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position DS235882:56737 are not observed at all in the sample genotypes. The position in the gvcf. > DS235882 56737 . A T,<NON_REF> 0 . BaseQRankSum=-3.172;DP=29;ExcessHet=3.0103;MLEAC=0,0;MLEAF=0.00,0.00;MQRankSum=-1.507;RAW_MQandDP=97098,29;Re; adPosRankSum=-0.312 GT:AD:DP:GQ:PL:SB 0/0:27,2,0:29:67:0,67,1051,81,1056,1070:13,14,1,1. When I exclude the Alleles, I get no errors for the samples I checked thus far. This error cannot be causing the initial problem because I got the same error (in different position) on some gvcfs that are currently working fine with CombineGVCFs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6913#issuecomment-716710858:111,error,error,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6913#issuecomment-716710858,5,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,"Oh nice, that seems to be the ticket. All reads are used when setting that parameter. May I ask what the logic behind performing the downsampling is? Isn't there a risk of removing valid alignments that contribute to low abundance variation events? This would maybe only really be a problem when you are analysing sequences from a population of cells/microbes, but maybe the reward is greater than the risk?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139111543:133,down,downsampling,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139111543,1,['down'],['downsampling']
Availability,"Oh, I hadn't noticed that there was a compilation warning causing the test to fail. ```; /gatk/src/test/java/org/broadinstitute/hellbender/MainTest.java:55: warning: [serial] serializable class ExitNotAllowedExcepion has no definition of serialVersionUID; private static final class ExitNotAllowedExcepion extends SecurityException {; ^; error: warnings found and -Werror specified; ```. Please fix that also :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4283#issuecomment-361661772:338,error,error,338,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4283#issuecomment-361661772,1,['error'],['error']
Availability,"Oh, I just realized I also changed from copying in whole dbsnp vcf to streaming it with NIO. Could this be from uncompressing a vcf.gz file rather than the bam? If so I can easily switch back to copying in dbsnp. Although that error message leads me to believe the problem is still with the BAM.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317029371:227,error,error,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317029371,1,['error'],['error']
Availability,"Oh, I meant we should fix the error message so it accurately tells you what's wrong.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6501#issuecomment-599610470:30,error,error,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6501#issuecomment-599610470,1,['error'],['error']
Availability,"Oh, I missed that they had already tried reindexing. @ahaessly can you find out what tool and what version was used to reindex this file ? We've had several reports of this error message in the last month or two, but reindexing has usually fixed it. I had been assuming that the corrupt index files were created by older versions of GATK/Picard tools, but it would super helpful to know if current versions of either samtools or GATK/Picard is creating these, and will help determine if we need to relax this check.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7152#issuecomment-804930832:173,error,error,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7152#issuecomment-804930832,1,['error'],['error']
Availability,"Oh, yes, it is surely not a typo, it was just my mental shortcut because of the previous messages. I use the local storage on our computing cluster. I will try to download the files once again, maybe it will solve the problem.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661896755:163,down,download,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661896755,1,['down'],['download']
Availability,Ohh. This looks like what we have really wanted when we refactor the test suite to test spark and other tools using the same methods. This should bring restful nights to us all. Unfortunately it looks like most of the docker tests have failed with errors along the lines of this: ; ```; org.gradle.api.internal.tasks.testing.TestSuiteExecutionException: Could not complete execution for Gradle Test Executor 1.; 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:63); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(Exec,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858:248,error,errors,248,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858,1,['error'],['errors']
Availability,"Ok @jean-philippe-martin, I have an updated patch that seems to resolve the 503 errors! It's here: https://github.com/droazen/google-cloud-java/tree/dr_retry_CloudStorageReadChannel_fetchSize. Will you have time before you leave on vacation to open a PR against google-cloud-java? If not, let me know and we'll try to sort out our CLA issues and PR it ourselves. I didn't have time to write unit tests, unfortunately, though we're running it now with 1000 concurrent jobs each accessing 11,000 files and not seeing any errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315447319:80,error,errors,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315447319,2,['error'],['errors']
Availability,"Ok, I know @danxmoran also kicked off jobs trying those three options @droazen mentioned above, but I at least tried the first option and got this error message:. ```; Using GATK jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:gc_log.log -Xms4000m -Dsamjdk.use_async_io_write_samtools=false -jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar BaseRecalibrator -R /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-960e-17578f6b382c/call-SortAndFixSampleBam/CHMI_CHMI3_WGS2.aligned.duplicate_marked.sorted.bam --useOriginalQualities -O CHIM.recal_data.csv -knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf -knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz -L chr5:1+; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.pDg1Ou; [July 24, 2017 5:46:04 PM UTC] BaseRecalibrator --useOriginalQualities true --knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf --knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz --output CHIM.recal_data.csv --intervals chr5:1+ --input gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-960e-17578f6b382c/call-SortAndFixSampleBam/CHMI_CHMI3_WGS2.aligned.duplicate_marked.sorted.bam --reference /cromwell_root/broad-references/hg38/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317520824:147,error,error,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317520824,1,['error'],['error']
Availability,"Ok, I was wrong download then.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6719#issuecomment-662769770:16,down,download,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6719#issuecomment-662769770,1,['down'],['download']
Availability,"Ok, so I was able to run all the python tests using ~/.matplotlib/matplotlibrc. @mbabadi Wondering why you changed this to DO NOT MERGE. Do you have an alternative proposal ? Removing the libgcc-ng dependency doesn't solve the whole mac problem, but at least if we do remove it the workaround for the matplotlib part is easily conveyed. Also, @samuelklee @mbabadi, is there any visibility for end-users that they need to establish the conda env to run these tools (like in the doc summary, etc.). I'm guessing the failure mode for just running without the environment will be pretty cryptic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4087#issuecomment-356111971:514,failure,failure,514,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4087#issuecomment-356111971,1,['failure'],['failure']
Availability,"Ok, thanks -- I'll add a note to the release notes retroactively to try to cut down on user surprise.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4390#issuecomment-364990476:79,down,down,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4390#issuecomment-364990476,1,['down'],['down']
Availability,"Ok, thanks for validating that. I'll make a PR for this change (the way those unit tests are written is a little sketchy so I may fix that at the same time). I forgot about the spark failures - can you post the log output for those failures as well ?. Also, to answer your original question, you should generally always be able to work directly from head of master and all tests should pass, though sometimes things like this can slip through. Also, to answer your original question",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5511#issuecomment-446964064:183,failure,failures,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511#issuecomment-446964064,2,['failure'],['failures']
Availability,"Ok, this seems to solve it. I ran :; ```; $ git checkout cn_left_align ; $ ./gradlew clean; $ ./gradlew bundle; $ ./gradlew test; ...; Results: FAILURE (500060 tests, 500057 successes, 2 failures, 1 skipped). 500060 tests completed, 2 failed, 1 skipped; ```. And got all the tests to succeed (except the two spark related ones as before).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5511#issuecomment-446901562:144,FAILURE,FAILURE,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511#issuecomment-446901562,2,"['FAILURE', 'failure']","['FAILURE', 'failures']"
Availability,Ok. It turns out I was hitting https://github.com/AdoptOpenJDK/openjdk-build/issues/1211. Reverting to java 11.0.3 resolves that and now I get the same error you see.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532761887:152,error,error,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532761887,1,['error'],['error']
Availability,"Okay apparently there is not a version number in the picard.jar file downloaded from https://github.com/broadinstitute/picard/releases and thus if easybuild detects a cache copy, it will use that instead of downloading it which was an older version. They forced it to download and now version is correct. I will re-try and see if the error persists. Thanks for catching that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1419783633:69,down,downloaded,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1419783633,4,"['down', 'error']","['download', 'downloaded', 'downloading', 'error']"
Availability,Okay thanks for getting back to me @mwalker174. Okay I guess that scenario is unlikely to occur when using real data. Is there some way to force GATK to use a particular PHRED style score?. The filtering of so many microbial reads (when we're simulating base read errors) is still worrisome to me because this is using 75 bp reads and we've got a lot of (paired) reads in the 30-40 bp range. Could you suggest some approach to figure out which of the many steps in the HostFilter step are filtering out the reads?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6705#issuecomment-661121807:264,error,errors,264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6705#issuecomment-661121807,1,['error'],['errors']
Availability,"Okay tranche filtering and training script are in. They're pure python right now but it would be simple to wrap them in java CLP via PythonScriptExecutor. These scripts add several dependencies which will probably make the already big docker quite a bit bigger. Long term I think we can get rid of most of them as we already have for inference, but we want to have some training functionality available by AGBT which is the week after next. Ready for a first round review @cmnbroad.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4245#issuecomment-362679008:393,avail,available,393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4245#issuecomment-362679008,2,['avail'],['available']
Availability,"Okay, I've had time to sit down and go through each tool. Sorry, but I'm WFH today so I've no paper proofs to hand you. For Jan 9 release, we are aiming for:; - Meaningful one-line summaries that convey the tool functionality; - Functional categorization of tools; - Example commands that are representative and of course that work, i.e. uses updated kebab syntax. --- . ## CalcMetadataSpark . 1. Revise one-line summary to something like:; Collects read metrics relevant to structural variant discovery. - Notice the lack of a period at the end above.; - Not statistics but metrics?. 2. Overview and Notes could use finessing but let's leave this for next year. One thing to do now is move this statement up top:; This tool is used in development and should not be of interest to most researchers. 3. I think this tool fits under the DiagnosticsAndQCProgramGroup.java.; 4. The tool takes a SAM/BAM/CRAM and calculates fragment length statistics...; 5. ""This is the first step in the workflow""--> makes it sound like this tool is necessary in the SV workflow but you say otherwise in the debugging sentence. I find this confusing. 6. I'm noticing that the example command does not have spark options despite the tool being a Spark tool. For such cases, it would be helpful to state, e.g. ""This tool can run in both Spark and non-Spark modes, depending on if --sparkMaster is set."" Then include a second example command that shows how to utilize Spark. There is an example from ChrisW in <https://github.com/broadinstitute/gatk/issues/3853>:. ```; 	-- \; --sparkRunner GCS \; --cluster my-dataproc-spark-cluster; ```. ---; ## DiscoverVariantsFromContigAlignmentsSAMSpark. 1. ""Parse"" is vague. How about: ; Parses aligned contig assemblies of genomic breakpoints and calls structural variants. And `6. ` from above. ---; ## ExtractOriginalAlignmentRecordsByNameSpark. 1. Subsets reads by names; 2. I think you mean FilterSamReads (Picard) and not PrintReads. AFAIK, PrintReads cannot subset based on a l",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3948#issuecomment-351467451:27,down,down,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3948#issuecomment-351467451,2,['down'],['down']
Availability,"Okay, there's the bug fix PR. @bklein345 the PR will close this ticket but please re-open if the error persists. Here's a jar to test: gs://broad-dsde-methods-davidben/gatk-builds/realignment_patch.jar",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6344#issuecomment-576877323:97,error,error,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6344#issuecomment-576877323,1,['error'],['error']
Availability,"On OS X El Capitan 10.11.6, when creating the conda environment I get the following error:; ```; gcc -undefined dynamic_lookup -L/Users/markw/anaconda/envs/gatk/lib -L/Users/markw/anaconda/envs/gatk/lib -arch x86_64 build/temp.macosx-10.7-x86_64-3.6/pysam/libchtslib.o build/temp.macosx-10.7-x86_64-3.6/pysam/htslib_util.o build/temp.macosx-10.7-x86_64-3.6/htslib/kfunc.o build/temp.macosx-10.7-x86_64-3.6/htslib/knetfile.o build/temp.macosx-10.7-x86_64-3.6/htslib/kstring.o build/temp.macosx-10.7-x86_64-3.6/htslib/bcf_sr_sort.o build/temp.macosx-10.7-x86_64-3.6/htslib/bgzf.o build/temp.macosx-10.7-x86_64-3.6/htslib/errmod.o build/temp.macosx-10.7-x86_64-3.6/htslib/faidx.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_net.o build/temp.macosx-10.7-x86_64-3.6/htslib/hts.o build/temp.macosx-10.7-x86_64-3.6/htslib/hts_os.o build/temp.macosx-10.7-x86_64-3.6/htslib/md5.o build/temp.macosx-10.7-x86_64-3.6/htslib/multipart.o build/temp.macosx-10.7-x86_64-3.6/htslib/probaln.o build/temp.macosx-10.7-x86_64-3.6/htslib/realn.o build/temp.macosx-10.7-x86_64-3.6/htslib/regidx.o build/temp.macosx-10.7-x86_64-3.6/htslib/sam.o build/temp.macosx-10.7-x86_64-3.6/htslib/synced_bcf_reader.o build/temp.macosx-10.7-x86_64-3.6/htslib/vcf_sweep.o build/temp.macosx-10.7-x86_64-3.6/htslib/tbx.o build/temp.macosx-10.7-x86_64-3.6/htslib/textutils.o build/temp.macosx-10.7-x86_64-3.6/htslib/thread_pool.o build/temp.macosx-10.7-x86_64-3.6/htslib/vcf.o build/temp.macosx-10.7-x86_64-3.6/htslib/vcfutils.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_codecs.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_decode.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_encode.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_external.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_index.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_io.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_samtools.o build/temp.macosx-10.7-x86_64-3.6/htslib/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4742:84,error,error,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742,1,['error'],['error']
Availability,"On a whim I took the latest code from master and commented out the two lines in HaplotypeCallerEngine:257-258 that disable phsyical phasing if `emitReferenceConfidence()` is false, and tried running HC to generate a genotyped VCF with phase. At least on a simple test of a ~200bp locus with a pair of phased variants it appears to do the right thing and not cause any errors. I know testing calling in one small locus isn't exactly comprehensive, and I'm trying now to call a larger set of regions and compare the calls generated to expected phase. Does anyone recall why this restriction was in place? I'm hoping that perhaps it was needed at the time, but isn't now and was just left in place because nobody needed it removed? I see the lines in question were last touched by @droazen in April 2016, but even that commit seems to be a large scale moving around of code rather than a commit that addressed this specific issue. I'm going to open a PR to remove those lines - mostly so I can have the tests run up in CI, and see if anything breaks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470618640:368,error,errors,368,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470618640,2,['error'],['errors']
Availability,"On my Mac, and on my Linux desktop machine at home, but not on Travis, I consistently fail to run unit tests. The precise location of the failure varies somewhat, but it's always in this vicinity:. ```Test: Test method testWritingToFileURL[0](~me/IdeaProjects/gatk/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf)(org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest) produced standard out/err: 14:49 DEBUG: [kryo] Write object reference 809: INFO```. The failing test makes my poor little machine's fan run like mad for a while, and then everything gets very quiet, but the test never returns.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2490:138,failure,failure,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490,1,['failure'],['failure']
Availability,"Once https://github.com/broadinstitute/gatk/pull/3620/ is in, we should be able to remove the download of picard.jar from .travis.yml, and change the M2 WDL to no longer depend having access to it. Workflow calls to picard tools can be replaced with calls to the same tools in GATK, although the argument syntax will have to change from picard style to Barclay style (""I=..."" to ""-I ..."").",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3625:94,down,download,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3625,1,['down'],['download']
Availability,"Once we choose the library to use for GATK configuration, let's have a design meeting to make sure we come up with something that works for Spark, downstream projects, our users, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3079:147,down,downstream,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079,1,['down'],['downstream']
Availability,"Once we have built junction trees for linked de Bruijn graphs we can use them to find phased haplotypes that handle repeats / cycles. This essentially amounts to running the current Dijkstra's algorithm on junction trees (which are, after all, DAGs) instead of `SeqGraph`s. That is, the edge weights can remain log branching ratios. The complication is that while using the the ""oldest"" junction tree for edge weights we must also traverse younger junction trees in order to rely on them downstream. We may or may not want to work out a rule for when to use a younger junction tree with much higher coverage than an older one, or to combine multiple trees into a single vote.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5925:488,down,downstream,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5925,1,['down'],['downstream']
Availability,"One more note: I did try to make the score/apply single pass when using the Java BGMM backend (since we can just cheaply score as we go, in contrast to using a file interface with python), but this seemed to slow down variant writing a bit more than expected and it came out as a wash with the two-pass approach. Didn’t seem worth the extra code at this point, but maybe we can get it working better in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1044868333:213,down,down,213,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1044868333,1,['down'],['down']
Availability,"One more question: I have everything passing locally. When tests run on travis, they fail with odd errors suggesting the VCF idx files are wrong. These were all created by IndexFactory.createDynamicIndex(), and work file for me. Have you ever seen a situation like that where indexes created on one machine doesnt transport? My machine is a windows laptop, if that's relevant.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431280748:99,error,errors,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431280748,1,['error'],['errors']
Availability,"One more step towards using this new tool. Does:; * output a single VCF containing `<INS>`, `<DEL>`, `<DUP>`, `<INV>` calls (there will be more `<INV>` calls, but that cannot happen until someone takes a look at PR #4789 and check if the proposed algorithm makes sense); * since this new tool applies more permissive filters on MQ and alignment length of the assembly contigs' mappings, I've introduced some downstream filtering parameters allowing to filter VCF records based on annotations `MAPPING_QUALITIES` and `MAX_ALIGN_LENGTH`; the default value is chosen after some experimentation using the CHM PacBio as truth and the branch ; sh-sv-interlvatree-eval.; * cleans up VCF headers and related tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4996:408,down,downstream,408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4996,1,['down'],['downstream']
Availability,"One more thing. If you rebase on master, you'll find the correct program group available to you. This tool should be in `Read Data Manipulation`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4019#issuecomment-355867489:79,avail,available,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4019#issuecomment-355867489,1,['avail'],['available']
Availability,"One observation that illustrates the need for care when optimizing metrics: for a few of the F1 optimizations, the haplotype-to-reference match-value parameter gets driven to its minimal value (1). Not 100% sure, but I'm guessing this might effectively boost precision by somehow cutting down on the complexity of proposed haplotypes---it depends on what the exact behavior of our SW algorithm is for negative scores. @davidbenjamin any thoughts on this behavior?. Something I don't quite understand yet is if we can impose some effective constraints on the parameters or otherwise reduce the number of independent dimensions. For example, it seems reasonable to me to fix the gap-extend penalties to -1 and let all other parameters be defined w.r.t. them. But perhaps we can also fix the match values similarly?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712268193:288,down,down,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712268193,1,['down'],['down']
Availability,One of the issues:. [Utils] [ERROR] [Error] java.lang.IllegalArgumentException: Invalid interval. Contig:1 start:350001 end:300000; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.&lt;init&gt;(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.tools.copynumber.utils.annotatedinterval.AnnotatedIntervalUtilsUnitTest.provideMergeByAnnotation(AnnotatedIntervalUtilsUnitTest.java:215); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:55); 	at org.testng.internal.MethodInvocationHelper.invokeMethodNoCheckedException(MethodInvocationHelper.java:45); 	at org.testng.internal.MethodInvocationHelper.invokeDataProvider(MethodInvocationHelper.java:115); 	at org.testng.internal.Parameters.handleParameters(Parameters.java:509); 	at org.testng.internal.Invoker.handleParameters(Invoker.java:1308); 	at org.testng.internal.Invoker.createParameters(Invoker.java:1036); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1126); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-431874410:29,ERROR,ERROR,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-431874410,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"One thing to address is that just use the --use-jdk-deflater/inflater options in the step where the error information was given, in my case, HaplotypeCaller, was not help. (In [Ref1], the author tried using --use-jdk-deflater/inflater options in GenomicsDBimport. Although the author said ""the error was solved"", actually it was not solved, because he/she mentioned that it was still ""end of the error log"")",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582#issuecomment-991513869:100,error,error,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582#issuecomment-991513869,3,['error'],['error']
Availability,"One thing, v4.1.8.0 is backward compatible with all the workspaces generated by versions <4.1.8.0, not the other way around. For example, v4.1.7.0 cannot read a GenomicsDB workspace created by v4.1.8.0, but 4.1.8.0 can read a 4.1.7.0 workspace. So some questions -; 1. Are the failures in the same set of nodes?; 2. Have all the ""nodes"" in the cluster been updated to running gatk v4.1.8.0.; 3. Is it possible to attach a file named `__array_schema.tdb` from one of the arrays causing the segfault?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-716832351:277,failure,failures,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-716832351,1,['failure'],['failures']
Availability,"One variable that we need to control for is OpenJDK vs. Oracle JDK. Apparently these errors happened with OpenJDK, which is known to be flakier in the networking department than Oracle JDK. We should test with Oracle's JDK and see if the errors persist.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300313874:85,error,errors,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300313874,2,['error'],['errors']
Availability,"Ongoing conversation from <https://github.com/broadinstitute/gatk-protected/pull/1130> can now continue in this merged repo. So far, @vdauwera @droazen and @samuelklee have agreed to delete the code that was requested to be archived in favor of using git versioning as the archive method with these stipulations from Geraldine:. - the PR and commit message specify whether there is a replacement for each of the tools. ; - should be a deprecation message so that if I try to run one of these tools in a newer version, I get a helpful error message that tells me the tool was removed and by what it was replaced if applicable. See GATK3 for how we implemented this previously. This should be done for all tools that we remove, regardless of whether they were purely internal or experimental. It's only a one line addition per tool and it can potentially save us a lot of headaches later down the road (even just internally). Currently, this PR is the original PR where I placed the to-be-archived code in an archive folder. . ### I have yet to make additional changes so as to follow the above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2809:534,error,error,534,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2809,2,"['down', 'error']","['down', 'error']"
Availability,"Only 2bit references load the reference data into memory and can be effectively broadcast -- need to add a check that we have a 2bit reference when using BROADCAST in `BaseRecalibratorSpark`, and throw an error with instructions on how to create one when we don't.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1130:205,error,error,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1130,1,['error'],['error']
Availability,"Only cuts the runtime down by 5%, but it produces the same vcf.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2593#issuecomment-293095839:22,down,down,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2593#issuecomment-293095839,1,['down'],['down']
Availability,"Only performance optimizations were made to the copy-ratio denoising method in the ModelSegments pipeline, which is otherwise identical to that used in GATK CNV. No special care is taken to preserve the normalization of the overall copy-ratio profile during the process. This may become important in downstream tumor-heterogeneity inference; estimates of the ploidy may be otherwise biased. We can investigate using simulated data. This issue could be obviated by #4121 in the near future, but a quick fix might nevertheless be in order.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4150:300,down,downstream,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4150,1,['down'],['downstream']
Availability,Only the first letter of unknown arguments is shown in the error.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1751:59,error,error,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1751,1,['error'],['error']
Availability,Oops -- good point. QUALapprox is not default. I've updated the docs with the appropriate argument to add. Still good @gbrandt6 ?. I will investigate the test failures. I'm 99% sure it's not related to my docs change....,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7231#issuecomment-828646984:159,failure,failures,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7231#issuecomment-828646984,1,['failure'],['failures']
Availability,"Oops, forgot to set some random seeds and got failures on Travis that were passing locally. Think it should be OK now. Good looking out Travis RNGs, you da real MVPs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6624#issuecomment-706253687:46,failure,failures,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6624#issuecomment-706253687,1,['failure'],['failures']
Availability,"Oops, that's my fault.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3601#issuecomment-331469899:16,fault,fault,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3601#issuecomment-331469899,1,['fault'],['fault']
Availability,Opening a new ticket to fix the new error messages #2622,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-297140282:36,error,error,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-297140282,1,['error'],['error']
Availability,"Ops reported several instances in which the allele-specific filtering failed. In the case I examined, the MQ distribution is much tighter around the mode at 60, which causes lin alg failures because that variable is effectively constant. Added more jitter, which has served well in the past.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6262:182,failure,failures,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6262,1,['failure'],['failures']
Availability,"Optimized(NioEventLoop.java:580); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-02-17 16:25:50 INFO MemoryStore:54 - MemoryStore cleared; 2019-02-17 16:25:50 INFO BlockManager:54 - BlockManager stopped; 2019-02-17 16:25:50 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-02-17 16:25:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-02-17 16:25:50 INFO SparkContext:54 - Successfully stopped SparkContext; 16:25:50.893 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 17, 2019 4:25:50 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 5.28 minutes.; Runtime.totalMemory()=5059379200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QName",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:46886,down,down,46886,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['down'],['down']
Availability,"Option 2, for sure. On Tue, May 8, 2018, 02:38 Jonn Smith <notifications@github.com> wrote:. > This is an issue with the Gencode files chosen for the datasources. The; > Fasta file we use currently is a subset of the total genes in Gencode, so; > this is an expected error (though it shouldn't be a user error).; >; > We have 2 options:; >; > 1. changing the code to throw a warning and ignore variants in; > transcripts not in the Fasta.; > 2. Download a larger set of sequences from Gencode and make sure all; > transcripts are represented in the Fasta file.; >; > My choice is 2 because it is a relatively seamless update (though the data; > sources will need to be updated, and this will add about 1.6 GB to the data; > sources).; >; > @LeeTL1220 <https://github.com/LeeTL1220> Thoughts?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_4_-aePFLbkcvBhmHeQhZRgWY0Sks5twT1igaJpZM4T1InN>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387377523:267,error,error,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387377523,3,"['Down', 'error']","['Download', 'error']"
Availability,Option to recover all dangling branches -- default in M2 mito mode,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5693:10,recover,recover,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5693,1,['recover'],['recover']
Availability,Originally by @vruano . Currently the dangling head and tail recovery algorithm only handle simple paths without furcations from the dangling source/sink vertex and the reference path. . However some variation that fail in complex dangling subgraphs can be lost. For example. https://www.pivotaltracker.com/story/show/80381400 ; So this story is about implementing an improved algorithm to handle these cases.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/266:61,recover,recovery,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/266,1,['recover'],['recovery']
Availability,"Originally by @vruano in Classic GATK Pivotal. Improve the read threading process in order to minimize loss of information without affecting the accuracy of calls. . Here I list a few details and ideas to take in consideration:. A. Currently (unless recover of dangling heads is active) we start threading at the first unique kmer of the read (sequence). There are at least two unsound aspect to this approach:. A.1 Since we are generating those vertices as we thread the resulting graph and edge weights may be different depending of the sequence (read) threading order. . A.2 We are throwing away information located at the beginning of the read before the first unique (and existing) k-mer in each sequence is found. This is partly fixed by the approach taken when we recover dangling heads yet it seems to have other problems downstream when selecting or pruning haplotypes:. ```; https://www.pivotaltracker.com/story/show/67601310; ```. B. Low support chain pruning might not be longer needed. Now we have a newer approach to select best haplotypes that can handle complex graph we might well not need to prune low supported hap early as they seemly they won't be selected if the are not amongst the best haplotypes. . B.1 Now that still would produce a considerable number of unlikely haplotypes that would cause a CPU burden. That can be changed by imposing another kinds of limit, For example we include all haplotypes with scores (likelihoods) that are Q0 - Q40 or we include haplotypes until the sum of their likelihoods is larger than the 99.99% probability mass. . B.2 This could provide a downstream solution to the problem caused by ranging heads recovery (explained above in A.2). B.3 If pruning is to be maintained, it makes more sense to do it at the very end after all dangling ends hav been recovered and the edges supports are finalized. Of course I assuming here that dangling end recovery does the sensible think of updating those supports are the graphs is modified. C. The use ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/264:250,recover,recover,250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/264,3,"['down', 'recover']","['downstream', 'recover']"
Availability,"Originally from @vruano . Depending of what ploidy we use AR may return different active region boundaries. This differences cause the haploid assembly to fail with the larger region hightlight the lack of robustness of the current approach. More concretely the problem seem to be the presence of cycle in the larger region. Files are located in . ```; /humgen/gsa-hpprojects/dev/valentin/bug-reports/non-rubsassembly-with-ploidy4. cd $THAT_DIR; sh ./run.sh; ```. in CEUTrio*ploidy4.vcf the variant 20:22064431 is missing (as some other in the same region) which is a TP in knowledge base. . If you look into the debug output ploidy2.err and ploidy4.err, the latter attempts to assemble a larger region failing due to a cycle. . AR traversal comes out with different active region boundaries because the engine used takes as a parameter the ploidy. That is not by itself a bug and a bad think is just that the assembly fails for the extended region. . The task here is to improve the assembly algorithm to cope with this situations better (perhaps handle cycles appropriately).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/267:206,robust,robustness,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/267,1,['robust'],['robustness']
Availability,"Otherwise, a possibility is to move to an static method that can be called by downstream `Main` classes, to be sure that the static initialization is done in the same way as GATK. In this case, this should also being documented.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5014#issuecomment-405100769:78,down,downstream,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5014#issuecomment-405100769,1,['down'],['downstream']
Availability,"Our R dependency is primarily for producing plots. It could be possible to create plots using javascript instead. Javascript plots have several potential advantages but also several major downsides. The biggest and most obvious drawback is that we don't have any code to produce them yet, and they are likely harder to generate and experiment with than R scripts. . The advantage would be that we could avoid requiring an R installation to run hellbender scripts, we could potentially also include interactive plotting or other neat tricks to make the plots more useful. I see 2 possible routes to replacing Rscripts with javascript. The first would be for tools that require graphs to perform some html generation and produce html reports with embedded javascript. The user could then open these in their browser and view the plots ( much like how our test suite report and jacoco is done). . A different option would be to use javascript plotting libraries directly within the jvm to generate SVG. Java 8 has a new javascript engine which is supposed to be reasonably fast and offers access to java objects from within it. Unfortunately it doesn't offer a full DOM like a browser does, so most existing javascript libraries will fall over. It seems like it would take a lot of hacking to get something like d3 to run directly on the jvm. (someone has done something of the kind here: http://jazdw.net/content/server-side-svg-generation-using-d3js) . Other options would be to use the javafx web panes to display a browser directly, or to plot directly on a canvas. Either of these options seem like they would be painful and awful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/248:188,down,downsides,188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/248,1,['down'],['downsides']
Availability,Our [spark tests on jenkins](https://gatk-jenkins.broadinstitute.org/view/Performance/) are failing with:; ```; Runtime.totalMemory()=554696704; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from gs://broad-gatk-test-jenkins/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; Caused by:null. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from gs://broad-gatk-test-jenkins/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; Caused by:null; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:189); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getParallelReads(ReadsSparkSource.java:93); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getUnfilteredReads(GATKSparkTool.java:238); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.Na,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2449:225,ERROR,ERROR,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449,1,['ERROR'],['ERROR']
Availability,Our current logging level argument (VERBOSITY) is only hooked up to the legacy Picard logger. We need to hook this up to log4j as well. Related to:; https://github.com/broadinstitute/hellbender/issues/146 (standardize on log4j across GATK + Picard); https://github.com/broadinstitute/hellbender/issues/216 (fix log4j error that happens on every run),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/243:317,error,error,317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/243,1,['error'],['error']
Availability,"Our default downsampling settings in HaplotypeCaller / Mutect2 (cap the maximum number of reads that can start at the same position) is uniquely unsuited to amplicon data. We should detect amplicon data on startup, and warn the user to adjust the downsampling settings (as discussed with @davidbenjamin). @ldgauthier Thoughts on this idea?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7567:12,down,downsampling,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7567,2,['down'],['downsampling']
Availability,"Our goal is to have the combination step for allele-specific annotations handled by TileDB, but we should still port this code to GATK4 for the following reasons:. -We can likely simplify the code greatly, reducing it down to the three cases of List concatenation, sum, and contingency table combination, making it easier for Intel to replicate in TileDB. -It will be good to have a non-TileDB way to combine gvcfs as a model implementation and fallback option.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1893:218,down,down,218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1893,1,['down'],['down']
Availability,"Our jenkins nightly tests are failing, but they're reporting success. This shouldn't be happening. . Some of the failures are due to #3067, but the spark failures look like something else is causing them. Notice the very short runtimes because nothing is actually happening.; <img width=""958"" alt=""screen shot 2017-06-09 at 2 04 31 pm"" src=""https://user-images.githubusercontent.com/4700332/26988271-a61ab3fc-4d1c-11e7-9110-9941888b66ce.png"">. This ticket is to fix the fact that the tests report success even when they fail, not to fix the tests themselves.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3077:113,failure,failures,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3077,2,['failure'],['failures']
Availability,Our jenkins worker nodes are dead with out of disk space errors. We need to either clean stuff off of them or give them more space. . We should determine what's eating their disk space as well so we can prevent this in the future. We may need to do some clean up after some of our tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2180:57,error,errors,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2180,1,['error'],['errors']
Availability,"Our patches to `google-cloud-java` in https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2281 and https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2283 to fix the transient NIO errors have now been merged into master, and will be part of their next release (which will be the release after `0.22.0`). We should update to the next release as soon as it's out, to remove our existing dependency on a SNAPSHOT build of `google-cloud-java`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3500:201,error,errors,201,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3500,1,['error'],['errors']
Availability,Our travis builds are getting killed intermittently with out-of-memory errors -- it's unclear whether it's the test suite JVM or the JVM with gradle that is getting killed. This is happening more and more often...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1113:71,error,errors,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1113,1,['error'],['errors']
Availability,"Out of 11 runs on exactly the same data, FilterByOrientationBias fails 6 times and succeeds 5 times. Assigning @LeeTL1220, given prior interaction with user. - User reports this error in: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40412#Comment_40412; - My recapitulation is in: https://github.com/broadinstitute/dsde-docs/issues/2294. Data is at `/humgen/gsa-scr1/pub/incoming/byoo_FilterByOrientationBias.zip`. Command is:; ```; gatk-launch FilterByOrientationBias \; -A 'G/T' -A 'C/T' \; -V test2.vcf \; -P test2.pre_adapter_detail_metrics \; --output ob_filtered2.vcf; ```. Error message changes between:; ```; java.lang.IllegalStateException: Allele in genotype C* not in the variant context [G*, T]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.tools.exome.orientationbiasvariantfilter.OrientationBiasFilterer.annotateVariantContextsWithFilterResults(OrientationBiasFilterer.java:216); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:168); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3291:178,error,error,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3291,2,"['Error', 'error']","['Error', 'error']"
Availability,Output a more infromative out of memory error for GermlineCNVCaller,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6362:40,error,error,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6362,1,['error'],['error']
Availability,Override mechanisms (in order of priority). -Individual config options specified on the command line manually / Or explicit config file ; -Override config file packaged into a downstream project; -Default GATK config file,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-307467520:176,down,downstream,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-307467520,1,['down'],['downstream']
Availability,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7498:1200,down,downstream,1200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498,1,['down'],['downstream']
Availability,PCR Error read indel quaility correction issues.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2915:4,Error,Error,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2915,1,['Error'],['Error']
Availability,PGEN max alt alleles sharp edge fix EchoCallset edition [VS-1279],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8815:36,Echo,EchoCallset,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8815,1,['Echo'],['EchoCallset']
Availability,"PGEN not compressed, Echo edition [VS-1412]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8874:21,Echo,Echo,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8874,1,['Echo'],['Echo']
Availability,"PM CST; > 21:14:29.495 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:14:29.495 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Version: 2.21.2; > 21:14:29.496 INFO GenotypeGVCFs - Picard Version: 2.21.9; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:14:29.496 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:14:29.496 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:14:29.496 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:14:29.496 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:14:29.496 INFO GenotypeGVCFs - Requester pays: disabled; > 21:14:29.496 INFO GenotypeGVCFs - Initializing engine; > **[TileDB::StorageManager] Error: Cannot lock consolidation filelock; Cannot lock.**; > 21:14:30.336 INFO GenotypeGVCFs - Shutting down engine; > [May 25, 2020 9:14:30 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.02 minutes.; > Runtime.totalMemory()=1199570944; > ***********************************************************************; > ; > A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader; > ; > ***********************************************************************; > Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace. and last GATK commands used:. ```; gatk GenomicsDBImport \; -V SRR630496.erc.g.vcf \; -V SRR630877.erc.g.vcf \; --genomicsdb-workspace-path mydatabase \; --intervals chr22; ```. > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6627:2829,Error,Error,2829,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6627,1,['Error'],['Error']
Availability,"PPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../../public/repo) -> [Help 1]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4686:2268,FAILURE,FAILURE,2268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686,2,"['ERROR', 'FAILURE']","['ERROR', 'FAILURE']"
Availability,PRs like https://github.com/broadinstitute/gatk/pull/2156 make it clear that we need some master configuration mechanism in the GATK that can be overridden by clients/downstream projects. . One promising option is `commons-configuration` (https://commons.apache.org/proper/commons-configuration/userguide/user_guide.html) using properties files -- we should look into this to see whether it does what we want.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2297:167,down,downstream,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2297,1,['down'],['downstream']
Availability,PSUtils getMatchesLessDeletions() should not throw error if number of…,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3639:51,error,error,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3639,1,['error'],['error']
Availability,"P_AF=1.000e-03;P_GERMLINE=-2.169e-04;TLOD=14.94	GT:AD:AF:ALT_F1R2:ALT_F2R1:FOXOG:MBQ:MCL:MFRL:MMQ:MPOS:REF_F1R2:REF_F2R1:SA_MAP_AF:SA_POST_PROB	0/1:6,5:0.455:3:2:0.400:30,33:0,0:191,278:60,60:11,20:1:5:0.404,0.444,0.455:0.025,0.025,0.950; WMCF9-CB5:working shlee$ gzcat 2_normalforpon.vcf.gz | grep 'chrX\t153909841'; chrX	153909841	.	C	A	.	.	DP=11;ECNT=1;POP_AF=1.000e-03;P_GERMLINE=-2.169e-04;TLOD=14.94	GT:AD:AF:ALT_F1R2:ALT_F2R1:FOXOG:MBQ:MCL:MFRL:MMQ:MPOS:REF_F1R2:REF_F2R1:SA_MAP_AF:SA_POST_PROB	0/1:6,5:0.455:3:2:0.400:30,33:0,0:191,278:60,60:11,20:1:5:0.404,0.444,0.455:0.025,0.025,0.950; WMCF9-CB5:working shlee$ gzcat 3_discard_practice_pon.vcf.gz | grep 'chrX'; ##contig=<ID=chrX,length=156040895>; ##contig=<ID=chrX_KI270880v1_alt,length=284869>; ##contig=<ID=chrX_KI270881v1_alt,length=144206>; ##contig=<ID=chrX_KI270913v1_alt,length=274009>; chrX	132097402	.	TACAC	T,TAC	.	.	.; ```; This site should have been called in the PoN. Finally, for the `-vcfs` parameter, if I provide a list of files, one per line, the tool errors with; ```; htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file, for input source: /Users/shlee/Desktop/August2017_tutorial_dev/working/list_of_normals_for_pon.txt; 	at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:253); 	at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:101); 	at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:126); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:110); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:74); 	at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:58); 	at org.broadinstitute.hellbender.tools.walkers.mutect.CreateSomaticPanelOfNormals.doWork(CreateSom",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3510:2623,error,errors,2623,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3510,1,['error'],['errors']
Availability,"P_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf -> file:///datastore/nextgenout5/share/labs/bioinformatics/alanh/test/nf-germline-exome_sim4/work/3f/5c862c695472a59dfab47a87afe4f3/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 14:24:34.617 INFO DataSourceUtils - Resolved data source file path: file:///datastore/nextgenout5/share/labs/bioinformatics/alanh/test/nf-germline-exome_sim4/work/3f/5c862c695472a59dfab47a87afe4f3/acmg_lof.tsv -> file:///datastore/nextgenout5/share/labs/bioinformatics/alanh/test/nf-germline-exome_sim4/work/3f/5c862c695472a59dfab47a87afe4f3/funcotator_dataSources.v1.7.20200521g/acmg_lof/hg38/acmg_lof.tsv; 14:24:35.311 INFO Funcotator - Shutting down engine; [October 29, 2020 2:24:35 PM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2055733248; code: 400; message: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Bad Request""; }; reason: null; location: null; retryable: false; com.google.cloud.storage.StorageException: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Bad Request""; }; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:439); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:244); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:241); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at shaded.cloud_nio.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at shaded.cloud_nio.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:240); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:736); 	at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926:7469,error,error,7469,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926,1,['error'],['error']
Availability,"Package precompiled HDF5 as hdf5-java-bindings, and pull it down via maven",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1990:60,down,down,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1990,1,['down'],['down']
Availability,PackagesNotFoundError: The following packages are not available from current channels:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8838:54,avail,available,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8838,1,['avail'],['available']
Availability,Part 1 of HC memory fixes: add downsampling to AssemblyRegion traversal,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1972:31,down,downsampling,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1972,1,['down'],['downsampling']
Availability,Partial dupe of #6035 ; Downsampling in HaplotypeCaller is exceedingly rare.; James's work might enable expanding the active region size.; @davidbenjamin what has to happen to turn on #5831 for HaplotypeCaller?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6151#issuecomment-580917501:24,Down,Downsampling,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6151#issuecomment-580917501,1,['Down'],['Downsampling']
Availability,Passing VAT from VDS run on Quickstart here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/533017b9-2dfb-42ec-83ef-42dfda67f5c1; And a 'passing' (two expected failures) ValidateVat run here:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/e0af0c06-c724-4ae2-b821-6a04b558b21e,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8144#issuecomment-1371268610:198,failure,failures,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8144#issuecomment-1371268610,1,['failure'],['failures']
Availability,"Passing a .ped file to the VariantAnnotator tool does not work:. > gatk4 VariantAnnotator **-ped** my.ped --annotation PossibleDeNovo --variant my.vcf --output out.vcf. Result: A USER ERROR has occurred: **p** is not a recognized option. > gatk4 VariantAnnotator **--ped** my.ped --annotation PossibleDeNovo --variant my.vcf --output out.vcf. Result: A USER ERROR has occurred: **ped** is not a recognized option. Above, ""gatk4"" is an alias of ""/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/bin/java -jar ~/lib/gatk/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4604:184,ERROR,ERROR,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4604,2,['ERROR'],['ERROR']
Availability,"Passing run of GvsCreateVATFromVDS [here](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20WGS%2010K%20Callset/job_history/7bb1410b-123c-4150-8a6b-f3d36234527a); Passing run of GvsCallsetStatistics [here](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20WGS%2010K%20Callset/job_history/ecc67d79-ecbf-41c6-bbee-52be83327d64); (both are on the AoU 10K - so need your PMI-OPS account to see). Integration test ran [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/b1f35d64-4406-4fef-a3af-e86703f36148) - had one failure, but it was in the cost tracking, so probably not a concern for this PR",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8519#issuecomment-1721658082:572,failure,failure,572,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8519#issuecomment-1721658082,1,['failure'],['failure']
Availability,Passing workflow here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/38d22351-33cd-4c2c-abf9-feccda71d40a. Mostly passing integration test here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/4a7e6628-6c19-442d-90b8-202da267d8bb; (the failure was a bq time out.),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8374:306,failure,failure,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8374,1,['failure'],['failure']
Availability,PathSeq Illumina adapter trimming and simple repeat masking,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3354:52,mask,masking,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3354,2,['mask'],['masking']
Availability,"PathSeq is failing on the input files I'm using and its difficult for me to interpret the error message. What I'm seeing is,. 1. ERROR LiveListenerBus: SparkListenerBus has already stopped!; 2. Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times; 3. WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout; 4. WARN ShutdownHookManager: ShutdownHook 'ClientFinalizer' timeout; 5. ERROR ShutdownHookManager: ShutdownHookManger shutdown forcefully.; 6. /var/spool/slurmd/job1619084/slurm_script: line 126: syntax error: unexpected end of file. In that order. I'm running this script in parallel on a SLURM scheduler (four cpus with 8Gb mem/cpu). Here is a sample of the last few lines of STDERR, but I'm also attaching the full error output.; [pathseq_TCGA.slurm.1619078_1.err.txt](https://github.com/broadinstitute/gatk/files/1965063/pathseq_TCGA.slurm.1619078_1.err.txt). Thanks so much for any help you can provide!. `; 18/05/01 14:20:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 10.12.137.46, 39719, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:90,error,error,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,6,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,PathSeqBuildKmers - Shutting down engine,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8204:29,down,down,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8204,1,['down'],['down']
Availability,PathSeqPipelineSpark aborted due to stage failure,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:42,failure,failure,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['failure'],['failure']
Availability,PathSeqPipelineSpark error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8339:21,error,error,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339,1,['error'],['error']
Availability,PathSeqPipelineSpark: ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6293:22,ERROR,ERROR,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6293,1,['ERROR'],['ERROR']
Availability,PathseqPipelineSpark stops with error message regarding com.esotericsoftware.kryo.KryoException: Buffer underflow.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:32,error,error,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['error'],['error']
Availability,"Per discussions with @fleharty, we are looking to significantly revamp the automated somatic CNV evaluations in preparation for benchmarking the TH prototype. The existing evaluations use a few unsupported/experimental tools and idiosyncratic/redundant classes (e.g., the `src/main/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedinterval` class this issue concerns), the functionality of which we can hopefully move to python-based validation code. . The aforementioned code was purposefully decoupled from supported CNV code, but since then it has been incorporated into `Funcotator` tools and `ValidateBasicSomaticShortMutations`, at least. @jonn-smith @davidbenjamin can we discuss a plan for cleaning this code up? Would it be easy to use an existing TSV/XSV class to handle the functionality needed for these tools?. @jonn-smith perhaps we should also discuss the plan for future `FuncotateSegments` development/integration with @fleharty.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526226506:243,redundant,redundant,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526226506,1,['redundant'],['redundant']
Availability,"Per team discussion: . - Taking the new VQSR package from master is a good thing, so yay to that. 🙂 ; - `.dockstore.yml` is used for making the GVS WDLs available in Terra from Dockstore, so this is something that can / should live in a GVS-specific repo and doesn't need to be in GATK.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8256#issuecomment-1480069136:153,avail,available,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8256#issuecomment-1480069136,1,['avail'],['available']
Availability,Perform downsampling in AssemblyRegionWalkerSpark's strict mode,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5508:8,down,downsampling,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5508,1,['down'],['downsampling']
Availability,"Picard Version: 3.0.0; 03:15:18.983 INFO PostprocessGermlineCNVCalls - Built for Spark Version: 3.3.1; 03:15:18.984 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 03:15:18.985 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 03:15:18.985 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 03:15:18.986 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 03:15:18.987 INFO PostprocessGermlineCNVCalls - Deflater: IntelDeflater; 03:15:18.988 INFO PostprocessGermlineCNVCalls - Inflater: IntelInflater; 03:15:18.988 INFO PostprocessGermlineCNVCalls - GCS max retries/reopens: 20; 03:15:18.989 INFO PostprocessGermlineCNVCalls - Requester pays: disabled; 03:15:18.990 INFO PostprocessGermlineCNVCalls - Initializing engine; 03:15:43.480 INFO PostprocessGermlineCNVCalls - Done initializing engine; 03:15:47.833 INFO PostprocessGermlineCNVCalls - Shutting down engine; [April 15, 2024, 3:15:47 AM CST] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.49 minutes.; Runtime.totalMemory()=1207959552; java.lang.IllegalArgumentException: Records were not strictly sorted in dictionary order.; 	at org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberArgumentValidationUtils.validateIntervals(CopyNumberArgumentValidationUtils.java:74); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection.getShardedCollectionSortOrder(AbstractLocatableCollection.java:142); 	at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.onTraversalStart(PostprocessGermlineCNVCalls.java:388); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1096); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8776:20956,down,down,20956,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776,1,['down'],['down']
Availability,Picard added a test that runs all the data providers and makes sure they don't error out and cause skipped tests. Maybe we should add a similar test. see https://github.com/broadinstitute/picard/pull/931,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3619:79,error,error,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3619,1,['error'],['error']
Availability,Picard tools don't perform validation of the sequence dictionary which will occasionally lead to errors. They should implement the same checking as the rest of our tools,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1272:97,error,errors,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1272,1,['error'],['errors']
Availability,"Picard tries to adhere to the following (but it's up to each tool to figure; out):. 1. Check input and output files for readability/writability as soon as; possible.; 2. Delete incomplete outputs in case of caught exception.; 3. Return non-zero value in case of error. On Sun, Mar 3, 2019 at 4:45 PM samuelklee <notifications@github.com> wrote:. > Just finished switching over all of the CNV tools to fail early if; > directories are not writeable---or do not exist and cannot be; > created---only to realize that this behavior is inconsistent with that of; > Picard IntervalListTools (which is used in the gCNV pipeline).; >; > That tool fails early if the output directory is not writeable or does not; > exist, and although there is a code path later that suggests that output; > directories should be created, it is not reached due to this early fail. It; > might be that this inconsistency was introduced in; > broadinstitute/picard#1208; > <https://github.com/broadinstitute/picard/pull/1208> and I did not catch; > it in my PR. @yfarjoun <https://github.com/yfarjoun> any opinions what; > the intended behavior should be? Are there any conventions for Picard tools; > in general?; >; > Perhaps we could enforce this at the engine level (maybe checks that are; > triggered by annotations such as suggested in #141; > <https://github.com/broadinstitute/gatk/issues/141>, if possible)? But; > this would only work for GATK tools and would still rely on the diligence; > of developers.; >; > In any case, I'll decide on and document a convention for the CNV tools,; > but I think it might be a quixotic dream to enforce consistent; > behavior---especially without breaking things downstream which may rely on; > existing, inconsistent behavior...; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4825#issuecomment-469067676>,; > or mute the thread; > <https://github.com/notifications",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-470113262:262,error,error,262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-470113262,1,['error'],['error']
Availability,Pileup-based error correction achieves this goal.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3020#issuecomment-592088629:13,error,error,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3020#issuecomment-592088629,1,['error'],['error']
Availability,Pileup-based read error corrector,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6470:18,error,error,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6470,1,['error'],['error']
Availability,Ping @jamesemery,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5723#issuecomment-519520053:0,Ping,Ping,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5723#issuecomment-519520053,1,['Ping'],['Ping']
Availability,Ping @lbergelson.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2751#issuecomment-314122086:0,Ping,Ping,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2751#issuecomment-314122086,1,['Ping'],['Ping']
Availability,Ping @ldgauthier,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6220#issuecomment-550256551:0,Ping,Ping,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6220#issuecomment-550256551,1,['Ping'],['Ping']
Availability,Ping @vdauwera,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3329#issuecomment-318642013:0,Ping,Ping,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3329#issuecomment-318642013,1,['Ping'],['Ping']
Availability,Ping @vdauwera @lbergelson,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3190#issuecomment-318657514:0,Ping,Ping,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3190#issuecomment-318657514,1,['Ping'],['Ping']
Availability,"Ping? It's been a week, a review would be useful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3885#issuecomment-350111042:0,Ping,Ping,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3885#issuecomment-350111042,1,['Ping'],['Ping']
Availability,Pinging @droazen,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2547#issuecomment-291201256:0,Ping,Pinging,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2547#issuecomment-291201256,1,['Ping'],['Pinging']
Availability,Pinging @tedsharpe who gave feedback on the above thread,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8557#issuecomment-1775300469:0,Ping,Pinging,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8557#issuecomment-1775300469,1,['Ping'],['Pinging']
Availability,"Please find below the user report of this issue:. Here is the error I got:; [February 25, 2019 8:18:10 PM PST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 11.08 minutes.; Runtime.totalMemory()=5379719168; java.lang.StringIndexOutOfBoundsException: String index out of range: 545; at java.lang.String.substring(String.java:1951); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.initializeForDeletion(ProteinChangeInfo.java:192); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.<init>(ProteinChangeInfo.java:96); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.create(ProteinChangeInfo.java:371); [...]. and here is offending record:; `chr12 70747693 . TAAAAAAA T,TAAAA,TAAAAA,TAAAAAA,TAAAAAAAA . artifact_in_normal;germline_risk;multiallelic CONTQ=93;DP=537;ECNT=1;GERMQ=253,113,0,0,18;MBQ=36,24,36,28,36,33;MFRL=293,529,291,288,325,299;MMQ=60,29,60,60,60,60;MPOS=43,43,41,44,26;NALOD=0.912,0.217,-2.040e+00,-1.342e+01,-4.057e+00;NLOD=20.49,13.33,2.58,-1.476e+01,2.13;POPAF=2.27,1.08,2.19,2.53,5.40;REF_BASES=GCAAGCCTTCTAAAAAAAAAA;RPA=25,18,22,23,24,26;RU=A;SAAF=0.394,0.404,0.420;SAPP=0.019,0.015,0.965;STR;TLOD=3.88,7.29,6.62,36.73,3.85 GT:AD:AF:DP:F1R2:F2R1 0/0:21,0,1,7,14,6:0.011,0.025,0.099,0.262,0.124:49:14,0,1,6,7,4:7,0,0,1,7,2 0/1/2/3/4/5:65,3,11,20,47,18:0.016,0.057,0.076,0.245,0.092:164:32,3,6,14,30,8:33,0,5,6,17,10`. As a result the Funcotator output is truncated. Bug?. Thanks!; Ivan. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23631/funcotator-reproducibly-crushes-on-specific-wes-vcf-record-produced-by-gatk4-1-0-java-1-8-0-45/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5783:62,error,error,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5783,1,['error'],['error']
Availability,"Please find the print_array_schema executable [here](https://github.com/GenomicsDB/GenomicsDB/releases/download/v1.3.1/print_array_schema). You may have to do a `chmod +x print_array_schema` after downloading.; ```; Usage: print_array_schema <genomicsdb-workspace-path>/<genomicsdb_array>; ```; This will print out all the information for the array, it does not know anything about the fragments. Look at the `Types` printed out by the tool. ; ```; e.g.; Types:; 	END: int64[1]; 	REF: char[var]; 	ALT: char[var]; 	ID: char[var]; 	QUAL: float32[1]; ...; Coordinates: int64; ```; Every fragment should have `__book_keeping.tdb.gz` and `__tiledb_fragment.tdb`. In addition, for each fragment, there should be one file for each type plus another file if it is a var type.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722709713:103,down,download,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722709713,2,['down'],"['download', 'downloading']"
Availability,"Please find the standalone consolidation tool [here](https://github.com/GenomicsDB/GenomicsDB/releases/download/v1.3.1/consolidate_genomicsdb_array). The usage is `consolidate_genomicsdb_array <genomicsdb_workspace_path> <array_name>`, for example `./consolidate_genomicsdb_array ~/my_ws/ ""20\$1\$63025520""`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-724446496:103,down,download,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-724446496,1,['down'],['download']
Availability,"Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:24:15.728 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/bundle/dsrc/dbsnp/hg38/hg38_All_20170710.vcf.gz; 10:24:15.884 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///omics/chatchawit/bundle/dsrc/oreganno/hg38/oreganno.tsv; 10:24:15.945 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/dsrc/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; WARNING 2018-04-28 10:24:17 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 10:24:29.369 INFO ProgressMeter - Starting traversal; 10:24:29.370 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 10:24:36.192 INFO Funcotator - Shutting down engine; [April 28, 2018 10:24:36 AM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.37 minutes.; Runtime.totalMemory()=5195694080; java.lang.StringIndexOutOfBoundsException: String index out of range: -2; at java.lang.String.substring(String.java:1967); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createUtrFuncotation(GencodeFuncotationFactory.java:1088); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationOnTranscript(GencodeFuncotationFactory.java:601); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:529); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsOnVariant(GencodeFuncotationFactory.java:276); at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotatio",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363:3918,down,down,3918,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363,1,['down'],['down']
Availability,"Please need help : . I ran the script for VQSR . gatk-4.2.0.0/gatk VariantRecalibrator\; -V variants_sitesonly.vcf.gz\; 	-trust-all-polymorphic\; -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0\; -an FS -an ReadPosRankSum -an MQRankSum -an QD -an SOR -an DP\ ; -mode INDEL\; -max-gaussians 4\; -resource:mills,known=false,training=true,truth=true,prior=12:Mills_and_1000G_gold_standard.indels.hg38.vcf.gz\; -resource:axiomPoly,known=false,training=true,truth=false,prior=Axiom_Exome_Plus.genotypes.all_populations.poly.hg38.vcf.gz\; -resource:dbsnp,known=true,training=false,truth=false,prior=2:Homo_sapiens_assembly38.dbsnp138.vcf\; -O cohort_indels.recal\; --tranches-file cohort_indels.tranches. ERROR - >. A USER ERROR has occurred: Argument resource was missing: Argument 'resource' is required. Any help would be really great !. thank you; Smeeta",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-885465593:849,ERROR,ERROR,849,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-885465593,2,['ERROR'],['ERROR']
Availability,"Please see <https://github.com/broadinstitute/gatk/issues/3030> for the errors I encountered when running this tool. I have not been able to run the tool successfully and so ask what are we missing in the documentation that will help users get this tool running. Otherwise, I recommend labeling this tool experimental as well as in BETA.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3031:72,error,errors,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3031,1,['error'],['errors']
Availability,"Please, @lbergelson, take into account that this will be confusing for downstream projects before accepting this PR. Thanks in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3307#issuecomment-316440155:71,down,downstream,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3307#issuecomment-316440155,1,['down'],['downstream']
Availability,Plotting throws the wrong error when given reference fasta instead of dict,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2941:26,error,error,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2941,1,['error'],['error']
Availability,"Plugins can define their own arguments, such as VariantAnnotation classes. We have a number of cases where multiple plugins share arguments. In other words, plugins A and B both require argument X. If either A or B is used, this argument is required. They cannot have independent values for argument X. Is there any way to accommodate this?. I created an ArgumentCollection class to define that argument, and then added this @ArgumentCollection to each plugin. Something like:. public class GenotypeConcordanceBySite extends PedigreeAnnotation implements InfoFieldAnnotation { ​; ​@ArgumentCollection; ​public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. ​. .etc......; }. public class GenotypeConcordance extends PedigreeAnnotation implements InfoFieldAnnotation {; ​@ArgumentCollection; ​public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . etc......; }. public class GenotypeConcordanceArgumentCollection {; ​@Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); ​public FeatureInput<VariantContext> referenceVcf = null;; }. When I run VariantAnnotator with both plugins, I get an error from within Barclay about arguments with duplicate names. Ideally these plugins would not be aware of each other (since they can be used independently). Is there a way to define arguments that might be declared in different plugins, but are somehow resolved as identical and therefore allowed?. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7213:1238,error,error,1238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213,1,['error'],['error']
Availability,Poisson regression is not robust to outliers and leads to wrong inferences in TargetCoverageSexGenotyper,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3015:26,robust,robust,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015,1,['robust'],['robust']
Availability,"Port VQSR tests, slimming down as necessary to achieve reasonable runtime",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2063:26,down,down,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2063,1,['down'],['down']
Availability,"Port https://github.com/broadgsa/gatk-protected/pull/24; ```; Without this patch the stream is only closed (thus, flushed) when the; object is garbage collected. This is problematic when subsequent jobs; proceed and expect the output to be available, for example; AnalyzeCovariates. We see failures in approximately 50% of runs due to; this issue and they are confirmed as fixed when applying the patch (on; a busy machine using NFS storage).; ```. The tool `BaseRecalibratorSparkSharded` is affected. The fix will be in `BaseRecalibratorEngineSparkWrapper.saveTextualReport`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3161:240,avail,available,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3161,2,"['avail', 'failure']","['available', 'failures']"
Availability,"Port the remainder of tools in picard.sam, excluding ViewSam (redundant with PrintReads) and SplitSamByLibrary (see https://github.com/broadinstitute/hellbender/issues/140). . Note that new unit tests will have to be written for some of these tools (see https://github.com/broadinstitute/hellbender/issues/144).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/156:62,redundant,redundant,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/156,1,['redundant'],['redundant']
Availability,Ported from Classic-GATK: Assembly not robust to changes in active region boundaries,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/267:39,robust,robust,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/267,1,['robust'],['robust']
Availability,"PostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:5995,down,down,5995,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['down'],['down']
Availability,"Posting at the suggetion of shlee. There's discussion about what parts of VariantEval will be ported to GATK4 or whether Picard's partially overlapping tool CollectVariantCallingMetrics will take this over. I want to at least make you aware that we've developed a tool we're calling VariantQC, which is built in GATK3 and runs VariantEval internally to generate data stratified in various ways to make HTML QC reports, sorta like FASTQC or MultiQC (https://github.com/bbimber/gatk-protected/releases). An example report is here: https://prime-seq.ohsu.edu/_webdav/Internal/Bimber/Public/%40files/VariantQC_Example.html. Our goal was always to port this to GATK4, polish it up, and then make it more generally available. Much of what this tool does is take the pre-built tables/reports from VariantEval and put them into HTML, but we also wrote some custom stratifications to bin data by filter, etc. Like this thread notes, VariantEval has a lot of features not in picard, and honestly we dont use many of them. However, the extensibility of Stratifier/VariantEvaluator are pretty important for us. . We realize this is prioritized against all the GATK4 features; however, 1) how set are plans about migration of VariantEval/merge w/ picard and 2) if most of VariantEval isnt going to be ported, can we pick it up in our repo? We could also potentially offer some assistance in porting the tool because we have a vested interest; however, unless the task is defined as porting VariantEval as close as possible to as-is (not that this is critical, but it's the simplest thing for the outsider to do), it would need some discussion around exactly what's planned.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616#issuecomment-320737252:709,avail,available,709,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616#issuecomment-320737252,1,['avail'],['available']
Availability,PostprocessGermlineCNVCalls error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8776:28,error,error,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776,1,['error'],['error']
Availability,PostprocessGermlineCNVCalls errors on chunk consisting of random and unplaced contigs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4840:28,error,errors,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840,1,['error'],['errors']
Availability,PreprocessIntervals contig start position error?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7009:42,error,error,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7009,1,['error'],['error']
Availability,"Prevent a git lfs error that was caused by accidentally checking storing a .gitattributes file; in gitlfs. When checkout out the repository for the first time or moving from an old commit to a newish one, there's been an error report from git lfs. This was caused by accidentally checking a .gitattributes file into git-lfs which then is read as part of the git lfs checkout process, but since the file is tracked by lfs at the point of checkout it is an lfs stub and throws an error. The problem was introduced here: #6694. See below to reproduce:; ```; git checkout 9951f77c6; git checkout f548ccd708009ddcdfead6525edd23a68d73027b; https://git-lfs.github.com/spec/v1 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:1; sha256:cb156adb10b491dd3ba88c2b491bfb021b3c94fc956d36310c67492504fcdc58 is not a valid attribute name: src/test/resources/large/mitochondria_references/.gitattributes:2; Updating files: 100% (363/363), done.; Note: switching to 'f548ccd708009ddcdfead6525edd23a68d73027b'.; ```. This fixes the problem going forward by removing the file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7594:18,error,error,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7594,3,['error'],['error']
Availability,"PrimaryAlignmentReadFilter filters out secondary alignments, but not supplementary alignments (this is what GATK3 does as well). This doesn't match the SAM spec (though it does match the language used by htsjdk); is redundant given that in GATK4 we have a NotSecondaryAlignmentReadFilter for use with InsertSizeMetricsCollector; and is likely confusing. Proposed change is to visit the usages of the PrimaryAlignmentReadFilter (BaseRecalibrator, Pileup, CalculateTargetBaseCallCoverage and HaplotypCaller) and for any that don't want supplementary alignments filtered, replace the filter with NotSecondary, and for those that do, change Primary to also reject supplementary reads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2165:216,redundant,redundant,216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2165,1,['redundant'],['redundant']
Availability,"PrintReadsSpark gives ""htsjdk.samtools.SAMFormatException: Invalid GZIP header"" error with WGS bam and interval file",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:80,error,error,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['error'],['error']
Availability,PrintReadsSpark log4j error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:22,error,error,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['error'],['error']
Availability,"PrintReadsSpark throws error ""Invalid splitting BAM index"" on writing BAM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2219:23,error,error,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219,1,['error'],['error']
Availability,"Prior to assembly (in `AssemblyBasedCallerUtils.assembleReads`, we transform reads in several ways that are meant to be permanent (that is, we want to use them in both assembly and genotyping) within `finalizeRegion`. (Additionally, we error reads within `ReadThreadingAssembler.runLocalAssembly`, but this is done on temporary copies of reads that are used for kmers and discarded). These transformations include hard clipping low-quality ends, adaptor sequences, and, optionally, soft-clipped bases, as well as correcting the base qualities of overlapping mates. According to the git history, these transformations have been accidentally temporary for quite a while. Let's look at the relevant code. First, in `Mutect2Engine.callRegion` we have (comments added and code simplified for clarity). ```; final AssemblyRegion assemblyActiveRegion = AssemblyBasedCallerUtils.assemblyRegionWithWellMappedReads(originalAssemblyRegion . . .);. // assembleReads finalizes region, modifying reads as a side effect; final AssemblyResultSet untrimmedAssemblyResult = AssemblyBasedCallerUtils.assembleReads(assemblyActiveRegion. . .);. final SortedSet<VariantContext> allVariationEvents = untrimmedAssemblyResult.getVariationEvents(MTAC.maxMnpDistance);. // when we trim on the originalAssemblyRegion, the trimmingResult takes its un-modified reads!; final AssemblyRegionTrimmer.Result trimmingResult = trimmer.trim(originalAssemblyRegion, allVariationEvents, referenceContext);. // now the assemblyResult gets the unmodified reads of the trimmingResult!; final AssemblyResultSet assemblyResult = untrimmedAssemblyResult.trimTo(trimmingResult.getVariantRegion());; ```. If we want things like `-dont-use-soft-clipped-bases` to work, we should call `trimmer.trim` on `untrimmedAssemblyResult`. I think that change alone may be all we need. Let's look at the corresponding code in `HaplotypeCallerEngine`:. ```; final AssemblyResultSet untrimmedAssemblyResult = AssemblyBasedCallerUtils.assembleReads(region. . .);.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6686:236,error,error,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6686,1,['error'],['error']
Availability,"Probably also a bug in the error message, because that's a really unhelpful message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2503#issuecomment-288235429:27,error,error,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2503#issuecomment-288235429,1,['error'],['error']
Availability,"Probably best to have a baked-in `gatk.properties`, and an `override.properties` that is looked for in known locations (eg., a package on the classpath, as well as the current working directory). The override logic could be captured in a method that can be overridden by downstream projects with more complex needs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-275505505:271,down,downstream,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-275505505,1,['down'],['downstream']
Availability,"Probably this is to late to be of any help, but I had the exact same issue, down to the index it prints out as problematic. Maybe others will stumble upon this and find the issue here as I have. I found some pertinent info here:; https://gatk.broadinstitute.org/hc/en-us/community/posts/12862204385051-Is-it-feasible-to-use-the-extracted-vcf-gz-file-for-CombineGVCFs-and-GenotypeGVCFs. Though it seems like they never got around to a more useful stdout message. Anyway, I did as advised and split the chromosome sizes (because I'm working with barley, and the seq lengths are > 2^19). BUT- when I try indexing the bgzipped second ""halves"" of each chromosome with IndexFeatureFile, I get the same message again! When they're not bgzipped, however, it actually works.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8747#issuecomment-2349595961:76,down,down,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8747#issuecomment-2349595961,1,['down'],['down']
Availability,"Processing an exome takes ~1 minute, which means most of the time is spent on spinning up a VM, pulling docker images, etc. This is not very cost efficient. This PR allows for a `batch_size` to be set and then each task processes that many samples as a unit. The default is `1` which yields the current behavior, but in exomes I have set it to 20 and seen the cost to ingest drop dramatically. The GitHub PR makes it look like a lot has changed but really the changes are:; - a new parameter; - a new task to turn the Array[File] for the VCFs into set of FOFNs (file-of-file-names) similar to how we split up intervals; - a loop in the actual Create TSV task to loop over the files in the FOFNs. For SA mode we copy down each file, and for non-SA mode we rely on the fact that localization is optional and we read them directly anywy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7382:716,down,down,716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7382,1,['down'],['down']
Availability,"Program	Processed 57,000,000 records. Elapsed time: 00:04:45s. Time for last 1,000,000: 5s. Last read position: chr1:225,314,304; INFO	2023-02-02 13:09:54	SinglePassSamProgram	Processed 58,000,000 records. Elapsed time: 00:04:49s. Time for last 1,000,000: 4s. Last read position: chr1:228,891,181; INFO	2023-02-02 13:09:58	SinglePassSamProgram	Processed 59,000,000 records. Elapsed time: 00:04:54s. Time for last 1,000,000: 4s. Last read position: chr1:232,844,624; INFO	2023-02-02 13:10:03	SinglePassSamProgram	Processed 60,000,000 records. Elapsed time: 00:04:59s. Time for last 1,000,000: 4s. Last read position: chr1:236,642,862; INFO	2023-02-02 13:10:08	SinglePassSamProgram	Processed 61,000,000 records. Elapsed time: 00:05:03s. Time for last 1,000,000: 4s. Last read position: chr1:241,003,784; INFO	2023-02-02 13:10:13	SinglePassSamProgram	Processed 62,000,000 records. Elapsed time: 00:05:08s. Time for last 1,000,000: 5s. Last read position: chr1:245,019,923; ERROR	2023-02-02 13:10:18	Slice	Reference MD5 mismatch for slice 0:248741017-248780008, GATGGGAGAT...AGGGAAATAT; [Thu Feb 02 13:10:18 CST 2023] picard.analysis.CollectInsertSizeMetrics done. Elapsed time: 5.23 minutes.; Runtime.totalMemory()=5051514880; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; Exception in thread ""main"" htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248741017, span 38992, expected MD5 37a71701c8b7513578af280cdbcc4bda; 	at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); 	at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); 	at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:592); 	at picard.analysis.SinglePassSamProgram.makeItSo(SinglePassSamProgram.java:129); 	at picard.analysis.SinglePassSamProgram.doWork(SinglePassSamProgram.java:77); 	at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:205); 	at picard.cmdline.PicardCommandLine.instanc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1414237417:11827,ERROR,ERROR,11827,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1414237417,1,['ERROR'],['ERROR']
Availability,Promote gradle build change that helps with certain spark config errors during build.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1447:65,error,errors,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1447,1,['error'],['errors']
Availability,Proposal: throw UserException with a better error message,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7246#issuecomment-836962267:44,error,error,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7246#issuecomment-836962267,1,['error'],['error']
Availability,"Propose to reduce redundantly cracking open a path/stream to discover the correct feature codec. We do this twice for each feature input, which for multi-variant walkers with large # of inputs can be a lot. This caches the codec class in a FeatureInout the first time we find it. Ideally FeatureManager would remember it, but not all of the FeatureDataSources are created by Feature Manager (and fixing that is a bigger refactoring).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2740:18,redundant,redundantly,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2740,1,['redundant'],['redundantly']
Availability,Provide an easy mechanism for downstream toolkits to customize CommandLineProgram output,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4101:30,down,downstream,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4101,1,['down'],['downstream']
Availability,"Providing a bad value for an argument is the definition of a ""user exception"" (an error that is the user's fault). If we can't make it a `UserException` directly because of the gatk/barclay split, then we should at least wrap these exceptions in `UserException` at the top level catch, and ensure that a full error message gets printed for the user for these exceptions. . Currently I think `CommandLineException` can be silently caught and ignored with no output due to a bug in `Main.mainEntry()`:. ```; protected final void mainEntry(final String[] args) {; try {; final Object result = instanceMain(args);; handleResult(result);; System.exit(0);; } catch (final CommandLineException e){; //the usage has already been printed so don't print it here.; if(printStackTraceOnUserExceptions()) {; e.printStackTrace();; }; System.exit(COMMANDLINE_EXCEPTION_EXIT_VALUE);; } catch (final UserException e){; CommandLineProgram.printDecoratedUserExceptionMessage(System.err, e);. if(printStackTraceOnUserExceptions()) {; e.printStackTrace();; }; ```. We should fix this as part of this ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2324:82,error,error,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324,3,"['error', 'fault']","['error', 'fault']"
Availability,"Pulls down a temp table of genotype counts, calculates excess het and call rate and writes them to a tsv for future upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6812:6,down,down,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6812,1,['down'],['down']
Availability,Pushed a commit that fixes the BigQueryUtilsUnitTest failure by upgrading several of our other Google dependencies. This may cause failures in other parts of our test suite -- we shall see.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1051087548:53,failure,failure,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1051087548,2,['failure'],"['failure', 'failures']"
Availability,Pushing MLLib down to public to make it easier for people to use with GATK4. Took SVD along for the ride (to test that it works).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1846:14,down,down,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1846,1,['down'],['down']
Availability,Put in docs for the error bars. Now this PR has no sloppy loose ends.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5413#issuecomment-440136061:20,error,error,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5413#issuecomment-440136061,1,['error'],['error']
Availability,"Q tag; ERROR: Record 145082, Read name UMI-AAC-ATG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 181500, Read name UMI-ACT-CTT-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186837, Read name UMI-CAA-CTC-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186862, Read name UMI-CGC-GCC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186904, Read name UMI-AGG-GTC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186919, Read name UMI-CGC-TGC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186947, Read name UMI-TAA-TAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186970, Read name UMI-GAG-GCC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186972, Read name UMI-TAT-TTC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186985, Read name UMI-ACG-TAA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186995, Read name UMI-CTT-GCA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187006, Read name UMI-CTA-GGG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187037, Read name UMI-AGT-CTG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187061, Read name UMI-CAT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187074, Read name UMI-AAA-CGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:3226,ERROR,ERROR,3226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 181500, Read name UMI-ACT-CTT-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186837, Read name UMI-CAA-CTC-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186862, Read name UMI-CGC-GCC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186904, Read name UMI-AGG-GTC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186919, Read name UMI-CGC-TGC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186947, Read name UMI-TAA-TAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186970, Read name UMI-GAG-GCC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186972, Read name UMI-TAT-TTC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186985, Read name UMI-ACG-TAA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186995, Read name UMI-CTT-GCA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187006, Read name UMI-CTA-GGG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187037, Read name UMI-AGT-CTG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187061, Read name UMI-CAT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187074, Read name UMI-AAA-CGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 367457, Read name UMI-GGA-TTA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:3316,ERROR,ERROR,3316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 186837, Read name UMI-CAA-CTC-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186862, Read name UMI-CGC-GCC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186904, Read name UMI-AGG-GTC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186919, Read name UMI-CGC-TGC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186947, Read name UMI-TAA-TAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186970, Read name UMI-GAG-GCC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186972, Read name UMI-TAT-TTC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186985, Read name UMI-ACG-TAA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186995, Read name UMI-CTT-GCA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187006, Read name UMI-CTA-GGG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187037, Read name UMI-AGT-CTG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187061, Read name UMI-CAT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187074, Read name UMI-AAA-CGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 367457, Read name UMI-GGA-TTA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 441607, Read name UMI-AGA-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:3406,ERROR,ERROR,3406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 186862, Read name UMI-CGC-GCC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186904, Read name UMI-AGG-GTC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186919, Read name UMI-CGC-TGC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186947, Read name UMI-TAA-TAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186970, Read name UMI-GAG-GCC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186972, Read name UMI-TAT-TTC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186985, Read name UMI-ACG-TAA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186995, Read name UMI-CTT-GCA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187006, Read name UMI-CTA-GGG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187037, Read name UMI-AGT-CTG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187061, Read name UMI-CAT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187074, Read name UMI-AAA-CGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 367457, Read name UMI-GGA-TTA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 441607, Read name UMI-AGA-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481504, Read name UMI-AAC-TCT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:3496,ERROR,ERROR,3496,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 186904, Read name UMI-AGG-GTC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186919, Read name UMI-CGC-TGC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186947, Read name UMI-TAA-TAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186970, Read name UMI-GAG-GCC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186972, Read name UMI-TAT-TTC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186985, Read name UMI-ACG-TAA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186995, Read name UMI-CTT-GCA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187006, Read name UMI-CTA-GGG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187037, Read name UMI-AGT-CTG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187061, Read name UMI-CAT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187074, Read name UMI-AAA-CGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 367457, Read name UMI-GGA-TTA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 441607, Read name UMI-AGA-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481504, Read name UMI-AAC-TCT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481532, Read name UMI-AAT-CAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:3586,ERROR,ERROR,3586,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 186919, Read name UMI-CGC-TGC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186947, Read name UMI-TAA-TAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186970, Read name UMI-GAG-GCC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186972, Read name UMI-TAT-TTC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186985, Read name UMI-ACG-TAA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186995, Read name UMI-CTT-GCA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187006, Read name UMI-CTA-GGG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187037, Read name UMI-AGT-CTG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187061, Read name UMI-CAT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187074, Read name UMI-AAA-CGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 367457, Read name UMI-GGA-TTA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 441607, Read name UMI-AGA-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481504, Read name UMI-AAC-TCT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481532, Read name UMI-AAT-CAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481722, Read name UMI-ATA-ATT-10, Zero-length read without FZ, CS or CQ tag; ERROR: Recor",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:3676,ERROR,ERROR,3676,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 186947, Read name UMI-TAA-TAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186970, Read name UMI-GAG-GCC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186972, Read name UMI-TAT-TTC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186985, Read name UMI-ACG-TAA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186995, Read name UMI-CTT-GCA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187006, Read name UMI-CTA-GGG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187037, Read name UMI-AGT-CTG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187061, Read name UMI-CAT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187074, Read name UMI-AAA-CGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 367457, Read name UMI-GGA-TTA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 441607, Read name UMI-AGA-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481504, Read name UMI-AAC-TCT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481532, Read name UMI-AAT-CAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481722, Read name UMI-ATA-ATT-10, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481989, Read name UMI-CGA-CTA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Recor",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:3766,ERROR,ERROR,3766,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 186970, Read name UMI-GAG-GCC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186972, Read name UMI-TAT-TTC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186985, Read name UMI-ACG-TAA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186995, Read name UMI-CTT-GCA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187006, Read name UMI-CTA-GGG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187037, Read name UMI-AGT-CTG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187061, Read name UMI-CAT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187074, Read name UMI-AAA-CGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 367457, Read name UMI-GGA-TTA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 441607, Read name UMI-AGA-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481504, Read name UMI-AAC-TCT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481532, Read name UMI-AAT-CAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481722, Read name UMI-ATA-ATT-10, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481989, Read name UMI-CGA-CTA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482114, Read name UMI-GAG-TAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Recor",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:3856,ERROR,ERROR,3856,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 186972, Read name UMI-TAT-TTC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186985, Read name UMI-ACG-TAA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186995, Read name UMI-CTT-GCA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187006, Read name UMI-CTA-GGG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187037, Read name UMI-AGT-CTG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187061, Read name UMI-CAT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187074, Read name UMI-AAA-CGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 367457, Read name UMI-GGA-TTA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 441607, Read name UMI-AGA-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481504, Read name UMI-AAC-TCT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481532, Read name UMI-AAT-CAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481722, Read name UMI-ATA-ATT-10, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481989, Read name UMI-CGA-CTA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482114, Read name UMI-GAG-TAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482150, Read name UMI-GCC-GTA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Recor",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:3946,ERROR,ERROR,3946,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 186985, Read name UMI-ACG-TAA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186995, Read name UMI-CTT-GCA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187006, Read name UMI-CTA-GGG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187037, Read name UMI-AGT-CTG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187061, Read name UMI-CAT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187074, Read name UMI-AAA-CGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 367457, Read name UMI-GGA-TTA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 441607, Read name UMI-AGA-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481504, Read name UMI-AAC-TCT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481532, Read name UMI-AAT-CAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481722, Read name UMI-ATA-ATT-10, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481989, Read name UMI-CGA-CTA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482114, Read name UMI-GAG-TAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482150, Read name UMI-GCC-GTA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482210, Read name UMI-GGT-TCC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Recor",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:4036,ERROR,ERROR,4036,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 186995, Read name UMI-CTT-GCA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187006, Read name UMI-CTA-GGG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187037, Read name UMI-AGT-CTG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187061, Read name UMI-CAT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187074, Read name UMI-AAA-CGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 367457, Read name UMI-GGA-TTA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 441607, Read name UMI-AGA-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481504, Read name UMI-AAC-TCT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481532, Read name UMI-AAT-CAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481722, Read name UMI-ATA-ATT-10, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481989, Read name UMI-CGA-CTA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482114, Read name UMI-GAG-TAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482150, Read name UMI-GCC-GTA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482210, Read name UMI-GGT-TCC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482222, Read name UMI-GTA-GTT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Recor",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:4126,ERROR,ERROR,4126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 187006, Read name UMI-CTA-GGG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187037, Read name UMI-AGT-CTG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187061, Read name UMI-CAT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187074, Read name UMI-AAA-CGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 367457, Read name UMI-GGA-TTA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 441607, Read name UMI-AGA-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481504, Read name UMI-AAC-TCT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481532, Read name UMI-AAT-CAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481722, Read name UMI-ATA-ATT-10, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481989, Read name UMI-CGA-CTA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482114, Read name UMI-GAG-TAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482150, Read name UMI-GCC-GTA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482210, Read name UMI-GGT-TCC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482222, Read name UMI-GTA-GTT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482251, Read name UMI-GTT-TAC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Recor",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:4216,ERROR,ERROR,4216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 187037, Read name UMI-AGT-CTG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187061, Read name UMI-CAT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187074, Read name UMI-AAA-CGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 367457, Read name UMI-GGA-TTA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 441607, Read name UMI-AGA-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481504, Read name UMI-AAC-TCT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481532, Read name UMI-AAT-CAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481722, Read name UMI-ATA-ATT-10, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481989, Read name UMI-CGA-CTA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482114, Read name UMI-GAG-TAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482150, Read name UMI-GCC-GTA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482210, Read name UMI-GGT-TCC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482222, Read name UMI-GTA-GTT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482251, Read name UMI-GTT-TAC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 541693, Read name UMI-AGG-GAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Recor",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:4306,ERROR,ERROR,4306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 187061, Read name UMI-CAT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187074, Read name UMI-AAA-CGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 367457, Read name UMI-GGA-TTA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 441607, Read name UMI-AGA-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481504, Read name UMI-AAC-TCT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481532, Read name UMI-AAT-CAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481722, Read name UMI-ATA-ATT-10, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481989, Read name UMI-CGA-CTA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482114, Read name UMI-GAG-TAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482150, Read name UMI-GCC-GTA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482210, Read name UMI-GGT-TCC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482222, Read name UMI-GTA-GTT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482251, Read name UMI-GTT-TAC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 541693, Read name UMI-AGG-GAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 763643, Read name UMI-GAG-TAT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Recor",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:4396,ERROR,ERROR,4396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 187074, Read name UMI-AAA-CGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 367457, Read name UMI-GGA-TTA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 441607, Read name UMI-AGA-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481504, Read name UMI-AAC-TCT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481532, Read name UMI-AAT-CAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481722, Read name UMI-ATA-ATT-10, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481989, Read name UMI-CGA-CTA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482114, Read name UMI-GAG-TAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482150, Read name UMI-GCC-GTA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482210, Read name UMI-GGT-TCC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482222, Read name UMI-GTA-GTT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482251, Read name UMI-GTT-TAC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 541693, Read name UMI-AGG-GAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 763643, Read name UMI-GAG-TAT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 763881, Read name UMI-AGC-TTT-3, Zero-length read without FZ, CS or CQ tag; ERROR: Recor",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:4486,ERROR,ERROR,4486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 187110, Read name UMI-ACG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187121, Read name UMI-CCG-GCC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187154, Read name UMI-CAA-CTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187181, Read name UMI-CGG-GAG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 187209, Read name UMI-CAA-GTT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 279812, Read name UMI-ACT-GGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 327672, Read name UMI-AGT-CGG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 367457, Read name UMI-GGA-TTA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 441607, Read name UMI-AGA-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481504, Read name UMI-AAC-TCT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481532, Read name UMI-AAT-CAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481722, Read name UMI-ATA-ATT-10, Zero-length read without FZ, CS or CQ tag; ERROR: Record 481989, Read name UMI-CGA-CTA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482114, Read name UMI-GAG-TAA-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482150, Read name UMI-GCC-GTA-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482210, Read name UMI-GGT-TCC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482222, Read name UMI-GTA-GTT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 482251, Read name UMI-GTT-TAC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 541693, Read name UMI-AGG-GAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 763643, Read name UMI-GAG-TAT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 763881, Read name UMI-AGC-TTT-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 764724, Read name UMI-AAT-ATA-14, Zero-length read without FZ, CS or CQ tag; ERROR: Reco",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:4576,ERROR,ERROR,4576,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 764749, Read name UMI-GCT-GTG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 764766, Read name UMI-AGC-TAG-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 764858, Read name UMI-AGA-GGT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 764950, Read name UMI-CTT-GCC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 765124, Read name UMI-CGG-TGT-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 765139, Read name UMI-GGA-GTC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 765157, Read name UMI-ATA-CTC-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 765213, Read name UMI-AGC-TCT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 765249, Read name UMI-AAG-GAT-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 765281, Read name UMI-AAG-ACT-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 765385, Read name UMI-CGA-CGT-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 765535, Read name UMI-GGG-TTG-10, Zero-length read without FZ, CS or CQ tag; ERROR: Record 765582, Read name UMI-ATG-TAA-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 765607, Read name UMI-CCG-CTA-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 765620, Read name UMI-AAA-ATT-16, Zero-length read without FZ, CS or CQ tag; ERROR: Record 765717, Read name UMI-AGG-TAT-8, Zero-length read without FZ, CS or CQ tag; ERROR: Record 766523, Read name UMI-GAA-GGA-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 822437, Read name UMI-AGA-CCT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 936121, Read name UMI-CGA-TTT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 964359, Read name UMI-ACT-TAA-16, Zero-length read without FZ, CS or CQ tag; ERROR: Record 965939, Read name UMI-GCA-GTT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 965956, Read name UMI-AAA-ATA-37, Zero-length read without FZ, CS or CQ tag; ERROR: Re",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:6558,ERROR,ERROR,6558,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Q tag; ERROR: Record 966397, Read name UMI-ACC-CGG-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966402, Read name UMI-CAG-TGT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966417, Read name UMI-CCG-CCT-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966450, Read name UMI-CCC-GAT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966462, Read name UMI-CCG-TCT-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966487, Read name UMI-GAT-GTT-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966491, Read name UMI-GTG-TTG-3, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966501, Read name UMI-AGA-ATG-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966509, Read name UMI-AGT-GGT-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966514, Read name UMI-ATC-GGA-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966517, Read name UMI-GAT-TGA-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966538, Read name UMI-ATA-GGG-23, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966542, Read name UMI-GTG-TAG-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966591, Read name UMI-CCG-TAT-6, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966596, Read name UMI-GTT-GTT-3-D2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966613, Read name UMI-ACC-GAC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966616, Read name UMI-ACG-TGG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966618, Read name UMI-ACT-GGG-11, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966620, Read name UMI-ACT-GGG-12, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966627, Read name UMI-GGC-TGT-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966674, Read name UMI-CCT-GTC-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 966699, Read name UMI-CCG-TGA-4, Zero-length read without FZ, CS or CQ tag; ERROR: ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:8813,ERROR,ERROR,8813,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['ERROR'],['ERROR']
Availability,"Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MBQ,Number=A,Type=Integer,Description=""median base quality"">; ##FORMAT=<ID=MFRL,Number=R,Type=Integer,Description=""median fragment length"">; ##FORMAT=<ID=MMQ,Number=A,Type=Integer,Description=""median mapping quality"">; ##FORMAT=<ID=MPOS,Number=A,Type=Integer,Description=""median distance from end of read"">; ##FORMAT=<ID=OBAM,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact modes."">; ##FORMAT=<ID=OBAMRC,Number=A,Type=String,Description=""Whether the variant can be one of the given REF/ALT artifact mode complements."">; ##FORMAT=<ID=OBF,Number=A,Type=Float,Description=""Fraction of alt reads indicating orientation bias error (taking into account artifact mode complement)."">; ##FORMAT=<ID=OBP,Number=A,Type=Float,Description=""Orientation bias p value for the given REF/ALT artifact or its complement."">; ##FORMAT=<ID=OBQ,Number=A,Type=Float,Description=""Measure (across entire bam file) of orientation bias for a given REF/ALT error."">; ##FORMAT=<ID=OBQRC,Number=A,Type=Float,Description=""Measure (across entire bam file) of orientation bias for the complement of a given REF/ALT error."">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=SA_MAP_AF,Number=3,Type=Float,Description=""MAP estimates of allele fraction given z"">; ##FORMAT=<ID=SA_POST_PROB,Number=3,Type=Float,Description=""posterior probabilities of the presence of strand artifact"">; etc..; etc..; etc..; 1 237752 . A G . artifact_in_n",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5158:3258,error,error,3258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5158,1,['error'],['error']
Availability,"R $REF/Chr06.fa \; --variant $NOW/w-1.raw.g.vcf \; --variant $NOW/w-10.raw.g.vcf \; --variant $NOW/w-100.raw.g.vcf \; -o KF427.raw.vcf. I got a error like this:. ##### ERROR MESSAGE: Invalid command line: No tribble type was provided on the command line and the type of the file could not be determined dynamically. Please add an explicit type tag :NAME listing the correct type from among the supported types:; ##### ERROR Name FeatureType Documentation; ##### ERROR BCF2 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF VariantContext (this is an external codec and is not documented within GATK); ##### ERROR VCF3 VariantContext (this is an external codec and is not documented within GATK); ##### ERROR ------------------------------------------------------------------------------------------. then I added a name like this:. --variant:VCF $NOW/w-91.raw.g.vcf \; --variant:VCF $NOW/w-92.raw.g.vcf \; --variant:VCF $NOW/w-93.raw.g.vcf \. also met a error like this:. ##### ERROR; ##### ERROR MESSAGE: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; ##### ERROR ------------------------------------------------------------------------------------------. and I change the name like this:. --variant:VCF3 $NOW/w-91.raw.g.vcf \; --variant:VCF3 $NOW/w-92.raw.g.vcf \; --variant:VCF3 $NOW/w-93.raw.g.vcf \. also error:. ##### ERROR MESSAGE: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv3 and does not support VCFv4.1, for input source: /gss1/home/hjb20181119/panyongpeng/NN1138-2/RIL_genotype/mapping/w-1.raw.g.vcf; ##### ERROR ------------------------------------------------------------------------------------------. I checked my GVCF file and the header is :. ##fileformat=VCFv4.1; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""L",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7315:1425,error,error,1425,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7315,1,['error'],['error']
Availability,"R addresses two phasing bugs, https://github.com/broadinstitute/gatk/issues/6463 and https://github.com/broadinstitute/gatk/issues/6845. https://github.com/broadinstitute/gatk/issues/6463 identified a bug in the phasing algorithm which caused the wrong phase information to be output for scenarios where the first variant in a phase set is homozygous variant and it is followed by het variants in opposite phase. Without this change the het variants were incorrectly placed on the same phase strand because the phase set was tied to the hom var variant, and the algorithm assumed that each het variant could be put in the same phase strand as it because it was on all haplotypes. I've modified the algorithm to keep track, for variants that occur on all haplotypes, of which of the haplotypes have already been used for phasing an upstream ""comp"" variant so that further downstream variants can be checked against the remaining set. https://github.com/broadinstitute/gatk/issues/6845 showed an example of phase sets being disrupted by the presence of an alternate haplotype that supported an additional, uncalled, variant in the region. In this case there was an alternate haplotype supported by two reads that supported a SNP downstream of two pairs of SNPs in alternate phase. The presence of an additional haplotype causes the phasing algorithm to break the phase sets in the region. I've modified the algorithm to only use haplotypes that support the alternate alleles present in called variants in phasing by modifying the number that we pass as `AssemblyBasedCallerUtils.constructPhaseSetMapping()`'s `totalAvailableHaplotypes` parameter. In my opinion this ; fix produces output that is still correct and is much easier to understand (since it only depends on sites that are visible in the output VCF), but if anyone objects to this change please let me know. . Non-test code changes for this PR are in two different commits to try to make it easier to understand the scope of the two changes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7019:1233,down,downstream,1233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7019,1,['down'],['downstream']
Availability,RAW_MQ/sumSquaredMQs parsing error when running GenotypeGVCFs for JointGenotyping,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5433:29,error,error,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5433,1,['error'],['error']
Availability,"RK: run using spark-submit on an existing cluster ; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after -- ; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated ; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the ; java JVM at runtime. ; Java options MUST be passed inside a single string with space-separated values. --debug-port <number> sets up a Java VM debug agent to listen to debugger connections on a; particular port number. This in turn will add the necessary java VM arguments; so that you don't need to explicitly indicate these using --java-options.; --debug-suspend sets the Java VM debug agent up so that the run get immediatelly suspended; waiting for a debugger to connect. By default the port number is 5005 but; can be customized using --debug-port. But when i run other commands with gatk like gatk --list, i got this error :; gatk --list; Using GATK jar /home/ameni/Documents/pharmacogenomics/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/ameni/Documents/pharmacogenomics/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main; 	java.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8280:2071,error,error,2071,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8280,2,"['Error', 'error']","['Error', 'error']"
Availability,RN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 15:09:38.533 INFO FeatureManager - Using codec IntervalListCodec to read file file:///home/n.liorni/snakemake_cnv_gatk/results/cnv/targets.preprocessed.interval_list; 15:09:38.659 INFO IntervalArgumentCollection - Processing 548086 bp from intervals; 15:09:38.697 INFO DetermineGermlineContigPloidy - Validating and aggregating coverage per contig from input read-count files...; 15:09:38.711 INFO DetermineGermlineContigPloidy - Aggregating read-count file results/cnv/hdf5/MGM20-0848_S4.hdf5 (1 / 4); 15:09:38.734 INFO DetermineGermlineContigPloidy - Aggregating read-count file results/cnv/hdf5/MGM20-0872_S2.hdf5 (2 / 4); 15:09:38.745 INFO DetermineGermlineContigPloidy - Aggregating read-count file results/cnv/hdf5/MGM20-1121_S4.hdf5 (3 / 4); 15:09:38.757 INFO DetermineGermlineContigPloidy - Aggregating read-count file results/cnv/hdf5/MGM20-1543_S10.hdf5 (4 / 4); 15:12:24.486 INFO DetermineGermlineContigPloidy - Shutting down engine; [18 ottobre 2021 15.12.24 CEST] org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy done. Elapsed time: 2.95 minutes.; Runtime.totalMemory()=2215116800; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python /tmp/cohort_determine_ploidy_and_depth.1340409154615700376.py --sample_coverage_metadata=/tmp/samples-by-coverage-per-contig6984698047199787039.tsv --output_calls_path=/home/n.liorni/snakemake_cnv_gatk/results/cnv/ploidy/ploidy-calls --mapping_error_rate=1.000000e-02 --psi_s_scale=1.000000e-04 --mean_bias_sd=1.000000e-02 --psi_j_scale=1.000000e-03 --learning_rate=5.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.990000e-01 --log_emission_samples_per_round=2000 --log_emission_sampling_rounds=100 --log_emission_sampling_median_rel_error=5.000000e-04 --max_advi_iter_first_epoch=1000 --max_advi_iter_subsequent_epochs=1000 --min_training_epochs=20 --max_training_epochs=1,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:4955,down,down,4955,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['down'],['down']
Availability,"ROR org.apache.spark.scheduler.TaskSetManager: Task 284 in stage 25.0 failed 4 times; aborting job; 18/01/12 20:38:37 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@23007ed{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(50,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(52,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(34,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(60,WrappedArray()); 20:38:37.897 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [January 12, 2018 8:38:37 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 42.74 minutes.; Runtime.totalMemory()=16692805632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 284 in stage 25.0 failed 4 times, most recent failure: Lost task 284.3 in stage 25.0 (TID 43224, cw-test-w-6.c.broad-dsde-methods.internal, executor 7): java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:26962677-26962806_-_482S130M491S_60_-1_281_S; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.ContigAlignmentsModifier.removeOverlap(ContigAlignmentsModifier.java:36); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.AssemblyContigAlignmentSignatureClassifier.lam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:5973,down,down,5973,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['down'],['down']
Availability,R] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.process.internal.ExecException: Process 'Gradle Test Executor 1' finished with non-zero exit value 134; 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:369); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcess.waitForStop(DefaultWorkerProcess.java:190); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.process.internal.worker.DefaultWorkerProcessBuilder$MemoryRequestingWorkerProcess.waitForStop(DefaultWorkerProcessBuilder.java:228); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.worker.ForkingTestClassProcessor.stop(ForkingTestClassProcessor.java:122); 11:54:40.436 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.endBatch(RestartEveryNTestClassProcessor.java:63); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.tasks.testing.processors.RestartEveryNTestClassProcessor.stop(RestartEveryNTestClassProcessor.java:57); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.dispatch.FailureHandlingDispatch.dispatch(FailureHandlingDispatch.java:29); 11:54:40.437 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:14447,ERROR,ERROR,14447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['ERROR'],['ERROR']
Availability,"R_SAMTOOLS : true; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:53:34.608 INFO ValidateVariants - Deflater: IntelDeflater; 19:53:34.608 INFO ValidateVariants - Inflater: IntelInflater; 19:53:34.608 INFO ValidateVariants - GCS max retries/reopens: 20; 19:53:34.608 INFO ValidateVariants - Requester pays: disabled; 19:53:34.608 INFO ValidateVariants - Initializing engine; 19:53:35.169 INFO FeatureManager - Using codec VCFCodec to read file file://chr1-22.phased.rename.reheader.vcf.gz; 19:53:35.594 INFO ValidateVariants - Done initializing engine; 19:53:35.594 WARN ValidateVariants - IDS validation cannot be done because no DBSNP file was provided; 19:53:35.594 WARN ValidateVariants - Other possible validations will still be performed; 19:53:35.594 INFO ProgressMeter - Starting traversal; 19:53:35.595 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 19:53:35.660 INFO ValidateVariants - Shutting down engine; [October 25, 2020 7:53:35 PM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2114453504; java.lang.ArrayIndexOutOfBoundsException: -87; 	at org.broadinstitute.hellbender.utils.BaseUtils.convertIUPACtoN(BaseUtils.java:123); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.getSubsequenceAt(CachingIndexedFastaSequenceFile.java:340); 	at org.broadinstitute.hellbender.engine.ReferenceFileSource.queryAndPrefetch(ReferenceFileSource.java:78); 	at org.broadinstitute.hellbender.engine.ReferenceDataSource.queryAndPrefetch(ReferenceDataSource.java:64); 	at org.broadinstitute.hellbender.engine.ReferenceContext.getBases(ReferenceContext.java:197); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants.apply(ValidateVariants.java:236); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at org.broadinstitute.h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:3266,down,down,3266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['down'],['down']
Availability,"Ran into this when trying to create a PoN with 100 samples x 100 bp bins = 1.27 * max int elements. This currently causes issues when truncating outliers, at which point all elements are loaded into an array so that Percentiles can be naively computed, resulting in a `java.lang.NegativeArraySizeException`. Solutions include: 1) simply throwing a message and failing early if the counts matrix is too large (perhaps recommend scattering by contig, see #4728), 2) changing the outlier truncation procedure to be more robust. I'm not sure how important outlier truncation is to the SVD, as it remains to be evaluated, but for now we should be able to get around this with no code changes by simply disabling it (i.e., setting the relevant truncation percentile to 0). Note that file I/O takes about an hour for this case. Also note that this is probably on the extreme end of what we should expect to support on a single machine with all counts in memory, as the SVD is probably sufficiently good with 100 samples and 100 bp is on the order of the read length. #4728 will get around this and also make downstream tasks complete faster in parallel, at the very small expense of reducing a few global parameters to per-contig parameters in the modeling step.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4734:517,robust,robust,517,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4734,2,"['down', 'robust']","['downstream', 'robust']"
Availability,"Ran the updated version on 1000 shards with 11k samples, and there were no 503 or SSL errors at all.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3295#issuecomment-316438413:86,error,errors,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3295#issuecomment-316438413,1,['error'],['errors']
Availability,Random and unplaced contigs are a part of the primary assembly. The tool also errors on decoy contigs but no other GRCh38 contig type. This ticket is to investigate why this error is happening for this subset of the data. Error is; ```; Stdout: 22:01:54.365 INFO segment_gcnv_calls - Loading ploidy calls...; 22:01:54.366 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chrM). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270706v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270707v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270708v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.381 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270709v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270710v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270711v1_random). Germline contig ploidy determination may not be reliable for decoy/non-standard contigs.; 22:01:54.382 WARNING gcnvkernel.structs.metadata - Sample HG01565 has an unrecognized contig (chr1_KI270712v1_random). Germline contig ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4840:78,error,errors,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840,6,"['Error', 'error', 'reliab']","['Error', 'error', 'errors', 'reliable']"
Availability,Rate of failure still seems high in 4.1.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-459927501:8,failure,failure,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-459927501,1,['failure'],['failure']
Availability,"Re: strandedness, while it would be much easier not to have to track strand (and would probably save me from writing a lot of bugs), I think that we do need to do so if we ever want these data elements to be used for calling more complicated events like inversions or translocations. For example, for inversions we need to record whether we saw evidence linking +/+ strands or -/- strands, since the breakpoints might appear different at the two junctions due to insertions or deletions. For translocations, you definitely need to know which strands were involved so that you can extrapolate the rest of the event from the breakpoint. Agreed that it's a pain to keep track of it, though, and I find myself introducing strand errors all the time.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-324204507:725,error,errors,725,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-324204507,1,['error'],['errors']
Availability,"Read error correction tends to follow the basic strategy of 1) collect kmer counts 2) replace rare kmers with their closest non-rare match. For germline calling where there is a huge gap between error rates and diploid het allele fractions this is sufficient. Mutect, however, must contend with cases where counts alone do not discriminate perfectly between errors and real mutations. Without committing to an approach, it seems like phasing might help. That is, we could construct haplotypes of rare kmers and error correct those. This should work because sequencing errors are unphased and real variants are. There are phased artifacts, of course, but we handle those in downstream filtering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4868:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4868,6,"['down', 'error']","['downstream', 'error', 'errors']"
Availability,"ReadFilter system needs to be ported from GATK. It should be available to tools by ""request"". The specifics are to be figured out as part of addressing this issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5:61,avail,available,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5,1,['avail'],['available']
Availability,ReadThreadingGraph.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3953755Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/readthreading/AbstractReadThreadingGraph.java:4: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.3960718Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/ReadLikelihoodCalculationEngine.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3962332Z src/main/java/org/broadinstitute/hellbender/utils/pairhmm/PairHMM.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3968675Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/PairHMMLikelihoodCalculationEngine.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3978229Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/Path.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3984771Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/Kmer.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3993495Z src/main/java/org/broadinstitute/hellbender/utils/pileup/ReadPileup.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4002426Z src/main/java/org/broadinstitute/hellbender/utils/pileup/PileupElement.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4005459Z src/main/java/org/broadinstitute/hellbender/utils/pileup/PileupElement.java:315: error: cannot find symbol; 2022-08-16T00:09:07.4005923Z @VisibleForTesting; 2022-08-16T00:09:07.4006520Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4006876Z location: class PileupElement; 2022-08-16T00:09:07.4015304Z src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/LeftAlignAndTrimVariants.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:9459,error,error,9459,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['error'],['error']
Availability,ReadThreadingGraph.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7873510Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/readthreading/AbstractReadThreadingGraph.java:4: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.7881195Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/ReadLikelihoodCalculationEngine.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7882811Z src/main/java/org/broadinstitute/hellbender/utils/pairhmm/PairHMM.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7889039Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/PairHMMLikelihoodCalculationEngine.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7926431Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/Path.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7973092Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/Kmer.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7983013Z src/main/java/org/broadinstitute/hellbender/utils/pileup/ReadPileup.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7993443Z src/main/java/org/broadinstitute/hellbender/utils/pileup/PileupElement.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7996577Z src/main/java/org/broadinstitute/hellbender/utils/pileup/PileupElement.java:315: error: cannot find symbol; 2022-08-16T22:45:53.7997033Z @VisibleForTesting; 2022-08-16T22:45:53.7997778Z symbol: class VisibleForTesting; 2022-08-16T22:45:53.7998135Z location: class PileupElement; 2022-08-16T22:45:53.8006697Z src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/LeftAlignAndTrimVariants.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:11497,error,error,11497,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,1,['error'],['error']
Availability,ReadTransformer system needs to be available to hellbender tools. The mechanism of how it gets enabled needs to be coordinated with ReadFilters (issue #5 ),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6:35,avail,available,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6,1,['avail'],['available']
Availability,Reader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:125); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:66); at org.broadinstitute.hellbender.engine.ReadsDataSource.prepareIteratorsForTraversal(ReadsDataSource.java:416); at org.broadinstitute.hellbender.engine.ReadsDataSource.iterator(ReadsDataSource.java:342); at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); ```. and my command:; ```; gatk-4.1.7.0/gatk Mutect2 -force-active -R ~/hg19/gatk_bundle/ucsc.hg19.fasta \; -I bam/T.BQSR.reheader.bam --alleles xx.vcf -O gga.vcf -L total.site.bed -ip 200; ```; Maybe the bam file is broken? I have no idea how to debug with the error messages showed above.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709:3029,error,error,3029,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709,1,['error'],['error']
Availability,Reader; at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:463); at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:365); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:319); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:291); at org.broadinstitute.hellbender.engine.VariantLocusWalker.initialize at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFava:726); at org.broadinstitute.hellbender.engine.VariantLocusWalker.onStartup(VariantLocusWalker.java:63); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.io.IOException: GenomicsDB JNI Error: Broad combine GVCFs exception : No sample/CallSet name specified in JSON file/Protobuf object for TileDB row 72; at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHeadersForQuery(GenomicsDBFeatureReader.java:200); at org.genomicsdb.reader.GenomicsDBFeatureReader.<init>(GenomicsDBFeatureReader.java:85); at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:460). ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8527:7436,Error,Error,7436,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8527,1,['Error'],['Error']
Availability,"Reading from GenomicsDB fails when a some records containing spanning deletion alleles are imported into a workspace. Not all records seem to cause this to fail; I haven't been able to figure out what specific properties of the records cause the error. Here's the contents (minus header) of a VCF file that causes the error:. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	NA12878; 20	10097436	.	CTTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTTCTTT	C,<NON_REF>	1054.73	.	BaseQRankSum=1.820;ClippingRankSum=0.000;DP=89;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=-6.464;RAW_MQ=262143.00;ReadPosRankSum=-3.231	GT:AD:DP:GQ:PL:SB	0/1:57,32,0:89:99:1092,0,2241,1263,2338,3601:23,34,11,21; 20	10097437	.	TTTTC	*,T,<NON_REF>	2089.73	.	DP=76;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=217330.00	GT:AD:DP:GQ:PL:SB	1/2:0,32,23,0:55:99:2127,940,1799,1195,0,1125,2201,1453,1262,2642:0,0,16,39; ```. Steps to reproduce:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V test_gdb_import.vcf.gz -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.vcf -L 20; ```. Error:. ```; java.lang.IllegalArgumentException: Duplicate allele added to VariantContext: T; at htsjdk.variant.variantcontext.VariantContext.makeAlleles(VariantContext.java:1490); at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:380); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:132); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java:357); at com.intel.genomicsdb.GenomicsDBFeatureReader$GenomicsDBFeatureIterator.next(GenomicsDBFeatureReader.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4716:246,error,error,246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4716,2,['error'],['error']
Availability,Readme: Git LFS download size not accurate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6932:16,down,download,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6932,1,['down'],['download']
Availability,"Reads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:00:28.284 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:00:28.284 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:00:28.284 INFO PrintReads - Deflater: IntelDeflater; 15:00:28.284 INFO PrintReads - Inflater: IntelInflater; 15:00:28.285 INFO PrintReads - GCS max retries/reopens: 20; 15:00:28.285 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 15:00:28.285 INFO PrintReads - Initializing engine; 15:00:33.117 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 15:00:33.134 INFO PrintReads - Shutting down engine; [October 5, 2017 3:00:34 PM EDT] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=2255486976; ***********************************************************************. A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--javaOptions '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; -bash-4.1$ ; ```. ## Confirm all files present in bucket; ```; WMCF9-CB5:newCNV shlee$ gsutil ls gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN*; gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.bam.bas; gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.crai; gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram; gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram.crai; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:23908,ERROR,ERROR,23908,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['ERROR'],['ERROR']
Availability,"Reads` from Mutect2 (https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/SomaticGenotypingEngine.java#L326); - separate consideration of overlapping reads when calculating genotype likelihoods, as used in UnifiedGenotyper (https://github.com/broadgsa/gatk-protected/blob/aa8764d6c3de146856b174a8674fa787a6311d7c/protected/gatk-tools-protected/src/main/java/org/broadinstitute/gatk/tools/walkers/genotyper/DiploidSNPGenotypeLikelihoods.java#L183). As I see it, #4958, which seems to be more related to read likelihood calculation, is where a more involved solution, with more fundamental changes, might be warranted. From my perspective (not being especially familiar with the pairHMM model), an ideal solution would transition the pairHMM from read likelihood to a ""fragment likelihood"" or ""haplotype likelihood"" when information from read pairs is available, even if they aren't overlapping. The idea would be that a modified pairHMM model could produce a single fragment (or haplotype) likelihood for a given read pair. Such an approach would unify the issues of ""merging"" read pairs and phasing in a read-pair aware manner (and potentially also modeling PCR errors). In principle, a ""fragment likelihood""-type approach could even incorporate info from corresponding PCR duplicates to improve the results when sequencing error rates are high. This sort of approach could also flow into the genotype likelihood calculation by providing a single, merged fragment likelihood to consider rather than a separate read likelihood for each read. UPDATE: On further thought, it probably wasn't a good idea for me to use the terms fragment likelihood or haplotype likelihood to distinguish the proposed approach, since the read likelihood is effectively already calculating those. Probably a better term would be ""read set likelihood"", where the read set would consist of paired reads and potentially also any corresponding PCR duplicate reads.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5436#issuecomment-443558420:2303,error,errors,2303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5436#issuecomment-443558420,2,['error'],"['error', 'errors']"
Availability,Realignment in Mutect2 reports error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6123:31,error,error,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6123,1,['error'],['error']
Availability,"Rebased on current master, and added a second commit to reflect the new test results that are presumably related to https://github.com/broadinstitute/gatk/pull/7394. I assume that the original issue (overwriting the expected results) masked the fact that these test results changed when the tests were run for https://github.com/broadinstitute/gatk/pull/7394.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7563#issuecomment-975866635:234,mask,masked,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7563#issuecomment-975866635,1,['mask'],['masked']
Availability,"RecalibratorEngine.java:43); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.onTraversalSuccess(VariantRecalibrator.java:625); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:895); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. I believe this is derived from an error earlier in the log, since the `stderr` gives the same Java heap space error: ; ```; [2019-09-16 19:05:59,50] [error] WorkflowManagerActor Workflow 9f7a01a4-0632-4817-8622-aa51e520abf1 failed (during ExecutingWorkflowState): Job JointGenotyping.SNPsVariantRecalibratorClassic:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: /path/to/stderr.; ```. I have read past issues (https://gatkforums.broadinstitute.org/gatk/discussion/23880/java-heap-space) regarding this that may suggest it is a bug. It has pointed me to increasing the available heap memory through the primary command of -Xmx. Is this the way to do it? ; ```; java -Xmx600G -Dconfig.file=' + re.sub('input.json', 'overrides.conf', input_json) + ' -jar ' + args.cromwell_path + ' run ' + re.sub('input.json', 'joint-discovery-gatk4.wdl', input_json) + ' -i ' + input_json; ```; where I substitute in the corresponding config, json, and wdl files. . Is 600G enough? Each vcf is around 6G large and since I have 150, does that mean I should be allocating more than 900G (6G x 150)?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6165:2460,avail,available,2460,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6165,1,['avail'],['available']
Availability,"Receiving objects: 100% (243170/243170), 608.21 MiB | 996.00 KiB/s, done.; Resolving deltas: 100% (130671/130671), done.; From https://github.com/broadgsa/gatk; * [new tag] 3.8-1 -> 3.8-1; * [new tag] 1.0 -> 1.0; git symbolic-ref refs/git-r3/sci-biology/gatk/0/__main__ refs/tags/3.8-1; * Checking out https://github.com/broadgsa/gatk.git to /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven usin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:1489,ERROR,ERROR,1489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514,2,['ERROR'],['ERROR']
Availability,"Recently I was setting up GATK to run in a VM and I had forgotten to install Java8 onto the machine. When I tried to run GATK from the launch script I ran into the following error: ; ```; Using GATK jar /home/emeryj/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/emeryj/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar -help; Traceback (most recent call last):; File ""./gatk"", line 479, in <module>; main(sys.argv[1:]); File ""./gatk"", line 152, in main; runGATK(sparkRunner, sparkSubmitCommand, dryRun, gatkArgs, sparkArgs, javaOptions); File ""./gatk"", line 328, in runGATK; runCommand(cmd, dryrun); File ""./gatk"", line 384, in runCommand; check_call(cmd, env=gatk_env); File ""/usr/lib/python2.7/subprocess.py"", line 181, in check_call; retcode = call(*popenargs, **kwargs); File ""/usr/lib/python2.7/subprocess.py"", line 168, in call; return Popen(*popenargs, **kwargs).wait(); File ""/usr/lib/python2.7/subprocess.py"", line 390, in __init__; errread, errwrite); File ""/usr/lib/python2.7/subprocess.py"", line 1024, in _execute_child; raise child_exception; OSError: [Errno 2] No such file or directory; ```; This should perhaps be made a little bit clearer for users as this isn't particularly helpful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5992:174,error,error,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5992,1,['error'],['error']
Availability,"Recently ran into an issue where spark.yarn.executor.memoryOverhead is not being set. . Running Cromwell v29 on DataProc (image version 1.1) with the following launch command in my WDL:; ````; set -eu; export GATK_GCS_STAGING=${jarCacheBucket}; ${gatk} \; PathSeqPipelineSpark \; ...; -- \; --spark-runner GCS \; --cluster ${clusterName} \; --driver-memory 8G \; --executor-memory 32G \; --num-executors 1 \; --executor-cores 30 \; --conf spark.yarn.executor.memoryOverhead=132000; ````; I get the following error:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 25 in stage 37.0 failed 4 times, most recent failure: Lost task 25.3 in stage 37.0 (TID 19238, mw-pathseq-w-3.c.broad-dsde-methods.internal): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 48.9 GB of 34 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; ```; In my Cromwell log I see the parameter is there:; ```; ...; Replacing spark-submit style args with dataproc style args. --cluster mw-pathseq --driver-memory 8G --executor-memory 32G --num-executors 1 --executor-cores 30 --; conf spark.yarn.executor.memoryOverhead=132000 -> --cluster mw-pathseq --properties spark.driver.userC; lassPathFirst=true,spark.io.compression.codec=lzf,spark.driver.maxResultSize=0,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600,spark.driver.memory=8G,spark.executor.memory=32G,spark.executor.instances=1,spark.execut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4273:508,error,error,508,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4273,3,"['error', 'failure']","['error', 'failure']"
Availability,"Recording key comments from the old pivotal entry ""Downsampling HC is wonky and misunderstood"" for posterity as a cautionary tale. ---. Reported in http://gatk.vanillaforums.com/discussion/3094/downsampling-with-haplotypecaller. User specifies -dcov 200 but DP per sample in VCF is higher than that. Local cmdline:. ```; java -Xmx16g -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/gsa-hpprojects/1kg/reference/hs37d5.fa -nct 8 -ped /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/families.ped -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND1/UDP2731_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND2/UDP3478_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND3/UDP4031_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND4/UDP4032_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND5/UDP4033_1.forGATK.bam -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND6/UDP4573_1.forGATK.bam -dcov 200 -minPruning 4 -o /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/eflynn90-test.vcf -L /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/intervals.vcf -stand_emit_conf 10 -pedValidationType SILENT; ```. ---. @eitanbanks said:. Updated command-line:. ```; java -Xmx6g -jar dist/GenomeAnalysisTK.jar -T HaplotypeCaller -R /humgen/1kg/reference/hs37d5.fasta -I /humgen/gsa-scr1/vdauwera/userfiles/Downsampling_with_HC/bams/IND1/UDP2731_1.forGATK.bam -dcov 200 -minPruning 4 -L 1:14464; ```. I can confirm that it appears that down-sampling is not working for the Haplotype Caller (when run through the Unified Genotyper the down-sampling works just fine).; I see in SAMDataSource line 668 that assumeDownstreamLIBSDownsampling is being set to true. But then it doesn't look like LIBS is actually down-sampling. Don't have time to debug more so passing on to David. ---. @droaz",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:194,down,downsampling-with-haplotypecaller,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['down'],['downsampling-with-haplotypecaller']
Availability,Reduce the logging a bit.; Probably should make a PR directly into gatk master so that when we next merge gatk master changes we'll get this goodness?. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f8c38f97-7945-414f-9432-13b2f12138bb) (note failed one of the subtests for a random docker pull error); Example CreateFilterSet run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/b2e7eb86-e494-4891-885b-5a96cb1056b3),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8650:343,error,error,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8650,1,['error'],['error']
Availability,Redundant implementations of dotProduct(),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3311:0,Redundant,Redundant,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3311,1,['Redundant'],['Redundant']
Availability,Redundant representations of genotypes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1907:0,Redundant,Redundant,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1907,1,['Redundant'],['Redundant']
Availability,Reenable CRAM tests in GatherBamFilesIntegrationTest and SortSamIntegrationTests when htsjdk issue #365 fix is available,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1141:111,avail,available,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1141,1,['avail'],['available']
Availability,Refactor AlleleListUtilsUnitTest to have no skips and be robustly deterministic,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/607:57,robust,robustly,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/607,1,['robust'],['robustly']
Availability,Reg error in DetermineGermlineContigPloidy,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4,error,error,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['error'],['error']
Availability,"Regarding GenomicsDbImport with intervals, here is a quick test. Again, the thing I'm trying to evaluate is whether it matters how I chunk the genome for GenomicsDBImport->GenotypeGVCFs. Downstream of this, I would pass the workspace to GenotypeGVCFs, with --only-output-calls-starting-in-intervals. The concern is whether we have variants spanning the intervals of two jobs, and whether separating the jobs would impact calls. In this example, GenotypeGVCFs would run over 1:1050-1150. For example, if we had a multi-NT variant that spanned 1148-1052, we'd want that called correctly no matter what intervals were used for the jobs. I tried using running GenomicsDBImport with -L over a small region, or I ran SelectVariants on the gVCF first (which behaves a little differently), and then used that subset gVCF as input to GenomicsDBImport, where GenomicsDBImport is given the entire contig as the interval. The resulting workspaces will be slightly different, with the latter containing information over a wider region (GenomicsDBIport truncates start/end of the input records to just the target interval). . So if either of these workspaces is passed to GenotypeGVCFs, using --only-output-calls-starting-in-intervals and -L 1:1050-1150:. I think any upstream padding doesnt matter. If you have a multi-nucleotide polymorphism that starts upstream of 1050 but spans 1050, this job wouldnt be responsible for calling that. The prior job, which has an interval set upstream of this one should call it. I think GenomicsDbImport's behavior is fine here. If you have a multi-NT variant that starts within 1050-1150, but extends outside (i.e. deletion or insertion starting at 1148), this could be a problem. The GenomicsDB workspace created with the interval 1:1050-1150 lacks the information to score that, right? The workspace created using the more permissive SelectVariants->GenomicsDBImport contains that downstream information and presumably would make the same call as if GenotypeGVCFs was given ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244:187,Down,Downstream,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244,1,['Down'],['Downstream']
Availability,"Regarding the non-Docker integration tests failing earlier today, I think this was because the R packages were added to the Travis cache in #3101. @cmnbroad cleared the cache to see if we could reproduce a compiler error introduced in #3934 on Travis (for the record, we could reproduce it on my local Ubuntu machine and gsa5, but not on Travis). This removed the cached getopt dependency, which then caused tests to fail. See #4246.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359999441:215,error,error,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359999441,2,['error'],['error']
Availability,"Related to VS-1395. On the PR to EchoCallset, Bec had a couple of minor suggestions. I folded them into that PR and am now adding them here to `ah_var_store`. Passing test run without a backslash on output_path [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/f02829f5-d546-4f54-85e6-e7ea6be5829e).; Passing test run with a backslash on output_path [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/3003841c-0e62-4ef6-bb37-6eeb038da9af).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8881:33,Echo,EchoCallset,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8881,1,['Echo'],['EchoCallset']
Availability,"Related to https://github.com/broadgsa/gatk/pull/22.; Port of https://github.com/broadinstitute/gsa-unstable/pull/1606. Also replaced --allowMissingData with --errorIfMissingData since we'd rather not fail by default for missing data.; The root cause of the problem is due the implementation of `Genotype.hasAnyAttribute`. It always returns true for FT, even if it's missing.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3252:160,error,errorIfMissingData,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3252,1,['error'],['errorIfMissingData']
Availability,"Related, I have found the documentation for the `-no-overlaps` option to be a bit unclear. Especially with it being a default option now in the WARP [VariantCalling WDL script](https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/variant_calling/VariantCalling.wdl), I would hope that there would be a better description of where the overlaps come from, when we do and don't want to allow them, how downstream results would be affected, and why this is now a default option. If I should make this documentation request a separate issue, let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8103#issuecomment-1329756065:431,down,downstream,431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103#issuecomment-1329756065,1,['down'],['downstream']
Availability,Relax CRAN remote failure mode.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5602:18,failure,failure,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5602,1,['failure'],['failure']
Availability,Remove GATK classes that are redundant with Picard classes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6678:29,redundant,redundant,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6678,1,['redundant'],['redundant']
Availability,Remove download of picard.jar from .travis.yml and update Mutect2 WDL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3625:7,down,download,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3625,1,['down'],['download']
Availability,Remove file causing git-lfs error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7594:28,error,error,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7594,1,['error'],['error']
Availability,"Remove guava-jdk5 dependency to fix ""stopWatch"" error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/867:48,error,error,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/867,1,['error'],['error']
Availability,Remove redundant references from test data now that we have full-sized references,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5313:7,redundant,redundant,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5313,1,['redundant'],['redundant']
Availability,"Remove the code in `org.broadinstitute.hellbender.utils.gene`, because it is redundant with the code in Picard `picard.annotation`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3695:77,redundant,redundant,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3695,1,['redundant'],['redundant']
Availability,Remove/repair bogus CRAM test files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1368:7,repair,repair,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1368,1,['repair'],['repair']
Availability,Removed mapping error rate from estimate of denoised copy ratios output by gCNV and updated sklearn.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7261:16,error,error,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7261,1,['error'],['error']
Availability,Removing the error check in RMSMappingQuality.getNumReads which was causing a crash when depth was being reported as 0 or less.; It will now return -1 which should match gatk3 behavior.; Fixes #2658,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2669:13,error,error,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2669,1,['error'],['error']
Availability,"Reopening -- @ldgauthier reports that this error still occurs even after the patch in https://github.com/broadinstitute/gatk/pull/5099. With that patch, we are now retrying on `UnknownHostException`, but the retries are all failing: . ```; [August 14, 2018 7:09:18 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 896.64 minutes.; Runtime.totalMemory()=3966238720; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:43,error,error,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,3,"['Error', 'Failure', 'error']","['Error', 'Failure', 'error']"
Availability,"Reopening this ticket, as others have encountered this error recently as well",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6179#issuecomment-1050074853:55,error,error,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6179#issuecomment-1050074853,1,['error'],['error']
Availability,Repair the CRAM detector/diagnostics test output to reflect the CRAM file name change that was introduced by updating the large CRAM files to v3.0.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8971:0,Repair,Repair,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8971,1,['Repair'],['Repair']
Availability,"Repeated runs of the mentioned tool results in different number of kmers and templates discovered, shown as below, which sometimes result in the number of variants discovered downstream to be slightly varying as well (so fart only the number of `DEL`s are observed to be affected). ```; 01:30:55.991 INFO FindBreakpointEvidenceSpark - Discovered 32911079 kmers.; 01:35:37.667 INFO FindBreakpointEvidenceSpark - Discovered 30639344 unique template names for assembly.; 02:03:37.992 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4910 variants.; 02:03:38.004 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - INV: 204; 02:03:38.004 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - DEL: 2777; 02:03:38.004 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - DUP: 954; 02:03:38.004 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - INS: 975. 02:09:24.685 INFO FindBreakpointEvidenceSpark - Discovered 32911147 kmers.; 02:14:21.462 INFO FindBreakpointEvidenceSpark - Discovered 30637728 unique template names for assembly.; 02:44:21.009 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4906 variants.; 02:44:21.023 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - INV: 204; 02:44:21.023 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - DEL: 2773; 02:44:21.023 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - DUP: 954; 02:44:21.023 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - INS: 975. 03:54:34.062 INFO FindBreakpointEvidenceSpark - Discovered 32911130 kmers.; 03:59:31.807 INFO FindBreakpointEvidenceSpark - Discovered 30637959 unique template names for assembly.; 04:31:05.403 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4907 variants.; 04:31:05.416 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - INV: 204; 04:31:05.416 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - DEL: 2774; 04:31:05.416 INFO DiscoverStructuralVa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2600:175,down,downstream,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2600,1,['down'],['downstream']
Availability,"Repeating the GenotypeGVCFs run on exactly the same input data results in success, so it appears to be a nondeterministic error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4518#issuecomment-372014808:122,error,error,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4518#issuecomment-372014808,1,['error'],['error']
Availability,"Replacement post/tutorial/etc. is not available just yet. In any case, it seems idiosyncratic to have such a link only for the CNV somatic workflow...I think it might've just been carried over from the gatk-protected README at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3992#issuecomment-352478160:38,avail,available,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3992#issuecomment-352478160,1,['avail'],['available']
Availability,"Report; ### Affected tool(s) or class(es); HaplotypeCaller --max-reads-per-alignment-start. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [date of test?]. ### Description; We used GATK4 to detect a fairly large duplication (60bp) in a control sample. We did sequenced two replicates for this sample, one having significantly more coverage than the other.With default GATK4 parameter the duplication was only detected in the sample with the lowest coverage. After inspection of GATK4 parameter we found that it was the downsampling throught the --max-reads-per-alignment-start that was in cause.Indeed, all the reads that contains the duplications are softcliped (see IGV capture below) because the insertion/duplication event is too bigged to be correctly aligned by BWA. This causes all reads containing the duplication to have the same start position in the BAM file. Then, the downsampling based on start position must drastically reduce the signal and the variant is skipped. This explains why the variant was missed at high coverage level and not in the replicates with lower signal.We think that the downsampling should take Softclips into account to be more reliable, but maybe you have a better idea.Also we did some performance evaluation and GATK4 runned faster with the downsampling desactivated. Is it normal ?; ![duplication](https://user-images.githubusercontent.com/53903734/62783152-17f41180-babc-11e9-9ddb-bed3c3042d97.png). #### Steps to reproduce; Run GATK4 with default parameters on the BAM containing the duplication (we can provide a toy). Disable --max-reads-per-alignment-start by switching the value to 0 to enable the identification of the duplication. #### Expected behavior; The duplication should have been found because the downsampling on start position does not take into accout the reads softclips. #### Actual behavior; The duplication is missed at high coverage depth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6088:577,down,downsampling,577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6088,6,"['down', 'reliab']","['downsampling', 'reliable']"
Availability,Reporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:112); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:106); 22:05:55.973 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); 22:05:55.976 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:6258,ERROR,ERROR,6258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"Reposting from the Slack channel:. When I run ValidateVariants on an *invalid* VCF without providing a reference or any ""--validation-type-to-exclude"" arguments, I don't get any validation errors. However, if I add ""--validation-type-to-exclude REF"", then I get validation errors as expected. Even when I get validation errors in the second case, the error message seems to terminate abruptly: `A USER ERROR has occurred: Input output.vcf fails strict validation: the Allele Count (AC) tag is incorrect for the record at position 1:1262288, 2 vs. 1 of type:`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4642:189,error,errors,189,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4642,5,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,"Request created from: ""Did not inflate expected amount"" Error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582:56,Error,Error,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582,1,['Error'],['Error']
Availability,Request: fine-grained configuration for codec packages for downstream projects,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4180:59,down,downstream,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4180,1,['down'],['downstream']
Availability,"Requested on the forum from a researcher.; ___; Hi GATK team,. I just wanted to let you know, that there is a minor mistake in the documentation here:; https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/org_broadinstitute_hellbender_tools_walkers_haplotypecaller_HaplotypeCaller.php#--pcr-indel-model. The file in which you want to replace --genotyping_mode with --genotyping-mode is this one:; gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/StandardCallerArgumentCollection.java. I don't seem to be able to create a pull request, so I'll leave it to you. Thanks,; Tommy. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23442/minor-error-in-the-documentation-regarding-genotyping-mode/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5657:741,error,error-in-the-documentation-regarding-genotyping-mode,741,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5657,1,['error'],['error-in-the-documentation-regarding-genotyping-mode']
Availability,"Researcher reports error in GenotypeGVCFs that uses a GenomicsDB database using v4.0.5.0. Removing `new qual` param OR using v4.0.4.0 allows the command to run without error. I saw a similar error with v4.0.5.1 when I tried to add `new qual` to a workshop hands-on tutorial GenotypeGVCFs step using a GenomicsDB database. ---; Hi,. I am trying to process locally 260 WES gvcf through joint discovery wdl pipeline. I encountered an error at GenotypeGVCFs below which I am not sure how to proceed. I have used all the default reference libraries and only modified the merge_count in the script to be 8144 so that my server resources won't be maxout fully in the ImportGVCFs step. . [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl ""https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl""). ```; 23:17:43.992 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:44.064 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:46.334 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),30.197597194999727,Cpu time(s),28.791204838999864; [June 25, 2018 11:17:46 PM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 4.01 minutes.; Runtime.totalMemory()=5354029056; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 2.559797571100845E-21,NaN; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotype",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4975:19,error,error,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975,4,['error'],['error']
Availability,"Researcher reports that GATK4 VariantAnnotator errors with a PED file that works fine with CalculateGenotypePosteriors. The same VariantAnnotator command and PED file work fine in GATK3. ---; Hi @shlee ,. We have ped files. And it worked fine for CalculateGenotypePosteriors too. But not with VariantAnnotator. So we downraded to GATK3 with same files on same parameters in order to finish that part - we hope calculations are same. Thank You!; Sergey. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/50327#Comment_50327",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4987:47,error,errors,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987,2,"['down', 'error']","['downraded', 'errors']"
Availability,"Resolves https://github.com/broadinstitute/dsp-spec-ops/issues/239. See README.md in this PR for full details. ----; To make this easier to review, the changes break down into a few sections. 1. Docs -- the README.md. Does it make sense? Could you follow it?. 2. Comparison Script (compare_data.py)-- is it clear? Obvs any bugs would be great. The Github Issue for this PR describes _what_ is compared. 3. WDL changes -- should be straightforward to review, just minor changes; ; 4. Code changes (java) -- we can walk through this together if that's more effective",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7106:166,down,down,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7106,1,['down'],['down']
Availability,"Results from a Travis run using a branch that throws whenever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5895:457,Failure,Failures,457,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895,1,['Failure'],['Failures']
Availability,"Results of merging 3 files with ~10,000,000 features each from a GCS bucket using my home linux box:; CPB 1 is not quite twice as fast as CPB 0, so you were right. CPB > 1 just slows things down. (Earlier comments erased, because they were stupid: I forgot that I hadn't hooked up prefetch.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1358234897:190,down,down,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1358234897,1,['down'],['down']
Availability,"Return NA if VariantsToTable is printing a null object, replace --allowMissingData with --errorIfMissingData",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3252:90,error,errorIfMissingData,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3252,1,['error'],['errorIfMissingData']
Availability,"Revamping the existing somatic validation pipeline needs to be done before development of the TH prototype can continue. - [ ] Identify test bed of TCGA samples from various tumor types. We can mix tumor-normal samples (as I've done at the counts/allelic-counts level in preliminary evaluations of the TH prototype) to expand the effective number of samples.; - [ ] Determine minimal version of current CGA ABSOLUTE pipeline (to be used as a baseline for comparison).; - [ ] Generate and manually curate ABSOLUTE results and narrow samples down to those with relatively robust solutions.; - [ ] Construct ModelSegments/M2 -> ABSOLUTE pipeline (will at least require minor development/tuning of ModelSegments output -> ABSOLUTE input conversion script, may also require germline tagging, see related #5804) and evaluate.; - [ ] Construct ModelSegments/M2 -> TH pipeline and evaluate.; - [ ] Remove unsupported code/tools. See https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199 for a summary. We should make sure that any users that would be affected by this are notified and prepare accordingly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-526272699:540,down,down,540,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-526272699,2,"['down', 'robust']","['down', 'robust']"
Availability,Revert exposure of optional parameters in CNV WDLs when Cromwell support is available.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4287:76,avail,available,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4287,1,['avail'],['available']
Availability,"Reverting the update to gkl 0.3.1 for now because we've encountered some downstream errors that are making it impossible to update gatk-protected. This reverts commit 9e3c6e3d7370c503d2a57be0c662fb1016d8b764, reversing; changes made to 767974906e91c90079cefa4512b463138ca09f68.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2319:73,down,downstream,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2319,2,"['down', 'error']","['downstream', 'errors']"
Availability,"Reverts VS-569 now that Temurin downloads are working again, leaves in the Corretto breadcrumbs and minor improvements like `-o xtrace`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7972:32,down,downloads,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7972,1,['down'],['downloads']
Availability,"Reverts the reversion in #5225, this time addressing the lexicographical ordering issue in #5217 at the WDL level by simply renaming gCNV output at the command line. If desired, we can eventually change gCNV itself to output filenames that are robust against lexicographic ordering, but this is low priority in my opinion. @vruano this is what we discussed last week. Tests pass on Travis, and I'm pretty sure this fix should work OK, but I have not done an actual run with enough samples to see the fix in action. Can I assign you to review once I get a chance to do this?. EDIT: Also went ahead and rolled an older PR #5304 into this one so I can test both at the same time. Closes #4724.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5490:244,robust,robust,244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5490,1,['robust'],['robust']
Availability,"Reviving this. This will essentially be a major refactor/rewrite of CreatePanelOfNormals to make it scalable enough to handle WGS. - [x] CombineReadCounts is too cumbersome for large matrices. Change CreatePanelOfNormals to take in multiple -I instead.; - [x] Rename NormalizeSomaticReadCounts to DenoiseReadCounts and require integer read counts as input. These will still be backed by a ReadCountCollection until @asmirnov239's changes are in.; - [x] Remove optional outputs (factor-normalized and beta-hats) from DenoiseReadCounts. For now, TN and PTN output will remain in the same format (log2) to maintain compatibility with downstream tools.; - [x] Maximum number of eigensamples K to retain in the PoN is specified; the smaller of this or the number of samples remaining after filtering is used. The number actually used to denoise can be specified in DenoiseReadCounts. If we are going to spend energy computing K eigensamples, there is no reason we shouldn't expose all of them in the PoN, even if we don't want to use all of them for denoising. (Also, the current SVD utility methods do not allow for specification of K < N when performing SVD on an MxN matrix, even though the backend implementations that are called do allow for this; this is terrible. In any case, randomized SVD should be much faster than the currently available implementations, even when K = N).; - [x] Rename CreatePanelOfNormals to CreateReadCountPanelOfNormals; - [x] Refer to ""targets"" as intervals. See #3246.; - [x] Remove QC.; - [x] Refer to proportional coverage as fractional coverage.; - [x] Perform optional GC-bias correction internally if annotated intervals are passed as input.; - [x] Make standardization process for panel and case samples identical. Currently, a sample mean is taken at one point in the PoN standardization process, while a sample median is taken in the case standardization process.; - [x] HDF5 PoN will store version number, all integer read counts, all/panel intervals, all/panel ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687:631,down,downstream,631,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687,1,['down'],['downstream']
Availability,"Right now it is not available (even after #3457).; Though the VCF spec doesn't seem to specify if it is mandated, it makes it easier for parsing and making sense of the event. Note that this is different from `PARID`, which, unlike BND that symbols novel adjacency, symbols ; ""novel"" disruptions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3508:20,avail,available,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3508,1,['avail'],['available']
Availability,"Right now the alignments are filtered in various places in SV discovery stage. ; Having a single logic unit for doing this makes; * debugging and modular development, ; * complex SV discovery/interpretation, and ; * future improvements (e.g. not filtering but downgrading certain alignments and use an optimization approach). easier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3221:260,down,downgrading,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3221,1,['down'],['downgrading']
Availability,Right now we get the following error message when we don't supply sufficient memory:. ```; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 137; ```. Ideally we would report out of memory message explicitly. Perhaps we can just catch that exception and output message that this error is likely due to insufficient memory. @cmnbroad Do you have any thoughts on this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6362:31,error,error,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6362,2,['error'],['error']
Availability,"Right now we publish test utils as part of the gatk artifact. Since these are part of our main compilation unit it means we have several test libraries as compile dependencies instead of as test compile dependencies. . If we separate our test utils into a separate group we can avoid having downstream tools gain various test dependencies if they don't want them. (i.e. TestNG, MiniDFSCluster).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1481:291,down,downstream,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1481,1,['down'],['downstream']
Availability,Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvVmFyaWFudFJlY2FsaWJyYXRvckludGVncmF0aW9uVGVzdC5qYXZh) | `98.315% <0.000%> (-0.481%)` | :arrow_down: |; | [...s/copynumber/models/AlleleFractionLikelihoods.java](https://codecov.io/gh/broadinstitute/gatk/pull/7918/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL21vZGVscy9BbGxlbGVGcmFjdGlvbkxpa2VsaWhvb2RzLmphdmE=) | `100.000% <0.000%> (ø)` | |; | [...s/solver/SynchronizedUnivariateSolverUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7918/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zb2x2ZXIvU3luY2hyb25pemVkVW5pdmFyaWF0ZVNvbHZlclVuaXRUZXN0LmphdmE=) | | |; | [...bender/utils/solver/RobustBrentSolverUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7918/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zb2x2ZXIvUm9idXN0QnJlbnRTb2x2ZXJVbml0VGVzdC5qYXZh) | | |; | [...ute/hellbender/utils/solver/RobustBrentSolver.java](https://codecov.io/gh/broadinstitute/gatk/pull/7918/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zb2x2ZXIvUm9idXN0QnJlbnRTb2x2ZXIuamF2YQ==) | | |; | [...r/utils/solver/UnivariateSolverSpecifications.java](https://codecov.io/gh/broadinstitute/gatk/pull/7918/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91d,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7918#issuecomment-1168977988:3445,Robust,RobustBrentSolverUnitTest,3445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7918#issuecomment-1168977988,1,['Robust'],['RobustBrentSolverUnitTest']
Availability,"Running GATK4 BwaSpark encounter the following fatal error message:; `[M::mem_sam_pe] Paired reads have different names: ""206B4ABXX100825:7:66:2632:21260"", ""206B4ABXX100825:7:66:2632:31752""`. Script: ; `$GATK_LAUNCH BwaSpark -I $unsorted_bam_hdfs -O $sorted_bam_hdfs -t 10 --disableSequenceDictionaryValidation true -R $ref_hdfs -K 10000000 -- --sparkRunner SPARK --sparkMaster yarn --num-executors 1 --executor-cores 10 --executor-memory 40g`. $unsorted_bam_hdfs is a file generated by FastqToBam, and copied to HDFS. ; spark 2.0 is used. . The original Fastq files are perfectly fine, and we have been using it for all our tests using previous versions, including 3.6. I also manually checked the generated name-sorted BAM file generated by FastToBam, and the neighboring lines are perfectly paired as well. . What I suspect is that chunk is cut inside a pair, and thus not just this one, all subsequent lines are all error'ed out. To confirm this, I ran the job with different -K and -bps options, and the error will occur at different locations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2296:53,error,error,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2296,3,['error'],['error']
Availability,Running `gatk --version` produces a dump of all the available tools and then a user exception.; ```. ... UpdateVcfSequenceDictionary (Picard) Takes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.; VariantAnnotator (BETA Tool) Tool for adding annotations to VCF files; VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. ***********************************************************************. A USER ERROR has occurred: '--version' is not a valid command. ***********************************************************************; ```; It should print the version instead.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5533:52,avail,available,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5533,2,"['ERROR', 'avail']","['ERROR', 'available']"
Availability,"Running `gatk IndexFeatureFile` with `--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true'` reveals that the underlying error is:. ```; htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 483650: unparsable vcf record with allele M; ```. Can you check whether the VCF is malformed at this location, as the error claims?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4184#issuecomment-358350049:125,error,error,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4184#issuecomment-358350049,2,['error'],['error']
Availability,"Running a spark tool with a read input that doesn't exist on hdfs results in a confusing error message. ex: `hdfs://user/local/print_reads.sorted.bam` doesn't exist on the file system ; produces . ```; java.lang.IllegalArgumentException: Wrong FS: hdfs://user/local/print_reads.sorted.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; ```. Full command line to reproduce:. ```; spark-submit --master yarn-client --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.executor.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 build/libs/gatk-all-4.pre-alpha-196-g94b53ee-SNAPSHOT-spark.jar PrintReadsSpark --sparkMaster yarn-client -I hdfs://user/local/print_reads.sorted.bam -O output.bam; ```. ```; java.lang.IllegalArgumentException: Wrong FS: hdfs://user/local/print_reads.sorted.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:474); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:163); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:281); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:261); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:252); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1257:89,error,error,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1257,1,['error'],['error']
Availability,Running in local mode on a large GCE instance when using external evidence (it looks like any attempt to use external evidence will trigger this) results in the following error:. ```; org.broadinstitute.hellbender.exceptions.GATKException: Partition boundaries are not coordinate sorted.; 	at org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.readExternalEvidence(FindBreakpointEvidenceSpark.java:309); 	at org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.getMappedQNamesSet(FindBreakpointEvidenceSpark.java:208); 	at org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.gatherEvidenceAndWriteContigSamFile(FindBreakpointEvidenceSpark.java:111); 	at org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark.runTool(StructuralVariationDiscoveryPipelineSpark.java:79); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:362); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); 	at org.broadinstitute.hellbender.Main.main(Main.java:239); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3717:171,error,error,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3717,1,['error'],['error']
Availability,"Running into a similar issue when launching [cnv_somatic_pair_workflow 1.3](https://dockstore.org/workflows/github.com/gatk-workflows/gatk4-somatic-cnvs/cnv_somatic_pair_workflow:1.3.0?tab=info) with [gatk docker 4.1](us.gcr.io/broad-gatk/gatk:4.1.0.0) on Terra. Any suggestions?. ```; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 22:50:56.367 INFO DenoiseReadCounts - Reading read-counts file (/cromwell_root/fc-88ae2d4c-8e67-478a-893c-65db97d40c80/9a75a2c6-0c0b-4903-80e6-0082435645c1/CNVSomaticPairWorkflow/ca25a6aa-92b6-4a41-b41a-740cf40b2143/call-CollectCountsTumor/S027-1A-deduped.counts.hdf5)...; ```; Also failing to delocalize files at the DenoiseReadCounts step:. ```; Task CNVSomaticPairWorkflow.DenoiseReadCountsTumor:NA:1 failed. Job exit code 1. ; Check gs://fc-88ae2d4c-8e67-478a-893c-65db97d40c80/65a36270-4934-4791-a57f-2c70c2f42c0e/CNVSomaticPairWorkflow/9c8d57c3-abf7-4805-b612-04d61e8c7727/call-DenoiseReadCountsTumor/stderr for more information. PAPI error code 5. ; 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/S027-1A-deduped.denoisedCR.tsv -> gs://fc-88ae2d4c-8e67-478a-893c-65db97d40c80/65a36270-4934-4791-a57f-2c70c2f42c0e/CNVSomaticPairWorkflow/9c8d57c3-abf7-4805-b612-04d61e8c7727/call-DenoiseReadCountsTumor/S027-1A-deduped.denoisedCR.tsv (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/S027-1A-deduped.denoisedCR.tsv gs://fc-88ae2d4c-8e67-478a-893c-65db97d40c80/65a36270-4934-4791-a57f-2c70c2f42c0e/CNVSomaticPairWorkflow/9c8d57c3-abf7-4805-b612-04d61e8c7727/call-DenoiseReadCountsTumor/S027-1A-deduped.denoisedCR.tsv, command failed: CommandException: No URLs matched: /mnt/local-disk/S027-1A-deduped.denoisedCR.tsv\nCommandException: 1 file/object could not be transferred.\n); ; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3763#issuecomment-491391261:1159,error,error,1159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3763#issuecomment-491391261,1,['error'],['error']
Availability,"Running on the hg19/b37 NA12878 bam file, I'm getting the following exception in stage 0:. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 589 in stage 0.0 failed 4 times, most recent failure: Lost task 589.3 in stage 0.0 (TID 757, cwhelan-na12878-pcr--30x-bam-w-6.c.broad-dsde-methods.internal): java.lang.IllegalArgumentException: observedValue must be non-negative; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); at org.broadinstitute.hellbender.tools.spark.utils.IntHistogram.addObservation(IntHistogram.java:50); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$LibraryRawStatistics.addRead(ReadMetadata.java:367); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$PartitionStatistics.<init>(ReadMetadata.java:431); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata.lambda$new$1dcab782$1(ReadMetadata.java:57); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGSchedul",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:154,failure,failure,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,2,['failure'],['failure']
Availability,"Running something like this on the outputs of the score tool from a validation shard could help determine the threshold. (This was done hastily, so excuse any errors!). Below I show the results of running with a test VCF of 50 1000G WES samples. Note that ""training"" and ""truth"" are used in the VQSR meanings, and that we derive positive labels for the validation set from their union; but one could imagine variations on this. ```; import h5py; import numpy as np; import matplotlib.pyplot as plt. annot_file = 'test.all-unlabeled.annot.hdf5'; scores_file = 'test.all-unlabeled.scores.hdf5'. with h5py.File(annot_file, 'r') as f:; is_training_n = f['/labels/training'][()].astype(bool); is_truth_n = f['/labels/truth'][()].astype(bool); ; with h5py.File(scores_file, 'r') as f:; score_n = f['/data/scores'][()]. score_sort_order_n = score_n.argsort(); sorted_score_n = score_n[score_sort_order_n]; is_positive_n = is_training_n | is_truth_n. p_n = is_positive_n[score_sort_order_n]; m_n = ~p_n. tp_n = np.cumsum(p_n[::-1])[::-1]; fn_n = np.cumsum(p_n); mp_n = np.cumsum(m_n[::-1])[::-1]. recall_n = tp_n / (tp_n + fn_n); LL_score_n = recall_n**2 / (mp_n / sum(m_n)). argmax_idx = np.argmax(LL_score_n); LL_score_max = LL_score_n[argmax_idx]; LL_score_argmax = sorted_score_n[argmax_idx]; recall_at_LL_score_argmax = recall_n[argmax_idx]. plt.plot(sorted_score_n, LL_score_n, label=f'LL score, max = {LL_score_max:.2f}, argmax = {LL_score_argmax:.2f}'); plt.plot(sorted_score_n, recall_n, label=f'recall, at LL score argmax = {recall_at_LL_score_argmax:.2f}'); plt.axvline(LL_score_argmax, c='grey'); plt.xlabel('score'); plt.legend(); plt.show(); ```. ![image](https://user-images.githubusercontent.com/11076296/158000937-79dcfc26-45c6-400f-9101-37a96087492e.png). Would be interesting to also plot e.g. F1 vs. score, where F1 is determined on a orthogonal set of positive/negative gold-standard labels (e.g., GIAB), to see how close the LL score determined on the ""training/truth"" labels gets us.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065794594:159,error,errors,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065794594,1,['error'],['errors']
Availability,"Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx25G -Xms25G -Djava.io.tmpdir=/Data/data/raja/cfdna/gatk-4.4.0.0/tmp -jar /Data/data/raja/cfdna/nextflow1/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar MarkDuplicatesSpark -I sout_hd.bam -O sout_hd.ctrl.bam.gz; Error: LinkageError occurred while loading main class org.broadinstitute.hellbender.Main; java.lang.UnsupportedClassVersionError: org/broadinstitute/hellbender/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0; ![Screenshot_2](https://github.com/broadinstitute/gatk/assets/75623749/cdaca619-fc57-4f6b-b350-0402036c099c)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8596:386,Error,Error,386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8596,1,['Error'],['Error']
Availability,S : true; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:37.247 INFO Mutect2 - Deflater: IntelDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializing engine; 15:47:42.352 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 15:47:42.423 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 15:47:42.482 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 15:47:42.483 INFO IntelPairHmm - Available threads: 8; 15:47:42.483 INFO IntelPairHmm - Requested threads: 4; 15:47:42.483 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 15:47:42.936 INFO ProgressMeter - Starting traversal; 15:47:42.936 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 15:47:53.565 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:19555 0.2 90 508.0; 15:48:05.962 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:136820 0.4 600 1563.5; 15:48:16.023 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:360783 0.6 1560 2828.9; 15:48:19.342 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.010346494000000001; 15:48:19.342 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 6.453042841; 15:48:19.347 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 10.39 sec; 15:48:19.348 INFO Mutect2 - Shutting down engine; [28 novembre 2019 15:48:19 CET] org.broadinstitute.hellbender.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:2595,Avail,Available,2595,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['Avail'],['Available']
Availability,S9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9odHNnZXRyZWFkZXIvSHRzZ2V0Rm9ybWF0LmphdmE=) | `0.000% <0.000%> (-100.000%)` | :arrow_down: |; | [...ender/utils/svd/ApacheSingularValueDecomposer.java](https://codecov.io/gh/broadinstitute/gatk/pull/7785/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zdmQvQXBhY2hlU2luZ3VsYXJWYWx1ZURlY29tcG9zZXIuamF2YQ==) | `0.000% <0.000%> (-100.000%)` | :arrow_down: |; | [...ender/utils/svd/OjAlgoSingularValueDecomposer.java](https://codecov.io/gh/broadinstitute/gatk/pull/7785/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zdmQvT2pBbGdvU2luZ3VsYXJWYWx1ZURlY29tcG9zZXIuamF2YQ==) | `0.000% <0.000%> (-100.000%)` | :arrow_down: |; | [...nder/utils/downsampling/FractionalDownsampler.java](https://codecov.io/gh/broadinstitute/gatk/pull/7785/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9kb3duc2FtcGxpbmcvRnJhY3Rpb25hbERvd25zYW1wbGVyLmphdmE=) | `0.000% <0.000%> (-100.000%)` | :arrow_down: |; | [...der/cmdline/programgroups/ExampleProgramGroup.java](https://codecov.io/gh/broadinstitute/gatk/pull/7785/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL3Byb2dyYW1ncm91cHMvRXhhbXBsZVByb2dyYW1Hcm91cC5qYXZh) | `0.000% <0.000%> (-100.000%)` | :arrow_down: |; | ... and [1349 more](https://codecov.io/gh/broadinstitute/gatk/pull/7785/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinsti,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7785#issuecomment-1099620560:4655,down,downsampling,4655,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7785#issuecomment-1099620560,1,['down'],['downsampling']
Availability,S9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9odHNnZXRyZWFkZXIvSHRzZ2V0Rm9ybWF0LmphdmE=) | `0.000% <0.000%> (-100.000%)` | :arrow_down: |; | [...ender/utils/svd/ApacheSingularValueDecomposer.java](https://codecov.io/gh/broadinstitute/gatk/pull/7947/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zdmQvQXBhY2hlU2luZ3VsYXJWYWx1ZURlY29tcG9zZXIuamF2YQ==) | `0.000% <0.000%> (-100.000%)` | :arrow_down: |; | [...ender/utils/svd/OjAlgoSingularValueDecomposer.java](https://codecov.io/gh/broadinstitute/gatk/pull/7947/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zdmQvT2pBbGdvU2luZ3VsYXJWYWx1ZURlY29tcG9zZXIuamF2YQ==) | `0.000% <0.000%> (-100.000%)` | :arrow_down: |; | [...nder/utils/downsampling/FractionalDownsampler.java](https://codecov.io/gh/broadinstitute/gatk/pull/7947/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9kb3duc2FtcGxpbmcvRnJhY3Rpb25hbERvd25zYW1wbGVyLmphdmE=) | `0.000% <0.000%> (-100.000%)` | :arrow_down: |; | [...der/cmdline/programgroups/ExampleProgramGroup.java](https://codecov.io/gh/broadinstitute/gatk/pull/7947/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL3Byb2dyYW1ncm91cHMvRXhhbXBsZVByb2dyYW1Hcm91cC5qYXZh) | `0.000% <0.000%> (-100.000%)` | :arrow_down: |; | ... and [1144 more](https://codecov.io/gh/broadinstitute/gatk/pull/7947/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinsti,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7947#issuecomment-1185962775:4779,down,downsampling,4779,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7947#issuecomment-1185962775,1,['down'],['downsampling']
Availability,SCII; 2022-08-16T22:45:53.6965863Z * Returns the STR period at a given position.??; 2022-08-16T22:45:53.6972650Z src/main/java/org/broadinstitute/hellbender/utils/dragstr/DragstrReferenceAnalyzer.java:87: error: unmappable character for encoding ASCII; 2022-08-16T22:45:53.6973221Z * Returns the STR period at a given position.??; 2022-08-16T22:45:53.6981573Z src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/GenotypingEngine.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7254348Z src/main/java/org/broadinstitute/hellbender/utils/genotyper/AlleleLikelihoods.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7492903Z src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/InfoFieldAnnotation.java:3: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.7498018Z src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/VariantAnnotation.java:3: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.7502319Z src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/GenotypeAnnotation.java:3: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.7512488Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7514136Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:4: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.7515610Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:5: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.7517156Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:6: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.7642312Z src/main/java/org/broad,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:5810,error,error,5810,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,1,['error'],['error']
Availability,SIGABRT/error running IntelInflaterDeflaterIntegrationTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2535:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2535,1,['error'],['error']
Availability,"SJDK Defaults.COMPRESSION_LEVEL : 2; 09:36:35.391 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:36:35.392 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:36:35.392 INFO IndexFeatureFile - Deflater: IntelDeflater; 09:36:35.392 INFO IndexFeatureFile - Inflater: IntelInflater; 09:36:35.392 INFO IndexFeatureFile - GCS max retries/reopens: 20; 09:36:35.392 INFO IndexFeatureFile - Requester pays: disabled; 09:36:35.393 INFO IndexFeatureFile - Initializing engine; 09:36:35.393 INFO IndexFeatureFile - Done initializing engine; 09:36:35.502 INFO FeatureManager - Using codec VCFCodec to read file file:///mnt/user1/snp_allsamples.vcf.gz; 09:36:35.518 INFO ProgressMeter - Starting traversal; 09:36:35.518 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 09:36:36.979 INFO IndexFeatureFile - Shutting down engine; [March 21, 2024 at 9:36:36 a.m. CST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=1241513984; java.lang.ArrayIndexOutOfBoundsException: Index 37451 out of bounds for length 37451; at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:102); at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:92); at htsjdk.tribble.index.IndexFactory.createIndex(IndexFactory.java:529); at htsjdk.tribble.index.IndexFactory.createTabixIndex(IndexFactory.java:476); at htsjdk.tribble.index.IndexFactory.createTabixIndex(IndexFactory.java:502); at htsjdk.tribble.index.IndexFactory.createIndex(IndexFactory.java:403); at org.broadinstitute.hellbender.tools.IndexFeatureFile.createAppropriateIndexInMemory(IndexFeatureFile.java:109); at org.broadinstitute.hellbender.tools.IndexFeatureFile.doWork(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8747:2653,down,down,2653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8747,1,['down'],['down']
Availability,"SJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:4787,ERROR,ERROR,4787,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['ERROR'],['ERROR']
Availability,"ST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-BenchmarkVCFControlSample/Benchmark/8c516721-e955-41d1-907e-fcee92f592d3/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""100.56416111111112"",; ""NIST evalHCsystemhours"": ""0.19999166666666665"",; ""NIST evalHCwallclockhours"": ""74.00048055555555"",; ""NIST evalHCwallclockmax"": ""4.007605555555555"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-BenchmarkVCFTestSample/Benchmark/427c5010-a177-42d8-81be-5a387beed653/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202:22105,error,errors,22105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202,1,['error'],['errors']
Availability,"SV pipeline failure on CHM WGS1 with ""two input alignments' overlap on read consumes completely one of them.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:12,failure,failure,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['failure'],['failure']
Availability,"SV tools confirmed running. With the following new error messages for all Spark tools that I've run. ```; 02:03:27.962 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.BUFFER_SIZE : 131072; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.COMPRESSION_LEVEL : 1; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.CREATE_INDEX : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.CREATE_MD5 : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.CUSTOM_READER_FACTORY :; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.REFERENCE_FASTA : null; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Deflater IntelDeflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Inflater IntelInflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Initializing engine; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363:51,error,error,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363,1,['error'],['error']
Availability,"SYNC_IO_READ_FOR_SAMTOOLS : false; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:58:26.914 INFO FilterMutectCalls - Deflater: IntelDeflater; 22:58:26.914 INFO FilterMutectCalls - Inflater: IntelInflater; 22:58:26.914 INFO FilterMutectCalls - GCS max retries/reopens: 20; 22:58:26.914 INFO FilterMutectCalls - Requester pays: disabled; 22:58:26.914 INFO FilterMutectCalls - Initializing engine; 22:58:27.401 INFO FeatureManager - Using codec VCFCodec to read file file://tumor-vs-normal.mutect.temp1.vcf; 22:58:27.518 INFO FilterMutectCalls - Done initializing engine; 22:58:27.570 INFO ProgressMeter - Starting traversal; 22:58:27.571 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 22:58:27.571 INFO FilterMutectCalls - Starting first pass through the variants; 22:58:28.484 INFO FilterMutectCalls - Shutting down engine; [January 6, 2019 10:58:28 PM SGT] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=2141192192; java.lang.IllegalArgumentException: errorRateLog10 must be good probability but got NaN; 	at org.broadinstitute.hellbender.utils.QualityUtils.phredScaleLog10ErrorRate(QualityUtils.java:321); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.lambda$applyGermlineVariantFilter$10(Mutect2FilteringEngine.java:207); 	at java.util.stream.DoublePipeline$3$1.accept(DoublePipeline.java:231); 	at java.util.Spliterators$DoubleArraySpliterator.forEachRemaining(Spliterators.java:1198); 	at java.util.Spliterator$OfDouble.forEachRemaining(Spliterator.java:822); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); 	at java.u",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5553#issuecomment-451749085:2732,down,down,2732,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553#issuecomment-451749085,1,['down'],['down']
Availability,"SYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:55:31.186 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:55:31.186 INFO CalibrateDragstrModel - Deflater: IntelDeflater; 13:55:31.186 INFO CalibrateDragstrModel - Inflater: IntelInflater; 13:55:31.186 INFO CalibrateDragstrModel - GCS max retries/reopens: 20; 13:55:31.186 INFO CalibrateDragstrModel - Requester pays: disabled; 13:55:31.187 WARN CalibrateDragstrModel -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CalibrateDragstrModel is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 13:55:31.187 INFO CalibrateDragstrModel - Initializing engine; 13:55:33.395 INFO CalibrateDragstrModel - Done initializing engine; 13:55:33.396 INFO ProgressMeter - Starting traversal; 13:55:33.396 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 13:55:42.364 INFO CalibrateDragstrModel - Shutting down engine; [April 4, 2021 1:55:42 PM EDT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 0.19 minutes.; Runtime.totalMemory()=2384986112; java.lang.IllegalArgumentException: Start cannot exceed end.; at htsjdk.samtools.util.IntervalTree.put(IntervalTree.java:74); at htsjdk.samtools.util.IntervalTree.merge(IntervalTree.java:137); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$ShardReadBuffer.add(CalibrateDragstrModel.java:949); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$1.tryAdvance(CalibrateDragstrModel.java:798); at java.util.Spliterator.forEachRemaining(Spliterator.java:326); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182:14611,down,down,14611,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182,1,['down'],['down']
Availability,"SYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:42:34.118 INFO VariantFiltration - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:42:34.118 INFO VariantFiltration - Deflater: IntelDeflater; 15:42:34.119 INFO VariantFiltration - Inflater: IntelInflater; 15:42:34.119 INFO VariantFiltration - GCS max retries/reopens: 20; 15:42:34.119 INFO VariantFiltration - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 15:42:34.119 INFO VariantFiltration - Initializing engine; 15:42:34.634 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/sherlock/dev/Bhatt_lab/crassphage/variants/6753_12-15-2015_first_pass_filtered.vcf; 15:42:34.663 INFO VariantFiltration - Done initializing engine; 15:42:34.750 INFO ProgressMeter - Starting traversal; 15:42:34.750 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 15:42:34.781 INFO VariantFiltration - Shutting down engine; [June 15, 2018 3:42:34 PM PDT] org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=342884352; java.lang.NumberFormatException: For input string: ""26.67""; 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); 	at java.lang.Long.parseLong(Long.java:589); 	at java.lang.Long.parseLong(Long.java:631); 	at org.apache.commons.jexl2.JexlArithmetic.toLong(JexlArithmetic.java:906); 	at org.apache.commons.jexl2.JexlArithmetic.compare(JexlArithmetic.java:718); 	at org.apache.commons.jexl2.JexlArithmetic.greaterThan(JexlArithmetic.java:790); 	at org.apache.commons.jexl2.Interpreter.visit(Interpreter.java:796); 	at org.apache.commons.jexl2.parser.ASTGTNode.jjtAccept(ASTGTNode.java:18); 	at org.apache.commons.jexl2.Interpreter.visit(Interpreter.java:449); 	at org.apache.commons.jexl2.parser.ASTAndNode.jjtAccept(ASTAndNode.java:18); 	at org.apache.commons.jexl2.Interpreter.visit(Interpreter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4921:3496,down,down,3496,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921,1,['down'],['down']
Availability,Same error here with gatk v 4.6.0.0. Please help!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8699#issuecomment-2403803538:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8699#issuecomment-2403803538,1,['error'],['error']
Availability,"Same error here, affecting mutect2 (gatk 4.1.7.0) in normal-tumor pair mode. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6516#issuecomment-637438949:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6516#issuecomment-637438949,1,['error'],['error']
Availability,"Same error here, any news? Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6516#issuecomment-626676218:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6516#issuecomment-626676218,1,['error'],['error']
Availability,"Same error in HaplotypeCaller 4.1.7.0:. 10:05:48.875 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 25.625500792; 10:05:48.875 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 11358.883564452; 10:05:48.876 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 3761.34 sec; 10:05:48.876 INFO HaplotypeCaller - Shutting down engine; [June 18, 2020 at 10:05:48 AM CEST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 385.96 minutes.; Runtime.totalMemory()=14143193088; java.lang.StringIndexOutOfBoundsException: String index out of range: -1; at java.base/java.lang.String.substring(String.java:1837); at org.broadinstitute.hellbender.tools.walkers.annotator.TandemRepeat.getNumTandemRepeatUnits(TandemRepeat.java:54); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyRegionTrimmer.trim(AssemblyRegionTrimmer.java:175); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:552); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:210); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292). Any news?. ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6516#issuecomment-645894021:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6516#issuecomment-645894021,2,"['down', 'error']","['down', 'error']"
Availability,"Same error in HaplotypeCaller 4.1.7.0:. 18:59:34.948 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 1.514351728; 18:59:34.948 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 887.7367702070001; 18:59:34.948 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 2117.34 sec; 18:59:34.948 INFO HaplotypeCaller - Shutting down engine; [May 25, 2020 6:59:34 PM CEST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 66.82 minutes.; Runtime.totalMemory()=69727158272; java.lang.StringIndexOutOfBoundsException: String index out of range: -1; 	at java.lang.String.substring(String.java:1927); 	at org.broadinstitute.hellbender.tools.walkers.annotator.TandemRepeat.getNumTandemRepeatUnits(TandemRepeat.java:54); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyRegionTrimmer.trim(AssemblyRegionTrimmer.java:175); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:552); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:210); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6516#issuecomment-633884711:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6516#issuecomment-633884711,2,"['down', 'error']","['down', 'error']"
Availability,"Same issue:. ```; error: external filter 'git-lfs filter-process' failed; fatal: src/test/resources/large/cnv/create-pon-control-target-coord-only.pcov: smudge filter lfs failed; warning: Clone succeeded, but checkout failed.; You can inspect what was checked out with 'git status'; and retry the checkout with 'git checkout -f HEAD'; The command ""eval git clone --depth=9999999 https://github.com/broadinstitute/gatk.git broadinstitute/gatk "" failed. Retrying, 2 of 3.; fatal: destination path 'broadinstitute/gatk' already exists and is not an empty directory.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-310768892:18,error,error,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-310768892,1,['error'],['error']
Availability,Sample code to show how to modify INFO field combine operation while querying GenomicsDB using the Protobuf API. #4541 . ping @ldgauthier @droazen @lbergelson @jamesemery,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4993:121,ping,ping,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4993,1,['ping'],['ping']
Availability,"Scheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). 11:00:53.977 INFO DAGScheduler - Job 1 failed: runJob at SparkHadoopWriter.scala:83, took 3.799268 s; 11:00:53.979 ERROR SparkHadoopWriter - Aborting job job_202408111100502620487673658411251_0021.; org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; at java.base/jdk.internal.util.ArraysSupport.hugeLength(ArraysSupport.java:649); at java.base/jdk.internal.util.ArraysSupport.newLength(ArraysSupport.java:642); at java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100); at java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:130); at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41); at java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862); at java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714); at org.apache.spark.util.Utils$$anon$2.write(Utils.scala:160); at com.esotericsoftware.kryo.io.Output.flush(Output.java:185); at com.esotericsoftware.kryo.io.Output.close(Output.java:196); at org.apache.sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:8648,failure,failure,8648,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['failure'],['failure']
Availability,"SchedulerImpl: Adding task set 2.0 with 2 tasks; 00:59 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, xx.xx.xx.25, executor 2, partition 0, PROCESS_LOCAL, 6010 bytes); 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:55:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.16:39037 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:55:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.25:41354 (size: 6.4 KB, free: 366.3 MB); **18/04/24 17:55:54 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 4, xx.xx.xx.16, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:24977,Error,Error,24977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Error'],['Error']
Availability,Scheduling that might be tricky on my end -- I'm working from home until next week then heading out to Europe & South Africa until the end of the month. I'll be onsite tomorrow morning (giving the MPG primer at 8:30) but I won't stick around very long. If you're available then we can definitely chat; otherwise I might recommend you get the conversation going with redteam and I'll provide air support over slack/email where I can.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334187438:263,avail,available,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334187438,1,['avail'],['available']
Availability,See [Issue 5277 - Migrate to org.genomicsdb fork](https://github.com/broadinstitute/gatk/issues/5277). . The first genomicsdb 1.0.0.beta jar consists of only a refactoring of all the packages to org.genomicsdb. Note that this pass should have no performance implications compared to the last [Intel release](https://mvnrepository.com/artifact/com.intel/genomicsdb/0.10.2-proto-3.0.0-beta-1+90dad1af8ce0e4d) as there is no change other than refactoring. Issues [5568-buffer resizing excessive logging](https://github.com/broadinstitute/gatk/issues/5568) and [5342-file synching error](https://github.com/broadinstitute/gatk/issues/5342) will both be addressed in the next release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5587:577,error,error,577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5587,1,['error'],['error']
Availability,"See below:. 1) Are the failures in the same set of nodes?. This job was split into 200 intervals and would have run across a lot of different nodes. One specific interval set is causing issues. That interval/job has failed 3 times, on different nodes with a similar error. 2) Have all the ""nodes"" in the cluster been updated to running gatk v4.1.8.0. they all use the exact same JAR when they execute, which is 4.1.8.0. . 3) Is it possible to attach a file named __array_schema.tdb from one of the arrays causing the segfault?. I renamed it '.txt' to keep github happy, but here is that file. It is from contig QNVO02001146.1, which is the last contig it logged progress from before the error: ; [__array_schema.tdb.txt](https://github.com/broadinstitute/gatk/files/5441668/__array_schema.tdb.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-716835521:23,failure,failures,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-716835521,3,"['error', 'failure']","['error', 'failures']"
Availability,See discussion at https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error#latest regarding output directory creation.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467450853:118,error,error,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467450853,1,['error'],['error']
Availability,See https://gatkforums.broadinstitute.org/gatk/discussion/23793/python-error-using-postprocessgermlinecnvcalls,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5852:71,error,error-using-postprocessgermlinecnvcalls,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5852,1,['error'],['error-using-postprocessgermlinecnvcalls']
Availability,"See https://github.com/broadinstitute/gatk/actions/runs/3567534442/jobs/5995349354 for one example. . We originally saw these failures on the branch to move GATK to Java 17, but recently have started seeing the same failures on current PRs when running the tests on Java 11. It looks like this started happening when the CI env recently started resolving to Java 11.0.16.1., where these tests appear to always (?) fail, whereas previously the CI env was resolving to Java 11.0.11+9, where they pass. Although I haven't compared the results for all of the failed cases, I think the failure modes and bad values are the same on both Java 11 and Java 17. We've temporarily pinned the CI environment to use 11.0.11+9 (see https://github.com/broadinstitute/gatk/pull/8102) until we can get this resolved. I'd suspect the easiest way to reproduce the failures is to try running the tests using either Java 17 or Java 11.0.16.1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8107:126,failure,failures,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8107,4,['failure'],"['failure', 'failures']"
Availability,"See https://github.com/broadinstitute/gatk/issues/4888, which is an older report for the same issue. As mentioned there, I think we should patch our fork of `google-cloud-java` to do a channel reopen on `UnknownHostException` for now as a quick fix. @jean-philippe-martin is eventually going to add an official configuration mechanism for clients of `google-cloud-java` to customize which errors should trigger a retry/reopen, which should provide a better way to deal with these errors as they crop up without having to modify the NIO library itself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412134420:389,error,errors,389,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412134420,2,['error'],['errors']
Availability,"See some issues---mostly stemming from the HDF5 library and the BLAS library optionally used by MLlib SVD at e.g. https://gatkforums.broadinstitute.org/gatk/discussion/23591/createreadcountpanelofnormals-in-gatk4-1-doesnt-output-valid-hdf5-files#latest; https://gatkforums.broadinstitute.org/gatk/discussion/12537/get-error-when-using-createreadcountpanelofnormals-in-calling-somatic-copy-number-variation; https://gatkforums.broadinstitute.org/gatk/discussion/11461/gatk-4-0-1-2-no-non-zero-singular-values-were-found-in-creating-a-panel-of-normals-for-somatic-cnv/p1. Would also be nice to to turn down the verbosity of Spark logging, which emits a ridiculous amount of messages for a simple SVD. I think this is a relatively ancient issue (https://github.com/broadinstitute/gatk/issues/1370), not sure if it's been resolved for other Spark tools since.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5771:318,error,error-when-using-createreadcountpanelofnormals-in-calling-somatic-copy-number-variation,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5771,2,"['down', 'error']","['down', 'error-when-using-createreadcountpanelofnormals-in-calling-somatic-copy-number-variation']"
Availability,See the log: https://people.freebsd.org/~pi/logs/gatk-4.1.2.0.43.log. The downstream bug report: https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=239901,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6097:74,down,downstream,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6097,1,['down'],['downstream']
Availability,Seeing a test failure due to errors with the service account access token. Possibly related to updating the NIO dependency. We've seen this multiple times today. ; ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile FAILED; com.google.cloud.storage.StorageException: Error getting access token for service account: ; at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:203); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:349); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:186); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:183); at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:183); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:194); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.<init>(CloudStorageReadChannel.java:72); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:62); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:268); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:229); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newInputStream(CloudStorageFileSystemProvider.java:348); at java.nio.file.Files.newInputStream(Files.java:152); at org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile(GcsNioIntegra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2514:14,failure,failure,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514,3,"['Error', 'error', 'failure']","['Error', 'errors', 'failure']"
Availability,"Seeing same issue, I'm running this from master node . --> ./gatk-launch PrintReadsSpark -I hdfs://gatk/input.bam -O hdfs://gatk/output.bam. getting following error:; ***********************************************************************. A USER ERROR has occurred: Failed to read bam header from hdfs://gatk/input.bam; Caused by:java.net.UnknownHostException: gatk. ***********************************************************************",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382#issuecomment-319464535:159,error,error,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382#issuecomment-319464535,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,Seeing some test failures unfortunately due to the following error:. ```; java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.FileSystem; at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7203#issuecomment-819800585:17,failure,failures,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7203#issuecomment-819800585,2,"['error', 'failure']","['error', 'failures']"
Availability,Seeking assistance on the build errors -- I don't quite have the bandwidth to decode the logs to figure out what is failing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7611#issuecomment-998217567:32,error,errors,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7611#issuecomment-998217567,1,['error'],['errors']
Availability,SelectVariants fails with no errors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6990:29,error,errors,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6990,1,['error'],['errors']
Availability,"ServletContextHandler@50f4b83d{/jobs/job/json,null,UNAVAILABLE}; 18/03/07 20:32:55 INFO handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@5d66ae3a{/jobs/job,null,UNAVAILABLE}; 18/03/07 20:32:55 INFO handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@30159886{/jobs/json,null,UNAVAILABLE}; 18/03/07 20:32:55 INFO handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@33de7f3d{/jobs,null,UNAVAILABLE}; 18/03/07 20:32:55 INFO ui.SparkUI: Stopped Spark web UI at http://10.48.225.55:4041; 18/03/07 20:32:55 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread; 18/03/07 20:32:55 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors; 18/03/07 20:32:55 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down; 18/03/07 20:32:55 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 18/03/07 20:32:55 INFO cluster.YarnClientSchedulerBackend: Stopped; 18/03/07 20:32:55 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/03/07 20:32:55 INFO memory.MemoryStore: MemoryStore cleared; 18/03/07 20:32:55 INFO storage.BlockManager: BlockManager stopped; 18/03/07 20:32:55 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/03/07 20:32:55 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/07 20:32:55 INFO spark.SparkContext: Successfully stopped SparkContext; 20:32:55.769 INFO FlagStatSpark - Shutting down engine; [March 7, 2018 8:32:55 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.FlagStatSpark done. Elapsed time: 1.60 minutes.; Runtime.totalMemory()=2091384832; 18/03/07 20:32:55 INFO util.ShutdownHookManager: Shutdown hook called; 18/03/07 20:32:55 INFO util.ShutdownHookManager: Deleting directory /tmp/farrell/spark-9e0f0525-00f3-4b37-b1d2-4cf55b4c8cb0. real 1m41.113s; user 0m49.698s; sys 0m4.432s. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371353888:13658,down,down,13658,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371353888,2,['down'],['down']
Availability,"ServletContextHandler@55c20a91{/stages/pool/json,null,AVAILABLE,@Spark}; 10:33:07.361 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3ba96967{/storage,null,AVAILABLE,@Spark}; 10:33:07.362 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1237cade{/storage/json,null,AVAILABLE,@Spark}; 10:33:07.363 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4509b7{/storage/rdd,null,AVAILABLE,@Spark}; 10:33:07.364 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@5dbc4598{/storage/rdd/json,null,AVAILABLE,@Spark}; 10:33:07.365 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@38a27ace{/environment,null,AVAILABLE,@Spark}; 10:33:07.366 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@7e8783b0{/environment/json,null,AVAILABLE,@Spark}; 10:33:07.367 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@53d2f0ec{/executors,null,AVAILABLE,@Spark}; 10:33:07.369 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@14d36bb2{/executors/json,null,AVAILABLE,@Spark}; 10:33:07.370 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@4452e13c{/executors/threadDump,null,AVAILABLE,@Spark}; 10:33:07.371 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@42172065{/executors/threadDump/json,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@8e77c5b{/static,null,AVAILABLE,@Spark}; 10:33:07.380 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@49741274{/,null,AVAILABLE,@Spark}; 10:33:07.382 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3e5b2630{/api,null,AVAILABLE,@Spark}; 10:33:07.383 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@1b6e4761{/jobs/job/kill,null,AVAILABLE,@Spark}; 10:33:07.384 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@642ec6{/stages/stage/kill,null,AVAILABLE,@Spark}; 10:33:07.389 INFO ContextHandler - Started o.s.j.s.ServletContextHandler@3fe5ad73{/metrics/json,null,AVAILABLE,@Spark}; 10:33:07.397",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:46918,AVAIL,AVAILABLE,46918,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['AVAIL'],['AVAILABLE']
Availability,Set channel reopen option and retry options for robustness,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2506:48,robust,robustness,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2506,1,['robust'],['robustness']
Availability,Set locale to US in Main is not applied to downstream projects Main classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3483:43,down,downstream,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3483,1,['down'],['downstream']
Availability,"Several experimental changes that improve precision results, and expand possible evaluations, of GATK CNV:. - `combine_tracks.wdl` for post-processing somatic CNV calls. This wdl will perform two operations:; - Increase precision by removing:; - germline segments. As a result, the WDL requires the matched normal segments.; - Areas of common germline activity or error from other cancer studies.; - Convert the tumor model seg file to the same format as AllelicCapSeg, which can be read by ABSOLUTE. This is currently done inline in the WDL. ; - This is not a trivial conversion, since each segment must be called whether it is balanced or not (MAF =? 0.5). The current algorithm relies on hard filtering and may need updating pending evaluation.; - For more information about AllelicCapSeg and ABSOLUTE, see: ; - Carter et al. *Absolute quantification of somatic DNA alterations in human cancer*, Nat Biotechnol. 2012 May; 30(5): 413–421 ; - https://software.broadinstitute.org/cancer/cga/absolute ; - Brastianos, P.K., Carter S.L., et al. *Genomic Characterization of Brain Metastases Reveals Branched Evolution and Potential Therapeutic Targets* (2015) Cancer Discovery PMID:26410082. - Changes to GATK tools to support the above:; - `SimpleGermlineTagger` now uses reciprocal overlap to in addition to breakpoint matching when determining a possible germline event. This greatly improved results in areas near centromeres.; - Added tool `MergeAnnotatedRegionsByAnnotation`. This simple tool will merge genomic regions (specified in a tsv) when given annotations (columns) contain exact values in neighboring segments and the segments are within a specified maximum genomic distance. . - `multi_combine_tracks.wdl` and `aggregate_combine_tracks.wdl` which run `combine_tracks.wdl` on multiple pairs and combine the results into one seg file for easy consumption by IGV.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5252:364,error,error,364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5252,1,['error'],['error']
Availability,"Several users have run into this issue where GenomicsDBImport errors out due to duplicate fields in their Info, Format, and/or Filter fields. They want to be able to run GenomicsDBImport without having to manually alter their files to remove duplicates. . This is the latest issue reported Sept 6. Here are some others I found that may be related: ; - [Feb 6](https://github.com/bcbio/bcbio-nextgen/issues/2674 ) ; - [April 5](https://gatkforums.broadinstitute.org/gatk/discussion/23824/gatk-4-1-1-0-genomicsdbimport-error-duplicate-field-name-af-found-in-vid-attribute-fields) ; - [July 11](https://gatkforums.broadinstitute.org/gatk/discussion/24212/gatk-4-1-1-0-genomicsdbimport-error-duplicate-fields-exist-in-vid-attribute-fields-and-2-errors). GATK version is 4.1.2.0. **_Command: _**; gatk_megs=$(head -n1 /proc/meminfo | awk '{print int(0.9*($2/1024))}'); ; gatk --java-options ""-Xmx${gatk_megs}m"" GenomicsDBImport --genomicsdb-workspace-path pon_db -V GHS_PT100006_233694007.gvcf.gz -L xgen_plus_spikein.b38.bed --batch-size 50 --reader-threads 5 --tmp-dir=./tmp2. **_Error log:_**; Using GATK jar /mnt/PoN_gvcf/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx14441m -jar /mnt/PoN_gvcf/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenomicsDBImport --genomicsdb-workspace-path pon_db -V GHS_PT100006_233694007.gvcf.gz -L xgen_plus_spikein.b38.bed --batch-size 50 --reader-threads 5 --tmp-dir=./tmp2; 16:20:56.770 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/PoN_gvcf/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:20:57.244 INFO GenomicsDBImport - ------------------------------------------------------------; 16:20:57.244 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.2.0; 16:20:57.245 INFO GenomicsDBImport - For support and do",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6158:62,error,errors,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6158,4,['error'],"['error-duplicate-field-name-af-found-in-vid-attribute-fields', 'error-duplicate-fields-exist-in-vid-attribute-fields-and-', 'errors']"
Availability,"Short answer: No, it's not using any information from the genotypes at all. You should get the same results with or without genotypes. Long technical answer: Both of these issues are related to the fact that when combining multiple possible variants at a site (i.e. variants with different alleles from the GGA `-alleles` input as well as any alleles detected in the input sample data), we often need to do remapping of the reference and alternate alleles so that they can all be described in the same context. In the places that caused these bugs we were trying to do this remapping to the alleles directly, but the fact that the variants had genotypes which referred to the original set of non-remapped alleles meant that those original alleles were still kept around and caused an error in the downstream code that tries to put together the final variant record to be emitted at the site. So it was just a case of links within the data structures that hold the genotypes interfering with the intended result, which should be based only on the input set of alternate alleles.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5355#issuecomment-433468224:784,error,error,784,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5355#issuecomment-433468224,2,"['down', 'error']","['downstream', 'error']"
Availability,"Should I follow the existing convention of using UserException for user errors and GATKException for everything else that doesn't clearly fall under a standard exception type?. The alternative would be to port PicardException, which we decided not to do IIRC.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/73#issuecomment-69978719:72,error,errors,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/73#issuecomment-69978719,1,['error'],['errors']
Availability,Should I wait for a bug fix ? Can I do something to avoid the error? Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-388403388:62,error,error,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-388403388,1,['error'],['error']
Availability,"Should the CreateHadoopBamSplittingIndex tool also work on a cram? I am getting this error below which suggests not. What are the benefits of a SplittingIndex for a spark job? On average-how long should it take a spark job to get the splits for a 30x bam or cram? . ```; gatk CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 11:47:53.243 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - The Genome Analysis Toolkit (GATK) v4.0.1.1; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-696.10.3.el6.x86_64 amd64; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Start Date/Time: March 7, 2018 11:47:52 AM EST; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Version: 2.14.1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Picard Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506:85,error,error,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506,1,['error'],['error']
Availability,Should throw an error if the client tries to write to TileDB in such a way as would corrupt an instance.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2536:16,error,error,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2536,1,['error'],['error']
Availability,Should work now:. ...; Download https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/com/google/cloud/google-cloud-nio/0.20.4-alpha-SNAPSHOT/google-cloud-nio-0.20.4-alpha-20171031.194208-5.pom; Download https://repo1.maven.org/maven2/org/ojalgo/ojalgo-extensions/1.0.0/ojalgo-extensions-1.0.0.pom; Download https://repo1.maven.org/maven2/com/intel/gkl/gkl/0.8.2/gkl-0.8.2.pom; ...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3970#issuecomment-351937244:23,Down,Download,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3970#issuecomment-351937244,3,['Down'],['Download']
Availability,Shrink NIO buffers sizes for GenomicsDBImport down to the smallest values that still give good performance,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2640:46,down,down,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2640,1,['down'],['down']
Availability,"Similar to AD, the new annotation (DD) captures the depth of each allele supporting evidence; or reads, however it does so by following a variational Bayes approach looking into the; likelihoods rather than applying a fix threshold. . This turns out to be more robust in some instances. To get the new non-standard annotation in HC you need to add -A AllelePseudoDepth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7303:261,robust,robust,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7303,1,['robust'],['robust']
Availability,"Since the Picard changes in #3620, SamAssertionUtils has been failing silently. See e.g. the Standard error tab for https://storage.googleapis.com/hellbender-test-logs/build_reports/13120.7/tests/test/classes/org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.html:. ```; USAGE: SortSam [arguments]; ...; input is not a recognized option; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3664:102,error,error,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3664,1,['error'],['error']
Availability,"Since the VCF in question is malformed, the task here becomes ""make IndexFeatureFile throw a more informative error message when the file being indexed is malformed""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4184#issuecomment-358353490:110,error,error,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4184#issuecomment-358353490,1,['error'],['error']
Availability,Since the problem disappear depending on the content of the VCF I suspect that the error message is actually misleading.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2545#issuecomment-290234626:83,error,error,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2545#issuecomment-290234626,1,['error'],['error']
Availability,"Since this touches a lot of files, I'll categorize changes here:; 1. Convenience Script Changes; - bug fixes for running on Linux; scripts/sv/manage_sv_pipeline.sh; - detect number of preemptible workers to choose NUM_EXECUTORS correctly; scripts/sv/run_whole_pipeline.sh; - allow sanity_checks.sh to run from outside GATK_DIR, exit correctly on error; scripts/sv/sanity_checks.sh. 2. Minor changes to existing utils to support new filter; - allow construction of IntHistogram.CDF from known cdfFractions and nCounts; src/main/java/org/broadinstitute/hellbender/tools/spark/utils/IntHistogram.java; - in constructor, coverage is passed as a float; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/ReadMetadata.java; - add xgboost maven repository for gradle; build.gradle. 3. Significant changes to existing code to support/invoke new filter; - add arguments for XGBoostEvidenceFilter, changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:346,error,error,346,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,2,['error'],['error']
Availability,"Since you said in an email you had some failures with this version, but not the version before this one, I'll wait until you give an explicit ok on this branch @jean-philippe-martin before I merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-278090971:40,failure,failures,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-278090971,1,['failure'],['failures']
Availability,"Sink.writeReadsSingle(ReadsSparkSink.java:228); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark.runTool(BwaAndMarkDuplicatesPipelineSpark.java:62); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); at org.broadinstitute.hellbender.Main.main(Main.java:230); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. I have to look more into BwaAndMarkDuplicatesPipelineSpark. The good news is at least we get BwaSpark working now: `BwaSpark` with `--bamPartitionSize`=4000000 or 64000000, the program finishes in less than 20 minutes without error. (It used to stalled if no `--bamPartitionSize` is specified).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363:5766,error,error,5766,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363,1,['error'],['error']
Availability,Skipping does look like it would be consistent with what's going to happen downstream anyways. Do you have some data I can use for testing?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6962#issuecomment-730665918:75,down,downstream,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6962#issuecomment-730665918,1,['down'],['downstream']
Availability,"Small PR containing fixes for various issues:; - Move CompareSAMs to picard package (fixes https://github.com/broadinstitute/hellbender/issues/139); - Move most of `CompareSAMs.doWork()` into a separate public method, to be used by external unit tests; - Use HTSJDK's SamFileValidator in assorted unit tests, rather than ValidateSamFile (which is just a CLP wrapper); - Insert `--VERBOSITY ERROR` into CommandLineProgramTest, which suppresses most logging output for CLPs that use HTSJDK-based logging (fixes https://github.com/broadinstitute/hellbender/issues/134)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/171:390,ERROR,ERROR,390,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/171,1,['ERROR'],['ERROR']
Availability,"Small test that demonstrates numerical error bug, and fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/878:39,error,error,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/878,1,['error'],['error']
Availability,Smith-Waterman parameters do not represent multiple events robustly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2498:59,robust,robustly,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2498,1,['robust'],['robustly']
Availability,Snappy-java error when running gatk-launch FastqToSam,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2868:12,error,error,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868,1,['error'],['error']
Availability,"So - the reason I asked for batch size and number of samples was to figure out the number of ""fragments"" within the contig folder (each fragment is associated with a folder starting with ""__"" and then a long uuid type string). Each batch is associated with a fragment, so number of fragments should be:. unknown + 250/50 + 200/50 + ceiling(461/30) + ceiling(101/21) = unknown + 30. Where unknown is the number of fragments for the first import. The total number of fragments in the contig folder was 49. So unless the batch size for the first import was 6, there would seem to be a few extraneous fragments in the contig folder. All of which leads me to wonder if there is some corner case being missed by the code that deals with append job failures. That could be one reason why the number of fragments is higher than expected (and some fragments are incomplete). . One last thing that might help a bit is if you could share the timestamps for the folders immediately under the contig folder. Tarballs don't preserve those timestamps, but looking at those timestamps could help us figure out which fragments belong to which batches.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-719623951:742,failure,failures,742,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-719623951,1,['failure'],['failures']
Availability,"So I just updated one of the newer tests, and now all of the tests for HaplotypeCaller seem to be passing locally. The previous commits updating the copy code were preserved when Louis reverted, so there were basically no changes I had to make to get this ""working."" That does leave us with one question now:. When looking into this a little with James and Louis earlier, we realized that the code for setting up the ActiveRegionGenotyper uses a weird partial copy of the standard CLI args method that has existed in the code for whoever knows how long. Conceptually this seems like a bad idea, but changing it now would possibly cause some older tests to fail, if they were based on this faulty method reasoning. Should we try to merge the PR as it is now, with all tests passing, and hopefully consistency with previous behavior, or try to update the logic around this genotyper as well at the same time? It's possible we can try to address the latter point as well at some point in the future when we try to get Louis's refactor code actually working. Maybe there could be some quarter goal around a HaplotypeCaller code revamp sometime inspired by some of these ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847916216:689,fault,faulty,689,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847916216,1,['fault'],['faulty']
Availability,So how has the -new-qual error modified? I have used gatk 4.0.4.0 for all my analysis. How badly should I have to re-analyse with the new version of GATK? Is it really important?. __(edited to remove distracting boilerplate text - lbergelson)__,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5000:25,error,error,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5000,1,['error'],['error']
Availability,"So this seems to only happen when trying to access a bucket from a job on dataproc. For example, the following throws the error:. ./gatk-launch PathSeqFilterSpark -I gs://bucket/in.bam -O gs://bucket/out.bam -- --sparkRunner GCS --cluster my-cluster. but the following does not:; ./gatk-launch PathSeqFilterSpark -I hdfs://bams/in.bam -O hdfs://bams/out.bam -- --sparkRunner GCS --cluster my-cluster. This happens even if I launch the cluster ""gcloud dataproc clusters create ... --scope cloud-platform"", which is supposed to grant full storage permissions. I believe this is equivalent to checking the ""Allow API access to all Google Cloud Services"" box if you launch a cluster through the web console. . Also explicitly adding the service account as a ""storage legacy bucket owner"" does not seem to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331002437:122,error,error,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331002437,1,['error'],['error']
Availability,"So, I have a question that I believe some discussions are necessary, regarding read filtering. The background that triggered this question is when I tried `PrintReads(Spark)` to filter out reads with `ReadFilterLibrary.GoodCigarReadFilter`. As it turns out, that one of the read (long read) have several alignment records, (unfortunately) the primary (i.e. 256 and 2048 flags not turned on) record is the one having a problematic CIGAR, hence filtered out. The alignment records left are&mdash;as expected&mdash;all records with either `not-primary` or `supplementary` flag turned on. . Should one consider such bam valid for analysis?; And more generally, should the collection of alignments after read filtering be checked with `ValidateBam` (assuming the tool checks for that error) as part of best practices?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6433:779,error,error,779,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6433,1,['error'],['error']
Availability,So... the reservoir downsampler seems to be sensitive to the state of the random number generator. Since several tests depend on the same expected output file I keep having failures. I'm resetting the seed for each test now to see if that will work.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5622#issuecomment-458770303:20,down,downsampler,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5622#issuecomment-458770303,2,"['down', 'failure']","['downsampler', 'failures']"
Availability,Somatic CNV WDL needs to support tumor-only mode where no matched normal is available.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983:76,avail,available,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983,1,['avail'],['available']
Availability,Somatic error correction for low allele fractions,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4868:8,error,error,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4868,1,['error'],['error']
Availability,"Some additional information I can add about this is that a potential solution to this problem is to remove altenate alleles and force the sites into being biallelic but this causes problems in other tools, like FastaAlternateReferenceMaker, so it's not a viable solution. . Since AD is 0-indexed and GT starts at 0, GT really seems like the best way to access the elements of the AD array. Perhaps a new feature could be added that automatically finds the called allele, like getAD().getCalledGT if giving the user access to GT is problematic for some reason. This would require getAD to accept something other than a pure number though and to my knowledge anything other than eg: getAD().1 or getAD().2 or 0, causes errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7448#issuecomment-909126111:717,error,errors,717,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7448#issuecomment-909126111,1,['error'],['errors']
Availability,"Some factors to consider in making this decision:. -Operations on zero-length intervals are error-prone due to lack of understanding/consensus about expected results (eg., should a query on a zero-length interval return records that abut it on either side?). -We need to determine how a query involving a zero-length interval is supposed to behave in the GA4GH API, as this does not seem to be clearly defined in the API documentation (eg., http://ga4gh.org/documentation/api/v0.5.1/ga4gh_api.html#/schema/%2FUsers%2Fkeenan%2FDropbox%2Fgit-checkouts%2Fschemas%2Fsrc%2Fmain%2Fresources%2Favro%2Ftarget%2Fall.avpr/org.ga4gh.GASearchReadsRequest). The representation is 0-based closed-open (like BED), which means zero-length intervals are possible, but their behavior appears undefined. -None of our current query interfaces (tribble/samtools) support computing overlap with zero-length intervals (although they don't throw an error when given such an interval -- they just never return any records for such queries). -It seems unlikely that we'll be moving anytime soon to representing insertions using zero-length intervals, given that the VCF spec requires insertions to be represented in terms of the preceding reference base.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/317:92,error,error-prone,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/317,2,['error'],"['error', 'error-prone']"
Availability,"Some filters are implemented in the ModelSegments CreatePoN code (since these filters were directly ported from GATK CNV). Other filters were implemented as external python scripts by @mbabadi for GPC2 validation. We should extract and productionize if possible. Ideally, the tool would take several coverage files (collected over identical bins) and filtering parameters as input, and output a filtered list of bins. Downstream tools would subset the original coverage files to these bins accordingly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2992#issuecomment-391682843:418,Down,Downstream,418,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2992#issuecomment-391682843,1,['Down'],['Downstream']
Availability,"Some kinds of interval files contain a sequence dictionary (eg., Picard-style interval files). We should use these sequence dictionaries when they're present and no other sequence dictionaries are available from the other inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/301:197,avail,available,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/301,1,['avail'],['available']
Availability,"Some notes from the 10k tieout:; ### Prepare Step; - ~20 min per full ref_ranges table to insert; - ~7 min per full vet table to insert; - ""bytes scanned"" are same as data table size. ### Extract. **Original Run - 293 min**. - 103 minutes pulling down data, scanning 237 GB; - 43 min on 20m vet records (20:26 - > 21:09); - 60 min on 291m vet records (21:09 -> 22:10). - 190 minutes writing the VCF; ; **Prepare Extract with minor tuning of sorting - 134 min**. - 25 minutes pulling down data ( faster), scanning 10 GB (50x reduction); - 4 min on 20m vet records(02:43 -> 02:47) - NOTE 103s of that was sorting (44s) and spilling to disk (59 s); - 21 min on 291m vet records (02:47 -> 03:08) - NOTE 9 min of that was sorting (6 min) and spilling to disk (3 min). - 109 minutes writing the VCF (this is the change to pre-sort the sample set merged to ah_var_store on 1/12/22) . **Tieout is identical**. ```; kcibul@kc-specops-tiny:~/stroke_tieout$ md5sum gold.jointcallset_0.vcf.gz; 496178eae4afe63c4391d8eba64a9947 gold.jointcallset_0.vcf.gz. kcibul@kc-specops-tiny:~/stroke_tieout$ md5sum trial.full.jointcallset_0.vcf.gz; 496178eae4afe63c4391d8eba64a9947 trial.full.jointcallset_0.vcf.gz; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7640:247,down,down,247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7640,2,['down'],['down']
Availability,"Some notes on individual commits:. Updated CallCopyRatioSegments and PreprocessIntervals; reorganized copynumber packages.; -For motivation of changes in CallCopyRatioSegments, see #3825.; -I added the ability to turn off binning in PreprocessIntervals by specifying bin_length = 0.; -I removed the separation between coverage and allelic packages to make the package structure a bit simpler.; -@MartonKN should review, since he wrote PreprocessIntervals and is updating the caller. Added segmentation classes and tests for ModelSegments CNV pipeline.; -I added implementations of copy-ratio, allele-fraction, and ""multidimensional"" (joint) segmentation. All implementations are pretty boilerplate; they simply partition by contig and then call out to KernelSegmenter. Note that there is some logic in multidimensional segmentation that only uses the first het in each copy-ratio interval and if any are available, and imputes the alt-allele fraction to 0.5 if not.; -Makes sense for @mbabadi to review this, since he reviewed the KernelSegmenter PR. Added modeling classes and tests for ModelSegments CNV pipeline.; -Most of this code is copied from the old MCMC code. However, I've done some overall code cleanup and refactoring, especially to remove some overextraction of methods in the allele-fraction likelihoods (see #2860). I also added downsampling and scaling of likelihoods to cut down on runtime. Tests have been simplified and rewritten to use simulated data.; -@LeeTL1220 do you think you could take a look?. Added ModelSegments CLI.; -Mostly control flow to handle optional inputs and validation, but there is some ugly and not well documented code that essentially does the GetHetCoverage step. We'll refactor later, I filed #3915.; -@asmirnov239 can review. This is lower priority than the gCNV VCF writing. Deleted gCNV WDL and Cromwell tests.; -Trivial to review. Added WDL and Cromwell tests for ModelSegments CNV pipeline.; -This includes the cost optimizations from @meganshand a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3913:904,avail,available,904,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3913,1,['avail'],['available']
Availability,"Some of the Docker work from `ah_var_store` needs to be on `EchoCallset` to be able to do the PGEN subsets, particularly the PLINK Docker and GAR changes upon which the PLINK Docker changes depend. I have freshly baked the Variants, PLINK, and Docker images just now for this PR. 👨‍🍳 . Integration run in progress here https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/2585a1b6-c5da-48f0-a196-b5679e7f40a5",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8805:60,Echo,EchoCallset,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8805,1,['Echo'],['EchoCallset']
Availability,"Some preliminary evaluation of the new ModelSegments pipeline on CRSP samples has revealed some weaknesses of the ReCapSeg caller (which is simply ported from the old pipeline) to me. I think there are a lot of confusing things going on:. 1) For determining copy-neutral segments, all segments with log2 mean below some threshold are used (rather than absolute log2). There is a comment that this is done to ""mimic the python code"" but I have no idea why this would be sensible, since it includes all deletions.; 2) There is some confusion arising from inconsistent use of z-score and T-statistic. Standard deviation, rather than standard error, is used for calling; i.e., a ""called segment"" is one that has a mean log2 copy ratio that has a z-score above some threshold with respect to the standard deviation of the log2 copy ratios of intervals that fall within copy-neutral segments (note also that these intervals have already been filtered by z-score to remove outliers). That is, any segment with a mean that falls sufficiently within the fuzziness of the caterpillar is not called.; 3) However, even calling using standard error is probably not what we want. This would simply be asking the question: given a population of copy-neutral intervals with a mean and standard deviation, does any non-copy-neutral segment contain intervals with a mean significantly different than the population? We've already answered this question during segmentation!. I think what we want to do instead is ask questions about the population of segment-level copy-ratio estimates, weighted by length.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3825:639,error,error,639,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3825,2,['error'],['error']
Availability,"Some recent work by the Green Team as well as some evaluations we have done on our own tools have illuminated that the HaplotypeCaller has a non-deterministic output for some sites (a handful of sites across a typical 30x bam). Typically the differences manifest themselves as minor differences in the annotations at a few sites, for example the qual score and annotations might jitter from run to run like the following two variants: ; `9	103454626	.	A	T	54.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=0.431;DP=4;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=26.85;MQRankSum=1.383;QD=13.65;ReadPosRankSum=0.000;SOR=2.303	GT:AD:DP:GQ:PL	0/1:2,2:4:62:62,0,78`; `9	103454626	.	A	T	52.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=0.431;DP=4;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=26.85;MQRankSum=1.383;QD=13.15;ReadPosRankSum=0.000;SOR=2.303	GT:AD:DP:GQ:PL	0/1:2,2:4:60:60,0,78`; We should track down what is causing this error and shore up our score. I have found a test case that apparently reproduces the non-determinism. It seems to be somehow related to running the input data through the Google Connector. That is, the results appear to be reproducibly deterministic (at least over ~25 trials) when the input bam is local, whereas it starts to jitter when run from a `gs://` URL.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6105:891,down,down,891,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6105,2,"['down', 'error']","['down', 'error']"
Availability,"Some things consider when we address this: htsjdk has code that [normalizes/standardizes](https://github.com/samtools/htsjdk/blob/fbba5364e1809de071bc479f30e4e2c8b17f5bbe/src/main/java/htsjdk/variant/vcf/VCFStandardHeaderLines.java#L150-L203) select known header lines. One of these is AD. So in this particular case, when it reads in a header containing this:. `##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""AD"">`. it will turn it in to the standardized version:. `##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">`. But not all header mutation code paths do this, and not all header lines are repaired. Its quite possible to read/create a header with two lines of a given type (FORMAT/INFO) with the same ID, but different attributes, and htsjdk will retain both (though the spec precludes this). The header query APIs (say, getFormatHeaderLines()) would only return one, but both will be serialized on write. We have a large htsjdk [PR](https://github.com/samtools/htsjdk/pull/835) to fix a bunch of VCF header bugs, including the duplicate header line issue, though its pretty old and stale at this point. This might be additional incentive to revive that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3839#issuecomment-344645282:666,repair,repaired,666,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3839#issuecomment-344645282,1,['repair'],['repaired']
Availability,Some times in the past I found that it would have been useful to be able to determine whether the user gave a value to an argument or not. I.e. the argument default value may be different based on the circumstances (by different tools if shared across tools or the same tool based on other argument values) and so it is important to make sure that we are not overriding the user request (or at least we can emit the appropriate warning or error message).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/110:439,error,error,439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/110,1,['error'],['error']
Availability,"Some tools have had usage examples ported from GATK3 that don't work in GATK4. We should fix ; these. . As well as fixing errors, it would be good to change the javadoc so it references parameters by the constant values instead of hardcoding them. (use `{@value StandardArgumentDefinitions#SOME_NAME}` ). These occur in at least the following tools, (found by `find in path -T`):; - [ ] ValidateVariants; - [ ] VariantFiltration; - [ ] AnalyzeCovariates; - [ ] BaseRecalibrator; - [ ] LeftAlignIndels",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1632:122,error,errors,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1632,1,['error'],['errors']
Availability,"Somewhere between #835 and now, BaseRecalibrator stopped working. When I try to run testBQSRBucket, I get the error below. This test is currently enabled so regression tests should have caught this. ```; java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;; at org.broadinstitute.hellbender.tools.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:131); at org.broadinstitute.hellbender.tools.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:104); at org.broadinstitute.hellbender.tools.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:86); at org.broadinstitute.hellbender.tools.dataflow.pipelines.BaseRecalibratorDataflowIntegrationTest.testBQSRBucket(BaseRecalibratorDataflowIntegrationTest.java:176); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/866:110,error,error,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/866,1,['error'],['error']
Availability,"Sorry @cmnbroad, I misunderstood your comment before - what I've got is that it is ok to have beta/experimental for this, but it's obviosly not what you said, so my fault for not reading carefully. In that case, I would like to have a proposal for how to proceed here:. * I will implement the port for the tools in two independent PRs - just direct translation into the new framework, documentation and kebab-case argument style.; * The port for `RealignerTargetCreator` will be similar to this one; * The port for `IndelRealigner` would not have support for n-way output, although it will be fully functional in other ways. The n-way option can be ported in the future as an extra feature if necessary (maybe the communications team can weight in, and tell if it is really a needed feature); * For the first test, which will be removed before merging, I will use the data from the tutorial. This will be the validation for the port before test data valid for the repository is provided; * For the final tests, I will draft the class with the tests from GAKT3 without the data and disabled, waiting for @sooheelee for meaningful tests (or other people from your team). The main idea is to have two PRs with the port validated with the tutorial data, and add other tests similar to the GATK3's ones for extra validation and/or covering missing codepaths using @sooheelee or ported GATK3 data. Does it makes sense, @cmnbroad and @sooheelee?. @sooheelee - I think that the 1000G data can be a good validation if chromosomes 20/21 have realigned reads without a pair mapping on other chromosomes. In that case, some of that reads can be extracted and reset to the state previous to realignment to validate the new tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371728774:165,fault,fault,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371728774,1,['fault'],['fault']
Availability,"Sorry @droazen, the previous commit had an error in the tests. I'm rebasing/squashing to make a clear PR and when all check pass (except CLOUD), you can review if you have time. Thank you very much.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-246346004:43,error,error,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-246346004,1,['error'],['error']
Availability,"Sorry for not letting this go, but this a severe bug with repercussions on clinical diagnostics that still has not been communicated to other users, nor fixed, for more than three months. @droazen, is there anything else that we could do to hasten this along? Is there any workaround other than downgrading Mutect2 to the version from more than two years ago?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1283503009:295,down,downgrading,295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1283503009,1,['down'],['downgrading']
Availability,"Sorry for replying so late. I did not expect the reply for my question came so soon until I checked my GitHub today.; Yes, I am using the default parameters. I am not sure about the difference after FilterMutectCalls. I will deal with it next week after going back to work.; As for as I know, the default limit for downsampling is 50, so the theoretical maximum coverage is about 5000, assuming the length of reads are 100bp. This may be an abnormal value for NGS WGS datas, but many NGS panel datas have very high coverages and their average coverages on panel are possibly more than 5000. Is there a model for selecting a proper downsampling limit for these high coverage datas?; Thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4325#issuecomment-385255586:315,down,downsampling,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4325#issuecomment-385255586,2,['down'],['downsampling']
Availability,"Sorry for the confusion, here's what's happening: . In the stable version, we don't do this de-overlapping at all, so this won't change our `inv_del_ins.vcf`. For the experimental code path, without this PR, we are doing this de-overlapping step before inferencing the exact location of the breakpoints, hence two alignments never overlap when they are sent downstream for pinning down the locations; because homology by definition will cause the same part of a read map to two different locations, hence two alignments must overlap on the read if homology is present, thus the de-overlapping step causing the bug in #3894.; This PR removes that problematic de-overlapping step (done first the 1-liner commit, i.e. the 2nd commit, which doesn't use the de-overlapped alignments, then once the generated variants are reviewed to be OK, I removed the de-overlapping step in the 3rd commit).; The in the 4th commit, I experimented with removing, again only in the experimental code path, the alignment filter based on its alignment length. I personally think that is a better treatment, but don't have a strong opinion on it. So for the experimental code path, if we accept the first 3 commits, no de-overlapping is done on the alignments, just like the stable code path. The stable and the experimental version would only differ on which contigs are sent for this analysis, i.e. they differ in the preprocessing step. To be more specific, ; * the stable version calls into `ChimericAlignment.parseOneContig()` for turning all assembly contigs, no matter how many alignments it has, gets a list of chimeric pairs, and sends them down for breakpoint inference; * in the experimental tool, `ChimericAlignment.extractSimpleChimera()` takes **only** contigs with exactly **two** good alignments and turn it into a **single** pair, then send the pair down for inference. Regarding `ChimericAlignment.parseOneContig()`, it is good for extracting novel adjacencies and outputting BND records for contigs with mu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4282#issuecomment-361753769:358,down,downstream,358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4282#issuecomment-361753769,2,['down'],"['down', 'downstream']"
Availability,"Sorry for the delay, when running the tests individually I get : ; ```; ./gradlew test --tests org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest; ...; Results: SUCCESS (2 tests, 2 successes, 0 failures, 0 skipped). BUILD SUCCESSFUL; ```; It works when the tests are launched individually, like the other tests.; So it seem like there is a similar problem for these tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5511#issuecomment-448972714:236,failure,failures,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511#issuecomment-448972714,1,['failure'],['failures']
Availability,"Sorry for the delayed response @cmatKhan. The fix here is a little complicated because its a confluence of expected behaviors adding up to this. . Specifically in `-ERC GVCF` mode in HaplotypeCaller we merge adjacent regions with similar `GQ` scores but within those merged reference blocks we track the DP (mean dp) and the MIN_DP. When we go to genotype in GenotypeGVCFs we fall back to Min_DP when reporting the DP since it must be at least that high at the sight in question. For Diploid samples and with the default reference blocking we use, this works fine and for most of the range up to 30+ bases you are able to recover a pretty close approximation of what your DP is in most cases. However for Haploid data the assumptions at play mean that the `GQ` gets very high and starts to max out the field (99) so you end up with extremely long reference confidence blocks with very high confidence and a min_DP of some low value (in your case for the data you shared with us of 4, which is the threshold for hitting GQ=90 in the haploid model). This is also part of why adjusting the intervals for traversal was relevant, as if they were too long they were pulling in bases 1000+ bp away that only have 4fold coverage with reads and torpedoing the reported DP. So far nothing seems to obviously be bugged (we don't test/use haploid calling mode very often so its less well tested and this sort of issue is not surprising). We might be over-confident about reporting `GQ` for haploid reference blocks (though mathematically its sound that you only need a few reference observations to be sure about the ref call). Short of adjusting that model drastically there will always be some threshold where ""after X fold coverage of reads everything is merged into a big adjacent ref block"". Unfortunately with block merging on we never expect the DP to be exact (since it should vary over the block and its expensive to store that information exactly) but the issue is much more pronounced in your haploid s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8943#issuecomment-2318660429:622,recover,recover,622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8943#issuecomment-2318660429,1,['recover'],['recover']
Availability,"Sorry to necrobump this, but were you able to figure out what the problem was, @chandrans ? I'm running into the exact same error in a slightly different context and was hoping that any insight you had would help me solve my issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126#issuecomment-502788589:124,error,error,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126#issuecomment-502788589,1,['error'],['error']
Availability,"Sorry, I guess I didn't see your edit pointing out that line of code. I've indeed looked at that test and more---there's a lot of similar duct tape and inconsistent resetting of the RNG throughout the entire test suite. But since I think we can reasonably assume that there's enough duct tape to make things deterministic overall, I don't think it's worth cleaning up the duct tape just to get neater, but equally deterministic behavior. (Or perhaps can you point to instances of persisting non-determinism, e.g. random failures in Travis?). In any case, I think it makes more sense to focus effort on making cleaner tests for the new tools, rather than make an 11th hour effort to revamp these existing tests. Do you agree?. See e.g. https://github.com/broadinstitute/gatk/issues/6112 for some related discussion. Also added a note mentioning that the original GATK3 expected results have been updated, although now looking back at the commit history, I'm not sure if that was already true.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1063002941:520,failure,failures,520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1063002941,1,['failure'],['failures']
Availability,"Sorry, I overlooked the tests failing with `IndexFeatureFile`. Now the checking of a correct index name is made before the creation of the index itself (early failure) and some tests with a block-gzipped extension were failing. I think that now they are solved, @droazen.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2247#issuecomment-258360353:159,failure,failure,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2247#issuecomment-258360353,1,['failure'],['failure']
Availability,"Sorry, I use to search but this time I forgot. My fault...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2533#issuecomment-289781301:50,fault,fault,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2533#issuecomment-289781301,1,['fault'],['fault']
Availability,"Sorry, but this bug still isn't fixed as of v4.2.6.1. Reproduce as follows:. ```; --read-filter MateDistantReadFilter; --mate-too-distant-length 1500; ```. Instead of a run-time exception (as in v4.2.5.0), HaplotypeCaller simply produces no variant calls at all. Expected behavior would be to exclude paired-end mappings whose TLEN exceeds the parameterized value. Perhaps there is an implementation bug, unrelated to the original problem, that contains faulty logic for doing this. Thanks...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1102943692:454,fault,faulty,454,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1102943692,2,['fault'],['faulty']
Availability,"Sorry, perhaps I wasn't clear. The bamout not only output the read records realigned to the reference (thru their alignment to te best haplotype) but also the haplotype themselves as reads records. I'm not 100% sure of this is true for your run but these special records probably have a sample name ""HC"" and as read id something like ""HCXXXXXXX"" where XXXXXXX is a number. . My hope was that by grouping by sample the haplotypes would stand out and that we could then verify what haplotypes were reconstructed. . My suspicion is that haplotype with no C mutation but with downstream mutation has not be reconstructed.. You need to identify the complete list of reconstructed haplotypes to confirm that, either by grouping somehow haplotypes away from the actual reads in the bamout or perhaps looking into HaplotypeCaller's debugging output. If that is true, what is happening is that reads that contain the downstream mutation would artifactually have support for the C mutation even if the have a ref base for that position or if they don't even overlap that position. So if this is confirmed, the following step would be to figure out why this is happening (not reconstructing that obvious haplotype) and fix the issue. Hopefully someone in the GATK developer team can look into this as you may well have hit an interesting edge case that needs to be ironed out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8238#issuecomment-1483076912:572,down,downstream,572,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8238#issuecomment-1483076912,4,['down'],['downstream']
Availability,"Sounds good! After I get this current version of the pipeline released,; I'll explore this further. Maybe we could follow-up on slack, or meet up; at Broad at some point if you're on-site. many thanks!. Brian. On Thu, Feb 29, 2024 at 9:32 AM Gökalp Çelik ***@***.***>; wrote:. > For that purpose I would still suggest using Sample Name IDs and Read; > Group IDs along with Mutect2 to call variants therefore contribution of; > each cell to a mutation will be quantified in terms of Allele Fractions.; > Especially if you also disable downsampling it will be quite the data to; > analyze.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/8703#issuecomment-1971271149>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABZRKX37ASEZBUTV2HCSR4TYV45W5AVCNFSM6AAAAABD4OZKJ6VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSNZRGI3TCMJUHE>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >. -- ; --; Brian J. Haas; The Broad Institute; http://broadinstitute.org/~bhaas <http://broad.mit.edu/~bhaas>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8703#issuecomment-1971281473:534,down,downsampling,534,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8703#issuecomment-1971281473,1,['down'],['downsampling']
Availability,"Sources(FeatureManager.java:182); 	at org.broadinstitute.hellbender.engine.FeatureManager.<init>(FeatureManager.java:153); 	at org.broadinstitute.hellbender.engine.GATKTool.initializeFeatures(GATKTool.java:415); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:636); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:160); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:133); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /home/vip/data/Mutect2/af-only-gnomad.raw.sites.hg19.vcf.gz has invalid uncompressedLength: -795051631, for input source: file:///home/vip/data/Mutect2/af-only-gnomad.raw.sites.hg19.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:97); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:82); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:353); 	... 14 more; Caused by: htsjdk.samtools.util.RuntimeIOException: /home/vip/data/Mutect2/af-only-gnomad.raw.sites.hg19.vcf.gz has invalid uncompressedLength: -795051631; 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:543); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextB",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6248:4675,error,error,4675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6248,1,['error'],['error']
Availability,Spaces in a sample name (inside the bam file) will cause a failure in the M2 WDL task.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4355:59,failure,failure,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4355,1,['failure'],['failure']
Availability,Spark Local Runner throws with unhelpful error message over gcs input,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369:41,error,error,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369,1,['error'],['error']
Availability,Spark metrics collector refactoring checkpoint.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1827:36,checkpoint,checkpoint,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827,1,['checkpoint'],['checkpoint']
Availability,"Spark tests on gatk-jenkins are currently failing:. Command:. ```; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; Error:. 16/11/29 16:21:01 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(M",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:377,Error,Error,377,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,3,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,SparkGenomeReadCounts Error on bai file check,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3909:22,Error,Error,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3909,1,['Error'],['Error']
Availability,SparkSharder: ensure robustness in the face of areas of high coverage,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2251:21,robust,robustness,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2251,1,['robust'],['robustness']
Availability,SplitNCigarReads error - Attempt to add record to closed writer,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8522:17,error,error,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522,1,['error'],['error']
Availability,"Spot(TM) 64-Bit Server VM 1.8.0_112-b15; Version: 4.alpha.2-1125-g27b5190-SNAPSHOT; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:4586,ERROR,ERROR,4586,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['ERROR'],['ERROR']
Availability,"Stacktrace is below. It looks like the default port (8020) is not being picked up.; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 5.0 failed 4 times, most recent failure: Lost task 8.3 in stage 5.0 (TID 82, tw-cluster-2-w-4.c.broad-gatk-collab.internal): java.lang.IllegalArgumentEx; ception: Wrong FS: hdfs://tw-cluster-2-m:-1/user/tom/small_spark_eval/dbsnp_138.b37.20.21.vcf, expected: hdfs://tw-cluster-2-m; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:648); at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:194); at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:106); at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305); at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301); at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426); at hdfs.jsr203.HadoopFileSystem.checkAccess(HadoopFileSystem.java:937); at hdfs.jsr203.HadoopFileSystemProvider.checkAccess(HadoopFileSystemProvider.java:75); at java.nio.file.Files.exists(Files.java:2385); at org.broadinstitute.hellbender.utils.io.IOUtils.assertFileIsReadable(IOUtils.java:551); at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:292); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:244); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:218); at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:202); at org.broadinstitute.hellbender.engine.spark.KnownSitesCache.loadFromFeatureDataSource(KnownSitesCache.java:43); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3468:147,failure,failure,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3468,2,['failure'],['failure']
Availability,"Standard disclaimer up front that I'm still a bit new and can't speak to; the needs of the GATK outside the small bits I've looked at. The data I've wanted so far had all come from the UCSC genome browser,; which has a reasonable format and good documentation. My personal feeling; is there's little reason to reformat the data or choose a different format; for data we may create. For me the real problem is establishing fast and; reliable look up of that data. I've so far just thrown a couple light; weight tracts into the resources on my git branch and loaded them into an; interval tree. That storage strategy will not scale for all of the UCSC; data, and in any case might antagonize them. On Mon, Apr 30, 2018, 11:59 PM samuelklee <notifications@github.com> wrote:. > I tend to agree. VCF is perhaps a special case of the format we actually; > need, so no point in shoehorning output that doesn’t fit just for the sake; > of standardization. @sooheelee <https://github.com/sooheelee> may feel; > more strongly otherwise.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385593864>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGKhCLeOf8aOnroZA0wirz8zeBoeodNsks5tt92xgaJpZM4TtIPq>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385654418:432,reliab,reliable,432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385654418,1,['reliab'],['reliable']
Availability,"Started o.s.j.s.ServletContextHandler@7530090a{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4492b393{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@55fb36de{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@63a7781{/storage,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@73b74615{/storage/json,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@22686ddb{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2e29f28e{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7bfa1eb5{/environment,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@32b46831{/environment/json,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5353dd09{/executors,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@320ff86f{/executors/json,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@192b472d{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@84f51d9{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@45b96e4c{/static,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3688baab{/,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletCon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:8730,AVAIL,AVAILABLE,8730,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['AVAIL'],['AVAILABLE']
Availability,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3140:1190,avail,available,1190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140,2,"['avail', 'down']","['available', 'down']"
Availability,Stop htsjdk CRAM reader from trying to download the reference when the expected reference is not available.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/677:39,down,download,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677,2,"['avail', 'down']","['available', 'download']"
Availability,"StorageLevel(memory, 1 replicas),127561,0)); 18/05/01 14:21:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/05/01 14:23:29 INFO MemoryStore: MemoryStore cleared; 18/05/01 14:23:29 INFO BlockManager: BlockManager stopped; 18/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:2182,heartbeat,heartbeat,2182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['heartbeat'],['heartbeat']
Availability,Store the NCBI build version in the Gencode datasource config files; in order to resolve an issue where this value was not always available; when annotating IGR variants. Resolves #4404,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5522:130,avail,available,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5522,1,['avail'],['available']
Availability,Stream closed error with gatk 4.1.1.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5919:14,error,error,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5919,1,['error'],['error']
Availability,StreamingProcessControllerUnitTest.testSerialCommands (intermittent?) test failure on Travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4014:75,failure,failure,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4014,1,['failure'],['failure']
Availability,Successful Joint Calling workflow (Exome) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/0ff13881-727f-4fdf-bde7-904559eac58f).; Successful Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5a35adca-9f75-467c-8441-53f922ab8a7d).; A more recent integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/e0a281c5-c412-4d27-a08d-cbd169f74a1c) (two failures on slight cost differences),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8515#issuecomment-1710547302:478,failure,failures,478,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8515#issuecomment-1710547302,1,['failure'],['failures']
Availability,"Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ef747737-4d19-4770-83b7-47715eff8237). tl;dr the only commit really worth looking at is 9ac0befbcc39b9c5a7eb0938dd79a7d5cbd5f297, everything else is a simple merge from master. This is just minor tweaks around recent changes in the JointVariantCalling WDL. I'll need to merge and push this locally to preserve history from master as that option is not available within the GATK GitHub repo.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8537:458,avail,available,458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8537,1,['avail'],['available']
Availability,"Successful run using the changes: https://app.terra.bio/#workspaces/gp-dsp-gvs-operations-terra/RDG_Broad_WGS_Internal_GVS/job_history/ab4b1a4f-6bc6-4fd0-a742-b98973e49767. Verified that errors of the form ; `column RP-1840_ENG_AZW_001_D1_v1_WGS_GCP at chr1:10120 .. FORMAT tag [AD] expected different number of values (expected 4, found 2)`; go away",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8057#issuecomment-1281272564:187,error,errors,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8057#issuecomment-1281272564,1,['error'],['errors']
Availability,"Summary of VQSR Changes; 	- ONLY populate AS_RAW_MQRankSum or AS_RAW_ReadPosRankSum for ref-alt genotypes (0/1, 0/2) not 1/1/,1/2,2,2; 	- AS_RAW_MQ for Non AS... Assign full MQ to alternate allele (don't distribute); 	- Compute SUM(AD) for future use; 	- provide command line option to force the AS-Approximate path even when AS annotations are available (for benchmarking/comparisons)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7718:345,avail,available,345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7718,1,['avail'],['available']
Availability,"Summary of changes:. - Fixed a minor issue in sampling error estimation that could lead to NaN (as a result of division by zero). - Introduced separate _internal_ and _external_ admixing rates. The _internal_ admixing rate is to be used internally by discrete RV posterior update routines (""callers"") as a safety measure to stabilize self-consistency loops. For example, consider the mean-field treatment of two coupled Markov chains: the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. Whi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4720:55,error,error,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720,1,['error'],['error']
Availability,"Super duper - glad it helped :) No problemo, Bioinformatics/CS is always most fun as a team - feel free to ping the list anytime on anything :) . Cheers,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2044#issuecomment-235121399:107,ping,ping,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2044#issuecomment-235121399,1,['ping'],['ping']
Availability,"Support for incrementally adding samples to existing genomicsdb workspaces. I've added these comments to the docs, but just wanted to call out again that we recommend making a backup of the existing workspace before trying to update the workspace. Otherwise, if the incremental update fails the workspace may be in a corrupted/inconsistent state. . If the user chooses not to backup (or can't), there is a (somewhat painful, manual) way to restore the workspace on failure IFF the --consolidate option has not been used. The tool will output a backup callset file (suffixed .inc.backup) and a file suffixed .fragmentlist with a list of all the original fragments. In order to roll back to a consistent workspace, the user must; - replace the callset file in the workspace with the one suffixed .inc.backup. That is, something like:; > mv _workspace_/callset.json.inc.backup _workspace_/callset.json; - delete all the directories in the workspace not named genomicsdb_meta_dir or included the file suffixed .fragmentlist",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970:465,failure,failure,465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970,1,['failure'],['failure']
Availability,Suppress the `cp: target '/gatk/srcdir' is not a directory` error message that appears in all of the travis logs by creating the target srcdir first in run_unit_tests.sh.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5878:60,error,error,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5878,1,['error'],['error']
Availability,"Sure, but then where would the WDL point? If the purpose for this PR was to make the WDL default to an available version of the file, that's done but it's pointing to that file on GCS. Maybe I'm missing something, but it seems like we can't test this with Travis and have the default location point to the actual GCS bucket holding the transcript list...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5872#issuecomment-482416039:103,avail,available,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5872#issuecomment-482416039,1,['avail'],['available']
Availability,"Sure, it's a fair point that we could write this genotype-compare operation as a standalone tool. It's convenient to make it work via VariantAnnotator so that we can do it concurrently with other annotation tasks, instead of needed two passes through the VCF. I primarily ask b/c this is a downgrade from GATK3, where the equivalent of FeatureContext was passed. . Would it be reasonable for the FeatureManager to somehow get exposed? In my example I am calling FeatureContext.getValues() without supplying an interval, but I certainly could (and maybe should). If one supplies a specific query interval, the Feautrecontext isnt doing much other than providing an abstraction between FeatureManager, and the boundaries set on Featurecontext (which i agree are tricky to figure out in some cases) dont really matter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754256795:290,down,downgrade,290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754256795,1,['down'],['downgrade']
Availability,"Sync uses the system call fsync() to synchronize contents and not sure how CIFS is erroring out. Please see my [previous comment](https://github.com/broadinstitute/gatk/issues/5342#issuecomment-433760934). Would it be possible to gather some more information using the gatk from [branch nalini_issue_5342](https://github.com/broadinstitute/gatk/tree/nalini_issue_5342)? This branch puts out additional information with errno, etc. One thing that can be done, is make an error return from the fsync() call non-fatal as it does not seem to be well supported for CIFS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453695003:83,error,erroring,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453695003,2,['error'],"['error', 'erroring']"
Availability,Syntax error on mutect2.wdl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7353:7,error,error,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7353,1,['error'],['error']
Availability,"System specs - (RedHat, Java 1.8.0_144); Clean git clone. ```; gradlew --stacktrace; Creating GATK Python package archive...; :createPythonPackageArchive UP-TO-DATE; :compileJava UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :gatkTabComplete; error: error reading /vsc-hard-mounts/leuven-user/304/vsc30484/.gradle/caches/modules-2/files-2.1/org.spire-math/spire_2.11/0.11.0/998b1c1d841baf4fc5d1b119ea55f165f6684ef5/spire_2.11-0.11.0.jar; error in opening zip file; 1 error; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/vsc-hard-mounts/leuven-data/304/vsc30484/git/gatk/build/tmp/gatkTabComplete/javadoc.options'. * Try:; Run with --info or --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':gatkTabComplete'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:64); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:52); at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52); at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:53); at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.exec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155:260,error,error,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155,5,"['FAILURE', 'error']","['FAILURE', 'error']"
Availability,System.exit in Main.java lead to a failure in yarn cluster mode,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3166:35,failure,failure,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3166,1,['failure'],['failure']
Availability,"System.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1448); 	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1436); 	at org.broadinstitute.hellbender.utils.spark.SparkUtils.pathExists(SparkUtils.java:100); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.setHadoopBAMConfigurationProperties(ReadsSparkSource.java:241); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:203); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [da63aa3c-e3bc-4893-9f40-42921719a343] entered state [ERROR] while waiting for [DONE].; ```. to reproduce this error, . ```bash; cd /Users/shuang/GATK/gatk. CLUSTER_NAME=""svdev-caller""; MASTER_NODE=""hdfs://svdev-caller-m:8020""; PROJECT_DIR=""user/shuang/NA12878_PCR-_30X"". ./gatk-launch FindBreakpointEvidenceSpark \; -R ""$MASTER_NODE""/reference/Homo_sapiens_assembly38.fasta \; -I ""$MASTER_NODE""/data/smallCram.cram \; -O ""$MASTER_NODE""/""$PROJECT_DIR""/fastq \; --exclusionIntervals gs://sv-data-dsde-dev/reference/GRCh38.kill.intervals \; --kmersToIgnore gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.dups \; --kmerIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/kmerIntervals \; --breakpointEvidenceDir ""$MASTER_NODE""/""$PROJECT_DIR""/evidence \; --breakpointIntervals ""$MASTER_NODE""/""$PROJECT_DIR""/intervals \; --qnameIntervalsMapped ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsMapped \; --qnameIntervalsForAssembly ""$MASTER_NODE""/""$PROJECT_DIR""/qnameIntervalsForAssembly \; --maxFASTQSize 10000000 \; -- \; --sparkRunner GCS \; --cluster svdev-caller; ```. ========================. On the other hand, w",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:4257,error,error,4257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['error'],['error']
Availability,"T --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46:35.795 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:46:42.808 INFO BaseRecalibrator - ------------------------------------------------------------; 15:46:42.810 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.0.0; 15:46:42.810 INFO BaseRecalibrator - For support and documentation go to https://software.broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807:2206,error,errors,2206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807,1,['error'],['errors']
Availability,T00:09:07.3905250Z symbol: class Range; 2022-08-16T00:09:07.3905751Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3906273Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:101: error: cannot find symbol; 2022-08-16T00:09:07.3906908Z static VCFHeaderLine rangeToVCFHeaderLine(Range<Integer> genotypeQualityBand) ***; 2022-08-16T00:09:07.3907793Z symbol: class Range; 2022-08-16T00:09:07.3908125Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3910592Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPruner.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3914013Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/BaseVertex.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3921838Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/MultiSampleEdge.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3936070Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/SeqGraph.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3937759Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/SeqGraph.java:4: error: package com.google.common.primitives does not exist; 2022-08-16T00:09:07.3941846Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/BaseGraph.java:3: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.3952011Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/readthreading/AbstractReadThreadingGraph.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3953755Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/readthreading/AbstractReadThreadingGraph.java:4: error: package com.g,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:7718,error,error,7718,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['error'],['error']
Availability,T00:09:07.3907793Z symbol: class Range; 2022-08-16T00:09:07.3908125Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3910592Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPruner.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3914013Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/BaseVertex.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3921838Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/MultiSampleEdge.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3936070Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/SeqGraph.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3937759Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/SeqGraph.java:4: error: package com.google.common.primitives does not exist; 2022-08-16T00:09:07.3941846Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/BaseGraph.java:3: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.3952011Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/readthreading/AbstractReadThreadingGraph.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3953755Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/readthreading/AbstractReadThreadingGraph.java:4: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.3960718Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/ReadLikelihoodCalculationEngine.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3962332Z src/main/java/org/broadinstitute/hellbender/utils/pairhmm/PairHMM.java:3: error: package com.google.common,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:8094,error,error,8094,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['error'],['error']
Availability,T00:09:07.4251408Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyBasedCallerUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4265184Z src/main/java/org/broadinstitute/hellbender/utils/read/AlignmentUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4267067Z src/main/java/org/broadinstitute/hellbender/utils/read/AlignmentUtils.java:4: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.4271200Z src/main/java/org/broadinstitute/hellbender/utils/downsampling/AlleleBiasedDownsamplingUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4272874Z src/main/java/org/broadinstitute/hellbender/utils/downsampling/AlleleBiasedDownsamplingUtils.java:4: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.4278681Z src/main/java/org/broadinstitute/hellbender/utils/Nucleotide.java:3: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.4292326Z src/main/java/org/broadinstitute/hellbender/utils/read/CigarUtils.java:3: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.4293163Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/ReferenceConfidenceModel.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4296749Z src/main/java/org/broadinstitute/hellbender/tools/walkers/qc/Pileup.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4303354Z src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/AlleleSubsettingUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4304128Z src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/AlleleSubsettingUtils.java:4: error: package com.google.common.primitives does not exist; 2022-08-16T00:09:07.4304857Z src/main/java/org/broa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:15918,error,error,15918,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['error'],['error']
Availability,T22:45:53.7779110Z symbol: class Range; 2022-08-16T22:45:53.7779574Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7780209Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:101: error: cannot find symbol; 2022-08-16T22:45:53.7780965Z static VCFHeaderLine rangeToVCFHeaderLine(Range<Integer> genotypeQualityBand) ***; 2022-08-16T22:45:53.7781896Z symbol: class Range; 2022-08-16T22:45:53.7782232Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7785096Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPruner.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7789228Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/BaseVertex.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7798240Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/MultiSampleEdge.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7810436Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/SeqGraph.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7857595Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/SeqGraph.java:4: error: package com.google.common.primitives does not exist; 2022-08-16T22:45:53.7861943Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/BaseGraph.java:3: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.7871768Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/readthreading/AbstractReadThreadingGraph.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7873510Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/readthreading/AbstractReadThreadingGraph.java:4: error: package com.g,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:9756,error,error,9756,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,1,['error'],['error']
Availability,T22:45:53.7781896Z symbol: class Range; 2022-08-16T22:45:53.7782232Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7785096Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPruner.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7789228Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/BaseVertex.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7798240Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/MultiSampleEdge.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7810436Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/SeqGraph.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7857595Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/SeqGraph.java:4: error: package com.google.common.primitives does not exist; 2022-08-16T22:45:53.7861943Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/BaseGraph.java:3: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.7871768Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/readthreading/AbstractReadThreadingGraph.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7873510Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/readthreading/AbstractReadThreadingGraph.java:4: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.7881195Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/ReadLikelihoodCalculationEngine.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7882811Z src/main/java/org/broadinstitute/hellbender/utils/pairhmm/PairHMM.java:3: error: package com.google.common,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:10132,error,error,10132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,1,['error'],['error']
Availability,T22:45:53.8180874Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyBasedCallerUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8265839Z src/main/java/org/broadinstitute/hellbender/utils/read/AlignmentUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8266584Z src/main/java/org/broadinstitute/hellbender/utils/read/AlignmentUtils.java:4: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.8267814Z src/main/java/org/broadinstitute/hellbender/utils/downsampling/AlleleBiasedDownsamplingUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8268598Z src/main/java/org/broadinstitute/hellbender/utils/downsampling/AlleleBiasedDownsamplingUtils.java:4: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.8269986Z src/main/java/org/broadinstitute/hellbender/utils/Nucleotide.java:3: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.8270563Z [0K; 2022-08-16T22:45:53.8270698Z [0K; 2022-08-16T22:45:53.8270832Z [0K; 2022-08-16T22:45:53.8272441Z [3A[1m<[0;32;1m====[0;39;1m---------> 33% EXECUTING [45m 31s][m[39D[1B[1m> :testOnPackagedReleaseJar > 1727 tests completed[m[50D[1B[1m> :testOnPackagedReleaseJar > Executing test org...help.DocumentationGeneration[m[79D[1B[3A src/main/java/org/broadinstitute/hellbender/transformers/DRAGENMappingQualityReadTransformer.java:3: error: package com.google.common.annotations does not exist[0K; 2022-08-16T22:45:53.8283859Z src/main/java/org/broadinstitute/hellbender/utils/read/CigarUtils.java:3: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.8284709Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/ReferenceConfidenceModel.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8288203Z src/main/java/org/br,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:17956,error,error,17956,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,1,['error'],['error']
Availability,T22:45:53.8270563Z [0K; 2022-08-16T22:45:53.8270698Z [0K; 2022-08-16T22:45:53.8270832Z [0K; 2022-08-16T22:45:53.8272441Z [3A[1m<[0;32;1m====[0;39;1m---------> 33% EXECUTING [45m 31s][m[39D[1B[1m> :testOnPackagedReleaseJar > 1727 tests completed[m[50D[1B[1m> :testOnPackagedReleaseJar > Executing test org...help.DocumentationGeneration[m[79D[1B[3A src/main/java/org/broadinstitute/hellbender/transformers/DRAGENMappingQualityReadTransformer.java:3: error: package com.google.common.annotations does not exist[0K; 2022-08-16T22:45:53.8283859Z src/main/java/org/broadinstitute/hellbender/utils/read/CigarUtils.java:3: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.8284709Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/ReferenceConfidenceModel.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8288203Z src/main/java/org/broadinstitute/hellbender/tools/walkers/qc/Pileup.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8294852Z src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/AlleleSubsettingUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8295684Z src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/AlleleSubsettingUtils.java:4: error: package com.google.common.primitives does not exist; 2022-08-16T22:45:53.8296401Z src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/AlleleSubsettingUtils.java:5: error: package com.google.common.primitives does not exist; 2022-08-16T22:45:53.8377958Z src/main/java/org/broadinstitute/hellbender/utils/runtime/ProcessControllerBase.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8390248Z src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ThresholdCalculator.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T2,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:19018,error,error,19018,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,1,['error'],['error']
Availability,"TCGA SNP validation looks about the same on WES---perhaps a slight tradeoff of sensitivity for specificity at the 0.1% level, but nothing to write home about. WGS validations are still running due to the (intermittent?) NIO failures discussed elsewhere.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5575#issuecomment-454570347:224,failure,failures,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5575#issuecomment-454570347,1,['failure'],['failures']
Availability,TK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$ensureBuildPrerequisites.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.run(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:143); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90); 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	... 58 more; 22:05:55.986 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] BUILD FAILED; 22:05:55.986 [LIFECYCLE] [org.gradle.internal.buildevents.BuildResultLogger] ; 22:05:55.987 [LIFECYCLE] [or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:13770,ERROR,ERROR,13770,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['ERROR'],['ERROR']
Availability,"TRIBBLE : fals; 08:27:10.888 INFO Mutect2 - Deflater: IntelDeflate; 08:27:10.889 INFO Mutect2 - Inflater: IntelInflate; 08:27:10.889 INFO Mutect2 - GCS max retries/reopens: 2; 08:27:10.889 INFO Mutect2 - Requester pays: disable; 08:27:10.889 INFO Mutect2 - Initializing engin; 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.<init>(ReferenceConfidenceModel.java:116); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticReferenceConfidenceModel.<init>(SomaticReferenceConfidenceModel.java:38); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.<init>(Mutect2Engine.java:149); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.onTraversalStart(Mutect2.java:286); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:982); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:3035,down,down,3035,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['down'],['down']
Availability,"TRIBBLE : false; 22:49:53.550 INFO CombineGVCFs - Deflater: IntelDeflater; 22:49:53.570 INFO CombineGVCFs - Inflater: IntelInflater; 22:49:53.571 INFO CombineGVCFs - GCS max retries/reopens: 20; 22:49:53.595 INFO CombineGVCFs - Requester pays: disabled; 22:49:53.597 INFO CombineGVCFs - Initializing engine; 22:49:53.869 INFO FeatureManager - Using codec VCFCodec to read file file:///mnt/d/projects/sequencing/gvcf/OPG0005F/OPG0005F.GATK.var.g.vcf; 22:49:53.923 INFO FeatureManager - Using codec VCFCodec to read file file:///mnt/d/projects/sequencing/gvcf/OPG0005M/OPG0005M.GATK.var.g.vcf; 22:49:53.939 INFO FeatureManager - Using codec VCFCodec to read file file:///mnt/d/projects/sequencing/gvcf/OPG0005P/OPG0005P.GATK.var.g.vcf; 22:49:54.132 INFO CombineGVCFs - Done initializing engine; 22:49:54.156 INFO ProgressMeter - Starting traversal; 22:49:54.157 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 22:49:54.193 INFO CombineGVCFs - Shutting down engine; [December 19, 2019 at 10:49:54 PM GMT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=156237824; java.lang.ArrayIndexOutOfBoundsException: Index -2 out of bounds for length 256; at org.broadinstitute.hellbender.utils.BaseUtils.convertIUPACtoN(BaseUtils.java:120); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.getSubsequenceAt(CachingIndexedFastaSequenceFile.java:326); at org.broadinstitute.hellbender.engine.ReferenceFileSource.queryAndPrefetch(ReferenceFileSource.java:78); at org.broadinstitute.hellbender.engine.ReferenceDataSource.queryAndPrefetch(ReferenceDataSource.java:64); at org.broadinstitute.hellbender.engine.ReferenceContext.getBases(ReferenceContext.java:197); at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.createIntermediateVariants(CombineGVCFs.java:216); at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.apply(CombineGVCFs.java:162); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6340:3108,down,down,3108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6340,1,['down'],['down']
Availability,"T] org.broadinstitute.hellbender.tools; .funcotator.Funcotator done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=1885339648; **org.broadinstitute.hellbender.exceptions.GATKException: Unable to query; the database for geneName: WASH7P**; ....; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.ins; tanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Mai; n.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); **Caused by: org.sqlite.SQLiteException: [SQLITE_IOERR_LOCK] I/O error i; n the advisory file locking logic (disk I/O error)**; at org.sqlite.core.DB.newSQLException(DB.java:909); ### Affected version(s); GATK 4.1.9.0. ### Description ; GATK Funcotator [SQLITE_IOERR_LOCK] I/O error in the advisory file locking logic (disk I/O error). I downloaded the data-sources by ""gsutil cp gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200521s.tar.gz ."". I can't find useful information for this error. Thank you. #### Steps to reproduce; Using GATK jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4.1.9.0/gatk-; package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_i; o_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjd; k.compression_level=2 -jar /lustre1/ruibinxi_pkuhpc/ljx/software/gatk-4; .1.9.0/gatk-package-4.1.9.0-local.jar Funcotator -R /home/ruibinxi_pkuh; pc/lustre1/ljx/reference_genomes/hg38_bwa/hg38.fa -V /home/ruibinxi_pku; hpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered.vcf.gz -O /home/ruibi; nxi_pkuhpc/lustre1/ljx/data/raodn/WES/GATK/P14P_filtered_funcotator.maf; --output-file-format MAF --data-sources-path /home/ruibinxi_pkuhpc/lus; tre1/ljx/reference_genomes/funcotator_dataSources.v1.7.20200521s/ --ref; -version hg38",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7474:1113,down,downloaded,1113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7474,2,"['down', 'error']","['downloaded', 'error']"
Availability,"TableCodec has traditionally taken advantage of a quirk of the htsjdk implementation of tabix indexing, where the input stream being indexed was closed and then reopened in between reading of the header and subsequent feature indexing. That quirk had several failure modes (see https://github.com/samtools/htsjdk/issues/393 and https://github.com/samtools/htsjdk/issues/943). These are fixed in https://github.com/samtools/htsjdk/pull/906, and the stream is no longer closed by htsjdk. However, TableCodec required a [modification](https://github.com/broadinstitute/gatk/pull/3403) in order to remain indexable with these fixes, due to its use of a CSV reader (indirectly through TableReader) that buffers input, which thwarts feature-by-feature indexing. We should find a better long term fix for this; either finding a way to prevent OpenCSV from buffering, or possibly using a different CSV implementation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3440:259,failure,failure,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3440,1,['failure'],['failure']
Availability,Tagged files aren't echoed back correctly as args,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3247:20,echo,echoed,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3247,1,['echo'],['echoed']
Availability,"TargetCodec is dependent on the old htsjdk indexing scheme whereby the indexer called readActualHeader on the codec first (for the side effect of initializing the codec header state), and then manually processed the feature file contents by:. - re-creating the input SOURCE/stream a second time; - NOT calling readActualHeader; - extracting and passing the features one at a time to the codec's decode method, using the stream position to find the feature file offsets. Although this scheme worked with TargetCodec, it had several other failure modes (see https://github.com/samtools/htsjdk/pull/906). With https://github.com/samtools/htsjdk/pull/906, the SOURCE/stream is only opened once for indexing. However, TargetCodec uses an underlying CSVReader that automatically buffers input, which confounds the indexer. This PR works around that issue for indexing. Note: this won't compile until there is an htsjdk snapshot available with https://github.com/samtools/htsjdk/pull/906.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3403:537,failure,failure,537,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3403,2,"['avail', 'failure']","['available', 'failure']"
Availability,TcGFyay5qYXZh) | `0% <0%> (-82.022%)` | `0 <0> (-23)` | |; | [...r/utils/solver/UnivariateSolverJobDescription.java](https://codecov.io/gh/broadinstitute/gatk/pull/4107/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zb2x2ZXIvVW5pdmFyaWF0ZVNvbHZlckpvYkRlc2NyaXB0aW9uLmphdmE=) | `0% <0%> (-100%)` | `0% <0%> (-6%)` | |; | [...ools/spark/pathseq/loggers/PSFilterFileLogger.java](https://codecov.io/gh/broadinstitute/gatk/pull/4107/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL2xvZ2dlcnMvUFNGaWx0ZXJGaWxlTG9nZ2VyLmphdmE=) | `0% <0%> (-100%)` | `0% <0%> (-8%)` | |; | [...ender/tools/ApplyBQSRUniqueArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4107/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9BcHBseUJRU1JVbmlxdWVBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-2%)` | |; | [...ender/utils/downsampling/ReservoirDownsampler.java](https://codecov.io/gh/broadinstitute/gatk/pull/4107/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9kb3duc2FtcGxpbmcvUmVzZXJ2b2lyRG93bnNhbXBsZXIuamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-21%)` | |; | [...er/tools/walkers/annotator/ReadPosRankSumTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4107/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9SZWFkUG9zUmFua1N1bVRlc3QuamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-10%)` | |; | [...ender/utils/codecs/sampileup/SAMPileupElement.java](https://codecov.io/gh/broadinstitute/gatk/pull/4107/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb2RlY3Mvc2FtcGlsZXVwL1NBTVBpbGV1cEVsZW1lbnQuamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-3%)` | |; | [...metrics/QualityYieldMetricsArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4107/diff?src=pr&e,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4107#issuecomment-356345394:2142,down,downsampling,2142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4107#issuecomment-356345394,1,['down'],['downsampling']
Availability,Temporarily swap in Corretto for Temurin as we can't download Temurin.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7969:53,down,download,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7969,1,['down'],['download']
Availability,Terrific! I'll merge once the bucket migration error gets resolved and that last failing test passes.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1551542600:47,error,error,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1551542600,1,['error'],['error']
Availability,Test failure is false alarm? @cmnbroad,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399568778:5,failure,failure,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399568778,1,['failure'],['failure']
Availability,Test failure is unrelated.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6628:5,failure,failure,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6628,1,['failure'],['failure']
Availability,"Tested using gatk-4.beta.6-151-g1ec409c-SNAPSHOT locally and with dataproc. Observed bug while testing commands for documentation updates in https://github.com/broadinstitute/gatk/pull/4068. . CollectInsertSizeMetricsSpark requires the `--histogramPlotFile` (`-H`, file to write insert size Histogram chart to) and current example commands add the `.pdf` extension to these files. The tool errors without this being specified but then doesn't write the file. In CollectBaseDistributionByCycleSpark, `--chart` (`-C`, A file (with .pdf extension) to write the chart to) is optional. When specified, the tool appears to ignore this option and does not write the file. . Metrics files defined by `-O` are written.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4085:390,error,errors,390,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4085,1,['error'],['errors']
Availability,"Testing branch `ck_3487_port_LeftAlignAndTrimVariants`, which ports LeftAlignAndTrimVariants from GATK3 to GATK4. ### stdout; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz -O zeta_snippet_leftalign.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz -O zeta_snippet_leftalign.vcf.gz; 16:34:35.251 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 05, 2018 4:34:35 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 16:34:35.413 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 16:34:35.414 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-24-gb43bc27-SNAPSHOT; 16:34:35.414 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:34:35.414 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 16:34:35.414 INFO LeftAlignAndTrimVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 16:34:35.414 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418875494:239,Down,Downloads,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418875494,2,['Down'],['Downloads']
Availability,"Testing updated branch with improved messaging.; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz -O zeta_snippet_leftalign_96branch.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz -O zeta_snippet_leftalign_96branch.vcf.gz; 12:55:31.964 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 06, 2018 12:55:32 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 12:55:32.083 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 12:55:32.083 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-25-g0c6f06f-SNAPSHOT; 12:55:32.083 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:55:32.083 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 12:55:32.083 INFO LeftAlignAndTrimVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 12:55:32.083 INFO LeftAlignAndTrimVariants - Start Date/Time: September 6",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326:162,Down,Downloads,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326,2,['Down'],['Downloads']
Availability,"Tests are ""failing"" with the ""code is too big"" error on the CNN testTrainingReadModel. I had to update my conda yml template to use a newer Tensorflow @cmnbroad found -- should I add that here too?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6330:47,error,error,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6330,1,['error'],['error']
Availability,"Tests with `TEST_DOCKER = true` failed, I'm not entirely clear why. Here's a bit of the log:. > Building 85% > :test > Resolving dependencies ':jacocoAgent'aven.org/maven2/org/jacoco/org.jacoco.agent/0.7.7.201606060606/org.jacoco.agent-0.7.7.201606060606.jar; > Building 85% > :test > 207 KB/233 KB downloaded> Building 85% > :test > 0 tests completed> Resolving dependencies ':testRuntime':test FAILED",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367:299,down,downloaded,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159#issuecomment-314208367,2,['down'],['downloaded']
Availability,"Thank you @SHuang-Broad. The error was gone after I copied bwaindeximage file to lustre file system, which can be accessed by all worker nodes.; The new problem is: the program started but didn't give any informative message/progress (see log below). It was stopped (Ctl-C) after 16 hours. The sequence data is regular human exome, which could be mapped in 1-2 hours in our traditional pipeline. ```; ../gatk/gatk-launch BwaAndMarkDuplicatesPipelineSpark ; -I hdfs://ln16/user/myname/NA12878/wes/NA12878-NGv3-LAB1360-A.unaligned.bam ; -O hdfs://ln16/user/myname/gatk4test/BwaAndMarkDuplicatesPipelineSpark_out.bam ; -R hdfs://ln16/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit ; --bwamemIndexImage /TEST/hadoop/myname/GRCh37.fa.img ; --disableSequenceDictionaryValidation ; -- --sparkRunner SPARK ; --sparkMaster spark://ln16:7077 ; --conf spark.cores.max=720 ; --executor-cores 20 ; --executor-memory 50g ; --conf spark.driver.memory=50g; Using GATK jar /home/myname/gatk4/gatk/build/libs/gatk-package-4.alpha.2-1125-g27b5190-SNAPSHOT-spark.jar; Running:; /opt/spark-2.1.0-bin-hadoop2.7/bin/spark-submit --master spark://ln16:7077 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOption; s=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.di$able=true --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --conf spark.cores.max=720 --executor-cores 20 --executor-memory 50g --conf spark.driver.memory=50g /home/myname/gatk4/gatk$build/libs/gatk",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:29,error,error,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['error'],['error']
Availability,"Thank you @cmnbroad .; > Java 17 uses strict floating point math by default. ; I didn't know this. I learned a lot. I downloaded and built gatk-4.4.0.0.; I realized that Log10Cache.java is not in gatk-4.4.0.0 but is in 4.3.0.0. In gatk-4.4.0.0, log10 value is calculated by Math.log10 directory.; Thus, I modified computeLogPenaltyScore in kBestHaplotype.java from Math.log10 to StrictMath.log10. I compared variant call results of original or modified to StrictMath class log10. ```; public static double computeLogPenaltyScore(int edgeMultiplicity, int totalOutgoingMultiplicity) {; // return Math.log10(edgeMultiplicity) - Math.log10(totalOutgoingMultiplicity);; return StrictMath.log10(edgeMultiplicity) - StrictMath.log10(totalOutgoingMultiplicity);; }; ```. Original gatk-4.4.0.0 and gatk-4.4.0.0 modified ver. from Math to StrictMath OpenJDK-17.0.6+10 output different results with this link bam ( https://pezycomputing-my.sharepoint.com/:f:/g/personal/sakai_pezy_co_jp/Eo5Gvfau1BpMszGCcfDrD14BOfMgxvk7Mt2JCFqcDfgItQ?e=wzZbpL ).; Same as gatk-4.3.0.0 openjdk-1.8.0; I previously examined Math.log10 output different result from StrictMath.log10 on x64 CPU but same result on arm CPU with following code. ```; import java.util.*;. public class log10_check {; public static void main(String[] args){; int n = 100;; for (int i = 2; i < n; ++i){; if (Math.log10(i) != StrictMath.log10(i)) {; System.out.printf(""i = %d, %20.16f, %20.16f\n"", i, Math.log10(i), StrictMath.log10(i));; } ; }; }; }; ```. Output on x64 CPU. On arm CPU, no output.; ```; i = 11, 1.0413926851582251, 1.0413926851582250; i = 40, 1.6020599913279623, 1.6020599913279625; i = 43, 1.6334684555795864, 1.6334684555795866; i = 52, 1.7160033436347992, 1.7160033436347990; i = 53, 1.7242758696007890, 1.7242758696007892; i = 85, 1.9294189257142926, 1.9294189257142929; i = 90, 1.9542425094393250, 1.9542425094393248; i = 92, 1.9637878273455553, 1.9637878273455551; i = 93, 1.9684829485539350, 1.9684829485539352; ```. Thus, gatk-4.4",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1560470696:118,down,downloaded,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1560470696,2,['down'],['downloaded']
Availability,"Thank you @mwalker174 . The input bamfile is about 7 GB. If no `--bamPartitionSize` is specified, the job would stuck at the first step `collect at ReadsSparkSource.java:220`, until we killed it. So I tried `--bamPartitionSize 4000000`, and it went through, but the Spark web interface showed errors in `sortByKey` steps:; ![sparkjob](https://user-images.githubusercontent.com/812850/27811313-9000019c-6097-11e7-82ac-aac557be31db.PNG).; And the program failed eventually:; ```; 18:24:57.885 INFO BwaAndMarkDuplicatesPipelineSpark - Shutting down engine; [July 3, 2017 6:24:57 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark done. Elapsed time: 269.29 minutes.; Runtime.totalMemory()=4172283904; org.apache.spark.SparkException: Job aborted due to stage failure: Task 607 in stage 3.0 failed 4 times, most recent failure: Lost task 607.13 in stage 3.0 (TID 14832, 12.9.68.0, executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 169939 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363:293,error,errors,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363,4,"['down', 'error', 'failure']","['down', 'errors', 'failure']"
Availability,"Thank you @mwalker174 for the suggestions. I ended up writing for loops to test which configurations work. Driver memory: 2-50g; executor memory: 2-50g; executor cores: 1-20; bamPartitionSize: 1-64m. Some combinations failed in minutes, some failed in hours, and some finished without errors. Bellow are three of which work for a ~33X WGS data:; ```; ../gatk-4.beta.1/gatk-launch BwaAndMarkDuplicatesPipelineSpark ; --bamPartitionSize 4000000 ; -I hdfs://bigdata/user/myname/gatk4test/wgs.sub4.unaligned.bam ; -O hdfs://bigdata/user/myname/gatk4test/wgs.sub4.BwaAndMarkDuplicatesPipelineSpark_out.bam ; -R hdfs://bigdata/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit ; --bwamemIndexImage /hadoop/myname/GRCh37.fa.img ; --disableSequenceDictionaryValidation ; -- --sparkRunner SPARK ; --sparkMaster spark://ln16:7077 ; --conf spark.cores.max=600 ; --executor-cores 20 ; --executor-memory 10g ; --conf spark.driver.memory=50g. ../gatk-4.beta.1/gatk-launch BwaAndMarkDuplicatesPipelineSpark ; --bamPartitionSize 4000000 ; -I hdfs://bigdata/user/myname/gatk4test/wgs.sub4.unaligned.bam ; -O hdfs://bigdata/user/myname/gatk4test/wgs.sub4.BwaAndMarkDuplicatesPipelineSpark_out.bam ; -R hdfs://bigdata/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit ; --bwamemIndexImage /hadoop/myname/GRCh37.fa.img ; --disableSequenceDictionaryValidation ; -- --sparkRunner SPARK ; --sparkMaster spark://ln16:7077 ; --conf spark.cores.max=600 ; --executor-cores 5 ; --executor-memory 50g ; --conf spark.driver.memory=50g. ../gatk-4.beta.1/gatk-launch BwaAndMarkDuplicatesPipelineSpark ; --bamPartitionSize 64000000 ; -I hdfs://bigdata/user/myname/gatk4test/wgs.sub4.unaligned.bam ; -O hdfs://bigdata/user/myname/gatk4test/wgs.sub4.BwaAndMarkDuplicatesPipelineSpark_out.bam ; -R hdfs://bigdata/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit ; --bwamemIndexImage /hadoop/myname/GRCh37.fa.img ; --disableSequenceDictionaryValidation ; -- --sparkRunner SPARK ; --sparkMaster spark://ln16:7077 ; --conf spark.core",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-313981314:285,error,errors,285,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-313981314,1,['error'],['errors']
Availability,"Thank you @tedsharpe for addressing my comments, I just have two additional requests:. 1. The tool still needs a Doc comment header (eg see [SelectVariants example](https://github.com/broadinstitute/gatk/blob/2d2cca2216842dfcf1f1ea028d9de68b23afd6b2/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants.java#L45)) to generate doc for the [GATK Tool Index](https://gatk.broadinstitute.org/hc/en-us/articles/4418051394587--Tool-Documentation-Index). It can be minimal with just ""Inputs"", ""Outputs"", and ""Usage examples"" sections.; 2. It would be nice to cut down on repeated code in the `EvidenceSortMerger` classes - it seems like the methods are all nearly the same, with the exception of the `Comparator` definition and the inner loop code in `resolveSameLocusFeatures()`. Could you move the common code up to an abstract `SVEvidenceSortMerger` class?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7695#issuecomment-1077778436:589,down,down,589,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7695#issuecomment-1077778436,1,['down'],['down']
Availability,"Thank you @vruano I am not in a hurry to see this fixed soon, but is there a minBQ that would make this class of problems disappear entirely? I would imagine that the same issue could still surface at any minBQ in other places of the genome. Also, ideally I would like to be able to run it with minBQ=20 as my analysis is very sensitive to sequencing errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6045#issuecomment-517399923:351,error,errors,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6045#issuecomment-517399923,1,['error'],['errors']
Availability,"Thank you for bringing this to our attention! ; This is closed by #8180, which fixes the edge case where the number of intervals in the analyzed contig is equal to two. ; Note that GermlineCNVCaller as well as any downstream tools do not support having only a single interval on any provided contig.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8097#issuecomment-1419460987:214,down,downstream,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8097#issuecomment-1419460987,1,['down'],['downstream']
Availability,"Thank you for fast reply. @python from the same shell where you activated the conda env, and see if the command import gatktool succeeds. Yes, this works well. but when I run the command below and submit the job to HPC ;. conda activate gatk4 ; gatk CNNScoreVariants -V 21002.HaplotypeCaller.output.g.vcf.gz -R hg19.fa -O annotated.vcf. same error happened. another point is that when I run the command (gatk CNNScoreVariants -V 21002.HaplotypeCaller.output.g.vcf.gz -R hg19.fa -O annotated.vcf) without HPC by setting memory as qlogin -l s_vmem=16G -l mem_req=16G. it produces this:. Runtime.totalMemory()=2132803584; org.broadinstitute.hellbender.exceptions.GATKException: Exception waiting for ack from Python: org.broadinstitute.hellbender.exceptions.GATKException: Expected message of length 3 but only found 0 bytes; 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.waitForAck(StreamingProcessController.java:233); 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.waitForAck(StreamingPythonScriptExecutor.java:216); 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.waitForPreviousBatchCompletion(StreamingPythonScriptExecutor.java:293); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.sendBatchIfReady(CNNScoreVariants.java:416); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.firstPassApply(CNNScoreVariants.java:336); 	at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.nthPassApply(TwoPassVariantWalker.java:17); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemain",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-895854147:342,error,error,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-895854147,1,['error'],['error']
Availability,"Thank you for looking into it. I am curious if that is really the problem. If the reference files were causing problems, shouldn't that impact all samples? I am seeing this error with some, but not most of the samples. Even using a different matched control sample with the same tumor sample will cause or fix the error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-478308972:173,error,error,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-478308972,2,['error'],['error']
Availability,"Thank you for looking into this and sharing the analysis. I always believed that there is _a lot_ to be gained from a decent coverage collection strategy. For example, I have seen Genome STRiP cleanly resolving cases that are essentially impossible to resolve from our raw data. Perhaps we should:. (1) Include genome mappability analysis tracks as a filtering strategy w/ or w/o MQ-based filtering. We can download fairly accurate mappability data based on noisy Illumina-like paired-end reads from here:; https://sourceforge.net/p/gma-bio/wiki/Home/; They have a decent publication too. (2) While a simple fragment-based coverage collection has major pitfalls, I am not quite convinced that one must throw away fragment information altogether. By theoretically considering various SV events (tandem duplication, disperse duplication, deletion, inversion, inter- and intra-contig translocation, etc.), and studying paired-end reads coming from various parts of such SVs case by case and how they would theoretically align to the reference, we can come up with a heuristic counting strategy that gives the most consistent signal for downstream tools. This analysis requires taking into account basic summary statistics such as read and fragment length distribution in order to resolve anomalous fragments to putative SV events. I have worked out a few cases and this is fairly doable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372064064:407,down,download,407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372064064,4,['down'],"['download', 'downstream']"
Availability,"Thank you for replying!; I have parallelized the GATK4 Mutect2 using thread pools in Java. I tested the parallelized GATK4 Mutect2 using a WGS data with control. The result came out that, about 0.6% variants were different from the original results. I found that the difference was caused by the random number generator in ReservoirDownsampler. The order of the input intervals after parallelism were different from the original, so the random numbers generated for each position with redundant reads were possibly different. Is there any solutions for this problem?; Thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4325#issuecomment-382586592:485,redundant,redundant,485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4325#issuecomment-382586592,1,['redundant'],['redundant']
Availability,Thank you for the additional information! Hopefully this will be helpful for the team tracking down these issues. There's going to be a new build of the GKL soon which I'm hoping will fix this.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-654398146:95,down,down,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-654398146,1,['down'],['down']
Availability,"Thank you for the speedy review @davidbenjamin. I agree with you that the obvious place to trim the alleles is in `removeAltAllelesIfTooManyGenotypes(ploidy, alleleMapper, mergedVC)` as it is the place where we actually edit the output. Indeed my first attempt at this fix was to make that change. Unfortunately, because the `readAlleleLikelihoods` object is constructed with the un-trimmed alleles in the `alleleMapper` was causing failures because the Liklihoods object would have mismatching alleles. To fix `removeAltAllelesIfTooManyGenotypes(ploidy, alleleMapper, mergedVC)` we would have to edit the alleleMapper object, which would be difficult given that I would prefer to just use the allele trimming library object. . Another proposal would have been to just hold onto the `mergedVC` object before we cull the extra alleles and then just compare the alleles at the end. Unfortunately due to engine code optimizations we have enabled an unsafe allele list copy for these alleles in the HaplotypeCaller (to save ourselves the cost of allocating dozens of identical ArrayLists to store Haplotypes every time we use the VariantContextBuilder). . To clarify, it is possible to move the check to the right place its likely to force me to write a non-library implementation of the trimming code that tracks what edits it made and I was trying to avoid doing that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6044#issuecomment-512355693:433,failure,failures,433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6044#issuecomment-512355693,1,['failure'],['failures']
Availability,Thank you for your suggestion. Haplotypecaller successed and no error reported when I used gatk v4.5.0.0.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8984#issuecomment-2359721380:64,error,error,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8984#issuecomment-2359721380,1,['error'],['error']
Availability,"Thank you so much, I will do that. Sincerely,; Emily. From: ldgauthier ***@***.***>; Sent: Monday, March 28, 2022 2:39 PM; To: broadinstitute/gatk ***@***.***>; Cc: Emily Elizabeth Puckett (puckett3) ***@***.***>; Mention ***@***.***>; Subject: Re: [broadinstitute/gatk] CombineGVCFs: ERROR input alleles must contain <NON_REF> (Issue #7737). CAUTION: This email originated from outside of the organization. Do not click links or open attachments unless you recognize the sender and trust the content is safe. If I'm reading the process correctly, I don't actually think this should work. CombineGVCFs is specifically for combining GVCFs and it expects GVCFs to have <NON_REF> alleles. If you've already run the data through GenotypeGVCFs then you can't use CombineGVCFs again because the <NON_REF> likelihoods have been applied and those alleles are gone. The vcfcombine tool from bcftools is quite fast if all you want to do is join the samples together. -; Reply to this email directly, view it on GitHub<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fbroadinstitute%2Fgatk%2Fissues%2F7737%23issuecomment-1081062021&data=04%7C01%7CEmily.Puckett%40memphis.edu%7C51db6aa9f41b483e1ce408da10f2aa5d%7Cae145aeacdb2446ab05a7858dde5ddba%7C0%7C0%7C637840931685525269%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=Pxg8joQfE51l5e3cUUbKA9bQEYDZjp0AxdX0aqDG1MY%3D&reserved=0>, or unsubscribe<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FALDFEHAXSKZ7YHSFGISLPUTVCIDGZANCNFSM5RZSK5PA&data=04%7C01%7CEmily.Puckett%40memphis.edu%7C51db6aa9f41b483e1ce408da10f2aa5d%7Cae145aeacdb2446ab05a7858dde5ddba%7C0%7C0%7C637840931685525269%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=6Dkb6rbHDZpS05bYUHhlIRHJitgVtR%2FPB5rNHHFMg%2FQ%3D&reserved=0>.; You are receiving this because you were mentioned.Message I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7737#issuecomment-1082170127:285,ERROR,ERROR,285,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7737#issuecomment-1082170127,1,['ERROR'],['ERROR']
Availability,"Thank you very much @cmnbroad . I really appreciate your help. I tried all the things you suggested, but I still get the error.; The program was installed in a singularity container using docker img. Do you think tha it might be a java issue?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6241#issuecomment-548846823:121,error,error,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6241#issuecomment-548846823,1,['error'],['error']
Availability,"Thank you very much for your response, it has been very helpful to me. There is a reason why we used a Panel of Normals with a significantly smaller sample size. Since we are using the BGI T7 sequencing platform, we wanted to create a Panel of Normals specific to our sequencing workflow. However, due to various constraints such as cost and time, the current Panel of Normals only includes 27 samples, which is indeed insufficient. That is why I specifically indicated in the file name of the Panel of Normals that it contains only 27 samples. . In fact, I had previously downloaded the Mutect2-exome-panel.vcf from GATK best practices. Initially, I did not realize that using a self-built Panel of Normals with such a small sample size would yield inferior results compared to GATK best practices' Mutect2-exome-panel.vcf. Once we have accumulated enough control samples, I plan to rebuild the Panel of Normals, and merge it with GATK best practices' Mutect2-exome-panel.vcf. I hope this will achieve better results. Furthermore, regarding clinical reports, we conduct multiple rounds of manual review before sending them to ensure that false positives like these are not reported. Thank you again for your reply and suggestions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8340#issuecomment-1614018856:573,down,downloaded,573,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8340#issuecomment-1614018856,1,['down'],['downloaded']
Availability,"Thank you! That was indeed the issue. Hunting down the exact way to install lfs from the command line took a little time, so I'll add the steps here in case, feel free to add them to your readme.; `sudo` was cut out since the docker I'm using doesn't like it and says it's not a valid command (based on debian 9).; ```; curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; apt install -y git-lfs; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6019#issuecomment-507519267:46,down,down,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6019#issuecomment-507519267,1,['down'],['down']
Availability,"Thank you!. On Tue, Oct 16, 2018 at 9:29 AM meganshand <notifications@github.com> wrote:. > @SusieX <https://github.com/SusieX> Sorry for the delay, we're just; > trying to finalize some potentially disruptive changes before we merge.; > I'll open an issue so I can ping you there once this PR has been both; > merged and released. Thanks!; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5193#issuecomment-430237566>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AQ4DHqWjVP9RQyGuRbbM-Nds_8l0RDtSks5uld84gaJpZM4WqV76>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-430238072:266,ping,ping,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-430238072,1,['ping'],['ping']
Availability,"Thank you, but I still got the error. Will post the full thing on that link you are sending though!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6350#issuecomment-571664268:31,error,error,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6350#issuecomment-571664268,1,['error'],['error']
Availability,Thanks @V-Z. I also just noticed that it's not a reference that I'm familiar with (I work on human cancer; Arabidopsis doesn't come up all that often). Could you send me the reference fasta or tell me where to download it?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-402249931:210,down,download,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-402249931,1,['down'],['download']
Availability,"Thanks @bbimber, I have the file downloaded, please go ahead and remove it. I have reproduced the seg fault - it is because of one faulty fragment - `WGS_Sept_1117.gdb/QNVO02001147.1$1$28016/__58eb4600-9c95-425e-a585-4bb23b486577140340709598976_1596797671601`. Not sure why, but it seems to be missing a number of files(e.g. `__coords.tdb`) causing the fault. Things work fine if you move the fragment out of the array. . For what its worth, all the samples from the callset.json seem to be represented in the rest of the fragments. So you might be fine with downstream processing with `__58eb4600-9c95-425e-a585-4bb23b486577140340709598976_1596797671601` removed for now. . For our debugging, . 1. Was the GenomicsDBImport run in an update mode at all after an initial import?; 2. Do you have any logs from `gatk GenomicsDBImport` for the `QNVO02001147.1$1$28016` array? ; 3. Was anything reported in the Lustre logs?; 4. I have attached the samples found in the callset.json for the `QNVO02001147.1$1$28016` array, can you confirm these were the expected samples? Are there any missing?. [samples.txt](https://github.com/broadinstitute/gatk/files/5460536/samples.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-718941513:33,down,downloaded,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-718941513,5,"['down', 'fault']","['downloaded', 'downstream', 'fault', 'faulty']"
Availability,"Thanks @cmnbroad - I just propose a super-fine grained scheme, but it can be a different one. I would like that for easier development using GATK4 for downstream projects, but by now I am fine with all dependencies generating quite a big jar, but I can live with it until this happen...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3900#issuecomment-348965479:151,down,downstream,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3900#issuecomment-348965479,1,['down'],['downstream']
Availability,"Thanks @davidbenjamin - my thought here was that since it's an available annotation in HaplotypeCaller it would be nice to just add a line to the release notes in case anyone else had started emitting it. To answer your question, we have been emitting it in a clinical pipeline, but haven't actually been using it for downstream filtering yet.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5848#issuecomment-480072316:63,avail,available,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5848#issuecomment-480072316,2,"['avail', 'down']","['available', 'downstream']"
Availability,"Thanks @davidbenjamin! To start with, I'll need a Mutect2 command line and some shareable data that will give meaningful profiling results. We plan to run the experiments at Intel, so we'll have to figure out how to get the data there (download or copy to a disk).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-291288892:236,down,download,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-291288892,1,['down'],['download']
Availability,"Thanks @davidbenjamin. . For what it is worth, here are some of my thoughts. From my perspective, it might helpful to separate the discussion into read likelihood calculation and the genotype likelihood calculation. As I understand it, sequencing error is directly relevant to the pairHMM read likelihood calculation. I guess the location where PCR error is most relevant (read likelihood vs genotype likelihood) would depend on whether the read likelihood is computing a ""fragment likelihood"" (the likely DNA sequence of the fragment being sequenced) or a ""haplotype likelihood"" (the likely source haplotype for the fragment being sequenced). In any case, as I see it, this particular issue could be interpreted as a shortcoming in only the HaplotypeCaller genotype likelihood calculation, and it would essentially be an issue of double counting. So I'm not sure that a quick fix for this issue is necessarily off the table...Maybe something like one of the following could be used just before or during the genotype likelihood calculation:; - something like `filterOverlappingReads` from Mutect2 (https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/SomaticGenotypingEngine.java#L326); - separate consideration of overlapping reads when calculating genotype likelihoods, as used in UnifiedGenotyper (https://github.com/broadgsa/gatk-protected/blob/aa8764d6c3de146856b174a8674fa787a6311d7c/protected/gatk-tools-protected/src/main/java/org/broadinstitute/gatk/tools/walkers/genotyper/DiploidSNPGenotypeLikelihoods.java#L183). As I see it, #4958, which seems to be more related to read likelihood calculation, is where a more involved solution, with more fundamental changes, might be warranted. From my perspective (not being especially familiar with the pairHMM model), an ideal solution would transition the pairHMM from read likelihood to a ""fragment likelihood"" or ""haplotype likelihood"" when information from read pairs is available,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5436#issuecomment-443558420:247,error,error,247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5436#issuecomment-443558420,2,['error'],['error']
Availability,"Thanks @droazen & @ldgauthier. I can certainly run a bunch more iterations of the same HC run on the same data. I'm not super hopeful it will turn anything up though. I can also try selecting a bunch of the different PairHMM implementations. I can't share too much, but this issue turned up in a very high throughput (1000s of samples a day) clinical pipeline. We're going back and looking for other instances where we see an excess of that `Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null` message, and re-running those samples to see if, on re-run, they generate different outputs. I realize the AVX-specific hardware issue is perhaps a little far-fetched, though given the volume of the pipeline and the fact that it runs in a cloud environment, I think it's entirely reasonable to suspect we'll run into hardware/instance issues occasionally. And there are AVX or at least SIMD specific registers, so if one of those were to see problems that could cause the PairHMM issues, without causing issues in other software that doesn't leverage the SIMD/AVX instructions. My main question really is this: is anyone familiar enough with the Intel PairHMM implementation and interface that they could weigh in on whether or not unexpected hardware errors could result in the return of empty likelihoods from the PairHMM instead of some kind of error, exception or segfault?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6889#issuecomment-709555915:1284,error,errors,1284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6889#issuecomment-709555915,2,['error'],"['error', 'errors']"
Availability,Thanks @erniebrau - with GKL 0.7 I do not experience any failure anymore!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3532#issuecomment-332446347:57,failure,failure,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3532#issuecomment-332446347,1,['failure'],['failure']
Availability,Thanks @gbggrant! @droazen @ldgauthier I just pushed a branch that resolves the error by simplifying the evidence-to-index cache.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-626448066:80,error,error,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-626448066,2,['error'],['error']
Availability,Thanks @jamesemery. The compile errors will be fixed once Disq 0.2.0 is released.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5485#issuecomment-454749917:32,error,errors,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5485#issuecomment-454749917,1,['error'],['errors']
Availability,"Thanks @jonn-smith, I really appreciate your work in supporting hg38. In case its helpful, I gave your files a go was met with a pretty abrupt error and exit. ... testing on 4.0.2.1. ```; 02:48:07.582 INFO Funcotator - Initializing engine; 02:48:07.988 INFO FeatureManager - Using codec VCFCodec to read file file:///cluster/jasonw/Work/pd.vcf; 02:48:08.012 INFO Funcotator - Done initializing engine; 02:48:08.047 INFO Funcotator - Shutting down engine; [March 22, 2018 2:48:08 AM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1948254208; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:330); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:897); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:288); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-375162106:143,error,error,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-375162106,2,"['down', 'error']","['down', 'error']"
Availability,"Thanks @lbergelson - nice to meet you too. Sorry for the delay here. I had to set up gsutils on my system and am having gdb issues. . Submitting `sudo gdb /nfsdata-tmp/tools/gatk /home/bduser/mepowers/core.114856` I get back . ```; Missing separate debuginfo for the main executable file; Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/6c/../../../jvm/java-1.8.0-openjdk-1.8.0.111-1.b15.el7_2.x86_64/bin/java; Core was generated by `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samt'.; Program terminated with signal 6, Aborted.; ```; I did try the yum --enablerepo, but it am getting the same error. . Any quick workarounds? Thanks in advance for the help. Will try again on Monday.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-466588771:642,error,error,642,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-466588771,1,['error'],['error']
Availability,"Thanks @lbergelson for the solution above. I just went into this conda env problem when trying to use CNNScoreVariants. By downgrading typing_extensions to 4.1.1, it works fine now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7800#issuecomment-1104972403:123,down,downgrading,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7800#issuecomment-1104972403,1,['down'],['downgrading']
Availability,"Thanks @lbergelson! I agree that it might be good to break into more layers—could be worth talking to SV team and seeing what lessons they learned in putting together their hierarchy of images. Also, note that I pushed the install of miniconda into the base, but I did not push down the setup of the GATK conda environment itself (which takes the bulk of the time during the main-image build, as it requires lots of downloading). I think I commented elsewhere that a good strategy might be to set up the conda environment with the non-GATK python dependencies in the base, and then update the environment via a pip install of the GATK python packages in the main image. This would let us make python code changes without having to rebuild the base, but might require a bit of scripting to create a final yml for non-Docker users. I also agree that it would be nice to cut down the Travis time, might be worth taking a look at other strategies to do that—could save everyone a lot of time!. Will try to add the test you suggested sometime tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-621487662:278,down,down,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-621487662,6,['down'],"['down', 'downloading']"
Availability,Thanks @lbergelson. Looks like the repo hasn't been updated since March. Recommend pinging Paolo from Intel and asking who the current maintainer is.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5393#issuecomment-437056873:83,ping,pinging,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5393#issuecomment-437056873,1,['ping'],['pinging']
Availability,Thanks @ldgauthier! That canary certainly looks alive to me. Happy to merge whenever you and/or Variants team approve.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1062112049:48,alive,alive,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1062112049,1,['alive'],['alive']
Availability,"Thanks @ldgauthier, that's definitely thought provoking. Perhaps you are right that for het-non-ref (i.e. 1/2 and similar) genotypes we should consider skipping the strand bias test. I had been assuming, incorrectly, that in a single sample case with a `1/2` genotype that the test would be based on the called alleles, not ref vs. alt. Just thinking out loud about this, I wonder if computing SOR on the `1/2` alleles would catch a small number of incorrect genotypes? I've been doing something analogous with allele balance filtering - whereby I compute a per-sample allele balance and directionality. If the genotype is imbalanced towards ref, I filter the variant. If the genotype is imbalanced towards a different allele I ""correct"" the genotype to be homozygous for that allele. In testing on reference samples from GIAB and PlatGen we are seeing that correct a handful of genotypes where the sample is really hom-alt, but called het due to a high error rate. . I'll read up on how the allele-specific strand bias works because I've never looked at that before. But I wonder if it's possible from the available annotations to get a SOR-like value for each allele that is that allele _vs_ all other alleles?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5698#issuecomment-466538825:954,error,error,954,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5698#issuecomment-466538825,2,"['avail', 'error']","['available', 'error']"
Availability,"Thanks @mwalker174 , we did a little digging and may have found a sample swap in the sample set being processed, but the swap is not the sample identified in the error log. I've got a couple more runs going now, hopefully I'll have more information soon.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8834#issuecomment-2116153640:162,error,error,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8834#issuecomment-2116153640,1,['error'],['error']
Availability,"Thanks @mwalker174! I think I responded to or addressed everything. The code paths for reading TSVs all go through the abstract CNV collection classes. Those require a bit of boilerplate, but were IMO a huge improvement over the horrowshow of utility methods from the old code... Happy to discuss possible further refactoring and improvement (and there are already catch-all issues open), if needed. If we decide to stream other locatable collections, we can start to extract more of these streaming/subsetting methods to `AbstractLocatableCollection`, which would give us something like the `LocatableTableReader` you're envisioning in your edit. We've discussed using @jonn-smith's `XSVLocatableTable` machinery as well. I think the only downsides are the conventional reliance on extensions/config files for decoding, as well as the need to accommodate CNV headers. Encoding is also not handled. We also still need to represent non-Locatable TSVs, ideally with a minimal number of code paths, although that probably won't present any major refactoring issues. Also recall that we discussed moving from Files -> Paths in previous PRs, so we should instead go from Files -> FeatureDataSources where it makes sense.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6266#issuecomment-558720770:740,down,downsides,740,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6266#issuecomment-558720770,1,['down'],['downsides']
Availability,"Thanks @samuelklee , I will incorporate your conda update into this branch, now that we've dealt with the test failures!. I patched the VETS test code to include the h5diff (and diff) output in the exception messages when one of these commands fails, and switched to the existing `BaseTest` methods for running the process and capturing the output. You can see what the output looks like (when we remove the epsilon tolerance) here:. https://storage.googleapis.com/hellbender-test-logs/build_reports/8610/merge_7165443572.3/tests/testOnPackagedReleaseJar/classes/org.broadinstitute.hellbender.tools.walkers.vqsr.scalable.ScoreVariantAnnotationsIntegrationTest.html. https://storage.googleapis.com/hellbender-test-logs/build_reports/8610/merge_7165443572.3/tests/testOnPackagedReleaseJar/classes/org.broadinstitute.hellbender.tools.walkers.vqsr.scalable.TrainVariantAnnotationsModelIntegrationTest.html. As you suspected/hoped, all the differences were tiny. When you have a chance, could you please review these changes to the VETS tests and let me know if you spot any issues?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1850563977:111,failure,failures,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1850563977,2,"['failure', 'toler']","['failures', 'tolerance']"
Availability,"Thanks @vidprijatelj. I see the [sticky bit](https://www.redhat.com/sysadmin/suid-sgid-sticky-bit) being used for groups for the workspace - `# flags: -s-`. That, by itself, seems to be OK, that is I am not able to reproduce the issue. But it looks like std::vector is not able to resize - `Caused by: java.io.IOException: GenomicsDB JNI Error: vector::_M_default_append`. What are the permissions to your tmp directory? Does it also have the sticky bit set? Even if the workspace only requires read permissions, GenomicsDB and probably the underlying standard C++ runtime may require write access to tmp and the sticky bit may be affecting the execution. . Also, can you please confirm that the user creating the workspace and the user reading from the workspace are the same?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1476526522:338,Error,Error,338,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1476526522,1,['Error'],['Error']
Availability,"Thanks Chris!; Yes, you are right! I tested the index with **SelectVariants** on different intervals, and they are all retrievable with ease and no errors! Now the indexing step can be confirmed right and the problem must be something else.; Thank you so much again for this advice.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5917#issuecomment-490171138:148,error,errors,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917#issuecomment-490171138,1,['error'],['errors']
Availability,"Thanks a lot @davidbenjamin ; In the meantime I have compiled the master branch when I saw this issue was resolved and it worked fine. I tried also to create a pon with this fresh compiled version but I got some errors (don't remember exactly what right now). Looks like you are in the middle of changing the pipeline of pon creation by integrating GenomicsDB as an intermediate, right ? Do you think it will also be ready for this next release ? I would like to create the pon with the same GATK version. Problem is I can not fall back on an earlier version because I would definitely get the bug we are talking about in this thread :-). Perhaps we should also change our computing nodes at some point I guess :). Thanks again",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470662216:212,error,errors,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470662216,1,['error'],['errors']
Availability,"Thanks a lot for taking care of this. I met different error this time. I; git clone it from the master. version info: The Genome Analysis Toolkit; (GATK) v4.1.5.0-16-gf1aea57-SNAPSHOT. I ran four samples. All return the same problem when just started the; haplotypecaller. Here is the error message.; [March 17, 2020 1:54:08 PM EDT]; org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller; done. Elapsed time: 1.09 minutes.; Runtime.totalMemory()=4182245376; java.lang.IllegalStateException: Smith-Waterman alignment failure. Cigar =; 64M40D29M with reference length 133 but expecting reference length of 413; ref =; AGCATCCGACAGCCTGGAGCAGCACCCACACCCCCAGTTGAGCAACTGATGGTCTGGAGCAGCACCCACAACCACAGGTGAACATCAGAGAGTCTGGAGCAGCGCCCACAACCCCAGGCGAGCATCTGACAGCCTGGAGCAGTGCCCAAACACCCAGGTGAGCATCTGACAGCATGGAGCAGCACCCATAGCCCAAGGTGAGCATCTGACAACCTGGAGCAGCACCCACACCCCGAGGTGAGCATCTGACCTCCCGGAGCAGGACCCATACCTCCAGGCGAGCATCTGAACCCATGGAGCAGCACCCACGCCCCCAGGCGAGCATCTGACCGAACAGAGCAGCACCCACAACCCCATGCGAGCATCTGTCAGCCTGGAACAGCACCCACAACCCCAGGTGAGCATCTGACAGC; path; AGCATCCGACAGCCTGGAGCAGCACCCACACCCCCAGTTGAGCAACTGATGGTCTGGAGCAGCACCCACAACCCCAGGTGAGCATCTGACAGC. David Benjamin <notifications@github.com> 于2020年3月17日周二 下午12:49写道：. > @phpeters <https://github.com/phpeters> I expect that the fix in #6498; > <https://github.com/broadinstitute/gatk/pull/6498> corrected both leading; > and trailing deletions, but please let us know if it did not.; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/6490#issuecomment-600179222>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AEORBD6XHZDY2RBOTVNGQODRH6SY3ANCNFSM4LENWAMQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6490#issuecomment-600218205:54,error,error,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6490#issuecomment-600218205,3,"['error', 'failure']","['error', 'failure']"
Availability,"Thanks a lot, @ddrichel. Given that the current Mutect2 release is still broken on both tumor-normal and tumor-only WES data, and downgrade is not possible on production systems due to the log4j vulnerability: is there any path forward for users that care for both accuracy and security, @davidbenjamin and @droazen ?. I fear waiting for Mutect3 isn't an option since even when it is finished there won't be independent benchmarks available for it for quite a while. Also, I suspect (as any other software product) the new version will have bugs, too, until it has matured in production. Therefore I'd suggest that identifying, understanding and fixing the bug in the current Mutect2 release would be the wisest path forward - do you agree?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1534464682:130,down,downgrade,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1534464682,2,"['avail', 'down']","['available', 'downgrade']"
Availability,"Thanks a lot. I am trying to run `mutect2` in **tumour-only** mode for which I need a panel of normal (PON). I have tried **somatic-hg38_1000g_pon.hg38.vcf.vcf** which gives . `./gatk Mutect2 -R resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta -I /data/Continuum/WES/testAlignmentBROADGenome/results/NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup_PicMD.bam -O 3.mt2.vcf -tumor NG-27280_CLTSS_LTS_001A_lib506241_7636_2_MarkedDup_PicMD.targeted_sequencing.sample_name --af-of-alleles-not-in-resource 2.5e-06 --germline-resource af-only-gnomad.hg38.vcf.gz -pon somatic-hg38_1000g_pon.hg38.vcf.vcf`. A USER ERROR has occurred: Cannot read file:///data/somatic-hg38_1000g_pon.hg38.vcf.vcf because no suitable codecs found. I know **gatk4_mutect2_4136_pon.vcf.gz** locates here for which I should register in GDC but because I am a postdoctoral researcher, I can not register. [1]: https://gdc.cancer.gov/about-data/gdc-data-processing/gdc-reference-files. Could you please help me to run mutect2 in tumour-only mode using another publicly available PON?. Thanks for any help",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8477:612,ERROR,ERROR,612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8477,2,"['ERROR', 'avail']","['ERROR', 'available']"
Availability,"Thanks for adding this! Let me discuss further with @mwalker174 to understand the need and typical use cases (e.g., combining fixed-grid bins) to make sure we don't run into any gotchas downstream. I'll try to review by EOD, but in the meantime, you might want to address a few issues I see at first glance:. 1) Correct the name of the tool (PreprocessIntervals) in the commit message and description.; 2) Add descriptions of the new parameters to the tool Javadoc.; 3) Amend the corresponding WDL task and expose the new parameters in all relevant germline and somatic WDLs.; 4) We should be sure to update the relevant documentation for all germline and somatic WDLs, which emphasizes how PreprocessIntervals should be run differently for WES and WGS, if we plan on changing the default behavior of the tool in the future.; 5) Tests are failing due to a compilation warning about a redundant cast to int.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5701#issuecomment-465978387:186,down,downstream,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5701#issuecomment-465978387,2,"['down', 'redundant']","['downstream', 'redundant']"
Availability,"Thanks for all the help. Using the 4.2.0.0 docker resolved the problem!. On Tue, Mar 23, 2021 at 10:06 AM Chris Norman ***@***.***>; wrote:. > Oh, I missed that they had already tried reindexing. @ahaessly; > <https://github.com/ahaessly> can you find out what tool and what version; > was used to reindex this file ? We've had several reports of this error; > message in the last month or two, but reindexing has usually fixed it. I; > had been assuming that the corrupt index files were created by older; > versions of GATK/Picard tools, but it would super helpful to know if; > current versions of either samtools or GATK/Picard is creating these, and; > will help determine if we need to relax this check.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/7152#issuecomment-804930832>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFO2VRJSHCFOODG56CXSO4TTFCN53ANCNFSM4ZNOBRZQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7152#issuecomment-804963976:352,error,error,352,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7152#issuecomment-804963976,1,['error'],['error']
Availability,"Thanks for bringing this to our attention, @Tintest. I think that we may be able to address this by setting `base_compiledir` via `os.environ[""THEANO_FLAGS""]` appropriately (see http://deeplearning.net/software/theano/library/config.html). @mbabadi @cmnbroad any thoughts? . In any case, thanks for trying out the GermlineCNVCaller pipeline. You may have to tune some parameters, depending on your data type. You may find the following discussions helpful:. https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. https://github.com/broadinstitute/gatk/issues/4719. Note that we're still in beta, but our preliminary evaluations have demonstrated improved performance over other callers in both WES and WGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432:558,error,error,558,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432,1,['error'],['error']
Availability,"Thanks for bringing this up! I actually think that I prefer option 1, although not ideal (since, as you say, it places more burden on the user). The whole point of having generically parameterized models is that we can apply them to many data types. To single out a few with hardcoded sets of defaults seems like a slippery slope to me. (Of course, we should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:609,down,downside,609,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289,1,['down'],['downside']
Availability,"Thanks for fixing the message! I couldn't track down the original `output.vcf` that caused the issue, and this is ancient enough that I can't even recall the context. However, because the site `1262288` only appears in our repo in the simulated gCNV test data, I suspect that this VCF was generated at some point during development of VCF output for PostprocessGermlineCNVCalls. Not sure in what way that VCF might have been invalid (I'm pretty sure the VCFs produced by that tool now are valid), but I can try to reproduce when I get a chance and will reopen if necessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6076#issuecomment-517813352:48,down,down,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6076#issuecomment-517813352,1,['down'],['down']
Availability,"Thanks for running the tests. I was hopeful that that would fix it but it; sounds something else is going on that needs investigation. On Thu, Mar 2, 2023, 3:51 PM danieljrichard ***@***.***>; wrote:. > Hi again,; > I tried installing java8 and switching to this version prior to running; > gatk. It runs and looks to be running the right Java, but spits out roughly; > the same error:; >; > Thoughts?; >; > /cold/drichard/gatk/./gatk --java-options ""-Xmx25g"" SplitNCigarReads; > -R /cold/drichard/VARIANTS/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I; > subset_TINY_rehead.bam; > --tmp-dir /thing -O thing.bam; > Using GATK jar; > /cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false; > -Dsamjdk.use_async_io_write_samtools=true; > -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2; > -Xmx25g -jar; > /cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar; > SplitNCigarReads -R; > /cold/drichard/VARIANTS/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I; > subset_TINY_rehead.bam --tmp-dir /thing -O thing.bam; > 15:34:59.974 INFO NativeLibraryLoader - Loading libgkl_compression.so from; > jar:file:/cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; > 15:35:00.220 INFO SplitNCigarReads -; > ------------------------------------------------------------; > 15:35:00.226 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK); > v4.3.0.0-44-g227bbca-SNAPSHOT; > 15:35:00.226 INFO SplitNCigarReads - For support and documentation go to; > https://software.broadinstitute.org/gatk/; > 15:35:00.226 INFO SplitNCigarReads - Executing as ***@***.*** on; > Linux v5.19.0-32-generic amd64; > 15:35:00.226 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server; > VM v1.8.0_362-8u362-ga-0ubuntu1~22.04-b09; > 15:35:00.226 INFO SplitNCigarReads - Start Date/Time: March 2, 2023; > 3",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:379,error,error,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['error'],['error']
Availability,"Thanks for the clarification @jean-philippe-martin. . Based on JP's analysis @kcibul, it might help decrease the frequency of these errors to run `GenomicsDBImport` with a smaller `--batchSize` value, since that controls the number of simultaneous open GCS connections. This would come at a cost of greater fragmentation within `GenomicsDB` itself, however, which might force us to run with `--consolidate`. Before resorting to running with a smaller batch size, we should probably evaluate the combined effects of JP's recently merged https://github.com/broadinstitute/gatk/pull/2750 as well as his [jp_aggressive_reopen](https://github.com/jean-philippe-martin/gcloud-java/tree/jp_aggressive_reopen) branch in gcloud. We hope to move to a newer gcloud release or snapshot soon, but in a pinch we could provide you with a custom-built jar built on the unmerged gcloud branch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-304128622:132,error,errors,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-304128622,1,['error'],['errors']
Availability,"Thanks for the comments @cmnbroad. > The builds are failing because there are no DocumentedFeature import statements in these files. Each one needs to have an ""import org.broadinstitute.barclay.help.DocumentedFeature;"" statement with the other imports at the top of the file (they're usually listed alphabetically). IntelliJ would do this for you automatically, so I'm assuming you didn't use IntelliJ to make these changes; doing so would also call your attention to any syntax errors that may have been introduced. I made changes via a browser, directly through Github. Making changes this way is actually faster for such minor changes than using IntelliJ. As I mentioned last week, I need to set up my IntelliJ again, starting with our institutional account key. > One minor point - we try to be consistent about use of whitespace; either use spaces around the ""="" everywhere, or don't use spaces. Most of these changes use a mixture:. I copied the spacing from a filters document. I assumed there was some significance to the differential spacing, but if there is not, I would be happy to be consistent. > Finally, I'm not sure there is value in using separate commits for every change like this. Its fine if its you're preferred work style, but its not necessary. This is a side effect of making changes directly through Github. I'll search and see if there is something I can do so all the changes get saved to the same commit.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3835#issuecomment-344597871:479,error,errors,479,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3835#issuecomment-344597871,1,['error'],['errors']
Availability,"Thanks for the ideas -- I found a few more details out this morning. I was trying to use a service account, rather than my personal account, using `gcloud auth activate-service-account`. This works for gcloud and gsutil commands, but doesn't seem to work with ADC very well evidently. Once I changed and used my personal account via `gcloud auth application-default login` GATK4 no longer gave that error. Then I found out that the file I gave you (which I picked because it's NA12878, but not where I originally found the problem) was not indexed. So I went back to using the original file which has `foo.vcf.gz` as well as `foo.vcf.gz.tbi`. GATK SelectVariants ran successfully. Finally, I spun up a GCE-vm which is running with the service account I want, installed Java and GATK4 and was able to run the command successfully. So it seems like the problem would be ""how do I run using a service account from a non-GCE VM"". If there's an answer to that, that would be great.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281694639:399,error,error,399,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2415#issuecomment-281694639,1,['error'],['error']
Availability,"Thanks for the inquiry, @matthdsm! This has indeed been a bit on the back burner since winter break, so perhaps it's time for a kick start. @mwalker174 (or anyone else interested in running scientific tests), a Docker image is available at: us.gcr.io/broad-dsde-methods/broad-gatk-snapshots/gatk-remote-builds@sha256:59310a7e8d635d4c09a6b6e09d188070628a42f7110805eeb26ca044aa1f71a5. Note that I haven't tested this image yet, so please let me know if you encounter any issues. @droazen can you provide a roadmap for the CNN filtering tools? These should be deprecated or otherwise managed before this PR is merged. I am not sure who the major stakeholders are here, but any users out there should feel free to chime in. I will try to do a quick self-review and some minor cleanup in the meantime. My expectation is that the timescale for this to get merged will be on the order of a few weeks to months. But it would be great if we can get it in sooner!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1939607894:227,avail,available,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1939607894,1,['avail'],['available']
Availability,"Thanks for the plots @skwalker ! To clarify, this is the AS- vs non-AS- annotations from the same callset at biallelic sites, right? I'm not surprised there are some differences, but it's hard to tell how significant they are at full alpha. Can you turn down the alpha on the geom_point or add some contours?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393155293:254,down,down,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393155293,1,['down'],['down']
Availability,"Thanks for the question @droazen. No, these tools are more meant to be an update to VQSR, i.e., they do not assume that the BAM/reads will be available and only use the annotations. I think such tools will remain useful going forward, especially for joint genotyping. We can probably eventually push CNN/etc.-based generation of additional features/annotations from the BAM/reads upstream of filtering, so that they’re generated at the same time as our traditional “handcrafted” annotations, after which we can throw everything through the annotation-based filtering tools here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1074265783:142,avail,available,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1074265783,1,['avail'],['available']
Availability,"Thanks for the response, @droazen! Technically, yes, that would be satisfactory & accurate... and if that's easiest, I'm fine with that. . From a user perspective though, it might be beneficial to report the first occurrence of this error, as that's most likely where I would go back to do future testing & troubleshooting. That being said, all of the overlapping intervals are already outputted to stderr, so all the information is retained regardless, and I could just look through the logs to find that first problematic interval. As an aside, I find it a bit weird that the overlapping interval message shows up as a _warning_ even when using the `-no-overlaps` option (I would assume it would be an error, not a warning). In my experience, most errors cause the program to quit immediately. So, perhaps instead, if this warning were an _error_ when using the `-no-overlaps` option, the program would stop after the first occurrence of this error... and then the error message would be accurate. Maybe that was the original intent of this code. But, again, if that requires much more testing & changes, when a quick rewording would also suffice, there's no need. If it's simply a rewording, I'm happy to make a pull request. Let me know what you think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8103#issuecomment-1329747570:233,error,error,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103#issuecomment-1329747570,10,['error'],"['error', 'errors']"
Availability,"Thanks for the review @jamesemery. I've addressed your comments. A few outstanding issues:; * `ActivityProfileStateIterator` and `AssemblyRegionFromActivityProfileStateIterator` duplicate parts of `AssemblyRegionIterator`, so it would be nice to remove the code duplication. Not totally straightforward as the latter does read caching, but the first two don't (Spark shouldn't be caching reads).; * Downsampling needs more work. I would be OK doing that separately, since I've only ever seen it when running on a full genome, and the new strict code needs more work to work on a full genome (I've only got it running on an exome so far).; * There are some improvements we could make to `ReadlessAssemblyRegion` regarding Java interface design and generics, but I'm not sure what they are yet. I'm not sure if these are blockers, since the strict codepath is a new option (off by default), but would like to know what you and @jonn-smith think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5416#issuecomment-439843425:399,Down,Downsampling,399,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5416#issuecomment-439843425,1,['Down'],['Downsampling']
Availability,"Thanks for the review, @fleharty! I addressed the comments (Travis not passing because of the CLOUD tests). Regarding the downgrade of quality, I think that it should be 0.8 to keep it as in samtools because it is already noted in the javadoc what we are doing. If someone wants to use a different approach should use other method (like `FragmentUtils.adjustQualsOfOverlappingPairedFragments`) and/or their own implementation. Another way to keep it consistent with samtools but give to the client some freedom, a `double` could be added (checking will be required for the range). Back to you @fleharty. And thanks again for have a look to this!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2154#issuecomment-253170324:122,down,downgrade,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2154#issuecomment-253170324,1,['down'],['downgrade']
Availability,"Thanks for the suggestion @fnothaft. Unfortunately, switching to Scala 2.11 didn't help as I still get a compile error due to the logging change in Spark 2:. ```; /Users/tom/workspace/gatk/src/main/java/org/broadinstitute/hellbender/utils/read/GATKReadToBDGAlignmentRecordConverter.java:38: error: cannot access Logging; return converter.convert(gatkRead.convertToSAMRecord(header), dict, readGroups);; ```. I get that with `org.bdgenomics.adam:adam-core_2.11:0.19.0` too.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2073#issuecomment-241995596:113,error,error,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073#issuecomment-241995596,2,['error'],['error']
Availability,"Thanks for the suggestion!. To answer the questions:; >Would it make more sense to have a much more stringent cutoff for alignment size after de-overlap? . Yes! The filtering step is actually done in the class `AssemblyContigAlignmentsConfigPicker` in the `alignment` package, where the unique read span length filter is defaulted to 10 base. I put it this way so that the contigs won't be ""re-classified"" in `CpxVariantInterpreter` as having a simple chimera and having to be sent back to `SimpleNovelAdjacencyInterpreter`. So, the idea was to separate the concerns of alignment picking from type inference. > What's the downstream effect of changing this cutoff that you're proposing here; and would it make sense to make it something much higher, like say 19 to match the minimum BWA-MEM seed length?. I'll experiment with the new suggested length.; The idea behind settling on this size-10 filter was to be more permissive when it comes to alignment filtering in `AssemblyContigAlignmentsConfigPicker`, and filter variants later in VCF.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403600389:622,down,downstream,622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403600389,2,['down'],['downstream']
Availability,"Thanks for the suggestions! The SV jobs are all running fine with no hanging after increasing the memory. The commandline below completed on 100 30x crams without any issues. . ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///user/farrell/adni/sv/$SAMPLE.contig-sam-file\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///user/farrell/$CENTER/sv/$SAMPLE.sv.vcf \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode cluster \; --executor-memory 60G\; --driver-memory 40g\; --num-executors 12\; --executor-cores 4\; --files $REF.img,GRCh38_ignored_kmers.txt \; --name ""$SAMPLE"" --conf spark.yarn.submit.waitAppCompletion=false\; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-381441151:935,heartbeat,heartbeatInterval,935,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-381441151,1,['heartbeat'],['heartbeatInterval']
Availability,"Thanks for the thoughts. Singularity is definitely awesome and I'm hoping to support it as an alternative choice to Docker for local HPC clusters where we won't require equivalent root permissions to run. So it helps avoid some of the potential external permission errors by creating a potentially cleaner path to running. Unfortunately it doesn't deal with the underlying issue of needing to map users inside of the containers so that Spark is happy with them. Having something more lightweight than needing user updates in the internal `/etc/passwd` would also help with potential issues on other container enginer (Singularity, rkt).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-381642529:265,error,errors,265,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-381642529,1,['error'],['errors']
Availability,Thanks for the update @mlathara. I have not seen a response yet from the second user so it looks like user error.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793#issuecomment-690774661:107,error,error,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793#issuecomment-690774661,1,['error'],['error']
Availability,"Thanks for the updates @fdchevalier. I see exactly what the issue is now; the presence of the .tbi file causes the code to go down a different path (through TabixFeatureReader), which in turn manifests the issue because it doesn't handle the spaces in the paths correctly. I'll create a ticket in htsjdk, where the bug is.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-651372730:126,down,down,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-651372730,1,['down'],['down']
Availability,"Thanks for your answer @davidbenjamin. I still think that a javadoc update may be useful. If GATK is used as a framework and the user does not know that they should fill in all the likelihoods for all the alleles and reads, even without knowing about `PerReadAlleleLikelihoodMap`, this may drive to errors in the code that may be solved with a implementation note in the documentation. Thanks a lot again for the information, it was very useful!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2311#issuecomment-272486775:299,error,errors,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2311#issuecomment-272486775,1,['error'],['errors']
Availability,"Thanks for your question, @MatthewP-Newbee. I suspect that you may not be using a suitable list of common SNP sites when running `CollectAllelicCounts`---`3.2G` seems quite large for an allelic-counts file. Can you confirm whether or not this is the case?. The GATK forums might be more helpful for answering this type of question; e.g., see my previous responses to a similar post at https://gatkforums.broadinstitute.org/gatk/discussion/11915/error-with-gatk-modelsegments",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5948#issuecomment-493975837:445,error,error-with-gatk-modelsegments,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5948#issuecomment-493975837,1,['error'],['error-with-gatk-modelsegments']
Availability,"Thanks for your response @droazen -- just tried it and it still does not work for me. I am able to access the bucket fine via gsutil -u {project} as expected. I wonder if this is some GCP issue because I am also unable to get Cromwell to pull down files from requester pays (via Terra, so this should be handled in theory), an issue that a colleague also has once I asked her to run this command on a different r/p bucket using her billing project and account. Also, for a different project and bucket the usual workflow I have to get Hail to read from r/p buckets seems to not work with this same error. Very confused. EDIT: https://support.terra.bio/hc/en-us/articles/4447388269851 seems to provide the most parsimonious explanation:; ```; It was determined that Google tweaked an error message causing Cromwell not to recognize buckets as requestor pays.; ```. Wonder if something similar is going on with GATK?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6179#issuecomment-1048028114:243,down,down,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6179#issuecomment-1048028114,3,"['down', 'error']","['down', 'error']"
Availability,"Thanks much for the help with this. That's a good question as representing multiple low frequency somatic variants at the same position doesn't have a great fit with VCF. You're right, these are essentially a pool so could have an arbitrarily large number of calls, but reflecting that as ploidy ends up being a little problematic for downstream processing. If I got to pick, I'd suggest normalizing these so each variant has it's own line. This also has the advantage of applying filters to individual variants instead of needing to treat all possibilities together. Running `bcftools norm -m '-any` gets it almost right, except for retaining the triploid calls:; ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT control_downsample tumor_downsample; 1 725556 . A G . . DP=460;ECNT=3;NLOD=54.98;N_ART_LOD=2.72;POP_AF=0.001;P_GERMLINE=-51.85;TLOD=6.33 GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:SA_MAP_AF:SA_POST_PROB 0/0:205,6:0.02:101,4:104,2:34:415,467:40:4:.:. 0/1/0:209,9:0.035:108,7:101,2:34:404,485:28:2:0.03,0.03,0.035:0.003983,0.005306,0.991; 1 725556 . A AGAATAGAATGGAATAGAAAGGAATG . . DP=460;ECNT=3;NLOD=46.06;N_ART_LOD=8;POP_AF=0.001;P_GERMLINE=-42.76;TLOD=19.76 GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:SA_MAP_AF:SA_POST_PROB 0/0:205,4:0.044:101,0:104,4:0:415,130:25:0:.:. 0/0/1:209,8:0.047:108,4:101,4:0:404,137:42:0:0.03,0.03,0.035:0.003983,0.005306,0.991; ```; Is this something worth adjusting in the MuTect2 output, or do you prefer if we normalize and re-fix ploidy as a post-processing step? Thanks again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-328853019:335,down,downstream,335,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-328853019,1,['down'],['downstream']
Availability,"Thanks zzxzxzzxz. . I insert a debug print to `score_and_write_batch()` function in `inference.py` and run CNNScoreVariants with `err.vcf`.; The following data is received by python process via fifo:. 'chrM\t16521\tC\t[T]\tGGGCCCATAACACTTGGGGGTAGCTAAAGTGAACTGTATCCGACATCTGGTTCCTACTTCAGGGCCATAAAGCCTAAATAGCCCACACGTTCCCCTTAAATAAGACATCACGATG\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\tAC=2;MQRankSum=-0.712;MQ=57.64;AF=1.00;MLEAC=2;BaseQRankSum=5.566;ExcessHet=3.0103;MLEAF=1.00;DP=4492;ReadPosRankSum=-1.183;AN=2;FS=0.000;QD=30.32;SOR=0.761;ClippingRankSum=0.053;\tSNP\n'. There are 13 NULL data in reference bases. This data are created by `transferToPythonViaFifo()` method in `CNNScoreVariants.java`. private void transferToPythonViaFifo(final VariantContext variant, final ReferenceContext referenceContext) {; try {; final String outDat = String.format(""%s\t%s\t%s\t%s\n"",; getVariantDataString(variant),; ---> new String(Arrays.copyOfRange(referenceContext.getBases(), 0, windowSize), ""UTF-8""),; getVariantInfoString(variant),; variant.isSNP() ? ""SNP"" : variant.isIndel() ? ""INDEL"" : ""OTHER"");. (windowSize == 128) / 2 - (chrM length(16571) - variant position(16521) + 1) = 13. I add the `fifo_data[4] = fifo_data[4].strip('\x00')` in `score_and_write_batch()` function and rerun:. ValueError: Error when checking : expected reference to have shape (128, 4) but got array with shape (115, 4). What should I do ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-388777837:1308,Error,Error,1308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-388777837,1,['Error'],['Error']
Availability,Thanks! ; Do you know when an updated docker image with this fix will become available? ; Is there an easy way to get that?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064280760:77,avail,available,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064280760,1,['avail'],['available']
Availability,Thanks! I'll just fix the womtool failure on the pon WDL now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7992#issuecomment-1218344877:34,failure,failure,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7992#issuecomment-1218344877,1,['failure'],['failure']
Availability,"Thanks, @yfarjoun, I think those are reasonable. Just to be clear, the code for the tool mentioned above is a little confusing, in that an early fail for writability when the directory does not exist prevents us from reaching code that appears to be intended to create the directory. Not a big deal in the end (and I checked that this was also the case before the PR). But minor things like this can easily break downstream scripts, etc., as was demonstrated above, so we should take some care. I agree that it's fine to leave some decisions up to each tool, but we should try to document them for the benefit of users and future devs that might need to maintain the behavior of the tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-470172806:413,down,downstream,413,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-470172806,2,['down'],['downstream']
Availability,"Thanks, I come here from this thread. ; It seems like I have to downgrade the pipeline to the older version with CombineGVCFs step, GenomicDBImport just can't do this stuff now :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7667#issuecomment-1260955047:64,down,downgrade,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7667#issuecomment-1260955047,1,['down'],['downgrade']
Availability,"Thanks, that's a more useful error message I think. What version of java are you running? The current version of GATK requires java 8. It sounds like you're running a java that includes a module system which is java 9+. However, the situation is changing very soon, we are upgrading gatk to use java 17.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452483074:29,error,error,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452483074,1,['error'],['error']
Availability,"Thanks, we have access now. I'm pretty sure that sl_revert_glob will fix the error. I've rebased my dev branch sl_filter (which includes the filtering steps Jack mentioned in the BSV meeting today) onto sl_revert and am testing cohort mode on FC now. I'll try to test scattered-case mode as well later today if that succeeds. As @asmirnov239 pointed out to me, this revert leaves #4397 unresolved, so we should go back and clean up at some point. However, our priority now is to get a stable v1 of the SFARI evaluation on FC.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5217#issuecomment-424436535:77,error,error,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5217#issuecomment-424436535,1,['error'],['error']
Availability,"Thanks, we have reproduced the issue with your files. Did you see any errors logged during the GenomicsDBImport phase? What OS are you running on? Will you be able to help by re-running GenomicsDBImport with a debug version of the libtiledbgenomicsdb.so and `./gatk --java-options ""-Dgenomicsdb.library.path=/path/to/libtiledbgenomicsdb.so"" GenomicsDBImport`? If so, will build and share a debug version with you. As a workaround for now, can you split the intervals to GenomicsDBImport - see https://gatk.broadinstitute.org/hc/en-us/articles/360035531852-Intervals-and-interval-lists? Splitting the chromosome into 2 or 3 roughly equal regions may help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-758304480:70,error,errors,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-758304480,1,['error'],['errors']
Availability,"Thanks. Is the splitting-bai only of use for the tools that generates it (in this case ApplyBQSR), or should it be kept around for other tools (e.g. Mutect2) in a downstream analysis?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4219#issuecomment-359676296:163,down,downstream,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4219#issuecomment-359676296,1,['down'],['downstream']
Availability,"Thanx for feedback. I obviously don’t know much if anything about the underlying logic but; have had enough experience to look in unusual places. Have a good weekend. RDB. On Fri, Nov 1, 2019 at 4:24 PM JP Martin <notifications@github.com> wrote:. > @rdbremel <https://github.com/rdbremel> for ""mystery 1"" see issue #5447; > <https://github.com/broadinstitute/gatk/issues/5447>. This should be an; > innocuous warning that it can't initialize the Google Cloud Storage code; > and shouldn't cause a failure unless you try to access paths that start; > with ""gs://"". Going through the Cloud initialization steps described in the; > README should remove the warning (though again, this isn't required if you; > don't need to read files from the cloud).; >; > Mystery 2: For what it's worth, ""GC overhead limit exceeded"" indicates; > that the VM was spending too much time in GC. Running low on memory is a; > possible cause but generating too many small objects or being stuck in an; > infinite loop of allocation/deallocation are others. In the past these have; > been caused by inputs that were malformed in some way. This isn't the place; > for this discussion though, please file a separate issue since it's a; > separate bug.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/6182?email_source=notifications&email_token=ANCR2VHWQ6XDSUQ6KEGISFDQRSM7TA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEC4GNZY#issuecomment-548955879>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANCR2VEC5ARUEQRTEDGJ3TDQRSM7TANCNFSM4I2MRFQA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548989454:498,failure,failure,498,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548989454,2,['failure'],['failure']
Availability,That PR is in so we're good. I see there were [four alpha releases](https://mvnrepository.com/artifact/com.google.cloud/google-cloud) in February so the code may be soon available officially.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2441#issuecomment-285814522:170,avail,available,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2441#issuecomment-285814522,1,['avail'],['available']
Availability,"That argument is only relevant to HaplotypeCaller and GenotypeGVCFs. It; should never have been available for Mutect2. On Thu, Mar 28, 2019, 11:09 PM igor <notifications@github.com> wrote:. > I just tried Mutect2 from GATK 4.1.1.0 and got an error:; >; > A USER ERROR has occurred: standard-min-confidence-threshold-for-calling is not a recognized option; >; > From the online documentation; > <https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/org_broadinstitute_hellbender_tools_walkers_mutect_Mutect2.php>; > :; >; > Note that the default was changed from 10.0 to 30.0 in version 4.1.0.0 to; > accompany the switch to use the the new quality score by default.; >; > Thus, it was still maintained in 4.1.0.0. Based on that, I am surprised; > that it was removed. I just wanted to confirm this was an intended change.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5845>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdCF1GhJcFOFF4YvLqB_rB0x5oquxks5vbYQFgaJpZM4cRiVD>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5845#issuecomment-477985961:96,avail,available,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5845#issuecomment-477985961,3,"['ERROR', 'avail', 'error']","['ERROR', 'available', 'error']"
Availability,That error is different than the bug we fixed. I've seen it when I have specified an incorrect project. It's possible there's a different bug though. I would check to make sure that the project name is spelled correctly AND that the service account you're running is has the necessary permissions to bill that project.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064423004:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7700#issuecomment-1064423004,1,['error'],['error']
Availability,"That is a good question Soo Hee. I myself don't set the argument unless I get an error. If I do get an error, I just set is to 100G which is outrageous, but I don't know how much is appropriate. If anyone else has suggestions, I would be interested in the answer too.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3137#issuecomment-319113656:81,error,error,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3137#issuecomment-319113656,2,['error'],['error']
Availability,"That is a separate matter altogether from both 1) unifying the allele-count collection tools, and 2) standardizing the format of tabular data. The most appropriate place for integration of Mutect2 SNV calls would be as input to the tumor-heterogeneity tool (along with the ModelSegments output) further downstream. This is because it is unlikely that including the SNVs as input to ModelSegments would significantly improve either segmentation or modeling there. If the allele-count collection tools are unified, I think that the only redundant work done across both pipelines would be the calling of hets from the pileups, which is extremely cheap. However, we should certainly also unify the code to do this (which I've spoken to @davidbenjamin about as well).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926:303,down,downstream,303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926,2,"['down', 'redundant']","['downstream', 'redundant']"
Availability,"That is not normal @jean-philippe-martin -- eg., a recent passing build of the unit tests on master (https://travis-ci.org/broadinstitute/gatk/builds/219053956) has just 5 out of 419980 tests skipped:. ```; Results: SUCCESS (419980 tests, 419975 successes, 0 failures, 5 skipped); ```. The ~9000+ skips you're seeing are almost certainly due to the intermittent git lfs quota issues -- we've contacted travis (and github) support about this. If you keep re-running the tests they should pass eventually.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-292031171:259,failure,failures,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-292031171,1,['failure'],['failures']
Availability,"That is possible. But I'm not sure; * why that is true only after NIO version 66.; * how to track down the ""difference"" between the two buckets.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5935#issuecomment-491950617:98,down,down,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935#issuecomment-491950617,1,['down'],['down']
Availability,"That method is used in tests, isn't it? . I'd be ok with commenting it out for now for the sake of getting this merged, provided that you open a ticket to uncomment the method and re-enable the relevant tests once the package rename is available.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2488#issuecomment-287906266:236,avail,available,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2488#issuecomment-287906266,1,['avail'],['available']
Availability,That sounds like the sort of error we'd get if we tried to index an empty bam. I wonder if it's the filter problem that Ted's PR is addressing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2503#issuecomment-288235270:29,error,error,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2503#issuecomment-288235270,1,['error'],['error']
Availability,"That sounds prudent. The new version of the GermlineCNVCaller workflow will be available at release; I think you'll find the workflow itself to be quite streamlined and hopefully easy to use. However, because the model is relatively sophisticated, there are some parameters and model priors that may need to be set appropriately to generate optimal results. We plan on spending some time shortly after release doing internal evaluations to determine some best-practices guidelines for data generated at the Broad.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3996#issuecomment-352850485:79,avail,available,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3996#issuecomment-352850485,2,['avail'],['available']
Availability,That would be a clearer error message.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/489#issuecomment-99104999:24,error,error,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/489#issuecomment-99104999,1,['error'],['error']
Availability,That's correct. I ran into a character encoding issue with a test file at one point. This will make the errors a little more explanatory.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5124#issuecomment-414813822:104,error,errors,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5124#issuecomment-414813822,1,['error'],['errors']
Availability,That's fine -- just make sure it launches with : ; Build after other projects are built - to gatk-perf test. That's the one that sets up the environment and builds latests dockers. So that needs to happen first then all the other tests downstream after. A clone should capture that but just double check.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3573#issuecomment-329283305:236,down,downstream,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3573#issuecomment-329283305,1,['down'],['downstream']
Availability,"That's not unique to out of memory errors, I think it's the result of the process being hit with a SIGKILL so there are other reasons it could happen. A message that suggests that it might be a memory issue but doesn't hide the actual error would be helpful in pointing people in the right direction.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6362#issuecomment-572622465:35,error,errors,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6362#issuecomment-572622465,2,['error'],"['error', 'errors']"
Availability,"That's why I am not using in ReadTools and other developmental toolkit the base class from GATK, due to the polluted command line with unused arguments. I think that for give flexibility, some of that arguments should be configurable by extending classes. For example, some tools that does not require reads at all should be able to turn off the read arguments. That will be very useful, although I am not sure how to do it in a proper way without adding more and more interfaces for argument collections. In context case of this PR, I think that adding it does not have any real effect on the GATK codebase, and a lot is gained by downstream projects. For example, if the wrapper script adds another argument that should be parsed in `Main` and documented, the GATK team just add it to its class. If a toolkit has a similar wrapper script, it can also add its own only-doc argument by simply overriding the method...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371822090:632,down,downstream,632,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371822090,2,['down'],['downstream']
Availability,"The ""WDL test"" CI failures are not related to the changes in this PR, please see this [sanity check PR](https://github.com/broadinstitute/gatk/pull/8369) which is also currently aflame.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8362#issuecomment-1599633824:18,failure,failures,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8362#issuecomment-1599633824,1,['failure'],['failures']
Availability,"The AlleleFrequencyCalculatorUnitTest failure is due to the fact that Genomics DB is pulling in a newer version of testNG (we were using 6.9.6, but now were getting 6.10):. ```; :dependencyInsight; org.testng:testng:6.10 (conflict resolution); \--- com.intel:genomicsdb:0.5.0-proto-3.0.0-beta-1; \--- compile. org.testng:testng:6.9.6 -> 6.10; \--- compile; ```; and 6.10 seems to have a bug in how it handles arrays in lists, which is causing the failure. So we need to add a force resolution statement for testNG in build.gradle:. `force 'org.testng:testng:6.9.6'`. We need to make sure we don't confer this issue on gatk-protected, but thats a [separate issue](https://github.com/broadinstitute/gatk-protected/issues/982).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296195599:38,failure,failure,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296195599,2,['failure'],['failure']
Availability,"The BigQuery library upgrade for extract, broke ingest :/ This fixes that. It also rethrows an exception we were eating that I noticed. The error seen during ingest was. ```; java.lang.IllegalArgumentException: JSONObject does not have a bytes field at root.sample_id.; 	at com.google.cloud.bigquery.storage.v1beta2.JsonToProtoMessage.fillField(JsonToProtoMessage.java:306); 	at com.google.cloud.bigquery.storage.v1beta2.JsonToProtoMessage.convertJsonToProtoMessageImpl(JsonToProtoMessage.java:138); 	at com.google.cloud.bigquery.storage.v1beta2.JsonToProtoMessage.convertJsonToProtoMessage(JsonToProtoMessage.java:86); 	at com.google.cloud.bigquery.storage.v1beta2.JsonStreamWriter.append(JsonStreamWriter.java:110); 	at com.google.cloud.bigquery.storage.v1beta2.JsonStreamWriter.append(JsonStreamWriter.java:90); 	at org.broadinstitute.hellbender.tools.gvs.ingest.CreateVariantIngestFiles.writeLoadStatus(CreateVariantIngestFiles.java:202); 	at org.broadinstitute.hellbender.tools.gvs.ingest.CreateVariantIngestFiles.onTraversalSuccess(CreateVariantIngestFiles.java:369); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1062); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7620:140,error,error,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7620,1,['error'],['error']
Availability,"The CNN tools launch python immediately even when they're just instantiated, rather than waiting until the tool actually starts executing. This can cause some build tasks (gatkDoc, gatkWDLGen, etc.) to fail if python isn't available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8128:223,avail,available,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8128,1,['avail'],['available']
Availability,"The Carrot run failed due to PAPI error code 9, by the way, not for any reason specific to this branch:. ```; ""executionStatus"": ""Failed"",; ""message"": ""Task BenchmarkComparison.EVALRuntimeTask:NA:4 failed. Job exit code 1. Check gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2aab3ed0-746b-451e-a7db-1c22fbb1bb29/call-CHMSampleHeadToHead/BenchmarkComparison/82289acc-83e7-49c8-acd0-9b2277166e10/call-EVALRuntimeTask/attempt-4/stderr for more information. PAPI error code 9. Please check the log file for more details: gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2aab3ed0-746b-451e-a7db-1c22fbb1bb29/call-CHMSampleHeadToHead/BenchmarkComparison/82289acc-83e7-49c8-acd0-9b2277166e10/call-EVALRuntimeTask/attempt-4/EVALRuntimeTask.log."",; ""message"": ""Workflow failed""; ""status"": ""Failed"",; ""message"": ""Task BenchmarkComparison.EVALRuntimeTask:NA:4 failed. Job exit code 1. Check gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2aab3ed0-746b-451e-a7db-1c22fbb1bb29/call-CHMSampleHeadToHead/BenchmarkComparison/82289acc-83e7-49c8-acd0-9b2277166e10/call-EVALRuntimeTask/attempt-4/stderr for more information. PAPI error code 9. Please check the log file for more details: gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2aab3ed0-746b-451e-a7db-1c22fbb1bb29/call-CHMSampleHeadToHead/BenchmarkComparison/82289acc-83e7-49c8-acd0-9b2277166e10/call-EVALRuntimeTask/attempt-4/EVALRuntimeTask.log."",; ""message"": ""Workflow failed""; ""message"": ""Workflow failed""; ```. Looks like the underlying cause is an R parsing issue:. ```; Error in parse(text = text) : <text>:1:1: unexpected '*'; 1: *; ^; Calls: ldply ... llply -> structure -> lapply -> FUN -> eval -> parse; ```. @jamesemery Have you seen that error before in the Carrot HC tests?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8862#issuecomment-2153282072:34,error,error,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8862#issuecomment-2153282072,5,"['Error', 'error']","['Error', 'error']"
Availability,"The GATK VCF header issue causing the underlying problem was fixed in #3351, so a new release of GATK4 should work correctly and avoid losing variants during GenomicsDB import/output for joint calling. I agree with Louis that failing with an error would be better than the current silent failures in case of any future issues. Thank you all again for the help debugging this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-325024708:242,error,error,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-325024708,2,"['error', 'failure']","['error', 'failures']"
Availability,"The GATK docker image uses samtools version 0.1.19 instead of the current version 1.9 and can therefore not read `gs://` resources. Samtools is installed in the gatkbase image via apt-get, the recent releases are not available there. Instead, it would have to be built manually (see http://www.htslib.org/download/).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6148:217,avail,available,217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6148,2,"['avail', 'down']","['available', 'download']"
Availability,"The GATK4 port of GATK3 VariantEval uses a MultiVariantWalker traversal, along with individual `FeatureInput` arguments for evals, knowns, comps, etc., which are all manually merged together as the walker's driving variants. The resulting variants are then manually processed in groups, by start position. Since the tool needs to know the origin of each variant (eval, comp, dbsnp, known, etc.), and since this isn't preserved by the engine, it re-queries the `FeatureContext` for each input to get the same set of variants grouped by source. Since the inputs are typed as `FeatureInput`, this results in all inputs being both consumed and cached twice; once by `MultiVariantDataSource` and once by `FeatureManager`. Once alternative would be to use a LocusWalker, but that would still require index queries (though the features would be cached), and it would still require manual filtering/aggregation on start position. Proposed fix is to switch the base class to use `MultiVariantWalkerGroupedOnStart` (this would allow removal of `PositionAggregator` class); change the engine to preserve the input source of each variant as proposed in https://github.com/broadinstitute/gatk/pull/4571; and change the input arguments for VariantEval from individual named arguments to tagged feature inputs. This would greatly simplify the initialization code, eliminate redundant reading and caching, and allow the tool to do the input source grouping by just looking at each variant's source field.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5439:1359,redundant,redundant,1359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5439,1,['redundant'],['redundant']
Availability,"The Genome Analysis Toolkit (GATK) v4.2.0.0. When I run gatk IndexFeatureFile --input ./merged_flt_c1.imputed.vcf, I got an error as below:. A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c1.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 343338: unparsable vcf record with allele - . I run this ""gatk IndexFeatureFile"" with different vcf fies, it return different error such as,; A USER ERROR has occurred: Error while trying to create index for ./merged_flt_c3.imputed.vcf. Error was: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 2124615: unparsable vcf record with allele +; ; So, How could I solve it?; thank y; ![1618915844](https://user-images.githubusercontent.com/67847482/115384785-027c3800-a20a-11eb-93d2-05431fe258b5.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7214:124,error,error,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7214,8,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"The Genome Analysis Toolkit (GATK) v4.5.0.0; ## Description; Hi,; Here is my situation, I'm testing the feasibility of incremental GenomicsDB，I have total 400 samples to joint calling, I have no problem directly using `GenomicsDBImport `and `GenotypeGVCFs `for joint calling of all 400 samples. The configuration used is 4c32g for `GenomicsDBImport `and 2c16g for `GenotypeGVCFs`. But when I first built a GenomicsDB of 200 samples using `GenomicsDBImport `successfully, and then use GenomicsDB `--genomicsdb-update-workspace-path` increment 200 samples into the GenomicsDB , use this incremental imported GenomicsDB to `GenotypeGVCFs`. The error happend and report GENOMICSDB_TIMER,Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; Here are my code; ```; gatk --java-options ""-Xms8000m -Xmx~{max_mem}m"" \; GenomicsDBImport \; --tmp-dir $PWD \; --genomicsdb-workspace-path ~{workspace_dir_name}~{prefix}.~{index} \; --batch-size 50 \; -L ~{intervals} \; --reader-threads 5 \; --merge-input-intervals \; --consolidate \; -V ~{sep = "" -V "" single_sample_gvcfs}. gatk --java-options ""-Xms8000m -Xmx~{max_mem}m"" \; GenomicsDBImport \; --tmp-dir $PWD \; --genomicsdb-update-workspace-path ~{workspace_dir_name} \; --batch-size 50 \; --reader-threads 5 \; --merge-input-intervals \; --consolidate \; -V ~{sep = "" -V "" single_sample_gvcfs}. gatk --java-options ""-Xms8000m -Xmx~{max_mem}m"" \; GenotypeGVCFs \; --tmp-dir $PWD \; -R ~{ref} \; -O ~{workspace_dir_name}.vcf.gz \; -G StandardAnnotation \; --only-output-calls-starting-in-intervals \; -V gendb://~{workspace_dir_name} \; -L ~{intervals} \; --merge-input-intervals \; -all-sites; ```; And I found that before report error the number of threads used by GATK increased, but the memory usage did not exceed the maximum limit of the server.; I also cheched `--max-alternate-alleles` and `--genomicsdb-max-alternate-alleles` to a smaller size but still the same error. I would appreciate some insights in why that is. Thanks,; Yang",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8777:1804,error,error,1804,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8777,2,['error'],['error']
Availability,"The Hardy-Weinberg equilibrium (HWE) theorem characterizes the distributions of genotype frequencies in populations that are not evolving. Let’s recall it in its simplest form. [Hardy-Weinberg] Let ( A ) and ( a ) be alleles at a single locus in a non-evolving population with random mating. Let ( p ) and ( q ) be their respective frequencies in that population. ( p ) and ( q ) will remain constant in average from generation to generation. The expected frequencies of the genotypes, ( AA ), ( Aa ) and ( aa ), will also remain constant and are respectively ( p^2 ), ( 2pq ), and (q^2 ). Description:. Use Wigginton’s exact test because it adequately controls type I errors in large and small samples. Calculated by:. Pedstats and vcftools use efficient implementations from Wigginton et al.; use code by Wigginton as your starting point (need to translate to java i think). Remark:. Deviations from HWE can indicate inbreeding, admixture, or population stratification. In order to avoid the latter, HWE tests should be run for each ethnicity/population separately. Typically a variant is filtered out if, for any of the ethnicities, the P-value is lower than (10^\textrm{-6}). HWE tests can also identify loci with systematic genotyping errors, which makes HWE useful for QC.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/538:669,error,errors,669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/538,2,['error'],['errors']
Availability,"The MacArthur lab has dealt with this by using our PID/PGT tags and post processing. If we did it for them it would probably be appreciated. Full disclosure: we don't have 100% sensitivity to adjacent phased SNPs with PID/PGT because the phasing heuristic requires cis-phase variants to be on ALL the same haplotypes, which breaks down occasionally in messy regions and some other scenarios.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4647#issuecomment-380465668:331,down,down,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4647#issuecomment-380465668,1,['down'],['down']
Availability,"The SV copy results script has to figure out the total number of workers including standard and preemptibles. The current version of the script had an error in parsing the `gcloud compute dataproc clusters list` command, which left a blank field when no preemptibles were used that couldn't be parsed because it was displayed in fixed-width format. This switches to the CSV format option for the cluster list command, making it easier to parse.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4767:151,error,error,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4767,1,['error'],['error']
Availability,The SV discovery pipeline threw a bunch of errors seemingly related to this:; https://issues.apache.org/jira/browse/SPARK-21133. A sample error from my log:; 17/07/17 14:33:17 ERROR org.apache.spark.util.Utils: Exception encountered; java.lang.NullPointerException; 	at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply$mcV$sp(MapStatus.scala:171); 	at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167); 	at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	at org.apache.spark.scheduler.HighlyCompressedMapStatus.writeExternal(MapStatus.scala:167); 	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459); 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430); 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); 	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378); 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174); 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); 	at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply$mcV$sp(MapOutputTracker.scala:617); 	at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616); 	at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:619); 	at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:562); 	at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:351); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.conc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3290#issuecomment-315846491:43,error,errors,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3290#issuecomment-315846491,3,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,"The Spark version of testExampleAssemblyRegionWalker fails reliably when I run all of the tests locally through Gradle. When I run the test by itself, either through Gradle or through IntelliJ, it always passes. . Most of the time when it fails, the test finishes, but the output doesn't match the expected output. The test appears to assume a single output part file; however when it fails there are many part files (about 70), and you get the (second) stack below. There may be a deeper problem when there are multiple partitions though, since sometimes the test fails mid-run with a ConcurrentModificationException:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 1251.0 failed 1 times, most recent failure: Lost task 16.0 in stage 1251.0 (TID 2169, localhost): java.util.ConcurrentModificationException; 	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:643); 	at org.broadinstitute.hellbender.engine.FeatureCache.getCachedFeaturesUpToStopPosition(FeatureCache.java:216); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:393); 	at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:163); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:115); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSpark.lambda$assemblyFunction$213c9289$1(ExampleAssemblyRegionWalkerSpark.java:95); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1204); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2349:59,reliab,reliably,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349,3,"['failure', 'reliab']","['failure', 'reliably']"
Availability,"The Talkowski lab version of this is in R and requires some packages that don't seem to be available anymore as well as the python tool svtk, also developed in their lab. It also localizes all the files with a separate Java program they developed. Their implementation is here (most critically gCNV_Pipeline.Rmd and gCNV_helper.jar): https://github.com/theisaacwong/talkowski/tree/master/gCNV It appears to be under active development. My simplified implementation is at https://app.terra.bio/#workspaces/broad-firecloud-dsde-methods/gCNV-CMG-test/notebooks/launch/perform_clustering.ipynb but it's still under development with some help from Brian in TAG.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5632#issuecomment-926857837:91,avail,available,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5632#issuecomment-926857837,2,['avail'],['available']
Availability,"The Travis error is:. The log length has exceeded the limit of 4 MB (this usually means that the test suite is; raising the same exception over and over). The job has been terminated."". Looks like we'll have to make our tests less chatty.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-292004572:11,error,error,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-292004572,1,['error'],['error']
Availability,The Travis failure looks like a network transient: it's complaining it can't download something from Maven. Trying again.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2786#issuecomment-305295217:11,failure,failure,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2786#issuecomment-305295217,2,"['down', 'failure']","['download', 'failure']"
Availability,The TwoBit API can't handle queries beyond the ends of contigs (we get; an assertion failure). This changes ReferenceTwoBitSource to truncate query; intervals at contig ends as necessary. Resolves #1214,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1236:85,failure,failure,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1236,1,['failure'],['failure']
Availability,"The WDL *input* parameter for any tool argument that is the name of a file that is *created* by the tool has WDL type `String` because if the input parameter were typed as `File`, cromwell would attempt to localize the (most likely non-existent) file at the start of the task. However, in the output block, the parameter has to have type `File` for the reverse reason; if it were `String`, it wouldn't be delocalized. The transformation is admittedly non-intuitive when reading the WDL code. Maybe there is some better alternative ?. The reason the parameter doesn't appear in the command block (in this case anyway) is because there is no corresponding tool parameter. Any `companion` files like this, such as input reference dictionary, input file index, etc., that don't appear as named tool parameters still have to be included in the WDL parameters, but aren't passed directly to the tool. I generally tried to keep the companion parameters adjacent to their source parameter whenever they appear in the WDL or JSON files, so they always travelled together. But since outputIndex is an *optional* companion for a *required* output, this results in it being listed under ""required"" parameters. We could separate the optional args into a separate ""Optional"" header comment as we do for (non-companinon) optional tool args, or we could just add a line comment like:; ```; # Required Arguments; String output_arg; String? outputIndex // optional companion for output_arg; ```. Agreed that it would be nice to find a robust way to prevent the name guessing required.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6984#issuecomment-736621798:1517,robust,robust,1517,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6984#issuecomment-736621798,2,['robust'],['robust']
Availability,The [Google Java Style guide](http://google-styleguide.googlecode.com/svn/trunk/javaguide.html) links are dead and give a 404 error. References were changed to [Google Java Style guide](https://google.github.io/styleguide/javaguide.html).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5405:126,error,error,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5405,1,['error'],['error']
Availability,"The `-selectType` argument now is `--select-type`, and that's what is failing. It is true that the error message is not that useful. @cmnbroad - is this related with https://github.com/broadinstitute/barclay/issues/119, no?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4705#issuecomment-384228852:99,error,error,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4705#issuecomment-384228852,1,['error'],['error']
Availability,"The `PS` tag should be type `Integer`, not `String` according to the spec, but no error is reported (for me). ```bash; bcftools view https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh38/HG001_GRCh38_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz chr1:4001310-4001310 > test.vcf; ```. Related: https://github.com/genome-in-a-bottle/giab_latest_release/issues/15",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6762:82,error,error,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6762,1,['error'],['error']
Availability,"The `Poisson` arises because we want to our model to generate the *occurrences*, assuming that each *count bin* provides equal weight---rather than the counts themselves. As usual, modeling each bin as Poisson is close enough to modeling all bins as multinomial for our purposes. If we directly use the NB likelihood and simply weight the count likelihood by occurrences, occurrences in the peak will strongly affect the result, adversely so if the count likelihood is actually misspecified there. As an example, consider trying to fit a Poisson to data that is actually zero-inflated Poisson---fitting the histogram will actually result in a more robust estimate for the mean. Another benefit is that truncated data (as we have here) is straightforwardly handled in an unbiased way. In the special case of complete, trivially-binned data, the full, unbinned likelihood is recovered. I think this sort of histogram fitting is pretty standard in the astro/particle community. We can certainly change up the model to include strictly quantized + free-floating states as you describe (rather than the ""fuzzily quantized"" states I use here), but I just wanted to avoid having another level of mixtures/logsumexps for this quick prototype. However, note that modeling mosaicism on the autosomes is desirable, but there we also want the strong diploid prior to nail down the depth and per-contig bias. So we will have to be a little careful about how we introduce free-floating states. Also, since I was not using gcnvkernel, I had to integrate out all discrete parameters. It may be that we can write down a nice model with discrete parameters if we use your inference framework. Finally, I did not further bin the counts here (or rather, the bin size is 1), which already yields the maximum information, but I did use a maximum-count cutoff. If we use the same cutoff for all samples, this allows us to simply pass a non-ragged matrix from Java (with dimensions of samples x contigs x maximum count) as a ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522:648,robust,robust,648,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522,4,"['recover', 'robust']","['recovered', 'robust']"
Availability,The `ReferenceBases` annotation fails with an NPE if there is no reference available. It should fail with a helpful UserException instead. ```; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.ReferenceBases.annotate(ReferenceBases.java:33); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:161); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngineUnitTest.testAllAnnotations(VariantAnnotatorEngineUnitTest.java:225); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestN,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2799:75,avail,available,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2799,1,['avail'],['available']
Availability,"The `google-cloud-nio` library should allow us to customize the errors that trigger a retry/reopen. Specifically, it should allow us to add additional http status codes to trigger a retry, as well as additional exception classes to trigger a reopen. It should be possible to either append to the existing defaults, or to replace the defaults completely with your own values.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5306:64,error,errors,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5306,1,['error'],['errors']
Availability,"The actual error sounds like one of the bam split guesser errors. I thought we'd fixed most of those, but it seems like maybe a corrupt file is triggering it?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4179#issuecomment-358373874:11,error,error,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4179#issuecomment-358373874,2,['error'],"['error', 'errors']"
Availability,"The array schema is fine. What is puzzling is this line `Problematic frame:; \# C [libtiledbgenomicsdb3086049122144672414.so+0x3e3a19] ArraySchema::tile_num(void const*) const+0x79; ` in the error. This particular function should not be invoked for sparse arrays and the array schema has this array correctly marked sparse. Would it be possible to include the full core dump, for instance `/home/groups/MgapGenomicsDb/@files/sequenceOutputPipeline/SequenceOutput_2020-10-06_16-46-33/Job734/hs_err_pid36873.log`? . Can you also try invoking GenomicsDBImport just for this problematic interval in a separate workspace and run SelectVariants/GenotypeGVFs to verify that the error produces?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-717013426:191,error,error,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-717013426,2,['error'],['error']
Availability,"The broad artifactory moved to https://broadinstitute.jfrog.io/broadinstitute/. There is a redirect in place which as been working for downloads, but uploads are failing with `401 Unauthorized`. It seems like updating the url fixes the problem. As a second issue, our builds try to upload archives for every integration test build, which worked when we only had 1 integration test build, but now that we have multiples we are uploading duplicates which isn't good. We should fix that, probably by adding either a new environment variable to the travis build, or a final build stage to perform the upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3068:135,down,downloads,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3068,1,['down'],['downloads']
Availability,"The build is failing since 1.21.0-SNAPSHOT is no longer available in any Maven repositories. It looks like 1.21.0 was released last week: https://repo1.maven.org/maven2/com/google/http-client/google-http-client/, and changing the build to use that version seems to fix the problem. Related to #650.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1185:56,avail,available,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1185,1,['avail'],['available']
Availability,The build should give a clear error message explaining how to skip building the native code if it fails to build.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1554:30,error,error,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1554,1,['error'],['error']
Availability,"The change doesn't affect anything when the BAM has a single sample, and when the BAM has more than one it handles what was previously a failure. So it's not going to break anything other than a pipeline that was somehow relying on the failure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8859#issuecomment-2146402849:137,failure,failure,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8859#issuecomment-2146402849,2,['failure'],['failure']
Availability,"The change is because we started using VariantsSparkSink to write VCFs on the cluster, rather than writing them in a single thread from the driver (which doesn't scale). VariantsSparkSink only supports .bgz extensions currently, not .gz. So if you change the output extension to .bgz it will work. Gzip is not splittable so if possible we'd avoid outputting it at all. Hail for example will not load .gz unless the force option is used. (There are actually two force options, one to read it as regular gzip, the other to read it as bgzip, so you have to know which flavour of gzip it is...). We could do one of the following:. 1. Throw an error if the extension is .gz.; 2. Write bgzipped output to the .gz file.; 3. Write regular gzipped output to the .gz file. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3725#issuecomment-341689307:639,error,error,639,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3725#issuecomment-341689307,1,['error'],['error']
Availability,"The change is fine: It's more tolerant of bogus input, and the cost is nearly 0.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3461#issuecomment-323450988:30,toler,tolerant,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3461#issuecomment-323450988,1,['toler'],['tolerant']
Availability,"The cloud tests are timing out after 10 minutes without emitting any output. It seems like `ApplyBQSRDataflowIntegrationTest.testPR_Cloud` is responsible. It looks like something is crashing in dataflow but the runner is never stopped so it keeps waiting indefinitely (or at least 10 minutes..) See the dataflow log [here](https://console.developers.google.com/project/broad-dsde-dev/dataflow/job/2015-07-24_12_44_26-17415749601435236766). . Executing locally also seems to hang forever, with messages like . ```; Error: (b65a2091061bf0f9): Workflow failed. Causes: (71540087aac21e37): Unable to create VMs. Causes: (71540087aac21994): Error:; Test: Test method testPR_Cloud[0](ApplyBQSR(args=''))(org.broadinstitute.hellbender.tools.walkers.bqsr.ApplyBQSRDataflowIntegrationTest) produced standard out/err: Message: Value for field 'resource.metadata.items[1].value' is too large; ```. Seems like this is possibly a dataflow bug. If the workflow fails in some way the client should be released.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/750:514,Error,Error,514,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/750,2,['Error'],['Error']
Availability,"The code changes here are actually fairly small all things considered after doing some cleaning of the working branches. There are 3 big differences between this branch and the previous version of LinkedDebrujin code: ; 1: Implemented an algorithm to the KBestHaplotypeFinder for coloring ""pivotal edges"" (i.e. edges in which we have made a choice that would be in the junction trees) and then upon fininshing with all of the junctinon tree reachable paths from reference source, we then check for edges that have not been recovered and attempt to rescue them (this fixes the loss of sensitivity from the previous version); 2: Changed the ReadThreadingAssembler to increment the kmer size it uses (when in JT mode) to increment its sizes AFTER it has attempted to recover haplotypes (this catches some new edge cases that causes complicated graphs to fail). This currently is a very rudimentary approach (we simply expand if the KBestHaplotypeFinder failed to find anything at all). ; 3: includes some code to squeeze extra sensitivity out of the junction trees by tolerating SNP errors when threading the junction trees themselves . There are a number of things I think maybe could be tweaked from here:; - I think ""k"" for max haplotypes can be lowered given the new haplotype recovery improvements; - We can and perhaps should revisit the question of how/when to expand the kmer size, as given recent fixes in this branch that could potentially save some sensitivity/specificity that we were losing before. (the code for one approach to this still lives in this branch). . Fixes #5924, #5923, #5828",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6394:523,recover,recovered,523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6394,5,"['error', 'recover', 'toler']","['errors', 'recover', 'recovered', 'recovery', 'tolerating']"
Availability,"The command lines: . gatk --java-options ""-Xmx16G"" GenotypeGVCFs --reference {genome} --variant gendb://workdir/db_{idx} --output output.vcf.gz. gatk --java-options ""-Xmx16G"" CreateSomaticPanelOfNormals --reference {genome} --germline-resource {afsource} --variant gendb://workdir/pon_db_{idx} --output pon.vcf.gz. The output vcf files from the above two steps have chromosome 10 before 7. ; GenomicsDBImport, GenotypeGVCF, or CreateSomaticPanelOfNormals did not give any error messages.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8416#issuecomment-1660490047:472,error,error,472,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8416#issuecomment-1660490047,1,['error'],['error']
Availability,The concise message is:. ```; cb2@cb2-VirtualBox:~/gatk$ ./gradlew bundle; > Configure project :; Executing: git lfs pull --include src/main/resources/large. > Task :condaStandardEnvironmentDefinition; Created standard Conda environment yml file: gatkcondaenv.yml. > Task :pythonPackageArchive; Created GATK Python package archive in /home/cb2/gatk/build/gatkPythonPackageArchive.zip. > Task :gatkDoc FAILED; Unable to find the 'javadoc' executable. Tried the java home: /usr/lib/jvm/java-11-openjdk-amd64 and the PATH. We will assume the executable can be ran in the current working folder. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Get more help at https://help.gradle.org; ```. And stacktrace flag output looks like:. ```; `cb2@cb2-VirtualBox:~/gatk$ ./gradlew bundle --stacktrace; > Task :gatkDoc FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':gatkDoc'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:166); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:163); at org.gradle.internal.Try$Failure.ifSuccessfulOrElse(Try.java:191); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(Execu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716:592,FAILURE,FAILURE,592,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716,1,['FAILURE'],['FAILURE']
Availability,"The constructors (some of them anyway) use the args during constructor execution, so validating them after the constructor is done executing is too late. Anyway, if you revert that commit so the validation happens as before, and instead add the following to the catch block in `VariantEvalEngine::createClass`, they will get propagated as expected:. ```; if (e.getCause() instanceof CommandLineException) {; // failures due to argument validation should be propagated; throw new CommandLineException(e.getCause().getMessage(), e.getCause());; }; throw new GATKException(""Problem making an instance of "" + clazz + "" Do check that the class has a constructor that accepts VariantEvalEngine"", e);; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827866803:411,failure,failures,411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-827866803,1,['failure'],['failures']
Availability,"The current `IsUsingCompressedReferences`; It does not pass project id that hosted dataset. ; When the project_id is missing in a BigQuery SQL query, the bq command will use the --project_id flag specified in the command as the default project for resolving dataset and table references.; Add additional parameter to allow passing dest project. . In our case; Error we saw in GCP console:; ```; Access Denied: Table terra-vpc-sc-dev-7ee328ad:1kg_wgs_2022q1.INFORMATION_SCHEMA.COLUMNS: User does not have permission to query table terra-vpc-sc-dev-7ee328ad:1kg_wgs_2022q1.INFORMATION_SCHEMA.COLUMNS, or perhaps it does not exist.; ```. `terra-vpc-sc-dev-7ee328ad:1kg_wgs_2022q1.INFORMATION_SCHEMA.COLUMNS` is wrong - `terra-vpc-sc-dev-7ee328ad` is the user workspace; It should be `fc-aou-cdr-synth-test-2.1kg_wgs_2022q1` - `fc-aou-cdr-synth-test-2` is the project that contains CDR data.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9023:360,Error,Error,360,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9023,1,['Error'],['Error']
Availability,"The current error handling swallows the detailed part of the error string that gets reported to the user, i.e., without this change you see:; A USER ERROR has occurred: Badly formed genome unclippedLoc: Failed to parse Genome Location string: 20:10000000-10010000. With this change:; A USER ERROR has occurred: Badly formed genome unclippedLoc: Parameters to GenomeLocParser are incorrect:The genome loc coordinates 10000000-10010000 exceed the contig size (200000). I originally removed the entire try/catch block, but there is downstream code that throws IllegalArgumentException, which can benefit from being decorated with the context info.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2191:12,error,error,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2191,5,"['ERROR', 'down', 'error']","['ERROR', 'downstream', 'error']"
Availability,"The current fatJar gradle task does not properly merge resource files, causing an error when you try to run a Spark tool from the resulting jar. This PR replaces the fatJar task by configuring our shadowJar task to properly merge resource files. I've attempted to share configuration with the sparkJar task, which is also of type ShadowJar. Discussed briefly with @lbergelson.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1213:82,error,error,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1213,1,['error'],['error']
Availability,"The current implementation does not check whether `histo.get(testStatU)` returns a valid pointer or not. If runtime optimization is used (such as the case in J9 JIT), JVM may remove that bin if there is nothing in it. The outcome is a `NullPointer` error at runtime for some corner cases. Adding a pointer check can solve the problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5190:249,error,error,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5190,1,['error'],['error']
Availability,"The current runTool method implementation masks exception occurring in either onStartup or doWork if there is a exception in onShutdown. It is more important to report the exception in onStartup or doWork that any occurring in onShutdown. . Apart from this, developers are supposed to make onShutdown robust to failure in onStartup?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/528:42,mask,masks,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/528,3,"['failure', 'mask', 'robust']","['failure', 'masks', 'robust']"
Availability,"The current top-level index document has a bogus tool category called ""tools"" in it (see https://software.broadinstitute.org/gatk/documentation/tooldocs/current/). This is created by the `@DocumentedFeature` annotation on the AlleleFrequency stratifier class. We don't actually have a category for this, and most are not documented. If at some point we want to add those, we can do so with a real category, but for now this removes the annotation. There are also some unnatural line breaks in the index, which are removed here. The html doc could use some more maintenance cleanup, but this fixes those two obvious issues.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6318:561,mainten,maintenance,561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6318,1,['mainten'],['maintenance']
Availability,The error : Invalid GZIP header when I run the BaseRecalibrator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5968:4,error,error,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5968,1,['error'],['error']
Availability,The error `Cannot use index file with textual SAM file` was reported by `AssemblyRegionUnitTest.testCreateFromReadShard`when trying to read what was likely a corrupted large file bam on travis. This error does not accurately reflect the problem and is likely caused by a chain of fallbacks in htsjdk. It's possible that the switch from file -> path has changed the sequence of fallbacks causing this error to appear.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2551:4,error,error,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2551,3,['error'],['error']
Availability,"The error comes from two annotations: InbreedingCoeff and ExcessHet. One solution is to add ""-AX ExcessHet -AX InbreedingCoeff"". It doesnt exactly solve the problem, but it avoids hitting the problem code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7938#issuecomment-1238883942:4,error,error,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938#issuecomment-1238883942,1,['error'],['error']
Availability,The error is not with CollectArtifactSequencingMetrics but with GATK's FilterByOrientationBias.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2351#issuecomment-306517117:4,error,error,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2351#issuecomment-306517117,1,['error'],['error']
Availability,The error is probably caused by the incompatibility of the intel and jdk deflaters/inflaters.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582#issuecomment-991512173:4,error,error,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582#issuecomment-991512173,1,['error'],['error']
Availability,"The error message about allele sorting is unhelpful -- it's from an integration test check for exact match of output vcf against an expected vcf and the ""sort order"" error really just means there are fewer alleles in the output than expected. Since this change is what we want, the solution is just to overwrite the expected VCF. I checked all the discordant files and found nothing wrong, so this is okay. I also looked at all the output after the change `!outputNonVariants` --> `true` that I suggested, and it is definitely more correct.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-582252469:4,error,error,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-582252469,2,['error'],['error']
Availability,"The error message indicates that the recalibration tables have different dimensions, but unfortunately it doesn't say which table it was. Looking at the source for `RecalibrationTables` it looks like it's probably `readGroupTable`, since it is the only 2D table I can see in `allTables`. So the question becomes, why is the number of read groups different for different tables created by `BaseRecalibratorSparkFn`? They all have the same header, so the number of read groups should be the same. One thing to try to see if it affects the result is to set a different partition size with `--bam-partition-size`. It defaults to the HDFS block size, which is 128MB, but you could try another value (e.g. `67108864`, which is 64MB) to see if you get the same error. Otherwise to make more progress I would need to debug locally. Are the files sharable @akkellogg?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854#issuecomment-479397018:4,error,error,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854#issuecomment-479397018,2,['error'],['error']
Availability,"The failure in the test seems to be some transient issue, rerunning the test should do the trick. I restarted it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7471#issuecomment-926901920:4,failure,failure,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7471#issuecomment-926901920,1,['failure'],['failure']
Availability,"The failure is: ""ERROR: (gcloud.components.update) The component [bq-nix] failed to download."" This looks like a transient issue related to gcloud components update. FWIW I was able to update my desktop's installation with no issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-288228672:4,failure,failure,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-288228672,3,"['ERROR', 'down', 'failure']","['ERROR', 'download', 'failure']"
Availability,The failures look transient so I'm restarting them.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4673#issuecomment-390758890:4,failure,failures,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4673#issuecomment-390758890,1,['failure'],['failures']
Availability,"The file extension check in this tool was redundant, since there is a subsequent check; that the input file is in BGZF format. Resolves #5800",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5801:42,redundant,redundant,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5801,1,['redundant'],['redundant']
Availability,"The first issue in this post has been solved by updating GATK versions but there appears to be a bug causing an ""unrecognized argument"" error when running GermlineCNVCaller. . This request was created from a contribution made by Ahmed S. Chakroun on December 05, 2021 13:52 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4412031649691-GermlineCNVCaller-prefix-string-too-short](https://gatk.broadinstitute.org/hc/en-us/community/posts/4412031649691-GermlineCNVCaller-prefix-string-too-short). \--. Hi everybody,. Unfortunately, I am getting stuck because the filename length of my hd5 is too short (< 3) to be used as prefix by the GermlineCNVCaller. So, I am posting just to check that nothing is left for me to do at this level of the analysis I am running other than renaming my input files and re-running everything from the scratch :-(. Accordingly, here is the needed info:. GATK version used: 4.2.0.0 ; ; openjdk version ""11.0.11"" 2021-04-20 ; ; OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.20.04) ; ; OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.20.04, mixed mode, sharing). Command:. gatk GermlineCNVCaller \\ ; ; \--run-mode COHORT \\ ; ; \-L Twist\_Exome\_Target\_hg38\_preprocessed\_annotated\_gc-filtered.interval\_list \\ ; ; \-imr OVERLAPPING\_ONLY \\ ; ; \--contig-ploidy-calls ploidy-calls \\ ; ; \--annotated-intervals Twist\_Exome\_Target\_hg38\_preprocessed\_annotated.interval\_list \\ ; ; \-I 13-20.counts.hd5 \\ ; ; \-I 722.counts.hd5 \\ ; ; \-I D19047.counts.hd5 \\ ; ; \-I F24F1.counts.hd5 \\ ; ; \-I NS.counts.hd5 \\ ; ; \-I TBC039.counts.hd5 \\ ; ; \-I VP.counts.hd5 \\ ; ; \-I WES002.counts.hd5 \\ ; ; \-I WES02.counts.hd5 \\ ; ; \-I 17062-T1-.counts.hd5 \\ ; ; \-I 18001-M1-.counts.hd5 \\ ; ; \-I 516.counts.hd5 \\ ; ; \-I 533.counts.hd5 \\ ; ; \-I NBH.counts.hd5 \\ ; ; \-I ADN492.counts.hd5 \\ ; ; \-I WES607.counts.hd5 \\ ; ; \--class-coherence-length 1000.0 \\ ; ; \--cnv-coherence-length 1000.0 \\ ; ; \--enable-bias",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7591:136,error,error,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7591,1,['error'],['error']
Availability,"The first issue looks like an out of memory error. You may need to scatter your intervals into separate shards, as is done in the WDLs: https://github.com/broadinstitute/gatk/tree/master/scripts/cnv_wdl/germline. The second issue regarding the output directory creation is by design---CNV tools require the output directory to exist beforehand.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467447753:44,error,error,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467447753,1,['error'],['error']
Availability,"The fix will enable run gradle build and test on PowerPC while the pairHMM native binary build is being sorted out. It also introduced tolerance when compare two floating point numbers, which caused test failure on PowerPC.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1761:135,toler,tolerance,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1761,2,"['failure', 'toler']","['failure', 'tolerance']"
Availability,The folder containing all the outputs (scripts and staged inputs) in the cloud is in https://console.cloud.google.com/storage/browser/fc-6da0ee01-0ec0-4606-b918-216fe3f7f098/ecdb3b72-7b4b-4612-9c87-1c0124f62708?pli=1. You should be able to browse down to each tasks sub-folder.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5217#issuecomment-424372961:247,down,down,247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5217#issuecomment-424372961,1,['down'],['down']
Availability,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2734:11,error,error,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734,11,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"The following command generates an error. Other spark programs work when specifying hdfs://scc/user/farrell/adsp/bams/SRR990385.bam as the input. It seems to be having a problem with testing for the presence of the SRR990385.bai file which is present. I tried running the command with hdfs://scc:**8020**/user/farrell/adsp/bams/SRR990385.bam and that works. . `/share/pkg/gatk/4.beta.5/install/bin/gatk-launch SparkGenomeReadCounts -I hdfs://scc/user/farrell/adsp/bams/SRR990385.bam -o SRR990385.ReadCounts -R /restricted/projectnb/genpro/bundle/2.8/b37/human_g1k_v37.fasta --verbosity ERROR -- --sparkRunner SPARK --sparkMaster yarn --num-executors 1 --executor-memory 4G --executor-cores 3`. [December 3, 2017 2:56:35 PM EST] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.57 minutes.; Runtime.totalMemory()=982515712; org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 0.0 failed 4 times, most recent failure: Lost task 12.3 in stage 0.0 (TID 14, scc-q09.scc.bu.edu, executor 1): java.lang.IllegalArgumentException: **Wrong FS: hdfs://scc:8020/user/farrell/adsp/bams/SRR990385.bai, expected: hdfs://scc**; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645); at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193); at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105); at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:302); at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:298); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:298); at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766); at org.seqdoop.hadoop_bam.util.WrapSeekable.openPath(WrapSeekable.java:60); at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3909:35,error,error,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3909,4,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"The following exception was thrown, which looks a lot like a user exception:. ```; htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /storageNGS/ngs3/projects/other1/KinderKlinik/Exomes/04_GRCh8_GATK4_cohort/GATK4_testrun/cromwell-executions/HaplotypeCallerGvcf_GATK4/3e5a2ae0-5529-4344-b187-9ceac771e1ed/call-MergeGVCFs/execution/pa.hg38.g.vcf.gz, for input source: file:///storageNGS/ngs3/projects/other1/KinderKlinik/Exomes/04_GRCh8_GATK4_cohort/GATK4_testrun/cromwell-executions/HaplotypeCallerGvcf_GATK4/3e5a2ae0-5529-4344-b187-9ceac771e1ed/call-MergeGVCFs/execution/pa.hg38.g.vcf.gz; at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:254); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:101); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:126); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:110); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:620); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getHeaderFromPath(GenomicsDBImport.java:355); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.initializeHeaderAndSampleMappings(GenomicsDBImport.java:341); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onStartup(GenomicsDBImport.java:296); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275) Caused by: java.nio.file.No",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4526:165,error,error,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4526,1,['error'],['error']
Availability,"The following one-liner (which I Googled, so I cannot vouch that this is the best way to do things) revealed 245Mb of files that are unused in master:. for ff in `find src/test/resources -name ""*""`; do file=`basename $ff`; git grep -l $file >/dev/null; rcode=$?; if [[ $rcode -ne 0 ]]; then echo $ff; fi; done. Note I didn't search in all branches, but I figure we can always recommit those files. Also, any index files, etc. should be retained if necessary. CNV team will delete their files, but I'll leave it up to engine and the other teams about how much we want to remove. src/test/resources/dbsnp_132.b36.excluding_sites_after_129.chr1_1k.vcf.idx; src/test/resources/empty.vcf.idx; src/test/resources/exampleFASTA.fasta.fai; src/test/resources/fastaWithoutDict.fasta.fai; src/test/resources/fastaWithoutFai.dict; src/test/resources/hg19micro.dict; src/test/resources/hg19micro.fasta.fai; src/test/resources/hg19mini.dict; src/test/resources/hg19mini.fasta.fai; src/test/resources/Homo_sapiens_assembly19_chr1_1M.dict; src/test/resources/Homo_sapiens_assembly19_chr1_1M.fasta.fai; src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf.idx; src/test/resources/HSA19.dbsnp135.chr1_1M.exome_intervals.modified.vcf.idx; src/test/resources/human_g1k_v37.chr17_1Mb.dict; src/test/resources/human_g1k_v37.chr17_1Mb.fasta.fai; src/test/resources/iupacFASTA.dict; src/test/resources/iupacFASTA.fasta.fai; src/test/resources/joint_calling.chr1_1M.1kg_samples.10samples.noINFO.vcf.idx; src/test/resources/large/1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx; src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram.bai; src/test/resources/large/cnv_germline_workflows_test_files/inputs/wes_pon/model_final/bias_covariates_ARD_coefficients.tsv; src/test/resources/large/cnv_germline_workflows_test_files/inputs/wes_pon/model_final/mean_bias_covariates_matrix.tsv; src/test/resources/large/cnv_germline_workflows_test_files/inputs/wes_pon/model_final/mean_bias_cov",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:291,echo,echo,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,1,['echo'],['echo']
Availability,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2357:43,error,error,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357,1,['error'],['error']
Availability,"The gap-opening and gap-continuation parameters of Smith-Waterman realignment depend on PCA slippage and other stuff that depends on the sequencing platform and sample prep. In other words, they are not global parameters (_note: Smith-Waterman is often used to determine sequence similarity between individuals or species in which case its parameters are constants of the population. But a read differs from a candidate haplotype via sequencing error, not mutation_). @ronlevine suggested (and I am reporting because I like the idea) that we probably have sufficient data to learn these parameters for each sample.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1902:445,error,error,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1902,1,['error'],['error']
Availability,"The git hash refers to the commit of GATK you have checked out in your branch as opposed to the commit of GATK that has been built in Java (and will be run on the cluster). I will clarify the error message: it means that you need to rebuild GATK to match your changes to the repository. The malformed object name is a different story, and seems like a bug. I will try to figure that out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3593#issuecomment-330818394:192,error,error,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3593#issuecomment-330818394,1,['error'],['error']
Availability,The goal of this PR is to adjust the ingest in two ways:; 1. To update the ingest to loop through all samples (not just the first 10k); 2. To update the ingest to be far more efficient in a few ways:; - To remove the files that are downloaded to each vm so that they do not carry around the extra weight; - To check that the samples in the fofns have not been ingested already so that additional work doesn't need to be done toward processing those samples. There is still work to do around making the bulk ingest process significantly more user-friendly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8197:232,down,downloaded,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8197,1,['down'],['downloaded']
Availability,"The hg19 gencode is missing the transcript `ENST00000515683.1`. It appears in the gtf, but not in the fasta. Using `funcotator_dataSources.v1.2.20180329`. ```; A USER ERROR has occurred: Bad input: Unable to find the given Transcript ID in our transcript list (not in given transcript FASTA file): ENST00000515683.1; ```; ```; gatk Funcotator --output-file-format VCF \; --ref-version hg19 --data-sources-path ~/data/funcotator/funcotator_dataSources.v1.2.20180329/ \; -R ~/data/Homo_sapiens_assembly19.fasta -V 0816201804HC0_R01C01.vcf.gz \; -O 0816201804HC0_R01C01.funcotated.vcf; ```. The error is triggered somewhere near the position`4:60143686`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4739:167,ERROR,ERROR,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,The hstjdk downstream tests are failing and have been since we merged the repos. It looks like the failure are due to missing R dependencies on the worker nodes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3234:11,down,downstream,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3234,2,"['down', 'failure']","['downstream', 'failure']"
Availability,"The htsget.ga4gh.org appears to be down (tests get 404s, ping fails). This output is from my PR https://github.com/broadinstitute/gatk/pull/6799 that prints out the target URI:. ```; org.broadinstitute.hellbender.exceptions.UserException: Invalid request https://htsget.ga4gh.org/reads/A1-B000168-3_57_F-1-1_R2.mus.Aligned.out.sorted.bam, received error code: 404, error type: NotFound, message: The requested resource could not be associated with a registered data source; at org.broadinstitute.hellbender.tools.HtsgetReader.doWork(HtsgetReader.java:266); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:146); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:187); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); at org.broadinstitute.hellbender.testutils.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:111); at org.broadinstitute.hellbender.tools.HtsgetReaderIntegrationTest.testSuccessfulParameters(HtsgetReaderIntegrationTest.java:85); ```; Jermey (GA4GH dev) says:. > I recently updated the server, but my understanding was that the gatk build was spinning up a local server from an older image; > 11:41; > so htsget.ga4gh.org is using a newer image, while the gatk tests should pull an older image, spin it up locally, and then request from http://localhost. But based on the output above, it looks like we actually target `https://htsget.ga4gh.org/read...`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6803:35,down,down,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6803,4,"['down', 'error', 'ping']","['down', 'error', 'ping']"
Availability,The htsjdk downstream tests were put together before gradle had composite builds and are very hacky. They should be refactored to use composite builds instead of installing a strangely named maven artifact. . We should also split them into unit/ integration tests to reduce wallclock time. This should be easy since we already to it in travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3235:11,down,downstream,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3235,1,['down'],['downstream']
Availability,"The idea is that this WDL will run all the checks for each release of the VAT table, one call for each validation. The first validation rule (""Validation Check confirms that data is put into the VAT table after completing without an error."") is included as a model for subsequent calls. - workflow should succeed if it's able to try all tests; - workflow output `validation_results` will contain details of each test result in an array of `{""testName"": ""result details""}`:; Example 1 — [fail](https://job-manager.dsde-prod.broadinstitute.org/jobs/2728b55b-5344-492a-951a-48fd416e9d0d); `{ ""EnsureVatTableHasVariants"": ""FAIL: The VAT table spec-ops-aou.rsa_gvs_quickstart.rsa_scratch has no variants in it."" }`; Example 2 — [pass](https://job-manager.dsde-prod.broadinstitute.org/jobs/83e3bd5a-9144-452e-93d9-9f273055177f); `{ ""EnsureVatTableHasVariants"": ""PASS: The VAT table spec-ops-aou.anvil_100_for_testing.aou_shard_223_vat has 294821 variants in it."" },`; Example 3 — [the test wasn't able to run](https://job-manager.dsde-prod.broadinstitute.org/jobs/7179d111-02aa-4bca-a0a0-f55e10e43791); `{ ""EnsureVatTableHasVariants"": ""Something went wrong. The attempt to count the variants returned: Error in query string: Error processing job 'spec-ops- aou:bqjob_r357c4b6fe6b0c6fb_0000017aac301de7_1': Unrecognized name: vid at [1:24]"" }`. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/364",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7352:233,error,error,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352,3,"['Error', 'error']","['Error', 'error']"
Availability,"The important distinction is not between ""reviewed"" and ""unreviewed"" exceptions (all; use of exceptions should be reviewed in code review, after all), but between user mistakes; and internal sanity check failures. To this end, I've ported UserException from the old GATK codebase (preserving only the; most generally useful subclasses -- we can add more later if needed), removed the silly; ReviewedHellbenderException, and renamed HellbenderException to GATKException, which is; what should be thrown for all errors that are not the user's fault.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/85:204,failure,failures,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/85,3,"['error', 'failure', 'fault']","['errors', 'failures', 'fault']"
Availability,"The independent alleles model has some pretty pernicious error modes, but I understand the value of the ""exact"" implementation of the old model. I definitely want to default to the new model, but I understand how other users in the community might still be doing studies with large(ish) numbers of low coverage samples. It's probably worth keeping the old implementations for them, but usually I let @vdauwera advocate for the users. Great job @davidbenjamin ! I'm really excited about this!!! Right now I'm trying to decide which callsets I want to re-run first. :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242849348:57,error,error,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242849348,1,['error'],['error']
Availability,"The issue with the tests is that this PR is from a fork so the tests don't have access to the permissions we need for BigQuery in the variantstore WDL tests. I need to fix this for the future (by making sure those tests don't run when the PR is from a fork), but I don't think that test failure should block this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6872#issuecomment-710041086:287,failure,failure,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6872#issuecomment-710041086,1,['failure'],['failure']
Availability,The java. lang. StackOverflowError error in HaplotypeCallerSpark is caused by too many contigs in the reference genome file. Removing the contigs from the genome fragments will enable normal use of the HaplotypeCallerSpark function,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-1664004695:35,error,error,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-1664004695,1,['error'],['error']
Availability,The jenkins spark tests are failing with the following error:. This seems to have been introduced in https://github.com/broadinstitute/gatk/pull/3576. ```; code: 0; message: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; reason: null; location: null; retryable: false; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:339); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:197); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:194); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:91); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:194); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:614); 	at java.nio.file.Files.exists(Files.java:2385); 	at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:346); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:162); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:118); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:87); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:182); 	at org.broadinstitute.hellbender.engine.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591:55,error,error,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591,3,"['Error', 'error']","['Error', 'error']"
Availability,"The latest GATK release does significantly cut down on the number of critical vulnerabilities (mainly by moving to the latest Ubuntu 18.04 image), but there is definitely more work to be done here, so I'll keep this ticket open",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1496162653:47,down,down,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1496162653,1,['down'],['down']
Availability,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7787:72,error,error,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7787,2,['error'],['error']
Availability,"The main issue was that the `StatusRuntimeException`s that the baseline error handling code was trying to catch in practice always seem to be wrapped in at least one layer of exception of a different type. There was no catch handing for these wrapper exception types so the `CreateVariantIngestFiles` tool would simply crash. ~The changes here also more generally try to follow the recommendations in the [BQ Write API documentation](https://cloud.google.com/bigquery/docs/write-api#error_handling), in particular `close`ing the `JsonStreamWriter` before retrying error codes not explicitly called out by the documentation.~. EDIT: actually closing the writer didn't work out too well as we use the writer in `PENDING` mode and closing it seems to lose all pending writes. 😬 So in this circumstance we just throw and let WDL-level `maxRetries` start the data loading over from the beginning. An exponential backoff was also added before retry attempts. Parallel logic was also added to load status writing which should reduce (but not eliminate) the possibility of inconsistent sample status writes that require manual intervention. There is still the possibility of an inopportunely timed preemption, which is why VS-262 exists. All of the WDL changes here are in support of a 2000-sample tieout, a large enough set that intermittent BigQuery errors are almost always observed. The tieout confirms that errors of the two major classes are seen (retryable and non-retryable) and that the number of rows per sample in the tieout dataset matches those in a reference dataset.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7841:72,error,error,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7841,4,['error'],"['error', 'errors']"
Availability,"The main purpose of this PR was to output the new estimated bytes read from the Read API for better monitoring and debugging. However in the course of that I discovered that we were using ancient versions of the google APIs. No massive improvements from the release logs, but a lot of nice features (cleaner logging, automatic retries for certain errors, ). bigQuery 1.131.1 -> 2.3.3 [Release log for bigQuery](https://github.com/googleapis/java-bigquery/blob/main/CHANGELOG.md). bigQueryStorage 1.22.3 -> 2.5.0 [Release log for bigQueryStorage](https://github.com/googleapis/java-bigquerystorage/blob/main/CHANGELOG.md)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7601:347,error,errors,347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7601,1,['error'],['errors']
Availability,"The master branch failed on BaseRealibratorSpark when running WGS. Try to test this branch, but got hit by a strange error message. The jar file looks right to me. @tomwhite did you have some environment variables? . ````Using GATK jar /home/genomics/Projects/TomWhitePatches/gatk/build/libs/gatk-package-4.alpha.2-230-g19db939-SNAPSHOT-spark.jar; Running:; /home/genomics/Projects/spark/bin/spark-submit --master spark://n001:7077 --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --executor-memory 25G --driver-memory 5G /home/genomics/Projects/TomWhitePatches/gatk/build/libs/gatk-package-4.alpha.2-230-g19db939-SNAPSHOT-spark.jar BaseRecalibratorSpark -I hdfs://n001:54310/GATK4TEST/LargeBroadData/WGS-G94982-NA12878.bam -knownSites hdfs://n001:54310/GATK4TEST/DBSNP/dbsnp_138.hg19.vcf.gz -R hdfs://n001:54310/GATK4TEST/OldData/human_g1k_v37.2bit -O hdfs://n001:54310/GATK4TEST/LargeOutput/WGS_BQSR --sparkMaster spark://n001:7077; Picked up JAVA_TOOL_OPTIONS: -XX:+UseG1GC -XX:ParallelGCThreads=4; Picked up JAVA_TOOL_OPTIONS: -XX:+UseG1GC -XX:ParallelGCThreads=4; java.lang.ClassNotFoundException: org.broadinstitute.hellbender.Main; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.ja",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2620#issuecomment-299259877:117,error,error,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2620#issuecomment-299259877,1,['error'],['error']
Availability,"The more the best alignment mismatches its assigned reference location, the more stringent the filter becomes, that is, the lower the threshold for secondary alignments to constitute a multi-mapping becomes. If the best mapping mismatches at one base and the second best mismatches at three, that is very different from the best mismatching at four bases and the second best mismatching at six. @takutosato This corrects a failure to filter some very obvious clustered events mapping artifacts. For example, there were 11 ""events"" within 150 bp in one MC3 sample that were all being called as false negatives for M2 and true positives for M1. If these are real, I will eat dog food.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5501:423,failure,failure,423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5501,1,['failure'],['failure']
Availability,"The n1-standard-1 instance we'll be using has a single hyper-thread available to it from either a; 2.6 GHz Intel Xeon E5 (Sandy Bridge), 2.5 GHz Intel Xeon E5 v2 (Ivy Bridge), 2.3 GHz Intel Xeon E5 v3 (Haswell), or 2.2 GHz Intel Xeon E5 v4 (Broadwell). [Source](https://cloud.google.com/compute/docs/machine-types)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2424#issuecomment-282143266:68,avail,available,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2424#issuecomment-282143266,1,['avail'],['available']
Availability,"The new version of SplitNCigarReads now sets reads that are split from one original read to be supplementary to each other. Unfortunately, the tool does not respect any existing supplementary reads as being supplementary to each other. Currently the tool clobbers any existing SA tag and sets one read from each read group as primary which is undesirable as it corresponds to loosing information from the aligner. One solution is to use the same ""predicting"" approach for existing SA information to attempt to repair the corresponding SA tag for each read based on how it would be split given its cigar string. Perhaps there is another solution that is more in line with what the SA tag gets used for. . In order to make these tags 100% accurate with overhang fixing on however, the OverhangFixingManager will probably need to be changed to store the mate information for every read it changes between file iterations. Currently it only stores information on a single read from every read group as it would be the only one that affects mate information but this assumption is invalidated as the SA tag needs to keep information on EVERY read in a canonical alignment to be correct, not just the first one.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2116:510,repair,repair,510,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2116,1,['repair'],['repair']
Availability,The newest release of GenomicsDB treats spanning deletions (spanning; from earlier positions) as deletions in the min PL value computation.; This behavior now matches the behavior of CombineGVCFs. A more detailed description of the issue is provided in; https://github.com/broadinstitute/gatk/pull/4963. * Deleted a couple of files which are no longer necessary.; * Fixed the index of newMQcalc.combined.g.vcf; * Fixes #5045 (error out with a helpful error message); * Fixes #5300,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5397:426,error,error,426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5397,2,['error'],['error']
Availability,The newly enabled GermlineCNV wdl tests seem to take ~1 hour 8 minutes. The maximum timeout for our travis jobs is 1 hour 10 minutes. This is causing random failures. It's also the slowest of our matrix entry by a large margin which makes our tests even slower than they are. Can these be speed up?. Alternatively we may need to talk to the travis people and get a special dispensation to increase our job timeouts even farther.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4064:157,failure,failures,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4064,1,['failure'],['failures']
Availability,"The next release is going to be a big 4.1 minor version release, so there are a lot of new features we're trying to finish up. Hopefully it will be by the end of the month, but a nightly will be available with this change by tomorrow: https://hub.docker.com/r/broadinstitute/gatk-nightly",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5563#issuecomment-452020693:195,avail,available,195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5563#issuecomment-452020693,1,['avail'],['available']
Availability,"The one check that fails, I have restarted twice with the same error message:; ```; The job exceeded the maximum time limit for jobs, and has been terminated.; ```. The limit appears to be around 49 minutes. I have seen this type of check failure for my other PRs. Why does this keep happening and is it possible to up the time limit?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2812#issuecomment-306047630:63,error,error,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2812#issuecomment-306047630,2,"['error', 'failure']","['error', 'failure']"
Availability,"The only reason the logs are so long right now is because git-lfs is failing intermittently to download the large files. Under normal circumstances we definitely want to see in the logs when a test is skipped, as there should not be many (or any) skipped tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-292027095:95,down,download,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-292027095,1,['down'],['download']
Availability,"The option -XX:ParallelGCThreads=8 will not increase the number of threads GATK uses. That increases the number of threads available to the JVM garbage collector, and this will not speed up your jobs. I suggest taking a look at the GATK forums, the following two articles are good starting points for running your jobs in parallel. Pipelining GATK with WDL and Cromwell; https://gatk.broadinstitute.org/hc/en-us/articles/360035889771. Parallelism - Multithreading - Scatter; https://gatk.broadinstitute.org/hc/en-us/articles/360035532012-Parallelism-Multithreading-Scatter-Gather",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6941#issuecomment-723179613:123,avail,available,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6941#issuecomment-723179613,1,['avail'],['available']
Availability,"The other option is to insert ""N"" for the REF in the VC objects when a reference genome isn't passed. Some other downstream tool must figure out the right base.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2624#issuecomment-297819378:113,down,downstream,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2624#issuecomment-297819378,1,['down'],['downstream']
Availability,"The packages for codecs is a key feature for downstream tools implementing new codecs for other formats or to include overrides of codecs already included. Nevertheless, the current implementation (at version 4.0.0.0) the only way of configuring this is at the package level using the `codec_packages` configuration. I request support for the following fine-grained configuration:. * Add/Remove concrete codec classes; * Exclude single classes from a concrete `codec_package` specified (this can be done by the previous requirement if it uses fully qualified codec names); * Exclude sub-packages from a concrete `codec_package` specified. Representing this in an YML format, I would like to have the ability to configure the codecs as following:. ```yml; - codecs:; - packages:; - htsjdk.variant; - htsjdk.tribble; - exclude_class: bed.BEDCodec; - org.broadinstitute.hellbender.utils.codecs; - exclude_package: gencode; - org.magicdgs.htsjdk.codecs; - classes:; - org.external.htsjdk_codecs.CustomBedCodec; ```. This would be even more useful if HTSJDK is moving to an interface-based library...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4180:45,down,downstream,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4180,1,['down'],['downstream']
Availability,"The patch clears up the 503 failures due to `fetchSize()`, but we are STILL seeing 503's with other metadata operations such as `Files.exists()`:. ```; com.google.cloud.storage.StorageException: 503 Service Unavailable; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:586); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:428); at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:217); at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:223); ```. I'm going to continue modifying the patch until we see all 503s go away, then post here once it's ready.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-314813629:28,failure,failures,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-314813629,2,['failure'],['failures']
Availability,"The point of the second test is to make sure that the new code path involving the spanning deletion has no effect if the spanning deletion has low likelihood. You could imagine that I introduced a bug where you lose sensitivity whenever a spanning deletion appears in the alleles list even if there's very little evidence for it, just because its potential existence sends the code down some new, incorrect, logic. I think `testSpanningDeletionIsNotConsideredVariant` is already doing what you want because before this fix the example VC is definitely considered to be variant even though the only non-ref evidence is for the symbolic `*` allele. The PLs in this example are 50, 100, 100, 0, 100, 100, which means that it is overwhelmingly likely that this sample's genotype is REF/SPAN_DEL. In the old code you get a qual of about 50 (the hom ref PL), whereas in the new code you get that the chances of a variant are about 1 in 10^10 since the only truly variant PLs are 100. Now, if you think a multi-sample case would be useful that sounds good to me. However, a ""confident deletion in sample1 and a low quality SNP in sample2"" would not look very different since the deletion would only exist in an upstream VC, not the VC being tested, so basically we would have sample1 = REF/SPAN_DEL, sample2 = REF/SNP with low quality. The current test is the same thing with just sample1. Is this what you had in mind?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391472795:382,down,down,382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391472795,1,['down'],['down']
Availability,The previous error was for another sample too. The was the only one I had recorded. That's why the chr position are not exactly the same,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1416093097:13,error,error,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1416093097,1,['error'],['error']
Availability,"The problem is in https://github.com/broadinstitute/picard/blob/aa911f9440e70f59a32dddadd90e5a07c7764d87/src/main/java/picard/cmdline/CommandLineProgram.java#L267-L274, which is using barclays `CommandLineParser.getVersion`, which implementation uses the `this.callerOptions.getClass().getPackage().getImplementationVersion()` (see https://github.com/broadinstitute/barclay/blob/master/src/main/java/org/broadinstitute/barclay/argparser/CommandLineArgumentParser.java#L268). I think that in general is a good idea to include the versions in the `MANIFEST`, but it should be somehow extensible for downstream projects - and I guess that it should be done at the barclay level to do not populate directly the `Version` attribute but the `ToolkitName-Version` attribute...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-387349805:597,down,downstream,597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-387349805,1,['down'],['downstream']
Availability,The problem is in practice if a read is unmapped along with its mate then there is no way that you can recover that read with any interval given to these tools. If you wish not to lose any reads then it is important not to provide any interval to ApplyBQSR step and run the tool as a single entity to collect all reads together. If you are ApplyBQSR in parallel to collect reads then you may not be able to rescue any unmapped pairs. Mapped reads with an unmapped pair may be rescued but I am not sure how other filters will affect. When I checked my own bam files sorted and bqsr applied bam files have the same number of reads inside already. . Also you may need to check your bam file to see if there are any problematic reads present.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8523#issuecomment-1725286670:103,recover,recover,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8523#issuecomment-1725286670,1,['recover'],['recover']
Availability,"The problem is that the catch block in `CommandLineProgram` is calling both `commandLineParser.usage()` and `printDecoratedUserExceptionMessage()` -- it should only be calling `commandLineParser.usage()`, and letting the catch block in `Main.mainEntry()` call `printDecoratedUserExceptionMessage()`. Otherwise there are cases where a `CommandLineException` will be caught without printing any error message. This is a bug and should be fixed. The distinction between ""errors that are the user's fault"" and ""errors that are not the user's fault"" is very important for our support team -- it allows them to deal with bug reports and forum questions much more efficiently. Whatever solution we come up with here should maintain that distinction, and clearly label errors like ""bad argument value"" as being a user error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938:393,error,error,393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268712938,14,"['error', 'fault']","['error', 'errors', 'fault']"
Availability,"The problem is that the package can't download any files during build due to security reasons. It has to use predefined, repeatable set of dependencies, and can't download random versions of gradle either. It has to use the ```devel/gradle``` port, and once this port has been upgraded to version 5.0 gatk became broken. ```devel/gradle4``` had to be created. Generally, indiscriminate downloads of files lead to security problems like the one that recently happened with a particular bitcoin-related node code, which was using a zillion node dependencies. One of them got infiltrated by a criminal who eventually stole bitcoins because he managed to inject his code into financial applications running on user's sites. To prevent such things from happening, all major packaging systems only use fingerprinted dependencies, and can't ""just download"" some alternative version of something during build, contrary to what devs might be expecting.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5483#issuecomment-444205059:38,down,download,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5483#issuecomment-444205059,4,['down'],"['download', 'downloads']"
Availability,"The problem is that when alignments are de-overlapped (i.e. they overlap on the read) in the CPX logic, some resulting alignment could be only 1 base long, leading to problems. This was not caught before because in practice a heuristic alignment filtering step is in place in `AssemblyContigAlignmentsConfigPicker` to filter out, in a post-hoc way, such small alignments.; When I experimented with switching orders of the filtering steps, this behavior was observed. The aim is to make the downstream logic agnostic to upstream filtering detail.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4962:490,down,downstream,490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962,1,['down'],['downstream']
Availability,"The problem was ported from GATK 3 ([here](https://github.com/broadinstitute/gsa-unstable/pull/1369) and [here](https://github.com/broadinstitute/gsa-unstable/issues/1368)), and seems like more error(s) are introduced. Essentially, the fields`cachePloidyCapacity` and `cacheAlleleCountCapacity` are mixed, and fixing should be easy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1856:194,error,error,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1856,1,['error'],['error']
Availability,"The proposed PR doesn't fix the problem completely. When running local[1] it fails with the same error(too many open files) after finishing ~400/555 partitions - without the fix it failed after ~25. When running in parallel(local[n], n>1) it starts hanging after processing around ~40 partitions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6642#issuecomment-640266295:97,error,error,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6642#issuecomment-640266295,1,['error'],['error']
Availability,"The pull request addresses two issues:. 1. Improved and more robust parsing of FlowBasedReads. Specifically, the code now determines the minimal reportable quality; 2. New tool AddFlowSNVQuality that allows users to convert the flow-based quality format when every base quality reports probability of an insertion or deletion to a conventional format that gives base qualities (total probability of mismatch and probability of each mismatch in separate tags). . We believe that this tool is going to be important for users of the Ultima Genomics data that care about calling SNVs, especially in somatic setting, so the goal was to make documentation more accessible. . Happy to receive feedback about it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8697:61,robust,robust,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8697,1,['robust'],['robust']
Availability,"The quality score sum (QSS) format field reports the sum of base qualities for ref and alt allele, and its correct format field count type is 'R' (see 1.2.2 in the [vcf spec](http://samtools.github.io/hts-specs/VCFv4.2.pdf)). We used to use the wrong count type of 1 and that caused a parsing error in the dream challenge evaluation script.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2229:293,error,error,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2229,1,['error'],['error']
Availability,"The reason I was getting different behavior was because my local htsjdk was built from a fork, and it had stale tags. That doesn't matter for GATK, since the GATK build.gradle uses a resolution strategy of ""force"" for htsjdk, forcing it to use the version I specified (even though it looks old due to the old tags). But the gatk-protected build.gradle doesn't have an explicit resolution strategy declared, so gradle resolves conflicts via ""pick the newest one"". Since there are newer versions available in gatk-protected via other transitive dependencies, it was choosing those over my local one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2578#issuecomment-292971228:494,avail,available,494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2578#issuecomment-292971228,1,['avail'],['available']
Availability,"The recent branch #8489 has demonstrated that there are some problematic edge cases in the pileup allele merging code that could cause pathological numbers of haplotypes to be handed to the genotyper. In updating the bug in that branch it was observed that it is very common that there are score ties at the 5th haplotype level for the pileupcaller as illustrated by the noise in the updated tests. This algorithm is not a good heuristic and we should replace it with something better, some ideas from that branch that might fix a few of its shortcomings:. 1) Increase/decouple the kmer size used with the reads from the assembly graph kmer size to prevent the filtering step from being redundant with assembly; 2) Normalize the scores to the haplotype lengths to deal with haplotype size bias.; 3) Change the scores to instead reflect the absolute count of unsupported kmers from the graph to also deal with hapotype size bias. ; 4) Iteratively expand the kmer size used for filtering to pare down the number of haplotypes in a more principled fashion.; 5) Utilize the read kmer occurrence counts to construct the scores in order to reduce the risk of spurious reads being sufficient support for a given haplotype. . We have observed that there can be significant changes to the actual genotyping engine output from the pileup engine from even relatively minor changes to the pileupcalling merging code. We should strive to find a more principled solution for merging haplotypes than the one we have currently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8494:687,redundant,redundant,687,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8494,2,"['down', 'redundant']","['down', 'redundant']"
Availability,"The result of this bug is that if you produce a BAM with Spark, then produce a different BAM with the same name with the walker-framework, then try to read _that_ bam with Spark, it produces checksum (and other more misleading) errors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1266:228,error,errors,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1266,1,['error'],['errors']
Availability,"The same error was reported in other modules of GATK4. For eacmple, in GenomicsDBimport [Ref.1]. And I encountered it again when I check the bams with ValidateSamFile. [1] https://gatk.broadinstitute.org/hc/en-us/community/posts/360076477211-GenomicsDBimport-not-importing-all-the-batches?page=1#community_comment_4411519756827",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582#issuecomment-991512945:9,error,error,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582#issuecomment-991512945,1,['error'],['error']
Availability,"The same for HaplotypeCaller (GATK 4.1.1.0); If this persists, and it seems like it, I will try to switch to pure-java, as retry takes couple of hours and ruins the workflow. ```; --smith-waterman / NA; Which Smith-Waterman implementation to use, generally FASTEST_AVAILABLE is the right choice; The --smith-waterman argument is an enumerated type (Implementation), which can have one of the following values:. FASTEST_AVAILABLE; use the fastest available Smith-Waterman aligner that runs on your hardware; AVX_ENABLED; use the AVX enabled Smith-Waterman aligner; JAVA; use the pure java implementation of Smith-Waterman, works on all hardware; ```. ```; *** Error in `java': munmap_chunk(): invalid pointer: 0x00007fafc8ed1000 ***; ======= Backtrace: =========; /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7fafce3f37e5]; /lib/x86_64-linux-gnu/libc.so.6(cfree+0x1a8)[0x7fafce400698]; /home/flexray/germline/cromwell-executions/PairedEndSingleSampleWorkflow/af1ee082-8661-4a7a-adf9-1b2a67333d37/call-HaplotypeCaller/shard-40/tmp.42584bbe/libgkl_smithwaterman205796788520033039.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7faf73bfcfa8]; /home/flexray/germline/cromwell-executions/PairedEndSingleSampleWorkflow/af1ee082-8661-4a7a-adf9-1b2a67333d37/call-HaplotypeCaller/shard-40/tmp.42584bbe/libgkl_smithwaterman205796788520033039.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7faf73bfcbf8]; [0x7fafb9a7eea2]; ```. and ; ```; *** Error in `java': double free or corruption (out): 0x00007f933d610780 ***; ======= Backtrace: =========; /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f93434427e5]; /lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f934344b37a]; /lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f934344f53c]; /home/flexray/germline/cromwell-executions/PairedEndSingleSampleWorkflow/fa8e6a15-021e-48cc-9429-c53596fc9c29/call-HaplotypeCaller/shard-19/tmp.ea81c1bd/libgkl_smithwaterman4419442010051805328.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f9248e4bfa8",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-485227509:446,avail,available,446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-485227509,2,"['Error', 'avail']","['Error', 'available']"
Availability,"The same situation here... ; I am using ploidy (100) samples and the newest gatk 4.1.8.1. I was calling variants using HaplotypeCallerSpark on HPC. gatk --java-options ""-Xmx30g"" CombineGVCFs \; --reference $path_ref/$assembly.fa \; --variant $path_calls/H1_1.spark.g.vcf.gz \; --variant $path_calls/H1_2.spark.g.vcf.gz \; --variant $path_calls/p33xLer_F2_1.g.spark.vcf.gz \; --variant $path_calls/p33xLer_F2_2.g.spark.vcf.gz \; --output $path_calls/cohort.g.vcf.gz. results in:. 15:11:14.493 INFO CombineGVCFs - Shutting down engine; [August 5, 2020 3:11:14 PM CEST] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 6.75 minutes.; ***********************************************************************. A USER ERROR has occurred: Bad input: Combining gVCFs containing MNPs is not supported. Unknown contained a MNP at 1:560576. ***********************************************************************. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-669184874:521,down,down,521,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-669184874,2,"['ERROR', 'down']","['ERROR', 'down']"
Availability,"The script copies the source workspace to a new location and executes the append/consolidate there. It did fail, but after failure it deletes the working copy and re-copies the original to that location. It's failed in the same contig/batch multiple times. Given this workspace has gone through multiple rounds of append and we did have errors during its creation, there is the possibility of an incomplete cleanup. Based on what @mlathara writes, it seems like it is possible for an error in the past to get carried forward, perhaps without causing new errors until --consolidate is used?. I do see some evidence of duplication, so I'm going to run a more thorough md5 check across all the contig folders. I also see subdirs with widely different modified dates; however, I suppose this could be the result of incremental import. This particular set of jobs is running GenomicsDbImport on the workspace, scattered as 48 jobs. About half the jobs have finished without errors. . I'll post the results of the workspace-wide check for duplicated fragments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722584082:123,failure,failure,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722584082,5,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"The somatic are labeled as germline and visa-versa. The files themselves must be changed, as well as the data source downloader (to point to the new paths).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5428:117,down,downloader,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5428,1,['down'],['downloader']
Availability,"The source of the index out-of-bounds errors, as @lbergelson discovered, was a line or two of code that was not ported from @vruano 's original PR (https://github.com/broadinstitute/gsa-unstable/pull/1389)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2710#issuecomment-321369363:38,error,errors,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2710#issuecomment-321369363,1,['error'],['errors']
Availability,"The symptom is that I'm getting an invalid record when using the Hadoop-BAM probabilistic splitter. I wrote a small program to verify the bug and the record it's happening on. To run this, check out da_key_bug and run something similar to my command. ```; gcloud beta dataproc jobs submit spark \; --cluster high-mem-16-8-sd \; --properties spark.executor.memory=38g,spark.executor.instances=15,spark.executor.cores=7 \; --class org.broadinstitute.hellbender.Main \; --jar build/libs/gatk-all-4.pre-alpha-*-spark.jar \; KeyReadsSpark \; -I hdfs:///user/davidada/CEUTrio.HiSeq.WGS.b37.NA12878.bam \; --bps 4194304 \; --sparkMaster yarn-clien; ```. The important bits are the BAM (`gsutil cp gs://jpmartin/hellbender-test-inputs/CEUTrio.HiSeq.WGS.b37.NA12878.bam .` should work to grab it) and the bps being set to `4194304`. It doesn't happen with a different split. (Yet more evidence pointing to Hadoop-BAM). The bad record is on chromosome 1 and starts at 801305857. I imagine that making a small input would work to find this issue (instead of needing to use the 300 GB BAM). Assigning to Uri (I'd assign it to Tom, but I don't know when he's back).; If you need help cutting down that BAM @droazen and @lbergelson can give some advice.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1098:1179,down,down,1179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1098,1,['down'],['down']
Availability,"The test `testSortingByColumn` doesn't actually test anything and likely never has. . It throws and silently swallows an exception, which masks the fact that it's creating an empty table to test, and doesn't work when the table isn't empty. This has been inherited unchanged from Gatk3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1465:138,mask,masks,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1465,1,['mask'],['masks']
Availability,"The test failure in the germline CNV tests is definitely unrelated to this PR and is a current known intermittent issue, so I'm going to go ahead and merge",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6807#issuecomment-706419777:9,failure,failure,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6807#issuecomment-706419777,1,['failure'],['failure']
Availability,"The test failures in the branch build are clearly related to the recent travis key migration. The PR build (which is the one we care about) passes, so this should be safe to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7393#issuecomment-953279491:9,failure,failures,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7393#issuecomment-953279491,2,['failure'],['failures']
Availability,The test is broken. It never checks that the actual file written was in the format that was expected. There's a bug where format isn't being set appropriately to bcf. Tracking down where that happens... I suspect we're not setting OUTPUT_VCF_FORMAT_PROPERTY correctly.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4463#issuecomment-385024632:176,down,down,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4463#issuecomment-385024632,1,['down'],['down']
Availability,"The tests were failing for some reason, probably due to some dependency downloads failing, so I'm rerunning them. Will merge when that's done.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5021#issuecomment-405984274:72,down,downloads,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5021#issuecomment-405984274,1,['down'],['downloads']
Availability,"The tests would be very short-running, so each run should be cheap. Most of the runtime will be just spinning up the cluster. If someone malicious opened up a crazy number of PRs, he would at least quickly hit our travis quota limit which would slow him down considerably. We can look into whether other mechanisms exist on travis to deal with this sort of hypothetical situation. We do ideally want these tests to work with forked PRs, but if that's simply not going to be possible on travis then we'll obviously have to choose between having these tests in the same place as all of our other PR-related tests on travis and just skipping them for forked PRs, vs. having them in jenkins and requiring those making and reviewing PRs to deal with/check both CI environments. Let's meet early next week to chat about this issue and come to a decision.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886:254,down,down,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287544886,2,['down'],['down']
Availability,"The two main resources limiting how many gVCFs you can import at once will be memory and file handles. . I'm not sure if you mean incremental import or batch size when you mention the iterative approach. I assume the latter, but just want to clarify that there isn't any reason to break up the import using incremental import. The batch size parameter effectively does that, so you might as well import all gVCFs you have available (optionally using batch size if you're running out of memory). I'll throw out batch sizes of 50 or 100 as being reasonable, but the size of the intervals being imported will make a difference. It would be best to try to monitor how much memory is being used with those settings. There won't be a huge difference in import performance depending on the number of batches (ignoring memory issues) but if you have more than a 100 or so batches and you don't enable the `--consolidate` option you may see some query performance degradation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656376952:422,avail,available,422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656376952,1,['avail'],['available']
Availability,"The underlying issue here is is that the GATK conda env environment isn't established since bioconda doesn't appear to configure it. The NPE needs is fixed by #7816. In this particular case it appears that some of the requirements are satisfied, since the code gets past the initial check to see if the GATK python code is available. But then the actual CNN code can't be loaded.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1110010269:323,avail,available,323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1110010269,1,['avail'],['available']
Availability,"The user has posted an update with the jstack logs and they can be downloaded here (https://gatk.broadinstitute.org/hc/en-us/community/posts/360076845511/comments/360014258071) ; They also provided info that when they run GenomicsDBImport for the two samples in the same command, GenotypeGVCFs completes in 14 minutes. But if they import one sample at a time (using --genomicsdb-update-workspace-path) the GenotypeGVCFs process appears hung.; @nalinigans @mlathara Any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7070#issuecomment-776084422:67,down,downloaded,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7070#issuecomment-776084422,1,['down'],['downloaded']
Availability,"The user mentioned and I agree:; Since the interval list isn't a required argument listed in the GenotypeGVCFs documentation, perhaps a note could be added to the --all-sites parameter to indicate that it should be used in conjunction with a specified -L parameter. Clarifying the error message will also be a big help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5865#issuecomment-480978282:281,error,error,281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865#issuecomment-480978282,1,['error'],['error']
Availability,"The varianteval package (VariantEval, evaluators/stratifiers and stratification manager) was ported directly from GATK3 to minimize diffs for review, and needs a code-style cleanup pass:. - rename variables with names in ALL_CAPS; - remove redundant type instantiation params in favor of <> operator; - add finals; - revisit the use of generic type params and required casts, etc in StratificationManager and stratifier classes",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5440:240,redundant,redundant,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5440,1,['redundant'],['redundant']
Availability,"The warnings the user is seeing are due to spanning deletion alleles which are currently not annotated with Funcotator. The bug here is what is causing the stack trace. It's in the protein sequence prediction code and I suspect that it has to do with the position of the variant relative to the exon/transcript boundaries. I have not been able to look at it yet, but thanks to the user posting the variants that are causing issues, it should be straight-forward to track down.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-644794980:471,down,down,471,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-644794980,1,['down'],['down']
Availability,"The way start positions are calculated causes issues when alleles span the boundaries of exons and the coding sequence itself. For exon boundaries:. The error stems from `FuncotatorUtils::getStartPositionInCodingSequence` and how the results of that method call are used in the `GencodeFuncotationFactory`. In addition:; Funcotator must be able to handle indels that span exon start boundaries. For example, in hg19 the following variant is not handled properly:. chr5:71622537-71622538 CA->C. The current workaround is to throw a FuncotatorUtils.TranscriptCodingSequenceException for the transcript causing this problem in Gencode. For Coding sequence boundaries, the following variant in hg38 causes a problem:. ```; chr17 80090386 rs71163918 CAGCACGTGCATGAACAACACAGGACACACACAGCACGTGCATGAACAACACAGGACACACACA C . PASS .; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4307:153,error,error,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4307,1,['error'],['error']
Availability,The work that @tomwhite has done in the HalpotypeCallerSpark has illuminated the fact that currently the downsampler is statefully dependent on the random generator underneath in terms of how it selects reads to be downsampled. This has become an issue since we would like to separate the process of assembly region construction and genotype calling across stages of the spark task. In order to do this successfully there needs to be some way to reproduce the same downsampled results for a given site based solely on the reads present at that site.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5437:105,down,downsampler,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5437,3,['down'],"['downsampled', 'downsampler']"
Availability,"There are currently two failing tests, both of which need fixes in htsjdk.; * CountVariantsSparkIntegrationTest. This is a concurrency issue where VCFCodec (which isn't threadsafe) is being shared by multiple tasks in each Spark executor. The fix is for each task (partition) to have its own VCFCodec. This needs a couple of small changes in htsjdk to make it possible to access the codec's version and header fields so the codec can be recreated. See https://github.com/samtools/htsjdk/pull/1176.; * CpxVariantReInterpreterSparkIntegrationTest. I tracked this down to a problem with the buffer in htsjdk's SeekableBufferedStream. See https://github.com/samtools/htsjdk/pull/1175 for the fix.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5138#issuecomment-417241035:561,down,down,561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5138#issuecomment-417241035,1,['down'],['down']
Availability,There are many useful utilities in there. Some of them should be pushed down to HTSJDK; some of them should be moved to Hellbender; and some of them are redundant with existing GATK utilities (e.g. MathUtil) and should be combined or skipped as necessary.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/215:72,down,down,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/215,2,"['down', 'redundant']","['down', 'redundant']"
Availability,"There are many ways of corrupting data in TileDB/GenomicsDB - I am listing the protections available.; * Under the current setup, data is imported once by a single process - the _failIfUpdating_ flag is used in the GATK-4 import tool to ensure this.; * If an importer process crashes in the middle of execution, only the fragments that are fully completed will be visible to downstream queries/reads. Partially written fragments are on disk but ignored. This is achieved by renaming the fragment directory from _.<fragment_id>_ to _fragment_id_. While this is not an atomic operation, under the current use case, I cannot think of any way that will cause issues. At most, you waste some cycles re-importing data. What's not protected:; * Mapping data - sample name to TileDB row id, chromosome name to TileDB column interval. No plans to protect this in the current implementation. Once we have the PostgreSQL based storage for the mapping data, we will be able to detect and prevent inconsistencies.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2536#issuecomment-305557716:91,avail,available,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2536#issuecomment-305557716,2,"['avail', 'down']","['available', 'downstream']"
Availability,"There are no error messages.; The process was interrupted without any error messages.; I attached the screenshot.; I attached chr14 variant calling (completed) and chr14 variant calling; (interrupted).; In the system monitor, when I am using GATK 4.6.0.0., they are eating up; memory continuously.; When they are reaching up to 512Gb, the process was interrupted.; I tried to run this process on only 2-3 chromosomes, and I found that the; process was completed on chr 14, and the process was interrupted on the; rest of two chromosomes (interval -L).; So I rolled back to GATK 4.5.0.0, the process was normal. I can do; GenotypeGVCFs command entire chromosome simultaneously. My machine has 512Gb memory and 64 cores (5995wx AMD threadripper) dell; 7865 workstation.; Thanks; Jinu Han. On Fri, Jul 19, 2024 at 12:08 AM Gökalp Çelik ***@***.***>; wrote:. > Can you provide your logs that shows the error message?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2236819113>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AG7IXWWGPB73BXPN4Z5E4VTZM7LAFAVCNFSM6AAAAABLBRETECVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDEMZWHAYTSMJRGM>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2238806751:13,error,error,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2238806751,3,['error'],['error']
Availability,"There are params in Extract Cohort that can be tightened up. Extract Tool has two params that are not used by Extract Features and are _already_ in Extract Cohort. The filter_set_name is used in the BQ filtering tables and looks like we can set it to be completely required for any type of filtering. There are 3 BQ filter tables -- 2 are needed for filtering (no matter what?) and 1 (tranches) is needed for thresholding and sensitivity calculations?. Genotype level filtering is true by default, but this doesn't seem like it should effect things after this cleanup. Though technically it should only be true if a filter_set_name has been specified. I will add another comment in the body of the code, but I would like to add this safety gate explicitly. Disable gnarly doesn't need to be a passed in param---so we'll rip it out for now. SNP and INDEL truth sensitivity and SNP and INDEL Lod scores are cumbersome to have to worry about passing in, but I dont see a better alternative. Should there be additional validation on these (where if they are specified, but no filter_set_name is, then they throw an error?)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7293:1111,error,error,1111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7293,1,['error'],['error']
Availability,There are places in the genome where IUPAC bases can actually be decoded because the amino acid code is partially redundant. Add this logic into getMitochondrialAminoAcidByCodon and getEukaryoticAminoAcidByCodon.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6777:114,redundant,redundant,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6777,1,['redundant'],['redundant']
Availability,"There are pretty significant incompatibilities between java 8 and 11 that make it hard to run the same code on both. It affects a number of our dependencies which use features which were removed/altered from java 8 -> 11. Unfortunately despite there being significant pain in switching to 11 there aren't particularly compelling new features after 8 so there isn't much incentive for developers to move forward. That said, you CAN now run gatk on java 11 if you build it using java 11, the jars built on 8 are incompatible with 11 and vice a versa. We consider running on 11 to be a beta feature and would love to hear feedback about either success or failure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6298#issuecomment-561371535:652,failure,failure,652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6298#issuecomment-561371535,2,['failure'],['failure']
Availability,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5862:1086,error,errors,1086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862,2,"['down', 'error']","['downstream', 'errors']"
Availability,"There are several changes in this PR:. * Re-use Picard's supercategory map and supercategory constants. This will keep in line Picard an GATK supercategories without the need of modifying the code in `HelpConstants`; * Re-use Picard's Testing program group. The `TestProgramGroup` is a duplicate of Picard's `Testing` program group. This duplicated also some constants from the Picard's code. Again, this will maintain in sync GATK.; * Remove the unused **""RNA-Specific Tools""** category constants, which was unused.; * Make public the `getSuperCategoryMap` as in Picard's code, to allow downstream projects to re-use supercategory assignation in their own documentation, and include their own.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4247:588,down,downstream,588,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4247,1,['down'],['downstream']
Availability,"There are some post-rebase failures, but nothing serious. You can go ahead with review and the rug won't be pulled out from under you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6520#issuecomment-602820540:27,failure,failures,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6520#issuecomment-602820540,1,['failure'],['failures']
Availability,There are still 17 (!) references to the script in its previous location. Is it possible to bring that number down?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8268#issuecomment-1508427383:110,down,down,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8268#issuecomment-1508427383,1,['down'],['down']
Availability,"There are two issues with running this on Travis:; 1. Spinning up Dataproc requires access to a bunch of secrets -- namely API Keys and Google Service accounts (usually in the form of a JSON). Travis allows for encrypted Secrets, so I'm okay with them existing there, but they are ONLY available to PRs that originate from the repo (ie, no forked PRs have access to them). So doing this in Travis would only work from your local PR. Is that acceptable? If not, no Travis that I can think of. 2. We don't want ANYONE to just come in and launch these tests as they are $ expensive (spinning up clusters in Google). If this activates with any PR that comes in, then we can easily run up a bill by any malicious A-hole who feels like launching a bunch of PRs. I know that when I was 14 or so I would have probably done that if I discovered it. Jenkins has some mechnaisms to prevent run away PR tests. Does Travis? I didn't find any info on that... Happy to hash it out in a room (and sorry it took this long to get to. Pester Banks if you need my attention faster otherwise Workbench and Security get first dibs). But be aware of these issues with Travis and running dataproc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287539859:286,avail,available,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2298#issuecomment-287539859,1,['avail'],['available']
Availability,There is a copy of the 4.1.0.0 pipeline here: https://app.terra.bio/#workspaces/help-gatk/Mitochondria-SNPs-Indels-hg38. We are working on some adjustments to this pipeline to make the low allele fraction calls more reliable with more aggressive filtering and we will put those updates in that same location once they're complete.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-488790341:216,reliab,reliable,216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-488790341,1,['reliab'],['reliable']
Availability,"There is a lot going on here... Let me see if I can explain... . First things first, there are two versions of ``CollectSequencingArtifactMetrics`` ... a GATK port and the Picard version. The GATK port has show-stopping issues, so the picard version is used in the meantime, which necessitates that ``sed`` stuff... Second, the way downstream tools are forced to parse the ``CollectSequencingArtifactMetrics`` output file is a bit weird. The class to parse the file is specified in a comment. So if this is incorrect or does not match GATK vs. Picard, the downstream tool (e.g. ``FilterByOrientationBias``) gets an error. > As for my input files being busted. Which file do you mean? As you can see from my late-night post, I do get the tool to run using the odd workarounds. ``java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1})``. This is has something to do with your input file. If I am wrong and this is truly a code error, then the fix is not in ``FilterByOrientationBias``. @lbergelson Can you confirm? . > Also, why @LeeTL1220 have you changed the title of this issue ticket? Can you please explain? Again, I reiterate that CollectSequencingArtifactMetrics works just fine. It is FilterByOrientationBias that errors without these workarounds. ``CollectSequencingArtifactMetrics`` (GATK version) definitely does not work just fine. I've cited the issue above. I do not think that any of the code fixes would actually go into FilterByOrientationBias. If the GATK version worked just fine, we would ditch the ``sed`` command and use that. ``CollectSequencingArtifactMetrics`` (Picard version) works just fine BUT it introduces that string which requires the ``sed`` statement. If you do not use ``sed``, a (gene",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306525143:332,down,downstream,332,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306525143,3,"['down', 'error']","['downstream', 'error']"
Availability,"There is a potential fix for the ""next on empty iterator"" error in PR #6652 -- this should be part of the next GATK release, and may enable us to close this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6319#issuecomment-664566481:58,error,error,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6319#issuecomment-664566481,1,['error'],['error']
Availability,There is also a new argument in FilterAlignmentArtifacts `--smith-waterman` that lets you choose to use the java version or the faster but previously buggy accelerated version so there is no need to downgrade to an old gatk to work around now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-993705731:199,down,downgrade,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-993705731,1,['down'],['downgrade']
Availability,"There is an issue in with gcloud 208.0.0 that prevents the `gcloud dataproc` commands from functioning. We should temporarily pin our version to 207.0.0 instead until the problem can be addressed by google. . ```; Customers affected by this issue are using gcloud version 208.0.0 and may experience an error like ""Problem loading gcloud.dataproc.clusters.create: No module named jsonschema."" when interacting with Google Cloud Dataproc.; Workaround; The workaround is to use gcloud version 207.0.0, a downgrade from 208.0.0 can be done by issuing the command: ""gcloud components update --version 207.0.0"". If installed via apt: sudo apt-get update && sudo apt-get install google-cloud-sdk=207.0.0-0. If installed via yum: sudo yum downgrade google-cloud-sdk-207.0.0; ```. We have to remember to unpin it afterward the problem is fixed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5003:302,error,error,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5003,3,"['down', 'error']","['downgrade', 'error']"
Availability,"There is another issue with how this annotation interacts with germline calling, specifically an issue coming up in GenomicsDBImport when users add all annotations to their GVCF from HaplotypeCaller. Forum post: https://gatk.broadinstitute.org/hc/en-us/community/posts/4659308628891-GenomicsDBImport-error-from-fields-in-AS-UNIQ-ALT-READ-COUNT",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7298#issuecomment-1067239683:300,error,error-from-fields-in-AS-UNIQ-ALT-READ-COUNT,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298#issuecomment-1067239683,1,['error'],['error-from-fields-in-AS-UNIQ-ALT-READ-COUNT']
Availability,"There is going to be a behavior different between the old iteration pattern. The existing code will iterate variants to establish whitelist of start sites, and then re-query variants overlapping those starts. This is generally identical in practice to MultiVariantWalkerGroupedOnStart; however, when -L is supplied, it can give differences. I'm not sure which I think is correct. . Example, from VariantEvalIntegrationTest.testFunctionClassWithSnpeff():. // The DbSNP VCF has two variants:; // 1 1423281 rs374004343 GGAAGC G . .; // 1 1423281 rs79849353 G A . .; // If we use 1423281 as the interval, we find both. // The SnpEff VCF has these two:; // 1 1423282 . GAAGC G; // 1 1423286 . C G. The SnpEff VCF is provided as -L when running the tool. Both SnpEff and DbSnp will be used as drivingVariants. Therefore when we hit interval 1423282, the DbSnp variant 1423281:GGAAGC>G variant will be included, but not 1423281:G>A. When using MultiVariantWalkerGroupedOnStart, this means only that variant is passed downstream. Previously, VariantEval would simply establish the whitelist of start sites (meaning the lowest from the group, or 1423281) and iterate them. This means that even though 1423282 is the interval from -L, it picks up the overlapping 1423281:GGAAGC>G, which effectively makes 1423281 a whitelisted start site. The existing behavior would iterate each start site and re-query overlapping variants in VariantEvalUtils.bindVariantContexts(). It would call:. List<VariantContext> prior = featureContext.getValues(track, referenceContext.getInterval().getStart());. In this instance, it would query on start=1423281, meaning it picked up both DbSnp sites, even though 1423281:G>A is not within the intervals supplied by -L. This is sort of a fringe case. I dont see how to fix this without reintroducing the expensive re-query step. I'm currently thinking that the existing behavior is probably what's wrong, but I'm going to consider this a little more. My last commit highlights this c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-730697357:1010,down,downstream,1010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-730697357,2,['down'],['downstream']
Availability,"There is no problem on runing GenomicsDBImport , and @gokalpcelik I have already tried Xmx10G to Xmx 14G and get the same error.; I'm most curious about why running GenomicsDB GenotypeGVCFs directly with 400 samples on the same computational resources can succeed, while running incremental GenomicsDB GenotypeGVCFs with 200 + 200 samples fails.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8777#issuecomment-2060284392:122,error,error,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8777#issuecomment-2060284392,1,['error'],['error']
Availability,There is one available as a part of GATK-SV. These built using high-coverage 1000GP samples recently generated at NYGC. You can find the contig ploidy model and gcnv model here:. https://github.com/broadinstitute/gatk-sv/blob/master/input_values/ref_panel_1kg_v2.json. and preprocessed intervals file here:. https://github.com/broadinstitute/gatk-sv/blob/master/input_values/resources_hg38.json#L39,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7116#issuecomment-804962111:13,avail,available,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7116#issuecomment-804962111,1,['avail'],['available']
Availability,There needs to be a validation tool for data sources to ensure that they conform to their formats properly. This tool is envisioned to be run just prior to data source release to fix any silent errors.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4380:194,error,errors,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4380,1,['error'],['errors']
Availability,"There should be a robust mechanism to check whether an index file is up-to-date with respect to the file it indexes (eg., UUIDs, hashes, etc.). Modification time alone is not sufficient, since files can get uploaded out-of-order in cloud environments.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5571:18,robust,robust,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5571,1,['robust'],['robust']
Availability,"There were a couple of things I needed to do to get the new Spark code running on a cluster:. i. Go back to using Spark's version of Kryo. Using a different version of Kryo is not actually needed (2.21 used by Spark passes the tests), and actually caused errors on the cluster when run with `--conf spark.driver.userClassPathFirst=true` (which is needed to avoid other library conflicts, like with jopt-simple). ii. Exclude Spark from the JAR file to avoid library conflicts. It's normal to exclude Spark and Hadoop from JAR files since they are supplied by `spark-submit`. Since Gradle doesn't have a 'provided' dependency (see https://github.com/broadinstitute/hellbender/issues/836), I had to do a bit of a workaround with the `shadowJar` target, which is now `sparkJar`. . Here's the command I ran:. ``` bash; NAMENODE=...; SPARK_MASTER=yarn-client; HELLBENDER_HOME=...; spark-submit \; --master $SPARK_MASTER \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; build/libs/hellbender-all-*-spark.jar ReadsPipelineSpark \; --input hdfs://$NAMENODE/user/$USER/bam/NA12878.chr17_69k_70k.dictFix.bam \; --output hdfs://$NAMENODE/user/$USER/out/spark-reads-pipeline \; --reference hdfs://$NAMENODE/user/$USER/fasta/human_g1k_v37.chr17_1Mb.fasta \; --baseRecalibrationKnownVariants $HELLBENDER_HOME/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf \; --sparkMaster $SPARK_MASTER ; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/882:255,error,errors,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/882,1,['error'],['errors']
Availability,There were a few dead links in the GATK to http://gatkforums.broadinstitute.org/gatk/discussion/58/companion-utilities-reordersam which is still archived here: https://web.archive.org/web/20160720131152/http://gatkforums.broadinstitute.org/gatk/discussion/58/companion-utilities-reordersam. We should write a new short technical article here: https://gatk.broadinstitute.org/hc/en-us/sections/360007134392-Glossary preserving the knowledge about sort ordering and update the remaining two links in our error messages to be current with that.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8272:502,error,error,502,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8272,1,['error'],['error']
Availability,There's been a request from production (https://broadinstitute.atlassian.net/browse/DSDEEPB-2789) to try to ensure that the walker BQSR tools in hellbender can run with less than 3.5 GB of RAM to get costs down.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1460:206,down,down,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1460,1,['down'],['down']
Availability,There's enough work with the annotation handling to justify this being a separate task for the HaplotypeCaller side. Let's just turn your new phasing off for HaplotypeCaller GVCF mode. I'm still interested in it being available for single-sample because it would be awesome for clinical.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-387833314:218,avail,available,218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-387833314,1,['avail'],['available']
Availability,"There's not enough information in this report too diagnose the problem. Based on the mention of a Scala IDE I suspect the way it's being run is part of the problem. . Could you please reopen this as an issue on the (GATK help forum)[https://gatk.broadinstitute.org/hc/en-us/community/topics)]? . When you open the new issue could you also include ; 1. The complete command line you ran which gave this error ( or if it was run from within some IDE an explanation of that.); 2. What version of GATK you're running; 3. Some details about the machine you're running on (OS, disk / ram available)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8587#issuecomment-1821302398:402,error,error,402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587#issuecomment-1821302398,2,"['avail', 'error']","['available', 'error']"
Availability,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with “localization by hard link failed” and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used [Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8087:185,error,error,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087,5,"['down', 'error', 'failure']","['down', 'error', 'failure', 'failures']"
Availability,"These are count files containing counts simulated according to the gCNV model, and are used as the primary inputs to generate the rest of the test files downstream. The dictionary is fictional and immaterial, so long as it is carried forward consistently from the count files to downstream outputs (which the tools do automatically). You could pretend that we started with simulated BAMs that were all aligned w.r.t. this fictional reference/dictionary. (It's just more work to simulate at the BAM level, which is why we start with the counts; simulating BAMs would also not increase test coverage in any meaningful way.). There are already a number of checks to guarantee dictionary consistency, but as you can see from that issue, there are a few scenarios where dictionaries from intervals might not be available via the engine. I don't think there should be a need to provide or check against external references/dictionaries at downstream steps; see point 4 in https://github.com/broadinstitute/gatk/issues/6924#issuecomment-719576249 about reverting the code change to allow external dictionaries in PostprocessGermlineCNVCalls. If you like, you can switch the dictionaries in the test files to something real/canonical, but I would still revert the code change. Again, I'm not sure if there was some additional context that I'm missing---e.g., is there some big analysis going on where they need to override sequence dictionaries and/or checks due to mismatched dictionaries in the BAMs? If so, could this be addressed with e.g. UpdateVcfSequenceDictionary or some other solution?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6957#issuecomment-726967714:153,down,downstream,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6957#issuecomment-726967714,4,"['avail', 'down']","['available', 'downstream']"
Availability,"These are failing with:. ```; Error: (converted from warning) unable to access index for repository http://lib.stat.cmu.edu/R/CRAN/src/contrib; Execution halted; The command ""if [[ $TEST_DOCKER != true ]]; then sudo mkdir -p /usr/local/lib/R/; sudo mkdir -p site-library; sudo ln -sFv ~/site-library /usr/local/lib/R/site-library; sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9; sudo add-apt-repository ""deb http://cran.rstudio.com/bin/linux/ubuntu trusty/""; sudo apt-get update; sudo apt-get install -y --force-yes r-base-dev=3.1.3-1trusty; sudo apt-get install -y --force-yes r-base-core=3.1.3-1trusty; sudo Rscript scripts/docker/gatkbase/install_R_packages.R; fi;"" failed and exited with 1 during; ```; which has nothing to do with the PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5595#issuecomment-456911203:30,Error,Error,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5595#issuecomment-456911203,1,['Error'],['Error']
Availability,"These are the changes needed to run on a whole genome in strict mode. We get out of memory errors without these changes. Reads downsampling was missing for the part where `AssemblyRegion`s are filled with reads - this PR adds it in. Downsampling is not deterministic yet, since that depends on #5437, but that's an orthogonal issue so it's OK to merge this change and add #5437 later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5721:91,error,errors,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5721,3,"['Down', 'down', 'error']","['Downsampling', 'downsampling', 'errors']"
Availability,These errors have been fixed,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7091#issuecomment-800608041:6,error,errors,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7091#issuecomment-800608041,1,['error'],['errors']
Availability,"These kind of errors are typically seen when the field description in the VCF header is incorrect. For example, describing the field length to be a fixed integer when the field is really variable length. I would recommend closely scanning the VCF header for inconsistencies first.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407450035:14,error,errors,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407450035,1,['error'],['errors']
Availability,"These measurement are useful when tuning performance (or hunting down performance anomalies), but they have a measurable overhead (10% difference on a test with 1000 intervals, 5x the standard deviation on 10 runs). So turn them off by default. Also refactor a few of those into a try-finally to avoid repetition and its associated risks on correctness.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2391:65,down,down,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391,1,['down'],['down']
Availability,"These things always happen just before a 3-day weekend :) Since we're about out of time for this week, we'll have to follow up on this on Tuesday when the Broad re-opens. Hopefully the new dylib fixes the travis failures -- if not, perhaps it would be a good idea to schedule a troubleshooting session after our regular weekly meeting. Have a good weekend @kdatta @kgururaj @cmnbroad !",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294237177:212,failure,failures,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294237177,1,['failure'],['failures']
Availability,"These tools use fread + grep preprocessing (`fread(""grep ..."")`) to quickly read in large TSVs in the backend R scripts. Unfortunately, because of https://github.com/Rdatatable/data.table/issues/1139 and the fact that /dev/shm is limited to 64MB in a standard GATK Docker container, this can yield the following error when running within Docker:. ````; Stderr: grep: write error: No space left on device; Error in fread(sprintf(""grep -v ^@ %s"", tsv_file), sep = ""\t"", stringsAsFactors = FALSE, : ; Expected sep ('	') but new line, EOF (or other non printing character) ends field 2 when detecting types from point 10: 2	229515751	229516; Calls: source ... eval -> eval -> WriteModeledSegmentsPlot -> ReadTSV. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:80); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:19); 	at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:131); 	at org.broadinstitute.hellbender.tools.copynumber.plotting.PlotModeledSegments.writeModeledSegmentsPlot(PlotModeledSegments.java:289); 	at org.broadinstitute.hellbender.tools.copynumber.plotting.PlotModeledSegments.doWork(PlotModeledSegments.java:206); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); ````. Starting a Docker container with a sufficiently large `--shm-size` resolves this, but I am not sure if we ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4140:312,error,error,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4140,3,"['Error', 'error']","['Error', 'error']"
Availability,"They discuss that add more ram.; I try with 8 cpu and 500 Go of RAM, but still not working. Error for one bam file:. ```. 15:47:36.554 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 28, 2019 3:47:37 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:47:37.239 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.240 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.4.0; 15:47:37.240 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:47:37.240 INFO Mutect2 - Executing as jpollet@cl1n031.genouest.org on Linux v3.10.0-693.21.1.el7.x86_64 amd64; 15:47:37.240 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:47:37.246 INFO Mutect2 - Start Date/Time: 28 novembre 2019 15:47:36 CET; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - HTSJDK Version: 2.20.3; 15:47:37.246 INFO Mutect2 - Picard Version: 2.21.1; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:37.247 INFO Mutect2 - Deflater: IntelDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:92,Error,Error,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['Error'],['Error']
Availability,"They're public, so just install Google Cloud gsutil and copy with `gsutil cp gs://gatk-best-practices/somatic-b37/Mutect2-exome-panel.vcf <local path to copy to>`. Or, if you're running on the cloud, you don't even need to download anything, just run Mutect2 with the cloud paths eg ; ```; gatk Mutect2 -R ref.fasta -I tumor.bam -pon gs://gatk-best-practices/somatic-b37/Mutect2-exome-panel.vcf -O calls.vcf; ```; If you install gsutil this works when running locally as well, but for speed I would recommend downloading the pon. > Is there a reason this is not in the GATK resource bundle?. Not that I can think of.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-481919351:223,down,download,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-481919351,2,['down'],"['download', 'downloading']"
Availability,They've been working as far as I know. Or at least I haven't gotten any; failure messages. That's why I closed the ticket.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2183#issuecomment-253016704:73,failure,failure,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183#issuecomment-253016704,1,['failure'],['failure']
Availability,"Think it might be worth saving a VariantFiltration pass for the bit of code it'd take, but up to you!. ScoreVariantAnnotations will output both the raw ""VQSLOD"" score and the converted sensitivity, so we're free to specify thresholds on either. However, given that different types of models may have scores in different ranges (e.g., BGMM vs. IsolationForest, positive/negative vs. positive-only, etc.), I think it's better to restrict all command-line options to be expressed in terms of a sensitivity. Same thing goes if you decide to filter externally with VariantFiltration for now. Even though you have both quantities available to you, just use the sensitivity. This brings us to questions related to whether we want to keep the old VQSR requirements of having both training and truth sets specified. For example, we could instead drop the distinction between training and truth for the new tools, and always calibrate sensitivity to the training set (you can essentially force this behavior with the current code by specifying training=true,truth=true for all of your resources). And yes, all of the tools should have a variety of command lines in the tests to demonstrate behavior. If you want to explore positive/negative mode, take a look at the *Unlabeled tests. Also feel free to ping me if anything isn't clear!. I'll push another round of minor updates here, too.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1069376523:624,avail,available,624,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1069376523,4,"['avail', 'ping']","['available', 'ping']"
Availability,"This **CRAM file** and **its reference** are identical to those I have in the cloud because I uploaded these exact files to the cloud after downloading them from the 1000 Genomes Project FTP site. Back then, in the cloud, I was able to use samtools to decram and index this CRAM file alongside 39 others. On our local server, I cannot get readwalkers PrintReads nor CalculateTargetCoverage to correctly decipher the CRAM. Both tools give the same error. Here is the PrintReads command:; ```; /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-launch \; PrintReads \; -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/cram/HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.cram \; -O HG02759.alt_bwamem_GRCh38DH.20150826.GWD.exome.bam; ```; And here is the error:; ```; 17:47:15.362 INFO ProgressMeter - chr1:198467627 2.6 8432000 3202552.3; 17:47:25.402 INFO ProgressMeter - chr1:236860077 2.8 10019000 3577916.1; ERROR 2017-06-22 17:47:27 Slice Reference MD5 mismatch for slice 0:248681942-248858764, ATAGCGGTCA...AGTGGCGGTG; 17:47:27.292 INFO CalculateTargetCoverage - Shutting down engine; [June 22, 2017 5:47:27 PM EDT] org.broadinstitute.hellbender.tools.exome.CalculateTargetCoverage done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=10377756672; htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 248681942, span 176823, expected MD5 4b8526e90896b01860301e5a1ef4988b; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:187); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:261); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:601); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.hasNext(SAMRecordToReadIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:115); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154:140,down,downloading,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154,3,"['down', 'error']","['downloading', 'error']"
Availability,"This Barclay [feature](https://github.com/broadinstitute/barclay/issues/25) automatically expands the contents of a file ending in "".list"" whenever the target argument is a collection. This precludes the use of Picard interval list files ending in "".list"" with -L in GATK, since they contain a sam header. The raw sam header lines wind up getting added as interval strings, which then fails parsing: A USER ERROR has occurred: Badly formed genome unclippedLoc: Failed to parse Genome Location string: @HD	VN:1.5: Problem parsing start/end value in interval string. Value was: 1.5. A short term GATK workaround is to use a file ending in one of the other known Picard interval list extensions (.interval_list, .intervals, or .picard) instead, but we should find a better fix for this since .list seems to be commonly used. Tools such as GetHetCoverage, which take an interval list in an argument typed as a File (--snpIntervals), are able to consume the interval file because the target argument is not a collection, so the auto-expansion is not triggered. I expect this issue could cause more problems in Picard as well once Barclay is the default parser there.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3555:407,ERROR,ERROR,407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3555,1,['ERROR'],['ERROR']
Availability,"This PR addresses spec-ops issue #235 - Use -m flag in final gsutil mv of files in ImportGenomes. . Additionally, this PR adds branch filters to the dockstore.yml file that will help with development. The filter for each workflow indicates which branch(es) will show up for that workflow in dockstore. If we don't include these filters, dockstore will run checks of ALL workflows on ALL branches, which causes timeouts. We could remove these filters later (before merging to master) or not, but for now this could help us develop on ah_var_store. Note that we'll need to add feature branches to that file as we work on them. This workflow was tested in Terra and the upload succeeded. Also confirmed that if one file fails, the entire process throws an error code (i.e. -m flag will not cause failures to silently pass) - in example below, `test_file_list.txt` was a list of 6 files, including 1 file that did not exist.; ```; ➜ cat test_file_list.txt | gsutil cp -I gs://dsp-fieldeng-dev/test_cp/; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; CommandException: No URLs matched: test4.txt; ➜ cat test_file_list.txt | gsutil -m cp -I gs://dsp-fieldeng-dev/test_cp/; If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. CommandException: No URLs matched: test4.txt; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test5.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; Copying file://test6.txt [Content-Type=text/plain]...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Ope",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7104:753,error,error,753,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7104,2,"['error', 'failure']","['error', 'failures']"
Availability,"This PR addresses two phasing bugs, https://github.com/broadinstitute/gatk/issues/6463 and https://github.com/broadinstitute/gatk/issues/6845. https://github.com/broadinstitute/gatk/issues/6463 identified a bug in the phasing algorithm which caused the wrong phase information to be output for scenarios where the first variant in a phase set is homozygous variant and it is followed by het variants in opposite phase. Without this change the het variants were incorrectly placed on the same phase strand because the phase set was tied to the hom var variant, and the algorithm assumed that each het variant could be put in the same phase strand as it because it was on all haplotypes. I've modified the algorithm to keep track, for variants that occur on all haplotypes, of which of the haplotypes have already been used for phasing an upstream ""comp"" variant so that further downstream variants can be checked against the remaining set. https://github.com/broadinstitute/gatk/issues/6845 showed an example of phase sets being disrupted by the presence of an alternate haplotype that supported an additional, uncalled, variant in the region. In this case there was an alternate haplotype supported by two reads that supported a SNP downstream of two pairs of SNPs in alternate phase. The presence of an additional haplotype causes the phasing algorithm to break the phase sets in the region. I've modified the algorithm to only use haplotypes that support the alternate alleles present in called variants in phasing by modifying the number that we pass as `AssemblyBasedCallerUtils.constructPhaseSetMapping()`'s `totalAvailableHaplotypes` parameter. In my opinion this ; fix produces output that is still correct and is much easier to understand (since it only depends on sites that are visible in the output VCF), but if anyone objects to this change please let me know. . Non-test code changes for this PR are in two different commits to try to make it easier to understand the scope of the two cha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7019:877,down,downstream,877,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7019,1,['down'],['downstream']
Availability,"This PR contains several distinct modifications necessary to protect our pipeline against errors when a location is specified on a BQ dataset. 1. modifying several WDLs that used the BQ CLI that specified ""--location=US"" Turns out, it is unnecessary and breaks things when the location is not US; 2. modifying two paths through BigQueryUtils to harden against non-""US"" locations, including explicitly passing in the dataset id to getQueryCostBytesProcessedEstimate so its location can be looked up and passed into the dry run job; 3. cutting our reliance on bqutil to be installed in the location in which our queries run by supplying a local version of ""median"" as a UDF (as udf_media.sql pulled in through changes to BigQueryUtils.java and referenced in feature_extract.sl). Only partially related, this PR also contains the creation of the script/variantstore/utils directory to hold useful scripts, and the useful pushGATKtoGCS script for pushing jars to an easily-referenced location for WDLs (h/t to Miguel). The entire tragic history of successes and failures can be seen in the job history of the workspace https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history. Every stage of the quickstart can be verified within to--eventually and only after the gods deemed my suffering sufficient--have passed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8047#issuecomment-1270126312:90,error,errors,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8047#issuecomment-1270126312,2,"['error', 'failure']","['errors', 'failures']"
Availability,"This PR converts the Mutect2Filtering engine to be allele specific. This required changes to SomaticClusteringModel and ThresholdCalculator as well as ErrorProbabilities and of course the filters themselves. There are some filters which have not yet been converted, but I am prioritizing the ones in this PR for Sarah Calvo and the mitochondria pipeline. This provides the implementation for dsp-spec-ops tickets 166, 168, 169",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6399:151,Error,ErrorProbabilities,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6399,1,['Error'],['ErrorProbabilities']
Availability,"This PR deals with long reads with exactly two alignments (no other equally good alignment configuration), mapped to the same chromosome with strand switch, but NOT significantly overlapping each other. We used to call inversions from such alignments, but it is more appropriate to emit BND records because a lot of times such signal is actually generated from inverted segmental duplications, or simply inverted mobile element insertions. To confidently interpret and distinguish between such events, we need other types of evidence, and is better to be dealt with downstream logic units. Inverted duplications are NOT dealt with in this PR and is going to be in the next. NEEDS TO WAIT UNTIL PART 1 & 2 ARE IN.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3457:566,down,downstream,566,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3457,1,['down'],['downstream']
Availability,"This PR deals with the test failures that were occurring when we ran ALL chromosomes through the integration test, rather than just chr20 and X and Y (the default). It adds another truth set for all chromosomes.; Also two small changes.; - Skip the cost/table size check for the Hail integration, to allow it to get to the hail part if there are spurious test failures in cost.; - Change the name of the files used for table size and cost checking. Makes it easier to install new test data. Passing integration test on all chromosomes [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/d8837252-26fa-4d40-bdf1-e42ff8932fd1); Passing integration test on chr20/x/y [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f552e7a3-d245-492d-b5e1-a35ba323fae8).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8787:28,failure,failures,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8787,2,['failure'],['failures']
Availability,"This PR dynamically sets the logging level for command line tools at runtime using the current version of log4j (we were headed down a path of downgrading to a previous version of log4j in order to implement this). However, it uses an API that is normally used in code for extending log4j rather than acting as a client to it, and requires an explicit cast of the value returned from LogManager.getContext. The Apache project site illustrates the use of this api in the first line of code in an example here: https://logging.apache.org/log4j/2.x/manual/customconfig.html#AddingToCurrent. We need to decide if we want to take this and stay on the current version or continue with the downgrade…",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/603:128,down,down,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/603,3,['down'],"['down', 'downgrade', 'downgrading']"
Availability,"This PR fixes a critical error that was causing an ""invalid interval"" exception to be thrown while calling imprecise deletion intervals. The problem was a mismatch between the sequence dictionary in the reference and the read metadata's contig ID - contig name mapping. I've modified the code to always use the read metadata when translating the contig ID in `EvidenceTargetLink` intervals into contig names.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3671:25,error,error,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3671,1,['error'],['error']
Availability,"This PR fixes an error in the cost calculation in that we were not handling pre-emption. That is, a shard could get preempted and thus we'd have multiple cost measurements for that shard. ; I've addressed this by looking for a single 'representative' instance of each shard. But despite this, I found there is still non-determinism in the values written to the cost table. So, I've reset the thresholds we use for calculating if the test 'passes'. ; We may need some time to deal with finding real numbers (that is, my threshold may now be too conservative). Passing test runs:; All Chromosomes [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/adc09aff-e6bc-4289-ac34-d74b86eb92e2).; Chromosome 20/X/Y [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/1b9a7d34-6d4f-42ca-8434-924ca84ee0ed).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8795:17,error,error,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8795,1,['error'],['error']
Availability,"This PR fixes an exception that was thrown when trying to serialize a ReferenceMultisource object. I was attempting to broadcast a 2bit reference in a Spark tool and found that I received a serialization error of the form:; ...; Caused by: com.esotericsoftware.kryo.KryoException: Unable to find class: ReferenceWindowFunctions$$Lambda$1/1599771323. Along with some related error reports (https://github.com/npgall/mobility-rpc/issues/12), this allowed me to track this down to the use of a Lambda function in the ReferenceWindowFunctions class. Replacing it with an explicitly defined function class solves the problem. Added a unit test which used to fail but now passes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1489:204,error,error,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1489,3,"['down', 'error']","['down', 'error']"
Availability,"This PR fixes two bugs. First, the SplitIntervals task would enter WeightedSplitIntervals and hang. I added an extra boolean argument to extract so you can specify that no, you really don't want to use a weighted bed. Relatedly, the code branch for running the original GATK SplitIntervals code wasn't correct, as passing weight-bed-file to it as an argument caused a failure. It uses a slightly hacky method of defining a string in WDL to be empty or not depending on if we use weighted beds, interpolating that string into the bash, then checking to see if it's empty there to transmit that state. There is likely a cleaner way to do this, and in the next revision I will likely rewrite this part cleaner. Second, after SplitIntervals passed we hit an error during ExtractTask. The way it expanded intervals to handle large deletions could sometimes subtract past the start of a chromosome, so that logic needed to be patched in a few separate places to handle the interval for the mitochondrial dna that started much closer to the beginning (instead of having a 10k base pair buffer). This PR has those changes too. Successful run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/a006a959-9300-42cf-84a7-38c70a35ee21. Successful run after incorporating PR changes: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/e2ee3abd-288e-4f1d-b5be-f78cf5400ce9. Successful run after last PR refactoring that allowed me to revert almost all changes to GvsUtils.SplitIntervals: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/94fed63a-98ca-466e-8d4c-ac97f24adf37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8113:368,failure,failure,368,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8113,2,"['error', 'failure']","['error', 'failure']"
Availability,"This PR includes two changes:; 1. Provide a command line argument to toggle the overlapping base quality correction (i.e. min(bq, 20)) before reassembly, which happens in FragmentUtils. I've found, however, that by the time SomaticGenotypingEngine runs, those the quality of these bases get bumped up to what they used to be, so this may be a no-op. I included it in case I missed something, and to be consistent with the branch @fleharty and @madduran have been using.; 2. Provide a command line argument to count the two reads in an overlapping pair separately in StrandArtfiact and StrandBiasBySample. This feature is only available in Mutect i.e. it won't affect other tools that use StrandBiasBySample",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5286:626,avail,available,626,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5286,1,['avail'],['available']
Availability,This PR is against ah_var_store. Need to make another against EchoCallset. The new reference disk is installed.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/ba18dc3d-0548-48ac-a67b-cec55250de8a) is a passing run of GvsCreateVatFromVDS using quickstart against the new reference.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8746:62,Echo,EchoCallset,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8746,1,['Echo'],['EchoCallset']
Availability,"This PR is intended to introduce several new tools related to the CleanVcf workflow in GATK-SV, which the use of these tools being documented in https://github.com/broadinstitute/gatk-sv/pull/733. These tools are intended to introduce several enhancements over the existing implementation, including but not limited to:; - Introduce various unit and integration tests into the workflow.; - Create more robust and generalizable tools that can be used independent of _CleanVcf_.; - Improve runtime and execution speed by leveraging Java.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8996:402,robust,robust,402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8996,1,['robust'],['robust']
Availability,This PR is the culmination of work from myself and @lbergelson to improve the runtime for MarkDuplicatesSpark on a single machine. This involved a rewrite of the tool as well as a number of improvements which should bring it into closer agreement with MarkDuplicates from picard. . Note: this is merely a checkpoint and there is still work that must be done to bring the work into agreement with recent MarkDuplicates development in picard. . Resolves #3706,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4656:305,checkpoint,checkpoint,305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4656,1,['checkpoint'],['checkpoint']
Availability,"This PR is the initial stage of implementing the calling of IMPRECISE variants in the SV pipeline. It introduces the concept of an evidence-target link, which joins an evidence interval to its distal target. This is an extension of the 'coherent' evidence concept previously used in determining evidence thresholds for assembly. The code in this PR contains the following changes:. - Evidence intervals and distal targets now are treated as stranded, and evidence-target link clustering depends on overlaps between both intervals and strands.; - Evidence target interval and distal target interval calculations have been modified to make sure that evidence supporting the same event clusters together (has overlapping intervals). This includes several changes such as extending the 'rest-of-fragment-size' calculation to try to capture almost all non-outlier fragment sizes in the library; increasing the split read location uncertainty a little; and being more precise about the boundaries of distal target intervals by taking advantage of information in the MD and MC tags if available.; - Evidence target links are gathered for every piece of evidence supporting a high-quality distal target. ; - Evidence target links are clustered together and store the amount of split-read and read-pair evidence that went into each cluster.; - All evidence target link clusters that are composed of at least 1 split read or at least 2 read pairs are collected in the driver and emitted in a BEDPE formatted file specified in the command line parameters.; - A `PairedStrandedIntervalTree` data structure is introduced to allow `SVIntervalTree`-style lookups for paired intervals. To finish this work, future PRs will 1) use the collected evidence target links to annotate our assembly called-variants with the number of split reads and read pairs observed in the original mappings and 2) create IMPRECISE VCF records for events that have enough evidence-target-link support, first for deletions and then possibl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469:1078,avail,available,1078,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469,1,['avail'],['available']
Availability,"This PR reimplements the overlap detector used in WeighedSplitIntervals in a much faster form for our particular use case. It also involved preprocessing the weighted bed input file in a new way, so the previous weights files will no longer work. As such, there's a new weights file uploaded and referred to as part of this pr. I pulled down the documentation and rationale for the original process from the git issue to a markdown file that can live in our repo, and made python scripts out of the necessary bits of python logic there (as well as a new one to do the further preprocessing step that I added). The motivation for this was the inability of the previous WeightedSplitIntervals task to complete when run against an exome interval list. This new one does, and it does so quickly. The link referenced below is not a ""successful"" run in the Terra sense because it was 190k exomes and that was simply too much for Terra to handle, but it DOES show a successful WeightedSplitIntervals run before the real extract started and I believe that is sufficient to merge. Delaying while ticket VS-189 gets figured out will create an unnecessary delay. Successful integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/294fd6a8-15ed-4722-a63e-bdf089c1c52a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8507:337,down,down,337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8507,1,['down'],['down']
Availability,"This PR removes the cap on insert size for calling an 'innie' read pair SV evidence. Previously this was set to a max Z score of 100, which worked out to be around 10kb for some libraries. This was causing us to not gather read-pair based evidence around deletions of over 10kb. The motivation for this limit was to exclude very long read pairs as evidence since they are predominantly mapping errors. I believe that a) this limit was set too low, causing us to miss real events and b) the reads with very long insert sizes likely cover many instances of real variation, even if the actual mappings locations are incorrect, and that the assembly process, variant filters, and correlation with coverage data should be able to filter out assemblies triggered by reads with long mappings. . Removing the limit on insert size does cause the imprecise deletion-calling code to produce many more spurious calls. In practice, I think that we should still evaluate this evidence but not make a call unless it is supported by coverage information, and a long deletion event should be very visible from depth-based calling. Therefore, I've added a limit on the size of imprecise deletions that will be called right now, with the expectation that this hard limit will be removed in favor of depth and other filtering in the future. Manual evaluation of the results shows a small increase in incorrect imprecise variants (mostly in centromeric regions) that were just small enough to slip under the hard threshold, a very small number of incorrect small insertions caused by assembling messy regions, and a number of real 10-25kb deletions that we now assemble and either call an imprecise or precise deletion variant at, so I think this is a small net gain in callset quality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4333:394,error,errors,394,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4333,1,['error'],['errors']
Availability,"This addresses issue 569 - the cleanup of format errors in bam and sam files in tests. I will send an archive with a README, validations for the post-modification bams and sams, and diffs for the bams to akiezun.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/809:49,error,errors,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/809,1,['error'],['errors']
Availability,"This addresses issues #5568 and #5342.; #5568 Buffer resize messages are now turned on only for Debug builds.; #5342: Added better general error reporting for system commands. For the file synching error in question, implemented a workaround. With environment variable - TILEDB_DISABLE_FILE_LOCKING - set to true or 1, there is no file locking and file synching error will only log warning messages and not return an error. Hopefully, this will mitigate the issues on NFS and CIFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5608:139,error,error,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608,4,['error'],['error']
Availability,"This affects `RunBWAMEMViaCommandLine.java` and `RunSGAViaProcessBuilderOnSpark.java`, where stdio are always captured.; Capturing BWA MEM's stdout is a must because the result is piped to stdout.; For SGA, this helps debugging. But this also slows down the performance. Hence an option would be useful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1878:249,down,down,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1878,1,['down'],['down']
Availability,"This allows the user to write their own description in the FILTER line in the header. Currently a user can define an interval list to filter with and name that filter, but the description line is always ""Doesn't overlap a user-input mask"" or ""Overlaps a user-input mask"", whereas it might be more useful to let the user provide a more detailed description (for example, ""Outside of sequencing target intervals"").",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8831:233,mask,mask,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8831,2,['mask'],['mask']
Availability,This also start the mark down for how to find cost info for the aou prod project,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7236:25,down,down,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7236,1,['down'],['down']
Availability,This branch (or some variation therin) will be necessary to keep track of how well #5980 is doing at bringing down the number of found haplotypes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6049:110,down,down,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6049,1,['down'],['down']
Availability,"This branch adds the ability to perform a Broadcast of the reference and also support for 2bit references. I'm submitting this PR against the `dr_lb_spark_bqsr` branch. I can run the following two commands on my local machine, which produce the same output. Broadcast:. ```; ~/Downloads/spark-1.4.1-bin-hadoop2.4/bin/spark-submit \; --master local[2] \; ~/repos/hellbender/build/libs/hellbender-all-GATK.4.*-spark.jar \; BaseRecalibratorSpark \; --input ~/repos/hellbender/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.bam \; --knownSites ~/repos/hellbender/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf \; --reference ~/repos/hellbender/src/test/resources/large/human_g1k_v37.20.21.2bit \; --joinStrategy BROADCAST \; --output ~/tmp/bqsr.out.2.txt; ```. Shuffle:. ```; ~/Downloads/spark-1.4.1-bin-hadoop2.4/bin/spark-submit \; --master local[2] \; ~/repos/hellbender/build/libs/hellbender-all-GATK.4.*-spark.jar \; BaseRecalibratorSpark \; --input ~/repos/hellbender/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.bam \; --knownSites ~/repos/hellbender/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf \; --reference ~/repos/hellbender/src/test/resources/large/human_g1k_v37.20.21.fasta \; --joinStrategy SHUFFLE \; --output ~/tmp/bqsr.out.3.txt; ```. Still need to try it on a cluster and on bigger input. cc @tomwhite @davidadamsphd for review.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/919:277,Down,Downloads,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/919,2,['Down'],['Downloads']
Availability,"This branch takes the version of gatk-public that gatk-protected currently depends on (4.alpha.2-188-g7332d10) and applies @davidbenjamin 's fix to the `TandemRepeat` annotation to it. The only purpose of this PR is to cause a snapshot to be generated -- do not merge!. This is necessary to unblock @davidbenjamin 's work, because the `HaplotypeCaller` tests are failing if we update protected to the latest public head, and although we've fixed some of the issues there are some unexplained failures in the concordance tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2569:492,failure,failures,492,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2569,1,['failure'],['failures']
Availability,"This brings in some additional retries for UnknownHostException and 502 errors,; and moves us from a fork in my personal github repository to the fork in; https://github.com/broadinstitute/google-cloud-java. Resolves #4888; Resolves #5094",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5099:72,error,errors,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5099,1,['error'],['errors']
Availability,This brings read-count + allelic-count collection down to ~34 cents / TCGA WGS BAM from ~57 cents / BAM. @LeeTL1220 TAG team said they would like to wait for this change to be in FireCloud before starting their tests. When should we make it (along with the necessary changes to get tests passing here---and should that be a fork)?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-405596508:50,down,down,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-405596508,1,['down'],['down']
Availability,"This can cut down the size of the PoN. However, see #4554.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4553:13,down,down,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4553,1,['down'],['down']
Availability,"This can occur in cases where there was a mixup with the samples, meaning the user intended to run a properly matched normal/tumor pair, but there is a provenance error. This is how @asmoe4 and myself hit this issue. So this is not the same use case as #5821, where they know there's a deliberate mismatch. While we're not expecting the contamination check to provide something sensible in this case, may I suggest that the tool provides a user-friendly message to help debug, rather than a stack traceback. This could happen to other people if they have an accidental mismatch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5880#issuecomment-483276300:163,error,error,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5880#issuecomment-483276300,2,['error'],['error']
Availability,This change is dependent on [this recent change](https://github.com/samtools/htsjdk/commit/4f550e1f1afabf21467957fa672ca2a4ad457897#diff-b678735810949d4263df7bd0fffdecb8L42) in htsjdk (and the build will fail without it). Once htsjdk2.0 is available we'll upgrade it in this branch/pr so the two changes can go in together.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1243:240,avail,available,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1243,1,['avail'],['available']
Availability,"This change was originally made to allow our joint calling pipeline to scale to handle the hundreds of thousands of samples in the AllOfUs project. I agree that it's problematic and confusing for downstream users. Since AllOfUs recently switched back to using the ./. convention in their callsets in response to similar complaints, we are going to revisit this decision in GATK GenotypeGVCFs and strongly consider reverting back to ./. as well in a future release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1932619397:196,down,downstream,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1932619397,1,['down'],['downstream']
Availability,"This class is now needed by other pipelines, so let's make it generally available; in the engine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/476:72,avail,available,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/476,1,['avail'],['available']
Availability,"This code adds log statements with ""START"" and ""END"" keywords so that we can then process the logs and see (i) how many times a specific operation was called, (ii) how long the operation took (per call, or in aggregate). In addition it has the notion of intermediate ""steps"" so we can drill down individual operations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/628:291,down,down,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/628,1,['down'],['down']
Availability,"This code wraps around BaseRecalibrator and presents a very basic interface (set up, add reads, teardown) that's going to be used at each Dataflow worker. Challenges here:; (1) I need to convert the intervals to Features because that's what the BaseRecalibrator class uses, and SimpleInterval is not a subclass of Feature. This may change in the future.; (2) BaseRecalibrator takes Features as inputs - the only simple Feature class I found I could reuse is ArtificialTestFeature. Please let me know if there is a better choice (solving (1) also solves this); (3) I didn't find code to test overlap between a SimpleInterval and a Feature. Rather than roll my own I chose to use the SimpleInterval overlap test and convert to Feature lazily instead of eagerly. This may cause an interval to be converted more than once. So please consider this the start of a discussion on ""here is something that works, but surely there's a better way?"" I'm not so much looking for every performance opportunity, but ideally I'd like to avoid using ArtificialTestFeature if a better candidate is available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/511:1079,avail,available,1079,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511,1,['avail'],['available']
Availability,"This constant controls both the maximum number of retries and the maximum number of reopens the GCS NIO library will perform in the face of transient errors. It's currently hardcoded, but should be exposed as an engine argument.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3315:150,error,errors,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3315,1,['error'],['errors']
Availability,"This doesn't seem really related to the other stuff I've been doing with the phasing code, but I've been taking a look since I've been working in that area. After some testing I can confirm that this does seem to be an error in the phasing algorithm logic that occurs when the first variant in the set of called variants is homozygous alt, as @tfenne suggests. I'll try to come up with a fix and either package it with my fix to https://github.com/broadinstitute/gatk/issues/6845 or in a separate PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6463#issuecomment-750312227:219,error,error,219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6463#issuecomment-750312227,1,['error'],['error']
Availability,"This enables PoNs to be built with a number of intervals greater than 16777216, which lets us push down to bin sizes <175bp if really necessary. At some point, it is better to scatter and build a separate PoN for each contig so we don't run into memory issues. However, the official WDL is not set up to do the scatter, so one would have to write a custom WDL. @mwalker174 Do you mind taking a look?. Closes #4365.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4528:99,down,down,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4528,1,['down'],['down']
Availability,This error also happens in AWS EMR. Is there a plan to fix this bug. @vdauwera ; Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066#issuecomment-354314573:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066#issuecomment-354314573,1,['error'],['error']
Availability,"This error does not occur with every VCF but at least with the one enclosed below. It passes vcf-validator with default arguments. . To reproduce: . ```; term1$ spark-shell # start you spark local cluster in another screen; ...; term2$ cd /dsde/working/valentin/bugs/gatk-var-walker-ser; term2$ git checkout 58cb99ec ; term2$ ./gradlew sparkJar; term2$ ./gatk-launch ExampleVariantWalkerSpark -V ./in.vcf.gz -- --sparkRunner SPARK --sparkMaster local. ```. ```; The stacktrace starts with:. 17/03/29 16:44:56 INFO SparkContext: Successfully stopped SparkContext; 16:44:56.000 INFO ExampleVariantWalkerSpark - Shutting down engine; [March 29, 2017 4:44:56 PM EDT] org.broadinstitute.hellbender.tools.examples.ExampleVariantWalkerSpark done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=576192512; java.lang.IllegalArgumentException: requirement failed: The partition coalescer passed in must be serializable.; 	at scala.Predef$.require(Predef.scala:224); 	at org.apache.spark.rdd.CoalescedRDD.<init>(CoalescedRDD.scala:84); 	at org.apache.spark.rdd.RDD$$anonfun$coalesce$1.apply(RDD.scala:466); 	at org.apache.spark.rdd.RDD$$anonfun$coalesce$1.apply(RDD.scala:445); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.coalesce(RDD.scala:445); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.coalesce(SparkSharder.java:321); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.joinOverlapping(SparkSharder.java:189); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.joinOverlapping(SparkSharder.java:126); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.shard(SparkSharder.java:99); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpark.getVariants(VariantWalkerSpark.java:129); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2545:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2545,2,"['down', 'error']","['down', 'error']"
Availability,"This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM). As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via `gcloud auth`, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on. The message is useful, for if we *were* running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424038095:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424038095,1,['error'],['error']
Availability,"This error message occurs when removing the network when running GATK in a docker container. GATK tools still run to completion, but the error message is disruptive. ; ```; $> docker run --rm --network none broadinstitute/gatk gatk -version. 2022-08-03 20:37:23,349 main ERROR Could not determine local host name java.net.UnknownHostException: de2c81c88ddc: de2c81c88ddc: Temporary failure in name resolution; at java.net.InetAddress.getLocalHost(InetAddress.java:1506); at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:54); at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:620); at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660); at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:620); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699); at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716); at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:599); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:72); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); Caused by: java.net.UnknownHostException: de2c81c88ddc: Temporary failure in name resolution; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324); at java.net.InetAddress.getLocalHost(InetAddress.java:1501); ...13 more. The Genome Analysis Toolkit (GATK) v4.2.6.1; HTSJDK Version: 2.24.1; Picard Ver",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7983:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7983,4,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,This error seems to be associated with the FeatureReader that reads VCFs off the GCS - any thoughts about how GenomicsDB should deal with this?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4592#issuecomment-376591406:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592#issuecomment-376591406,1,['error'],['error']
Availability,"This error was reported by a user: . Dear GATK Team, . I am writing you to discuss a an error while using FilterMutectCalls program and it seems a potential bug. ; I wanted to change the default value of **--normal-p-value-threshold **. It looks like the tool doesn't accept this parameter at all. I tried using it with the default value of 0.001 as well. . The program works fine when this parameter is removed. Moreover, the error message also states that **BUG: couldn't set field value** . please see the attached command and the error message. . ** gatk FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz**. Using GATK jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: **BUG: couldn't set field value. For normalPileupPValueThreshold in org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection@69d45cca with value 1.0E-4 This shouldn't happen since we setAccessible(true)**; 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1248); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:710); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5978:5,error,error,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978,4,['error'],['error']
Availability,This fixes a bug in handling the defaults for setting and using our default cluster initialization script for the SV pipeline. The master version will error if no init script parameters are specified.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3467:151,error,error,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3467,1,['error'],['error']
Availability,"This fixes a bug in the `AlleleFrequencyCalculator` that was causing quality to be overestimated for sites with `*` alleles representing spanning deletions. The bug was causing the calculator to not include homozygous `*` genotypes in the sum of non-site specific variant allele probabilities that is the basis for the qual score. The bug was caused by an off-by-one index error: `IndexRange(0,2)` returns `[0,1]`, not `[0,1,2]` as intended. Not including this genotype inflated the quality score for these sites. . Due to interactions with QUAL-based variant and allele trimming, this causes slightly different behavior when HaplotyeCaller is run in modes where it is forced to emit variants for every locus, as can be seen in the `expected/gvcf.basepairResolution.includeNonVariantSites.vcf` test file for `GenotypeGVCFsIntegrationTest`: 1) Sites spanned by a deletion are now reported with a `*` alt allele and have QUAL 0 and a LowQual filter. Also added a mechanism to `GenotypeGCVFsIntegrationTest` to automatically update the expected result files, similar to what already exists in `HaplotypeCallerIntegrationTest` and `CombineGVCFsIntegrationTest`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6859:373,error,error,373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6859,1,['error'],['error']
Availability,"This fixes a bug that @meganshand found. Here's the background:. By construction we do not assemble graphs with cycles (although @kvg has something to say about this). However, in rare cases recovering a dangling end may create a cycle. Although it's debatable whether this is an issue for the new, Dijkstra's algorithm-based best haplotype finding algorithm, we remove cycles before finding best haplotypes. It seems that the code for removing cycles can go into an infinite loop when, as in Mutect2's mitochondria mode, we allow for the recovery of forked dangling ends. This PR deletes a single line. `parentVertices` is the set of previously visited vertices in the depth-first search. When an edge is incident on one of these vertices it creates a cycle and we mark it for removal. My best guess (@ldgauthier could you be an extra set of brain? @droazen you're welcome to look, too.) is that the idea behind removing a `currentVertex` from `parentVertices` once all its edges were processed was to optimize the O(log n) cost of subsequent `parentVertices.contains` calls. Since it's a depth-first search, you would think that `currentVertex` will never be seen again and that this is innocuous. However, if some other branch of the depth-first search that is not descended from `currentVertex` also leads to a cycle that goes through `currentVertex`, forgetting that it has been visited creates a huge problem. I believe that forked dangling ends create this possibility. . Removing the line in question will incur a tiny performance cost, if any. By the time we get here the graph has been zipped into a `SeqGraph`, so it doesn't have very many vertices. In any case, `Set.contains` is not an expensive operation. We might even save runtime by eliminating all the `Set.remove`. I have tested this on several WGS samples and it does no harm.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5786:191,recover,recovering,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5786,2,['recover'],"['recovering', 'recovery']"
Availability,This fixes a small math error in Permutect that was harming precision in tumor-normal mode.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8668:24,error,error,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8668,1,['error'],['error']
Availability,This fixes the first day's downloads on a new release which previously were set to 0. @jonn-smith This is what I was talking about.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7794:27,down,downloads,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7794,1,['down'],['downloads']
Availability,"This fixes two issues discovered while testing my URI migration branch:. - ReadsSourceSpark.getHeader doesn't propagate the reference at all when a CRAM file input resides on GCS, so it always results in a ""no reference was provided"" error, even when a reference was provided.; - ReadsSourceSpark.checkCramReference always tried to create a Hadoop Path object for the reference no matter what file system it lives on, which fails when using a reference on GCS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6517:234,error,error,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6517,1,['error'],['error']
Availability,"This gives a compilation error.; On line 1973, you have; disjointPairCounts.bumpCount(reportType);. It should be; disjointPairCounts.bumpCount(ignoredMate);",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8557#issuecomment-1821638583:25,error,error,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8557#issuecomment-1821638583,1,['error'],['error']
Availability,This gives better errors in the case of sequence dictionary mismatches.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2615:18,error,errors,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2615,1,['error'],['errors']
Availability,This greatly reduces wall clock time in M2 scatters without affecting sensitivity. It is decoupled from HaplotypeCaller's downsampling.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3988:122,down,downsampling,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3988,1,['down'],['downsampling']
Availability,This h5diff error is the same thing I saw when I tried to just bump the version. We must be either getting a new version of it?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848091972:12,error,error,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848091972,1,['error'],['error']
Availability,"This includes the fix for the position overflowing in CloudStorageReadChannel; (https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2283), as well; as the fix for the intermittent 503 errors we've already been depending on; (https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2281)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3373:194,error,errors,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3373,1,['error'],['errors']
Availability,"This introduces a feature for `GenomicsDBImport` that allows merging multiple contigs into fewer genomicsdb partitions. This should give a huge boost for cases where users have a very large number of contigs (see [here ](https://gatk.broadinstitute.org/hc/en-us/community/posts/360060623952-GenomicsDBImport-very-slow-on-genome-with-many-contigs), for instance). Currently, GenomicsDB would create a separate folder/partition for each contig and this slows down import to a crawl with a large number of contigs. . To use this feature, users should set the flag `--merge-contigs-into-num-partitions` to the number of partitions. Using the feature requires that entire contigs be passed as input intervals -- we don't support merging together an interval list that contains partial contigs. . There's no magic threshold where this would start to be useful - we currently warn users when they specify more than 100 intervals, and I think the same threshold makes sense for when they should consider using this flag. Choosing the right value for `--merge-contigs-into-num-partitions` would be dependent on amount of parallelism users want to use (for example, do they want to want to import using `max-num-intervals-to-import-in-parallel`). If no parallelism is envisioned either on import or query, setting `--merge-contigs-into-num-partitions` to `1` should work as well -- though the user may find it more reassuring to break up the work into more partitions just so you can see some progress being made....",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6681:457,down,down,457,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6681,1,['down'],['down']
Availability,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1827:10,checkpoint,checkpoint,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827,1,['checkpoint'],['checkpoint']
Availability,"This is a follow up from a discussion from the GATK Office Hours meeting. The user has not found that the --dont-use-soft-clip-bases parameter is the culprit of this difference. The user is sending in a bug report. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078111952-The-depth-of-SNV-InDels-calculated-by-Mutect2-in-GATK4-2-0-0-is-much-lower-than-real-sequencing-depth-why-](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078111952-The-depth-of-SNV-InDels-calculated-by-Mutect2-in-GATK4-2-0-0-is-much-lower-than-real-sequencing-depth-why-). \--. Hi GATK team, . GATK version: 4.2.0.0. Sample: Target region sequencing, human cancer, everage depth 1000X. I'm using Mutect2 in GATK4.2.0.0 to call somatic SNV/InDels. What confused me is that I found the depth of each location emitted from ""AD"" field in vcf is much lower than the real depth. I think I have disabled downsampling by set ""--max-reads-per-alignment-start"" to 0. The command line I used is as follow: . gatk  Mutect2  -R  reference.fa  -I  tumor.bam  --panel-of-normals  pon.vcf.gz  -L  target.bed  -O  sample.snvIndels.vcf  --callable-depth  30  --f1r2-tar-gz  sample.f1r2.tar.gz  --min-base-quality-score  25  --max-reads-per-alignment-start  0  --minimum-allele-fraction  0.002  --dont-use-soft-clipped-bases  --force-active  --mitochondria-mode  --enable-all-annotations . For example, a mutated point information in vcf called by GATK4.2.0.0-Mutect2 is: . 1 24868045 . A G . . AC=1;AF=0.500;AN=2;AS\_MQ=60.00;AS\_SB\_TABLE=51,50|46,23;AS\_UNIQ\_ALT\_READ\_COUNT=69;BaseQRankSum=0.561;ClippingRankSum=-1.473;DP=179;ECNT=2;FS=13.849;LikelihoodRankSum=-0.392;MBQ=37,37;MFRL=236,239;MMQ=60,60;MPOS=44;MQ=60.00;MQ0=0;MQRankSum=0.000;NCC=0;NCount=0;OCM=0;PON;POPAF=7.30;REF\_BASES=GCTCAGCAGAACAGACCCAGA;ReadPosRankSum=1.335;SOR=1.545;Samples=HD786\_4-1;TLOD=230.09 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:**101,69**:0.408:170:54,30:45,39:51,50,46,23. But the information of the same point in vcf called by GATK4.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7285:903,down,downsampling,903,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7285,1,['down'],['downsampling']
Availability,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8737:66,Echo,EchoCallset,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737,4,['Echo'],['EchoCallset']
Availability,"This is a port of the GATK3 version I actually ran. Some differences in the genotyping engines and the fixed median calculation in MathUtils between GATK3 and GATK4 make the results _slightly_ different in some cases, but still within tolerances.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4940:235,toler,tolerances,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4940,1,['toler'],['tolerances']
Availability,"This is a prototype of the basic infrastructure that must go in to make the junction tree based Haplotype finding work. I have pulled out a toggle for the HaplotypeCaller that that enables a separate ReadThreadingAssembler codepath for haplotype finding. Right now when this mode is enabled `ExperimentalReadThreadingAssembler` is used in conjunction with `JuncitonTreeKBestHalotypeFinder` to extract only haplotypes that show up in our junction trees with evidence of > 3 reads. This still poses problems with dangling end recovery as definitionally those branches never include complete junction tree data. . I will continue to work on this branch (as it is in a somewhat rough state still) but I would like to at least get some eyes on it before i get too deep in the weeds to at least validate the structural approach I have chosen. . Currently known issues in this branch: ; - Tests are failing due to resolution of non-unique reference sink vertexes, I would solicit help as to how best to resolve the case where junction trees point to both a reference stop allele and a continued path.; - There is at least one very degenerate edge case that might cause the code to hang, I would also ask after what is the best way to close out of looping assembly structures that never have reads to close them (i.e. a ""dangling end"" hom-var that happens to point to a non-unique reference base). ; - Probably after discussion the threshold for discarding junction trees will be changed to instead use paths from the discarded tree first. . Resolves #5925",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6034:524,recover,recovery,524,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6034,1,['recover'],['recovery']
Availability,"This is a user error, not a GATK bug. For questions about tool usage, please use the GATK forum: https://gatk.broadinstitute.org/hc/en-us/community/topics",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7214#issuecomment-824111722:15,error,error,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7214#issuecomment-824111722,1,['error'],['error']
Availability,"This is an issue with the Gencode files chosen for the datasources. The Fasta file we use currently is a subset of the total genes in Gencode, so this is an expected error (though it shouldn't be a user error). We have 2 options:; 1) changing the code to throw a warning and ignore variants in transcripts not in the Fasta.; 2) Download a larger set of sequences from Gencode and make sure all transcripts are represented in the Fasta file. My choice is 2 because it is a relatively seamless update (though the data sources will need to be updated, and this will add about 1.6 GB to the data sources). @LeeTL1220 Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313:166,error,error,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313,3,"['Down', 'error']","['Download', 'error']"
Availability,"This is an update of where I am at with this issue. It turns out that using --forceActive and --dontTrimActiveRegions only worked for picking up some of the het SNP calls with HaplotypeCaller. A fun side effect was that some calls that were made with the 'vanilla' best practices HC options were now being missed with the forceActive/dontTrim options. So our clinical team decided to use samtools/bcftools for a pileup approach in combination with HC. We call variants with samtools/bcftools then filter the 'samtools' vcf for VAF > 0.15 and pass that vcf to HC with the -L flag to force HC to make these calls. This is working, all of the calls we are trying to pick up are now being found with our combined method. We also run the vanilla best practices HC on our data and merge the vanilla and samtools vcfs after they go through HC for downstream hard filtering and annotation. Part of this hybrid vanilla/samtools method is for continuity, we're been running 'vanilla' HC for awhile now and didn't want to completely drop it for our new samtools/HC calling approach, so we are combining both to be extra conservative. We decided to keep HC around for 2 reasons, 1) it's not going to give us as many false positives as a 'pileup' method and 2) our downstream annotation software has been set up for dealing with HC vcf files and switching to another vcf INFO format would be painful. But it certainly has causes some alarm about the 'unknown unknowns' that we could be missing in a clinical context. All of these troublesome variants checked out with Sanger sequencing, so this is definitely a real issue and the problem is occurring in clinically-relevant genes, such as F5. I'm happy to provide additional info to help the GATK development team figure out why these variants are missed with HC in the 'vanilla' best practices mode.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-431985127:840,down,downstream,840,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-431985127,2,['down'],['downstream']
Availability,"This is based on some digging for a user query here: https://gatk.broadinstitute.org/hc/en-us/community/posts/360060283092-GenotypeGVCFs-java-lang-NullPointerException. The background here is that GenomicsDB added support for 64 bit INFO fields starting 4.1.5.0. When using bcf codec, GenomicsDB will pass a 64 bit type to GATK, which isn't supported by GATK. But instead of flagging that as a missing type, it seems like GATK still tries to decode the missing type hitting the NPE. We probably want to throw a better error so the user knows what the issue is.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6548:518,error,error,518,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6548,1,['error'],['error']
Availability,This is being addressed by the downsampling code in #3106,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2251#issuecomment-308117105:31,down,downsampling,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2251#issuecomment-308117105,1,['down'],['downsampling']
Availability,"This is blocked, I believe, by the Spark tests not actually currently working. Once they're working (and I need action from the team for that) I can add Spark Sanity tests. I sent email about it but posting the error here. ; Command:; ```; ./gatk-launch MarkDuplicatesSpark \; --shardedOutput true \; -O /scratch/tmp.md.bam \; --numReducers 0 \; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Error:; ```; 16/11/29 16:21:01 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethod",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2288#issuecomment-264212007:211,error,error,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288#issuecomment-264212007,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"This is definitely a bug with the way serialization is handled, but it's hard to tell where the issue is exactly. Spark is trying to serialize something into a byte buffer, but it's trying to put more bytes in than fit in a java array. If you could produce a very small bam file that reliably reproduces this problem we might be able to investigate it, but I don't have bandwidth to really look into this right now. Spark tools are a low priority at the moment. I would recommend sorting the file with the non-spark SortSam for now. I'm sorry I don't have a better answer, but dealing with serialization issues is very often a huge can of worms.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949#issuecomment-2286853649:284,reliab,reliably,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949#issuecomment-2286853649,1,['reliab'],['reliably']
Availability,"This is due to`GATKVAriantContextUtils::makeGenotypeCall` using `GenotypeLikelihoods.getAlleles`, which throws an error because you have to explicitly tell `GenotypeLikelihoods` to initialize ploidies != 2. We can fix this by explicitly initializing but it seems that `GenotypeLikelihoodCalculators` has the same functionality but more robust.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2214:114,error,error,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2214,2,"['error', 'robust']","['error', 'robust']"
Availability,This is exactly the sort of nightmare bug we get every java update. Just 1 isn't bad at all. I'm surprised replacing all the hashsets didn't work. It could come down to inadequate tiebreaking in a sort which falls back to identityHash. We saw that in a different bug recently. I'll take a look.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532701834:161,down,down,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532701834,1,['down'],['down']
Availability,This is great! Thanks for putting this together Jonn. Could we also add the option to compare a vcf and a reference to see if the vcf was generated using that reference? Sometime users have vcfs from a previous study and don't know for sure if they are using the right ref. We see this often. Users want to use post-variantcalling tools and end up getting weird errors due to a wrong reference.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6837#issuecomment-697138607:362,error,errors,362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6837#issuecomment-697138607,1,['error'],['errors']
Availability,This is important for Infogain and other potential hybrid data because it will allow Permutect to separately normalize within each read group and otherwise keep track of different read groups downstream. It also makes the data structures in Permutect much more convenient because numeric data is easier to write to a memory map.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8860:192,down,downstream,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8860,1,['down'],['downstream']
Availability,"This is not working. Is there another way to install the python pacakges for `CNNScoreVariants`?. I get this error:. ```; conda create -n gatk -f ./scripts/gatkcondaenv.yml; Collecting package metadata (current_repodata.json): done; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - ./scripts/gatkcondaenv.yml. Current channels:. - https://conda.anaconda.org/bioconda/linux-64; - https://conda.anaconda.org/bioconda/noarch; - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch; - https://conda.anaconda.org/r/linux-64; - https://conda.anaconda.org/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-578660611:109,error,error,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-578660611,2,"['avail', 'error']","['available', 'error']"
Availability,"This is only changing codepaths related with the help. So this change will change the usage to say that all filters are valid for disabeFilter to say that only the available ones are valid. The only problem could be in the docgen code, but not in the behavior of the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2360#issuecomment-275496845:164,avail,available,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2360#issuecomment-275496845,1,['avail'],['available']
Availability,"This is part of the effort to port GATK3 VariantEval to GATK4. Because VariantEval is a pretty big tool, I thought it would help to separate changes in the VariantEval package from core GATK changes. If you would prefer I could split this PR into several, based on what they target. However, here is a summary:. 1) I added a method to FeatureInput so downstream code and determine if the name is actually user-supplied or whether this is the default one assigned by GATK. 2) I split MultiVariantWalker into a base class that does not have arguments specified. This is comparable to what already exists for VariantWalkerBase - this is a pretty minor change. 3) I added a method to VariantWalkerBase to return the set of intervals potentially being iterated. The intent is to return either the full genome, or the user-supplied intervals. . 4) I restored some code to MendelianViolation that is used by components of VariantEval. This is very close to identical from GATK3. 5) Likewise to SampleDBBuilder (code had to be tweaked for GATK4). 6) IntervalUtils/MathUtils/Utils: I restored from GATK3 methods not ported to GATK4. These should be exact copies of GATK3 unless a change was needed. Once these are closed out, I will make a separate PR with VariantEval itself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4495:351,down,downstream,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495,1,['down'],['downstream']
Availability,"This is probably complicated by a bug in the htsjdk warning from previous versions, which should be fixed in the latest master now. There's probably still a bug, but the error will be more informative now. There may be a ploidy-related bug since the somatic genotypes are a little funky that way. I don't like the fact that this is calling a biallelic method. @fleharty if you still care about this, can you run it again with the latest master?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-898580381:170,error,error,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-898580381,1,['error'],['error']
Availability,"This is probably complicated by a bug in the htsjdk warning from previous versions, which should be fixed in the latest master now. There's probably still a bug, but the error will be more informative now. There may be a ploidy-related bug since the somatic genotypes are a little funky that way. I don't like the fact that this is calling a biallelic method. @fleharty if you still care about this, can you run it again with the latest master?. _Originally posted by @ldgauthier in https://github.com/broadinstitute/gatk/issues/6689#issuecomment-898580381_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7729:170,error,error,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7729,1,['error'],['error']
Availability,This is really nice for downstream projects! Thank you very much!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4098#issuecomment-356290364:24,down,downstream,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4098#issuecomment-356290364,1,['down'],['downstream']
Availability,"This is rebased off of https://github.com/broadinstitute/gatk/pull/3716, since it depends on code there. Hence, only the second commit needs to be reviewed in this PR. The code and tests are quite similar to that for PlotSegmentedCopyRatio/PlotACNVResults. However, I've changed the R scripts to be more efficient (WGS plots no longer take several hours). Furthermore, PlotModeledSegments is more flexible than PlotACNVResults in that it plots CR, AF, or both on the fly depending on the available inputs. I've also added some more input validation, changed some terminology, and moved over to data.table for reading TSVs in R.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3729:488,avail,available,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3729,1,['avail'],['available']
Availability,"This is set to an unexposed 250 samples. Since the sampling and posterior estimation is done online to prevent the entire samples x intervals x dCR samples matrix from causing OOM, I think we suffer from lack of vectorization. However, the good news is that we can probably get by with far fewer samples, say ~20) if all we want are estimates of posterior mean and standard deviation---not even sure if the latter will be used by downstream analyses anytime soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5754:430,down,downstream,430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5754,1,['down'],['downstream']
Availability,"This is something we got to only in a rather basic way in GATK but is very useful to enable in order to save users from themselves. This would involve three components:. 1) Hard min/max values that correspond to limits beyond which values could cause errors/program failures; violation should throw a User Exception;. 2) Recommended min/max values that correspond to limits beyond which values do not make sense for a given analysis functionality for standard use cases; violation should log a WARN entry. 3) Behavior-disabling value if applicable. Let's say we have an argument that provides a threshold for filtering; and it takes min. 4, max. 20. We may want to set it up so that passing -1 disables the behavior controlled by the argument (so in the filtering case, ""-1"" means ""don't filter at all"") without tripping the min value check. . These should all be accessible to the GATKDoclet (or equivalent) for documentation purposes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/143:251,error,errors,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/143,2,"['error', 'failure']","['errors', 'failures']"
Availability,"This is still throwing errors. Importing `chr2	6861126	.	ACCCACACAC	*,<NON_REF>	2.22	.	AS_RAW_BaseQRankSum=|||;AS_RAW_MQ=0.00|0.00|10800.00|0.00;AS_RAW_MQRankSum=|||;AS_RAW_ReadPosRankSum=|||;AS_SB_TABLE=0,0|0,0|3,0|0,0;DP=6;QUALapprox=126;RAW_MQandDP=21600,6;VarDP=3	GT:AD:DP:GQ:PL:SB	1/1:0,3,0:3:10:126,10,0,129,18,168:0,0,3,0`; for interval -L chr2:6861134-7268940 produces the log; ```; 11:11:14.566 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:11:15.838 INFO GenomicsDBImport - Importing batch 1 with 1 samples; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Found cell [ 0, [ 255817547, 255817547 ] ] that does not belong to TileDB/GenomicsDB partition with row_bounds [ 0, 0 ] column_bounds [ 255817555, 256225361 ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5449#issuecomment-484558863:23,error,errors,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5449#issuecomment-484558863,1,['error'],['errors']
Availability,"This is super interesting - the Java 11 integration tests that are failing in this last run are exactly the same tests that have been problematic on the Java 17 branch. I looked at the failing values that were being produced on that branch (which we've updated), and they're identical to the failing values seen here. Even on the Java 17 branch, there has been some inconsistency in the failures (they usually fail, but not always).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8102#issuecomment-1329162240:387,failure,failures,387,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8102#issuecomment-1329162240,1,['failure'],['failures']
Availability,"This is the method we use to determine whether or not we should retry an error. https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/io/IoActor.scala#L158. The ""how is it retried"" is a bit spaghetti but basically it's asynchronous exponential backoff up to a certain number of retries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300276723:73,error,error,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300276723,1,['error'],['error']
Availability,"This isn't just a matter of changing the number. [RegisterCoder was made more stringent](https://cloud.google.com/dataflow/release-notes/java) and this will force some code changes. Hopefully only little ones, but I got only as far as getting an internal Java error and I think that's a sign I should go to bed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/754:260,error,error,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/754,1,['error'],['error']
Availability,"This issue affect version of GATK4 from version 4.0.10.1 and onwards, and is related to github feature/pull request [#4969: ""Improve MQ calculation accuracy""](https://github.com/broadinstitute/gatk/pull/4969). GVCFs with large `RAW_MQ` sum of sumSquaredMQs values end up with an error like:. ```; A USER ERROR has occurred: Bad input: malformed RAW_MQ annotation: 3415207168,1749038; ```. The error happens when the sumSquaredMQs element value is greater than Java's. `Integer.MAX_VALUE`. See the following [github pull request comment for more details](https://github.com/broadinstitute/gatk/pull/4969#issuecomment-439642813). . Should a `java.lang.Long`, `java.math.BigInteger`, `java.lang.Double`, or `java.math.BigDecimal` be used here instead of a `java.lang.Integer` for method: [`parseRawDataString` inside `org.broadinstitute.hellbender.tools.walkers.annotator.RMSMappingQuality`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RMSMappingQuality.java#L251) ?. I've noticed this error when using GATK4 version 4.0.11.0 on the [GATK4 Germline SNPs-INDELs WDL workflow](https://github.com/gatk-workflows/gatk4-germline-snps-indels). It happens on the [`JointGenotyping.GenotypeGVCFs` task](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl#L127).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5433:279,error,error,279,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5433,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,This issue also pops up during FilterAlignmentArtifacts in GATK 4.1.7.0 (experimental). ```; *** Error in `java': munmap_chunk(): invalid pointer: 0x00007fb4c8e6e540 ***; ======= Backtrace: =========; /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7fb4cdfee7e5]; /lib/x86_64-linux-gnu/libc.so.6(cfree+0x1a8)[0x7fb4cdffb698]; /cromwell_root/tmp.be1fb8a9/libgkl_smithwaterman4505316410124989699.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7fb4ac3cffa8]; /cromwell_root/tmp.be1fb8a9/libgkl_smithwaterman4505316410124989699.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7fb4ac3cfbf8]; [0x7fb4b8b95f92]. ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-631377270:97,Error,Error,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-631377270,1,['Error'],['Error']
Availability,"This issue came up on the forum and Ted Brookings took a look at the stack trace to verify that it is most likely not an issue with the input data. The user also specified that they did not get this issue with GATK 4.2.0.0. This request was created from a contribution made by Quentin Chartreux on September 10, 2021 12:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406578679195-GenotypeGVCFs-error-IndexOutOfBoundsException](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406578679195-GenotypeGVCFs-error-IndexOutOfBoundsException). \--. I try to perform joint genotyping with genotypeGVCF. . Before i run haplotypecaller then Genomicsdbiimport and now genotypesGVCF. . I used gatk 4.2.2.0. . I run Genomicsdbiimport by interval so i would like to perform genotypesGVCF by interval. . the command use for genotypesgvcf is : . gatk \\ ; ; \--java-options ""-Xmx${memory\_java}M -Xms${memory\_java}M -XX:ParallelGCThreads=${SLURM\_CPUS\_PER\_TASK}"" \\ ; ; GenotypeGVCFs \\ ; ; \-R ${REF\_Genome} \\ ; ; \-V gendb://${vcf\_database\_tmp} \\ ; ; \-O ${TMP\_DIR}/gentaumix\_interval\_${SLURM\_ARRAY\_TASK\_ID}\_raw.vcf.gz \\ ; ; \-D ${DBSNP} \\ ; ; \--sequence-dictionary ${Dict} \\ ; ; \-L ${Interval} \\ ; ; \-G StandardAnnotation -G AS\_StandardAnnotation \\ ; ; \--only-output-calls-starting-in-intervals \\ ; ; \--merge-input-intervals \\ ; ; 2> ${log\_DIR}/Interval\_${SLURM\_ARRAY\_TASK\_ID}. And the log (for one interval but it's the same for all): . Using GATK jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx7000M -Xms7000M -XX:ParallelGCThreads=2 -jar /shared/ifbstor1/projects/gentaumix/conda/envs/gatk\_4.2.2.0/share/gatk4-4.2.2.0-0/gatk-package-4.2.2.0-local.jar GenotypeGVCFs -R /shared/p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7465:420,error,error-IndexOutOfBoundsException,420,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7465,2,['error'],['error-IndexOutOfBoundsException']
Availability,"This issue has come up during my work on #6634 and has resulted in the decision to introduce a new argument to GATK `--use-original-alignments-for-genotyping-overlap` in order to better match DRAGEN for concordance. Reads in the GATK undergo a number of modifications before they are used for genotyping that I have listed down below. (Note: between each of these steps some reads get lost to various filtering code and this is not an exhaustive list). 1. Reads undergo modification in `AssemblyBasedCallerUtils.finalizeRegion()` where the reads have their soft-clipped bases reverted, low quality ends removed, mate overlapping base qualities modified, and overhangs outside of the active region removed. Then these reads are used for assembly to discover haplotypes. ; 2. Once we have discovered haplotypes the whole assembly region (reads, haplotypes and all) gets trimmed down to a smaller span that ~overlaps the variants discovered the haplotypes plus either 75+ or 20 bases of padding depending on what type of events are seen. ; 3. These clipped reads (with reads below 10 bases in length being removed) have their base qualities farther modified in `PairHMMLikelihoodCalculationEngine.createQualityModifiedRead()` in various ways. This modification does not stick however since the base qualities are all modified on a clean partial copy of the read.; 4. Following this the reads (the ones from step 2) are realigned to the reference according to their best haplotypes. Sometimes this means as few as 11 bases of ""read"" are being realigned at this stage. . It is these realigned reads that are used for genotyping, where the only reads that are actually used to contribute likelihoods for calls are reads that overlap the variant event within 2 bases of overlap on either side. In DRAGEN they do something different that we had to replicate to achieve concordance. Dragen still performs equivalent modifications for steps 1-3 as they apply to the reads but rather than performing step 4 and u",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6706:323,down,down,323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6706,2,['down'],['down']
Availability,"This issue is for tracking https://github.com/samtools/samtools/issues/704. The samtools-generated .idx and .dict entries for a reference sequence that contains a ""."" have different lengths (the .dict appears to not count the "".""), and the MD5s are different. This causes GATK to throw:. > A USER ERROR has occurred: Couldn't read file /Users/cnorman/projects/htsjdk/src/test/resources/htsjdk/samtools/cram/amb.fa. Error was: Index length does not match dictionary length for contig: iupac with exception: Index length does not match dictionary length for contig: iupac. and causes failures when reading a CRAM file using that reference. It works fine if the .dict is regenerated using GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3306:297,ERROR,ERROR,297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3306,3,"['ERROR', 'Error', 'failure']","['ERROR', 'Error', 'failures']"
Availability,"This issue only happens in genomes with a large number of chromosomes, such as hg38. b37 and hg19 are fine. workaround: `--conf spark.driver.extraJavaOptions=-Xss2m --conf spark.executor.extraJavaOptions=-Xss2m`. debug log:; ```; ...; 00:05 DEBUG: [kryo] Write object reference 100367: HLA-DRB1*15:03:01:01; 00:05 DEBUG: [kryo] Write object reference 100369: HLA-DRB1*15:03:01:02; 00:05 DEBUG: [kryo] Write object reference 100371: HLA-DRB1*16:02:01; 21/09/12 22:10:49 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040; 21/09/12 22:10:49 INFO StandaloneSchedulerBackend: Shutting down all executors; 21/09/12 22:10:49 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 21/09/12 22:10:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 21/09/12 22:10:49 INFO MemoryStore: MemoryStore cleared; 21/09/12 22:10:49 INFO BlockManager: BlockManager stopped; 21/09/12 22:10:49 INFO BlockManagerMaster: BlockManagerMaster stopped; 21/09/12 22:10:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 21/09/12 22:10:49 INFO SparkContext: Successfully stopped SparkContext; 22:10:49.533 INFO HaplotypeCallerSpark - Shutting down engine; [September 12, 2021 10:10:49 PM CST] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=1788346368; Exception in thread ""main"" java.lang.StackOverflowError; at com.esotericsoftware.kryo.util.MapReferenceResolver.useReferences(MapReferenceResolver.java:70); at com.esotericsoftware.kryo.Kryo.writeReferenceOrNull(Kryo.java:665); at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:570); at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:79); at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508); at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:575); at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectFiel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-917650984:590,down,down,590,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869#issuecomment-917650984,4,['down'],['down']
Availability,"This looks like the auth service saw some transient error and instead of replying with a 503 (service temporarily unavailable), it replied with a 403 (forbidden). A workaround would be to retry on those. The side effect will be that if we genuinely do not have access, it'll take a bit longer to report it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3735#issuecomment-338778985:52,error,error,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3735#issuecomment-338778985,1,['error'],['error']
Availability,This method (validateSequenceDictionaries) in GATKTool needs to be modified so that the vcf file names associated with each sequence dictionary are passed into validateDictionaries() to make error messages more useful.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/660:191,error,error,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/660,1,['error'],['error']
Availability,This migrates ONLY that part of the code from ah_var_store to EchoCallset branch (there are other changes in ah_var_store),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8755:62,Echo,EchoCallset,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8755,1,['Echo'],['EchoCallset']
Availability,"This moves us to a snapshot of google-cloud-java based off of a branch in my fork here: https://github.com/droazen/google-cloud-java/tree/dr_retry_CloudStorageReadChannel_fetchSize. This patch wraps many more operations within retries, and in our tests resolves the intermittent 503/SSL errors completely when running at scale. This PR also migrates us from setting retry settings per-Path to setting it globally, using a new API from that google-cloud-java branch. This fixes an issue where the number of reopens was getting set to 0 deep in the google-cloud-java library. Resolves #2749; Resolves #2685; Resolves #3118; Resolves #3120; Resolves #3253",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3295:287,error,errors,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3295,1,['error'],['errors']
Availability,"This new PathSeq WDL redesigns the workflow for improved performance in the cloud. Downsampling can be applied to BAMs with high microbial content (ie >10M reads) that normally cause performance issues. . Other improvements include:. * Removed microbial fasta input, as only the sequence dictionary is needed.; * Broke pipeline down to into smaller tasks. This helps reduce costs by a) provisioning fewer resources at the filter and score phases of the pipeline and b) reducing job wall time to minimize the likelihood of VM preemption.; * Filter-only option, which can be used to cheaply estimate the number of microbial reads in the sample.; * Metrics are now parsed so they can be fed as output to the Terra data model.; * CRAM-to-BAM capability; * Updated WDL readme; * Deleted unneeded WDL json configuration, as the configuration can be provided in Terra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6536:83,Down,Downsampling,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6536,2,"['Down', 'down']","['Downsampling', 'down']"
Availability,"This occurs when there is an issue with the coding sequence for a given transcript in Gencode. It's a problem with the Gencode data itself. I would prefer to keep as an error so the user is more likely to see it. There is some work to be done here, though - the error message should also contain a description of an exception that was caught to produce this log statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4882#issuecomment-396389331:169,error,error,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4882#issuecomment-396389331,2,['error'],['error']
Availability,This one seems to be failing tests -- I'm going to try re-running the test suite to determine whether the failures are real.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6718#issuecomment-705674708:106,failure,failures,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6718#issuecomment-705674708,1,['failure'],['failures']
Availability,"This read filter removes unnmapped reads and reads with unmapped mates. When used in combination with `MateOnSameContigOrNoMappedMateReadFilter` this subsets down to reads only on chrM whose mate is also on chrM. If we only used the `MateOnSameContigOrNoMappedMateReadFilter` we end up with reads whose mate is unmapped still in the BAM, but not the unmapped read, which causes problems downstream in the mitochondria pipeline. This read filter will make the subsetting step faster when we no longer need the NuMTs. I would really appreciate this getting in before the next release (on Tuesday). (fyi @droazen) @ldgauthier @jsotobroad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5826:158,down,down,158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5826,2,['down'],"['down', 'downstream']"
Availability,This release also errors out with a descriptive error message if the length of a field in the data lines does not match the length descriptor in the header - see https://github.com/broadinstitute/gatk/issues/5045.; Error out behavior as per [Laura's comment here](https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413667356),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5397#issuecomment-437140783:18,error,errors,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5397#issuecomment-437140783,3,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"This release contains important bugfixes, including a fix for https://github.com/broadinstitute/gatk/issues/8141 (intermittent failure to properly compress outputs)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8409:127,failure,failure,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8409,1,['failure'],['failure']
Availability,This removes some symlinks that were checked into git-lfs and puts them back into normal git. It stops lfs from outputting failure messages on checkout.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5229:123,failure,failure,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5229,1,['failure'],['failure']
Availability,"This request removes all calls to getChr() in the code by replacing calls to getChr() with calls to getContig() and removing a redundant assertion that contains a call to getChr(). To complete the removal of getChr() from the code, we could:. Remove the getChr() method overrides in the TableFeature and ArtificialTestFeature classes and convert these classes from implementing the Feature interface to implementing the Locatable interface. Remove the definition of the Feature interface from htsjdk.tribble (since all Feature does is add getChr() to the Locatable interface), and replace all references to the Feature interface in the tools and libraries with references to the Locatable interface. These steps will also remove the deprecation warnings for getChr() (part of #377).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/478:127,redundant,redundant,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/478,1,['redundant'],['redundant']
Availability,"This request was created from a contribution made by ABours on May 29, 2020 18:23 UTC. Link: https://gatk.broadinstitute.org/hc/en-us/community/posts/360067695771-GenotypeGvcfs-has-formatting-issues-in-both-v4-1-6-0-as-v4-1-7-0. --. Hi,. I'm using v4.1.6.0 of GenotypeGvcfs to make a vcf, out of whole genome data from 19 samples (following your recommendations). When I run ValidateVariants to check the output of GenotypeGvcfs I get a error message, which states that one or more of the ALT allele are actually not in the samples provided. A previous user already found a similar error in ValidateVariants (https://gatk.broadinstitute.org/hc/en-us/community/posts/360061452132-GATK4-RNAseq-short-variant-discovery-SNPs-Indels-), but then for Haplotypecaller, and you have opened a bugreport to add a feature to ValidateVariants: https://github.com/broadinstitute/gatk/issues/6553. However, it would be nice if you could actually investigate the formatting error. Unfortunately my formatting error isn't the same as reported in the other post. I have 105 error in which the 1st alternative allele is a spanning deletion and the 2nd (and 3rd) is either an indel or snp. It's true that the 2nd and 3rd allele is actually not found in my samples. I even have 7 occurances in which the 1st allele (spanning deletion) has allele frequency 1.00. my code is the following for GenotypeGVCFs:. java -Xms32G -Xmx32G -jar ${gatk4} GenotypeGVCFs -R ${ref} -V ${pipeline}/${name}\_v4.1.6.0.g.vcf.gz -O ${vcf}/${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list 2> ${log}/${name}\_v4.1.6.0\_genotype.log. for ValidateVariants:. java -Xms10G -Xmx10G -jar ${gatk4} ValidateVariants -R ${ref} -V ${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list --warn-on-errors 2> ${log}/${name}\_v4.1.6.0\_genotype\_valivar.log. the warning in ValidateVariants and the site look like this:. 14:12:15.126 WARN ValidateVariants - \*\*\*\*\* Input 1st\_v4.1.6.0.vcf.gz fails strict validation of type ALL: on",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6630:437,error,error,437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6630,3,['error'],['error']
Availability,"This request was created from a contribution made by D B on January 17, 2020 14:43 UTC. Link: https://gatk.broadinstitute.org/hc/en-us/community/posts/360056339072-Mutect2-Need-one-or-two-reads-to-construct-a-fragment. --. The command I used below is able to generate the .vcf output along with its index and stats file, but my snakemake run fails to complete due to exit status (3 instead of 0). I wonder if below error is caused by trying to split the runs by chromosome and setting improper interval padding. Thank you for your time. ------------------------------------------------------------------------------------------------------------------------------------. a) GATK version used. _The Genome Analysis Toolkit (GATK) v4.1.4.1_. b) Exact GATK commands used. _/usr/bin/time -v gatk --java-options ""-Xmx10G"" Mutect2 -R ../reference/indices\_010920/GRCh38.d1.vd1.fa -L chr4.bed -I chr4.bam --max-mnp-distance 0 --interval-padding 100 -O chr4.vcf.gz_. c) The entire error log if applicable. _java.lang.IllegalArgumentException: Need one or two reads to construct a fragment_ ; _at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:725)_ ; _at org.broadinstitute.hellbender.utils.read.Fragment.create(Fragment.java:43)_ ; _at org.broadinstitute.hellbender.utils.read.Fragment.createAndAvoidFailure(Fragment.java:58)_ ; _at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)_ ; _at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1376)_ ; _at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)_ ; _at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)_ ; _at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)_ ; _at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)_ ; _at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)_ ; _at org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods.groupEvidence(AlleleLikeliho",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6419:415,error,error,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6419,2,['error'],['error']
Availability,"This request was created from a contribution made by Duo Xie on August 20, 2022 16:16 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator](https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator). \--. REQUIRED for all errors and issues: ; ; a) GATK version used:v4.2.6.1  ; ; b) Exact command used: see below ; ; c) Entire program log: see below ; ; **How can I assign a temp directory and won't get the bug?**. I always got error when I assigned the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --know",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:337,error,errors,337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,2,['error'],"['error', 'errors']"
Availability,"This request was created from a contribution made by Elizabeth Lee on October 19, 2022 17:47 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/9761457082907-JointGenotyping-ImportGvcfs-terminates-without-an-active-exception](https://gatk.broadinstitute.org/hc/en-us/community/posts/9761457082907-JointGenotyping-ImportGvcfs-terminates-without-an-active-exception). \--. JointGenotyping fails in ImportGvcfs with the c++ error ""terminate called without an active exception"", which occurs when a thread goes out of scope without calling join() or detach(). This occurs when running JointGenotyping on 345 gvcfs created by GATK4 ExomeGermlineSingleSample; the workflow is running on an HPC cluster in Singularity (single node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the wh",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:439,error,error,439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['error'],['error']
Availability,"This request was created from a contribution made by FranBC on July 23, 2021 18:59 UTC.; This user is receiveing a NullPointerException error when running CollectvariantCallingMetrics and has verified that the vcf headers match the reference. The user uploaded their file to the GATK FTP server as ""CollectVariantCallingMetrics_dbsnp155_FranBC2.tar.gz"". Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360074951711-No-results-of-CollectvariantCallingMetrics#community\_comment\_4404056347547](https://gatk.broadinstitute.org/hc/en-us/community/posts/360074951711-No-results-of-CollectvariantCallingMetrics#community_comment_4404056347547). \--. Dear GATK Team,. I am having a similar issue to Yenan, when using CollectVariantCallingMetrics, was a cause/workaround ever found for this?. I have no problem when using this tool with the GRCh38 dbSNP (build 138) vcf file provided in the resource bundle, however whenever I try a different dbSNP build, it throws this error:. \[Fri Jul 23 13:25:03 CEST 2021\] picard.vcf.CollectVariantCallingMetrics done. Elapsed time: 70.55 minutes. Runtime.totalMemory()=1623195648. To get help, see [http://broadinstitute.github.io/picard/index.html#GettingHelp](http://broadinstitute.github.io/picard/index.html#GettingHelp). java.lang.NullPointerException: Cannot invoke ""htsjdk.samtools.SAMSequenceRecord.getSequenceLength()"" because the return value of ""htsjdk.samtools.SAMSequenceDictionary.getSequence(String)"" is null. at picard.util.DbSnpBitSetUtil.loadVcf(DbSnpBitSetUtil.java:163). at picard.util.DbSnpBitSetUtil.createSnpAndIndelBitSets(DbSnpBitSetUtil.java:131). at picard.vcf.CollectVariantCallingMetrics.doWork(CollectVariantCallingMetrics.java:101). at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:308). at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37). at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160). a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7383:136,error,error,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7383,2,['error'],['error']
Availability,"This request was created from a contribution made by Jay Singh on May 09, 2020 12:47 UTC. Link: https://gatk.broadinstitute.org/hc/en-us/community/posts/360062649991-java-lang-IllegalArgumentException-Invalid-interval-Contig-chr1-start-9-end-10464. --. Can you please provide ; a) GATK version used - **gatk-4.1.7.0** ; b) Exact GATK commands used. **./gatk Funcotator --variant /home/deepak/software\_library/gatk-4.1.7.0/SAMPLE3\_Haplo.g.vcf.gz --reference /media/deepak/EXTRA/Genomedir/hg38/hg38.fasta --ref-version hg38 --data-sources-path /media/deepak/EXTRA/FUNCOTATOR\_DATA/DATA\_SOURCES --output SAMPLE\_3\_variants.funcotated.vcf --output-file-format VCF** ; **Using GATK jar /home/deepak/software\_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar**. I am using Funcotator and have used the above commands but I am not getting the correct annotated output file. Also I am getting the following error "" **java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:-9 end:10464** "". I have also attached the screenshot of the output result. Thanks. ![](https://gatk.broadinstitute.org/hc/user_images/ra0laiOjkI-jPJAnYIbjpw.png) ![](https://gatk.broadinstitute.org/hc/user_images/L_-FFyP26boUMsHYjvf_Vg.png)<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/5579'>Zendesk ticket #5579</a>)<br>gz#5579</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598:906,error,error,906,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598,1,['error'],['error']
Availability,"This request was created from a contribution made by Joyce Anon on April 25, 2022 06:30 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations](https://gatk.broadinstitute.org/hc/en-us/community/posts/5573282748699-Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations). \--. FilterFuncotations stops with an error. The input file with the reference genome seems to pass ValidateVariants (no errors). It looks like ""FuncotationMap"" doesn't have enough values to go with the keys. I started with a .vcf file downloaded from Nebula Genomics, and sequentially used CNNScoreVariants, FilterVariantTranches (CNN\_1D), and Funcotator, with default settings. I am trying to find the most pathogenic variants. I considered using FilterVcf to remove synonymous and intron variants, but it doesn't look like it can do that. So then I tried FilterFuncotations, but it returns an error. What I want is some way to sort the variants by severity, to find the most pathogenic ones, but I don't know how to do that. GATK version: 4.2.6.1 ; ; Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1-Ubuntu-0ubuntu1.20.04. Excerpt: ; ; \[April 25, 2022 at 2:00:35 AM EDT\] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 0.03 minutes. ; ; Runtime.totalMemory()=319815680 ; ; org.broadinstitute.hellbender.exceptions.GATKException$ShouldNeverReachHereException: Cannot parse the funcotation attribute.  Num values: 31   Num keys: 53. Copied from the terminal: ; ; (gatk) aru@BioinformaticsVM:/mnt/sdb/gatk$ ./gatk FilterFuncotations --allele-frequency-data-source gnomad -O ./output/nebulaFilterFuncotations.vcf --ref-version hg38 -V ./output/nebulaFuncotatorAnnotated.vcf --java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true' ; ; Using GATK jar /mnt/sdb/gatk/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7865:171,Error,Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865,6,"['Error', 'down', 'error']","['Error-ShouldNeverReachHereException-FuncotationMap-in-FilterFuncotations', 'downloaded', 'error', 'errors']"
Availability,"This request was created from a contribution made by Lucas Kopecky Bobadilla on June 05, 2020 15:53 UTC. Link: https://gatk.broadinstitute.org/hc/en-us/community/posts/360067974992-AnalyzeCovariates. --.  . Hello I am using the current 4.17 GATK version and I am trying to run AnalyzeCovariates. I ran the baserecalibrator to get the table file to run AnalyzeCovariates but I am getting this following error in the R command:. [June 5, 2020 10:45:08 AM CDT] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.02 minutes. Runtime.totalMemory()=1233649664. org.broadinstitute.hellbender.utils.R.RScriptExecutorException: . Rscript exited with 1. Command Line: Rscript -e tempLibDir = '/tmp/Rlib.323393943272793217';source('/tmp/BQSR.1306882797239975225.R'); /tmp/AnalyzeCovariates5750988274473323663.csv /media/brent/lucas\_SSD/dicamba\_rnaseq/GATK/qlty\_recalibration/trim\_DIC\_CHR\_F2\_R\_104\_1.table /media/brent/lucas\_SSD/dicamba\_rnaseq/GATK/qlty\_recalibration/AnalyzeCovariates.pdf. Stdout: . Stderr: . Attaching package: ‘gplots’.  . The following object is masked from ‘package:stats’:.  .     lowess.  . Error in distributeGraphRows(list(a, b, c), c(1, 1, 1)) : .   object 'a' not found. Calls: source -> withVisible -> eval -> eval -> distributeGraphRows. Execution halted.  . Any idea what is going on?.  . Thanks!<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/5868'>Zendesk ticket #5868</a>)<br>gz#5868</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6650:402,error,error,402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6650,3,"['Error', 'error', 'mask']","['Error', 'error', 'masked']"
Availability,"This request was created from a contribution made by Mark Godek on May 28, 2020 12:43 UTC. Link: https://gatk.broadinstitute.org/hc/en-us/community/posts/360067471451-Funcotator-cannot-complete-funcotaion-for-variant-due-to-alternate-allele. --. I'm attempting to annotate germline variants after VQSR with Funcotator using GATK 4.1.4.1. GATK command is:. gatk Funcotator \ ; -R ${REFERENCE\_GENOME} \ ; -V ${OUT}/germline.filtered.vcf.gz \ ; -O ${OUT}/annotated.germline.vcf \ ; --output-file-format VCF \ ; --data-sources-path /mnt/data/rbueno/analysis\_files/MedGenome\_FamilialMPMs/Annotation\_data\_sources/funcotator\_dataSources.v1.6.20190124s \ ; --ref-version hg19 ; ; I get many warnings and it terminates with a String index out of range error. Any help is appreciated.  . The tail end of the output follows: ; ; ; 07:33:14.569 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756762-69756762 due to alternate allele: \* ; 07:33:14.575 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756763-69756763 due to alternate allele: \* ; 07:33:14.575 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756763-69756763 due to alternate allele: \* ; 07:33:14.580 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756764-69756764 due to alternate allele: \* ; 07:33:14.580 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756764-69756764 due to alternate allele: \* ; 07:33:16.681 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:70289137-70289137 due to alternate allele: \* ; 07:33:16.681 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:70289137-70289137 due to alternate allele: \* ; 07:33:17.957 INFO VcfFuncotationFactory - dbSNP 9606\_b150 cache hits/total: 521/453691 ; 07:33:18.138 INFO Funcotator - Shut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651:749,error,error,749,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651,1,['error'],['error']
Availability,"This request was created from a contribution made by Matt Johnson on July 05, 2021 21:23 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360067819992-SelectVariants-v4-1-6-0-doesn-t-select-the-variants-as-expected-#community\_comment\_4403173344411](https://gatk.broadinstitute.org/hc/en-us/community/posts/360067819992-SelectVariants-v4-1-6-0-doesn-t-select-the-variants-as-expected-#community_comment_4403173344411). \--. Hello, I am also having this issue. \[This page\](/hc/en-us/articles/360035530752-What-types-of-variants-can-GATK-tools-detect-or-handle-) indicates the following definition of SYMBOLIC:. SYMBOLIC (such as the  `<NON-REF>`  allele used in GVCFs produced by HaplotypeCaller, the  `*`  allele used to signify the presence of a  [spanning deletion](https://gatk.zendesk.com/hc/en-us/articles/360035531912), or undefined events like a very large allele or one that's fuzzy and not fully modeled; i.e. there's some event going on here but we don't know what exactly). Therefore I would expect SelectVariants --select-type-to-exclude SYMBOLIC to not have any calls containing spanning deletions. However, the output VCFs still do (in gatk4 4.2.0.0). This causes problems for downstream tools like FastaAlternateReferenceMaker:. java.lang.IllegalArgumentException: the input sequence contains invalid base calls like: \*. Is there any way to force GATK to exclude spanning deletions when filtering a VCF?<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/168722'>Zendesk ticket #168722</a>)<br>gz#168722</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7341:1214,down,downstream,1214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7341,1,['down'],['downstream']
Availability,"This request was created from a contribution made by Min-Hwan Sohn on March 05, 2020 01:00 UTC. Link: https://gatk.broadinstitute.org/hc/en-us/community/posts/360057956031-PathseqPipelineSpark-stop-with-error-message-regarding-com-esotericsoftware-kryo-KryoException-Buffer-underflow-. --. Hi GATK team. I recently used PathseqPipelineSpark embedded in GATK v4.1.4.1 (installed from anaconda) to identify potential microbial composition of human tissue Whole-Genome samples. . NovaSeq-sequenced paired-end reads (2X151bp) were aligned (onto hg19 reference), duplicate-removed, base quality score-recalibrated and BQSR-applied, which eventually used as an input to the PathseqPipelineSpark. . Since I failed to find hg19 host reference in the GATK resource bundle, first I created a BWA image file and a Kmer file originated from hg19 reference fasta with the command below. But for microbe-related files, I used ones that were contained in the bundle.  . **'''** ; ; **gatk --java-options ""-Xmx50G"" BwaMemIndexImageCreator -I ./ref.fasta** ; **gatk --java-options ""-Xmx50G"" PathSeqBuildKmers --reference ./ref.fasta -O ref.hss** ; ; **'''**.  . And then I ran PathSeq with the following command.  . **'''** ; ; **gatk --java-options ""-Xmx200G"" PathSeqPipelineSpark \** ; **--input sample.bam \** ; **--filter-bwa-image ref.fasta.img \** ; **--kmer-file ref.hss \** ; **--is-host-aligned true \** ; **--min-clipped-read-length 70 \** ; **--microbe-fasta pathseq\_microbe.fa \** ; **--microbe-bwa-image pathseq\_microbe.fa.img \** ; **--taxonomy-file pathseq\_taxonomy.db \** ; **--output sample.pathseq.bam \** ; **--scores-output sample.pathseq.txt** ; ; ; **'''**.  . and unfortunately it was shut down by this error message. **09:27:43.974 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/mnt/clinix1/Analysis/mongol/phenomata/Tools/Anaconda3/envs/gatk4/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so** ; **Mar 05, 2020 9:27",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:203,error,error-message-regarding-com-esotericsoftware-kryo-KryoException-Buffer-underflow,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['error'],['error-message-regarding-com-esotericsoftware-kryo-KryoException-Buffer-underflow']
Availability,"This request was created from a contribution made by Rahul Gupta on February 03, 2022 19:42 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4417613488411-In-Mutect2-force-calling-allele-via-alleles-does-not-force-call-the-allele](https://gatk.broadinstitute.org/hc/en-us/community/posts/4417613488411-In-Mutect2-force-calling-allele-via-alleles-does-not-force-call-the-allele). \--. If not an error, choose a category for your question(REQUIRED): ; ; c) Why do I see (......)?. GATK version: 4.2.4.1. Hi all,. I'm running Mutect2 over samples for which I have provided a VCF of alleles that I would like force-called. In 99% of cases this works great, but in a few instances a couple of alleles that I have included for force-calling do not actually make it into the output calls VCF. My command is:. gatk --java-options -Xmx3000m Mutect2 \\ ; ; \-R /data\_in/MY1776/MY1776.self.ref.shifted\_by\_8000\_bases.fasta \\ ; ; \-I /data\_in/MY1776/self\_realigned\_shifted.bam \\ ; ; \--read-filter MateOnSameContigOrNoMappedMateReadFilter \\ ; ; \--read-filter MateUnmappedAndUnmappedReadFilter \\ ; ; \-O /out\_default.vcf \\ ; ; \--alleles /data\_in/MY1776/MY1776.self.ref.reversed.selfRef.shifted.homoplasmies.vcf.bgz \\ ; ; \--annotation StrandBiasBySample \\ ; ; \--mitochondria-mode \\ ; ; \--max-reads-per-alignment-start 75 \\ ; ; \--max-mnp-distance 0 \\ ; ; \-L chrM:8023-9140 \\ ; ; \--genotype-filtered-alleles \\ ; ; \--debug-assembly-variants-out /rej.vcf \\ ; ; \--bam-output bamout.bam. In this instance the variant in question is listed in the rej.vcf file obtained via `--debug-assembly-variants-out`. I have examined `bamout.bam` as well as the input bam and there appears to be ample coverage at the site of interest (the T at position 8316 is the position of interest, highlighted):. ![](https://gatk.broadinstitute.org/hc/user_images/aGbHKebG7Tb8Lgu33gGzXw.png). I have tried running this with some of the additional parameters in \[[https://gatk.broadinstitute.o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7672:414,error,error,414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7672,1,['error'],['error']
Availability,"This request was created from a contribution made by Yangyxt on August 30, 2021 08:18 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4405983290395-run-into-PythonScriptExecutorException-when-executing-PostprocessGermlineCNVCalls-about-positional-arguments](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405983290395-run-into-PythonScriptExecutorException-when-executing-PostprocessGermlineCNVCalls-about-positional-arguments). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.2.0 ; ; b) Exact command used:. ${gatk} PostprocessGermlineCNVCalls \\. \--model-shard-path ${gCNV\_model\_prefix}-model \\. \--calls-shard-path ${gCNV\_case\_prefix}-calls \\. \--allosomal-contig chrX --allosomal-contig chrY \\. \--contig-ploidy-calls ${ploidy\_case\_prefix}-calls \\. \--sample-index ${sample\_index} \\. \--output-denoised-copy-ratios ${cnv\_dir}/${sampleID}.sample\_${sample\_index}.denoised\_copy\_ration.tsv \\. \--output-genotyped-intervals ${cnv\_dir}/genotyped-intervals-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--output-genotyped-segments ${cnv\_dir}/genotyped-segments-case-${sampleID}-vs-${probe}cohort.vcf.gz \\. \--sequence-dictionary ${ref\_gen}/ucsc.hg19.dict. c) Entire error log:. 11:04:20.841 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Aug 30, 2021 11:04:20 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:04:20.983 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - For support and documentation go to [https://software.broadinstitute.org/gat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444:487,error,error,487,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444,1,['error'],['error']
Availability,"This request was created from a contribution made by Yanis Chrys on August 19, 2021 11:35 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4405429551515-JEXL-expression-for-filtering-on-AD-SelectVariants-FastaAlternateReferenceMaker-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405429551515-JEXL-expression-for-filtering-on-AD-SelectVariants-FastaAlternateReferenceMaker-). \--. Hi, ; ; I am working on haploid bacterial data and I ran into a limitation of the program that I either can't solve or it would be nice to add a funtion for it in the future. I'll explain the issue:. Let's say I have (low coverage) data that I want to turn into an alternate fasta reference where: ; ; REF: A. ALT: AAGT,T,CA. If I want to keep variants where the AD > \[threshold\] I can't do. \-select 'vc.getGenotype(""sample"").getAD.1'. because for my sample it could be that the called ALT is getAD.2 and so far I haven't been able to use anything other than a number as an index to getAD. This would be solved if we could do:. getAD.getGT OR getAD.IndexOfAlleleWithHighestCount. but to my knowledge none of these will work because JEXL will give an error. Maybe extending JEXL java operation to the AD array could fix it? Because even getAD\[0\] gives an error. Do you have a solution to this?. PS. I am sorry if this should have been under General Questions<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/177956'>Zendesk ticket #177956</a>)<br>gz#177956</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7448:1167,error,error,1167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7448,2,['error'],['error']
Availability,"This request was created from a contribution made by tc on February 09, 2022 17:49 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4418364848795-java-lang-IllegalArgumentException-Invalid-interval-in-FuncotateSegments](https://gatk.broadinstitute.org/hc/en-us/community/posts/4418364848795-java-lang-IllegalArgumentException-Invalid-interval-in-FuncotateSegments). \--. Hi,. I tried to annotated a called segment file after following the somatic CNV detection workflow of GATK:. gatk --java-options ""-Xmx10g -Djava.io.tmpdir=/lscratch/$SLURM\_JOBID"" FuncotateSegments \\ ; ; \--data-sources-path funcotator\_dataSources.v1.7.20200521s/ \\ ; ; \--ref-version hg19 \\ ; ; \--output-file-format SEG \\ ; ; \-R hs37d5.fa \\ ; ; \--segments sample.called.seg \\ ; ; \-O sample.seg.funcotated.tsv \\ ; ; \--transcript-list funcotator\_dataSources.v1.7.20200521s/transcriptList.exact\_uniprot\_matches.AKT1\_CRLF2\_FGFR1.txt. But I got the following error message:. 12:37:55.534 INFO  FuncotateSegments - The following datasources support funcotation on segments:  ; ; 12:37:55.535 INFO  FuncotateSegments -  Gencode 34 CANONICAL ; ; 12:37:55.542 INFO  FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode.  Performing conversion. ; ; 12:37:55.542 WARN  FuncotatorEngine - WARNING: You are using B37 as a reference.  Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases.  There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references. ; ; 12:37:55.679 INFO  ProgressMeter - Starting traversal ; ; 12:37:55.679 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Features Processed  Features/Minute ; ; 12:37:56.198 WARN  FuncotatorUtils - Reference allele is different than the reference coding sequence (strand: -, alt = G, ref G != T reference coding seq) @\[chr1:13839497\]!  Substituting given allele for sequence code (TTC-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7676:964,error,error,964,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676,1,['error'],['error']
Availability,"This seems like a compiler bug to me. It only produces a warning on some machines, and we're not sure which ones. It runs fine on travis which is running ubuntu 16.04 I think, and it runs fine on OSX. I'm not sure if producing the warning is the bug, or not producing the warning, but there's definitely a bug somewhere in one instance of the compiler. . You'll notice that the code it's referring to in the error message is NOT the code that's causing the issue, which is another manifestation of it's bugginess.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4248#issuecomment-360212288:408,error,error,408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4248#issuecomment-360212288,1,['error'],['error']
Availability,"This seems like a consequence of the fact that we use `java.nio.file.Path`for a lot of things in gatk. This requires a custom `java.nio.file.spi.FileSystemProvider` to be available for each type of path you want to be able to resolve. Spark native uses `org.apache.hadoop.fs.Path` for a lot of things. It's seems likely that that maprfs provides a hadoop file system plugin, which many spark applications can consume, but it's unlikely that it also provides a java.nio.file.Path implementation. ; ; I don't think we'd be able to implement a provider for maprfs ourselves. We don't have any systems with maprfs and don't have the bandwidth to take it on right now. Implementing a file system provider isn't a terribly complicated project, but it's not a trivial one either. However, there's an implementation for hadoop here https://github.com/damiencarol/jsr203-hadoop which is sufficient for what gatk does. If maprfs provides a hadoop file system, it would probably not be too difficult to take that project as a template and modify it to use the maprfs implementation. . I think the only things you'd have to implement for the spark tools to work are the basic Path operations that support the simple operations like `Paths.get()`,`Files.exists()`, and `Path.resolve()`. (although that's not a complete list. . If you are interested in writing a plugin like that, you can add it to the gatk class path at runtime. We might also be open to packaging such a plugin with the gatk if there was wide demand for it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-350070555:171,avail,available,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-350070555,2,['avail'],['available']
Availability,"This seems to be a regression with GATK 4.1.0.0. The code does check for compatible versions before beginning traversal. However, the following log was reported using the M2 WDL:; ```; Runtime.totalMemory()=58851328; ***********************************************************************. A USER ERROR has occurred: Bad input: Config file for datasource (file:///cromwell_root/funcotator_dataSources.v1.4.20180615/gencode/hg19/gencode.config) does not contain required key: ""ncbi_build_version"". ***********************************************************************; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5660#issuecomment-463020005:297,ERROR,ERROR,297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5660#issuecomment-463020005,1,['ERROR'],['ERROR']
Availability,"This seems to be a simple typo. The minimal data to calculate the segmentation cost should be `2 * windowSize`, rather than `windowSize`, as the error message indicates. In the current logic, the segmentation cost at a particular point is calculated as the difference between the sum of costs of two windows to the left and right of that point and the cost of a big window of size `2 * windowSize`. If the # of the data points is less than the `2 * windowSize`, the cost for the full window will be wrong in the circular buffer representation; it will get the wrong cost of a window of size `2 * windowSize - data_size`, instead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6835:145,error,error,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6835,1,['error'],['error']
Availability,This seems to resolve the issue (which I had no problem reproducing) locally. Interestingly when I run the test suite locally I get test failures here but apparently we never caught this on travis. There must be something different about the tests on travis... I am happy for advice as to how to write a test for this fix though because the error seems to be occurring under a dizzying array of spark/hadoop internal serialization code and I'm not sure how to test that properly. . Fixes #6513 #6738,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6741:137,failure,failures,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6741,2,"['error', 'failure']","['error', 'failures']"
Availability,"This seg fault looks like the fragment is missing the `__coords.tdb` file again. Can you do a `cd <array_name>; find . -type d '!' -exec test -e ""{}/__coords.tdb"" "";"" -print | grep ""__""`? This should list the fragments that do not have the `__coords.tdb` file at least.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722602986:9,fault,fault,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722602986,1,['fault'],['fault']
Availability,"This set of optimizations brings the GATK4 HaplotypeCaller performance into line; with GATK3.x performance. Note that HaplotypeCallerSpark is not touched by this PR (that is for a future PR). Summary of changes:. * AssemblyRegionWalker: query all intervals on each contig simultaneously, rather than individually; * GATKRead: Cache adaptor boundary, soft start/end, and cigar length; * GATKRead: add getBasesNoCopy() / getBaseQualitiesNoCopy(); * ReadPileup: speed up stratified constructor; * LIBS.lazyLoadNextAlignmentContext(): don't keep pileup elements unnecessarily separated by sample during pileup creation; * Restore faster GATK3 version of ReferenceConfidenceModel.sumMismatchingQualities(); * RefVsAnyResult: nest within ReferenceConfidenceModel, and allow direct field access; * Remove redundant getBases() call in ReadThreadingGraph; * Fix BaseGraph Utils.validateArg() call; * ReadPileup: replace Collections.unmodifiableList(pileupElements).iterator() with direct return of an iterator that forbids removal; * Kill expensive bounds checking in GATKRead getBase()/getBaseQuality()/getCigarElement(); * Kill nonNull checks in PileupElement; * Kill expensive PileupElement and ReadPileup arg validation; * GATKRead adapter: clear cached values upon mutation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4031:798,redundant,redundant,798,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4031,1,['redundant'],['redundant']
Availability,"This should emit a big file with lines consisting of: a kmer, and the count in the pileup of each of the following to occur at the *center* of the kmer:. * no error; * A substitution; * C substitution; * G substitution; * T substitution; * *beginning* of deletion; * *beginning* of insertion. We might later get fancier and distinguish between different lengths of indels.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3092:159,error,error,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3092,1,['error'],['error']
Availability,This should fix the failures.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/217:20,failure,failures,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/217,1,['failure'],['failures']
Availability,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7682:27,failure,failure,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682,3,"['down', 'error', 'failure']","['downloaded', 'error', 'failure']"
Availability,"This should hopefully fix the out of memory errors on CircleCI, although it's hard to test since they happen kind of randomly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/654:44,error,errors,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/654,1,['error'],['errors']
Availability,This should resolve #650. This may require a change to the hellbender-protected build to include sonatype snapshots as an available maven repo if it doesn't already. This is the same issue as #779.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/801:122,avail,available,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/801,1,['avail'],['available']
Availability,This should work around a rare error in `_calculate_new_intervals` that could generate invalid partitioners in a way that; `calculate_new_intervals` cannot.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8603:31,error,error,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8603,1,['error'],['error']
Availability,"This sounds like a good idea, but it might be tricky because much of the file reading is likely done by native code that we're just wrapping. We could do something like automatically downloading and caching the file locally and handing the cached version to the native library. Would that suit your needs?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3178#issuecomment-314147710:183,down,downloading,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3178#issuecomment-314147710,1,['down'],['downloading']
Availability,"This test input is malformed. When I try to read it with the Dataflow code, I get this error:. htsjdk.samtools.SAMFormatException: SAM validation error: ERROR: Record 129, Read name 809R9ABXX101220:5:6:17918:145992, Mate Alignment start should be 0 because reference name = *. Here's the corresponding read:. 809R9ABXX101220:5:6:17918:145992 97 17 69400 37 67M9S \* 71202348 0 ACTCCCCACCTTACCTGACTCCTTCCAGGGTTTGTCGCCTTTCCGGTCCCTGACCCCAGTGGATGGGAGTCTGTCC ?ABDDEEABEECBDBDAB=DEDCDEEBFADABCEAD?EEEDCFE?ABEEE@FCDEEEBF@F?C<E@########## MD:Z:67 PG:Z:BWA RG:Z:809R9.5 AM:i:0 NM:i:0 SM:i:37 MQ:i:0 OQ:Z:DGEGGGGBFGGGGGDF8@@FGFBGGGBGCECCEEDFGGGFGFGGGBDGGF9DBFFGFBF;@>A4@@########## UQ:i:0. @droazen confirms that Picard's ValidateSAMFile utility reports that this bam has multiple errors. We should replace it with a clean input, and update the ""known good"" output accordingly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/568:87,error,error,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/568,4,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,This ticket aims to centralize small documentation errors such as typos and syntax errors that can be addressed in bulk. - [x] HaplotypeCaller doc has some syntax errors in links causing entire paragraphs to be included in the link. ; - [x] CollectAllelicCounts has a syntax error that causes a code format block to extend to most of the page (probably a missing closing tag). ; - [x] CalculateContamination has a missing `</pre>` tag that also causes a code format block to be extended to the rest of the page.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3173:51,error,errors,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3173,4,['error'],"['error', 'errors']"
Availability,"This ticket is done -- the tool has physically been run on 11k samples. We still need to move to a new google-cloud-java snapshot/release to fix the intermittent GCS failures, but that is captured by https://github.com/broadinstitute/gatk/issues/3120",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-314129844:166,failure,failures,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-314129844,1,['failure'],['failures']
Availability,"This ticket opens a discussion between @samuelklee, @davidbenjamin and anyone else who is interested towards deciding if this is useful. . I think it would be useful for (i) the outputs of CollectAllelicCounts and GetPileupSummaries and other similar tools to be in VCF format and for (ii) downstream tools that take these results to then also take in VCF format. I think it's good to adhere to standard formats, for results that already nearly resemble variant calls, for versatility. At the least, it would be great if the _format_ that is output by the two tools and accepted by downstream tools is unified. There is also discussion on whether these two particular tools should be merged. There may be other tickets on this particular discussion.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717:290,down,downstream,290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717,2,['down'],['downstream']
Availability,"This tool creates a data file containing a map from reference accessions to NCBI taxonomic IDs, and the taxonomic tree, which includes parent/child relationships as well as other metadata like the reference length and scientific name of each node. . The input files are available from the NCBI FTP server. One is a ""catalog"" file that gives the mapping from reference contig accession to taxonomic ID. There are catalog files available for RefSeq and for Genbank - the tool can take in either. . There are two other files - a ""names"" and ""nodes"" file contained in a single tarball - that contain the scientific names of each node and parent/child relationships. For convenience, the tool takes in the path to the tarball and extracts the two files automatically. The resulting database size is minimized using the given reference. Once the full NCBI taxonomy tree is built, any organism node that is neither in the reference nor an ancestor of a reference organism is removed. The resulting datafile is read in by the ClassifyReads tool (coming in a future PR) to assign relative abundance scores to each taxonomic node. Also made some changed to the way the PSTree and PSTreeNode are serialized (using Kryo read/writeObject instead of read/writeClassAndObject) so that loading old files won't break if these classes change packages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2730:270,avail,available,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2730,2,['avail'],['available']
Availability,"This user is noticing that when running BQSR with the default inflater, some blocks have compression errors that result in issues while running HaplotypeCaller. This request was created from a contribution made by Jacob Wang on November 02, 2021 08:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4409429876123--Did-not-inflate-expected-amount-Error](https://gatk.broadinstitute.org/hc/en-us/community/posts/4409429876123--Did-not-inflate-expected-amount-Error). \--. Hi! I'm doing WGS analysis of a pedigree of three individuals using GATK 4.2.0.0. Everything went on well for the first individual. However, in the step of generating gvcf file from bam file, I encountered the error \[htsjdk.samtools.SAMFormatException: Did not inflate expected amount\] in the other two of the individuals. Please help me! Thank you in advance!. a) GATK version used:. GATK 4.2.0.0. b) Exact command used:. java -jar /home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar \\ ; ; HaplotypeCaller \\ ; ; \-R /media/ngs/NGS0/Database/RefSeq/Homo\_sapiens\_NCBI\_GRCh38Decoy/Homo\_sapiens/NCBI/GRCh38Decoy/Sequence/WholeGenomeFasta/NewIndex/genome.fa \\ ; ; \-I /media/ngs/BAM5T/WGS\_analysis/Data/9\_BQSRBam/Ped-San-3\_merged\_realigned\_bqsr.bam \\ ; ; \-ERC GVCF \\ ; ; \-O /media/ngs/BAM5T/WGS\_analysis/Data/10\_gvcf/Ped-San-3\_merged\_realigned\_bqsr.g.vcf. c) Entire error log:. 14:14:32.075 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 01, 2021 2:14:32 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.573 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:14:32.573 INFO HaplotypeC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582:101,error,errors,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582,4,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"This user is receiving a memory allocation error when running MarkDuplicatesSpark. The user has tried limiting the number of executors and raising and lowering the memory allocations but continues to get the same error. . This request was created from a contribution made by Udi L on August 04, 2021 07:27 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360058452072-MarkDuplicatesSpark-consumes-enormous-amount-of-RAM#community\_comment\_4404649417755](https://gatk.broadinstitute.org/hc/en-us/community/posts/360058452072-MarkDuplicatesSpark-consumes-enormous-amount-of-RAM#community_comment_4404649417755). \--. Hi,. Did you figure it?. I have the same problem. I am using --java-options ""-Xmx80G""<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/173588'>Zendesk ticket #173588</a>)<br>gz#173588</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7406:43,error,error,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7406,2,['error'],['error']
Availability,"This user is receiving an error in their workflow when using GATK4.0.3.0. The error in the particular step can be resolved using a newer GATK version but the user has used the older version for the rest of the workflow and would like a solution that allows them to continue with the older version.; This request was created from a contribution made by HT on August 23, 2021 05:44 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4405613731739-GATK4-0-3-0-GenotypeGVCFs-Could-not-open-array-genomicsdbarray](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405613731739-GATK4-0-3-0-GenotypeGVCFs-Could-not-open-array-genomicsdbarray). \--. a) GATK version used:  **GATK 4.0.3.0**. b) Exact command used:. **\[Tool\]: GenomicsDBImport**. export TILEDB\_DISABLE\_FILE\_LOCKING=1. time ${dir\_tool\_gatk}/gatk --java-options ""-Xmx85g -Xms85g"" GenomicsDBImport \\ ; ; \-R ${dir\_refdata}/b37\_human\_g1k\_v37\_decoy.fasta \\ ; ; \--sample-name-map ${dir\_CombineGVCFs}/S2\_cohort.sample\_map \\ ; ; \--genomicsdb-workspace-path ${dir\_CombineGVCFs}/temporary/tmp4 \\ ; ; \--TMP\_DIR ${dir\_CombineGVCFs}/temporary \\ ; ; \--intervals ${dir\_CombineGVCFs}/intervals/bed3\_tmp.intervals \\ ; ; \--reader-threads 5 \\ ; ; \--batch-size 50. **\[output\]**:. folders and files in; ====================. \--genomicsdb-workspace-path ${dir\_CombineGVCFs}/temporary/tmp4; ================================================================. callset.json ; ; genomicsdb\_array ; ; \_\_tiledb\_workspace.tdb ; ; vcfheader.vcf ; ; vidmap.json. **\[Tool\]: GenotypeGVCFs**. export TILEDB\_DISABLE\_FILE\_LOCKING=1. time ${dir\_tool\_gatk}/gatk --java-options ""-Xmx4g"" GenotypeGVCFs \\ ; ; \-R ${dir\_refdata}/b37\_human\_g1k\_v37\_decoy.fasta \\ ; ; \-V gendb://${dir\_GenomicsDBImport}/tmp4 \\ ; ; \-O ${dir\_GenotypeVCFs}/tmp4.vcf.gz. c) Entire error log:. Using GATK jar /home/projects/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_sa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7442:26,error,error,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442,2,['error'],['error']
Availability,"This user received an ArrayIndexOutofBoundsException error when running GenotypeGVCFs. The user confirmed that the headers of their vcf files and the their fasta files have matching IDs and contig lengths. The user also tried running ValidateVariants and received the following error: A USER ERROR has occurred: Input MA1.g.vcf fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position 1A:3456221 are not observed at all in the sample genotypes. This request was created from a contribution made by Alon Ziv on July 07, 2021 12:21 UTC. . Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs](https://gatk.broadinstitute.org/hc/en-us/community/posts/4403256366107--java-lang-ArrayIndexOutOfBoundsException-32772-while-running-GenotypeGVCFs). \--. i don't get an error but the massge  java.lang.ArrayIndexOutOfBoundsException: 32772 ; ; i use : GATK version used: 4.2.0 ; ; b) Exact command used  this line for Geomics DBImport. gatk GenomicsDBImport -V MA1.g.vcf -V MA2.g.vcf -V MA3.g.vcf -V MH1.g.vcf -V MH2.g.vcf -V MH3.g.vcf -V F4\_1.g.vcf -V F4\_2.g.vcf -V F4\_3.g.vcf --genomicsdb-workspace-path my\_database1AB -L 1A -L 1B -L 2A -L 2B -L 3A -L 3B -L 4A -L 4B -L 5A -L 5B -L 6A -L 6B -L 7A -L 7B. and this for GenotypeGVCFs. gatk --java-options ""-Xmx12g -Xms12g"" GenotypeGVCFs -R Triticum\_dicoccoides.WEWSeq\_v.1.0.dna.toplevel.fa -V gendb://my\_database -O output.vcf.gz --new-qual --tmp-dir temp/. c) Entire error log:. Using GATK jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx12g -Xms12g -jar /home/alonzi/miniconda3/envs/rna-seq/share/gatk4-4.2.0.0-1/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R Triticum\_d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7348:53,error,error,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7348,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"This user was getting a 'java.lang.IllegalArgumentException: Dictionary cannot have size zero' error message when they submitted a VCF as the -I input instead of a BAM. It would save other users a lot of troubleshooting if we added a check and a better error message. This request was created from a contribution made by Ruiqiao Bai on September 12, 2021 01:06 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4406653433499-Why-do-I-get-java-lang-IllegalArgumentException-Dictionary-cannot-have-size-zero-when-using-GetPileupSummaries-). \--. Hi! I am using GATK4 following the tutorial \[(How to) Call somatic mutations using GATK4 Mutect2 – GATK (broadinstitute.org)\](/hc/en-us/articles/360035531132--How-to-Call-somatic-mutations-using-GATK4-Mutect2) for detecting somatic variants. I have received an error when using GetPileupSummaries. Specifically, the command line I used is: . gatk GetPileupSummaries -I /gatk/my\_data/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -V /gatk/my\_data/wgs\_processing\_facilitating\_data/hg38\_to\_hg19/lifted\_small\_exac\_common\_3.hg19.vcf.gz -O /gatk/my\_data/wgs\_BAM/step1\_3/getpileupsummaries\_LP6005115-DNA\_B07.table. The entire error log has been pasted below. May I know what might cause this problem? Thanks for your help!. Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_s amtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_leve l=2 -jar /gatk/gatk-package-4.2.0.0-local.jar GetPileupSummaries -I /gatk/my\_dat a/wgs\_BAM/step1\_1/unfiltered\_LP6005115-DNA\_B07.vcf -L /gatk/my\_data/wgs\_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7479:95,error,error,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7479,2,['error'],['error']
Availability,"This version introduces a change that (at least on my machine) fixes the mysterious ""happens only on the command line"" test failure. Also uses a newer version of genomics-dataflow because I had to fix a bug there for API_KEY to work in our setting. Finally, this version also moves the files around so they match the local tree, and changes the environment variables naming scheme to be a little more consistent.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535:124,failure,failure,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535,1,['failure'],['failure']
Availability,"This was a bug I uncovered while finalizing #5607, fixing it seems to change the GVCF blocking by enough that I would like somebody to take a look at it @ldgauthier. Specifically look at the last commit on this branch as that is what actually changes the test/code to no longer duplicate the old behavior. For a description of the error this fixes, see #5646. . Fixes #5646 . Blocked by #5607",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5665:331,error,error,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5665,1,['error'],['error']
Availability,"This was a very useful debug tool when working on issue #2685. It sends many parallel reads for a long time. This makes sure that the combination of the cloud provider's throttling and our own retry parameters allows us to eventually read everything to completion and not fail with disconnection errors. This test is disabled by default, because it takes too long to be run every time. But if there's a doubt about retries we can dust it off and run it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3070:296,error,errors,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3070,1,['error'],['errors']
Availability,This was evil and insidious and we should probably file a spark bug to produce a better error message...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296821517:88,error,error,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296821517,1,['error'],['error']
Availability,"This was my fault, sorry about that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5243#issuecomment-426388368:12,fault,fault,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5243#issuecomment-426388368,1,['fault'],['fault']
Availability,This was previously an error with GATK2.1-11 and was fixed then. More info on that [here](https://gatkforums.broadinstitute.org/gatk/discussion/comment/2102/#Comment_2102).; Now the same error is showing up with GATK4.1.0.0. Could this be a bug? As discussed during gatk4 office hours I am creating this issue ticket for it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807#issuecomment-474059338:23,error,error,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807#issuecomment-474059338,2,['error'],['error']
Availability,This will eliminate the transient Spark errors that look like this:. ```; java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_3_piece0 of broadcast_3; ```. (Reported by @akiezun.),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/886:40,error,errors,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/886,1,['error'],['errors']
Availability,"This will seemingly make the clinical pipeline data sources so large they are completely unusable. Gnomad data for the whole genome is apparently **106Gb**, exome data is **16Gb** (this would be OK, but is at the upper limit) (http://gnomad.broadinstitute.org/downloads). @LeeTL1220 - what are your thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5259#issuecomment-430327508:260,down,downloads,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5259#issuecomment-430327508,1,['down'],['downloads']
Availability,This would be a lot easier to debug if we got data to reproduce the error: https://gatk.broadinstitute.org/hc/en-us/articles/360035889671,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-582043436:68,error,error,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-582043436,1,['error'],['error']
Availability,This would cut the total download size in ~half,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6711#issuecomment-661125405:25,down,download,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6711#issuecomment-661125405,1,['down'],['download']
Availability,"Three major changes here.; 1. Added in logic to create the ploidy table during ingest (with necessary supporting class) and use it during extract automatically as part of the default joint workflow. Also removed a column that we won't need when creating it automatically.; 2. Rearranged the PAR checking logic to consolidate it in its own class (PloidyUtils.java). Successful run against tiny sample set ""PLOIDY_TEST"" in echo callset project:. https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20WGS%20Echo%20Callset%20v2/job_history/a93aa2ef-9cef-451d-8cf8-b31f1c6a8407. You'll need your aou credentials to see the results. Successful integration run on XY:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/6a9a5fdf-ffaa-4dcb-af73-56a4b25e69a4. This run shows all of the OTHER integration tests running successfully except BGE, due to the test data needing an updates for BGE X and Y:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/21664810-7516-49f2-a60c-51b2e05faf06. The only difference between those two tests running was an update to the expected values for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8994:421,echo,echo,421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8994,1,['echo'],['echo']
Availability,Tiny improvement that was languishing in an ancient and redundant PR https://github.com/broadinstitute/gatk/pull/6736,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8725:56,redundant,redundant,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8725,1,['redundant'],['redundant']
Availability,"To add some commentary to why this is happening: It looks like multiple threads are hitting this line simultaneously and based on the overload of `ArrayList.add()` this error could be triggered by multiple calls to `ensureCapacityInternal()` inside the add method:; ```; final List<ReadsPathDataSource> readSources = new ArrayList<>(threads);; final ThreadLocal<ReadsPathDataSource> threadReadSource = ThreadLocal.withInitial(; () -> {; final ReadsPathDataSource result = new ReadsPathDataSource(readArguments.getReadPaths(), factory);; readSources.add(result);; return result;; });; ```; The fix should be simple you just have to make sure ti synchronize the initialization or swap out the readSources object to one that is itself thread safe. @vruano",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899732721:169,error,error,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899732721,2,['error'],['error']
Availability,"To add some context, the test file was a file with a single no-call with basically no information at each position in the genome. A very weird (but valid) input. The problem was that we were regenerating the sequence dictionary on every site which is an expensive operation. Caching the sequence dictionary at the beginning improved the speed by 2 orders of magnitude on this file. . There are probably additional performance optimizations available for data like this. We spend a lot of time in overhead of creating empty AlleleLikelihoods objects and things like that. I don't think it's worth pursuing at the moment though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6663#issuecomment-648232849:440,avail,available,440,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6663#issuecomment-648232849,1,['avail'],['available']
Availability,To avoid quota issues when pulling down the base image during tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7102:35,down,down,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7102,1,['down'],['down']
Availability,"To chime in, there has been discussion of changing our usage of the random generator before. In order to achieve parity between HaplotypeCaller and HaplotypeCallerSpark we need to be able to reset the random generator for downsampling by site. Currently not only is it not consistent with the non-spark version but it might be internally inconsistent in downsampling between active region determination and the actual calling code. There is a PR related to this #5448",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6112#issuecomment-524887119:222,down,downsampling,222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6112#issuecomment-524887119,2,['down'],['downsampling']
Availability,"To clarify our experimental setup: for the benchmark we are using the latest release (v.1.2) somatic ""ground truth"" of the HCC1395 cell line from . [https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/seqc/Somatic_Mutation_WG/release/latest/](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/seqc/Somatic_Mutation_WG/release/latest/). It contains ~40k SNVs and ~2k INDELs in ~2.4Gb high-confidence regions.; In high-confidence regions intersected with WES bed, we still have ~1.1k SNVs and ~100 INDELs. Therefore, even in the WES analysis scenario, the SNV counts should be high enough to draw reliable conclusions when comparing performance between different callers and releases.; For WES INDELs, the counts are indeed rather low and results should be interpreted with caution.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1172101032:597,reliab,reliable,597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1172101032,1,['reliab'],['reliable']
Availability,"To clarify this one:. -If VCF(s) are the only tool input(s) (ie., no reference or reads are available), we should require that they all have sequence dictionaries in their headers, and throw if they don't. But if a reference or reads are available, we should use their dictionary and not require the VCF(s) to have one. -We should also add a `GATKTool`-level `--sequenceDictionary` argument that allows the user to provide a master sequence dictionary in the form of a `.dict` file. When this argument is specified, the sequence dictionary from the `.dict` should be used as the master dictionary everywhere, and we should not require sequence dictionaries in VCF headers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2223#issuecomment-256702908:92,avail,available,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2223#issuecomment-256702908,2,['avail'],['available']
Availability,"To clarify this ticket: in `GATKTool.initializeReads()`, just check `readArguments.getReadFiles()` for files ending with a cram extension (should see if there's a canonical method in htsjdk for checking whether a file is cram) -- if you find any and we don't have a reference according to `hasReference()`, throw a `UserException` with a clear error message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/673#issuecomment-125265449:344,error,error,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/673#issuecomment-125265449,1,['error'],['error']
Availability,"To facilitate the download of gnomAD records, I removed all info annotations that had nothing to do with `allele frequency`, leaving behind only the fields that we would want to use for our use case. The WDLs/jsons are now in the `funcotator/scripts/data_sources` directory (separate PR). All variants were kept, even those that were filtered. . The network IO slows down Funcotator significantly, but not enough to make it unusable. For this reason, and partly because gnomAD requires an internet connection, the gnomAD data sources are disabled by default.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5259#issuecomment-457704047:18,down,download,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5259#issuecomment-457704047,2,['down'],"['down', 'download']"
Availability,"To follow that up, I see the same md5 for the unpacked local jar as you had, so I don't think there's something wrong with your download. `gatk-package-4.2.5.0-local.jar` is indeed the shadow jar you're looking for. We make 2 different ones so they have more specific names than ""shadow"". `gatk-package-4.2.5.0-spark.jar` is the other one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042022123:128,down,download,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042022123,1,['down'],['download']
Availability,"To test I ran this before and after the fix. Before it would sit there after the line . ```; Tool returned:; 0; ```; for ~10 minutes before it would time out and exit. After the fix it shuts down cleanly. ```; gatk --java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 \; CreateVariantIngestFiles \; -V gs://fc-e2f6ffa2-4033-4517-98fc-889bee4cc7a6/5e6b194b-5f69-40f2-a6de-f4f3f80ce05a/ReblockGVCF/702cdbf7-0666-4ee5-b889-91ba0ffa90bd/call-Reblock/HG00405.haplotypeCalls.er.raw.vcf.gz.rb.g.vcf.gz \; -L chr20:1-100000 \; -IG FORTY \; --ignore-above-gq-threshold false \; --project-id broad-dsp-spec-ops \; --dataset-name gvs_qs_v2_kc \; --output-type BQ \; --enable-reference-ranges true \; --enable-pet false \; --enable-vet true \; -SN ERS4367795 \; --gvs-sample-id 99 \; --ref-version 38; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7571:191,down,down,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7571,1,['down'],['down']
Availability,"To update:. Chris and I just tested copying the bam and indices (3 files) from `gs://broad-dsde-methods-sv/samples/G94797_CHM_MIX/WGS1` (note that Chris reports it works on this bucket) to a just-created ""directory"" `gs://broad-dsde-methods-sv/samples/G94797_CHM_MIX/WGS1/tmp`, and it fails. Also an interested behavior we noticed, and a suspicion that is hard to test (due to lack of access to time machine), that this might be related when the ""directory"" is created: any directory freshly created after October 2018 might be susceptible to this, which is also the month when newer (>66) release of NIO became available.; In the mean time, if one does ; `gsutil ls -L gs://broad-dsde-methods-sv/samples/G94797_CHM_MIX/WGS1`; you'd get, at the last line, `TOTAL: 3 objects`, which is expected, whereas if one does; `gsutil ls -L gs://broad-dsde-methods-sv/samples/G94797_CHM_MIX/WGS1/tmp`; guess what: `TOTAL: 4 objects`!; It seems to list the ""directory"" itself as an object as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5935#issuecomment-491969032:612,avail,available,612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935#issuecomment-491969032,1,['avail'],['available']
Availability,"To upgrade ApplyBQSR for cloud execution I had to:. (i) change the input to remove reads with the unaligned flag; (ii) load the recalibration report from GCS instead of shipping it as a serialized object, because Dataflow explodes if we ship too much (error is: ""malformed JSON"").; (iii) for the case of a remote execution with a local output file name, add logic to copy the output via GCS to the client's machine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/595:252,error,error,252,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/595,1,['error'],['error']
Availability,ToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 718; 11:39:07.668 DEBUG ReadThreadingGraph - Recovered 32 of 33 dangling tails; 11:39:07.713 DEBUG ReadThreadingGraph - Recovered 31 of 50 dangling heads; 11:39:07.996 DEBUG Mutect2Engine - Active Region chrM:7494-7771; 11:39:07.998 DEBUG Mutect2Engine - Extended Act Region chrM:7394-7871; 11:39:07.999 DEBUG Mutect2Engine - Ref haplotype coords chrM:7394-7871; 11:39:08.000 DEBUG Mutect2Engine - Haplotype count 128; 11:39:08.001 DEBUG Mutect2Engine - Kmer sizes count 0; 11:39:08.002 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:12.623 DEBUG Mutect2 - Processing assembly region at chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:14167,Recover,Recovered,14167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Recover'],['Recovered']
Availability,Tool to merge M2 calls into MC3 vcf for downstream processing,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5007:40,down,downstream,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5007,1,['down'],['downstream']
Availability,Tool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:184); 	... 21 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [dee81497-fad3-4d70-a33e-68a5d5584d9a] entered state [ERROR] while waiting for [DONE].; ```. I'm not sure if it's a jenkins problem or a real regression but we need to investigate it either way.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2449:2776,ERROR,ERROR,2776,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449,2,['ERROR'],['ERROR']
Availability,Tools run. These error/warn messages are new.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-297048203:17,error,error,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-297048203,1,['error'],['error']
